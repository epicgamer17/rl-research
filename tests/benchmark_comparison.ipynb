{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
                        "  from pkg_resources import resource_stream, resource_exists\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imports complete. Device check:\n",
                        "Using CPU\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import time\n",
                "import torch\n",
                "import gymnasium as gym\n",
                "from tabulate import tabulate\n",
                "import copy\n",
                "\n",
                "# Add project root to path\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
                "\n",
                "# Imports\n",
                "from game_configs.tictactoe_config import TicTacToeConfig\n",
                "from agent_configs.muzero_config import MuZeroConfig\n",
                "from agents.tictactoe_expert import TicTacToeBestAgent\n",
                "from agents.random import RandomAgent\n",
                "\n",
                "# Dynamic imports for agents\n",
                "from agents.muzero import MuZeroAgent as MuZeroRay\n",
                "from agents.muzero_tmp import MuZeroAgent as MuZeroTorchMP\n",
                "from modules.world_models.muzero_world_model import MuzeroWorldModel\n",
                "from losses.basic_losses import CategoricalCrossentropyLoss\n",
                "\n",
                "print(\"Imports complete. Device check:\")\n",
                "if torch.cuda.is_available():\n",
                "    print(\"CUDA Available\")\n",
                "else:\n",
                "    print(\"Using CPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Benchmark:\n",
                "    def __init__(self):\n",
                "        self.device = torch.device(\"cpu\")\n",
                "\n",
                "        self.game_config = TicTacToeConfig()\n",
                "\n",
                "        # Base Params\n",
                "        # self.base_params = {\n",
                "        #     \"search_batch_size\": 1,\n",
                "        #     \"use_virtual_mean\": True,\n",
                "        #     \"use_mixed_precision\": True,\n",
                "        #     \"compile\": True,\n",
                "        #     \"use_quantization\": True,\n",
                "        #     \"qat\": True,\n",
                "        #     \"transfer_interval\": 100,\n",
                "        #     \"world_model_cls\": MuzeroWorldModel,\n",
                "        #     \"minibatch_size\": 8,\n",
                "        #     \"training_steps\": 1000,\n",
                "        #     \"min_replay_buffer_size\": 100,\n",
                "        #     \"replay_buffer_size\": 500,\n",
                "        #     \"games_per_generation\": 1,\n",
                "        #     \"optimizer\": torch.optim.Adam,\n",
                "        #     \"learning_rate\": 0.001,\n",
                "        #     \"adam_epsilon\": 1e-8,\n",
                "        #     \"weight_decay\": 0,\n",
                "        #     \"momentum\": 0.9,\n",
                "        #     \"clipnorm\": 10,\n",
                "        #     \"training_iterations\": 1,\n",
                "        #     \"num_minibatches\": 1,\n",
                "        #     \"n_step\": 5,\n",
                "        #     \"discount_factor\": 0.997,\n",
                "        #     \"per_alpha\": 1,\n",
                "        #     \"per_beta\": 1,\n",
                "        #     \"per_epsilon\": 1e-6,\n",
                "        #     \"per_use_batch_weights\": False,\n",
                "        #     \"per_use_initial_max_priority\": False,\n",
                "        #     \"lstm_horizon_len\": 5,\n",
                "        #     \"value_prefix\": True,\n",
                "        #     \"reanalyze_tau\": 1,\n",
                "        #     \"lr_ratio\": 10,\n",
                "        #     \"unroll_steps\": 5,\n",
                "        #     \"reanalyze_ratio\": 0.0,\n",
                "        #     \"projector_hidden_dim\": 16,\n",
                "        #     \"predictor_hidden_dim\": 16,\n",
                "        #     \"projector_output_dim\": 16,\n",
                "        #     \"predictor_output_dim\": 16,\n",
                "        #     \"num_simulations\": 10,\n",
                "        #     \"root_dirichlet_alpha\": 0.25,\n",
                "        #     \"root_exploration_fraction\": 0.25,\n",
                "        #     \"residual_layers\": [(16, 3, 1)] * 2,\n",
                "        #     \"conv_layers\": [(16, 3, 1)],\n",
                "        #     \"dense_layer_widths\": [],\n",
                "        # }\n",
                "        self.base_params = {\n",
                "            \"num_simulations\": 25,\n",
                "            \"per_alpha\": 0.0,\n",
                "            \"per_beta\": 0.0,\n",
                "            \"per_beta_final\": 0.0,\n",
                "            \"n_step\": 10,\n",
                "            \"root_dirichlet_alpha\": 0.25,\n",
                "            \"residual_layers\": [(24, 3, 1)],\n",
                "            \"reward_dense_layer_widths\": [],\n",
                "            \"reward_conv_layers\": [(16, 1, 1)],\n",
                "            \"actor_dense_layer_widths\": [],\n",
                "            \"actor_conv_layers\": [(16, 1, 1)],\n",
                "            \"critic_dense_layer_widths\": [],\n",
                "            \"critic_conv_layers\": [(16, 1, 1)],\n",
                "            \"to_play_dense_layer_widths\": [],\n",
                "            \"to_play_conv_layers\": [(16, 1, 1)],\n",
                "            \"known_bounds\": [-1, 1],\n",
                "            \"support_range\": None,\n",
                "            \"minibatch_size\": 8,\n",
                "            \"replay_buffer_size\": 10000,\n",
                "            \"gumbel\": False,\n",
                "            \"gumbel_m\": 16,\n",
                "            \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
                "            \"training_steps\": 100,  # Reduced for benchmark speed\n",
                "            \"transfer_interval\": 100,\n",
                "            \"num_workers\": 4,\n",
                "            \"world_model_cls\": MuzeroWorldModel,\n",
                "            \"search_batch_size\": 5,  # Iterative\n",
                "            \"use_virtual_mean\": True,\n",
                "            \"virtual_loss\": 3.0,\n",
                "            \"use_torch_compile\": True,\n",
                "            \"use_mixed_precision\": True,\n",
                "            \"use_quantization\": True,\n",
                "            \"qat\": True,\n",
                "        }\n",
                "\n",
                "    def run_benchmark(self, agent_cls, name, num_steps=400):\n",
                "        print(f\"\\n--- Benchmarking {name} ---\")\n",
                "\n",
                "        # 1. Setup Config\n",
                "        params = self.base_params.copy()\n",
                "        params[\"training_steps\"] = num_steps\n",
                "        params[\"multi_process\"] = True\n",
                "        params[\"num_workers\"] = 4\n",
                "\n",
                "        # Create Config Object\n",
                "        config = MuZeroConfig(params, self.game_config)\n",
                "\n",
                "        # Create Environment\n",
                "        env = self.game_config.make_env()\n",
                "\n",
                "        # Instantiate Agent\n",
                "        test_agents = [RandomAgent(), TicTacToeBestAgent()]\n",
                "        try:\n",
                "            agent = agent_cls(\n",
                "                env=env,\n",
                "                config=config,\n",
                "                name=f\"bench_{name.lower().replace(' ', '_')}\",\n",
                "                device=torch.device(\"cpu\"),\n",
                "                test_agents=test_agents,\n",
                "            )\n",
                "\n",
                "            # Override testing to avoid slowdowns\n",
                "            agent.test_interval = 100000\n",
                "            agent.checkpoint_interval = 100\n",
                "\n",
                "            # Run Training\n",
                "            print(f\"  Starting training for {num_steps} steps...\")\n",
                "            start_time = time.time()\n",
                "\n",
                "            agent.training_step = 0\n",
                "            agent.train()\n",
                "\n",
                "            end_time = time.time()\n",
                "            duration = end_time - start_time\n",
                "            print(f\"  Finished. Time: {duration:.2f}s\")\n",
                "            return duration\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"  FAILED: {e}\")\n",
                "            import traceback\n",
                "\n",
                "            traceback.print_exc()\n",
                "            return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Benchmark initialized.\n"
                    ]
                }
            ],
            "source": [
                "bench = Benchmark()\n",
                "results = []\n",
                "print(\"Benchmark initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Benchmarking MuZero (Ray) ---\n",
                        "Using default save_intermediate_weights     : False\n",
                        "Using         training_steps                : 1000\n",
                        "Using default adam_epsilon                  : 1e-08\n",
                        "Using default momentum                      : 0.9\n",
                        "Using default learning_rate                 : 0.001\n",
                        "Using default clipnorm                      : 0\n",
                        "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
                        "Using default weight_decay                  : 0.0\n",
                        "Using default num_minibatches               : 1\n",
                        "Using default training_iterations           : 1\n",
                        "Using default lr_schedule_type              : none\n",
                        "Using default lr_schedule_steps             : []\n",
                        "Using default lr_schedule_steps             : []\n",
                        "Using default lr_schedule_values            : []\n",
                        "Using         use_mixed_precision           : True\n",
                        "Using         use_torch_compile             : True\n",
                        "Using default compile_mode                  : reduce-overhead\n",
                        "Using         minibatch_size                : 8\n",
                        "Using         replay_buffer_size            : 10000\n",
                        "Using default min_replay_buffer_size        : 8\n",
                        "Using         n_step                        : 10\n",
                        "Using default discount_factor               : 0.99\n",
                        "Using         per_alpha                     : 0.0\n",
                        "Using         per_beta                      : 0.0\n",
                        "Using         per_beta_final                : 0.0\n",
                        "Using default per_epsilon                   : 1e-06\n",
                        "Using default per_use_batch_weights         : False\n",
                        "Using default per_use_initial_max_priority  : True\n",
                        "Using default loss_function                 : <class 'losses.basic_losses.MSELoss'>\n",
                        "Using default activation                    : relu\n",
                        "Using         kernel_initializer            : None\n",
                        "Using         prob_layer_initializer        : None\n",
                        "Using default norm_type                     : none\n",
                        "Using default soft_update                   : False\n",
                        "Using default min_max_epsilon               : 0.01\n",
                        "Using         world_model_cls               : <class 'modules.world_models.muzero_world_model.MuzeroWorldModel'>\n",
                        "Using         known_bounds                  : [-1, 1]\n",
                        "Using         residual_layers               : [(24, 3, 1)]\n",
                        "Using default conv_layers                   : []\n",
                        "Using default dense_layer_widths            : []\n",
                        "Using default representation_residual_layers: [(24, 3, 1)]\n",
                        "Using default representation_conv_layers    : []\n",
                        "Using default representation_dense_layer_widths: []\n",
                        "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
                        "Using default dynamics_conv_layers          : []\n",
                        "Using default dynamics_dense_layer_widths   : []\n",
                        "Using         reward_conv_layers            : [(16, 1, 1)]\n",
                        "Using         reward_dense_layer_widths     : []\n",
                        "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
                        "Using         to_play_dense_layer_widths    : []\n",
                        "Using         critic_conv_layers            : [(16, 1, 1)]\n",
                        "Using         critic_dense_layer_widths     : []\n",
                        "Using         actor_conv_layers             : [(16, 1, 1)]\n",
                        "Using         actor_dense_layer_widths      : []\n",
                        "Using default noisy_sigma                   : 0.0\n",
                        "Using default games_per_generation          : 100\n",
                        "Using default value_loss_factor             : 1.0\n",
                        "Using default to_play_loss_factor           : 1.0\n",
                        "Using         num_simulations               : 25\n",
                        "Using         search_batch_size             : 5\n",
                        "Using         use_virtual_mean              : True\n",
                        "Using         virtual_loss                  : 3.0\n",
                        "Using         root_dirichlet_alpha          : 0.25\n",
                        "Using default root_exploration_fraction     : 0.25\n",
                        "Using default root_dirichlet_alpha_adaptive : False\n",
                        "Using         gumbel                        : False\n",
                        "Using         gumbel_m                      : 16\n",
                        "Using default gumbel_cvisit                 : 50\n",
                        "Using default gumbel_cscale                 : 1.0\n",
                        "Using default pb_c_base                     : 19652\n",
                        "Using default pb_c_init                     : 1.25\n",
                        "Using default temperatures                  : [1.0, 0.0]\n",
                        "Using default temperature_updates           : [5]\n",
                        "Using default temperature_with_training_steps: False\n",
                        "Using default clip_low_prob                 : 0.0\n",
                        "Using default value_loss_function           : <losses.basic_losses.MSELoss object at 0x315a142f0>\n",
                        "Using default reward_loss_function          : <losses.basic_losses.MSELoss object at 0x315a15280>\n",
                        "Using         policy_loss_function          : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x14d274140>\n",
                        "Using default to_play_loss_function         : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x315a14230>\n",
                        "Using default unroll_steps                  : 5\n",
                        "Using default atom_size                     : 1\n",
                        "Using         support_range                 : None\n",
                        "Using         multi_process                 : True\n",
                        "Using         num_workers                   : 4\n",
                        "Using default lr_ratio                      : inf\n",
                        "Using         transfer_interval             : 100\n",
                        "Using default reanalyze_ratio               : 0.0\n",
                        "Using         use_quantization              : True\n",
                        "Using         qat                           : True\n",
                        "Using default reanalyze_method              : mcts\n",
                        "Using default reanalyze_tau                 : 0.3\n",
                        "Using default injection_frac                : 0.0\n",
                        "Using default reanalyze_noise               : False\n",
                        "Using default reanalyze_update_priorities   : False\n",
                        "Using default consistency_loss_factor       : 0.0\n",
                        "Using default projector_output_dim          : 128\n",
                        "Using default projector_hidden_dim          : 128\n",
                        "Using default predictor_output_dim          : 128\n",
                        "Using default predictor_hidden_dim          : 64\n",
                        "Using default mask_absorbing                : True\n",
                        "Using default value_prefix                  : False\n",
                        "Using default lstm_horizon_len              : 5\n",
                        "Using default lstm_hidden_size              : 64\n",
                        "Using default q_estimation_method           : v_mix\n",
                        "Using default stochastic                    : False\n",
                        "Using default use_true_chance_codes         : False\n",
                        "Using default num_chance                    : 32\n",
                        "Using default sigma_loss                    : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x315a169c0>\n",
                        "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
                        "Using default afterstate_conv_layers        : []\n",
                        "Using default afterstate_dense_layer_widths : []\n",
                        "Using default chance_conv_layers            : [(32, 3, 1)]\n",
                        "Using default chance_dense_layer_widths     : [256]\n",
                        "Using default vqvae_commitment_cost_factor  : 1.0\n",
                        "Using default action_embedding_dim          : 32\n",
                        "Using default single_action_plane           : False\n",
                        "Using default latent_viz_method             : umap\n",
                        "Using default latent_viz_interval           : 10\n",
                        "[bench_muzero_(ray)] Using device: cpu\n",
                        "Observation dimensions: torch.Size([9, 3, 3])\n",
                        "Num actions: 9 (Discrete: True)\n",
                        "Making test env...\n",
                        "Test env configured for video recording.\n",
                        "MARL Agent 'bench_muzero_(ray)' initialized. Test agents: ['random', 'tictactoe_expert']\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-28 20:20:41,112\tINFO worker.py:2007 -- Started a local Ray instance.\n",
                        "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Max size: 10000\n",
                        "Initializing stat 'score' with subkeys None\n",
                        "Initializing stat 'policy_loss' with subkeys None\n",
                        "Initializing stat 'value_loss' with subkeys None\n",
                        "Initializing stat 'reward_loss' with subkeys None\n",
                        "Initializing stat 'to_play_loss' with subkeys None\n",
                        "Initializing stat 'cons_loss' with subkeys None\n",
                        "Initializing stat 'loss' with subkeys None\n",
                        "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
                        "Initializing stat 'episode_length' with subkeys None\n",
                        "Initializing stat 'policy_entropy' with subkeys None\n",
                        "Initializing stat 'value_diff' with subkeys None\n",
                        "Initializing stat 'policy_improvement' with subkeys ['network', 'search']\n",
                        "Initializing stat 'root_children_values' with subkeys None\n",
                        "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_1_score', 'player_2_score', 'player_1_win%', 'player_2_win%']\n",
                        "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_1_score', 'player_2_score', 'player_1_win%', 'player_2_win%']\n",
                        "  Starting training for 1000 steps...\n",
                        "Compiling models in train()...\n",
                        "Broadcasting initial weights to workers...\n",
                        "Starting initial batch of games...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(MuZeroWorker pid=6493)\u001b[0m /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
                        "\u001b[36m(MuZeroWorker pid=6493)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(MuZeroWorker pid=6497)\u001b[0m Hidden state shape: (1, 24, 3, 3)\n",
                        "\u001b[36m(MuZeroWorker pid=6497)\u001b[0m Hidden state shape: (1, 24, 3, 3)\n",
                        "\u001b[36m(MuZeroWorker pid=6497)\u001b[0m encoder input shape (1, 18, 3, 3)\n",
                        "Size: 0\n",
                        "Size: 7\n",
                        "0\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[8, 6, 4, 2, 0],\n",
                        "        [6, 4, 2, 0, 0],\n",
                        "        [2, 0, 0, 4, 3],\n",
                        "        [0, 0, 7, 4, 3],\n",
                        "        [7, 2, 1, 8, 0],\n",
                        "        [1, 8, 0, 5, 0],\n",
                        "        [8, 0, 5, 0, 3],\n",
                        "        [0, 5, 0, 4, 3]])\n",
                        "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
                        "predicted values tensor([[[-0.0254],\n",
                        "         [-0.0874],\n",
                        "         [-0.1074],\n",
                        "         [-0.1245],\n",
                        "         [-0.1089],\n",
                        "         [-0.0947]],\n",
                        "\n",
                        "        [[-0.1680],\n",
                        "         [-0.1680],\n",
                        "         [-0.1533],\n",
                        "         [-0.1484],\n",
                        "         [-0.1270],\n",
                        "         [-0.1108]],\n",
                        "\n",
                        "        [[-0.0884],\n",
                        "         [-0.0391],\n",
                        "         [-0.0069],\n",
                        "         [-0.0062],\n",
                        "         [-0.0376],\n",
                        "         [-0.0273]],\n",
                        "\n",
                        "        [[-0.0212],\n",
                        "         [ 0.0069],\n",
                        "         [ 0.0076],\n",
                        "         [ 0.0076],\n",
                        "         [ 0.0014],\n",
                        "         [ 0.0068]],\n",
                        "\n",
                        "        [[-0.0254],\n",
                        "         [-0.0564],\n",
                        "         [-0.0557],\n",
                        "         [-0.0630],\n",
                        "         [-0.0684],\n",
                        "         [-0.0645]],\n",
                        "\n",
                        "        [[ 0.0069],\n",
                        "         [-0.0096],\n",
                        "         [-0.0220],\n",
                        "         [-0.0309],\n",
                        "         [-0.0275],\n",
                        "         [-0.0280]],\n",
                        "\n",
                        "        [[-0.0889],\n",
                        "         [-0.0977],\n",
                        "         [-0.1099],\n",
                        "         [-0.1187],\n",
                        "         [-0.1206],\n",
                        "         [-0.1045]],\n",
                        "\n",
                        "        [[-0.0957],\n",
                        "         [-0.0874],\n",
                        "         [-0.0806],\n",
                        "         [-0.0771],\n",
                        "         [-0.0493],\n",
                        "         [-0.0198]]], dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
                        "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [-0.0232],\n",
                        "         [-0.0454],\n",
                        "         [-0.0649],\n",
                        "         [-0.0879],\n",
                        "         [-0.0835]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0688],\n",
                        "         [-0.0688],\n",
                        "         [-0.0620],\n",
                        "         [-0.0684],\n",
                        "         [-0.0613]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0713],\n",
                        "         [-0.0640],\n",
                        "         [-0.0554],\n",
                        "         [-0.0635],\n",
                        "         [-0.0732]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1113],\n",
                        "         [-0.1104],\n",
                        "         [-0.0996],\n",
                        "         [-0.0830],\n",
                        "         [-0.0757]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0459],\n",
                        "         [-0.0635],\n",
                        "         [-0.0732],\n",
                        "         [-0.0835],\n",
                        "         [-0.0879]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0371],\n",
                        "         [-0.0654],\n",
                        "         [-0.0894],\n",
                        "         [-0.1069],\n",
                        "         [-0.1113]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0554],\n",
                        "         [-0.0320],\n",
                        "         [-0.0135],\n",
                        "         [-0.0175],\n",
                        "         [-0.0144]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0405],\n",
                        "         [-0.0381],\n",
                        "         [-0.0300],\n",
                        "         [-0.0505],\n",
                        "         [-0.0776]]], grad_fn=<PermuteBackward0>)\n",
                        "target to plays tensor([[[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.6211, 0.3809],\n",
                        "         [0.6133, 0.3867],\n",
                        "         [0.6055, 0.3965],\n",
                        "         [0.5977, 0.4023],\n",
                        "         [0.5898, 0.4102]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5273, 0.4746],\n",
                        "         [0.5312, 0.4707],\n",
                        "         [0.5430, 0.4570],\n",
                        "         [0.5508, 0.4492],\n",
                        "         [0.5547, 0.4434]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4941, 0.5078],\n",
                        "         [0.4883, 0.5117],\n",
                        "         [0.4902, 0.5117],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.5078, 0.4941]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5859, 0.4141],\n",
                        "         [0.5781, 0.4199],\n",
                        "         [0.5742, 0.4258],\n",
                        "         [0.5742, 0.4258],\n",
                        "         [0.5820, 0.4160]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.6055, 0.3945],\n",
                        "         [0.6016, 0.3965],\n",
                        "         [0.5977, 0.4023],\n",
                        "         [0.5859, 0.4121],\n",
                        "         [0.5820, 0.4160]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5742, 0.4258],\n",
                        "         [0.5742, 0.4258],\n",
                        "         [0.5742, 0.4277],\n",
                        "         [0.5781, 0.4199],\n",
                        "         [0.5781, 0.4199]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5312, 0.4688],\n",
                        "         [0.5391, 0.4609],\n",
                        "         [0.5469, 0.4512],\n",
                        "         [0.5508, 0.4492],\n",
                        "         [0.5625, 0.4375]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5938, 0.4062],\n",
                        "         [0.6016, 0.4004],\n",
                        "         [0.5977, 0.4004],\n",
                        "         [0.5977, 0.4043],\n",
                        "         [0.5938, 0.4062]]], grad_fn=<PermuteBackward0>)\n",
                        "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True, False, False]])\n",
                        "Initializing stat 'q_loss' with subkeys None\n",
                        "Initializing stat 'sigma_loss' with subkeys None\n",
                        "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
                        "Size: 15\n",
                        "Size: 24\n",
                        "Size: 31\n",
                        "Size: 41\n",
                        "Size: 49\n",
                        "Size: 56\n",
                        "Size: 64\n",
                        "Size: 74\n",
                        "Size: 82\n",
                        "Size: 90\n",
                        "Size: 99\n",
                        "Size: 107\n",
                        "Size: 113\n",
                        "Size: 122\n",
                        "Size: 131\n",
                        "Size: 141\n",
                        "Size: 151\n",
                        "Size: 161\n",
                        "Size: 171\n",
                        "Size: 177\n",
                        "Size: 187\n",
                        "Size: 193\n",
                        "Size: 199\n",
                        "Size: 209\n",
                        "Size: 216\n",
                        "Size: 225\n",
                        "Size: 234\n",
                        "Size: 244\n",
                        "Size: 254\n",
                        "Size: 264\n",
                        "Size: 273\n",
                        "Size: 281\n",
                        "Size: 290\n",
                        "Size: 300\n",
                        "Size: 310\n",
                        "Size: 319\n",
                        "Size: 329\n",
                        "Size: 339\n",
                        "Size: 346\n",
                        "Size: 356\n",
                        "Size: 366\n",
                        "Size: 375\n",
                        "Size: 385\n",
                        "Size: 395\n",
                        "Size: 401\n",
                        "Size: 409\n",
                        "Size: 415\n",
                        "Size: 424\n",
                        "Size: 431\n",
                        "Size: 440\n",
                        "Size: 448\n",
                        "Size: 454\n",
                        "Size: 461\n",
                        "Size: 467\n",
                        "Size: 477\n",
                        "Size: 487\n",
                        "Size: 495\n",
                        "Size: 505\n",
                        "Size: 512\n",
                        "Size: 520\n",
                        "Size: 529\n",
                        "Size: 538\n",
                        "Size: 546\n",
                        "Size: 555\n",
                        "Size: 563\n",
                        "Size: 569\n",
                        "Size: 579\n",
                        "Size: 589\n",
                        "Size: 598\n",
                        "Size: 606\n",
                        "Size: 616\n",
                        "Size: 626\n",
                        "Size: 632\n",
                        "Size: 642\n",
                        "Size: 651\n",
                        "Size: 661\n",
                        "Size: 670\n",
                        "Size: 680\n",
                        "Size: 690\n",
                        "Size: 699\n",
                        "Size: 707\n",
                        "Size: 716\n",
                        "Size: 726\n",
                        "Size: 736\n",
                        "Size: 746\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:20:51,158 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3703 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[36m(MuZeroWorker pid=6499)\u001b[0m /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
                        "\u001b[36m(MuZeroWorker pid=6499)\u001b[0m   from pkg_resources import resource_stream, resource_exists\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Size: 754\n",
                        "Size: 764\n",
                        "Size: 772\n",
                        "Size: 779\n",
                        "Size: 789\n",
                        "Size: 797\n",
                        "Size: 807\n",
                        "Size: 815\n",
                        "Size: 821\n",
                        "Size: 828\n",
                        "Size: 837\n",
                        "Size: 847\n",
                        "Size: 857\n",
                        "Size: 865\n",
                        "Size: 874\n",
                        "\u001b[36m(MuZeroWorker pid=6497)\u001b[0m Worker 1: Compiling INT8 model...\n",
                        "\u001b[36m(MuZeroWorker pid=6494)\u001b[0m Hidden state shape: (1, 24, 3, 3)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
                        "\u001b[36m(MuZeroWorker pid=6494)\u001b[0m encoder input shape (1, 18, 3, 3)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
                        "Size: 884\n",
                        "Size: 890\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m(MuZeroWorker pid=6493)\u001b[0m /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:368: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
                        "\u001b[36m(MuZeroWorker pid=6493)\u001b[0m   if not check_min_max_valid(min_val, max_val):\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Size: 898\n",
                        "Size: 907\n",
                        "Size: 916\n",
                        "Size: 924\n",
                        "Size: 934\n",
                        "Size: 942\n",
                        "Size: 950\n",
                        "Size: 959\n",
                        "Size: 965\n",
                        "Size: 971\n",
                        "Size: 981\n",
                        "Size: 990\n",
                        "Size: 997\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:01,247 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3701 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[36m(MuZeroWorker pid=6499)\u001b[0m /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:368: UserWarning: must run observer before calling calculate_qparams. Returning default values.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
                        "\u001b[36m(MuZeroWorker pid=6499)\u001b[0m   if not check_min_max_valid(min_val, max_val):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
                        "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-28 20:21:10,187 E 6486 57593] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
                        "\u001b[36m(MuZeroWorker pid=6499)\u001b[0m /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:368: UserWarning: must run observer before calling calculate_qparams. Returning default values.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
                        "\u001b[36m(MuZeroWorker pid=6499)\u001b[0m   if not check_min_max_valid(min_val, max_val):\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:11,114 E 6490 57705] (raylet) main.cc:1032: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:11,332 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3699 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[36m(MuZeroWorker pid=6493)\u001b[0m [2026-01-28 20:21:11,740 E 6493 57957] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
                        "[2026-01-28 20:21:11,906 E 6419 57753] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:21,411 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3687 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[36m(pid=6495)\u001b[0m [2026-01-28 20:21:11,886 E 6495 58172] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:31,498 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3688 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Interrupted by User\n",
                        "Shutting down workers...\n",
                        "All workers shut down.\n",
                        "Finished Training\n",
                        "Testing Player 0 vs Agent random\n",
                        "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0800, 0.1200, 0.4000, 0.0400, 0.0400, 0.0400, 0.1600]), tensor([0.0800, 0.0400, 0.0800, 0.1200, 0.4000, 0.0400, 0.0400, 0.0400, 0.1600]), 0.1384068141974795, tensor(4), {'network_policy': tensor([0.1120, 0.0990, 0.1237, 0.1472, 0.1472, 0.0735, 0.1014, 0.0811, 0.1148]), 'network_value': 0.16412188112735748, 'search_policy': tensor([0.0800, 0.0400, 0.0800, 0.1200, 0.4000, 0.0400, 0.0400, 0.0400, 0.1600]), 'search_value': 0.1384068141974795, 'root_children_values': tensor([-0.1376,  0.0484, -0.1193, -0.1606, -0.2906, -0.0552, -0.1403, -0.0168,\n",
                        "        -0.1755])})\n",
                        "action: 4\n",
                        "Player 1 random action: 6\n",
                        "Player 0 prediction: (tensor([0.0800, 0.0400, 0.2000, 0.1200, 0.0000, 0.3200, 0.0000, 0.0400, 0.2000]), tensor([0.0800, 0.0400, 0.2000, 0.1200, 0.0000, 0.3200, 0.0000, 0.0400, 0.2000]), 0.19292533192578587, tensor(5), {'network_policy': tensor([0.1234, 0.0897, 0.2067, 0.1502, 0.0000, 0.1234, 0.0000, 0.0755, 0.2281]), 'network_value': 0.21484263241291046, 'search_policy': tensor([0.0800, 0.0400, 0.2000, 0.1200, 0.0000, 0.3200, 0.0000, 0.0400, 0.2000]), 'search_value': 0.19292533192578587, 'root_children_values': tensor([-0.1072,  0.0139, -0.1390, -0.1695,  0.0000, -0.2130,  0.0000, -0.0556,\n",
                        "        -0.2438])})\n",
                        "action: 5\n",
                        "Player 1 random action: 2\n",
                        "Player 0 prediction: (tensor([0.2400, 0.0400, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.3600]), tensor([0.2400, 0.0400, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.3600]), 0.1813569793451194, tensor(8), {'network_policy': tensor([0.1926, 0.1306, 0.0000, 0.2174, 0.0000, 0.0000, 0.0000, 0.1371, 0.3129]), 'network_value': 0.045467559248209, 'search_policy': tensor([0.2400, 0.0400, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.3600]), 'search_value': 0.1813569793451194, 'root_children_values': tensor([-0.0561,  0.1346,  0.0000, -0.1020,  0.0000,  0.0000,  0.0000,  0.0247,\n",
                        "        -0.1267])})\n",
                        "action: 8\n",
                        "Player 1 random action: 1\n",
                        "Player 0 prediction: (tensor([0.6000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000]), tensor([0.6000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000]), 0.19927537187651387, tensor(0), {'network_policy': tensor([0.3942, 0.0000, 0.0000, 0.2503, 0.0000, 0.0000, 0.0000, 0.3415, 0.0000]), 'network_value': 0.25497865676879883, 'search_policy': tensor([0.6000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000]), 'search_value': 0.19927537187651387, 'root_children_values': tensor([ 0.0157,  0.0000,  0.0000, -0.0280,  0.0000,  0.0000,  0.0000,  0.0479,\n",
                        "         0.0000])})\n",
                        "action: 0\n",
                        "Player 0 (player_1) win percentage vs random: 50.0 and average score: 0.0\n",
                        "Testing Player 1 vs Agent random\n",
                        "Player 0 random action: 2\n",
                        "Player 1 prediction: (tensor([0.1600, 0.0400, 0.0000, 0.1600, 0.2800, 0.0800, 0.0800, 0.0800, 0.1200]), tensor([0.1600, 0.0400, 0.0000, 0.1600, 0.2800, 0.0800, 0.0800, 0.0800, 0.1200]), -0.26409365473483837, tensor(4), {'network_policy': tensor([0.1036, 0.0990, 0.0000, 0.1160, 0.1907, 0.0904, 0.1299, 0.1299, 0.1390]), 'network_value': -0.14779742062091827, 'search_policy': tensor([0.1600, 0.0400, 0.0000, 0.1600, 0.2800, 0.0800, 0.0800, 0.0800, 0.1200]), 'search_value': -0.26409365473483837, 'root_children_values': tensor([0.2162, 0.3411, 0.0000, 0.2243, 0.1410, 0.2669, 0.2476, 0.2369, 0.2851])})\n",
                        "action: 4\n",
                        "Player 0 random action: 8\n",
                        "Player 1 prediction: (tensor([0.1600, 0.0800, 0.0000, 0.0800, 0.0000, 0.2000, 0.4000, 0.0800, 0.0000]), tensor([0.1600, 0.0800, 0.0000, 0.0800, 0.0000, 0.2000, 0.4000, 0.0800, 0.0000]), -0.2169219521786106, tensor(6), {'network_policy': tensor([0.1717, 0.1570, 0.0000, 0.1501, 0.0000, 0.1311, 0.2633, 0.1225, 0.0000]), 'network_value': -0.14782482385635376, 'search_policy': tensor([0.1600, 0.0800, 0.0000, 0.0800, 0.0000, 0.2000, 0.4000, 0.0800, 0.0000]), 'search_value': -0.2169219521786106, 'root_children_values': tensor([0.2001, 0.2581, 0.0000, 0.2790, 0.0000, 0.2119, 0.2328, 0.2578, 0.0000])})\n",
                        "action: 6\n",
                        "Player 0 random action: 3\n",
                        "Player 1 prediction: (tensor([0.2000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.4400, 0.0000]), tensor([0.2000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.4400, 0.0000]), -0.12694582653288974, tensor(7), {'network_policy': tensor([0.2493, 0.2607, 0.0000, 0.0000, 0.0000, 0.2134, 0.0000, 0.2549, 0.0000]), 'network_value': -0.13486066460609436, 'search_policy': tensor([0.2000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.4400, 0.0000]), 'search_value': -0.12694582653288974, 'root_children_values': tensor([0.2084, 0.2835, 0.0000, 0.0000, 0.0000, 0.2649, 0.0000, 0.2777, 0.0000])})\n",
                        "action: 7\n",
                        "Player 0 random action: 0\n",
                        "Player 1 prediction: (tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.6400, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.6400, 0.0000, 0.0000, 0.0000]), -0.04518380462514546, tensor(5), {'network_policy': tensor([0.0000, 0.4757, 0.0000, 0.0000, 0.0000, 0.4757, 0.0000, 0.0000, 0.0000]), 'network_value': -0.09596306085586548, 'search_policy': tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.6400, 0.0000, 0.0000, 0.0000]), 'search_value': -0.04518380462514546, 'root_children_values': tensor([0.0000, 0.3045, 0.0000, 0.0000, 0.0000, 0.2641, 0.0000, 0.0000, 0.0000])})\n",
                        "action: 5\n",
                        "Player 0 random action: 1\n",
                        "Player 1 (player_2) win percentage vs random: 0.0 and average score: -1.0\n",
                        "Results vs random: {'player_1_score': 0.0, 'player_1_win%': 0.5, 'player_2_score': -1.0, 'player_2_win%': 0.0, 'score': -0.5}\n",
                        "Testing Player 0 vs Agent tictactoe_expert\n",
                        "Started recording episode 4 to checkpoints/bench_muzero_(ray)/step_574/videos/tictactoe_expert/episode_000004.mp4\n",
                        "Player 0 prediction: (tensor([0.0800, 0.0400, 0.1200, 0.0800, 0.2800, 0.0400, 0.0800, 0.2000, 0.0800]), tensor([0.0800, 0.0400, 0.1200, 0.0800, 0.2800, 0.0400, 0.0800, 0.2000, 0.0800]), 0.10990483420589368, tensor(4), {'network_policy': tensor([0.1083, 0.0938, 0.1251, 0.1416, 0.1603, 0.0779, 0.1040, 0.0829, 0.1061]), 'network_value': 0.183270663022995, 'search_policy': tensor([0.0800, 0.0400, 0.1200, 0.0800, 0.2800, 0.0400, 0.0800, 0.2000, 0.0800]), 'search_value': 0.10990483420589368, 'root_children_values': tensor([-0.0958,  0.0554, -0.1367, -0.0677, -0.2398, -0.0679, -0.0811, -0.0444,\n",
                        "        -0.1078])})\n",
                        "action: 4\n",
                        "Player 1 tictactoe_expert action: 5\n",
                        "Player 0 prediction: (tensor([0.3600, 0.0400, 0.1200, 0.1200, 0.0000, 0.0000, 0.1600, 0.0400, 0.1600]), tensor([0.3600, 0.0400, 0.1200, 0.1200, 0.0000, 0.0000, 0.1600, 0.0400, 0.1600]), 0.1843551978708001, tensor(0), {'network_policy': tensor([0.1208, 0.0851, 0.1716, 0.1580, 0.0000, 0.0000, 0.2024, 0.0678, 0.1902]), 'network_value': 0.2978688180446625, 'search_policy': tensor([0.3600, 0.0400, 0.1200, 0.1200, 0.0000, 0.0000, 0.1600, 0.0400, 0.1600]), 'search_value': 0.1843551978708001, 'root_children_values': tensor([-0.1680,  0.0031, -0.1042, -0.2172,  0.0000,  0.0000, -0.1486, -0.0292,\n",
                        "        -0.1989])})\n",
                        "action: 0\n",
                        "Player 1 tictactoe_expert action: 8\n",
                        "Player 0 prediction: (tensor([0.0000, 0.0800, 0.2000, 0.2000, 0.0000, 0.0000, 0.3200, 0.2000, 0.0000]), tensor([0.0000, 0.0800, 0.2000, 0.2000, 0.0000, 0.0000, 0.3200, 0.2000, 0.0000]), 0.23506632411139108, tensor(6), {'network_policy': tensor([0.0000, 0.1348, 0.2450, 0.2450, 0.0000, 0.0000, 0.2772, 0.0875, 0.0000]), 'network_value': 0.26095229387283325, 'search_policy': tensor([0.0000, 0.0800, 0.2000, 0.2000, 0.0000, 0.0000, 0.3200, 0.2000, 0.0000]), 'search_value': 0.23506632411139108, 'root_children_values': tensor([ 0.0000, -0.0013, -0.0602, -0.1689,  0.0000,  0.0000, -0.0835, -0.0551,\n",
                        "         0.0000])})\n",
                        "action: 6\n",
                        "Player 1 tictactoe_expert action: 2\n",
                        "Stopped recording episode 4. Recorded 7 frames.\n",
                        "Player 0 (player_1) win percentage vs tictactoe_expert: 0.0 and average score: -0.5\n",
                        "Testing Player 1 vs Agent tictactoe_expert\n",
                        "Player 0 tictactoe_expert action: 8\n",
                        "Player 1 prediction: (tensor([0.1600, 0.0800, 0.0800, 0.1200, 0.2000, 0.0400, 0.1600, 0.1600, 0.0000]), tensor([0.1600, 0.0800, 0.0800, 0.1200, 0.2000, 0.0400, 0.1600, 0.1600, 0.0000]), -0.12743252538244887, tensor(4), {'network_policy': tensor([0.1129, 0.1270, 0.1270, 0.1245, 0.1457, 0.0966, 0.1545, 0.1086, 0.0000]), 'network_value': -0.05433953180909157, 'search_policy': tensor([0.1600, 0.0800, 0.0800, 0.1200, 0.2000, 0.0400, 0.1600, 0.1600, 0.0000]), 'search_value': -0.12743252538244887, 'root_children_values': tensor([0.1002, 0.1892, 0.2030, 0.1629, 0.1070, 0.2678, 0.1092, 0.1082, 0.0000])})\n",
                        "action: 4\n",
                        "Player 0 tictactoe_expert action: 6\n",
                        "Player 1 prediction: (tensor([0.1200, 0.1200, 0.3600, 0.1200, 0.0000, 0.0800, 0.0000, 0.2000, 0.0000]), tensor([0.1200, 0.1200, 0.3600, 0.1200, 0.0000, 0.0800, 0.0000, 0.2000, 0.0000]), -0.18350118862013998, tensor(2), {'network_policy': tensor([0.1608, 0.1486, 0.2201, 0.1844, 0.0000, 0.1295, 0.0000, 0.1486, 0.0000]), 'network_value': -0.05972738564014435, 'search_policy': tensor([0.1200, 0.1200, 0.3600, 0.1200, 0.0000, 0.0800, 0.0000, 0.2000, 0.0000]), 'search_value': -0.18350118862013998, 'root_children_values': tensor([0.2129, 0.2422, 0.2047, 0.2619, 0.0000, 0.2605, 0.0000, 0.1786, 0.0000])})\n",
                        "action: 2\n",
                        "Player 0 tictactoe_expert action: 7\n",
                        "Player 1 (player_2) win percentage vs tictactoe_expert: 0.0 and average score: -1.0\n",
                        "Results vs tictactoe_expert: {'player_1_score': -0.5, 'player_1_win%': 0.0, 'player_2_score': -1.0, 'player_2_win%': 0.0, 'score': -0.75}\n",
                        "Started recording episode 9 to checkpoints/bench_muzero_(ray)/step_574/videos/bench_muzero_(ray)/episode_000009.mp4\n",
                        "Stopped recording episode 9. Recorded 8 frames.\n",
                        "average score: 0.2\n",
                        "Test score {'score': 0.2, 'max_score': 1, 'min_score': -1}\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting test_score\n",
                        "  subkey score\n",
                        "  subkey max_score\n",
                        "  subkey min_score\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting test_score_vs_random\n",
                        "  subkey score\n",
                        "  subkey player_1_score\n",
                        "  subkey player_2_score\n",
                        "  subkey player_1_win%\n",
                        "  subkey player_2_win%\n",
                        "plotting test_score_vs_tictactoe_expert\n",
                        "  subkey score\n",
                        "  subkey player_1_score\n",
                        "  subkey player_2_score\n",
                        "  subkey player_1_win%\n",
                        "  subkey player_2_win%\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/bench_muzero_(ray)/graphs/bench_muzero_(ray)_latent_root_umap.png\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:41,570 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3631 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Finished. Time: 61.58s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:21:51,648 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.362 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:22:01,723 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.362 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:22:11,809 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3618 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:22:21,892 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3618 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:22:31,971 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.362 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:22:42,047 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.362 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:22:52,124 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3614 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:23:02,202 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3615 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:23:12,270 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3597 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:23:22,343 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3586 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:23:32,419 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3587 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:23:42,506 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3581 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:23:52,585 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3563 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:24:02,660 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3565 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:24:12,738 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3566 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:24:22,812 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3652 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:24:32,891 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3621 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:24:42,969 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3618 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:24:53,044 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3599 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:25:03,127 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3599 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "\u001b[33m(raylet)\u001b[0m [2026-01-28 20:25:13,198 E 6490 57722] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2026-01-28_20-20-39_848521_6419 is over 95% full, available space: 20.3597 GB; capacity: 460.432 GB. Object creation will fail if spilling is required.\n",
                        "*** SIGTERM received at time=1769649916 ***\n",
                        "PC: @        0x1941afd04  (unknown)  kevent\n",
                        "    @        0x138f01f4c  (unknown)  absl::lts_20230802::AbslFailureSignalHandler()\n",
                        "    @        0x1942256a4  (unknown)  _sigtramp\n",
                        "    @        0x104ed95b4  (unknown)  select_kqueue_control_impl\n",
                        "    @        0x1060beb28  (unknown)  _PyEval_EvalFrameDefault\n",
                        "    @        0x1060b14c0  (unknown)  PyEval_EvalCode\n",
                        "    @        0x1060acf1c  (unknown)  builtin_exec\n",
                        "    @        0x105fe60a4  (unknown)  cfunction_vectorcall_FASTCALL_KEYWORDS\n",
                        "    @        0x1060bce9c  (unknown)  _PyEval_EvalFrameDefault\n",
                        "    @        0x10615d2f0  (unknown)  pymain_run_module\n",
                        "    @        0x10615cc30  (unknown)  Py_RunMain\n",
                        "    @        0x10615ce98  (unknown)  pymain_main\n",
                        "    @        0x10615d018  (unknown)  Py_BytesMain\n",
                        "    @        0x193e4ab98  (unknown)  start\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474: *** SIGTERM received at time=1769649916 ***\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474: PC: @        0x1941afd04  (unknown)  kevent\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x138f0206c  (unknown)  absl::lts_20230802::AbslFailureSignalHandler()\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x1942256a4  (unknown)  _sigtramp\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x104ed95b4  (unknown)  select_kqueue_control_impl\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x1060beb28  (unknown)  _PyEval_EvalFrameDefault\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x1060b14c0  (unknown)  PyEval_EvalCode\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x1060acf1c  (unknown)  builtin_exec\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x105fe60a4  (unknown)  cfunction_vectorcall_FASTCALL_KEYWORDS\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x1060bce9c  (unknown)  _PyEval_EvalFrameDefault\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x10615d2f0  (unknown)  pymain_run_module\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x10615cc30  (unknown)  Py_RunMain\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x10615ce98  (unknown)  pymain_main\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x10615d018  (unknown)  Py_BytesMain\n",
                        "[2026-01-28 20:25:16,661 E 6419 56378] logging.cc:474:     @        0x193e4ab98  (unknown)  start\n"
                    ]
                }
            ],
            "source": [
                "# Run Ray Benchmark\n",
                "# Note: Ray initialization happens inside the agent if not already started.\n",
                "ray_time = bench.run_benchmark(MuZeroRay, \"MuZero (Ray)\", num_steps=1000)\n",
                "results.append({\"Agent\": \"MuZero (Ray)\", \"Time (s)\": ray_time})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== RESULTS ===\")\n",
                "df = tabulate(results, headers=\"keys\", tablefmt=\"pretty\", floatfmt=\".2f\")\n",
                "print(df)\n",
                "\n",
                "if ray_time and mp_time:\n",
                "    speedup = ray_time / mp_time\n",
                "    print(f\"\\nTime Ratio (Ray / TorchMP): {speedup:.2f}x\")\n",
                "    if speedup > 1.0:\n",
                "        print(f\"TorchMP is {speedup:.2f}x FASTER than Ray\")\n",
                "    else:\n",
                "        print(f\"Ray is {1/speedup:.2f}x FASTER than TorchMP\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
