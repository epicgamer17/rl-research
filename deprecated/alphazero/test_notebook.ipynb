{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Using default save_intermediate_weights     : False\n",
                                    "Using         training_steps                : 10\n",
                                    "Using         adam_epsilon                  : 1e-08\n",
                                    "Using default momentum                      : 0.9\n",
                                    "Using         learning_rate                 : 0.002\n",
                                    "Using         clipnorm                      : 0.5\n",
                                    "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
                                    "Using         weight_decay                  : 0.0001\n",
                                    "Using         loss_function                 : None\n",
                                    "Using default activation                    : relu\n",
                                    "Using         kernel_initializer            : None\n",
                                    "Using         minibatch_size                : 16\n",
                                    "Using default replay_buffer_size            : 5000\n",
                                    "Using default min_replay_buffer_size        : 16\n",
                                    "Using default num_minibatches               : 1\n",
                                    "Using default training_iterations           : 1\n",
                                    "Using default print_interval                : 100\n",
                                    "Using         residual_layers               : [(128, 3, 1), (128, 3, 1), (128, 3, 1)]\n",
                                    "Using default conv_layers                   : []\n",
                                    "Using default dense_layer_widths            : []\n",
                                    "Using default conv_layers                   : [(32, 3, 1)]\n",
                                    "Using default dense_layer_widths            : [256]\n",
                                    "Using default conv_layers                   : [(32, 3, 1)]\n",
                                    "Using default dense_layer_widths            : [256]\n",
                                    "Using default noisy_sigma                   : 0.0\n",
                                    "Using         games_per_generation          : 16\n",
                                    "Using default value_loss_factor             : 1.0\n",
                                    "Using         weight_decay                  : 0.0001\n",
                                    "Using         root_dirichlet_alpha          : 1.0\n",
                                    "Using         root_exploration_fraction     : 0.25\n",
                                    "Using         num_simulations               : 800\n",
                                    "Using         num_sampling_moves            : 4\n",
                                    "Using         exploration_temperature       : 1.0\n",
                                    "Using         exploitation_temperature      : 0.1\n",
                                    "Using default clip_low_prob                 : 0.0\n",
                                    "Using         pb_c_base                     : 19652\n",
                                    "Using         pb_c_init                     : 1.25\n",
                                    "making test env\n",
                                    "Test env with record video\n",
                                    "gym env\n",
                                    "Test env: <RecordVideo<TimeLimit<OrderEnforcing<PassiveEnvChecker<TicTacToeEnv<custom_gym_envs/TicTacToe-v0>>>>>>\n",
                                    "<class 'gymnasium.spaces.box.Box'>\n",
                                    "Observation dimensions: (3, 3, 3)\n",
                                    "Observation dtype: uint8\n",
                                    "num_actions:  9\n",
                                    "3\n",
                                    "128\n",
                                    "128\n",
                                    "Max size: 5000\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/alphazero folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
                                    "  logger.warn(\n"
                              ]
                        }
                  ],
                  "source": [
                        "from agent_configs.alphazero_config import AlphaZeroConfig\n",
                        "from game_configs.tictactoe_config import TicTacToeConfig\n",
                        "from alphazero_agent import AlphaZeroAgent\n",
                        "import gymnasium as gym\n",
                        "import numpy as np\n",
                        "import custom_gym_envs\n",
                        "from torch.optim import Adam, SGD\n",
                        "\n",
                        "env = gym.make(\"custom_gym_envs/TicTacToe-v0\", render_mode=\"rgb_array\")\n",
                        "\n",
                        "config = {\n",
                        "    \"optimizer_function\": Adam,\n",
                        "    \"learning_rate\": 0.002,\n",
                        "    \"adam_epsilon\": 1e-8,\n",
                        "    # \"momentum\": 0.9,\n",
                        "    \"clipnorm\": 0.5,\n",
                        "    # NORMALIZATION?\n",
                        "    # REWARD CLIPPING\n",
                        "    \"training_steps\": 10,\n",
                        "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
                        "    \"critic_conv_layers\": [(32, 3, 1)],\n",
                        "    \"critic_widths\": [],\n",
                        "    \"actor_conv_layers\": [(32, 3, 1)],\n",
                        "    \"actor_widths\": [],\n",
                        "    \"memory_size\": 1600,\n",
                        "    \"minibatch_size\": 16,\n",
                        "    \"root_dirichlet_alpha\": 1.0,\n",
                        "    \"root_exploration_fraction\": 0.25,\n",
                        "    \"pb_c_init\": 1.25,\n",
                        "    \"pb_c_base\": 19652,\n",
                        "    \"num_simulations\": 800,\n",
                        "    \"weight_decay\": 1e-4,\n",
                        "    \"num_sampling_moves\": 4,\n",
                        "    \"loss_function\": None,\n",
                        "    \"games_per_generation\": 16,\n",
                        "    \"exploration_temperature\": 1.0,\n",
                        "    \"exploitation_temperature\": 0.1,\n",
                        "}\n",
                        "\n",
                        "config = AlphaZeroConfig(config, TicTacToeConfig())\n",
                        "\n",
                        "agent = AlphaZeroAgent(env, config, name=\"alphazero\", device=\"cpu\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Resuming training at step 1 / 10\n",
                                    "Resuming training at step 1 / 10\n",
                                    "Training step: 1/10\n",
                                    "Training Game  1\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be uint8, actual type: float64\u001b[0m\n",
                                    "  logger.warn(\n",
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
                                    "  logger.warn(f\"{pre} is not within the observation space.\")\n",
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be uint8, actual type: float64\u001b[0m\n",
                                    "  logger.warn(\n",
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
                                    "  logger.warn(f\"{pre} is not within the observation space.\")\n",
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:246: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
                                    "  logger.warn(\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Target Policy [0.045   0.625   0.0475  0.04875 0.05375 0.03875 0.04625 0.0425  0.0525 ]\n",
                                    "Temperature Policy  [0.045   0.625   0.0475  0.04875 0.05375 0.03875 0.04625 0.0425  0.0525 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.16249999 0.         0.12875    0.14       0.06       0.06\n",
                                    " 0.11125    0.18875    0.14875001]\n",
                                    "Temperature Policy  [0.1625  0.12875 0.14    0.06    0.06    0.11125 0.18875 0.14875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0175     0.         0.01625    0.0225     0.89375001 0.\n",
                                    " 0.0175     0.015      0.0175    ]\n",
                                    "Temperature Policy  [0.0175  0.01625 0.0225  0.89375 0.0175  0.015   0.0175 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.105   0.      0.09375 0.1     0.      0.      0.09375 0.49625 0.11125]\n",
                                    "Temperature Policy  [0.105   0.09375 0.1     0.09375 0.49625 0.11125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.01625 0.      0.02375 0.015   0.      0.      0.0125  0.9325  0.     ]\n",
                                    "Temperature Policy  [2.5825294e-18 1.1485443e-16 1.1599098e-18 1.8733190e-19 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.05       0.60750002 0.05625    0.0525     0.05875    0.03875\n",
                                    " 0.04625    0.04125    0.04875   ]\n",
                                    "Temperature Policy  [0.05    0.6075  0.05625 0.0525  0.05875 0.03875 0.04625 0.04125 0.04875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.16249999 0.         0.06125    0.14       0.06125    0.07875\n",
                                    " 0.11125    0.18875    0.19625001]\n",
                                    "Temperature Policy  [0.1625  0.06125 0.14    0.06125 0.07875 0.11125 0.18875 0.19625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.35749999 0.         0.28125    0.1125     0.         0.10375\n",
                                    " 0.04625    0.04375    0.055     ]\n",
                                    "Temperature Policy  [0.3575  0.28125 0.1125  0.10375 0.04625 0.04375 0.055  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.03375    0.         0.06375    0.0925     0.         0.73124999\n",
                                    " 0.04125    0.0375     0.        ]\n",
                                    "Temperature Policy  [0.03375 0.06375 0.0925  0.73125 0.04125 0.0375 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02125    0.         0.0175     0.92374998 0.         0.\n",
                                    " 0.01875    0.01875    0.        ]\n",
                                    "Temperature Policy  [4.14996877e-17 5.95437237e-18 1.00000000e+00 1.18704785e-17\n",
                                    " 1.18704785e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.04625    0.         0.0275     0.         0.         0.\n",
                                    " 0.89999998 0.02625    0.        ]\n",
                                    "Temperature Policy  [1.2843763e-13 7.0941741e-16 1.0000000e+00 4.4552080e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0275     0.         0.94875002 0.         0.         0.\n",
                                    " 0.         0.02375    0.        ]\n",
                                    "Temperature Policy  [4.1860948e-16 1.0000000e+00 9.6631398e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.97500002 0.         0.         0.         0.         0.\n",
                                    " 0.         0.025      0.        ]\n",
                                    "Temperature Policy  [1.00000e+00 1.22844e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.0475     0.61250001 0.05625    0.04875    0.055      0.04125\n",
                                    " 0.0475     0.0425     0.04875   ]\n",
                                    "Temperature Policy  [0.0475  0.6125  0.05625 0.04875 0.055   0.04125 0.0475  0.0425  0.04875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.105      0.         0.23875    0.06375    0.05375    0.05\n",
                                    " 0.1075     0.185      0.19625001]\n",
                                    "Temperature Policy  [0.105   0.23875 0.06375 0.05375 0.05    0.1075  0.185   0.19625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03125    0.         0.         0.03625    0.75875002 0.03125\n",
                                    " 0.03       0.0775     0.035     ]\n",
                                    "Temperature Policy  [0.03125 0.03625 0.75875 0.03125 0.03    0.0775  0.035  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.02       0.         0.         0.0275     0.         0.02375\n",
                                    " 0.01875    0.88875002 0.02125   ]\n",
                                    "Temperature Policy  [0.02    0.0275  0.02375 0.01875 0.88875 0.02125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0325  0.      0.      0.0975  0.      0.40875 0.05    0.      0.41125]\n",
                                    "Temperature Policy  [4.8953454e-12 2.8906524e-07 4.8476061e-01 3.6362166e-10 5.1523912e-01]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0225     0.         0.         0.93124998 0.         0.\n",
                                    " 0.02375    0.         0.0225    ]\n",
                                    "Temperature Policy  [6.7789489e-17 1.0000000e+00 1.1640545e-16 6.7789489e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.50125003 0.         0.         0.         0.         0.\n",
                                    " 0.11625    0.         0.38249999]\n",
                                    "Temperature Policy  [9.3724799e-01 4.2192988e-07 6.2751636e-02]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.0275     0.         0.97250003]\n",
                                    "Temperature Policy  [3.2691197e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.0475     0.60874999 0.05       0.04875    0.0575     0.05\n",
                                    " 0.05       0.0425     0.045     ]\n",
                                    "Temperature Policy  [0.0475  0.60875 0.05    0.04875 0.0575  0.05    0.05    0.0425  0.045  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.1575  0.      0.23875 0.14    0.05375 0.14375 0.09875 0.06    0.1075 ]\n",
                                    "Temperature Policy  [0.1575  0.23875 0.14    0.05375 0.14375 0.09875 0.06    0.1075 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03125    0.         0.         0.035      0.75625002 0.03125\n",
                                    " 0.03375    0.07625    0.03625   ]\n",
                                    "Temperature Policy  [0.03125 0.035   0.75625 0.03125 0.03375 0.07625 0.03625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.02625    0.         0.         0.02625    0.         0.01875\n",
                                    " 0.01875    0.88875002 0.02125   ]\n",
                                    "Temperature Policy  [0.02625 0.02625 0.01875 0.01875 0.88875 0.02125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.03625 0.      0.      0.095   0.      0.40875 0.05    0.      0.41   ]\n",
                                    "Temperature Policy  [1.4818058e-11 2.2643692e-07 4.9236688e-01 3.6932715e-10 5.0763291e-01]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02375 0.      0.      0.9325  0.      0.      0.02    0.      0.02375]\n",
                                    "Temperature Policy  [1.1485443e-16 1.0000000e+00 2.0597360e-17 1.1485443e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.49250001 0.         0.         0.         0.         0.\n",
                                    " 0.1225     0.         0.38499999]\n",
                                    "Temperature Policy  [9.214708e-01 8.351800e-07 7.852835e-02]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.      0.03125 0.      0.96875]\n",
                                    "Temperature Policy  [1.2200653e-15 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.04625 0.65125 0.04375 0.04625 0.05    0.03625 0.04125 0.0375  0.0475 ]\n",
                                    "Temperature Policy  [0.04625 0.65125 0.04375 0.04625 0.05    0.03625 0.04125 0.0375  0.0475 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.15875    0.         0.14875001 0.14       0.0525     0.06\n",
                                    " 0.1075     0.1875     0.145     ]\n",
                                    "Temperature Policy  [0.15875 0.14875 0.14    0.0525  0.06    0.1075  0.1875  0.145  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.02125 0.      0.84125 0.02    0.06625 0.0175  0.0175  0.01625 0.     ]\n",
                                    "Temperature Policy  [0.02125 0.84125 0.02    0.06625 0.0175  0.0175  0.01625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.51375002 0.         0.         0.09       0.1125     0.08875\n",
                                    " 0.09       0.105      0.        ]\n",
                                    "Temperature Policy  [0.51375 0.09    0.1125  0.08875 0.09    0.105  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.89749998 0.         0.         0.025      0.02625    0.\n",
                                    " 0.02625    0.025      0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.8122603e-16 4.5808754e-16 4.5808754e-16 2.8122603e-16]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.045      0.61874998 0.0525     0.05       0.06       0.04375\n",
                                    " 0.045      0.04       0.045     ]\n",
                                    "Temperature Policy  [0.045   0.61875 0.0525  0.05    0.06    0.04375 0.045   0.04    0.045  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.15875    0.         0.06125    0.14       0.27125001 0.05\n",
                                    " 0.06875    0.18875    0.06125   ]\n",
                                    "Temperature Policy  [0.15875 0.06125 0.14    0.27125 0.05    0.06875 0.18875 0.06125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.21625    0.         0.42124999 0.11125    0.         0.10375\n",
                                    " 0.04625    0.045      0.05625   ]\n",
                                    "Temperature Policy  [0.21625 0.42125 0.11125 0.10375 0.04625 0.045   0.05625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.90750003 0.01875    0.         0.0175\n",
                                    " 0.01875    0.02       0.0175    ]\n",
                                    "Temperature Policy  [0.9075  0.01875 0.0175  0.01875 0.02    0.0175 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.02       0.         0.025\n",
                                    " 0.91250002 0.02625    0.01625   ]\n",
                                    "Temperature Policy  [2.5584162e-17 2.3827108e-16 1.0000000e+00 3.8811849e-16 3.2077826e-18]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.      0.      0.9325  0.      0.02625 0.      0.02125 0.02   ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.1246738e-16 3.7765996e-17 2.0597360e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.94999999\n",
                                    " 0.         0.03       0.02      ]\n",
                                    "Temperature Policy  [1.0000000e+00 9.8622608e-16 1.7102669e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.         0.44125    0.55874997]\n",
                                    "Temperature Policy  [0.08620285 0.91379714]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.04625    0.60750002 0.05       0.0475     0.06125    0.04375\n",
                                    " 0.04875    0.05       0.045     ]\n",
                                    "Temperature Policy  [0.04625 0.6075  0.05    0.0475  0.06125 0.04375 0.04875 0.05    0.045  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1675     0.13375001 0.1275     0.10125    0.         0.11875\n",
                                    " 0.1225     0.1175     0.11125   ]\n",
                                    "Temperature Policy  [0.1675  0.13375 0.1275  0.10125 0.11875 0.1225  0.1175  0.11125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.41125    0.03125    0.38874999 0.         0.04375\n",
                                    " 0.03       0.0675     0.0275    ]\n",
                                    "Temperature Policy  [0.41125 0.03125 0.38875 0.04375 0.03    0.0675  0.0275 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.025      0.02125    0.         0.         0.89375001\n",
                                    " 0.02       0.02125    0.01875   ]\n",
                                    "Temperature Policy  [0.025   0.02125 0.89375 0.02    0.02125 0.01875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.41125    0.41874999 0.         0.         0.\n",
                                    " 0.03125    0.09125    0.0475    ]\n",
                                    "Temperature Policy  [4.5494059e-01 5.4505932e-01 2.9200869e-12 1.3159043e-07 1.9223464e-10]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.01875 0.      0.      0.      0.025   0.935   0.02125]\n",
                                    "Temperature Policy  [1.0517114e-17 1.8676004e-16 1.0000000e+00 3.6768271e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.42750001 0.         0.         0.\n",
                                    " 0.46125001 0.         0.11125   ]\n",
                                    "Temperature Policy  [3.1867674e-01 6.8132281e-01 4.5393051e-07]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.97250003 0.         0.0275    ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.2691197e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.05125    0.60500002 0.05125    0.05       0.0575     0.04375\n",
                                    " 0.04875    0.0425     0.05      ]\n",
                                    "Temperature Policy  [0.05125 0.605   0.05125 0.05    0.0575  0.04375 0.04875 0.0425  0.05   ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.05375    0.08375    0.06125    0.3125     0.08625    0.\n",
                                    " 0.28749999 0.055      0.06      ]\n",
                                    "Temperature Policy  [0.05375 0.08375 0.06125 0.3125  0.08625 0.2875  0.055   0.06   ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.015      0.         0.01875    0.01875    0.89999998 0.\n",
                                    " 0.0175     0.015      0.015     ]\n",
                                    "Temperature Policy  [0.015   0.01875 0.01875 0.9     0.0175  0.015   0.015  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.10375    0.         0.125      0.48249999 0.         0.\n",
                                    " 0.095      0.09125    0.1025    ]\n",
                                    "Temperature Policy  [0.10375 0.125   0.4825  0.095   0.09125 0.1025 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0225     0.         0.0225     0.         0.         0.\n",
                                    " 0.01625    0.0175     0.92124999]\n",
                                    "Temperature Policy  [7.5517950e-17 7.5517950e-17 2.9158063e-18 6.1179440e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.39250001 0.         0.38       0.         0.         0.\n",
                                    " 0.13500001 0.0925     0.        ]\n",
                                    "Temperature Policy  [5.8020627e-01 4.1978005e-01 1.3443845e-05 3.0662187e-07]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.96375 0.      0.      0.      0.01875 0.0175  0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.7690664e-18 3.8970555e-18]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.05       0.61500001 0.0525     0.05125    0.0575     0.03875\n",
                                    " 0.045      0.04125    0.04875   ]\n",
                                    "Temperature Policy  [0.05    0.615   0.0525  0.05125 0.0575  0.03875 0.045   0.04125 0.04875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.05125    0.09       0.055      0.06875    0.38124999 0.14\n",
                                    " 0.0675     0.14624999 0.        ]\n",
                                    "Temperature Policy  [0.05125 0.09    0.055   0.06875 0.38125 0.14    0.0675  0.14625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01625    0.0175     0.015      0.01625    0.89749998 0.02125\n",
                                    " 0.01625    0.         0.        ]\n",
                                    "Temperature Policy  [0.01625 0.0175  0.015   0.01625 0.8975  0.02125 0.01625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.11375    0.1275     0.39250001 0.11625    0.15125    0.\n",
                                    " 0.09875    0.         0.        ]\n",
                                    "Temperature Policy  [0.11375 0.1275  0.3925  0.11625 0.15125 0.09875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.01375    0.         0.91874999 0.0125     0.04125    0.\n",
                                    " 0.01375    0.         0.        ]\n",
                                    "Temperature Policy  [5.6370704e-19 1.0000000e+00 2.1733346e-19 3.3286338e-14 5.6370704e-19]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.05125    0.61124998 0.04875    0.05375    0.05625    0.03875\n",
                                    " 0.05       0.04125    0.04875   ]\n",
                                    "Temperature Policy  [0.05125 0.61125 0.04875 0.05375 0.05625 0.03875 0.05    0.04125 0.04875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.23375    0.06875    0.08       0.2475     0.16500001 0.06125\n",
                                    " 0.         0.0825     0.06125   ]\n",
                                    "Temperature Policy  [0.23375 0.06875 0.08    0.2475  0.165   0.06125 0.0825  0.06125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.87124997 0.         0.015      0.04875    0.01625    0.015\n",
                                    " 0.         0.01875    0.015     ]\n",
                                    "Temperature Policy  [0.87125 0.015   0.04875 0.01625 0.015   0.01875 0.015  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.1        0.45500001 0.125      0.09875\n",
                                    " 0.         0.12375    0.0975    ]\n",
                                    "Temperature Policy  [0.1     0.455   0.125   0.09875 0.12375 0.0975 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.01375    0.93124998 0.         0.015\n",
                                    " 0.         0.0275     0.0125    ]\n",
                                    "Temperature Policy  [4.9245226e-19 1.0000000e+00 1.1755734e-18 5.0427111e-16 1.8986167e-19]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.04375 0.65625 0.04625 0.04625 0.0525  0.035   0.0425  0.03625 0.04125]\n",
                                    "Temperature Policy  [0.04375 0.65625 0.04625 0.04625 0.0525  0.035   0.0425  0.03625 0.04125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.235   0.09375 0.09875 0.25125 0.10375 0.075   0.      0.08125 0.06125]\n",
                                    "Temperature Policy  [0.235   0.09375 0.09875 0.25125 0.10375 0.075   0.08125 0.06125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02125 0.0225  0.0175  0.      0.025   0.01875 0.      0.0225  0.8725 ]\n",
                                    "Temperature Policy  [0.02125 0.0225  0.0175  0.025   0.01875 0.0225  0.8725 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0875     0.095      0.09       0.         0.14       0.10375\n",
                                    " 0.         0.48374999 0.        ]\n",
                                    "Temperature Policy  [0.0875  0.095   0.09    0.14    0.10375 0.48375]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0175     0.0175     0.02375    0.         0.92624998 0.015\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [5.7955988e-18 5.7955988e-18 1.2284400e-16 1.0000000e+00 1.2405960e-18]\n",
                                    "Action  4\n",
                                    "Target Policy [0.40125 0.1125  0.37    0.      0.      0.11625 0.      0.      0.     ]\n",
                                    "Temperature Policy  [6.9228011e-01 2.0780531e-06 3.0771497e-01 2.8844368e-06]\n",
                                    "Action  2\n",
                                    "Target Policy [0.95249999 0.0225     0.         0.         0.         0.025\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 5.4097268e-17 1.5514945e-16]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.045   0.6225  0.0475  0.0475  0.05375 0.045   0.04875 0.04625 0.04375]\n",
                                    "Temperature Policy  [0.045   0.6225  0.0475  0.0475  0.05375 0.045   0.04875 0.04625 0.04375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0425     0.08375    0.04875    0.05375    0.63125002 0.\n",
                                    " 0.04625    0.0425     0.05125   ]\n",
                                    "Temperature Policy  [0.0425  0.08375 0.04875 0.05375 0.63125 0.04625 0.0425  0.05125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.035      0.03       0.0325     0.0575     0.78750002 0.\n",
                                    " 0.03       0.0275     0.        ]\n",
                                    "Temperature Policy  [0.035  0.03   0.0325 0.0575 0.7875 0.03   0.0275]\n",
                                    "Action  4\n",
                                    "Target Policy [0.0225     0.0225     0.01875    0.89249998 0.         0.\n",
                                    " 0.02       0.02375    0.        ]\n",
                                    "Temperature Policy  [0.0225  0.0225  0.01875 0.8925  0.02    0.02375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.04       0.11125    0.02875    0.         0.         0.\n",
                                    " 0.4075     0.41249999 0.        ]\n",
                                    "Temperature Policy  [3.8994624e-11 1.0799534e-06 1.4347734e-12 4.6954903e-01 5.3044987e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02625    0.0225     0.93124998 0.         0.         0.\n",
                                    " 0.         0.02       0.        ]\n",
                                    "Temperature Policy  [3.1668698e-16 6.7789489e-17 1.0000000e+00 2.0875511e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.10375    0.48124999 0.         0.         0.         0.\n",
                                    " 0.         0.41499999 0.        ]\n",
                                    "Temperature Policy  [1.7668128e-07 8.1473607e-01 1.8526375e-01]\n",
                                    "Action  1\n",
                                    "Target Policy [0.02875 0.      0.      0.      0.      0.      0.      0.97125 0.     ]\n",
                                    "Temperature Policy  [5.164974e-16 1.000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.05       0.60624999 0.05125    0.04875    0.0575     0.0425\n",
                                    " 0.05125    0.045      0.0475    ]\n",
                                    "Temperature Policy  [0.05    0.60625 0.05125 0.04875 0.0575  0.0425  0.05125 0.045   0.0475 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.15875 0.      0.06    0.14    0.14375 0.0575  0.1075  0.1875  0.145  ]\n",
                                    "Temperature Policy  [0.15875 0.06    0.14    0.14375 0.0575  0.1075  0.1875  0.145  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.25125 0.      0.58875 0.0375  0.03375 0.0325  0.0275  0.      0.02875]\n",
                                    "Temperature Policy  [0.25125 0.58875 0.0375  0.03375 0.0325  0.0275  0.02875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.90125 0.      0.      0.02    0.01875 0.0225  0.02125 0.      0.01625]\n",
                                    "Temperature Policy  [0.90125 0.02    0.01875 0.0225  0.02125 0.01625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.03625    0.03375    0.03375\n",
                                    " 0.145      0.         0.75125003]\n",
                                    "Temperature Policy  [6.8428214e-14 3.3488281e-14 3.3488281e-14 7.1752183e-08 9.9999988e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.         0.0225     0.02125    0.92874998\n",
                                    " 0.0275     0.         0.        ]\n",
                                    "Temperature Policy  [6.9636498e-17 3.9318876e-17 1.0000000e+00 5.1801062e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.115      0.45500001 0.\n",
                                    " 0.43000001 0.         0.        ]\n",
                                    "Temperature Policy  [6.783246e-07 6.376364e-01 3.623629e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.0275     0.97250003 0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [3.2691197e-16 1.0000000e+00]\n",
                                    "Action  4\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.04625    0.60750002 0.0525     0.055      0.0575     0.04125\n",
                                    " 0.04625    0.04375    0.05      ]\n",
                                    "Temperature Policy  [0.04625 0.6075  0.0525  0.055   0.0575  0.04125 0.04625 0.04375 0.05   ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.14624999 0.         0.2375     0.14       0.05625    0.14375\n",
                                    " 0.06875    0.0625     0.145     ]\n",
                                    "Temperature Policy  [0.14625 0.2375  0.14    0.05625 0.14375 0.06875 0.0625  0.145  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.70625001 0.         0.0325     0.02625    0.1575     0.025\n",
                                    " 0.         0.02375    0.02875   ]\n",
                                    "Temperature Policy  [0.70625 0.0325  0.02625 0.1575  0.025   0.02375 0.02875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.505   0.09    0.1     0.08625 0.      0.10625 0.1125 ]\n",
                                    "Temperature Policy  [0.505   0.09    0.1     0.08625 0.10625 0.1125 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.0125     0.94375002 0.01625\n",
                                    " 0.         0.0125     0.015     ]\n",
                                    "Temperature Policy  [1.6616153e-19 1.0000000e+00 2.2906780e-18 1.6616153e-19 1.0288284e-18]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.08875    0.         0.14624999\n",
                                    " 0.         0.38       0.38499999]\n",
                                    "Temperature Policy  [2.2567863e-07 3.3324646e-05 4.6735060e-01 5.3261590e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.      0.      0.0175  0.      0.01875 0.      0.      0.96375]\n",
                                    "Temperature Policy  [3.8970555e-18 7.7690664e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.0475     0.62374997 0.04625    0.0525     0.05625    0.03875\n",
                                    " 0.04875    0.04       0.04625   ]\n",
                                    "Temperature Policy  [0.0475  0.62375 0.04625 0.0525  0.05625 0.03875 0.04875 0.04    0.04625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.05       0.24875    0.0475     0.05       0.45875001 0.0525\n",
                                    " 0.0425     0.         0.05      ]\n",
                                    "Temperature Policy  [0.05    0.24875 0.0475  0.05    0.45875 0.0525  0.0425  0.05   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.04125    0.0425     0.04875    0.09875    0.         0.04125\n",
                                    " 0.17749999 0.         0.55000001]\n",
                                    "Temperature Policy  [0.04125 0.0425  0.04875 0.09875 0.04125 0.1775  0.55   ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0225     0.01875    0.01625    0.02125    0.         0.01875\n",
                                    " 0.90249997 0.         0.        ]\n",
                                    "Temperature Policy  [0.0225  0.01875 0.01625 0.02125 0.01875 0.9025 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02375 0.02375 0.90875 0.02125 0.      0.0225  0.      0.      0.     ]\n",
                                    "Temperature Policy  [1.4865925e-16 1.4865925e-16 1.0000000e+00 4.8881570e-17 8.6572703e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0225     0.0225     0.         0.025      0.         0.93000001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [6.8706164e-17 6.8706164e-17 1.9704734e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02375    0.02625    0.         0.94999999 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.536743e-17 2.594520e-16 1.000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0.47874999 0.52125001 0.         0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.2993254 0.7006746]\n",
                                    "Action  1\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.04375    0.64875001 0.04625    0.04375    0.05375    0.0375\n",
                                    " 0.04375    0.04125    0.04125   ]\n",
                                    "Temperature Policy  [0.04375 0.64875 0.04625 0.04375 0.05375 0.0375  0.04375 0.04125 0.04125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.155   0.      0.2375  0.13875 0.055   0.06625 0.06875 0.18875 0.09   ]\n",
                                    "Temperature Policy  [0.155   0.2375  0.13875 0.055   0.06625 0.06875 0.18875 0.09   ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.01625    0.         0.0375     0.         0.87875003 0.0175\n",
                                    " 0.015      0.01625    0.01875   ]\n",
                                    "Temperature Policy  [0.01625 0.0375  0.87875 0.0175  0.015   0.01625 0.01875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.13375001 0.         0.09375    0.         0.         0.09875\n",
                                    " 0.1275     0.44999999 0.09625   ]\n",
                                    "Temperature Policy  [0.13375 0.09375 0.09875 0.1275  0.45    0.09625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0375     0.         0.0125     0.         0.         0.01375\n",
                                    " 0.         0.92250001 0.01375   ]\n",
                                    "Temperature Policy  [1.2321085e-14 2.0865865e-19 5.4120680e-19 1.0000000e+00 5.4120680e-19]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x107dd1cc0>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x3286f44f0>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x32840c730>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x3286f44f0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32840c730>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x328514850>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4310>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x325bf4910>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x107dd0ac0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x328514850>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286563b0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x325bf4160>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x328514850>, 0)]\n",
                                    "Observations [array([[[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]])]\n",
                                    "Policies [array([0.105  , 0.     , 0.09375, 0.1    , 0.     , 0.     , 0.09375,\n",
                                    "       0.49625, 0.11125]), array([0.13375001, 0.        , 0.09375   , 0.        , 0.        ,\n",
                                    "       0.09875   , 0.1275    , 0.44999999, 0.09625   ]), array([0.04625   , 0.        , 0.0275    , 0.        , 0.        ,\n",
                                    "       0.        , 0.89999998, 0.02625   , 0.        ]), array([0.155  , 0.     , 0.2375 , 0.13875, 0.055  , 0.06625, 0.06875,\n",
                                    "       0.18875, 0.09   ]), array([0.97500002, 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.        , 0.        , 0.025     , 0.        ]), array([0.40125, 0.1125 , 0.37   , 0.     , 0.     , 0.11625, 0.     ,\n",
                                    "       0.     , 0.     ]), array([0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0.        , 0.        , 0.        , 0.0275    , 0.97250003,\n",
                                    "       0.        , 0.        , 0.        , 0.        ]), array([0.045     , 0.61874998, 0.0525    , 0.05      , 0.06      ,\n",
                                    "       0.04375   , 0.045     , 0.04      , 0.045     ]), array([0.50125003, 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.        , 0.11625   , 0.        , 0.38249999]), array([0.03625, 0.     , 0.     , 0.095  , 0.     , 0.40875, 0.05   ,\n",
                                    "       0.     , 0.41   ]), array([0.        , 0.        , 0.        , 0.115     , 0.45500001,\n",
                                    "       0.        , 0.43000001, 0.        , 0.        ]), array([0.0175    , 0.0175    , 0.02375   , 0.        , 0.92624998,\n",
                                    "       0.015     , 0.        , 0.        , 0.        ]), array([0.87124997, 0.        , 0.015     , 0.04875   , 0.01625   ,\n",
                                    "       0.015     , 0.        , 0.01875   , 0.015     ]), array([0.        , 0.        , 0.42750001, 0.        , 0.        ,\n",
                                    "       0.        , 0.46125001, 0.        , 0.11125   ]), array([0.04375, 0.65625, 0.04625, 0.04625, 0.0525 , 0.035  , 0.0425 ,\n",
                                    "       0.03625, 0.04125])]\n",
                                    "NP Array Policies [[0.105      0.         0.09375    0.1        0.         0.\n",
                                    "  0.09375    0.49625    0.11125   ]\n",
                                    " [0.13375001 0.         0.09375    0.         0.         0.09875\n",
                                    "  0.1275     0.44999999 0.09625   ]\n",
                                    " [0.04625    0.         0.0275     0.         0.         0.\n",
                                    "  0.89999998 0.02625    0.        ]\n",
                                    " [0.155      0.         0.2375     0.13875    0.055      0.06625\n",
                                    "  0.06875    0.18875    0.09      ]\n",
                                    " [0.97500002 0.         0.         0.         0.         0.\n",
                                    "  0.         0.025      0.        ]\n",
                                    " [0.40125    0.1125     0.37       0.         0.         0.11625\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.         0.         1.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.         0.         0.0275     0.97250003 0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.045      0.61874998 0.0525     0.05       0.06       0.04375\n",
                                    "  0.045      0.04       0.045     ]\n",
                                    " [0.50125003 0.         0.         0.         0.         0.\n",
                                    "  0.11625    0.         0.38249999]\n",
                                    " [0.03625    0.         0.         0.095      0.         0.40875\n",
                                    "  0.05       0.         0.41      ]\n",
                                    " [0.         0.         0.         0.115      0.45500001 0.\n",
                                    "  0.43000001 0.         0.        ]\n",
                                    " [0.0175     0.0175     0.02375    0.         0.92624998 0.015\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.87124997 0.         0.015      0.04875    0.01625    0.015\n",
                                    "  0.         0.01875    0.015     ]\n",
                                    " [0.         0.         0.42750001 0.         0.         0.\n",
                                    "  0.46125001 0.         0.11125   ]\n",
                                    " [0.04375    0.65625    0.04625    0.04625    0.0525     0.035\n",
                                    "  0.0425     0.03625    0.04125   ]]\n",
                                    "Predicted: tensor([[0.1279, 0.1240, 0.0826, 0.1209, 0.1021, 0.1148, 0.1105, 0.1209, 0.0964],\n",
                                    "        [0.1349, 0.1151, 0.0761, 0.1273, 0.1110, 0.1085, 0.1113, 0.1187, 0.0971],\n",
                                    "        [0.1132, 0.1182, 0.0917, 0.1246, 0.1080, 0.1201, 0.1095, 0.1170, 0.0978],\n",
                                    "        [0.1274, 0.1242, 0.0903, 0.1156, 0.1094, 0.1125, 0.1044, 0.1162, 0.1002],\n",
                                    "        [0.1052, 0.1303, 0.0821, 0.1317, 0.1188, 0.1167, 0.1046, 0.1147, 0.0959],\n",
                                    "        [0.1110, 0.1308, 0.0835, 0.1395, 0.1063, 0.1185, 0.1063, 0.1111, 0.0931],\n",
                                    "        [0.1294, 0.1196, 0.0949, 0.1172, 0.1236, 0.1143, 0.1052, 0.1020, 0.0938],\n",
                                    "        [0.1220, 0.1206, 0.0830, 0.1511, 0.1083, 0.1111, 0.1058, 0.1094, 0.0889],\n",
                                    "        [0.1179, 0.1181, 0.1021, 0.1197, 0.1070, 0.1126, 0.1062, 0.1082, 0.1082],\n",
                                    "        [0.1173, 0.1331, 0.0872, 0.1244, 0.1153, 0.1150, 0.1000, 0.1038, 0.1038],\n",
                                    "        [0.1170, 0.1278, 0.1027, 0.1291, 0.1124, 0.1095, 0.1050, 0.1041, 0.0923],\n",
                                    "        [0.1304, 0.1269, 0.0973, 0.1236, 0.1054, 0.1142, 0.1084, 0.0954, 0.0985],\n",
                                    "        [0.1160, 0.1340, 0.0923, 0.1260, 0.1136, 0.1186, 0.0971, 0.1056, 0.0969],\n",
                                    "        [0.1160, 0.1187, 0.0975, 0.1216, 0.1083, 0.1168, 0.1071, 0.1113, 0.1027],\n",
                                    "        [0.1133, 0.1286, 0.0937, 0.1230, 0.1148, 0.1184, 0.1033, 0.1059, 0.0991],\n",
                                    "        [0.1179, 0.1181, 0.1021, 0.1197, 0.1070, 0.1126, 0.1062, 0.1082, 0.1082]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.1279, 0.1240, 0.0826, 0.1209, 0.1021, 0.1148, 0.1105, 0.1209, 0.0964],\n",
                                    "        [0.1349, 0.1151, 0.0761, 0.1273, 0.1110, 0.1085, 0.1113, 0.1187, 0.0971],\n",
                                    "        [0.1132, 0.1182, 0.0917, 0.1246, 0.1080, 0.1201, 0.1095, 0.1170, 0.0978],\n",
                                    "        [0.1274, 0.1242, 0.0903, 0.1156, 0.1094, 0.1125, 0.1044, 0.1162, 0.1002],\n",
                                    "        [0.1052, 0.1303, 0.0821, 0.1317, 0.1188, 0.1167, 0.1046, 0.1147, 0.0959],\n",
                                    "        [0.1110, 0.1308, 0.0835, 0.1395, 0.1063, 0.1185, 0.1063, 0.1111, 0.0931],\n",
                                    "        [0.1294, 0.1196, 0.0949, 0.1172, 0.1236, 0.1143, 0.1052, 0.1020, 0.0938],\n",
                                    "        [0.1220, 0.1206, 0.0830, 0.1511, 0.1083, 0.1111, 0.1058, 0.1094, 0.0889],\n",
                                    "        [0.1179, 0.1181, 0.1021, 0.1197, 0.1070, 0.1126, 0.1062, 0.1082, 0.1082],\n",
                                    "        [0.1173, 0.1331, 0.0872, 0.1244, 0.1153, 0.1150, 0.1000, 0.1038, 0.1038],\n",
                                    "        [0.1170, 0.1278, 0.1027, 0.1291, 0.1124, 0.1095, 0.1050, 0.1041, 0.0923],\n",
                                    "        [0.1304, 0.1269, 0.0973, 0.1236, 0.1054, 0.1142, 0.1084, 0.0954, 0.0985],\n",
                                    "        [0.1160, 0.1340, 0.0923, 0.1260, 0.1136, 0.1186, 0.0971, 0.1056, 0.0969],\n",
                                    "        [0.1160, 0.1187, 0.0975, 0.1216, 0.1083, 0.1168, 0.1071, 0.1113, 0.1027],\n",
                                    "        [0.1133, 0.1286, 0.0937, 0.1230, 0.1148, 0.1184, 0.1033, 0.1059, 0.0991],\n",
                                    "        [0.1179, 0.1181, 0.1021, 0.1197, 0.1070, 0.1126, 0.1062, 0.1082, 0.1082]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.1279, 0.1240, 0.0826, 0.1209, 0.1021, 0.1148, 0.1105, 0.1209, 0.0964],\n",
                                    "        [0.1349, 0.1151, 0.0761, 0.1273, 0.1110, 0.1085, 0.1113, 0.1187, 0.0971],\n",
                                    "        [0.1132, 0.1182, 0.0917, 0.1246, 0.1080, 0.1201, 0.1095, 0.1170, 0.0978],\n",
                                    "        [0.1274, 0.1242, 0.0903, 0.1156, 0.1094, 0.1125, 0.1044, 0.1162, 0.1002],\n",
                                    "        [0.1052, 0.1303, 0.0821, 0.1317, 0.1188, 0.1167, 0.1046, 0.1147, 0.0959],\n",
                                    "        [0.1110, 0.1308, 0.0835, 0.1395, 0.1063, 0.1185, 0.1063, 0.1111, 0.0931],\n",
                                    "        [0.1294, 0.1196, 0.0949, 0.1172, 0.1236, 0.1143, 0.1052, 0.1020, 0.0938],\n",
                                    "        [0.1220, 0.1206, 0.0830, 0.1511, 0.1083, 0.1111, 0.1058, 0.1094, 0.0889],\n",
                                    "        [0.1179, 0.1181, 0.1021, 0.1197, 0.1070, 0.1126, 0.1062, 0.1082, 0.1082],\n",
                                    "        [0.1173, 0.1331, 0.0872, 0.1244, 0.1153, 0.1150, 0.1000, 0.1038, 0.1038],\n",
                                    "        [0.1170, 0.1278, 0.1027, 0.1291, 0.1124, 0.1095, 0.1050, 0.1041, 0.0923],\n",
                                    "        [0.1304, 0.1269, 0.0973, 0.1236, 0.1054, 0.1142, 0.1084, 0.0954, 0.0985],\n",
                                    "        [0.1160, 0.1340, 0.0923, 0.1260, 0.1136, 0.1186, 0.0971, 0.1056, 0.0969],\n",
                                    "        [0.1160, 0.1187, 0.0975, 0.1216, 0.1083, 0.1168, 0.1071, 0.1113, 0.1027],\n",
                                    "        [0.1133, 0.1286, 0.0937, 0.1230, 0.1148, 0.1184, 0.1033, 0.1059, 0.0991],\n",
                                    "        [0.1179, 0.1181, 0.1021, 0.1197, 0.1070, 0.1126, 0.1062, 0.1082, 0.1082]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.0566, -2.0871, -2.4942, -2.1126, -2.2820, -2.1649, -2.2029, -2.1132,\n",
                                    "         -2.3394],\n",
                                    "        [-2.0033, -2.1616, -2.5761, -2.0611, -2.1981, -2.2207, -2.1959, -2.1313,\n",
                                    "         -2.3320],\n",
                                    "        [-2.1788, -2.1355, -2.3892, -2.0830, -2.2255, -2.1196, -2.2117, -2.1458,\n",
                                    "         -2.3249],\n",
                                    "        [-2.0607, -2.0862, -2.4045, -2.1580, -2.2131, -2.1847, -2.2600, -2.1522,\n",
                                    "         -2.3011],\n",
                                    "        [-2.2520, -2.0382, -2.5003, -2.0275, -2.1304, -2.1479, -2.2572, -2.1654,\n",
                                    "         -2.3440],\n",
                                    "        [-2.1982, -2.0343, -2.4833, -1.9697, -2.2412, -2.1332, -2.2416, -2.1975,\n",
                                    "         -2.3740],\n",
                                    "        [-2.0452, -2.1237, -2.3545, -2.1435, -2.0904, -2.1691, -2.2522, -2.2824,\n",
                                    "         -2.3671],\n",
                                    "        [-2.1041, -2.1157, -2.4894, -1.8897, -2.2233, -2.1970, -2.2462, -2.2131,\n",
                                    "         -2.4206],\n",
                                    "        [-2.1376, -2.1364, -2.2821, -2.1229, -2.2345, -2.1840, -2.2429, -2.2238,\n",
                                    "         -2.2234],\n",
                                    "        [-2.1426, -2.0166, -2.4390, -2.0839, -2.1601, -2.1632, -2.3030, -2.2652,\n",
                                    "         -2.2652],\n",
                                    "        [-2.1454, -2.0574, -2.2760, -2.0470, -2.1855, -2.2117, -2.2535, -2.2623,\n",
                                    "         -2.3826],\n",
                                    "        [-2.0370, -2.0644, -2.3300, -2.0908, -2.2500, -2.1698, -2.2222, -2.3499,\n",
                                    "         -2.3181],\n",
                                    "        [-2.1544, -2.0101, -2.3829, -2.0717, -2.1752, -2.1321, -2.3320, -2.2479,\n",
                                    "         -2.3341],\n",
                                    "        [-2.1544, -2.1310, -2.3278, -2.1072, -2.2232, -2.1469, -2.2343, -2.1952,\n",
                                    "         -2.2758],\n",
                                    "        [-2.1781, -2.0509, -2.3681, -2.0958, -2.1643, -2.1340, -2.2698, -2.2452,\n",
                                    "         -2.3120],\n",
                                    "        [-2.1376, -2.1364, -2.2821, -2.1229, -2.2345, -2.1840, -2.2429, -2.2238,\n",
                                    "         -2.2234]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.47827953 2.2100766 2.688356\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.17749999 0.0725     0.0625     0.19       0.0675     0.055\n",
                                    " 0.07       0.0575     0.2475    ]\n",
                                    "Temperature Policy  [0.1775 0.0725 0.0625 0.19   0.0675 0.055  0.07   0.0575 0.2475]\n",
                                    "Action  8\n",
                                    "Target Policy [0.155   0.145   0.06    0.13    0.145   0.1175  0.14375 0.10375 0.     ]\n",
                                    "Temperature Policy  [0.155   0.145   0.06    0.13    0.145   0.1175  0.14375 0.10375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.035   0.0125  0.04    0.025   0.01375 0.84875 0.025   0.     ]\n",
                                    "Temperature Policy  [0.035   0.0125  0.04    0.025   0.01375 0.84875 0.025  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.09625 0.09375 0.09625 0.09375 0.09125 0.      0.52875 0.     ]\n",
                                    "Temperature Policy  [0.09625 0.09375 0.09625 0.09375 0.09125 0.52875]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.0175     0.90750003 0.01875    0.03875    0.0175\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [7.1107299e-18 1.0000000e+00 1.4175761e-17 2.0148871e-14 7.1107299e-18]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.19499999 0.         0.12       0.42250001 0.26249999\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.3469263e-04 3.3857011e-06 9.9106759e-01 8.4943324e-03]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.02375    0.         0.0175     0.         0.95875001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.7012651e-17 4.1051286e-18 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.27625    0.075      0.0675     0.0725     0.085      0.09\n",
                                    " 0.19374999 0.06       0.08      ]\n",
                                    "Temperature Policy  [0.27625 0.075   0.0675  0.0725  0.085   0.09    0.19375 0.06    0.08   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0625     0.07625    0.18875    0.14       0.0625\n",
                                    " 0.17       0.105      0.19499999]\n",
                                    "Temperature Policy  [0.0625  0.07625 0.18875 0.14    0.0625  0.17    0.105   0.195  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.11375    0.11875    0.24625    0.         0.13375001\n",
                                    " 0.1425     0.1025     0.1425    ]\n",
                                    "Temperature Policy  [0.11375 0.11875 0.24625 0.13375 0.1425  0.1025  0.1425 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.54624999 0.08375    0.08625    0.         0.\n",
                                    " 0.06625    0.14749999 0.07      ]\n",
                                    "Temperature Policy  [0.54625 0.08375 0.08625 0.06625 0.1475  0.07   ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.90625 0.02375 0.025   0.      0.      0.02375 0.      0.02125]\n",
                                    "Temperature Policy  [1.0000000e+00 1.5281147e-16 2.5522304e-16 1.5281147e-16 5.0246884e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.92624998 0.025      0.         0.\n",
                                    " 0.025      0.         0.02375   ]\n",
                                    "Temperature Policy  [1.000000e+00 2.051719e-16 2.051719e-16 1.228440e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.025      0.         0.\n",
                                    " 0.95249999 0.         0.0225    ]\n",
                                    "Temperature Policy  [1.5514945e-16 1.0000000e+00 5.4097268e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.97874999 0.         0.\n",
                                    " 0.         0.         0.02125   ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.3274032e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.27000001 0.0725     0.07       0.07375    0.0725     0.06\n",
                                    " 0.075      0.06       0.24625   ]\n",
                                    "Temperature Policy  [0.27    0.0725  0.07    0.07375 0.0725  0.06    0.075   0.06    0.24625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.12625    0.15625    0.         0.115      0.11125    0.145\n",
                                    " 0.14875001 0.125      0.0725    ]\n",
                                    "Temperature Policy  [0.12625 0.15625 0.115   0.11125 0.145   0.14875 0.125   0.0725 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.73000002 0.0575     0.         0.         0.0525     0.0225\n",
                                    " 0.0975     0.0275     0.0125    ]\n",
                                    "Temperature Policy  [0.73   0.0575 0.0525 0.0225 0.0975 0.0275 0.0125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.40125    0.         0.         0.12125    0.13500001\n",
                                    " 0.115      0.11625    0.11125   ]\n",
                                    "Temperature Policy  [0.40125 0.12125 0.135   0.115   0.11625 0.11125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.92874998 0.015\n",
                                    " 0.0225     0.015      0.01875   ]\n",
                                    "Temperature Policy  [1.00000000e+00 1.20760346e-18 6.96364977e-17 1.20760346e-18\n",
                                    " 1.12466834e-17]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.14375\n",
                                    " 0.39875001 0.13249999 0.32499999]\n",
                                    "Temperature Policy  [3.2825821e-05 8.8540912e-01 1.4531011e-05 1.1454356e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.025\n",
                                    " 0.         0.02375    0.95125002]\n",
                                    "Temperature Policy  [1.572003e-16 9.412163e-17 1.000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.27625 0.0825  0.065   0.19    0.11875 0.055   0.07    0.0675  0.075  ]\n",
                                    "Temperature Policy  [0.27625 0.0825  0.065   0.19    0.11875 0.055   0.07    0.0675  0.075  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.13249999 0.15875    0.115      0.17       0.125      0.08375\n",
                                    " 0.         0.10875    0.10625   ]\n",
                                    "Temperature Policy  [0.1325  0.15875 0.115   0.17    0.125   0.08375 0.10875 0.10625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.89625001 0.025      0.015      0.0175     0.0175     0.015\n",
                                    " 0.         0.         0.01375   ]\n",
                                    "Temperature Policy  [0.89625 0.025   0.015   0.0175  0.0175  0.015   0.01375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.11125    0.10625    0.44749999 0.1125     0.10625\n",
                                    " 0.         0.         0.11625   ]\n",
                                    "Temperature Policy  [0.11125 0.10625 0.4475  0.1125  0.10625 0.11625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.01625    0.01375    0.93875003 0.         0.01875\n",
                                    " 0.         0.         0.0125    ]\n",
                                    "Temperature Policy  [2.4156509e-18 4.5449330e-19 1.0000000e+00 1.0104463e-17 1.7522684e-19]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.27000001 0.07       0.0625     0.0775     0.075      0.0575\n",
                                    " 0.08       0.06       0.2475    ]\n",
                                    "Temperature Policy  [0.27   0.07   0.0625 0.0775 0.075  0.0575 0.08   0.06   0.2475]\n",
                                    "Action  7\n",
                                    "Target Policy [0.13625 0.06    0.055   0.13875 0.17375 0.12375 0.15625 0.      0.15625]\n",
                                    "Temperature Policy  [0.13625 0.06    0.055   0.13875 0.17375 0.12375 0.15625 0.15625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.105   0.12375 0.12625 0.12875 0.      0.15125 0.1275  0.      0.2375 ]\n",
                                    "Temperature Policy  [0.105   0.12375 0.12625 0.12875 0.15125 0.1275  0.2375 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.02375    0.02125    0.0225     0.0225     0.         0.02125\n",
                                    " 0.88875002 0.         0.        ]\n",
                                    "Temperature Policy  [0.02375 0.02125 0.0225  0.0225  0.02125 0.88875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02375 0.02375 0.90625 0.0225  0.      0.02375 0.      0.      0.     ]\n",
                                    "Temperature Policy  [1.5281147e-16 1.5281147e-16 1.0000000e+00 8.8990777e-17 1.5281147e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03125    0.02       0.         0.02375    0.         0.92500001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.9367917e-15 2.2329688e-17 1.2451419e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.025      0.0275     0.         0.94749999 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.6353395e-16 4.2416493e-16 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0.50875002 0.49125001 0.         0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.58662623 0.4133738 ]\n",
                                    "Action  1\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.20625    0.0675     0.065      0.09125    0.28999999 0.055\n",
                                    " 0.075      0.0725     0.0775    ]\n",
                                    "Temperature Policy  [0.20625 0.0675  0.065   0.09125 0.29    0.055   0.075   0.0725  0.0775 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.1825  0.0625  0.18875 0.13625 0.12    0.175   0.0775  0.0575 ]\n",
                                    "Temperature Policy  [0.1825  0.0625  0.18875 0.13625 0.12    0.175   0.0775  0.0575 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.02       0.         0.03125    0.11375    0.02\n",
                                    " 0.78874999 0.01375    0.0125    ]\n",
                                    "Temperature Policy  [0.02    0.03125 0.11375 0.02    0.78875 0.01375 0.0125 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.09125 0.      0.53375 0.09625 0.09125 0.      0.0925  0.095  ]\n",
                                    "Temperature Policy  [0.09125 0.53375 0.09625 0.09125 0.0925  0.095  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02       0.         0.         0.0425     0.0175\n",
                                    " 0.         0.0175     0.90249997]\n",
                                    "Temperature Policy  [2.856458e-17 5.363116e-14 7.514644e-18 7.514644e-18 1.000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.12125    0.         0.         0.42875001 0.1675\n",
                                    " 0.         0.2825     0.        ]\n",
                                    "Temperature Policy  [3.22175629e-06 9.84728634e-01 8.15498788e-05 1.51865715e-02]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.01875    0.         0.         0.         0.02375\n",
                                    " 0.         0.95749998 0.        ]\n",
                                    "Temperature Policy  [8.291344e-18 8.815528e-17 1.000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.27625    0.0675     0.0675     0.065      0.26124999 0.05\n",
                                    " 0.07       0.055      0.0875    ]\n",
                                    "Temperature Policy  [0.27625 0.0675  0.0675  0.065   0.26125 0.05    0.07    0.055   0.0875 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.06       0.0575     0.18875    0.14       0.085\n",
                                    " 0.17       0.105      0.19374999]\n",
                                    "Temperature Policy  [0.06    0.0575  0.18875 0.14    0.085   0.17    0.105   0.19375]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.01375    0.         0.02875    0.0375     0.0175\n",
                                    " 0.82625002 0.02       0.05625   ]\n",
                                    "Temperature Policy  [0.01375 0.02875 0.0375  0.0175  0.82625 0.02    0.05625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.09125 0.      0.53125 0.09375 0.095   0.      0.09375 0.095  ]\n",
                                    "Temperature Policy  [0.09125 0.53125 0.09375 0.095   0.09375 0.095  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.035      0.         0.         0.04375    0.015\n",
                                    " 0.         0.0175     0.88875002]\n",
                                    "Temperature Policy  [8.9718983e-15 8.3557307e-14 1.8754974e-18 8.7616195e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.12       0.         0.         0.44749999 0.145\n",
                                    " 0.         0.28749999 0.        ]\n",
                                    "Temperature Policy  [1.8997827e-06 9.8814774e-01 1.2605832e-05 1.1837772e-02]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0175     0.         0.         0.         0.0225\n",
                                    " 0.         0.95999998 0.        ]\n",
                                    "Temperature Policy  [4.051988e-18 5.001645e-17 1.000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.27500001 0.08       0.0675     0.19       0.0825     0.06\n",
                                    " 0.1175     0.06       0.0675    ]\n",
                                    "Temperature Policy  [0.275  0.08   0.0675 0.19   0.0825 0.06   0.1175 0.06   0.0675]\n",
                                    "Action  1\n",
                                    "Target Policy [0.15375    0.         0.1525     0.13249999 0.1125     0.065\n",
                                    " 0.15000001 0.125      0.10875   ]\n",
                                    "Temperature Policy  [0.15375 0.1525  0.1325  0.1125  0.065   0.15    0.125   0.10875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.15625    0.         0.13625    0.15000001 0.         0.15375\n",
                                    " 0.14749999 0.10875    0.14749999]\n",
                                    "Temperature Policy  [0.15625 0.13625 0.15    0.15375 0.1475  0.10875 0.1475 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.66500002 0.         0.1        0.         0.         0.05125\n",
                                    " 0.085      0.05875    0.04      ]\n",
                                    "Temperature Policy  [0.665   0.1     0.05125 0.085   0.05875 0.04   ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02625 0.      0.      0.      0.      0.02    0.90875 0.02375 0.02125]\n",
                                    "Temperature Policy  [4.0443512e-16 2.6659730e-17 1.0000000e+00 1.4865925e-16 4.8881570e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.9325  0.      0.      0.      0.      0.02625 0.      0.02125 0.02   ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.1246738e-16 3.7765996e-17 2.0597360e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.02625\n",
                                    " 0.         0.0275     0.94625002]\n",
                                    "Temperature Policy  [2.6991944e-16 4.2980156e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.02375\n",
                                    " 0.         0.97624999 0.        ]\n",
                                    "Temperature Policy  [7.261489e-17 1.000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.26499999 0.0725     0.08       0.0675     0.0725     0.06\n",
                                    " 0.075      0.06       0.2475    ]\n",
                                    "Temperature Policy  [0.265  0.0725 0.08   0.0675 0.0725 0.06   0.075  0.06   0.2475]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.06    0.13    0.18875 0.14    0.06    0.17    0.06    0.19125]\n",
                                    "Temperature Policy  [0.06    0.13    0.18875 0.14    0.06    0.17    0.06    0.19125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.68374997 0.035      0.         0.04       0.085\n",
                                    " 0.04       0.04       0.07625   ]\n",
                                    "Temperature Policy  [0.68375 0.035   0.04    0.085   0.04    0.04    0.07625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.46250001 0.         0.115      0.11125\n",
                                    " 0.11       0.10125    0.1       ]\n",
                                    "Temperature Policy  [0.4625  0.115   0.11125 0.11    0.10125 0.1    ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.      0.      0.935   0.      0.      0.0175  0.0175  0.01625 0.01375]\n",
                                    "Temperature Policy  [1.0000000e+00 5.2755094e-18 5.2755094e-18 2.5143025e-18 4.7305415e-19]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.2775 0.0825 0.07   0.1875 0.0825 0.055  0.0875 0.07   0.0875]\n",
                                    "Temperature Policy  [0.2775 0.0825 0.07   0.1875 0.0825 0.055  0.0875 0.07   0.0875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.1825  0.12875 0.18875 0.14125 0.06    0.17    0.07125 0.0575 ]\n",
                                    "Temperature Policy  [0.1825  0.12875 0.18875 0.14125 0.06    0.17    0.07125 0.0575 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.83499998 0.0325     0.         0.0325     0.0425\n",
                                    " 0.03       0.015      0.0125    ]\n",
                                    "Temperature Policy  [0.835  0.0325 0.0325 0.0425 0.03   0.015  0.0125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.45875001 0.         0.1025     0.12125\n",
                                    " 0.1175     0.10125    0.09875   ]\n",
                                    "Temperature Policy  [0.45875 0.1025  0.12125 0.1175  0.10125 0.09875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.      0.      0.      0.      0.90625 0.02    0.03625 0.0175  0.02   ]\n",
                                    "Temperature Policy  [1.00000000e+00 2.74043663e-17 1.04857605e-14 7.20942001e-18\n",
                                    " 2.74043663e-17]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.18875\n",
                                    " 0.1175     0.31874999 0.375     ]\n",
                                    "Temperature Policy  [8.7121729e-04 7.6143979e-06 1.6434589e-01 8.3477527e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.02125\n",
                                    " 0.02375    0.         0.95499998]\n",
                                    "Temperature Policy  [2.9754702e-17 9.0490383e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.27625    0.07       0.065      0.1225     0.08       0.0575\n",
                                    " 0.19374999 0.065      0.07      ]\n",
                                    "Temperature Policy  [0.27625 0.07    0.065   0.1225  0.08    0.0575  0.19375 0.065   0.07   ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.155      0.14125    0.06       0.13       0.14875001 0.1175\n",
                                    " 0.145      0.1025     0.        ]\n",
                                    "Temperature Policy  [0.155   0.14125 0.06    0.13    0.14875 0.1175  0.145   0.1025 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.22624999 0.         0.04       0.0425     0.57375002 0.0375\n",
                                    " 0.045      0.035      0.        ]\n",
                                    "Temperature Policy  [0.22625 0.04    0.0425  0.57375 0.0375  0.045   0.035  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.46625 0.      0.1225  0.1025  0.      0.1025  0.10375 0.1025  0.     ]\n",
                                    "Temperature Policy  [0.46625 0.1225  0.1025  0.1025  0.10375 0.1025 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.89625001 0.         0.02125    0.         0.         0.02\n",
                                    " 0.03375    0.02875    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 5.6143269e-17 3.0620218e-17 5.7338472e-15 1.1536839e-15]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.27000001 0.0725     0.0675     0.19       0.1275     0.0525\n",
                                    " 0.07       0.065      0.085     ]\n",
                                    "Temperature Policy  [0.27   0.0725 0.0675 0.19   0.1275 0.0525 0.07   0.065  0.085 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.11125    0.1425     0.10375    0.         0.13375001 0.105\n",
                                    " 0.17749999 0.12       0.10625   ]\n",
                                    "Temperature Policy  [0.11125 0.1425  0.10375 0.13375 0.105   0.1775  0.12    0.10625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.90499997 0.         0.01375    0.         0.01625    0.01375\n",
                                    " 0.0175     0.01375    0.02      ]\n",
                                    "Temperature Policy  [0.905   0.01375 0.01625 0.01375 0.0175  0.01375 0.02   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.11875    0.         0.11125    0.10875\n",
                                    " 0.42124999 0.12125    0.11875   ]\n",
                                    "Temperature Policy  [0.11875 0.11125 0.10875 0.42125 0.12125 0.11875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.0175     0.         0.92500001 0.01875\n",
                                    " 0.         0.0225     0.01625   ]\n",
                                    "Temperature Policy  [5.8743956e-18 1.0000000e+00 1.1711039e-17 7.2511665e-17 2.7997313e-18]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.11625    0.         0.         0.3175\n",
                                    " 0.         0.17       0.39625001]\n",
                                    "Temperature Policy  [4.2578417e-06 9.8332517e-02 1.9043601e-04 9.0147281e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.01875    0.         0.         0.95875001\n",
                                    " 0.         0.0225     0.        ]\n",
                                    "Temperature Policy  [8.1838752e-18 1.0000000e+00 5.0672396e-17]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.26499999 0.07       0.1175     0.19       0.0825     0.0525\n",
                                    " 0.085      0.0725     0.065     ]\n",
                                    "Temperature Policy  [0.265  0.07   0.1175 0.19   0.0825 0.0525 0.085  0.0725 0.065 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.065   0.13    0.19    0.14125 0.0675  0.185   0.105   0.11625]\n",
                                    "Temperature Policy  [0.065   0.13    0.19    0.14125 0.0675  0.185   0.105   0.11625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.045      0.04       0.71749997 0.045      0.\n",
                                    " 0.06375    0.0375     0.05125   ]\n",
                                    "Temperature Policy  [0.045   0.04    0.7175  0.045   0.06375 0.0375  0.05125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02375    0.0225     0.         0.025      0.\n",
                                    " 0.88499999 0.0225     0.02125   ]\n",
                                    "Temperature Policy  [0.02375 0.0225  0.025   0.885   0.0225  0.02125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.03875    0.72750002 0.         0.04625    0.\n",
                                    " 0.         0.0375     0.15000001]\n",
                                    "Temperature Policy  [1.8381757e-13 9.9999988e-01 1.0784185e-12 1.3242887e-13 1.3886174e-07]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.92750001 0.         0.         0.025      0.\n",
                                    " 0.         0.02875    0.01875   ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.0242348e-16 8.1891590e-16 1.1399178e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.54000002 0.\n",
                                    " 0.         0.115      0.345     ]\n",
                                    "Temperature Policy  [9.8879617e-01 1.8973506e-07 1.1203665e-02]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.         0.0275     0.97250003]\n",
                                    "Temperature Policy  [3.2691197e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.27625 0.07    0.07375 0.19    0.08    0.08    0.0825  0.06    0.0875 ]\n",
                                    "Temperature Policy  [0.27625 0.07    0.07375 0.19    0.08    0.08    0.0825  0.06    0.0875 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.1825  0.105   0.18875 0.13625 0.0575  0.17    0.105   0.055  ]\n",
                                    "Temperature Policy  [0.1825  0.105   0.18875 0.13625 0.0575  0.17    0.105   0.055  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0175     0.0175     0.0225     0.0375\n",
                                    " 0.82499999 0.0675     0.0125    ]\n",
                                    "Temperature Policy  [0.0175 0.0175 0.0225 0.0375 0.825  0.0675 0.0125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.      0.115   0.1175  0.08875 0.06875 0.405   0.      0.205  ]\n",
                                    "Temperature Policy  [0.115   0.1175  0.08875 0.06875 0.405   0.205  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.04625    0.045      0.75749999 0.0475\n",
                                    " 0.         0.         0.10375   ]\n",
                                    "Temperature Policy  [7.1993117e-13 5.4739243e-13 1.0000000e+00 9.3996078e-13 2.3230291e-09]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.02125    0.03375    0.         0.0225\n",
                                    " 0.         0.         0.92250001]\n",
                                    "Temperature Policy  [4.206546e-17 4.296097e-15 7.450089e-17 1.000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.1        0.50125003 0.         0.39875001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.0671726e-08 9.0785342e-01 9.2146493e-02]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.03       0.         0.         0.97000003\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.0074683e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.27625 0.085   0.07    0.0675  0.0875  0.055   0.075   0.0775  0.20625]\n",
                                    "Temperature Policy  [0.27625 0.085   0.07    0.0675  0.0875  0.055   0.075   0.0775  0.20625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.15000001 0.         0.145      0.13124999 0.1125     0.1225\n",
                                    " 0.14875001 0.125      0.065     ]\n",
                                    "Temperature Policy  [0.15    0.145   0.13125 0.1125  0.1225  0.14875 0.125   0.065  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.41374999 0.         0.09       0.09625    0.23625    0.0425\n",
                                    " 0.         0.08125    0.04      ]\n",
                                    "Temperature Policy  [0.41375 0.09    0.09625 0.23625 0.0425  0.08125 0.04   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.0275     0.         0.02       0.02375    0.         0.0225\n",
                                    " 0.         0.88499999 0.02125   ]\n",
                                    "Temperature Policy  [0.0275  0.02    0.02375 0.0225  0.885   0.02125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.025   0.      0.02    0.0275  0.      0.02125 0.      0.      0.90625]\n",
                                    "Temperature Policy  [2.5522304e-16 2.7404366e-17 6.6198282e-16 5.0246884e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.9325  0.      0.01875 0.02875 0.      0.02    0.      0.      0.     ]\n",
                                    "Temperature Policy  [1.00000000e+00 1.08025015e-17 7.76050686e-16 2.05973597e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.025      0.95249999 0.         0.0225\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.5514945e-16 1.0000000e+00 5.4097268e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.03       0.         0.         0.97000003\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.0074683e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.27625 0.07    0.0825  0.19    0.0825  0.0575  0.10375 0.06    0.0775 ]\n",
                                    "Temperature Policy  [0.27625 0.07    0.0825  0.19    0.0825  0.0575  0.10375 0.06    0.0775 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.12625 0.15625 0.      0.115   0.11125 0.1275  0.155   0.125   0.08375]\n",
                                    "Temperature Policy  [0.12625 0.15625 0.115   0.11125 0.1275  0.155   0.125   0.08375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0325  0.83875 0.      0.03    0.0275  0.      0.03    0.0275  0.01375]\n",
                                    "Temperature Policy  [0.0325  0.83875 0.03    0.0275  0.03    0.0275  0.01375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.47    0.      0.      0.10625 0.10625 0.      0.10625 0.11    0.10125]\n",
                                    "Temperature Policy  [0.47    0.10625 0.10625 0.10625 0.11    0.10125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.0225     0.92374998 0.\n",
                                    " 0.01875    0.01625    0.01875   ]\n",
                                    "Temperature Policy  [7.34988752e-17 1.00000000e+00 1.18704785e-17 2.83784823e-18\n",
                                    " 1.18704785e-17]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.185      0.         0.\n",
                                    " 0.38999999 0.29750001 0.1275    ]\n",
                                    "Temperature Policy  [5.40483335e-04 9.36938584e-01 6.25078753e-02 1.30667295e-05]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.025      0.         0.\n",
                                    " 0.         0.95875001 0.01625   ]\n",
                                    "Temperature Policy  [1.4532700e-16 1.0000000e+00 1.9565003e-18]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x32840c730>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x328654790>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x107dbdd20>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4610>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4310>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x325bf4910>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x325b83fd0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x107dd0ac0>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x325b831c0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x3286563b0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x107dd0ac0>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x325b83280>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x31a5148b0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x325b831c0>, 3)]\n",
                                    "Observations [array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]])]\n",
                                    "Policies [array([0.05      , 0.60750002, 0.05625   , 0.0525    , 0.05875   ,\n",
                                    "       0.03875   , 0.04625   , 0.04125   , 0.04875   ]), array([0.        , 0.        , 0.        , 0.0125    , 0.94375002,\n",
                                    "       0.01625   , 0.        , 0.0125    , 0.015     ]), array([0.90125, 0.     , 0.     , 0.02   , 0.01875, 0.0225 , 0.02125,\n",
                                    "       0.     , 0.01625]), array([0.15000001, 0.        , 0.145     , 0.13124999, 0.1125    ,\n",
                                    "       0.1225    , 0.14875001, 0.125     , 0.065     ]), array([0.     , 0.     , 0.     , 0.     , 0.90625, 0.02   , 0.03625,\n",
                                    "       0.0175 , 0.02   ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.        , 0.        , 0.44125   , 0.55874997]), array([0.105     , 0.        , 0.23875   , 0.06375   , 0.05375   ,\n",
                                    "       0.05      , 0.1075    , 0.185     , 0.19625001]), array([0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0.49250001, 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.        , 0.1225    , 0.        , 0.38499999]), array([0., 0., 0., 0., 0., 1., 0., 0., 0.]), array([0.23375   , 0.06875   , 0.08      , 0.2475    , 0.16500001,\n",
                                    "       0.06125   , 0.        , 0.0825    , 0.06125   ]), array([0.02375, 0.     , 0.     , 0.9325 , 0.     , 0.     , 0.02   ,\n",
                                    "       0.     , 0.02375]), array([0.15875, 0.     , 0.06   , 0.14   , 0.14375, 0.0575 , 0.1075 ,\n",
                                    "       0.1875 , 0.145  ]), array([0.0225    , 0.0225    , 0.        , 0.025     , 0.        ,\n",
                                    "       0.93000001, 0.        , 0.        , 0.        ]), array([0.02125, 0.     , 0.84125, 0.02   , 0.06625, 0.0175 , 0.0175 ,\n",
                                    "       0.01625, 0.     ]), array([0.66500002, 0.        , 0.1       , 0.        , 0.        ,\n",
                                    "       0.05125   , 0.085     , 0.05875   , 0.04      ])]\n",
                                    "NP Array Policies [[0.05       0.60750002 0.05625    0.0525     0.05875    0.03875\n",
                                    "  0.04625    0.04125    0.04875   ]\n",
                                    " [0.         0.         0.         0.0125     0.94375002 0.01625\n",
                                    "  0.         0.0125     0.015     ]\n",
                                    " [0.90125    0.         0.         0.02       0.01875    0.0225\n",
                                    "  0.02125    0.         0.01625   ]\n",
                                    " [0.15000001 0.         0.145      0.13124999 0.1125     0.1225\n",
                                    "  0.14875001 0.125      0.065     ]\n",
                                    " [0.         0.         0.         0.         0.90625    0.02\n",
                                    "  0.03625    0.0175     0.02      ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         0.44125    0.55874997]\n",
                                    " [0.105      0.         0.23875    0.06375    0.05375    0.05\n",
                                    "  0.1075     0.185      0.19625001]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         0.         1.        ]\n",
                                    " [0.49250001 0.         0.         0.         0.         0.\n",
                                    "  0.1225     0.         0.38499999]\n",
                                    " [0.         0.         0.         0.         0.         1.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.23375    0.06875    0.08       0.2475     0.16500001 0.06125\n",
                                    "  0.         0.0825     0.06125   ]\n",
                                    " [0.02375    0.         0.         0.9325     0.         0.\n",
                                    "  0.02       0.         0.02375   ]\n",
                                    " [0.15875    0.         0.06       0.14       0.14375    0.0575\n",
                                    "  0.1075     0.1875     0.145     ]\n",
                                    " [0.0225     0.0225     0.         0.025      0.         0.93000001\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.02125    0.         0.84125    0.02       0.06625    0.0175\n",
                                    "  0.0175     0.01625    0.        ]\n",
                                    " [0.66500002 0.         0.1        0.         0.         0.05125\n",
                                    "  0.085      0.05875    0.04      ]]\n",
                                    "Predicted: tensor([[0.1517, 0.1146, 0.0890, 0.1272, 0.1380, 0.0929, 0.1211, 0.0833, 0.0823],\n",
                                    "        [0.1547, 0.1088, 0.0882, 0.1220, 0.1473, 0.0943, 0.1200, 0.0804, 0.0844],\n",
                                    "        [0.1718, 0.0881, 0.0898, 0.1348, 0.1251, 0.0868, 0.1284, 0.1050, 0.0701],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1594, 0.1027, 0.0898, 0.1317, 0.1403, 0.0927, 0.1209, 0.0800, 0.0825],\n",
                                    "        [0.1905, 0.0771, 0.0769, 0.1671, 0.1108, 0.0691, 0.1285, 0.1135, 0.0665],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1875, 0.0893, 0.0703, 0.1441, 0.1492, 0.0880, 0.1239, 0.0739, 0.0738],\n",
                                    "        [0.1739, 0.0900, 0.0764, 0.1439, 0.1419, 0.0876, 0.1230, 0.0773, 0.0861],\n",
                                    "        [0.1848, 0.0930, 0.0754, 0.1542, 0.1565, 0.0783, 0.1177, 0.0738, 0.0664],\n",
                                    "        [0.1623, 0.0900, 0.0926, 0.1349, 0.1245, 0.0881, 0.1212, 0.1086, 0.0777],\n",
                                    "        [0.1832, 0.0773, 0.0764, 0.1557, 0.1197, 0.0774, 0.1316, 0.1160, 0.0628],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1818, 0.0881, 0.0766, 0.1271, 0.1213, 0.0806, 0.1370, 0.1093, 0.0782],\n",
                                    "        [0.1490, 0.1068, 0.0924, 0.1323, 0.1411, 0.0930, 0.1201, 0.0839, 0.0814],\n",
                                    "        [0.1689, 0.0827, 0.0862, 0.1342, 0.1144, 0.0848, 0.1387, 0.1158, 0.0743]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.1517, 0.1146, 0.0890, 0.1272, 0.1380, 0.0929, 0.1211, 0.0833, 0.0823],\n",
                                    "        [0.1547, 0.1088, 0.0882, 0.1220, 0.1473, 0.0943, 0.1200, 0.0804, 0.0844],\n",
                                    "        [0.1718, 0.0881, 0.0898, 0.1348, 0.1251, 0.0868, 0.1284, 0.1050, 0.0701],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1594, 0.1027, 0.0898, 0.1317, 0.1403, 0.0927, 0.1209, 0.0800, 0.0825],\n",
                                    "        [0.1905, 0.0771, 0.0769, 0.1671, 0.1108, 0.0691, 0.1285, 0.1135, 0.0665],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1875, 0.0893, 0.0703, 0.1441, 0.1492, 0.0880, 0.1239, 0.0739, 0.0738],\n",
                                    "        [0.1739, 0.0900, 0.0764, 0.1439, 0.1419, 0.0876, 0.1230, 0.0773, 0.0861],\n",
                                    "        [0.1848, 0.0930, 0.0754, 0.1542, 0.1565, 0.0783, 0.1177, 0.0738, 0.0664],\n",
                                    "        [0.1623, 0.0900, 0.0926, 0.1349, 0.1245, 0.0881, 0.1212, 0.1086, 0.0777],\n",
                                    "        [0.1832, 0.0773, 0.0764, 0.1557, 0.1197, 0.0774, 0.1316, 0.1160, 0.0628],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1818, 0.0881, 0.0766, 0.1271, 0.1213, 0.0806, 0.1370, 0.1093, 0.0782],\n",
                                    "        [0.1490, 0.1068, 0.0924, 0.1323, 0.1411, 0.0930, 0.1201, 0.0839, 0.0814],\n",
                                    "        [0.1689, 0.0827, 0.0862, 0.1342, 0.1144, 0.0848, 0.1387, 0.1158, 0.0743]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.1517, 0.1146, 0.0890, 0.1272, 0.1380, 0.0929, 0.1211, 0.0833, 0.0823],\n",
                                    "        [0.1547, 0.1088, 0.0882, 0.1220, 0.1473, 0.0943, 0.1200, 0.0804, 0.0844],\n",
                                    "        [0.1718, 0.0881, 0.0898, 0.1348, 0.1251, 0.0868, 0.1284, 0.1050, 0.0701],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1594, 0.1027, 0.0898, 0.1317, 0.1403, 0.0927, 0.1209, 0.0800, 0.0825],\n",
                                    "        [0.1905, 0.0771, 0.0769, 0.1671, 0.1108, 0.0691, 0.1285, 0.1135, 0.0665],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1875, 0.0893, 0.0703, 0.1441, 0.1492, 0.0880, 0.1239, 0.0739, 0.0738],\n",
                                    "        [0.1739, 0.0900, 0.0764, 0.1439, 0.1419, 0.0876, 0.1230, 0.0773, 0.0861],\n",
                                    "        [0.1848, 0.0930, 0.0754, 0.1542, 0.1565, 0.0783, 0.1177, 0.0738, 0.0664],\n",
                                    "        [0.1623, 0.0900, 0.0926, 0.1349, 0.1245, 0.0881, 0.1212, 0.1086, 0.0777],\n",
                                    "        [0.1832, 0.0773, 0.0764, 0.1557, 0.1197, 0.0774, 0.1316, 0.1160, 0.0628],\n",
                                    "        [0.1637, 0.0879, 0.0930, 0.1348, 0.1236, 0.0866, 0.1257, 0.1083, 0.0764],\n",
                                    "        [0.1818, 0.0881, 0.0766, 0.1271, 0.1213, 0.0806, 0.1370, 0.1093, 0.0782],\n",
                                    "        [0.1490, 0.1068, 0.0924, 0.1323, 0.1411, 0.0930, 0.1201, 0.0839, 0.0814],\n",
                                    "        [0.1689, 0.0827, 0.0862, 0.1342, 0.1144, 0.0848, 0.1387, 0.1158, 0.0743]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-1.8861, -2.1662, -2.4192, -2.0623, -1.9805, -2.3757, -2.1115, -2.4858,\n",
                                    "         -2.4973],\n",
                                    "        [-1.8662, -2.2185, -2.4283, -2.1041, -1.9151, -2.3615, -2.1203, -2.5205,\n",
                                    "         -2.4726],\n",
                                    "        [-1.7612, -2.4291, -2.4106, -2.0038, -2.0786, -2.4444, -2.0524, -2.2535,\n",
                                    "         -2.6575],\n",
                                    "        [-1.8100, -2.4314, -2.3756, -2.0040, -2.0905, -2.4463, -2.0737, -2.2230,\n",
                                    "         -2.5713],\n",
                                    "        [-1.8363, -2.2760, -2.4104, -2.0275, -1.9640, -2.3786, -2.1125, -2.5258,\n",
                                    "         -2.4944],\n",
                                    "        [-1.6580, -2.5630, -2.5650, -1.7893, -2.1996, -2.6728, -2.0521, -2.1758,\n",
                                    "         -2.7103],\n",
                                    "        [-1.8100, -2.4314, -2.3756, -2.0040, -2.0905, -2.4463, -2.0737, -2.2230,\n",
                                    "         -2.5713],\n",
                                    "        [-1.6741, -2.4156, -2.6552, -1.9370, -1.9028, -2.4309, -2.0881, -2.6049,\n",
                                    "         -2.6058],\n",
                                    "        [-1.7493, -2.4082, -2.5722, -1.9389, -1.9523, -2.4355, -2.0958, -2.5596,\n",
                                    "         -2.4523],\n",
                                    "        [-1.6886, -2.3756, -2.5853, -1.8693, -1.8546, -2.5472, -2.1396, -2.6070,\n",
                                    "         -2.7123],\n",
                                    "        [-1.8181, -2.4079, -2.3792, -2.0033, -2.0836, -2.4291, -2.1103, -2.2200,\n",
                                    "         -2.5544],\n",
                                    "        [-1.6974, -2.5603, -2.5720, -1.8600, -2.1227, -2.5590, -2.0283, -2.1541,\n",
                                    "         -2.7671],\n",
                                    "        [-1.8100, -2.4314, -2.3756, -2.0040, -2.0905, -2.4463, -2.0737, -2.2230,\n",
                                    "         -2.5713],\n",
                                    "        [-1.7046, -2.4294, -2.5696, -2.0625, -2.1096, -2.5186, -1.9879, -2.2134,\n",
                                    "         -2.5487],\n",
                                    "        [-1.9036, -2.2372, -2.3821, -2.0226, -1.9583, -2.3749, -2.1197, -2.4778,\n",
                                    "         -2.5082],\n",
                                    "        [-1.7787, -2.4924, -2.4515, -2.0081, -2.1680, -2.4673, -1.9754, -2.1561,\n",
                                    "         -2.5996]], grad_fn=<LogBackward0>)\n",
                                    "Losses 1.1811566 2.1859279 3.3670845\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_1/videos/alphazero/1/alphazero-episode-4.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_1/videos/alphazero/1/alphazero-episode-4.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                   "
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_1/videos/alphazero/1/alphazero-episode-4.mp4\n",
                                    "score:  0\n",
                                    "Plotting score...\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:245: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
                                    "  axs[row][col].set_xlim(1, len(values))\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.17749999 0.1175     0.1        0.09       0.10875    0.0925\n",
                                    " 0.1075     0.0975     0.10875   ]\n",
                                    "Temperature Policy  [0.1775  0.1175  0.1     0.09    0.10875 0.0925  0.1075  0.0975  0.10875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.155      0.07125    0.06375    0.32499999 0.13124999\n",
                                    " 0.095      0.07125    0.0875    ]\n",
                                    "Temperature Policy  [0.155   0.07125 0.06375 0.325   0.13125 0.095   0.07125 0.0875 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.21250001 0.10625    0.12375    0.         0.155\n",
                                    " 0.175      0.11125    0.11625   ]\n",
                                    "Temperature Policy  [0.2125  0.10625 0.12375 0.155   0.175   0.11125 0.11625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.78625 0.04    0.0375  0.      0.      0.0425  0.0475  0.04625]\n",
                                    "Temperature Policy  [0.78625 0.04    0.0375  0.0425  0.0475  0.04625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.02375 0.0225  0.      0.      0.02375 0.90625 0.02375]\n",
                                    "Temperature Policy  [1.5281147e-16 8.8990777e-17 1.5281147e-16 1.0000000e+00 1.5281147e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.0275     0.0275     0.         0.\n",
                                    " 0.88625002 0.         0.05875   ]\n",
                                    "Temperature Policy  [8.2749322e-16 8.2749322e-16 1.0000000e+00 1.6387633e-12]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.94375002 0.0275     0.         0.\n",
                                    " 0.         0.         0.02875   ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.4132375e-16 6.8834927e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.0275     0.         0.\n",
                                    " 0.         0.         0.97250003]\n",
                                    "Temperature Policy  [3.2691197e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.13124999 0.17125    0.1        0.08875    0.10625    0.095\n",
                                    " 0.105      0.09625    0.10625   ]\n",
                                    "Temperature Policy  [0.13125 0.17125 0.1     0.08875 0.10625 0.095   0.105   0.09625 0.10625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.2375     0.11375    0.07       0.065      0.         0.19625001\n",
                                    " 0.07375    0.07375    0.17      ]\n",
                                    "Temperature Policy  [0.2375  0.11375 0.07    0.065   0.19625 0.07375 0.07375 0.17   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.44749999 0.0525     0.08875    0.         0.0975\n",
                                    " 0.17125    0.07625    0.06625   ]\n",
                                    "Temperature Policy  [0.4475  0.0525  0.08875 0.0975  0.17125 0.07625 0.06625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.02125    0.         0.02625    0.         0.0225\n",
                                    " 0.88749999 0.02125    0.02125   ]\n",
                                    "Temperature Policy  [0.02125 0.02625 0.0225  0.8875  0.02125 0.02125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.01625    0.         0.91624999 0.         0.0275\n",
                                    " 0.         0.01875    0.02125   ]\n",
                                    "Temperature Policy  [3.0788872e-18 1.0000000e+00 5.9318072e-16 1.2878724e-17 4.5024559e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02125    0.         0.         0.         0.92750001\n",
                                    " 0.         0.02375    0.0275    ]\n",
                                    "Temperature Policy  [3.9852004e-17 1.0000000e+00 1.2119842e-16 5.2503438e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.39500001 0.         0.         0.         0.\n",
                                    " 0.         0.47749999 0.1275    ]\n",
                                    "Temperature Policy  [1.3047269e-01 8.6952573e-01 1.6019558e-06]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.96875 0.      0.      0.      0.      0.      0.      0.03125]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2200653e-15]\n",
                                    "Action  1\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.17749999 0.16500001 0.1        0.04375    0.1075     0.0925\n",
                                    " 0.11       0.0975     0.10625   ]\n",
                                    "Temperature Policy  [0.1775  0.165   0.1     0.04375 0.1075  0.0925  0.11    0.0975  0.10625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.13       0.1425     0.         0.07875    0.22125    0.10625\n",
                                    " 0.09375    0.09625    0.13124999]\n",
                                    "Temperature Policy  [0.13    0.1425  0.07875 0.22125 0.10625 0.09375 0.09625 0.13125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0725     0.74624997 0.         0.025      0.02125    0.0475\n",
                                    " 0.045      0.0425     0.        ]\n",
                                    "Temperature Policy  [0.0725  0.74625 0.025   0.02125 0.0475  0.045   0.0425 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.37625 0.      0.12    0.12125 0.12375 0.1425  0.11625 0.     ]\n",
                                    "Temperature Policy  [0.37625 0.12    0.12125 0.12375 0.1425  0.11625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.02625    0.02625    0.01625\n",
                                    " 0.91250002 0.01875    0.        ]\n",
                                    "Temperature Policy  [3.8811849e-16 3.8811849e-16 3.2077826e-18 1.0000000e+00 1.3417883e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.      0.      0.31375 0.4025  0.11625 0.      0.1675  0.     ]\n",
                                    "Temperature Policy  [7.6481223e-02 9.2337120e-01 3.7294699e-06 1.4383534e-04]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.96125001 0.         0.015\n",
                                    " 0.         0.02375    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.5614843e-19 8.4775941e-17]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.13124999 0.16500001 0.1025     0.09       0.10625    0.0925\n",
                                    " 0.1075     0.0975     0.1075    ]\n",
                                    "Temperature Policy  [0.13125 0.165   0.1025  0.09    0.10625 0.0925  0.1075  0.0975  0.1075 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.15625    0.0725     0.06375    0.32499999 0.13124999\n",
                                    " 0.09375    0.07125    0.08625   ]\n",
                                    "Temperature Policy  [0.15625 0.0725  0.06375 0.325   0.13125 0.09375 0.07125 0.08625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.0175     0.015      0.02       0.0275     0.01875\n",
                                    " 0.88625002 0.015      0.        ]\n",
                                    "Temperature Policy  [0.0175  0.015   0.02    0.0275  0.01875 0.88625 0.015  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.045      0.76875001 0.04125    0.04375    0.06125\n",
                                    " 0.04       0.         0.        ]\n",
                                    "Temperature Policy  [0.045   0.76875 0.04125 0.04375 0.06125 0.04   ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.01875    0.         0.0275     0.0225     0.91000003\n",
                                    " 0.02125    0.         0.        ]\n",
                                    "Temperature Policy  [1.3791098e-17 6.3520372e-16 8.5390841e-17 1.0000000e+00 4.8214254e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.0275     0.         0.05125    0.89499998 0.\n",
                                    " 0.02625    0.         0.        ]\n",
                                    "Temperature Policy  [7.5006096e-16 3.7906034e-13 1.0000000e+00 4.7104535e-16]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.02375    0.         0.02625    0.         0.\n",
                                    " 0.94999999 0.         0.        ]\n",
                                    "Temperature Policy  [9.536743e-17 2.594520e-16 1.000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.02       0.         0.98000002 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.2532543e-17 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.13124999 0.17125    0.1        0.0875     0.1075     0.095\n",
                                    " 0.10375    0.0975     0.10625   ]\n",
                                    "Temperature Policy  [0.13125 0.17125 0.1     0.0875  0.1075  0.095   0.10375 0.0975  0.10625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.13124999 0.145      0.         0.09       0.23625    0.1075\n",
                                    " 0.095      0.0975     0.0975    ]\n",
                                    "Temperature Policy  [0.13125 0.145   0.09    0.23625 0.1075  0.095   0.0975  0.0975 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.06875 0.8125  0.      0.02375 0.0175  0.02    0.04375 0.01375 0.     ]\n",
                                    "Temperature Policy  [0.06875 0.8125  0.02375 0.0175  0.02    0.04375 0.01375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.52249998 0.         0.         0.09       0.12375    0.0875\n",
                                    " 0.10625    0.07       0.        ]\n",
                                    "Temperature Policy  [0.5225  0.09    0.12375 0.0875  0.10625 0.07   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.01375    0.94625002 0.0125\n",
                                    " 0.0125     0.015      0.        ]\n",
                                    "Temperature Policy  [4.1972809e-19 1.0000000e+00 1.6182335e-19 1.6182335e-19 1.0019676e-18]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.14375    0.         0.10375\n",
                                    " 0.39750001 0.35499999 0.        ]\n",
                                    "Temperature Policy  [2.8920458e-05 1.1091944e-06 7.5595856e-01 2.4401143e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.      0.      0.0175  0.      0.01625 0.      0.96625 0.     ]\n",
                                    "Temperature Policy  [3.7973922e-18 1.8098333e-18 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.17625 0.1175  0.10125 0.08875 0.1075  0.09375 0.10625 0.0975  0.11125]\n",
                                    "Temperature Policy  [0.17625 0.1175  0.10125 0.08875 0.1075  0.09375 0.10625 0.0975  0.11125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.145      0.105      0.075      0.10125    0.34999999 0.08125\n",
                                    " 0.         0.06875    0.07375   ]\n",
                                    "Temperature Policy  [0.145   0.105   0.075   0.10125 0.35    0.08125 0.06875 0.07375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0275     0.0225     0.01625    0.         0.025      0.0175\n",
                                    " 0.         0.015      0.87625003]\n",
                                    "Temperature Policy  [0.0275  0.0225  0.01625 0.025   0.0175  0.015   0.87625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.125   0.11375 0.1025  0.      0.15625 0.15375 0.      0.34875 0.     ]\n",
                                    "Temperature Policy  [0.125   0.11375 0.1025  0.15625 0.15375 0.34875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.03375    0.         0.03125    0.         0.03625    0.02\n",
                                    " 0.         0.87875003 0.        ]\n",
                                    "Temperature Policy  [6.9836808e-15 3.2347955e-15 1.4270091e-14 3.7294652e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.17625 0.1175  0.10125 0.08875 0.1075  0.095   0.11    0.0975  0.10625]\n",
                                    "Temperature Policy  [0.17625 0.1175  0.10125 0.08875 0.1075  0.095   0.11    0.0975  0.10625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.145      0.10375    0.07375    0.10375    0.35124999 0.0825\n",
                                    " 0.         0.07       0.07      ]\n",
                                    "Temperature Policy  [0.145   0.10375 0.07375 0.10375 0.35125 0.0825  0.07    0.07   ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.88749999 0.         0.01625    0.0225     0.0175     0.01625\n",
                                    " 0.         0.0225     0.0175    ]\n",
                                    "Temperature Policy  [0.8875  0.01625 0.0225  0.0175  0.01625 0.0225  0.0175 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.11       0.39625001 0.13625    0.115\n",
                                    " 0.         0.1275     0.115     ]\n",
                                    "Temperature Policy  [0.11    0.39625 0.13625 0.115   0.1275  0.115  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.0125     0.92624998 0.         0.01375\n",
                                    " 0.         0.035      0.0125    ]\n",
                                    "Temperature Policy  [2.0036318e-19 1.0000000e+00 5.1969049e-19 5.9346931e-15 2.0036318e-19]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.13124999 0.16625001 0.10125    0.08875    0.1075     0.095\n",
                                    " 0.10625    0.0975     0.10625   ]\n",
                                    "Temperature Policy  [0.13125 0.16625 0.10125 0.08875 0.1075  0.095   0.10625 0.0975  0.10625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.19       0.1125     0.06875    0.06625    0.         0.19625001\n",
                                    " 0.125      0.0725     0.16875   ]\n",
                                    "Temperature Policy  [0.19    0.1125  0.06875 0.06625 0.19625 0.125   0.0725  0.16875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.88499999 0.         0.0175     0.01625    0.         0.01625\n",
                                    " 0.0175     0.02       0.0275    ]\n",
                                    "Temperature Policy  [0.885   0.0175  0.01625 0.01625 0.0175  0.02    0.0275 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.10625    0.13625    0.         0.125\n",
                                    " 0.12375    0.1225     0.38624999]\n",
                                    "Temperature Policy  [0.10625 0.13625 0.125   0.12375 0.1225  0.38625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.02375    0.035      0.         0.02125\n",
                                    " 0.90249997 0.0175     0.        ]\n",
                                    "Temperature Policy  [1.5928103e-16 7.6949953e-15 5.2374181e-17 1.0000000e+00 7.5146439e-18]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.37875    0.35249999 0.         0.17125\n",
                                    " 0.         0.0975     0.        ]\n",
                                    "Temperature Policy  [6.7206132e-01 3.2769793e-01 2.3998732e-04 8.5887240e-07]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.94875002 0.         0.         0.0275\n",
                                    " 0.         0.02375    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.1860948e-16 9.6631398e-17]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.13124999 0.17       0.10125    0.08875    0.10875    0.0925\n",
                                    " 0.1075     0.095      0.105     ]\n",
                                    "Temperature Policy  [0.13125 0.17    0.10125 0.08875 0.10875 0.0925  0.1075  0.095   0.105  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.145   0.14125 0.09625 0.09625 0.1925  0.105   0.0975  0.      0.12625]\n",
                                    "Temperature Policy  [0.145   0.14125 0.09625 0.09625 0.1925  0.105   0.0975  0.12625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.06375    0.         0.05375    0.05       0.065      0.0575\n",
                                    " 0.37875    0.         0.33125001]\n",
                                    "Temperature Policy  [0.06375 0.05375 0.05    0.065   0.0575  0.37875 0.33125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.025      0.         0.0225     0.025      0.02375    0.02\n",
                                    " 0.88375002 0.         0.        ]\n",
                                    "Temperature Policy  [0.025   0.0225  0.025   0.02375 0.02    0.88375]\n",
                                    "Action  6\n",
                                    "Target Policy [0.175      0.         0.70875001 0.05125    0.02625    0.03875\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.4226627e-07 9.9999917e-01 3.9084777e-12 4.8569318e-15 2.3866347e-13]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02875    0.         0.         0.02375    0.0225     0.92500001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.4131992e-16 1.2451419e-16 7.2511665e-17 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.48875001 0.         0.         0.1225     0.38874999 0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.0797335e-01 8.8831354e-07 9.2025764e-02]\n",
                                    "Action  4\n",
                                    "Target Policy [0.97624999 0.         0.         0.02375    0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.000000e+00 7.261489e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.17749999 0.1175     0.10125    0.08875    0.1075     0.09375\n",
                                    " 0.11       0.0975     0.10625   ]\n",
                                    "Temperature Policy  [0.1775  0.1175  0.10125 0.08875 0.1075  0.09375 0.11    0.0975  0.10625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.13249999 0.14375    0.         0.0925     0.23125    0.1075\n",
                                    " 0.09625    0.09875    0.0975    ]\n",
                                    "Temperature Policy  [0.1325  0.14375 0.0925  0.23125 0.1075  0.09625 0.09875 0.0975 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.05625    0.         0.         0.02125    0.07125    0.02125\n",
                                    " 0.06       0.02125    0.74874997]\n",
                                    "Temperature Policy  [0.05625 0.02125 0.07125 0.02125 0.06    0.02125 0.74875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.11125    0.         0.         0.095      0.1175     0.45625001\n",
                                    " 0.10625    0.11375    0.        ]\n",
                                    "Temperature Policy  [0.11125 0.095   0.1175  0.45625 0.10625 0.11375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0375     0.         0.         0.         0.03875    0.88875002\n",
                                    " 0.01625    0.01875    0.        ]\n",
                                    "Temperature Policy  [1.7886137e-14 2.4826810e-14 1.0000000e+00 4.1757794e-18 1.7466931e-17]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.17749999 0.1175     0.10375    0.0875     0.1075     0.095\n",
                                    " 0.1075     0.0975     0.10625   ]\n",
                                    "Temperature Policy  [0.1775  0.1175  0.10375 0.0875  0.1075  0.095   0.1075  0.0975  0.10625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.09875    0.25125    0.0625     0.0575     0.26875001 0.13124999\n",
                                    " 0.0675     0.0625     0.        ]\n",
                                    "Temperature Policy  [0.09875 0.25125 0.0625  0.0575  0.26875 0.13125 0.0675  0.0625 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1375     0.12       0.0875     0.16       0.         0.18000001\n",
                                    " 0.1825     0.13249999 0.        ]\n",
                                    "Temperature Policy  [0.1375 0.12   0.0875 0.16   0.18   0.1825 0.1325]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02625    0.01875    0.89249998 0.02375    0.         0.\n",
                                    " 0.02       0.01875    0.        ]\n",
                                    "Temperature Policy  [0.02625 0.01875 0.8925  0.02375 0.02    0.01875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0275  0.02    0.      0.02375 0.      0.      0.90625 0.0225  0.     ]\n",
                                    "Temperature Policy  [6.6198282e-16 2.7404366e-17 1.5281147e-16 1.0000000e+00 8.8990777e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02625    0.02375    0.         0.02125    0.         0.\n",
                                    " 0.         0.92874998 0.        ]\n",
                                    "Temperature Policy  [3.2531555e-16 1.1957707e-16 3.9318876e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.03125    0.94125003 0.         0.0275     0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.6272352e-15 1.0000000e+00 4.5318656e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.60374999 0.         0.         0.39625001 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.98538744 0.01461263]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.17749999 0.125      0.1        0.08875    0.10625    0.0925\n",
                                    " 0.10625    0.09875    0.105     ]\n",
                                    "Temperature Policy  [0.1775  0.125   0.1     0.08875 0.10625 0.0925  0.10625 0.09875 0.105  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.15875    0.17375    0.11125    0.         0.10625    0.11625\n",
                                    " 0.14749999 0.09       0.09625   ]\n",
                                    "Temperature Policy  [0.15875 0.17375 0.11125 0.10625 0.11625 0.1475  0.09    0.09625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.02625    0.         0.05       0.         0.80624998 0.01375\n",
                                    " 0.05125    0.025      0.0275    ]\n",
                                    "Temperature Policy  [0.02625 0.05    0.80625 0.01375 0.05125 0.025   0.0275 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.13124999 0.         0.13375001 0.         0.         0.37375\n",
                                    " 0.11625    0.1175     0.1275    ]\n",
                                    "Temperature Policy  [0.13125 0.13375 0.37375 0.11625 0.1175  0.1275 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.045      0.         0.0175     0.         0.         0.87625003\n",
                                    " 0.0325     0.         0.02875   ]\n",
                                    "Temperature Policy  [1.2759829e-13 1.0094858e-17 1.0000000e+00 4.9266685e-15 1.4457667e-15]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.17749999 0.1175     0.1025     0.08875    0.10875    0.09375\n",
                                    " 0.105      0.09625    0.11      ]\n",
                                    "Temperature Policy  [0.1775  0.1175  0.1025  0.08875 0.10875 0.09375 0.105   0.09625 0.11   ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.14624999 0.10375    0.0725     0.1025     0.35249999 0.08125\n",
                                    " 0.         0.07       0.07125   ]\n",
                                    "Temperature Policy  [0.14625 0.10375 0.0725  0.1025  0.3525  0.08125 0.07    0.07125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1725     0.1725     0.0975     0.0975     0.         0.17874999\n",
                                    " 0.         0.1025     0.17874999]\n",
                                    "Temperature Policy  [0.1725  0.1725  0.0975  0.0975  0.17875 0.1025  0.17875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.025      0.015      0.01875    0.0225     0.         0.02\n",
                                    " 0.         0.89875001 0.        ]\n",
                                    "Temperature Policy  [0.025   0.015   0.01875 0.0225  0.02    0.89875]\n",
                                    "Action  7\n",
                                    "Target Policy [0.03       0.89875001 0.02375    0.02625    0.         0.02125\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.7172104e-15 1.0000000e+00 1.6605315e-16 4.5175613e-16 5.4600965e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0525     0.         0.0325     0.17749999 0.         0.73750001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [3.3417386e-12 2.7619325e-14 6.5217330e-07 9.9999928e-01]\n",
                                    "Action  5\n",
                                    "Target Policy [0.025      0.         0.02125    0.95375001 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.5312798e-16 3.0146981e-17 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0.98000002 0.         0.02       0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2532543e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.17749999 0.1175     0.10625    0.085      0.1075     0.09375\n",
                                    " 0.10875    0.0975     0.10625   ]\n",
                                    "Temperature Policy  [0.1775  0.1175  0.10625 0.085   0.1075  0.09375 0.10875 0.0975  0.10625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.145      0.14125    0.09375    0.09625    0.21250001 0.10375\n",
                                    " 0.09625    0.         0.11125   ]\n",
                                    "Temperature Policy  [0.145   0.14125 0.09375 0.09625 0.2125  0.10375 0.09625 0.11125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.12       0.1375     0.10875    0.06125    0.         0.125\n",
                                    " 0.21875    0.         0.22875001]\n",
                                    "Temperature Policy  [0.12    0.1375  0.10875 0.06125 0.125   0.21875 0.22875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.04       0.04375    0.045      0.04625    0.         0.\n",
                                    " 0.0625     0.         0.76249999]\n",
                                    "Temperature Policy  [0.04    0.04375 0.045   0.04625 0.0625  0.7625 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91624999 0.01625    0.02375    0.02625    0.         0.\n",
                                    " 0.0175     0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.0788872e-18 1.3692926e-16 3.7252308e-16 6.4601205e-18]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.0525  0.72375 0.07125 0.      0.      0.1525  0.      0.     ]\n",
                                    "Temperature Policy  [4.0337334e-12 9.9999988e-01 8.5499365e-11 1.7250935e-07]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.025      0.         0.0275     0.         0.\n",
                                    " 0.94749999 0.         0.        ]\n",
                                    "Temperature Policy  [1.6353395e-16 4.2416493e-16 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.0225     0.         0.97750002 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.1750102e-17 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.17874999 0.1175     0.1025     0.08875    0.1075     0.09375\n",
                                    " 0.1075     0.09625    0.1075    ]\n",
                                    "Temperature Policy  [0.17875 0.1175  0.1025  0.08875 0.1075  0.09375 0.1075  0.09625 0.1075 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.15625    0.07       0.06375    0.32499999 0.13249999\n",
                                    " 0.0925     0.07375    0.08625   ]\n",
                                    "Temperature Policy  [0.15625 0.07    0.06375 0.325   0.1325  0.0925  0.07375 0.08625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.83625001 0.01875    0.02       0.04375    0.0175\n",
                                    " 0.         0.02       0.04375   ]\n",
                                    "Temperature Policy  [0.83625 0.01875 0.02    0.04375 0.0175  0.02    0.04375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.47624999 0.09       0.11625    0.0825\n",
                                    " 0.         0.11375    0.12125   ]\n",
                                    "Temperature Policy  [0.47625 0.09    0.11625 0.0825  0.11375 0.12125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.93124998 0.0125     0.015      0.015\n",
                                    " 0.         0.02625    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.8986167e-19 1.1755734e-18 1.1755734e-18 3.1668698e-16]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.13    0.20625 0.10125 0.05375 0.10625 0.095   0.1075  0.095   0.105  ]\n",
                                    "Temperature Policy  [0.13    0.20625 0.10125 0.05375 0.10625 0.095   0.1075  0.095   0.105  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.145      0.14       0.09375    0.09625    0.21250001 0.1175\n",
                                    " 0.09625    0.         0.09875   ]\n",
                                    "Temperature Policy  [0.145   0.14    0.09375 0.09625 0.2125  0.1175  0.09625 0.09875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0625     0.         0.055      0.05375    0.06375    0.05625\n",
                                    " 0.37875    0.         0.33000001]\n",
                                    "Temperature Policy  [0.0625  0.055   0.05375 0.06375 0.05625 0.37875 0.33   ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.1375     0.         0.06125    0.         0.0875     0.09125\n",
                                    " 0.51999998 0.         0.1025    ]\n",
                                    "Temperature Policy  [0.1375  0.06125 0.0875  0.09125 0.52    0.1025 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0475     0.         0.06125    0.         0.70125002 0.14\n",
                                    " 0.         0.         0.05      ]\n",
                                    "Temperature Policy  [2.0333275e-12 2.5842358e-11 9.9999988e-01 1.0058926e-07 3.3960280e-12]\n",
                                    "Action  4\n",
                                    "Target Policy [0.0275     0.         0.02125    0.         0.         0.92374998\n",
                                    " 0.         0.         0.0275    ]\n",
                                    "Temperature Policy  [5.4674198e-16 4.1499688e-17 1.0000000e+00 5.4674198e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.47749999 0.         0.1175     0.         0.         0.\n",
                                    " 0.         0.         0.405     ]\n",
                                    "Temperature Policy  [8.3845359e-01 6.8253343e-07 1.6154572e-01]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.02625 0.      0.      0.      0.      0.      0.97375]\n",
                                    "Temperature Policy  [2.0268348e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x107dbd8a0>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x325b83fd0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4310>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x32d85abc0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x328654790>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x107dbdd20>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x107e20400>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x325b83fd0>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x3286f5600>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x31a5148b0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32d8f4610>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x107dd1cc0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4310>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x328654790>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x107dbd690>, 4)]\n",
                                    "Observations [array([[[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]])]\n",
                                    "Policies [array([0.        , 0.        , 0.11      , 0.39625001, 0.13625   ,\n",
                                    "       0.115     , 0.        , 0.1275    , 0.115     ]), array([0.        , 0.11375   , 0.11875   , 0.24625   , 0.        ,\n",
                                    "       0.13375001, 0.1425    , 0.1025    , 0.1425    ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.        , 0.        , 0.44125   , 0.55874997]), array([0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0.14624999, 0.        , 0.2375    , 0.14      , 0.05625   ,\n",
                                    "       0.14375   , 0.06875   , 0.0625    , 0.145     ]), array([0.        , 0.        , 0.03      , 0.        , 0.        ,\n",
                                    "       0.97000003, 0.        , 0.        , 0.        ]), array([0.89625001, 0.025     , 0.015     , 0.0175    , 0.0175    ,\n",
                                    "       0.015     , 0.        , 0.        , 0.01375   ]), array([0.        , 0.        , 0.        , 0.97874999, 0.        ,\n",
                                    "       0.        , 0.        , 0.        , 0.02125   ]), array([0.13124999, 0.17125   , 0.1       , 0.08875   , 0.10625   ,\n",
                                    "       0.095     , 0.105     , 0.09625   , 0.10625   ]), array([0.15875   , 0.        , 0.14875001, 0.14      , 0.0525    ,\n",
                                    "       0.06      , 0.1075    , 0.1875    , 0.145     ]), array([0.15875, 0.     , 0.06   , 0.14   , 0.14375, 0.0575 , 0.1075 ,\n",
                                    "       0.1875 , 0.145  ]), array([0.        , 0.        , 0.47624999, 0.09      , 0.11625   ,\n",
                                    "       0.0825    , 0.        , 0.11375   , 0.12125   ]), array([0.01625, 0.     , 0.02375, 0.015  , 0.     , 0.     , 0.0125 ,\n",
                                    "       0.9325 , 0.     ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.94999999, 0.        , 0.03      , 0.02      ]), array([0.        , 0.        , 0.        , 0.08875   , 0.        ,\n",
                                    "       0.14624999, 0.        , 0.38      , 0.38499999]), array([0.045     , 0.        , 0.0175    , 0.        , 0.        ,\n",
                                    "       0.87625003, 0.0325    , 0.        , 0.02875   ])]\n",
                                    "NP Array Policies [[0.         0.         0.11       0.39625001 0.13625    0.115\n",
                                    "  0.         0.1275     0.115     ]\n",
                                    " [0.         0.11375    0.11875    0.24625    0.         0.13375001\n",
                                    "  0.1425     0.1025     0.1425    ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         0.44125    0.55874997]\n",
                                    " [0.         0.         0.         1.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.14624999 0.         0.2375     0.14       0.05625    0.14375\n",
                                    "  0.06875    0.0625     0.145     ]\n",
                                    " [0.         0.         0.03       0.         0.         0.97000003\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.89625001 0.025      0.015      0.0175     0.0175     0.015\n",
                                    "  0.         0.         0.01375   ]\n",
                                    " [0.         0.         0.         0.97874999 0.         0.\n",
                                    "  0.         0.         0.02125   ]\n",
                                    " [0.13124999 0.17125    0.1        0.08875    0.10625    0.095\n",
                                    "  0.105      0.09625    0.10625   ]\n",
                                    " [0.15875    0.         0.14875001 0.14       0.0525     0.06\n",
                                    "  0.1075     0.1875     0.145     ]\n",
                                    " [0.15875    0.         0.06       0.14       0.14375    0.0575\n",
                                    "  0.1075     0.1875     0.145     ]\n",
                                    " [0.         0.         0.47624999 0.09       0.11625    0.0825\n",
                                    "  0.         0.11375    0.12125   ]\n",
                                    " [0.01625    0.         0.02375    0.015      0.         0.\n",
                                    "  0.0125     0.9325     0.        ]\n",
                                    " [0.         0.         0.         0.         0.         0.94999999\n",
                                    "  0.         0.03       0.02      ]\n",
                                    " [0.         0.         0.         0.08875    0.         0.14624999\n",
                                    "  0.         0.38       0.38499999]\n",
                                    " [0.045      0.         0.0175     0.         0.         0.87625003\n",
                                    "  0.0325     0.         0.02875   ]]\n",
                                    "Predicted: tensor([[0.1903, 0.0602, 0.0769, 0.1571, 0.0948, 0.0946, 0.0878, 0.1130, 0.1255],\n",
                                    "        [0.1395, 0.0978, 0.1077, 0.1114, 0.1428, 0.1021, 0.0999, 0.0838, 0.1150],\n",
                                    "        [0.2291, 0.0418, 0.0642, 0.1512, 0.0730, 0.0793, 0.0832, 0.1342, 0.1439],\n",
                                    "        [0.1502, 0.0886, 0.1024, 0.1063, 0.1625, 0.1096, 0.0928, 0.0669, 0.1207],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2606, 0.0404, 0.0580, 0.1669, 0.0834, 0.0847, 0.0820, 0.1091, 0.1149],\n",
                                    "        [0.1447, 0.0942, 0.1056, 0.1099, 0.1469, 0.1067, 0.0940, 0.0786, 0.1194],\n",
                                    "        [0.2437, 0.0408, 0.0602, 0.1635, 0.0785, 0.0818, 0.0829, 0.1208, 0.1279],\n",
                                    "        [0.1414, 0.1068, 0.1079, 0.1076, 0.1423, 0.0985, 0.0992, 0.0823, 0.1140],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2405, 0.0441, 0.0647, 0.1596, 0.0909, 0.0852, 0.0827, 0.1145, 0.1177],\n",
                                    "        [0.1403, 0.0940, 0.1132, 0.1081, 0.1340, 0.1056, 0.1038, 0.0861, 0.1148],\n",
                                    "        [0.1509, 0.0921, 0.1031, 0.0979, 0.1644, 0.1088, 0.0956, 0.0705, 0.1166],\n",
                                    "        [0.2417, 0.0382, 0.0579, 0.1811, 0.0825, 0.0785, 0.0776, 0.1197, 0.1229],\n",
                                    "        [0.1503, 0.0889, 0.1024, 0.1112, 0.1268, 0.1058, 0.1042, 0.0856, 0.1248]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.1903, 0.0602, 0.0769, 0.1571, 0.0948, 0.0946, 0.0878, 0.1130, 0.1255],\n",
                                    "        [0.1395, 0.0978, 0.1077, 0.1114, 0.1428, 0.1021, 0.0999, 0.0838, 0.1150],\n",
                                    "        [0.2291, 0.0418, 0.0642, 0.1512, 0.0730, 0.0793, 0.0832, 0.1342, 0.1439],\n",
                                    "        [0.1502, 0.0886, 0.1024, 0.1063, 0.1625, 0.1096, 0.0928, 0.0669, 0.1207],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2606, 0.0404, 0.0580, 0.1669, 0.0834, 0.0847, 0.0820, 0.1091, 0.1149],\n",
                                    "        [0.1447, 0.0942, 0.1056, 0.1099, 0.1469, 0.1067, 0.0940, 0.0786, 0.1194],\n",
                                    "        [0.2437, 0.0408, 0.0602, 0.1635, 0.0785, 0.0818, 0.0829, 0.1208, 0.1279],\n",
                                    "        [0.1414, 0.1068, 0.1079, 0.1076, 0.1423, 0.0985, 0.0992, 0.0823, 0.1140],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2405, 0.0441, 0.0647, 0.1596, 0.0909, 0.0852, 0.0827, 0.1145, 0.1177],\n",
                                    "        [0.1403, 0.0940, 0.1132, 0.1081, 0.1340, 0.1056, 0.1038, 0.0861, 0.1148],\n",
                                    "        [0.1509, 0.0921, 0.1031, 0.0979, 0.1644, 0.1088, 0.0956, 0.0705, 0.1166],\n",
                                    "        [0.2417, 0.0382, 0.0579, 0.1811, 0.0825, 0.0785, 0.0776, 0.1197, 0.1229],\n",
                                    "        [0.1503, 0.0889, 0.1024, 0.1112, 0.1268, 0.1058, 0.1042, 0.0856, 0.1248]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.1903, 0.0602, 0.0769, 0.1571, 0.0948, 0.0946, 0.0878, 0.1130, 0.1255],\n",
                                    "        [0.1395, 0.0978, 0.1077, 0.1114, 0.1428, 0.1021, 0.0999, 0.0838, 0.1150],\n",
                                    "        [0.2291, 0.0418, 0.0642, 0.1512, 0.0730, 0.0793, 0.0832, 0.1342, 0.1439],\n",
                                    "        [0.1502, 0.0886, 0.1024, 0.1063, 0.1625, 0.1096, 0.0928, 0.0669, 0.1207],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2606, 0.0404, 0.0580, 0.1669, 0.0834, 0.0847, 0.0820, 0.1091, 0.1149],\n",
                                    "        [0.1447, 0.0942, 0.1056, 0.1099, 0.1469, 0.1067, 0.0940, 0.0786, 0.1194],\n",
                                    "        [0.2437, 0.0408, 0.0602, 0.1635, 0.0785, 0.0818, 0.0829, 0.1208, 0.1279],\n",
                                    "        [0.1414, 0.1068, 0.1079, 0.1076, 0.1423, 0.0985, 0.0992, 0.0823, 0.1140],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2429, 0.0454, 0.0641, 0.1565, 0.0914, 0.0854, 0.0847, 0.1125, 0.1171],\n",
                                    "        [0.2405, 0.0441, 0.0647, 0.1596, 0.0909, 0.0852, 0.0827, 0.1145, 0.1177],\n",
                                    "        [0.1403, 0.0940, 0.1132, 0.1081, 0.1340, 0.1056, 0.1038, 0.0861, 0.1148],\n",
                                    "        [0.1509, 0.0921, 0.1031, 0.0979, 0.1644, 0.1088, 0.0956, 0.0705, 0.1166],\n",
                                    "        [0.2417, 0.0382, 0.0579, 0.1811, 0.0825, 0.0785, 0.0776, 0.1197, 0.1229],\n",
                                    "        [0.1503, 0.0889, 0.1024, 0.1112, 0.1268, 0.1058, 0.1042, 0.0856, 0.1248]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-1.6594, -2.8096, -2.5658, -1.8512, -2.3562, -2.3585, -2.4330, -2.1804,\n",
                                    "         -2.0755],\n",
                                    "        [-1.9696, -2.3245, -2.2285, -2.1950, -1.9465, -2.2816, -2.3038, -2.4790,\n",
                                    "         -2.1628],\n",
                                    "        [-1.4734, -3.1748, -2.7458, -1.8892, -2.6172, -2.5345, -2.4867, -2.0082,\n",
                                    "         -1.9383],\n",
                                    "        [-1.8960, -2.4236, -2.2793, -2.2413, -1.8168, -2.2112, -2.3768, -2.7043,\n",
                                    "         -2.1148],\n",
                                    "        [-1.4150, -3.0930, -2.7465, -1.8549, -2.3921, -2.4600, -2.4689, -2.1849,\n",
                                    "         -2.1450],\n",
                                    "        [-1.3449, -3.2093, -2.8467, -1.7905, -2.4840, -2.4682, -2.5013, -2.2158,\n",
                                    "         -2.1633],\n",
                                    "        [-1.9331, -2.3620, -2.2483, -2.2080, -1.9178, -2.2381, -2.3650, -2.5431,\n",
                                    "         -2.1251],\n",
                                    "        [-1.4120, -3.1990, -2.8096, -1.8108, -2.5451, -2.5038, -2.4905, -2.1138,\n",
                                    "         -2.0565],\n",
                                    "        [-1.9564, -2.2370, -2.2264, -2.2294, -1.9500, -2.3172, -2.3105, -2.4974,\n",
                                    "         -2.1715],\n",
                                    "        [-1.4150, -3.0930, -2.7465, -1.8549, -2.3921, -2.4600, -2.4689, -2.1849,\n",
                                    "         -2.1450],\n",
                                    "        [-1.4150, -3.0930, -2.7465, -1.8549, -2.3921, -2.4600, -2.4689, -2.1849,\n",
                                    "         -2.1450],\n",
                                    "        [-1.4252, -3.1202, -2.7379, -1.8349, -2.3982, -2.4625, -2.4921, -2.1671,\n",
                                    "         -2.1396],\n",
                                    "        [-1.9639, -2.3644, -2.1782, -2.2249, -2.0096, -2.2477, -2.2655, -2.4525,\n",
                                    "         -2.1642],\n",
                                    "        [-1.8913, -2.3848, -2.2723, -2.3239, -1.8053, -2.2183, -2.3472, -2.6515,\n",
                                    "         -2.1486],\n",
                                    "        [-1.4200, -3.2647, -2.8493, -1.7088, -2.4955, -2.5451, -2.5563, -2.1229,\n",
                                    "         -2.0963],\n",
                                    "        [-1.8953, -2.4207, -2.2791, -2.1964, -2.0651, -2.2459, -2.2613, -2.4578,\n",
                                    "         -2.0810]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.30068725 2.1898954 2.4905825\n",
                                    "score:  0\n",
                                    "score:  1\n",
                                    "score:  0\n",
                                    "score:  0\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_2/videos/alphazero/2/alphazero-episode-9.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_2/videos/alphazero/2/alphazero-episode-9.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                           \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_2/videos/alphazero/2/alphazero-episode-9.mp4\n",
                                    "score:  0\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.49250001 0.055      0.0525     0.06125    0.065      0.0625\n",
                                    " 0.06625    0.0575     0.0875    ]\n",
                                    "Temperature Policy  [0.4925  0.055   0.0525  0.06125 0.065   0.0625  0.06625 0.0575  0.0875 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.285      0.0525     0.32624999 0.06375    0.06       0.0675\n",
                                    " 0.         0.06375    0.08125   ]\n",
                                    "Temperature Policy  [0.285   0.0525  0.32625 0.06375 0.06    0.0675  0.06375 0.08125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.015      0.0225     0.02       0.0225     0.0175\n",
                                    " 0.         0.02375    0.87875003]\n",
                                    "Temperature Policy  [0.015   0.0225  0.02    0.0225  0.0175  0.02375 0.87875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.13       0.12625    0.0825     0.07625    0.11\n",
                                    " 0.         0.47499999 0.        ]\n",
                                    "Temperature Policy  [0.13    0.12625 0.0825  0.07625 0.11    0.475  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.01375    0.92750001 0.0175     0.02125    0.02\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [5.1272888e-19 1.0000000e+00 5.7179629e-18 3.9852004e-17 2.1735057e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.1875     0.         0.12       0.35624999 0.33625001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0436680e-03 1.2032673e-05 6.3987976e-01 3.5906458e-01]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.0175     0.         0.0325     0.94999999 0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.4992947e-18 2.1958243e-15 1.0000000e+00]\n",
                                    "Action  4\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.4675  0.05625 0.055   0.08    0.065   0.0625  0.0675  0.05875 0.0875 ]\n",
                                    "Temperature Policy  [0.4675  0.05625 0.055   0.08    0.065   0.0625  0.0675  0.05875 0.0875 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0675     0.05       0.0575     0.         0.065      0.59500003\n",
                                    " 0.055      0.05375    0.05625   ]\n",
                                    "Temperature Policy  [0.0675  0.05    0.0575  0.065   0.595   0.055   0.05375 0.05625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.19625001 0.025      0.0275     0.         0.02375    0.\n",
                                    " 0.65875    0.03       0.03875   ]\n",
                                    "Temperature Policy  [0.19625 0.025   0.0275  0.02375 0.65875 0.03    0.03875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0425     0.         0.1025     0.         0.0725     0.\n",
                                    " 0.65499997 0.05625    0.07125   ]\n",
                                    "Temperature Policy  [0.0425  0.1025  0.0725  0.655   0.05625 0.07125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02875    0.         0.65499997 0.         0.1275     0.\n",
                                    " 0.         0.15000001 0.03875   ]\n",
                                    "Temperature Policy  [2.6543934e-14 9.9999952e-01 7.8106758e-08 3.9673398e-07 5.2517474e-13]\n",
                                    "Action  2\n",
                                    "Target Policy [0.93374997 0.         0.         0.         0.015      0.\n",
                                    " 0.         0.025      0.02625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.1444753e-18 1.8927530e-16 3.0830951e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.         0.35374999 0.\n",
                                    " 0.         0.51249999 0.13375001]\n",
                                    "Temperature Policy  [2.3960322e-02 9.7603828e-01 1.4304283e-06]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.         0.96499997 0.\n",
                                    " 0.         0.         0.035     ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.9391936e-15]\n",
                                    "Action  4\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.52249998 0.055      0.0525     0.0625     0.05875    0.0625\n",
                                    " 0.06625    0.0575     0.0625    ]\n",
                                    "Temperature Policy  [0.5225  0.055   0.0525  0.0625  0.05875 0.0625  0.06625 0.0575  0.0625 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0725     0.         0.06375    0.0575     0.54374999 0.06375\n",
                                    " 0.0575     0.07       0.07125   ]\n",
                                    "Temperature Policy  [0.0725  0.06375 0.0575  0.54375 0.06375 0.0575  0.07    0.07125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.80624998 0.         0.045      0.0325     0.0275     0.0325\n",
                                    " 0.025      0.         0.03125   ]\n",
                                    "Temperature Policy  [0.80625 0.045   0.0325  0.0275  0.0325  0.025   0.03125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.88875002 0.0225     0.0175     0.02625\n",
                                    " 0.0225     0.         0.0225    ]\n",
                                    "Temperature Policy  [0.88875 0.0225  0.0175  0.02625 0.0225  0.0225 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.035      0.02875    0.03875\n",
                                    " 0.78250003 0.         0.115     ]\n",
                                    "Temperature Policy  [3.2050663e-14 4.4826521e-15 8.8689784e-14 1.0000000e+00 4.7004014e-09]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.      0.      0.9325  0.01875 0.0225  0.      0.      0.02625]\n",
                                    "Temperature Policy  [1.00000000e+00 1.08025015e-17 6.68862412e-17 3.12467384e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.33625001 0.13500001\n",
                                    " 0.         0.         0.52875   ]\n",
                                    "Temperature Policy  [1.0701532e-02 1.1645569e-06 9.8929727e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.      0.      0.      0.      0.96875 0.03125 0.      0.      0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2200653e-15]\n",
                                    "Action  4\n",
                                    "Target Policy [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.465   0.05625 0.055   0.0625  0.05875 0.0875  0.06875 0.05875 0.0875 ]\n",
                                    "Temperature Policy  [0.465   0.05625 0.055   0.0625  0.05875 0.0875  0.06875 0.05875 0.0875 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.04375 0.04625 0.65125 0.05625 0.04875 0.      0.05    0.0425  0.06125]\n",
                                    "Temperature Policy  [0.04375 0.04625 0.65125 0.05625 0.04875 0.05    0.0425  0.06125]\n",
                                    "Action  2\n",
                                    "Target Policy [0.04625    0.0225     0.         0.05125    0.78874999 0.\n",
                                    " 0.02875    0.03       0.0325    ]\n",
                                    "Temperature Policy  [0.04625 0.0225  0.05125 0.78875 0.02875 0.03    0.0325 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.02125    0.015      0.         0.88999999 0.         0.\n",
                                    " 0.02375    0.02375    0.02625   ]\n",
                                    "Temperature Policy  [0.02125 0.015   0.89    0.02375 0.02375 0.02625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.4325     0.40000001 0.         0.         0.         0.\n",
                                    " 0.03       0.11       0.0275    ]\n",
                                    "Temperature Policy  [6.8593335e-01 3.1406584e-01 1.7686152e-12 7.7686872e-07 7.4087974e-13]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.015   0.      0.      0.      0.      0.01625 0.02875 0.94   ]\n",
                                    "Temperature Policy  [1.0706168e-18 2.3837195e-18 7.1630823e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.28       0.         0.         0.         0.\n",
                                    " 0.1275     0.59249997 0.        ]\n",
                                    "Temperature Policy  [5.5520772e-04 2.1280263e-07 9.9944454e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.96875 0.      0.      0.      0.      0.03125 0.      0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2200653e-15]\n",
                                    "Action  1\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.495   0.055   0.0525  0.0625  0.065   0.0575  0.06625 0.05875 0.0875 ]\n",
                                    "Temperature Policy  [0.495   0.055   0.0525  0.0625  0.065   0.0575  0.06625 0.05875 0.0875 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.07125    0.         0.05625    0.0575     0.55500001 0.0625\n",
                                    " 0.05625    0.07       0.07125   ]\n",
                                    "Temperature Policy  [0.07125 0.05625 0.0575  0.555   0.0625  0.05625 0.07    0.07125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.80250001 0.         0.045      0.03       0.02875    0.0325\n",
                                    " 0.03       0.         0.03125   ]\n",
                                    "Temperature Policy  [0.8025  0.045   0.03    0.02875 0.0325  0.03    0.03125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.78500003 0.         0.04125    0.0425     0.04       0.0475\n",
                                    " 0.         0.         0.04375   ]\n",
                                    "Temperature Policy  [0.785   0.04125 0.0425  0.04    0.0475  0.04375]\n",
                                    "Action  2\n",
                                    "Target Policy [0.36625001 0.         0.         0.40375    0.04625    0.12625\n",
                                    " 0.         0.         0.0575    ]\n",
                                    "Temperature Policy  [2.7392358e-01 7.2606993e-01 2.8246877e-10 6.4888386e-06 2.4919053e-09]\n",
                                    "Action  3\n",
                                    "Target Policy [0.93374997 0.         0.         0.         0.02       0.02375\n",
                                    " 0.         0.         0.0225    ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.0323280e-17 1.1332611e-16 6.5996215e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.         0.36875001 0.51999998\n",
                                    " 0.         0.         0.11125   ]\n",
                                    "Temperature Policy  [3.1155923e-02 9.6884382e-01 1.9463482e-07]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.         0.96499997 0.\n",
                                    " 0.         0.         0.035     ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.9391936e-15]\n",
                                    "Action  4\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.0875  0.05625 0.05375 0.0625  0.065   0.0625  0.06625 0.075   0.47125]\n",
                                    "Temperature Policy  [0.0875  0.05625 0.05375 0.0625  0.065   0.0625  0.06625 0.075   0.47125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.055      0.05       0.055      0.05       0.62374997 0.06375\n",
                                    " 0.05125    0.05125    0.        ]\n",
                                    "Temperature Policy  [0.055   0.05    0.055   0.05    0.62375 0.06375 0.05125 0.05125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.03125 0.03625 0.045   0.03125 0.      0.0925  0.1125  0.65125 0.     ]\n",
                                    "Temperature Policy  [0.03125 0.03625 0.045   0.03125 0.0925  0.1125  0.65125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02       0.015      0.02       0.0225     0.         0.0275\n",
                                    " 0.89499998 0.         0.        ]\n",
                                    "Temperature Policy  [0.02   0.015  0.02   0.0225 0.0275 0.895 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0225     0.02       0.91374999 0.0225     0.         0.02125\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.1950444e-17 2.5236323e-17 1.0000000e+00 8.1950444e-17 4.6271703e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02125    0.0175     0.         0.0225     0.         0.93875003\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [3.5325625e-17 5.0685182e-18 6.2564170e-17 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0275     0.01875    0.         0.95375001 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [3.9717454e-16 8.6231751e-18 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0.69125003 0.30875    0.         0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.9968404e-01 3.1592284e-04]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.52125001 0.05625    0.0525     0.0625     0.06375    0.0575\n",
                                    " 0.06625    0.0575     0.0625    ]\n",
                                    "Temperature Policy  [0.52125 0.05625 0.0525  0.0625  0.06375 0.0575  0.06625 0.0575  0.0625 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.04875    0.04625    0.62374997 0.0575     0.05       0.\n",
                                    " 0.06       0.05125    0.0625    ]\n",
                                    "Temperature Policy  [0.04875 0.04625 0.62375 0.0575  0.05    0.06    0.05125 0.0625 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02875    0.0275     0.         0.0525     0.80500001 0.\n",
                                    " 0.02375    0.03       0.0325    ]\n",
                                    "Temperature Policy  [0.02875 0.0275  0.0525  0.805   0.02375 0.03    0.0325 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0325     0.         0.         0.0375     0.82749999 0.\n",
                                    " 0.03375    0.03625    0.0325    ]\n",
                                    "Temperature Policy  [0.0325  0.0375  0.8275  0.03375 0.03625 0.0325 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.02375 0.      0.      0.0225  0.      0.      0.90875 0.02125 0.02375]\n",
                                    "Temperature Policy  [1.4865925e-16 8.6572703e-17 1.0000000e+00 4.8881570e-17 1.4865925e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.30875    0.         0.         0.045      0.         0.\n",
                                    " 0.         0.0425     0.60374999]\n",
                                    "Temperature Policy  [1.2217078e-03 5.2847301e-12 2.9839185e-12 9.9877828e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.95125002 0.         0.         0.02625    0.         0.\n",
                                    " 0.         0.0225     0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.5606272e-16 5.4812359e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.97000003 0.         0.\n",
                                    " 0.         0.03       0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.0074683e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.52499998 0.05375    0.0525     0.0625     0.05875    0.0625\n",
                                    " 0.06625    0.05625    0.0625    ]\n",
                                    "Temperature Policy  [0.525   0.05375 0.0525  0.0625  0.05875 0.0625  0.06625 0.05625 0.0625 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.65625 0.0475  0.045   0.045   0.      0.04625 0.05    0.05    0.06   ]\n",
                                    "Temperature Policy  [0.65625 0.0475  0.045   0.045   0.04625 0.05    0.05    0.06   ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0275     0.0325     0.05625    0.11       0.         0.71125001\n",
                                    " 0.03       0.0325     0.        ]\n",
                                    "Temperature Policy  [0.0275  0.0325  0.05625 0.11    0.71125 0.03    0.0325 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02125    0.02375    0.0225     0.88999999 0.         0.\n",
                                    " 0.01875    0.02375    0.        ]\n",
                                    "Temperature Policy  [0.02125 0.02375 0.0225  0.89    0.01875 0.02375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.055      0.07375    0.0275     0.         0.         0.\n",
                                    " 0.38749999 0.45625001 0.        ]\n",
                                    "Temperature Policy  [5.42154988e-10 1.01886535e-08 5.29448230e-13 1.63385540e-01\n",
                                    " 8.36614430e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02875    0.92250001 0.025      0.         0.         0.\n",
                                    " 0.02375    0.         0.        ]\n",
                                    "Temperature Policy  [8.6440003e-16 1.0000000e+00 2.1366646e-16 1.2793001e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.12875    0.         0.48625001 0.         0.         0.\n",
                                    " 0.38499999 0.         0.        ]\n",
                                    "Temperature Policy  [1.5443158e-06 9.1171664e-01 8.8281766e-02]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03375 0.      0.      0.      0.      0.      0.96625 0.      0.     ]\n",
                                    "Temperature Policy  [2.7029791e-15 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.48750001 0.05625    0.055      0.0625     0.06375    0.0625\n",
                                    " 0.0675     0.0575     0.0875    ]\n",
                                    "Temperature Policy  [0.4875  0.05625 0.055   0.0625  0.06375 0.0625  0.0675  0.0575  0.0875 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.06375    0.04875    0.32499999 0.05875    0.315      0.06625\n",
                                    " 0.         0.0625     0.06      ]\n",
                                    "Temperature Policy  [0.06375 0.04875 0.325   0.05875 0.315   0.06625 0.0625  0.06   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.075      0.0275     0.0325     0.08125    0.         0.03125\n",
                                    " 0.         0.63875002 0.11375   ]\n",
                                    "Temperature Policy  [0.075   0.0275  0.0325  0.08125 0.03125 0.63875 0.11375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0325     0.11875    0.02875    0.025      0.         0.\n",
                                    " 0.         0.74874997 0.04625   ]\n",
                                    "Temperature Policy  [0.0325  0.11875 0.02875 0.025   0.74875 0.04625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01875    0.91250002 0.01875    0.02625    0.         0.\n",
                                    " 0.         0.         0.02375   ]\n",
                                    "Temperature Policy  [1.3417883e-17 1.0000000e+00 1.3417883e-17 3.8811849e-16 1.4266170e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.90125 0.      0.045   0.02875 0.      0.      0.      0.      0.025  ]\n",
                                    "Temperature Policy  [1.0000000e+00 9.6310221e-14 1.0912537e-15 2.6974122e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.02       0.02375    0.         0.\n",
                                    " 0.         0.         0.95625001]\n",
                                    "Temperature Policy  [1.6017158e-17 8.9314436e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.      0.      0.96875 0.03125 0.      0.      0.      0.      0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2200653e-15]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.49625 0.05625 0.0525  0.0625  0.05875 0.0625  0.06625 0.0575  0.0875 ]\n",
                                    "Temperature Policy  [0.49625 0.05625 0.0525  0.0625  0.05875 0.0625  0.06625 0.0575  0.0875 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.045   0.05375 0.05375 0.625   0.055   0.05125 0.0525  0.06375]\n",
                                    "Temperature Policy  [0.045   0.05375 0.05375 0.625   0.055   0.05125 0.0525  0.06375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0325     0.03125    0.76625001 0.         0.03375\n",
                                    " 0.07       0.03125    0.035     ]\n",
                                    "Temperature Policy  [0.0325  0.03125 0.76625 0.03375 0.07    0.03125 0.035  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.015      0.02       0.         0.         0.025\n",
                                    " 0.89375001 0.0225     0.02375   ]\n",
                                    "Temperature Policy  [0.015   0.02    0.025   0.89375 0.0225  0.02375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.015      0.02       0.         0.         0.\n",
                                    " 0.91874999 0.02375    0.0225    ]\n",
                                    "Temperature Policy  [1.3456716e-18 2.3896068e-17 1.0000000e+00 1.3324859e-16 7.7598204e-17]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.46250001 0.05625    0.0525     0.0625     0.06625    0.0875\n",
                                    " 0.0675     0.0575     0.0875    ]\n",
                                    "Temperature Policy  [0.4625  0.05625 0.0525  0.0625  0.06625 0.0875  0.0675  0.0575  0.0875 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.045      0.05375    0.06       0.62124997 0.05375\n",
                                    " 0.05       0.05375    0.0625    ]\n",
                                    "Temperature Policy  [0.045   0.05375 0.06    0.62125 0.05375 0.05    0.05375 0.0625 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0325     0.03125    0.75625002 0.         0.035\n",
                                    " 0.07       0.03125    0.04375   ]\n",
                                    "Temperature Policy  [0.0325  0.03125 0.75625 0.035   0.07    0.03125 0.04375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01375    0.01875    0.         0.         0.02375\n",
                                    " 0.89625001 0.025      0.0225    ]\n",
                                    "Temperature Policy  [0.01375 0.01875 0.02375 0.89625 0.025   0.0225 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.01875    0.91374999 0.         0.         0.0225\n",
                                    " 0.         0.02       0.025     ]\n",
                                    "Temperature Policy  [1.3235455e-17 1.0000000e+00 8.1950444e-17 2.5236323e-17 2.3503158e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.92624998 0.         0.         0.         0.02125\n",
                                    " 0.         0.0275     0.025     ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.0393095e-17 5.3216306e-16 2.0517189e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.0225\n",
                                    " 0.         0.95125002 0.02625   ]\n",
                                    "Temperature Policy  [5.4812359e-17 1.0000000e+00 2.5606272e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.57999998\n",
                                    " 0.         0.         0.41999999]\n",
                                    "Temperature Policy  [0.9618647  0.03813528]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.45249999 0.055      0.0525     0.075      0.06375    0.0875\n",
                                    " 0.06875    0.0575     0.0875    ]\n",
                                    "Temperature Policy  [0.4525  0.055   0.0525  0.075   0.06375 0.0875  0.06875 0.0575  0.0875 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.045   0.05375 0.055   0.625   0.05375 0.05    0.05375 0.06375]\n",
                                    "Temperature Policy  [0.045   0.05375 0.055   0.625   0.05375 0.05    0.05375 0.06375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0325     0.03125    0.76249999 0.         0.0375\n",
                                    " 0.06875    0.03125    0.03625   ]\n",
                                    "Temperature Policy  [0.0325  0.03125 0.7625  0.0375  0.06875 0.03125 0.03625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.01375    0.01875    0.89999998 0.         0.0225\n",
                                    " 0.         0.0225     0.0225    ]\n",
                                    "Temperature Policy  [0.01375 0.01875 0.9     0.0225  0.0225  0.0225 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.015      0.0225     0.         0.         0.92000002\n",
                                    " 0.         0.02       0.0225    ]\n",
                                    "Temperature Policy  [1.3274994e-18 7.6550305e-17 1.0000000e+00 2.3573370e-17 7.6550305e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.49250001 0.04375    0.         0.         0.\n",
                                    " 0.         0.41624999 0.0475    ]\n",
                                    "Temperature Policy  [8.4318036e-01 2.5801129e-11 1.5681969e-01 5.8721104e-11]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.02125    0.         0.         0.\n",
                                    " 0.         0.95125002 0.0275    ]\n",
                                    "Temperature Policy  [3.0948718e-17 1.0000000e+00 4.0773710e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.      0.02625 0.      0.      0.      0.      0.      0.97375]\n",
                                    "Temperature Policy  [2.0268348e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.49000001 0.055      0.0525     0.0625     0.065      0.0625\n",
                                    " 0.06625    0.05875    0.0875    ]\n",
                                    "Temperature Policy  [0.49    0.055   0.0525  0.0625  0.065   0.0625  0.06625 0.05875 0.0875 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.06375    0.05       0.065      0.0575     0.57499999 0.06625\n",
                                    " 0.         0.0625     0.06      ]\n",
                                    "Temperature Policy  [0.06375 0.05    0.065   0.0575  0.575   0.06625 0.0625  0.06   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.07       0.02875    0.03375    0.08125    0.         0.03125\n",
                                    " 0.         0.64249998 0.1125    ]\n",
                                    "Temperature Policy  [0.07    0.02875 0.03375 0.08125 0.03125 0.6425  0.1125 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02125    0.015      0.0225     0.0225     0.         0.02125\n",
                                    " 0.         0.         0.89749998]\n",
                                    "Temperature Policy  [0.02125 0.015   0.0225  0.0225  0.02125 0.8975 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91750002 0.0175     0.02       0.0225     0.         0.0225\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 6.3726453e-18 2.4223629e-17 7.8661905e-17 7.8661905e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.01875    0.02       0.93624997 0.         0.025\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0377541e-17 1.9787077e-17 1.0000000e+00 1.8428152e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02       0.02125    0.         0.         0.95875001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.5604369e-17 2.8611168e-17 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.46250001 0.53750002 0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.182005 0.817995]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.52125001 0.05625    0.0525     0.06125    0.05875    0.0625\n",
                                    " 0.0675     0.0575     0.0625    ]\n",
                                    "Temperature Policy  [0.52125 0.05625 0.0525  0.06125 0.05875 0.0625  0.0675  0.0575  0.0625 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.06625    0.0475     0.05625    0.         0.60750002 0.06625\n",
                                    " 0.04875    0.0525     0.055     ]\n",
                                    "Temperature Policy  [0.06625 0.0475  0.05625 0.6075  0.06625 0.04875 0.0525  0.055  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.76749998 0.0275     0.0275     0.         0.         0.03125\n",
                                    " 0.08125    0.03375    0.03125   ]\n",
                                    "Temperature Policy  [0.7675  0.0275  0.0275  0.03125 0.08125 0.03375 0.03125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.01875    0.02       0.         0.         0.02375\n",
                                    " 0.89375001 0.02125    0.0225    ]\n",
                                    "Temperature Policy  [0.01875 0.02    0.02375 0.89375 0.02125 0.0225 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.01875    0.91374999 0.         0.         0.025\n",
                                    " 0.         0.02       0.0225    ]\n",
                                    "Temperature Policy  [1.3235455e-17 1.0000000e+00 2.3503158e-16 2.5236323e-17 8.1950444e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.92500001 0.         0.         0.         0.0225\n",
                                    " 0.         0.02875    0.02375   ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.2511665e-17 8.4131992e-16 1.2451419e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.02125\n",
                                    " 0.         0.95375001 0.025     ]\n",
                                    "Temperature Policy  [3.0146981e-17 1.0000000e+00 1.5312798e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.53250003\n",
                                    " 0.         0.         0.4675    ]\n",
                                    "Temperature Policy  [0.7861437  0.21385628]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.505   0.055   0.0525  0.06125 0.06375 0.0575  0.0675  0.075   0.0625 ]\n",
                                    "Temperature Policy  [0.505   0.055   0.0525  0.06125 0.06375 0.0575  0.0675  0.075   0.0625 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.04375 0.05375 0.05375 0.6275  0.055   0.05    0.05375 0.0625 ]\n",
                                    "Temperature Policy  [0.04375 0.05375 0.05375 0.6275  0.055   0.05    0.05375 0.0625 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.0275     0.02125    0.04       0.03375    0.02125\n",
                                    " 0.81124997 0.         0.045     ]\n",
                                    "Temperature Policy  [0.0275  0.02125 0.04    0.03375 0.02125 0.81125 0.045  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.1175     0.105      0.45750001 0.11875    0.1\n",
                                    " 0.         0.         0.10125   ]\n",
                                    "Temperature Policy  [0.1175  0.105   0.4575  0.11875 0.1     0.10125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01625    0.02375    0.         0.91500002 0.0225\n",
                                    " 0.         0.         0.0225    ]\n",
                                    "Temperature Policy  [3.1212083e-18 1.3881143e-16 1.0000000e+00 8.0837761e-17 8.0837761e-17]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0825     0.38874999 0.         0.         0.105\n",
                                    " 0.         0.         0.42375001]\n",
                                    "Temperature Policy  [5.5011704e-08 2.9690644e-01 6.1349158e-07 7.0309287e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.0175     0.95125002 0.         0.         0.03125\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.4405198e-18 1.0000000e+00 1.4640419e-15]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.0875     0.055      0.05375    0.075      0.05875    0.0575\n",
                                    " 0.06625    0.0575     0.48875001]\n",
                                    "Temperature Policy  [0.0875  0.055   0.05375 0.075   0.05875 0.0575  0.06625 0.0575  0.48875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.04625    0.05       0.05375    0.045      0.64375001 0.0625\n",
                                    " 0.04875    0.05       0.        ]\n",
                                    "Temperature Policy  [0.04625 0.05    0.05375 0.045   0.64375 0.0625  0.04875 0.05   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.035      0.02875    0.0475     0.035      0.         0.0925\n",
                                    " 0.11375    0.64749998 0.        ]\n",
                                    "Temperature Policy  [0.035   0.02875 0.0475  0.035   0.0925  0.11375 0.6475 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02       0.0175     0.02125    0.02375    0.         0.02375\n",
                                    " 0.89375001 0.         0.        ]\n",
                                    "Temperature Policy  [0.02    0.0175  0.02125 0.02375 0.02375 0.89375]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02125    0.015      0.91750002 0.0225     0.         0.02375\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.4414891e-17 1.3641177e-18 1.0000000e+00 7.8661905e-17 1.3507513e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02    0.02125 0.      0.02375 0.      0.935   0.      0.      0.     ]\n",
                                    "Temperature Policy  [2.0053207e-17 3.6768271e-17 1.1182014e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02375    0.0225     0.         0.95375001 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.1683382e-17 5.3392427e-17 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0.65499997 0.345      0.         0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.9983592  0.00164083]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x31a517760>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32d854460>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32878e8f0>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x32ec6a230>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x107e23d30>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x107e23d30>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x32c6098d0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x328773610>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x32ec6a230>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x32d854460>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x32c60b010>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x325b83520>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x325b831c0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x3286bbf70>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x107dd1cc0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x107dbd180>, 0)]\n",
                                    "Observations [array([[[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]])]\n",
                                    "Policies [array([0.0225    , 0.        , 0.0225    , 0.        , 0.        ,\n",
                                    "       0.        , 0.01625   , 0.0175    , 0.92124999]), array([0.        , 0.155     , 0.07125   , 0.06375   , 0.32499999,\n",
                                    "       0.13124999, 0.095     , 0.07125   , 0.0875    ]), array([0.1375    , 0.        , 0.06125   , 0.        , 0.0875    ,\n",
                                    "       0.09125   , 0.51999998, 0.        , 0.1025    ]), array([0.175     , 0.        , 0.70875001, 0.05125   , 0.02625   ,\n",
                                    "       0.03875   , 0.        , 0.        , 0.        ]), array([0.        , 0.        , 0.        , 0.97000003, 0.        ,\n",
                                    "       0.        , 0.        , 0.03      , 0.        ]), array([0.30875   , 0.        , 0.        , 0.045     , 0.        ,\n",
                                    "       0.        , 0.        , 0.0425    , 0.60374999]), array([0.        , 0.015     , 0.0225    , 0.        , 0.        ,\n",
                                    "       0.92000002, 0.        , 0.02      , 0.0225    ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.0225    , 0.        , 0.95125002, 0.02625   ]), array([0.06375   , 0.        , 0.05375   , 0.05      , 0.065     ,\n",
                                    "       0.0575    , 0.37875   , 0.        , 0.33125001]), array([0.     , 0.78625, 0.04   , 0.0375 , 0.     , 0.     , 0.0425 ,\n",
                                    "       0.0475 , 0.04625]), array([0.        , 0.1175    , 0.105     , 0.45750001, 0.11875   ,\n",
                                    "       0.1       , 0.        , 0.        , 0.10125   ]), array([0.        , 0.06      , 0.0575    , 0.18875   , 0.14      ,\n",
                                    "       0.085     , 0.17      , 0.105     , 0.19374999]), array([0.15375   , 0.        , 0.1525    , 0.13249999, 0.1125    ,\n",
                                    "       0.065     , 0.15000001, 0.125     , 0.10875   ]), array([1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0.01625, 0.     , 0.02375, 0.015  , 0.     , 0.     , 0.0125 ,\n",
                                    "       0.9325 , 0.     ]), array([0.27625, 0.07   , 0.0825 , 0.19   , 0.0825 , 0.0575 , 0.10375,\n",
                                    "       0.06   , 0.0775 ])]\n",
                                    "NP Array Policies [[0.0225     0.         0.0225     0.         0.         0.\n",
                                    "  0.01625    0.0175     0.92124999]\n",
                                    " [0.         0.155      0.07125    0.06375    0.32499999 0.13124999\n",
                                    "  0.095      0.07125    0.0875    ]\n",
                                    " [0.1375     0.         0.06125    0.         0.0875     0.09125\n",
                                    "  0.51999998 0.         0.1025    ]\n",
                                    " [0.175      0.         0.70875001 0.05125    0.02625    0.03875\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.         0.         0.97000003 0.         0.\n",
                                    "  0.         0.03       0.        ]\n",
                                    " [0.30875    0.         0.         0.045      0.         0.\n",
                                    "  0.         0.0425     0.60374999]\n",
                                    " [0.         0.015      0.0225     0.         0.         0.92000002\n",
                                    "  0.         0.02       0.0225    ]\n",
                                    " [0.         0.         0.         0.         0.         0.0225\n",
                                    "  0.         0.95125002 0.02625   ]\n",
                                    " [0.06375    0.         0.05375    0.05       0.065      0.0575\n",
                                    "  0.37875    0.         0.33125001]\n",
                                    " [0.         0.78625    0.04       0.0375     0.         0.\n",
                                    "  0.0425     0.0475     0.04625   ]\n",
                                    " [0.         0.1175     0.105      0.45750001 0.11875    0.1\n",
                                    "  0.         0.         0.10125   ]\n",
                                    " [0.         0.06       0.0575     0.18875    0.14       0.085\n",
                                    "  0.17       0.105      0.19374999]\n",
                                    " [0.15375    0.         0.1525     0.13249999 0.1125     0.065\n",
                                    "  0.15000001 0.125      0.10875   ]\n",
                                    " [1.         0.         0.         0.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.01625    0.         0.02375    0.015      0.         0.\n",
                                    "  0.0125     0.9325     0.        ]\n",
                                    " [0.27625    0.07       0.0825     0.19       0.0825     0.0575\n",
                                    "  0.10375    0.06       0.0775    ]]\n",
                                    "Predicted: tensor([[0.1313, 0.0717, 0.1151, 0.1181, 0.1011, 0.1334, 0.0899, 0.1037, 0.1357],\n",
                                    "        [0.1037, 0.0232, 0.1082, 0.1735, 0.0435, 0.1590, 0.0430, 0.1768, 0.1692],\n",
                                    "        [0.1122, 0.0323, 0.1076, 0.1492, 0.0532, 0.1597, 0.0531, 0.1605, 0.1722],\n",
                                    "        [0.1329, 0.0810, 0.1122, 0.1243, 0.1109, 0.1273, 0.0854, 0.1026, 0.1233],\n",
                                    "        [0.0961, 0.0185, 0.0875, 0.1762, 0.0303, 0.1502, 0.0363, 0.2005, 0.2041],\n",
                                    "        [0.0992, 0.0192, 0.0851, 0.1752, 0.0318, 0.1506, 0.0381, 0.1964, 0.2043],\n",
                                    "        [0.1301, 0.0696, 0.1123, 0.1264, 0.1048, 0.1417, 0.0859, 0.0988, 0.1303],\n",
                                    "        [0.1190, 0.0716, 0.1174, 0.1252, 0.1020, 0.1369, 0.0885, 0.1094, 0.1299],\n",
                                    "        [0.1318, 0.0809, 0.1134, 0.1222, 0.1092, 0.1271, 0.0869, 0.1036, 0.1249],\n",
                                    "        [0.1033, 0.0249, 0.1005, 0.1717, 0.0398, 0.1565, 0.0448, 0.1820, 0.1764],\n",
                                    "        [0.1054, 0.0251, 0.1037, 0.1799, 0.0458, 0.1604, 0.0455, 0.1661, 0.1681],\n",
                                    "        [0.1037, 0.0232, 0.1082, 0.1735, 0.0435, 0.1590, 0.0430, 0.1768, 0.1692],\n",
                                    "        [0.1044, 0.0163, 0.0989, 0.1751, 0.0345, 0.1636, 0.0356, 0.1909, 0.1807],\n",
                                    "        [0.1387, 0.0637, 0.1144, 0.1144, 0.1017, 0.1449, 0.0831, 0.0934, 0.1456],\n",
                                    "        [0.1210, 0.0774, 0.1240, 0.1164, 0.0995, 0.1326, 0.0880, 0.1125, 0.1285],\n",
                                    "        [0.1279, 0.0856, 0.1162, 0.1174, 0.1085, 0.1234, 0.0904, 0.1040, 0.1266]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.1313, 0.0717, 0.1151, 0.1181, 0.1011, 0.1334, 0.0899, 0.1037, 0.1357],\n",
                                    "        [0.1037, 0.0232, 0.1082, 0.1735, 0.0435, 0.1590, 0.0430, 0.1768, 0.1692],\n",
                                    "        [0.1122, 0.0323, 0.1076, 0.1492, 0.0532, 0.1597, 0.0531, 0.1605, 0.1722],\n",
                                    "        [0.1329, 0.0810, 0.1122, 0.1243, 0.1109, 0.1273, 0.0854, 0.1026, 0.1233],\n",
                                    "        [0.0961, 0.0185, 0.0875, 0.1762, 0.0303, 0.1502, 0.0363, 0.2005, 0.2041],\n",
                                    "        [0.0992, 0.0192, 0.0851, 0.1752, 0.0318, 0.1506, 0.0381, 0.1964, 0.2043],\n",
                                    "        [0.1301, 0.0696, 0.1123, 0.1264, 0.1048, 0.1417, 0.0859, 0.0988, 0.1303],\n",
                                    "        [0.1190, 0.0716, 0.1174, 0.1252, 0.1020, 0.1369, 0.0885, 0.1094, 0.1299],\n",
                                    "        [0.1318, 0.0809, 0.1134, 0.1222, 0.1092, 0.1271, 0.0869, 0.1036, 0.1249],\n",
                                    "        [0.1033, 0.0249, 0.1005, 0.1717, 0.0398, 0.1565, 0.0448, 0.1820, 0.1764],\n",
                                    "        [0.1054, 0.0251, 0.1037, 0.1799, 0.0458, 0.1604, 0.0455, 0.1661, 0.1681],\n",
                                    "        [0.1037, 0.0232, 0.1082, 0.1735, 0.0435, 0.1590, 0.0430, 0.1768, 0.1692],\n",
                                    "        [0.1044, 0.0163, 0.0989, 0.1751, 0.0345, 0.1636, 0.0356, 0.1909, 0.1807],\n",
                                    "        [0.1387, 0.0637, 0.1144, 0.1144, 0.1017, 0.1449, 0.0831, 0.0934, 0.1456],\n",
                                    "        [0.1210, 0.0774, 0.1240, 0.1164, 0.0995, 0.1326, 0.0880, 0.1125, 0.1285],\n",
                                    "        [0.1279, 0.0856, 0.1162, 0.1174, 0.1085, 0.1234, 0.0904, 0.1040, 0.1266]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.1313, 0.0717, 0.1151, 0.1181, 0.1011, 0.1334, 0.0899, 0.1037, 0.1357],\n",
                                    "        [0.1037, 0.0232, 0.1082, 0.1735, 0.0435, 0.1590, 0.0430, 0.1768, 0.1692],\n",
                                    "        [0.1122, 0.0323, 0.1076, 0.1492, 0.0532, 0.1597, 0.0531, 0.1605, 0.1722],\n",
                                    "        [0.1329, 0.0810, 0.1122, 0.1243, 0.1109, 0.1273, 0.0854, 0.1026, 0.1233],\n",
                                    "        [0.0961, 0.0185, 0.0875, 0.1762, 0.0303, 0.1502, 0.0363, 0.2005, 0.2041],\n",
                                    "        [0.0992, 0.0192, 0.0851, 0.1752, 0.0318, 0.1506, 0.0381, 0.1964, 0.2043],\n",
                                    "        [0.1301, 0.0696, 0.1123, 0.1264, 0.1048, 0.1417, 0.0859, 0.0988, 0.1303],\n",
                                    "        [0.1190, 0.0716, 0.1174, 0.1252, 0.1020, 0.1369, 0.0885, 0.1094, 0.1299],\n",
                                    "        [0.1318, 0.0809, 0.1134, 0.1222, 0.1092, 0.1271, 0.0869, 0.1036, 0.1249],\n",
                                    "        [0.1033, 0.0249, 0.1005, 0.1717, 0.0398, 0.1565, 0.0448, 0.1820, 0.1764],\n",
                                    "        [0.1054, 0.0251, 0.1037, 0.1799, 0.0458, 0.1604, 0.0455, 0.1661, 0.1681],\n",
                                    "        [0.1037, 0.0232, 0.1082, 0.1735, 0.0435, 0.1590, 0.0430, 0.1768, 0.1692],\n",
                                    "        [0.1044, 0.0163, 0.0989, 0.1751, 0.0345, 0.1636, 0.0356, 0.1909, 0.1807],\n",
                                    "        [0.1387, 0.0637, 0.1144, 0.1144, 0.1017, 0.1449, 0.0831, 0.0934, 0.1456],\n",
                                    "        [0.1210, 0.0774, 0.1240, 0.1164, 0.0995, 0.1326, 0.0880, 0.1125, 0.1285],\n",
                                    "        [0.1279, 0.0856, 0.1162, 0.1174, 0.1085, 0.1234, 0.0904, 0.1040, 0.1266]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.0302, -2.6357, -2.1619, -2.1361, -2.2912, -2.0146, -2.4090, -2.2660,\n",
                                    "         -1.9976],\n",
                                    "        [-2.2662, -3.7640, -2.2237, -1.7517, -3.1352, -1.8390, -3.1457, -1.7330,\n",
                                    "         -1.7769],\n",
                                    "        [-2.1876, -3.4326, -2.2298, -1.9026, -2.9341, -1.8342, -2.9359, -1.8292,\n",
                                    "         -1.7589],\n",
                                    "        [-2.0184, -2.5127, -2.1877, -2.0848, -2.1989, -2.0610, -2.4599, -2.2772,\n",
                                    "         -2.0932],\n",
                                    "        [-2.3420, -3.9885, -2.4360, -1.7359, -3.4950, -1.8955, -3.3155, -1.6068,\n",
                                    "         -1.5890],\n",
                                    "        [-2.3103, -3.9535, -2.4640, -1.7416, -3.4469, -1.8934, -3.2665, -1.6277,\n",
                                    "         -1.5881],\n",
                                    "        [-2.0394, -2.6650, -2.1864, -2.0682, -2.2559, -1.9542, -2.4541, -2.3144,\n",
                                    "         -2.0376],\n",
                                    "        [-2.1283, -2.6368, -2.1418, -2.0778, -2.2825, -1.9886, -2.4251, -2.2128,\n",
                                    "         -2.0407],\n",
                                    "        [-2.0264, -2.5143, -2.1768, -2.1025, -2.2150, -2.0626, -2.4434, -2.2669,\n",
                                    "         -2.0799],\n",
                                    "        [-2.2702, -3.6925, -2.2974, -1.7621, -3.2239, -1.8545, -3.1051, -1.7035,\n",
                                    "         -1.7349],\n",
                                    "        [-2.2504, -3.6836, -2.2665, -1.7153, -3.0825, -1.8302, -3.0898, -1.7953,\n",
                                    "         -1.7831],\n",
                                    "        [-2.2662, -3.7640, -2.2237, -1.7517, -3.1352, -1.8390, -3.1457, -1.7330,\n",
                                    "         -1.7769],\n",
                                    "        [-2.2597, -4.1162, -2.3139, -1.7422, -3.3662, -1.8102, -3.3348, -1.6563,\n",
                                    "         -1.7109],\n",
                                    "        [-1.9755, -2.7530, -2.1677, -2.1680, -2.2853, -1.9317, -2.4871, -2.3712,\n",
                                    "         -1.9272],\n",
                                    "        [-2.1121, -2.5585, -2.0877, -2.1507, -2.3076, -2.0201, -2.4300, -2.1847,\n",
                                    "         -2.0515],\n",
                                    "        [-2.0563, -2.4581, -2.1528, -2.1422, -2.2207, -2.0923, -2.4031, -2.2635,\n",
                                    "         -2.0671]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.75534 2.2448592 3.0001988\n",
                                    "score:  0\n",
                                    "score:  1\n",
                                    "score:  0\n",
                                    "score:  1\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_3/videos/alphazero/3/alphazero-episode-14.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_3/videos/alphazero/3/alphazero-episode-14.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                          \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_3/videos/alphazero/3/alphazero-episode-14.mp4\n",
                                    "score:  1\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.08375 0.04375 0.05875 0.05625 0.5025  0.065   0.0675  0.06125 0.06125]\n",
                                    "Temperature Policy  [0.08375 0.04375 0.05875 0.05625 0.5025  0.065   0.0675  0.06125 0.06125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.04125    0.03625    0.0375     0.0375     0.73874998 0.03625\n",
                                    " 0.         0.0325     0.04      ]\n",
                                    "Temperature Policy  [0.04125 0.03625 0.0375  0.0375  0.73875 0.03625 0.0325  0.04   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.07625    0.02875    0.0275     0.14749999 0.         0.0375\n",
                                    " 0.         0.58249998 0.1       ]\n",
                                    "Temperature Policy  [0.07625 0.02875 0.0275  0.1475  0.0375  0.5825  0.1    ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01625 0.01375 0.01875 0.025   0.      0.02    0.      0.      0.90625]\n",
                                    "Temperature Policy  [0.01625 0.01375 0.01875 0.025   0.02    0.90625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91374999 0.0175     0.025      0.02125    0.         0.0225\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 6.6390604e-18 2.3503158e-16 4.6271703e-17 8.1950444e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.01375    0.02125    0.94125003 0.         0.02375\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.4256500e-19 3.4398494e-17 1.0000000e+00 1.0461314e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01625    0.0275     0.         0.         0.95625001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [2.0082566e-18 3.8691224e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.40000001 0.60000002 0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.01704593 0.9829541 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.52375001 0.04375    0.05875    0.0575     0.06625    0.06\n",
                                    " 0.06375    0.04875    0.0775    ]\n",
                                    "Temperature Policy  [0.52375 0.04375 0.05875 0.0575  0.06625 0.06    0.06375 0.04875 0.0775 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0375     0.03875    0.04125    0.71749997 0.045\n",
                                    " 0.03625    0.03625    0.0475    ]\n",
                                    "Temperature Policy  [0.0375  0.03875 0.04125 0.7175  0.045   0.03625 0.03625 0.0475 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0425     0.10625    0.64625001 0.         0.04375\n",
                                    " 0.085      0.03125    0.045     ]\n",
                                    "Temperature Policy  [0.0425  0.10625 0.64625 0.04375 0.085   0.03125 0.045  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.015   0.02125 0.90375 0.      0.02    0.      0.02    0.02   ]\n",
                                    "Temperature Policy  [0.015   0.02125 0.90375 0.02    0.02    0.02   ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01875    0.02       0.         0.         0.91624999\n",
                                    " 0.         0.02       0.025     ]\n",
                                    "Temperature Policy  [1.2878724e-17 2.4556139e-17 1.0000000e+00 2.4556139e-17 2.2869686e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.51499999 0.07125    0.         0.         0.\n",
                                    " 0.         0.36375001 0.05      ]\n",
                                    "Temperature Policy  [9.7002620e-01 2.4920681e-09 2.9973758e-02 7.2179061e-11]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.02625    0.         0.         0.\n",
                                    " 0.         0.94999999 0.02375   ]\n",
                                    "Temperature Policy  [2.594520e-16 1.000000e+00 9.536743e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.025      0.         0.         0.\n",
                                    " 0.         0.         0.97500002]\n",
                                    "Temperature Policy  [1.22844e-16 1.00000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.08375 0.045   0.04875 0.05125 0.52875 0.0575  0.0625  0.06    0.0625 ]\n",
                                    "Temperature Policy  [0.08375 0.045   0.04875 0.05125 0.52875 0.0575  0.0625  0.06    0.0625 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.05       0.         0.03875    0.04625    0.70625001 0.03875\n",
                                    " 0.03875    0.03875    0.0425    ]\n",
                                    "Temperature Policy  [0.05    0.03875 0.04625 0.70625 0.03875 0.03875 0.03875 0.0425 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.79374999 0.         0.04125    0.0325     0.         0.03375\n",
                                    " 0.02875    0.0325     0.0375    ]\n",
                                    "Temperature Policy  [0.79375 0.04125 0.0325  0.03375 0.02875 0.0325  0.0375 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.89875001 0.025      0.         0.02125\n",
                                    " 0.01625    0.01875    0.02      ]\n",
                                    "Temperature Policy  [0.89875 0.025   0.02125 0.01625 0.01875 0.02   ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.0225     0.         0.02375\n",
                                    " 0.91250002 0.0175     0.02375   ]\n",
                                    "Temperature Policy  [8.307999e-17 1.426617e-16 1.000000e+00 6.730569e-18 1.426617e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.      0.      0.9325  0.      0.02125 0.      0.02375 0.0225 ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.7765996e-17 1.1485443e-16 6.6886241e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.95625001\n",
                                    " 0.         0.01875    0.025     ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.4003674e-18 1.4917142e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.         0.47624999 0.52375001]\n",
                                    "Temperature Policy  [0.27874097 0.721259  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.5675  0.04375 0.04875 0.05125 0.05375 0.0575  0.0575  0.06    0.06   ]\n",
                                    "Temperature Policy  [0.5675  0.04375 0.04875 0.05125 0.05375 0.0575  0.0575  0.06    0.06   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.03625 0.03625 0.04125 0.72375 0.04    0.0375  0.04125 0.04375]\n",
                                    "Temperature Policy  [0.03625 0.03625 0.04125 0.72375 0.04    0.0375  0.04125 0.04375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.33000001 0.0475     0.43625    0.         0.045\n",
                                    " 0.0775     0.02875    0.035     ]\n",
                                    "Temperature Policy  [0.33    0.0475  0.43625 0.045   0.0775  0.02875 0.035  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.01375    0.01875    0.90249997 0.         0.025\n",
                                    " 0.         0.02       0.02      ]\n",
                                    "Temperature Policy  [0.01375 0.01875 0.9025  0.025   0.02    0.02   ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01375    0.02125    0.         0.         0.91750002\n",
                                    " 0.         0.0225     0.025     ]\n",
                                    "Temperature Policy  [5.7143420e-19 4.4414891e-17 1.0000000e+00 7.8661905e-17 2.2560013e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.53250003 0.05       0.         0.         0.\n",
                                    " 0.         0.36000001 0.0575    ]\n",
                                    "Temperature Policy  [9.8044527e-01 5.2230872e-11 1.9554751e-02 2.1130302e-10]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0225     0.         0.         0.\n",
                                    " 0.         0.95375001 0.02375   ]\n",
                                    "Temperature Policy  [5.3392427e-17 1.0000000e+00 9.1683382e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.      0.02875 0.      0.      0.      0.      0.      0.97125]\n",
                                    "Temperature Policy  [5.164974e-16 1.000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.0775  0.0425  0.05875 0.05125 0.53625 0.0575  0.0575  0.05875 0.06   ]\n",
                                    "Temperature Policy  [0.0775  0.0425  0.05875 0.05125 0.53625 0.0575  0.0575  0.05875 0.06   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.69999999 0.0375     0.0425     0.045      0.         0.0525\n",
                                    " 0.04       0.03875    0.04375   ]\n",
                                    "Temperature Policy  [0.7     0.0375  0.0425  0.045   0.0525  0.04    0.03875 0.04375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0275     0.03875    0.77125001 0.         0.07\n",
                                    " 0.0325     0.03125    0.02875   ]\n",
                                    "Temperature Policy  [0.0275  0.03875 0.77125 0.07    0.0325  0.03125 0.02875]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02125    0.02       0.         0.         0.88999999\n",
                                    " 0.02125    0.02375    0.02375   ]\n",
                                    "Temperature Policy  [0.02125 0.02    0.89    0.02125 0.02375 0.02375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.375      0.41624999 0.         0.         0.\n",
                                    " 0.02375    0.12375    0.06125   ]\n",
                                    "Temperature Policy  [2.6045492e-01 7.3954111e-01 2.7043070e-13 3.9890729e-06 3.5194985e-09]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.0175     0.         0.         0.         0.\n",
                                    " 0.93000001 0.0275     0.025     ]\n",
                                    "Temperature Policy  [5.5661000e-18 1.0000000e+00 5.1109005e-16 1.9704734e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.34       0.         0.         0.         0.\n",
                                    " 0.         0.52375001 0.13625   ]\n",
                                    "Temperature Policy  [1.3116384e-02 9.8688215e-01 1.4008449e-06]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.96875 0.      0.      0.      0.      0.      0.      0.03125]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2200653e-15]\n",
                                    "Action  1\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.0775     0.0425     0.06625    0.05625    0.52125001 0.06\n",
                                    " 0.06625    0.04875    0.06125   ]\n",
                                    "Temperature Policy  [0.0775  0.0425  0.06625 0.05625 0.52125 0.06    0.06625 0.04875 0.06125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.70375001 0.04       0.04375    0.045      0.         0.04375\n",
                                    " 0.04       0.04       0.04375   ]\n",
                                    "Temperature Policy  [0.70375 0.04    0.04375 0.045   0.04375 0.04    0.04    0.04375]\n",
                                    "Action  8\n",
                                    "Target Policy [0.025      0.085      0.04       0.0875     0.         0.70249999\n",
                                    " 0.03125    0.02875    0.        ]\n",
                                    "Temperature Policy  [0.025   0.085   0.04    0.0875  0.7025  0.03125 0.02875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02       0.0175     0.02375    0.89499998 0.         0.\n",
                                    " 0.02125    0.0225     0.        ]\n",
                                    "Temperature Policy  [0.02    0.0175  0.02375 0.895   0.02125 0.0225 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.05625    0.08375    0.0325     0.         0.         0.\n",
                                    " 0.39375001 0.43375    0.        ]\n",
                                    "Temperature Policy  [9.7486341e-10 5.2187524e-08 4.0415891e-12 2.7537480e-01 7.2462517e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02625 0.01625 0.9325  0.      0.      0.      0.      0.025   0.     ]\n",
                                    "Temperature Policy  [3.1246738e-16 2.5825294e-18 1.0000000e+00 1.9182787e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.12       0.33375001 0.         0.         0.         0.\n",
                                    " 0.         0.54624999 0.        ]\n",
                                    "Temperature Policy  [2.5987282e-07 7.1971598e-03 9.9280256e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0325     0.96749997 0.         0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.8294641e-15 1.0000000e+00]\n",
                                    "Action  1\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.52999997 0.04375    0.05875    0.05625    0.06625    0.0575\n",
                                    " 0.06625    0.06       0.06125   ]\n",
                                    "Temperature Policy  [0.53    0.04375 0.05875 0.05625 0.06625 0.0575  0.06625 0.06    0.06125]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03375    0.03375    0.         0.04125    0.72874999 0.05125\n",
                                    " 0.03625    0.03375    0.04125   ]\n",
                                    "Temperature Policy  [0.03375 0.03375 0.04125 0.72875 0.05125 0.03625 0.03375 0.04125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.01625    0.0125     0.         0.02       0.015      0.0175\n",
                                    " 0.         0.01625    0.90249997]\n",
                                    "Temperature Policy  [0.01625 0.0125  0.02    0.015   0.0175  0.01625 0.9025 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.12       0.07625    0.         0.11125    0.07875    0.53500003\n",
                                    " 0.         0.07875    0.        ]\n",
                                    "Temperature Policy  [0.12    0.07625 0.11125 0.07875 0.535   0.07875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.93000001 0.01375    0.         0.02       0.0175     0.\n",
                                    " 0.         0.01875    0.        ]\n",
                                    "Temperature Policy  [1.00000000e+00 4.99111372e-19 2.11577978e-17 5.56610000e-18\n",
                                    " 1.10964286e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.27000001 0.         0.22       0.41874999 0.\n",
                                    " 0.         0.09125    0.        ]\n",
                                    "Temperature Policy  [1.2247356e-02 1.5799068e-03 9.8617250e-01 2.3808576e-07]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.95125002 0.         0.0325     0.         0.\n",
                                    " 0.         0.01625    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.1671397e-15 2.1163474e-18]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.52499998 0.045      0.05875    0.05875    0.06625    0.05875\n",
                                    " 0.06625    0.05875    0.0625    ]\n",
                                    "Temperature Policy  [0.525   0.045   0.05875 0.05875 0.06625 0.05875 0.06625 0.05875 0.0625 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0375     0.03625    0.0425     0.0425     0.71125001 0.04625\n",
                                    " 0.03875    0.         0.045     ]\n",
                                    "Temperature Policy  [0.0375  0.03625 0.0425  0.0425  0.71125 0.04625 0.03875 0.045  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.03       0.025      0.02625    0.03375    0.         0.0375\n",
                                    " 0.60500002 0.         0.24250001]\n",
                                    "Temperature Policy  [0.03    0.025   0.02625 0.03375 0.0375  0.605   0.2425 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.01875    0.01375    0.02125    0.025      0.         0.0225\n",
                                    " 0.89875001 0.         0.        ]\n",
                                    "Temperature Policy  [0.01875 0.01375 0.02125 0.025   0.0225  0.89875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02375    0.015      0.91500002 0.0225     0.         0.02375\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.3881143e-16 1.4018504e-18 1.0000000e+00 8.0837761e-17 1.3881143e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0175     0.015      0.         0.02875    0.         0.93875003\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [5.0685182e-18 1.0849584e-18 7.2590369e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02125    0.01625    0.         0.96249998 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [2.7515789e-17 1.8815956e-18 1.0000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0.51125002 0.48875001 0.         0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.6106573 0.3893427]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.06625    0.0425     0.05875    0.0575     0.55250001 0.0575\n",
                                    " 0.0575     0.0475     0.06      ]\n",
                                    "Temperature Policy  [0.06625 0.0425  0.05875 0.0575  0.5525  0.0575  0.0575  0.0475  0.06   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.69125003 0.04125    0.04       0.04625    0.         0.05375\n",
                                    " 0.04       0.04125    0.04625   ]\n",
                                    "Temperature Policy  [0.69125 0.04125 0.04    0.04625 0.05375 0.04    0.04125 0.04625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0275     0.0375     0.77375001 0.         0.07\n",
                                    " 0.03       0.02625    0.035     ]\n",
                                    "Temperature Policy  [0.0275  0.0375  0.77375 0.07    0.03    0.02625 0.035  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01625    0.02625    0.         0.         0.89125001\n",
                                    " 0.01875    0.02375    0.02375   ]\n",
                                    "Temperature Policy  [0.01625 0.02625 0.89125 0.01875 0.02375 0.02375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.37375    0.41874999 0.         0.         0.\n",
                                    " 0.0225     0.12375    0.06125   ]\n",
                                    "Temperature Policy  [2.4289463e-01 7.5710154e-01 1.5185571e-13 3.8464318e-06 3.3936485e-09]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.02125    0.         0.         0.\n",
                                    " 0.02       0.93374997 0.025     ]\n",
                                    "Temperature Policy  [3.726346e-17 2.032328e-17 1.000000e+00 1.892753e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.47375    0.         0.         0.\n",
                                    " 0.39875001 0.         0.1275    ]\n",
                                    "Temperature Policy  [8.4857142e-01 1.5142694e-01 1.6916014e-06]\n",
                                    "Action  2\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.      0.96625 0.      0.03375]\n",
                                    "Temperature Policy  [1.0000000e+00 2.7029791e-15]\n",
                                    "Action  6\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.08375 0.045   0.06    0.0575  0.5     0.05875 0.0675  0.04875 0.07875]\n",
                                    "Temperature Policy  [0.08375 0.045   0.06    0.0575  0.5     0.05875 0.0675  0.04875 0.07875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03875 0.0325  0.      0.04375 0.71375 0.05125 0.03875 0.035   0.04625]\n",
                                    "Temperature Policy  [0.03875 0.0325  0.04375 0.71375 0.05125 0.03875 0.035   0.04625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.04625    0.035      0.         0.045      0.         0.75875002\n",
                                    " 0.0275     0.02875    0.05875   ]\n",
                                    "Temperature Policy  [0.04625 0.035   0.045   0.75875 0.0275  0.02875 0.05875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01875    0.01875    0.         0.02125    0.         0.\n",
                                    " 0.01375    0.02       0.90750003]\n",
                                    "Temperature Policy  [0.01875 0.01875 0.02125 0.01375 0.02    0.9075 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91624999 0.02125    0.         0.0225     0.         0.\n",
                                    " 0.01875    0.02125    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.5024559e-17 7.9741667e-17 1.2878724e-17 4.5024559e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.92874998 0.         0.0275     0.         0.\n",
                                    " 0.0175     0.02625    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 5.1801062e-16 5.6414698e-18 3.2531555e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.02625    0.         0.\n",
                                    " 0.02375    0.94999999 0.        ]\n",
                                    "Temperature Policy  [2.594520e-16 9.536743e-17 1.000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.61874998 0.         0.\n",
                                    " 0.38124999 0.         0.        ]\n",
                                    "Temperature Policy  [0.9921741 0.0078259]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.0775     0.04625    0.04875    0.0575     0.53500003 0.0575\n",
                                    " 0.06625    0.04875    0.0625    ]\n",
                                    "Temperature Policy  [0.0775  0.04625 0.04875 0.0575  0.535   0.0575  0.06625 0.04875 0.0625 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.70125002 0.045      0.04125    0.045      0.         0.04375\n",
                                    " 0.04       0.03875    0.045     ]\n",
                                    "Temperature Policy  [0.70125 0.045   0.04125 0.045   0.04375 0.04    0.03875 0.045  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.06       0.03875    0.74624997 0.         0.07\n",
                                    " 0.03       0.02625    0.02875   ]\n",
                                    "Temperature Policy  [0.06    0.03875 0.74625 0.07    0.03    0.02625 0.02875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0225     0.02625    0.         0.0225\n",
                                    " 0.02       0.88625002 0.0225    ]\n",
                                    "Temperature Policy  [0.0225  0.02625 0.0225  0.02    0.88625 0.0225 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.035      0.39750001 0.         0.09125\n",
                                    " 0.4075     0.         0.06875   ]\n",
                                    "Temperature Policy  [1.2273940e-11 4.3820250e-01 1.7808702e-07 5.6179732e-01 1.0496162e-08]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.92624998 0.02625    0.         0.0225\n",
                                    " 0.         0.         0.025     ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.3420341e-16 7.1539020e-17 2.0517189e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.43000001 0.         0.43625\n",
                                    " 0.         0.         0.13375001]\n",
                                    "Temperature Policy  [4.6398497e-01 5.3601104e-01 3.9333017e-06]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.97500002 0.         0.\n",
                                    " 0.         0.         0.025     ]\n",
                                    "Temperature Policy  [1.00000e+00 1.22844e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.08375 0.0425  0.05875 0.05125 0.06625 0.05875 0.52875 0.04875 0.06125]\n",
                                    "Temperature Policy  [0.08375 0.0425  0.05875 0.05125 0.06625 0.05875 0.52875 0.04875 0.06125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.05125    0.03375    0.04125    0.         0.70625001 0.04375\n",
                                    " 0.04125    0.04125    0.04125   ]\n",
                                    "Temperature Policy  [0.05125 0.03375 0.04125 0.70625 0.04375 0.04125 0.04125 0.04125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.69125003 0.02375    0.04125    0.         0.         0.0325\n",
                                    " 0.13500001 0.0325     0.04375   ]\n",
                                    "Temperature Policy  [0.69125 0.02375 0.04125 0.0325  0.135   0.0325  0.04375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.015      0.0225     0.         0.         0.0225\n",
                                    " 0.89749998 0.01875    0.02375   ]\n",
                                    "Temperature Policy  [0.015   0.0225  0.0225  0.8975  0.01875 0.02375]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.02       0.91374999 0.         0.         0.02125\n",
                                    " 0.         0.0225     0.0225    ]\n",
                                    "Temperature Policy  [2.5236323e-17 1.0000000e+00 4.6271703e-17 8.1950444e-17 8.1950444e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.92624998 0.         0.         0.         0.02375\n",
                                    " 0.         0.02375    0.02625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2284400e-16 1.2284400e-16 3.3420341e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.025\n",
                                    " 0.         0.95249999 0.0225    ]\n",
                                    "Temperature Policy  [1.5514945e-16 1.0000000e+00 5.4097268e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.57875001\n",
                                    " 0.         0.         0.42124999]\n",
                                    "Temperature Policy  [0.959938   0.04006197]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.06625    0.0425     0.04875    0.05625    0.54124999 0.06\n",
                                    " 0.06375    0.06       0.06125   ]\n",
                                    "Temperature Policy  [0.06625 0.0425  0.04875 0.05625 0.54125 0.06    0.06375 0.06    0.06125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.05125    0.04125    0.04125    0.         0.70499998 0.04375\n",
                                    " 0.0375     0.03875    0.04125   ]\n",
                                    "Temperature Policy  [0.05125 0.04125 0.04125 0.705   0.04375 0.0375  0.03875 0.04125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.78750002 0.02375    0.03125    0.         0.         0.04625\n",
                                    " 0.04375    0.03375    0.03375   ]\n",
                                    "Temperature Policy  [0.7875  0.02375 0.03125 0.04625 0.04375 0.03375 0.03375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.015      0.0175     0.         0.         0.02125\n",
                                    " 0.90499997 0.01875    0.0225    ]\n",
                                    "Temperature Policy  [0.015   0.0175  0.02125 0.905   0.01875 0.0225 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.01875    0.91624999 0.         0.         0.0225\n",
                                    " 0.         0.02125    0.02125   ]\n",
                                    "Temperature Policy  [1.2878724e-17 1.0000000e+00 7.9741667e-17 4.5024559e-17 4.5024559e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.92374998 0.         0.         0.         0.02375\n",
                                    " 0.         0.02375    0.02875   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2620938e-16 1.2620938e-16 8.5277408e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.025\n",
                                    " 0.         0.94875002 0.02625   ]\n",
                                    "Temperature Policy  [1.6139207e-16 1.0000000e+00 2.6289068e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.43125001\n",
                                    " 0.         0.         0.56875002]\n",
                                    "Temperature Policy  [0.05910422 0.9408958 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.08375    0.04375    0.04875    0.0575     0.52499998 0.0575\n",
                                    " 0.06375    0.06       0.06      ]\n",
                                    "Temperature Policy  [0.08375 0.04375 0.04875 0.0575  0.525   0.0575  0.06375 0.06    0.06   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.70375001 0.03875    0.0425     0.0475     0.         0.04375\n",
                                    " 0.03875    0.04125    0.04375   ]\n",
                                    "Temperature Policy  [0.70375 0.03875 0.0425  0.0475  0.04375 0.03875 0.04125 0.04375]\n",
                                    "Action  8\n",
                                    "Target Policy [0.03375    0.08375    0.03625    0.0875     0.         0.69875002\n",
                                    " 0.03125    0.02875    0.        ]\n",
                                    "Temperature Policy  [0.03375 0.08375 0.03625 0.0875  0.69875 0.03125 0.02875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.01875    0.015      0.         0.02375    0.         0.02375\n",
                                    " 0.89749998 0.02125    0.        ]\n",
                                    "Temperature Policy  [0.01875 0.015   0.02375 0.02375 0.8975  0.02125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0175     0.0175     0.         0.02375    0.         0.02\n",
                                    " 0.         0.92124999 0.        ]\n",
                                    "Temperature Policy  [6.1179440e-18 6.1179440e-18 1.2967646e-16 2.3255460e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.025      0.92124999 0.         0.02625    0.         0.0275\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [2.1658335e-16 1.0000000e+00 3.5279145e-16 5.6176141e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.14624999 0.         0.         0.44499999 0.         0.40875\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0298302e-05 7.0049816e-01 2.9949152e-01]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02375    0.         0.         0.         0.         0.97624999\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [7.261489e-17 1.000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.06625    0.04375    0.06       0.05125    0.54624999 0.0575\n",
                                    " 0.065      0.04875    0.06125   ]\n",
                                    "Temperature Policy  [0.06625 0.04375 0.06    0.05125 0.54625 0.0575  0.065   0.04875 0.06125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.70375001 0.03875    0.04       0.04625    0.         0.04375\n",
                                    " 0.0425     0.03875    0.04625   ]\n",
                                    "Temperature Policy  [0.70375 0.03875 0.04    0.04625 0.04375 0.0425  0.03875 0.04625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.03125    0.0375     0.77249998 0.         0.06875\n",
                                    " 0.02375    0.0325     0.03375   ]\n",
                                    "Temperature Policy  [0.03125 0.0375  0.7725  0.06875 0.02375 0.0325  0.03375]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.89749998 0.0175     0.0225     0.         0.0225\n",
                                    " 0.015      0.         0.025     ]\n",
                                    "Temperature Policy  [0.8975 0.0175 0.0225 0.0225 0.015  0.025 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.91624999 0.02375    0.         0.02\n",
                                    " 0.01375    0.         0.02625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.3692926e-16 2.4556139e-17 5.7927805e-19 3.7252308e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.0275     0.         0.025\n",
                                    " 0.92374998 0.         0.02375   ]\n",
                                    "Temperature Policy  [5.4674198e-16 2.1079271e-16 1.0000000e+00 1.2620938e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.94875002 0.         0.02625\n",
                                    " 0.         0.         0.025     ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.6289068e-16 1.6139207e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.97125 0.      0.      0.02875]\n",
                                    "Temperature Policy  [1.000000e+00 5.164974e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.0775     0.0425     0.0475     0.05125    0.55000001 0.05625\n",
                                    " 0.06375    0.04875    0.0625    ]\n",
                                    "Temperature Policy  [0.0775  0.0425  0.0475  0.05125 0.55    0.05625 0.06375 0.04875 0.0625 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.70375001 0.03875    0.03875    0.04875    0.         0.04375\n",
                                    " 0.04125    0.04125    0.04375   ]\n",
                                    "Temperature Policy  [0.70375 0.03875 0.03875 0.04875 0.04375 0.04125 0.04125 0.04375]\n",
                                    "Action  6\n",
                                    "Target Policy [0.06125    0.025      0.02625    0.69749999 0.         0.105\n",
                                    " 0.         0.0325     0.0525    ]\n",
                                    "Temperature Policy  [0.06125 0.025   0.02625 0.6975  0.105   0.0325  0.0525 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02125    0.01625    0.02125    0.         0.         0.89125001\n",
                                    " 0.         0.0225     0.0275    ]\n",
                                    "Temperature Policy  [0.02125 0.01625 0.02125 0.89125 0.0225  0.0275 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02625    0.08875    0.04875    0.         0.         0.\n",
                                    " 0.         0.42750001 0.40875   ]\n",
                                    "Temperature Policy  [4.6501160e-13 9.0751605e-08 2.2694305e-10 6.1028385e-01 3.8971609e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.93124998 0.02375    0.02125    0.         0.         0.\n",
                                    " 0.         0.02375    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.1640545e-16 3.8275996e-17 1.1640545e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.33875    0.11625    0.         0.         0.\n",
                                    " 0.         0.54500002 0.        ]\n",
                                    "Temperature Policy  [8.5330866e-03 1.9330484e-07 9.9146670e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.96499997 0.035      0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.9391936e-15]\n",
                                    "Action  1\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x107e214b0>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x32c6504c0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3286bbf70>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32d854fa0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x325b831c0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32d8f6200>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x3286f44f0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x32878c850>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x31a5148b0>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x325b831c0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x107e214b0>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x3286f7fd0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x3286f7fd0>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x3286f5600>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x325b83fd0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32c60b010>, 0)]\n",
                                    "Observations [array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]])]\n",
                                    "Policies [array([0.03125   , 0.02      , 0.        , 0.02375   , 0.        ,\n",
                                    "       0.92500001, 0.        , 0.        , 0.        ]), array([0.90499997, 0.        , 0.01375   , 0.        , 0.01625   ,\n",
                                    "       0.01375   , 0.0175    , 0.01375   , 0.02      ]), array([0.045  , 0.6225 , 0.0475 , 0.0475 , 0.05375, 0.045  , 0.04875,\n",
                                    "       0.04625, 0.04375]), array([0.        , 0.        , 0.04625   , 0.045     , 0.75749999,\n",
                                    "       0.0475    , 0.        , 0.        , 0.10375   ]), array([0.15375   , 0.        , 0.1525    , 0.13249999, 0.1125    ,\n",
                                    "       0.065     , 0.15000001, 0.125     , 0.10875   ]), array([0., 1., 0., 0., 0., 0., 0., 0., 0.]), array([0.01625   , 0.        , 0.0375    , 0.        , 0.87875003,\n",
                                    "       0.0175    , 0.015     , 0.01625   , 0.01875   ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.95625001, 0.        , 0.01875   , 0.025     ]), array([0.04625, 0.65125, 0.04375, 0.04625, 0.05   , 0.03625, 0.04125,\n",
                                    "       0.0375 , 0.0475 ]), array([0.02625, 0.     , 0.     , 0.     , 0.     , 0.02   , 0.90875,\n",
                                    "       0.02375, 0.02125]), array([0.025     , 0.0275    , 0.        , 0.94749999, 0.        ,\n",
                                    "       0.        , 0.        , 0.        , 0.        ]), array([0.     , 0.1825 , 0.0625 , 0.18875, 0.13625, 0.12   , 0.175  ,\n",
                                    "       0.0775 , 0.0575 ]), array([0.     , 0.1825 , 0.0625 , 0.18875, 0.13625, 0.12   , 0.175  ,\n",
                                    "       0.0775 , 0.0575 ]), array([0.        , 0.02125   , 0.        , 0.        , 0.        ,\n",
                                    "       0.92750001, 0.        , 0.02375   , 0.0275    ]), array([0.     , 0.90625, 0.02375, 0.025  , 0.     , 0.     , 0.02375,\n",
                                    "       0.     , 0.02125]), array([0.505  , 0.055  , 0.0525 , 0.06125, 0.06375, 0.0575 , 0.0675 ,\n",
                                    "       0.075  , 0.0625 ])]\n",
                                    "NP Array Policies [[0.03125    0.02       0.         0.02375    0.         0.92500001\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.90499997 0.         0.01375    0.         0.01625    0.01375\n",
                                    "  0.0175     0.01375    0.02      ]\n",
                                    " [0.045      0.6225     0.0475     0.0475     0.05375    0.045\n",
                                    "  0.04875    0.04625    0.04375   ]\n",
                                    " [0.         0.         0.04625    0.045      0.75749999 0.0475\n",
                                    "  0.         0.         0.10375   ]\n",
                                    " [0.15375    0.         0.1525     0.13249999 0.1125     0.065\n",
                                    "  0.15000001 0.125      0.10875   ]\n",
                                    " [0.         1.         0.         0.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.01625    0.         0.0375     0.         0.87875003 0.0175\n",
                                    "  0.015      0.01625    0.01875   ]\n",
                                    " [0.         0.         0.         0.         0.         0.95625001\n",
                                    "  0.         0.01875    0.025     ]\n",
                                    " [0.04625    0.65125    0.04375    0.04625    0.05       0.03625\n",
                                    "  0.04125    0.0375     0.0475    ]\n",
                                    " [0.02625    0.         0.         0.         0.         0.02\n",
                                    "  0.90875    0.02375    0.02125   ]\n",
                                    " [0.025      0.0275     0.         0.94749999 0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.1825     0.0625     0.18875    0.13625    0.12\n",
                                    "  0.175      0.0775     0.0575    ]\n",
                                    " [0.         0.1825     0.0625     0.18875    0.13625    0.12\n",
                                    "  0.175      0.0775     0.0575    ]\n",
                                    " [0.         0.02125    0.         0.         0.         0.92750001\n",
                                    "  0.         0.02375    0.0275    ]\n",
                                    " [0.         0.90625    0.02375    0.025      0.         0.\n",
                                    "  0.02375    0.         0.02125   ]\n",
                                    " [0.505      0.055      0.0525     0.06125    0.06375    0.0575\n",
                                    "  0.0675     0.075      0.0625    ]]\n",
                                    "Predicted: tensor([[0.0716, 0.0332, 0.1043, 0.2052, 0.0424, 0.1822, 0.0382, 0.1406, 0.1823],\n",
                                    "        [0.1134, 0.0731, 0.1183, 0.1204, 0.0876, 0.1357, 0.0918, 0.1244, 0.1353],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311],\n",
                                    "        [0.1189, 0.0725, 0.1130, 0.1297, 0.0908, 0.1361, 0.0851, 0.1223, 0.1316],\n",
                                    "        [0.0562, 0.0186, 0.1027, 0.2368, 0.0275, 0.1706, 0.0252, 0.1565, 0.2058],\n",
                                    "        [0.1058, 0.0667, 0.1143, 0.1483, 0.0826, 0.1436, 0.0751, 0.1276, 0.1358],\n",
                                    "        [0.1158, 0.0730, 0.1177, 0.1249, 0.0925, 0.1379, 0.0884, 0.1146, 0.1353],\n",
                                    "        [0.1178, 0.0626, 0.1062, 0.1327, 0.0910, 0.1628, 0.0816, 0.1066, 0.1385],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311],\n",
                                    "        [0.1098, 0.0701, 0.1193, 0.1255, 0.0920, 0.1411, 0.0895, 0.1226, 0.1302],\n",
                                    "        [0.1083, 0.0703, 0.1119, 0.1456, 0.0843, 0.1364, 0.0774, 0.1304, 0.1355],\n",
                                    "        [0.0631, 0.0253, 0.1089, 0.2295, 0.0343, 0.1663, 0.0307, 0.1499, 0.1920],\n",
                                    "        [0.0631, 0.0253, 0.1089, 0.2295, 0.0343, 0.1663, 0.0307, 0.1499, 0.1920],\n",
                                    "        [0.0678, 0.0253, 0.0944, 0.2089, 0.0335, 0.1894, 0.0324, 0.1498, 0.1985],\n",
                                    "        [0.1139, 0.0652, 0.1136, 0.1317, 0.0898, 0.1425, 0.0866, 0.1209, 0.1358],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.0716, 0.0332, 0.1043, 0.2052, 0.0424, 0.1822, 0.0382, 0.1406, 0.1823],\n",
                                    "        [0.1134, 0.0731, 0.1183, 0.1204, 0.0876, 0.1357, 0.0918, 0.1244, 0.1353],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311],\n",
                                    "        [0.1189, 0.0725, 0.1130, 0.1297, 0.0908, 0.1361, 0.0851, 0.1223, 0.1316],\n",
                                    "        [0.0562, 0.0186, 0.1027, 0.2368, 0.0275, 0.1706, 0.0252, 0.1565, 0.2058],\n",
                                    "        [0.1058, 0.0667, 0.1143, 0.1483, 0.0826, 0.1436, 0.0751, 0.1276, 0.1358],\n",
                                    "        [0.1158, 0.0730, 0.1177, 0.1249, 0.0925, 0.1379, 0.0884, 0.1146, 0.1353],\n",
                                    "        [0.1178, 0.0626, 0.1062, 0.1327, 0.0910, 0.1628, 0.0816, 0.1066, 0.1385],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311],\n",
                                    "        [0.1098, 0.0701, 0.1193, 0.1255, 0.0920, 0.1411, 0.0895, 0.1226, 0.1302],\n",
                                    "        [0.1083, 0.0703, 0.1119, 0.1456, 0.0843, 0.1364, 0.0774, 0.1304, 0.1355],\n",
                                    "        [0.0631, 0.0253, 0.1089, 0.2295, 0.0343, 0.1663, 0.0307, 0.1499, 0.1920],\n",
                                    "        [0.0631, 0.0253, 0.1089, 0.2295, 0.0343, 0.1663, 0.0307, 0.1499, 0.1920],\n",
                                    "        [0.0678, 0.0253, 0.0944, 0.2089, 0.0335, 0.1894, 0.0324, 0.1498, 0.1985],\n",
                                    "        [0.1139, 0.0652, 0.1136, 0.1317, 0.0898, 0.1425, 0.0866, 0.1209, 0.1358],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.0716, 0.0332, 0.1043, 0.2052, 0.0424, 0.1822, 0.0382, 0.1406, 0.1823],\n",
                                    "        [0.1134, 0.0731, 0.1183, 0.1204, 0.0876, 0.1357, 0.0918, 0.1244, 0.1353],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311],\n",
                                    "        [0.1189, 0.0725, 0.1130, 0.1297, 0.0908, 0.1361, 0.0851, 0.1223, 0.1316],\n",
                                    "        [0.0562, 0.0186, 0.1027, 0.2368, 0.0275, 0.1706, 0.0252, 0.1565, 0.2058],\n",
                                    "        [0.1058, 0.0667, 0.1143, 0.1483, 0.0826, 0.1436, 0.0751, 0.1276, 0.1358],\n",
                                    "        [0.1158, 0.0730, 0.1177, 0.1249, 0.0925, 0.1379, 0.0884, 0.1146, 0.1353],\n",
                                    "        [0.1178, 0.0626, 0.1062, 0.1327, 0.0910, 0.1628, 0.0816, 0.1066, 0.1385],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311],\n",
                                    "        [0.1098, 0.0701, 0.1193, 0.1255, 0.0920, 0.1411, 0.0895, 0.1226, 0.1302],\n",
                                    "        [0.1083, 0.0703, 0.1119, 0.1456, 0.0843, 0.1364, 0.0774, 0.1304, 0.1355],\n",
                                    "        [0.0631, 0.0253, 0.1089, 0.2295, 0.0343, 0.1663, 0.0307, 0.1499, 0.1920],\n",
                                    "        [0.0631, 0.0253, 0.1089, 0.2295, 0.0343, 0.1663, 0.0307, 0.1499, 0.1920],\n",
                                    "        [0.0678, 0.0253, 0.0944, 0.2089, 0.0335, 0.1894, 0.0324, 0.1498, 0.1985],\n",
                                    "        [0.1139, 0.0652, 0.1136, 0.1317, 0.0898, 0.1425, 0.0866, 0.1209, 0.1358],\n",
                                    "        [0.1178, 0.0830, 0.1172, 0.1198, 0.0939, 0.1294, 0.0895, 0.1182, 0.1311]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.6370, -3.4053, -2.2604, -1.5839, -3.1599, -1.7029, -3.2639, -1.9619,\n",
                                    "         -1.7019],\n",
                                    "        [-2.1764, -2.6158, -2.1345, -2.1169, -2.4348, -1.9971, -2.3886, -2.0844,\n",
                                    "         -2.0006],\n",
                                    "        [-2.1386, -2.4891, -2.1440, -2.1217, -2.3650, -2.0448, -2.4134, -2.1351,\n",
                                    "         -2.0317],\n",
                                    "        [-2.1296, -2.6238, -2.1806, -2.0423, -2.3996, -1.9944, -2.4635, -2.1010,\n",
                                    "         -2.0283],\n",
                                    "        [-2.8789, -3.9866, -2.2762, -1.4404, -3.5920, -1.7682, -3.6817, -1.8545,\n",
                                    "         -1.5806],\n",
                                    "        [-2.2464, -2.7076, -2.1689, -1.9083, -2.4932, -1.9404, -2.5887, -2.0585,\n",
                                    "         -1.9962],\n",
                                    "        [-2.1560, -2.6176, -2.1394, -2.0804, -2.3810, -1.9811, -2.4255, -2.1666,\n",
                                    "         -2.0005],\n",
                                    "        [-2.1387, -2.7705, -2.2424, -2.0194, -2.3965, -1.8151, -2.5062, -2.2383,\n",
                                    "         -1.9766],\n",
                                    "        [-2.1386, -2.4891, -2.1440, -2.1217, -2.3650, -2.0448, -2.4134, -2.1351,\n",
                                    "         -2.0317],\n",
                                    "        [-2.2094, -2.6572, -2.1265, -2.0755, -2.3856, -1.9584, -2.4138, -2.0992,\n",
                                    "         -2.0387],\n",
                                    "        [-2.2230, -2.6548, -2.1904, -1.9267, -2.4737, -1.9922, -2.5594, -2.0374,\n",
                                    "         -1.9986],\n",
                                    "        [-2.7634, -3.6759, -2.2172, -1.4717, -3.3731, -1.7942, -3.4825, -1.8979,\n",
                                    "         -1.6504],\n",
                                    "        [-2.7634, -3.6759, -2.2172, -1.4717, -3.3731, -1.7942, -3.4825, -1.8979,\n",
                                    "         -1.6504],\n",
                                    "        [-2.6916, -3.6762, -2.3605, -1.5658, -3.3958, -1.6638, -3.4304, -1.8986,\n",
                                    "         -1.6169],\n",
                                    "        [-2.1722, -2.7308, -2.1751, -2.0272, -2.4099, -1.9483, -2.4469, -2.1127,\n",
                                    "         -1.9967],\n",
                                    "        [-2.1386, -2.4891, -2.1440, -2.1217, -2.3650, -2.0448, -2.4134, -2.1351,\n",
                                    "         -2.0317]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.36891064 2.2800927 2.6490035\n",
                                    "score:  1\n",
                                    "score:  -1\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_4/videos/alphazero/4/alphazero-episode-19.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_4/videos/alphazero/4/alphazero-episode-19.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                          \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_4/videos/alphazero/4/alphazero-episode-19.mp4\n",
                                    "score:  1\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.1075  0.0625  0.06    0.05    0.4975  0.05625 0.0575  0.05375 0.055  ]\n",
                                    "Temperature Policy  [0.1075  0.0625  0.06    0.05    0.4975  0.05625 0.0575  0.05375 0.055  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.16625001 0.11875    0.085      0.12375    0.         0.25\n",
                                    " 0.09125    0.0775     0.0875    ]\n",
                                    "Temperature Policy  [0.16625 0.11875 0.085   0.12375 0.25    0.09125 0.0775  0.0875 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.89749998 0.015      0.01625    0.         0.         0.0225\n",
                                    " 0.015      0.01625    0.0175    ]\n",
                                    "Temperature Policy  [0.8975  0.015   0.01625 0.0225  0.015   0.01625 0.0175 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.09875 0.1125  0.      0.      0.11625 0.095   0.10375 0.47375]\n",
                                    "Temperature Policy  [0.09875 0.1125  0.11625 0.095   0.10375 0.47375]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.02125    0.02875    0.         0.         0.02375\n",
                                    " 0.03375    0.         0.89249998]\n",
                                    "Temperature Policy  [5.8547334e-17 1.2030848e-15 1.7805489e-16 5.9793716e-15 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.1125     0.0575     0.05375    0.05125    0.46250001 0.06\n",
                                    " 0.09       0.045      0.0675    ]\n",
                                    "Temperature Policy  [0.1125  0.0575  0.05375 0.05125 0.4625  0.06    0.09    0.045   0.0675 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.18125001 0.11875    0.08       0.12375    0.         0.24875\n",
                                    " 0.085      0.0775     0.085     ]\n",
                                    "Temperature Policy  [0.18125 0.11875 0.08    0.12375 0.24875 0.085   0.0775  0.085  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.04625 0.01375 0.015   0.02125 0.      0.      0.0125  0.01625 0.875  ]\n",
                                    "Temperature Policy  [0.04625 0.01375 0.015   0.02125 0.0125  0.01625 0.875  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.44624999 0.1225     0.105      0.11375    0.         0.\n",
                                    " 0.1        0.1125     0.        ]\n",
                                    "Temperature Policy  [0.44625 0.1225  0.105   0.11375 0.1     0.1125 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.05625    0.0225     0.025      0.         0.\n",
                                    " 0.87374997 0.0225     0.        ]\n",
                                    "Temperature Policy  [1.2227943e-12 1.2821928e-16 3.6772930e-16 1.0000000e+00 1.2821928e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.2175     0.39750001 0.10875    0.         0.\n",
                                    " 0.         0.27625    0.        ]\n",
                                    "Temperature Policy  [2.3384977e-03 9.7211057e-01 2.2836891e-06 2.5548603e-02]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.025      0.         0.0175     0.         0.\n",
                                    " 0.         0.95749998 0.        ]\n",
                                    "Temperature Policy  [1.4723541e-16 4.1590360e-18 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.10625    0.0725     0.04125    0.06125    0.45500001 0.08\n",
                                    " 0.05875    0.04375    0.08125   ]\n",
                                    "Temperature Policy  [0.10625 0.0725  0.04125 0.06125 0.455   0.08    0.05875 0.04375 0.08125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.15625 0.11875 0.08    0.14125 0.      0.25    0.09125 0.0775  0.085  ]\n",
                                    "Temperature Policy  [0.15625 0.11875 0.08    0.14125 0.25    0.09125 0.0775  0.085  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.01375    0.         0.0175     0.0225     0.         0.89875001\n",
                                    " 0.0125     0.0175     0.0175    ]\n",
                                    "Temperature Policy  [0.01375 0.0175  0.0225  0.89875 0.0125  0.0175  0.0175 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.1025  0.115   0.      0.10875 0.1075  0.1025  0.46375]\n",
                                    "Temperature Policy  [0.1025  0.115   0.10875 0.1075  0.1025  0.46375]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.02875    0.03375    0.         0.0425\n",
                                    " 0.01625    0.         0.87875003]\n",
                                    "Temperature Policy  [1.4051578e-15 6.9836808e-15 7.0022228e-14 4.6760623e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.10875    0.05375    0.0475     0.0525     0.48500001 0.075\n",
                                    " 0.0675     0.0475     0.0625    ]\n",
                                    "Temperature Policy  [0.10875 0.05375 0.0475  0.0525  0.485   0.075   0.0675  0.0475  0.0625 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0775     0.08125    0.075      0.09       0.45750001 0.\n",
                                    " 0.0775     0.065      0.07625   ]\n",
                                    "Temperature Policy  [0.0775  0.08125 0.075   0.09    0.4575  0.0775  0.065   0.07625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.10625    0.0725     0.35499999 0.06625    0.         0.\n",
                                    " 0.07       0.0675     0.26249999]\n",
                                    "Temperature Policy  [0.10625 0.0725  0.355   0.06625 0.07    0.0675  0.2625 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.01875    0.01625    0.90249997 0.0225     0.         0.\n",
                                    " 0.0175     0.0225     0.        ]\n",
                                    "Temperature Policy  [0.01875 0.01625 0.9025  0.0225  0.0175  0.0225 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02       0.02       0.         0.025      0.         0.\n",
                                    " 0.91250002 0.0225     0.        ]\n",
                                    "Temperature Policy  [2.5584162e-17 2.5584162e-17 2.3827108e-16 1.0000000e+00 8.3079989e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.01875 0.02    0.      0.02625 0.      0.      0.      0.935   0.     ]\n",
                                    "Temperature Policy  [1.0517114e-17 2.0053207e-17 3.0421241e-16 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0225     0.94875002 0.         0.02875    0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [5.6273937e-17 1.0000000e+00 6.5292095e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.40625 0.      0.      0.59375 0.      0.      0.      0.      0.     ]\n",
                                    "Temperature Policy  [0.02199077 0.9780092 ]\n",
                                    "Action  3\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.10625    0.06       0.04125    0.05625    0.50125003 0.05875\n",
                                    " 0.0675     0.05125    0.0575    ]\n",
                                    "Temperature Policy  [0.10625 0.06    0.04125 0.05625 0.50125 0.05875 0.0675  0.05125 0.0575 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.07875    0.07875    0.07625    0.         0.39125001 0.18125001\n",
                                    " 0.07625    0.04       0.0775    ]\n",
                                    "Temperature Policy  [0.07875 0.07875 0.07625 0.39125 0.18125 0.07625 0.04    0.0775 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.46625 0.0675  0.06875 0.      0.      0.07    0.18625 0.07    0.07125]\n",
                                    "Temperature Policy  [0.46625 0.0675  0.06875 0.07    0.18625 0.07    0.07125]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0625     0.70375001 0.         0.         0.         0.06375\n",
                                    " 0.0375     0.0875     0.045     ]\n",
                                    "Temperature Policy  [0.0625  0.70375 0.06375 0.0375  0.0875  0.045  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02    0.90625 0.      0.      0.      0.0275  0.02    0.      0.02625]\n",
                                    "Temperature Policy  [2.7404366e-17 1.0000000e+00 6.6198282e-16 2.7404366e-17 4.1573145e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.935  0.     0.     0.     0.     0.025  0.0175 0.     0.0225]\n",
                                    "Temperature Policy  [1.0000000e+00 1.8676004e-16 5.2755094e-18 6.5119205e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.02625\n",
                                    " 0.02375    0.         0.94999999]\n",
                                    "Temperature Policy  [2.594520e-16 9.536743e-17 1.000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.97375 0.02625 0.      0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.0268348e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.12125    0.065      0.05625    0.05375    0.48750001 0.06125\n",
                                    " 0.0575     0.04125    0.05625   ]\n",
                                    "Temperature Policy  [0.12125 0.065   0.05625 0.05375 0.4875  0.06125 0.0575  0.04125 0.05625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.13500001 0.13875    0.05625    0.0725     0.1525     0.09625\n",
                                    " 0.21125001 0.         0.1375    ]\n",
                                    "Temperature Policy  [0.135   0.13875 0.05625 0.0725  0.1525  0.09625 0.21125 0.1375 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0325     0.025      0.02875    0.0325     0.03       0.\n",
                                    " 0.0425     0.         0.80874997]\n",
                                    "Temperature Policy  [0.0325  0.025   0.02875 0.0325  0.03    0.0425  0.80875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0975     0.09625    0.115      0.13500001 0.13124999 0.\n",
                                    " 0.42500001 0.         0.        ]\n",
                                    "Temperature Policy  [0.0975  0.09625 0.115   0.135   0.13125 0.425  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02125    0.015      0.02       0.02125    0.92250001 0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.2065459e-17 1.2919594e-18 2.2942261e-17 4.2065459e-17 1.0000000e+00]\n",
                                    "Action  4\n",
                                    "Target Policy [0.41499999 0.29499999 0.09125    0.19875    0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.6751457e-01 3.1871073e-02 2.5556730e-07 6.1411248e-04]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.95625001 0.01375    0.03       0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.7784398e-19 9.2363014e-16]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.1     0.055   0.0425  0.04875 0.49875 0.08625 0.05625 0.045   0.0675 ]\n",
                                    "Temperature Policy  [0.1     0.055   0.0425  0.04875 0.49875 0.08625 0.05625 0.045   0.0675 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.17749999 0.11875    0.085      0.1225     0.         0.24875\n",
                                    " 0.085      0.0775     0.085     ]\n",
                                    "Temperature Policy  [0.1775  0.11875 0.085   0.1225  0.24875 0.085   0.0775  0.085  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.37625    0.02625    0.39250001 0.         0.055\n",
                                    " 0.04125    0.06125    0.0475    ]\n",
                                    "Temperature Policy  [0.37625 0.02625 0.3925  0.055   0.04125 0.06125 0.0475 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.0175     0.01875    0.         0.         0.89875001\n",
                                    " 0.01875    0.02125    0.025     ]\n",
                                    "Temperature Policy  [0.0175  0.01875 0.89875 0.01875 0.02125 0.025  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.41125 0.38    0.      0.      0.      0.03    0.11875 0.06   ]\n",
                                    "Temperature Policy  [6.8789393e-01 3.1210330e-01 2.9354516e-12 2.7720341e-06 3.0059024e-09]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0225     0.         0.         0.\n",
                                    " 0.02625    0.92750001 0.02375   ]\n",
                                    "Temperature Policy  [7.0580708e-17 3.2972653e-16 1.0000000e+00 1.2119842e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.46375    0.         0.         0.\n",
                                    " 0.41499999 0.         0.12125   ]\n",
                                    "Temperature Policy  [7.5225335e-01 2.4774554e-01 1.1229033e-06]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.96749997 0.         0.0325    ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.8294641e-15]\n",
                                    "Action  6\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.10125    0.0575     0.0525     0.06       0.44624999 0.06875\n",
                                    " 0.1125     0.04375    0.0575    ]\n",
                                    "Temperature Policy  [0.10125 0.0575  0.0525  0.06    0.44625 0.06875 0.1125  0.04375 0.0575 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.07875    0.075      0.         0.06875    0.28125    0.19374999\n",
                                    " 0.075      0.05625    0.17125   ]\n",
                                    "Temperature Policy  [0.07875 0.075   0.06875 0.28125 0.19375 0.075   0.05625 0.17125]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01875 0.87    0.      0.02375 0.025   0.      0.01875 0.02    0.02375]\n",
                                    "Temperature Policy  [0.01875 0.87    0.02375 0.025   0.01875 0.02    0.02375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.47    0.      0.      0.10625 0.1275  0.      0.1025  0.0925  0.10125]\n",
                                    "Temperature Policy  [0.47    0.10625 0.1275  0.1025  0.0925  0.10125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.02125    0.92000002 0.\n",
                                    " 0.02       0.02125    0.0175    ]\n",
                                    "Temperature Policy  [4.322262e-17 1.000000e+00 2.357337e-17 4.322262e-17 6.201578e-18]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.1925     0.         0.\n",
                                    " 0.39625001 0.315      0.09625   ]\n",
                                    "Temperature Policy  [6.6469359e-04 9.0783501e-01 9.1499701e-02 6.4911484e-07]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.02625    0.         0.\n",
                                    " 0.         0.95875001 0.015     ]\n",
                                    "Temperature Policy  [2.367224e-16 1.000000e+00 8.787368e-19]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.10125    0.05875    0.05625    0.04875    0.50999999 0.0575\n",
                                    " 0.065      0.045      0.0575    ]\n",
                                    "Temperature Policy  [0.10125 0.05875 0.05625 0.04875 0.51    0.0575  0.065   0.045   0.0575 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.17625 0.11875 0.085   0.12375 0.      0.24875 0.085   0.0775  0.085  ]\n",
                                    "Temperature Policy  [0.17625 0.11875 0.085   0.12375 0.24875 0.085   0.0775  0.085  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.05125    0.85624999 0.01625    0.025      0.         0.\n",
                                    " 0.01625    0.01625    0.01875   ]\n",
                                    "Temperature Policy  [0.05125 0.85625 0.01625 0.025   0.01625 0.01625 0.01875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.09625 0.      0.13    0.10125 0.      0.      0.1075  0.44    0.125  ]\n",
                                    "Temperature Policy  [0.09625 0.13    0.10125 0.1075  0.44    0.125  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.90125 0.      0.035   0.02    0.      0.      0.01875 0.      0.025  ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.8023906e-15 2.8963243e-17 1.5190077e-17 2.6974122e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.35874999 0.095      0.         0.\n",
                                    " 0.1375     0.         0.40875   ]\n",
                                    "Temperature Policy  [2.1335854e-01 3.6176536e-07 1.4595464e-05 7.8662646e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.96749997 0.01875    0.         0.\n",
                                    " 0.01375    0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.4731376e-18 3.3613771e-19]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.10875    0.06       0.04625    0.0575     0.47749999 0.08625\n",
                                    " 0.0675     0.04       0.05625   ]\n",
                                    "Temperature Policy  [0.10875 0.06    0.04625 0.0575  0.4775  0.08625 0.0675  0.04    0.05625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.11375 0.11125 0.1375  0.09125 0.25125 0.10375 0.1075  0.08375 0.     ]\n",
                                    "Temperature Policy  [0.11375 0.11125 0.1375  0.09125 0.25125 0.10375 0.1075  0.08375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.015      0.         0.89499998 0.0175     0.01875    0.02\n",
                                    " 0.01375    0.02       0.        ]\n",
                                    "Temperature Policy  [0.015   0.895   0.0175  0.01875 0.02    0.01375 0.02   ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.09875    0.         0.         0.0925     0.10125    0.50999999\n",
                                    " 0.09875    0.09875    0.        ]\n",
                                    "Temperature Policy  [0.09875 0.0925  0.10125 0.51    0.09875 0.09875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0225     0.         0.         0.02125    0.02       0.\n",
                                    " 0.91500002 0.02125    0.        ]\n",
                                    "Temperature Policy  [8.0837761e-17 4.5643446e-17 2.4893675e-17 1.0000000e+00 4.5643446e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.09625    0.         0.         0.14875001 0.375      0.\n",
                                    " 0.         0.38       0.        ]\n",
                                    "Temperature Policy  [5.7933732e-07 4.5029166e-05 4.6691394e-01 5.3304040e-01]\n",
                                    "Action  4\n",
                                    "Target Policy [0.01125 0.      0.      0.0225  0.      0.      0.      0.96625 0.     ]\n",
                                    "Temperature Policy  [4.5775190e-20 4.6873794e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.1        0.05375    0.07125    0.0475     0.50999999 0.055\n",
                                    " 0.0625     0.0425     0.0575    ]\n",
                                    "Temperature Policy  [0.1     0.05375 0.07125 0.0475  0.51    0.055   0.0625  0.0425  0.0575 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.18000001 0.11       0.08       0.12375    0.         0.24875\n",
                                    " 0.09125    0.08125    0.085     ]\n",
                                    "Temperature Policy  [0.18    0.11    0.08    0.12375 0.24875 0.09125 0.08125 0.085  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.36500001 0.05125    0.39625001 0.         0.055\n",
                                    " 0.07       0.02875    0.03375   ]\n",
                                    "Temperature Policy  [0.365   0.05125 0.39625 0.055   0.07    0.02875 0.03375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.025      0.0225     0.         0.02375\n",
                                    " 0.01625    0.88999999 0.0225    ]\n",
                                    "Temperature Policy  [0.025   0.0225  0.02375 0.01625 0.89    0.0225 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.03375    0.38624999 0.         0.1025\n",
                                    " 0.41999999 0.         0.0575    ]\n",
                                    "Temperature Policy  [7.8358812e-12 3.0201951e-01 5.2310480e-07 6.9797993e-01 1.6144645e-09]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.92624998 0.03125    0.         0.0225\n",
                                    " 0.         0.         0.02      ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.9108123e-15 7.1539020e-17 2.2030166e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.42375001 0.         0.45375001\n",
                                    " 0.         0.         0.1225    ]\n",
                                    "Temperature Policy  [3.3536258e-01 6.6463608e-01 1.3670310e-06]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.97500002 0.         0.\n",
                                    " 0.         0.         0.025     ]\n",
                                    "Temperature Policy  [1.00000e+00 1.22844e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.1125     0.0575     0.0525     0.05       0.47874999 0.0575\n",
                                    " 0.06       0.045      0.08625   ]\n",
                                    "Temperature Policy  [0.1125  0.0575  0.0525  0.05    0.47875 0.0575  0.06    0.045   0.08625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.18000001 0.11875    0.08       0.12375    0.         0.25\n",
                                    " 0.085      0.0775     0.085     ]\n",
                                    "Temperature Policy  [0.18    0.11875 0.08    0.12375 0.25    0.085   0.0775  0.085  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.05125    0.015      0.01625    0.0225     0.         0.\n",
                                    " 0.05125    0.01625    0.82749999]\n",
                                    "Temperature Policy  [0.05125 0.015   0.01625 0.0225  0.05125 0.01625 0.8275 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.44999999 0.1        0.105      0.12       0.         0.\n",
                                    " 0.11125    0.11375    0.        ]\n",
                                    "Temperature Policy  [0.45    0.1     0.105   0.12    0.11125 0.11375]\n",
                                    "Action  7\n",
                                    "Target Policy [0.90125 0.01625 0.025   0.02375 0.      0.      0.03375 0.      0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.6314569e-18 2.6974122e-16 1.6150404e-16 5.4235668e-15]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.10125    0.11875    0.04625    0.05875    0.45875001 0.0575\n",
                                    " 0.05875    0.04125    0.05875   ]\n",
                                    "Temperature Policy  [0.10125 0.11875 0.04625 0.05875 0.45875 0.0575  0.05875 0.04125 0.05875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.095      0.07625    0.06875    0.40000001 0.09125\n",
                                    " 0.115      0.05       0.10375   ]\n",
                                    "Temperature Policy  [0.095   0.07625 0.06875 0.4     0.09125 0.115   0.05    0.10375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.0175     0.02125    0.01875    0.04375    0.\n",
                                    " 0.86250001 0.0175     0.01875   ]\n",
                                    "Temperature Policy  [0.0175  0.02125 0.01875 0.04375 0.8625  0.0175  0.01875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.0875     0.1        0.49125001 0.11       0.\n",
                                    " 0.         0.09375    0.1175    ]\n",
                                    "Temperature Policy  [0.0875  0.1     0.49125 0.11    0.09375 0.1175 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.0325  0.03125 0.8725  0.03    0.      0.      0.      0.03375]\n",
                                    "Temperature Policy  [5.1425589e-15 3.4741285e-15 1.0000000e+00 2.3097140e-15 7.5003827e-15]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.1225  0.10125 0.0475  0.05375 0.4425  0.07    0.06125 0.04375 0.0575 ]\n",
                                    "Temperature Policy  [0.1225  0.10125 0.0475  0.05375 0.4425  0.07    0.06125 0.04375 0.0575 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.11375    0.10625    0.13375001 0.0925     0.25749999 0.10375\n",
                                    " 0.10875    0.08375    0.        ]\n",
                                    "Temperature Policy  [0.11375 0.10625 0.13375 0.0925  0.2575  0.10375 0.10875 0.08375]\n",
                                    "Action  2\n",
                                    "Target Policy [0.01625    0.01375    0.         0.02125    0.01625    0.0225\n",
                                    " 0.89499998 0.015      0.        ]\n",
                                    "Temperature Policy  [0.01625 0.01375 0.02125 0.01625 0.0225  0.895   0.015  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.12       0.13124999 0.         0.1075     0.1125     0.1025\n",
                                    " 0.         0.42625001 0.        ]\n",
                                    "Temperature Policy  [0.12    0.13125 0.1075  0.1125  0.1025  0.42625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.02375 0.      0.      0.01375 0.01375 0.01625 0.      0.9325  0.     ]\n",
                                    "Temperature Policy  [1.1485443e-16 4.8589070e-19 4.8589070e-19 2.5825294e-18 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.10875 0.065   0.05125 0.06375 0.47    0.06    0.06    0.04375 0.0775 ]\n",
                                    "Temperature Policy  [0.10875 0.065   0.05125 0.06375 0.47    0.06    0.06    0.04375 0.0775 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1725  0.11875 0.085   0.12375 0.      0.25    0.085   0.0775  0.0875 ]\n",
                                    "Temperature Policy  [0.1725  0.11875 0.085   0.12375 0.25    0.085   0.0775  0.0875 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.36750001 0.04875    0.39375001 0.         0.055\n",
                                    " 0.07125    0.02625    0.0375    ]\n",
                                    "Temperature Policy  [0.3675  0.04875 0.39375 0.055   0.07125 0.02625 0.0375 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01625    0.02       0.         0.         0.89625001\n",
                                    " 0.02125    0.02125    0.025     ]\n",
                                    "Temperature Policy  [0.01625 0.02    0.89625 0.02125 0.02125 0.025  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.41       0.39625001 0.         0.         0.\n",
                                    " 0.0325     0.10125    0.06      ]\n",
                                    "Temperature Policy  [5.8446187e-01 4.1553766e-01 5.7246803e-12 4.9302383e-07 2.6328741e-09]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0225     0.         0.         0.\n",
                                    " 0.01875    0.93000001 0.02875   ]\n",
                                    "Temperature Policy  [6.87061643e-17 1.10964286e-17 1.00000000e+00 7.97166424e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.47375    0.         0.         0.\n",
                                    " 0.41499999 0.         0.11125   ]\n",
                                    "Temperature Policy  [7.8984821e-01 2.1015145e-01 4.0276493e-07]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.97000003 0.         0.         0.\n",
                                    " 0.         0.         0.03      ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.0074683e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.10625 0.055   0.0525  0.05125 0.49875 0.0625  0.06    0.05375 0.06   ]\n",
                                    "Temperature Policy  [0.10625 0.055   0.0525  0.05125 0.49875 0.0625  0.06    0.05375 0.06   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.17749999 0.10875    0.085      0.12375    0.         0.24875\n",
                                    " 0.09375    0.0775     0.085     ]\n",
                                    "Temperature Policy  [0.1775  0.10875 0.085   0.12375 0.24875 0.09375 0.0775  0.085  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.89749998 0.015      0.015      0.         0.         0.025\n",
                                    " 0.01625    0.015      0.01625   ]\n",
                                    "Temperature Policy  [0.8975  0.015   0.015   0.025   0.01625 0.015   0.01625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.09875 0.10625 0.      0.      0.11375 0.105   0.1075  0.46875]\n",
                                    "Temperature Policy  [0.09875 0.10625 0.11375 0.105   0.1075  0.46875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.91624999 0.02125    0.         0.         0.02375\n",
                                    " 0.0175     0.02125    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.5024559e-17 1.3692926e-16 6.4601205e-18 4.5024559e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.31625    0.         0.         0.13124999\n",
                                    " 0.19       0.36250001 0.        ]\n",
                                    "Temperature Policy  [2.0318323e-01 3.0801766e-05 1.2448543e-03 7.9554111e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.95999998 0.         0.         0.02125\n",
                                    " 0.01875    0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.8240803e-17 8.0779357e-18]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x32ec69060>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c3820>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32ecfbb20>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x107dbd780>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3321cd2d0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32d854460>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x32d844850>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x32c646050>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32ec6e860>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x31a517760>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32d8be0e0>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x3321b6ad0>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x3286f5600>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x330a0f7f0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4460>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x32d8bc400>, 7)]\n",
                                    "Observations [array([[[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 1.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]])]\n",
                                    "Policies [array([0.02125   , 0.015     , 0.        , 0.88999999, 0.        ,\n",
                                    "       0.        , 0.02375   , 0.02375   , 0.02625   ]), array([0.17749999, 0.1175    , 0.1025    , 0.08875   , 0.10875   ,\n",
                                    "       0.09375   , 0.105     , 0.09625   , 0.11      ]), array([0.06625   , 0.0425    , 0.04875   , 0.05625   , 0.54124999,\n",
                                    "       0.06      , 0.06375   , 0.06      , 0.06125   ]), array([0.80250001, 0.        , 0.045     , 0.03      , 0.02875   ,\n",
                                    "       0.0325    , 0.03      , 0.        , 0.03125   ]), array([0.        , 0.        , 0.        , 0.02125   , 0.92000002,\n",
                                    "       0.        , 0.02      , 0.02125   , 0.0175    ]), array([0.        , 0.21250001, 0.10625   , 0.12375   , 0.        ,\n",
                                    "       0.155     , 0.175     , 0.11125   , 0.11625   ]), array([0.        , 0.27000001, 0.        , 0.22      , 0.41874999,\n",
                                    "       0.        , 0.        , 0.09125   , 0.        ]), array([0.10875, 0.065  , 0.05125, 0.06375, 0.47   , 0.06   , 0.06   ,\n",
                                    "       0.04375, 0.0775 ]), array([0.02      , 0.02      , 0.        , 0.025     , 0.        ,\n",
                                    "       0.        , 0.91250002, 0.0225    , 0.        ]), array([0.0225    , 0.        , 0.0225    , 0.        , 0.        ,\n",
                                    "       0.        , 0.01625   , 0.0175    , 0.92124999]), array([0.        , 0.2175    , 0.39750001, 0.10875   , 0.        ,\n",
                                    "       0.        , 0.        , 0.27625   , 0.        ]), array([0.        , 0.        , 0.        , 0.02625   , 0.        ,\n",
                                    "       0.        , 0.02375   , 0.94999999, 0.        ]), array([0.     , 0.96875, 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
                                    "       0.     , 0.03125]), array([0.02375, 0.     , 0.     , 0.01375, 0.01375, 0.01625, 0.     ,\n",
                                    "       0.9325 , 0.     ]), array([0.52249998, 0.        , 0.        , 0.09      , 0.12375   ,\n",
                                    "       0.0875    , 0.10625   , 0.07      , 0.        ]), array([0.        , 0.0225    , 0.        , 0.97750002, 0.        ,\n",
                                    "       0.        , 0.        , 0.        , 0.        ])]\n",
                                    "NP Array Policies [[0.02125    0.015      0.         0.88999999 0.         0.\n",
                                    "  0.02375    0.02375    0.02625   ]\n",
                                    " [0.17749999 0.1175     0.1025     0.08875    0.10875    0.09375\n",
                                    "  0.105      0.09625    0.11      ]\n",
                                    " [0.06625    0.0425     0.04875    0.05625    0.54124999 0.06\n",
                                    "  0.06375    0.06       0.06125   ]\n",
                                    " [0.80250001 0.         0.045      0.03       0.02875    0.0325\n",
                                    "  0.03       0.         0.03125   ]\n",
                                    " [0.         0.         0.         0.02125    0.92000002 0.\n",
                                    "  0.02       0.02125    0.0175    ]\n",
                                    " [0.         0.21250001 0.10625    0.12375    0.         0.155\n",
                                    "  0.175      0.11125    0.11625   ]\n",
                                    " [0.         0.27000001 0.         0.22       0.41874999 0.\n",
                                    "  0.         0.09125    0.        ]\n",
                                    " [0.10875    0.065      0.05125    0.06375    0.47       0.06\n",
                                    "  0.06       0.04375    0.0775    ]\n",
                                    " [0.02       0.02       0.         0.025      0.         0.\n",
                                    "  0.91250002 0.0225     0.        ]\n",
                                    " [0.0225     0.         0.0225     0.         0.         0.\n",
                                    "  0.01625    0.0175     0.92124999]\n",
                                    " [0.         0.2175     0.39750001 0.10875    0.         0.\n",
                                    "  0.         0.27625    0.        ]\n",
                                    " [0.         0.         0.         0.02625    0.         0.\n",
                                    "  0.02375    0.94999999 0.        ]\n",
                                    " [0.         0.96875    0.         0.         0.         0.\n",
                                    "  0.         0.         0.03125   ]\n",
                                    " [0.02375    0.         0.         0.01375    0.01375    0.01625\n",
                                    "  0.         0.9325     0.        ]\n",
                                    " [0.52249998 0.         0.         0.09       0.12375    0.0875\n",
                                    "  0.10625    0.07       0.        ]\n",
                                    " [0.         0.0225     0.         0.97750002 0.         0.\n",
                                    "  0.         0.         0.        ]]\n",
                                    "Predicted: tensor([[0.0647, 0.0465, 0.0843, 0.2300, 0.0571, 0.2218, 0.0495, 0.1012, 0.1449],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1133, 0.0819, 0.1081, 0.1229, 0.0956, 0.1512, 0.0899, 0.1122, 0.1251],\n",
                                    "        [0.1049, 0.0887, 0.1143, 0.1271, 0.0931, 0.1416, 0.0856, 0.1219, 0.1228],\n",
                                    "        [0.1131, 0.0847, 0.1071, 0.1274, 0.0983, 0.1430, 0.0906, 0.1185, 0.1173],\n",
                                    "        [0.0693, 0.0494, 0.0925, 0.2112, 0.0639, 0.2295, 0.0535, 0.1006, 0.1302],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1132, 0.0811, 0.1048, 0.1376, 0.0927, 0.1460, 0.0864, 0.1160, 0.1221],\n",
                                    "        [0.1166, 0.0705, 0.1071, 0.1252, 0.0820, 0.1578, 0.0867, 0.1174, 0.1368],\n",
                                    "        [0.0701, 0.0468, 0.0869, 0.2185, 0.0631, 0.2314, 0.0531, 0.0961, 0.1340],\n",
                                    "        [0.1084, 0.0778, 0.1052, 0.1351, 0.0902, 0.1507, 0.0901, 0.1205, 0.1219],\n",
                                    "        [0.0753, 0.0480, 0.0870, 0.1885, 0.0616, 0.2416, 0.0574, 0.1012, 0.1396],\n",
                                    "        [0.1157, 0.0769, 0.1086, 0.1315, 0.0876, 0.1590, 0.0828, 0.1130, 0.1248],\n",
                                    "        [0.0635, 0.0432, 0.0907, 0.2220, 0.0545, 0.2435, 0.0480, 0.0990, 0.1356],\n",
                                    "        [0.0716, 0.0599, 0.0896, 0.2098, 0.0644, 0.1970, 0.0583, 0.1070, 0.1424]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.0647, 0.0465, 0.0843, 0.2300, 0.0571, 0.2218, 0.0495, 0.1012, 0.1449],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1133, 0.0819, 0.1081, 0.1229, 0.0956, 0.1512, 0.0899, 0.1122, 0.1251],\n",
                                    "        [0.1049, 0.0887, 0.1143, 0.1271, 0.0931, 0.1416, 0.0856, 0.1219, 0.1228],\n",
                                    "        [0.1131, 0.0847, 0.1071, 0.1274, 0.0983, 0.1430, 0.0906, 0.1185, 0.1173],\n",
                                    "        [0.0693, 0.0494, 0.0925, 0.2112, 0.0639, 0.2295, 0.0535, 0.1006, 0.1302],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1132, 0.0811, 0.1048, 0.1376, 0.0927, 0.1460, 0.0864, 0.1160, 0.1221],\n",
                                    "        [0.1166, 0.0705, 0.1071, 0.1252, 0.0820, 0.1578, 0.0867, 0.1174, 0.1368],\n",
                                    "        [0.0701, 0.0468, 0.0869, 0.2185, 0.0631, 0.2314, 0.0531, 0.0961, 0.1340],\n",
                                    "        [0.1084, 0.0778, 0.1052, 0.1351, 0.0902, 0.1507, 0.0901, 0.1205, 0.1219],\n",
                                    "        [0.0753, 0.0480, 0.0870, 0.1885, 0.0616, 0.2416, 0.0574, 0.1012, 0.1396],\n",
                                    "        [0.1157, 0.0769, 0.1086, 0.1315, 0.0876, 0.1590, 0.0828, 0.1130, 0.1248],\n",
                                    "        [0.0635, 0.0432, 0.0907, 0.2220, 0.0545, 0.2435, 0.0480, 0.0990, 0.1356],\n",
                                    "        [0.0716, 0.0599, 0.0896, 0.2098, 0.0644, 0.1970, 0.0583, 0.1070, 0.1424]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.0647, 0.0465, 0.0843, 0.2300, 0.0571, 0.2218, 0.0495, 0.1012, 0.1449],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1133, 0.0819, 0.1081, 0.1229, 0.0956, 0.1512, 0.0899, 0.1122, 0.1251],\n",
                                    "        [0.1049, 0.0887, 0.1143, 0.1271, 0.0931, 0.1416, 0.0856, 0.1219, 0.1228],\n",
                                    "        [0.1131, 0.0847, 0.1071, 0.1274, 0.0983, 0.1430, 0.0906, 0.1185, 0.1173],\n",
                                    "        [0.0693, 0.0494, 0.0925, 0.2112, 0.0639, 0.2295, 0.0535, 0.1006, 0.1302],\n",
                                    "        [0.1159, 0.0903, 0.1110, 0.1202, 0.0947, 0.1398, 0.0877, 0.1158, 0.1245],\n",
                                    "        [0.1132, 0.0811, 0.1048, 0.1376, 0.0927, 0.1460, 0.0864, 0.1160, 0.1221],\n",
                                    "        [0.1166, 0.0705, 0.1071, 0.1252, 0.0820, 0.1578, 0.0867, 0.1174, 0.1368],\n",
                                    "        [0.0701, 0.0468, 0.0869, 0.2185, 0.0631, 0.2314, 0.0531, 0.0961, 0.1340],\n",
                                    "        [0.1084, 0.0778, 0.1052, 0.1351, 0.0902, 0.1507, 0.0901, 0.1205, 0.1219],\n",
                                    "        [0.0753, 0.0480, 0.0870, 0.1885, 0.0616, 0.2416, 0.0574, 0.1012, 0.1396],\n",
                                    "        [0.1157, 0.0769, 0.1086, 0.1315, 0.0876, 0.1590, 0.0828, 0.1130, 0.1248],\n",
                                    "        [0.0635, 0.0432, 0.0907, 0.2220, 0.0545, 0.2435, 0.0480, 0.0990, 0.1356],\n",
                                    "        [0.0716, 0.0599, 0.0896, 0.2098, 0.0644, 0.1970, 0.0583, 0.1070, 0.1424]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.7379, -3.0677, -2.4738, -1.4698, -2.8637, -1.5058, -3.0060, -2.2905,\n",
                                    "         -1.9316],\n",
                                    "        [-2.1551, -2.4043, -2.1978, -2.1184, -2.3567, -1.9676, -2.4343, -2.1560,\n",
                                    "         -2.0831],\n",
                                    "        [-2.1551, -2.4043, -2.1978, -2.1184, -2.3567, -1.9676, -2.4343, -2.1560,\n",
                                    "         -2.0831],\n",
                                    "        [-2.1779, -2.5025, -2.2250, -2.0967, -2.3480, -1.8895, -2.4091, -2.1873,\n",
                                    "         -2.0787],\n",
                                    "        [-2.2544, -2.4226, -2.1688, -2.0626, -2.3740, -1.9546, -2.4585, -2.1048,\n",
                                    "         -2.0975],\n",
                                    "        [-2.1792, -2.4685, -2.2337, -2.0608, -2.3201, -1.9450, -2.4011, -2.1327,\n",
                                    "         -2.1431],\n",
                                    "        [-2.6690, -3.0073, -2.3811, -1.5552, -2.7507, -1.4719, -2.9280, -2.2965,\n",
                                    "         -2.0390],\n",
                                    "        [-2.1551, -2.4043, -2.1978, -2.1184, -2.3567, -1.9676, -2.4343, -2.1560,\n",
                                    "         -2.0831],\n",
                                    "        [-2.1785, -2.5120, -2.2556, -1.9831, -2.3781, -1.9240, -2.4491, -2.1543,\n",
                                    "         -2.1027],\n",
                                    "        [-2.1489, -2.6521, -2.2343, -2.0782, -2.5016, -1.8463, -2.4454, -2.1425,\n",
                                    "         -1.9889],\n",
                                    "        [-2.6576, -3.0617, -2.4426, -1.5212, -2.7624, -1.4638, -2.9364, -2.3422,\n",
                                    "         -2.0098],\n",
                                    "        [-2.2218, -2.5533, -2.2517, -2.0016, -2.4056, -1.8925, -2.4068, -2.1159,\n",
                                    "         -2.1046],\n",
                                    "        [-2.5863, -3.0375, -2.4423, -1.6689, -2.7876, -1.4207, -2.8572, -2.2911,\n",
                                    "         -1.9688],\n",
                                    "        [-2.1565, -2.5656, -2.2201, -2.0284, -2.4351, -1.8387, -2.4908, -2.1807,\n",
                                    "         -2.0808],\n",
                                    "        [-2.7561, -3.1424, -2.4004, -1.5049, -2.9104, -1.4127, -3.0374, -2.3124,\n",
                                    "         -1.9977],\n",
                                    "        [-2.6362, -2.8149, -2.4123, -1.5616, -2.7425, -1.6246, -2.8421, -2.2351,\n",
                                    "         -1.9493]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.19833706 2.2460372 2.444374\n",
                                    "score:  -1\n",
                                    "score:  0\n",
                                    "score:  -1\n",
                                    "score:  1\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_5/videos/alphazero/5/alphazero-episode-24.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_5/videos/alphazero/5/alphazero-episode-24.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                          \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_5/videos/alphazero/5/alphazero-episode-24.mp4\n",
                                    "score:  -1\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.115      0.11375    0.10375    0.1        0.14749999 0.105\n",
                                    " 0.11125    0.09625    0.1075    ]\n",
                                    "Temperature Policy  [0.115   0.11375 0.10375 0.1     0.1475  0.105   0.11125 0.09625 0.1075 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.12125    0.         0.09       0.07875    0.32875001 0.0925\n",
                                    " 0.08625    0.11       0.0925    ]\n",
                                    "Temperature Policy  [0.12125 0.09    0.07875 0.32875 0.0925  0.08625 0.11    0.0925 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.31125    0.         0.29624999 0.09625    0.         0.0925\n",
                                    " 0.06875    0.065      0.07      ]\n",
                                    "Temperature Policy  [0.31125 0.29625 0.09625 0.0925  0.06875 0.065   0.07   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.89125001 0.0275     0.         0.02375\n",
                                    " 0.01875    0.02125    0.0175    ]\n",
                                    "Temperature Policy  [0.89125 0.0275  0.02375 0.01875 0.02125 0.0175 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.89999998 0.0275     0.         0.02\n",
                                    " 0.0325     0.         0.02      ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.0941741e-16 2.9368032e-17 3.7705833e-15 2.9368032e-17]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.11375    0.115      0.10375    0.0975     0.14875001 0.10375\n",
                                    " 0.11125    0.09875    0.1075    ]\n",
                                    "Temperature Policy  [0.11375 0.115   0.10375 0.0975  0.14875 0.10375 0.11125 0.09875 0.1075 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.14624999 0.145      0.09125    0.095      0.215      0.08875\n",
                                    " 0.11625    0.1025     0.        ]\n",
                                    "Temperature Policy  [0.14625 0.145   0.09125 0.095   0.215   0.08875 0.11625 0.1025 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.11375    0.07       0.08375    0.0775     0.         0.27000001\n",
                                    " 0.2775     0.1075     0.        ]\n",
                                    "Temperature Policy  [0.11375 0.07    0.08375 0.0775  0.27    0.2775  0.1075 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.41999999 0.04125    0.14125    0.         0.20874999\n",
                                    " 0.0475     0.14125    0.        ]\n",
                                    "Temperature Policy  [0.42    0.04125 0.14125 0.20875 0.0475  0.14125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.02125 0.0275  0.      0.0225  0.0175  0.91125 0.     ]\n",
                                    "Temperature Policy  [4.7556943e-17 6.2654392e-16 8.4226700e-17 6.8234671e-18 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.02       0.02875    0.         0.025\n",
                                    " 0.92624998 0.         0.        ]\n",
                                    "Temperature Policy  [2.2030166e-17 8.3003474e-16 2.0517189e-16 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.94499999 0.02625    0.         0.02875\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.7351112e-16 6.7929814e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0.      0.      0.      0.02625 0.      0.97375 0.      0.      0.     ]\n",
                                    "Temperature Policy  [2.0268348e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.11375    0.11375    0.1025     0.1        0.14749999 0.10375\n",
                                    " 0.11125    0.09625    0.11125   ]\n",
                                    "Temperature Policy  [0.11375 0.11375 0.1025  0.1     0.1475  0.10375 0.11125 0.09625 0.11125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.08625    0.16875    0.1075     0.08       0.23625    0.09375\n",
                                    " 0.0925     0.         0.13500001]\n",
                                    "Temperature Policy  [0.08625 0.16875 0.1075  0.08    0.23625 0.09375 0.0925  0.135  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.03375 0.0175  0.015   0.      0.8775  0.01875 0.0175  0.      0.02   ]\n",
                                    "Temperature Policy  [0.03375 0.0175  0.015   0.8775  0.01875 0.0175  0.02   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.11125    0.46000001 0.10625    0.         0.         0.105\n",
                                    " 0.11       0.         0.1075    ]\n",
                                    "Temperature Policy  [0.11125 0.46    0.10625 0.105   0.11    0.1075 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.02       0.         0.02125    0.         0.         0.025\n",
                                    " 0.02625    0.         0.90750003]\n",
                                    "Temperature Policy  [2.7029227e-17 4.9559054e-17 2.5172927e-16 4.1004047e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.39375001 0.         0.1575     0.         0.         0.11\n",
                                    " 0.33875    0.         0.        ]\n",
                                    "Temperature Policy  [8.1817818e-01 8.5792199e-05 2.3690220e-06 1.8173362e-01]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.01625    0.         0.         0.01875\n",
                                    " 0.96499997 0.         0.        ]\n",
                                    "Temperature Policy  [1.833414e-18 7.669016e-18 1.000000e+00]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.11625    0.11375    0.10375    0.0975     0.14624999 0.10375\n",
                                    " 0.11375    0.0975     0.1075    ]\n",
                                    "Temperature Policy  [0.11625 0.11375 0.10375 0.0975  0.14625 0.10375 0.11375 0.0975  0.1075 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.17375 0.1     0.1075  0.08625 0.2325  0.08375 0.      0.08875 0.1275 ]\n",
                                    "Temperature Policy  [0.17375 0.1     0.1075  0.08625 0.2325  0.08375 0.08875 0.1275 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.09       0.07625    0.07875    0.17749999 0.         0.0825\n",
                                    " 0.         0.22375    0.27125001]\n",
                                    "Temperature Policy  [0.09    0.07625 0.07875 0.1775  0.0825  0.22375 0.27125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.01875    0.02125    0.0175     0.025      0.         0.01875\n",
                                    " 0.         0.89875001 0.        ]\n",
                                    "Temperature Policy  [0.01875 0.02125 0.0175  0.025   0.01875 0.89875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01125    0.015      0.0125     0.0325     0.         0.\n",
                                    " 0.         0.92874998 0.        ]\n",
                                    "Temperature Policy  [6.8004392e-20 1.2076035e-18 1.9503470e-19 2.7532482e-15 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.115   0.1125  0.10375 0.09875 0.14375 0.10375 0.11125 0.1025  0.10875]\n",
                                    "Temperature Policy  [0.115   0.1125  0.10375 0.09875 0.14375 0.10375 0.11125 0.1025  0.10875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1825     0.12375    0.125      0.12       0.         0.13500001\n",
                                    " 0.115      0.085      0.11375   ]\n",
                                    "Temperature Policy  [0.1825  0.12375 0.125   0.12    0.135   0.115   0.085   0.11375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.38874999 0.04375    0.33625001 0.         0.06\n",
                                    " 0.06625    0.05       0.055     ]\n",
                                    "Temperature Policy  [0.38875 0.04375 0.33625 0.06    0.06625 0.05    0.055  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02375    0.02375    0.         0.         0.88749999\n",
                                    " 0.0175     0.02625    0.02125   ]\n",
                                    "Temperature Policy  [0.02375 0.02375 0.8875  0.0175  0.02625 0.02125]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.40625 0.40375 0.      0.      0.      0.03375 0.0925  0.06375]\n",
                                    "Temperature Policy  [5.1542717e-01 4.8457271e-01 8.0718175e-12 1.9304159e-07 4.6669748e-09]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.02125 0.      0.      0.      0.02125 0.9325  0.025  ]\n",
                                    "Temperature Policy  [3.7765996e-17 3.7765996e-17 1.0000000e+00 1.9182787e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.45124999 0.         0.         0.\n",
                                    " 0.42500001 0.         0.12375   ]\n",
                                    "Temperature Policy  [6.455005e-01 3.544979e-01 1.553018e-06]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.97250003 0.         0.         0.\n",
                                    " 0.         0.         0.0275    ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.2691197e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.115      0.11375    0.1025     0.09875    0.14749999 0.10375\n",
                                    " 0.1125     0.0975     0.10875   ]\n",
                                    "Temperature Policy  [0.115   0.11375 0.1025  0.09875 0.1475  0.10375 0.1125  0.0975  0.10875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.175      0.1        0.105      0.08625    0.25874999 0.08375\n",
                                    " 0.         0.0875     0.10375   ]\n",
                                    "Temperature Policy  [0.175   0.1     0.105   0.08625 0.25875 0.08375 0.0875  0.10375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.09       0.07625    0.07       0.17749999 0.         0.08\n",
                                    " 0.         0.27250001 0.23375   ]\n",
                                    "Temperature Policy  [0.09    0.07625 0.07    0.1775  0.08    0.2725  0.23375]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0225     0.01875    0.02       0.02625    0.         0.01875\n",
                                    " 0.         0.         0.89375001]\n",
                                    "Temperature Policy  [0.0225  0.01875 0.02    0.02625 0.01875 0.89375]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91000003 0.0225     0.0175     0.02625    0.         0.02375\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.5390841e-17 6.9177776e-18 3.9891390e-16 1.4662979e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.02125 0.01875 0.935   0.      0.025   0.      0.      0.     ]\n",
                                    "Temperature Policy  [3.6768271e-17 1.0517114e-17 1.0000000e+00 1.8676004e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02375    0.02625    0.         0.         0.94999999\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [9.536743e-17 2.594520e-16 1.000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.57125002 0.42875001 0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.94631946 0.05368055]\n",
                                    "Action  1\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.115   0.11375 0.10375 0.09875 0.145   0.10375 0.11375 0.09625 0.11   ]\n",
                                    "Temperature Policy  [0.115   0.11375 0.10375 0.09875 0.145   0.10375 0.11375 0.09625 0.11   ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1825     0.12375    0.12375    0.12       0.         0.13500001\n",
                                    " 0.115      0.085      0.115     ]\n",
                                    "Temperature Policy  [0.1825  0.12375 0.12375 0.12    0.135   0.115   0.085   0.115  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.38999999 0.05125    0.34       0.         0.05125\n",
                                    " 0.06625    0.05       0.05125   ]\n",
                                    "Temperature Policy  [0.39    0.05125 0.34    0.05125 0.06625 0.05    0.05125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.02125    0.0275     0.         0.02625\n",
                                    " 0.025      0.87875003 0.02125   ]\n",
                                    "Temperature Policy  [0.02125 0.0275  0.02625 0.025   0.87875 0.02125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.0325     0.41125    0.         0.105\n",
                                    " 0.38749999 0.         0.06375   ]\n",
                                    "Temperature Policy  [6.1232568e-12 6.4447778e-01 7.5865063e-07 3.5552153e-01 5.1635780e-09]\n",
                                    "Action  3\n",
                                    "Target Policy [0.      0.      0.02125 0.      0.      0.935   0.02125 0.      0.0225 ]\n",
                                    "Temperature Policy  [3.6768271e-17 1.0000000e+00 3.6768271e-17 6.5119205e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.42500001 0.         0.         0.\n",
                                    " 0.47       0.         0.105     ]\n",
                                    "Temperature Policy  [2.6767796e-01 7.3232174e-01 2.2678606e-07]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.96749997 0.         0.         0.\n",
                                    " 0.         0.         0.0325    ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.8294641e-15]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.115   0.11375 0.105   0.09875 0.145   0.105   0.11125 0.0975  0.10875]\n",
                                    "Temperature Policy  [0.115   0.11375 0.105   0.09875 0.145   0.105   0.11125 0.0975  0.10875]\n",
                                    "Action  7\n",
                                    "Target Policy [0.09875    0.16625001 0.1075     0.08       0.23625    0.08625\n",
                                    " 0.09       0.         0.13500001]\n",
                                    "Temperature Policy  [0.09875 0.16625 0.1075  0.08    0.23625 0.08625 0.09    0.135  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.06875    0.065      0.06625    0.08125    0.         0.08875\n",
                                    " 0.52249998 0.         0.1075    ]\n",
                                    "Temperature Policy  [0.06875 0.065   0.06625 0.08125 0.08875 0.5225  0.1075 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.01625    0.0175     0.025      0.02625    0.         0.01875\n",
                                    " 0.         0.         0.89625001]\n",
                                    "Temperature Policy  [0.01625 0.0175  0.025   0.02625 0.01875 0.89625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.02       0.01625    0.         0.02375    0.         0.02125\n",
                                    " 0.         0.         0.91874999]\n",
                                    "Temperature Policy  [2.3896068e-17 2.9961266e-18 1.3324859e-16 4.3814295e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.115   0.1125  0.10375 0.0975  0.1525  0.1025  0.1125  0.0975  0.10625]\n",
                                    "Temperature Policy  [0.115   0.1125  0.10375 0.0975  0.1525  0.1025  0.1125  0.0975  0.10625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0875     0.17       0.105      0.08125    0.23625    0.09375\n",
                                    " 0.09125    0.         0.13500001]\n",
                                    "Temperature Policy  [0.0875  0.17    0.105   0.08125 0.23625 0.09375 0.09125 0.135  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.055      0.05       0.04875    0.05125    0.69875002 0.05\n",
                                    " 0.04625    0.         0.        ]\n",
                                    "Temperature Policy  [0.055   0.05    0.04875 0.05125 0.69875 0.05    0.04625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.02125    0.88749999 0.01875    0.03       0.         0.0225\n",
                                    " 0.02       0.         0.        ]\n",
                                    "Temperature Policy  [0.02125 0.8875  0.01875 0.03    0.0225  0.02   ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.045      0.         0.40000001 0.10125    0.         0.41999999\n",
                                    " 0.03375    0.         0.        ]\n",
                                    "Temperature Policy  [1.2352415e-10 3.8038784e-01 4.1074949e-07 6.1961174e-01 6.9560789e-12]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02125 0.      0.0225  0.935   0.      0.      0.02125 0.      0.     ]\n",
                                    "Temperature Policy  [3.6768271e-17 6.5119205e-17 1.0000000e+00 3.6768271e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.13       0.         0.43125001 0.         0.         0.\n",
                                    " 0.43875    0.         0.        ]\n",
                                    "Temperature Policy  [2.8317786e-06 4.5700067e-01 5.4299653e-01]\n",
                                    "Action  2\n",
                                    "Target Policy [0.03       0.         0.         0.         0.         0.\n",
                                    " 0.97000003 0.         0.        ]\n",
                                    "Temperature Policy  [8.0074683e-16 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.115   0.1125  0.10375 0.0975  0.14375 0.105   0.11125 0.1     0.11125]\n",
                                    "Temperature Policy  [0.115   0.1125  0.10375 0.0975  0.14375 0.105   0.11125 0.1     0.11125]\n",
                                    "Action  3\n",
                                    "Target Policy [0.11       0.115      0.08       0.         0.29374999 0.17375\n",
                                    " 0.07875    0.07       0.07875   ]\n",
                                    "Temperature Policy  [0.11    0.115   0.08    0.29375 0.17375 0.07875 0.07    0.07875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.27875    0.05375    0.0525     0.         0.0625     0.\n",
                                    " 0.44999999 0.0475     0.055     ]\n",
                                    "Temperature Policy  [0.27875 0.05375 0.0525  0.0625  0.45    0.0475  0.055  ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.51375002 0.1025     0.         0.         0.09625    0.\n",
                                    " 0.14875001 0.06125    0.0775    ]\n",
                                    "Temperature Policy  [0.51375 0.1025  0.09625 0.14875 0.06125 0.0775 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0425     0.         0.         0.69749999 0.\n",
                                    " 0.1075     0.04875    0.10375   ]\n",
                                    "Temperature Policy  [7.0541907e-13 1.0000000e+00 7.5621021e-09 2.7816603e-12 5.3019904e-09]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.02375    0.         0.         0.         0.\n",
                                    " 0.92750001 0.02375    0.025     ]\n",
                                    "Temperature Policy  [1.2119842e-16 1.0000000e+00 1.2119842e-16 2.0242348e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.45375001 0.         0.         0.         0.\n",
                                    " 0.         0.435      0.11125   ]\n",
                                    "Temperature Policy  [6.0396224e-01 3.9603719e-01 4.7407127e-07]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.         0.97000003 0.03      ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.0074683e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.11625    0.1125     0.105      0.0975     0.14749999 0.10375\n",
                                    " 0.1125     0.09625    0.10875   ]\n",
                                    "Temperature Policy  [0.11625 0.1125  0.105   0.0975  0.1475  0.10375 0.1125  0.09625 0.10875]\n",
                                    "Action  3\n",
                                    "Target Policy [0.11       0.115      0.07875    0.         0.29249999 0.1725\n",
                                    " 0.07875    0.075      0.0775    ]\n",
                                    "Temperature Policy  [0.11    0.115   0.07875 0.2925  0.1725  0.07875 0.075   0.0775 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.29624999 0.0575     0.0475     0.         0.06       0.\n",
                                    " 0.4375     0.05       0.05125   ]\n",
                                    "Temperature Policy  [0.29625 0.0575  0.0475  0.06    0.4375  0.05    0.05125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.115      0.045      0.045      0.         0.0575     0.\n",
                                    " 0.68000001 0.0575     0.        ]\n",
                                    "Temperature Policy  [0.115  0.045  0.045  0.0575 0.68   0.0575]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0925     0.0575     0.         0.0475     0.\n",
                                    " 0.41749999 0.38499999 0.        ]\n",
                                    "Temperature Policy  [1.9727966e-07 1.6995872e-09 2.5153657e-10 6.9219714e-01 3.0780262e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.025   0.02125 0.      0.02125 0.      0.      0.9325  0.     ]\n",
                                    "Temperature Policy  [1.9182787e-16 3.7765996e-17 3.7765996e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.1025     0.41999999 0.         0.47749999 0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.6264971e-07 2.1702389e-01 7.8297591e-01]\n",
                                    "Action  2\n",
                                    "Target Policy [0.      0.02625 0.      0.      0.97375 0.      0.      0.      0.     ]\n",
                                    "Temperature Policy  [2.0268348e-16 1.0000000e+00]\n",
                                    "Action  4\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.115      0.115      0.1025     0.0975     0.14749999 0.105\n",
                                    " 0.1125     0.09625    0.10875   ]\n",
                                    "Temperature Policy  [0.115   0.115   0.1025  0.0975  0.1475  0.105   0.1125  0.09625 0.10875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.1825     0.12375    0.125      0.12       0.         0.13500001\n",
                                    " 0.115      0.08375    0.115     ]\n",
                                    "Temperature Policy  [0.1825  0.12375 0.125   0.12    0.135   0.115   0.08375 0.115  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.38999999 0.05125    0.3425     0.         0.05125\n",
                                    " 0.06875    0.05       0.04625   ]\n",
                                    "Temperature Policy  [0.39    0.05125 0.3425  0.05125 0.06875 0.05    0.04625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.0175     0.02375    0.         0.         0.88999999\n",
                                    " 0.0175     0.02625    0.025     ]\n",
                                    "Temperature Policy  [0.0175  0.02375 0.89    0.0175  0.02625 0.025  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.40625 0.4025  0.      0.      0.      0.0375  0.10875 0.045  ]\n",
                                    "Temperature Policy  [5.2316695e-01 4.7683200e-01 2.3497369e-11 9.8855128e-07 1.4548952e-10]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.0275     0.         0.         0.         0.\n",
                                    " 0.92624998 0.02375    0.0225    ]\n",
                                    "Temperature Policy  [5.3216306e-16 1.0000000e+00 1.2284400e-16 7.1539020e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.43125001 0.         0.         0.         0.\n",
                                    " 0.         0.44       0.12875   ]\n",
                                    "Temperature Policy  [4.4995016e-01 5.5004734e-01 2.5313120e-06]\n",
                                    "Action  7\n",
                                    "Target Policy [0.      0.96875 0.      0.      0.      0.      0.      0.      0.03125]\n",
                                    "Temperature Policy  [1.0000000e+00 1.2200653e-15]\n",
                                    "Action  1\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.11625 0.11375 0.10375 0.0975  0.145   0.105   0.11125 0.09875 0.10875]\n",
                                    "Temperature Policy  [0.11625 0.11375 0.10375 0.0975  0.145   0.105   0.11125 0.09875 0.10875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.12625    0.12625    0.         0.07875    0.22750001 0.14875001\n",
                                    " 0.1025     0.07875    0.11125   ]\n",
                                    "Temperature Policy  [0.12625 0.12625 0.07875 0.2275  0.14875 0.1025  0.07875 0.11125]\n",
                                    "Action  8\n",
                                    "Target Policy [0.89625001 0.01375    0.         0.0225     0.02125    0.01625\n",
                                    " 0.01375    0.01625    0.        ]\n",
                                    "Temperature Policy  [0.89625 0.01375 0.0225  0.02125 0.01625 0.01375 0.01625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.4725  0.      0.10625 0.105   0.10625 0.1225  0.0875  0.     ]\n",
                                    "Temperature Policy  [0.4725  0.10625 0.105   0.10625 0.1225  0.0875 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.0225     0.01625    0.02125\n",
                                    " 0.92374998 0.01625    0.        ]\n",
                                    "Temperature Policy  [7.3498875e-17 2.8378482e-18 4.1499688e-17 1.0000000e+00 2.8378482e-18]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.33000001 0.36125001 0.11875\n",
                                    " 0.         0.19       0.        ]\n",
                                    "Temperature Policy  [2.8773522e-01 7.1110243e-01 1.0475863e-05 1.1518333e-03]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.96249998 0.         0.01625\n",
                                    " 0.         0.02125    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.8815956e-18 2.7515789e-17]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.115   0.1125  0.10125 0.1     0.1525  0.10375 0.11125 0.095   0.10875]\n",
                                    "Temperature Policy  [0.115   0.1125  0.10125 0.1     0.1525  0.10375 0.11125 0.095   0.10875]\n",
                                    "Action  7\n",
                                    "Target Policy [0.08625    0.17125    0.1075     0.08125    0.23625    0.08875\n",
                                    " 0.09375    0.         0.13500001]\n",
                                    "Temperature Policy  [0.08625 0.17125 0.1075  0.08125 0.23625 0.08875 0.09375 0.135  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.05375    0.         0.05125    0.055      0.065      0.0575\n",
                                    " 0.57875001 0.         0.13875   ]\n",
                                    "Temperature Policy  [0.05375 0.05125 0.055   0.065   0.0575  0.57875 0.13875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.70625001 0.         0.115      0.045      0.         0.045\n",
                                    " 0.04875    0.         0.04      ]\n",
                                    "Temperature Policy  [0.70625 0.115   0.045   0.045   0.04875 0.04   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.90750003 0.03       0.         0.02375\n",
                                    " 0.0175     0.         0.02125   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.5586413e-15 1.5071963e-16 7.1107299e-18 4.9559054e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.025      0.         0.02625\n",
                                    " 0.92500001 0.         0.02375   ]\n",
                                    "Temperature Policy  [2.0796141e-16 3.3874724e-16 1.0000000e+00 1.2451419e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.95125002 0.         0.02125\n",
                                    " 0.         0.         0.0275    ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.0948718e-17 4.0773710e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.97125 0.      0.      0.02875]\n",
                                    "Temperature Policy  [1.000000e+00 5.164974e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.115   0.1125  0.105   0.1     0.145   0.10375 0.1125  0.09875 0.1075 ]\n",
                                    "Temperature Policy  [0.115   0.1125  0.105   0.1     0.145   0.10375 0.1125  0.09875 0.1075 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.13249999 0.08625    0.0875     0.19499999 0.1275\n",
                                    " 0.15125    0.0925     0.1275    ]\n",
                                    "Temperature Policy  [0.1325  0.08625 0.0875  0.195   0.1275  0.15125 0.0925  0.1275 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.07375    0.65750003 0.05125    0.05875    0.055\n",
                                    " 0.         0.0475     0.05625   ]\n",
                                    "Temperature Policy  [0.07375 0.6575  0.05125 0.05875 0.055   0.0475  0.05625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.44999999 0.         0.11       0.11       0.10875\n",
                                    " 0.         0.09625    0.125     ]\n",
                                    "Temperature Policy  [0.45    0.11    0.11    0.10875 0.09625 0.125  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.02125    0.02       0.02125\n",
                                    " 0.         0.01875    0.91874999]\n",
                                    "Temperature Policy  [4.3814295e-17 2.3896068e-17 4.3814295e-17 1.2532543e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.         0.1125     0.38124999 0.32249999\n",
                                    " 0.         0.18375    0.        ]\n",
                                    "Temperature Policy  [4.2122374e-06 8.4156203e-01 1.5786462e-01 5.6919502e-04]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.02125    0.         0.95875001\n",
                                    " 0.         0.02       0.        ]\n",
                                    "Temperature Policy  [2.8611168e-17 1.0000000e+00 1.5604369e-17]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.115   0.115   0.1025  0.1     0.145   0.10375 0.1125  0.09875 0.1075 ]\n",
                                    "Temperature Policy  [0.115   0.115   0.1025  0.1     0.145   0.10375 0.1125  0.09875 0.1075 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.13375001 0.1275     0.09       0.2        0.13\n",
                                    " 0.1        0.09125    0.1275    ]\n",
                                    "Temperature Policy  [0.13375 0.1275  0.09    0.2     0.13    0.1     0.09125 0.1275 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.01625 0.02    0.02    0.055   0.      0.84875 0.02125 0.01875]\n",
                                    "Temperature Policy  [0.01625 0.02    0.02    0.055   0.84875 0.02125 0.01875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.07875    0.1025     0.51249999 0.12125    0.\n",
                                    " 0.         0.0875     0.0975    ]\n",
                                    "Temperature Policy  [0.07875 0.1025  0.5125  0.12125 0.0875  0.0975 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.01125    0.01375    0.94749999 0.         0.\n",
                                    " 0.         0.0125     0.015     ]\n",
                                    "Temperature Policy  [5.5684338e-20 4.1422356e-19 1.0000000e+00 1.5970112e-19 9.8882721e-19]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x107dd1cc0>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x3286f5600>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x32d854fa0>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x107e21690>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x107e21ae0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x325bf4160>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3321cd2d0>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32c60b8b0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c1480>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x32c60b8b0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c1480>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x32ece4640>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x3321b6ad0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x32d854460>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x3286f5600>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x3286bab00>, 2)]\n",
                                    "Observations [array([[[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 1.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]])]\n",
                                    "Policies [array([0.105  , 0.     , 0.09375, 0.1    , 0.     , 0.     , 0.09375,\n",
                                    "       0.49625, 0.11125]), array([0.     , 0.96875, 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
                                    "       0.     , 0.03125]), array([0.     , 0.     , 0.115  , 0.1175 , 0.08875, 0.06875, 0.405  ,\n",
                                    "       0.     , 0.205  ]), array([0.03375   , 0.08375   , 0.03625   , 0.0875    , 0.        ,\n",
                                    "       0.69875002, 0.03125   , 0.02875   , 0.        ]), array([0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0.        , 0.41125   , 0.03125   , 0.38874999, 0.        ,\n",
                                    "       0.04375   , 0.03      , 0.0675    , 0.0275    ]), array([0.10125   , 0.0575    , 0.0525    , 0.06      , 0.44624999,\n",
                                    "       0.06875   , 0.1125    , 0.04375   , 0.0575    ]), array([0.        , 0.03875   , 0.72750002, 0.        , 0.04625   ,\n",
                                    "       0.        , 0.        , 0.0375    , 0.15000001]), array([0.0175    , 0.015     , 0.        , 0.02875   , 0.        ,\n",
                                    "       0.93875003, 0.        , 0.        , 0.        ]), array([0., 0., 0., 0., 0., 0., 0., 1., 0.]), array([0.02125   , 0.01625   , 0.        , 0.96249998, 0.        ,\n",
                                    "       0.        , 0.        , 0.        , 0.        ]), array([0.1       , 0.05375   , 0.07125   , 0.0475    , 0.50999999,\n",
                                    "       0.055     , 0.0625    , 0.0425    , 0.0575    ]), array([0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0.17749999, 0.1175    , 0.1       , 0.09      , 0.10875   ,\n",
                                    "       0.0925    , 0.1075    , 0.0975    , 0.10875   ]), array([0.     , 0.96875, 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
                                    "       0.     , 0.03125]), array([0.     , 0.035  , 0.0125 , 0.04   , 0.025  , 0.01375, 0.84875,\n",
                                    "       0.025  , 0.     ])]\n",
                                    "NP Array Policies [[0.105      0.         0.09375    0.1        0.         0.\n",
                                    "  0.09375    0.49625    0.11125   ]\n",
                                    " [0.         0.96875    0.         0.         0.         0.\n",
                                    "  0.         0.         0.03125   ]\n",
                                    " [0.         0.         0.115      0.1175     0.08875    0.06875\n",
                                    "  0.405      0.         0.205     ]\n",
                                    " [0.03375    0.08375    0.03625    0.0875     0.         0.69875002\n",
                                    "  0.03125    0.02875    0.        ]\n",
                                    " [0.         0.         0.         1.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.41125    0.03125    0.38874999 0.         0.04375\n",
                                    "  0.03       0.0675     0.0275    ]\n",
                                    " [0.10125    0.0575     0.0525     0.06       0.44624999 0.06875\n",
                                    "  0.1125     0.04375    0.0575    ]\n",
                                    " [0.         0.03875    0.72750002 0.         0.04625    0.\n",
                                    "  0.         0.0375     0.15000001]\n",
                                    " [0.0175     0.015      0.         0.02875    0.         0.93875003\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         1.         0.        ]\n",
                                    " [0.02125    0.01625    0.         0.96249998 0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.1        0.05375    0.07125    0.0475     0.50999999 0.055\n",
                                    "  0.0625     0.0425     0.0575    ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  1.         0.         0.        ]\n",
                                    " [0.17749999 0.1175     0.1        0.09       0.10875    0.0925\n",
                                    "  0.1075     0.0975     0.10875   ]\n",
                                    " [0.         0.96875    0.         0.         0.         0.\n",
                                    "  0.         0.         0.03125   ]\n",
                                    " [0.         0.035      0.0125     0.04       0.025      0.01375\n",
                                    "  0.84875    0.025      0.        ]]\n",
                                    "Predicted: tensor([[0.0610, 0.0888, 0.0599, 0.3034, 0.0834, 0.1707, 0.0575, 0.0884, 0.0868],\n",
                                    "        [0.0801, 0.1035, 0.0688, 0.2149, 0.0996, 0.1689, 0.0738, 0.0933, 0.0971],\n",
                                    "        [0.0789, 0.1072, 0.0734, 0.2333, 0.0982, 0.1528, 0.0728, 0.0908, 0.0926],\n",
                                    "        [0.1147, 0.0993, 0.1039, 0.1140, 0.0997, 0.1293, 0.0981, 0.1264, 0.1145],\n",
                                    "        [0.1098, 0.0910, 0.0903, 0.1484, 0.1038, 0.1401, 0.0933, 0.1123, 0.1110],\n",
                                    "        [0.1113, 0.1005, 0.1030, 0.1192, 0.1061, 0.1269, 0.0977, 0.1234, 0.1119],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.1108, 0.1012, 0.1010, 0.1237, 0.1108, 0.1290, 0.0956, 0.1198, 0.1081],\n",
                                    "        [0.0826, 0.1063, 0.0729, 0.2189, 0.1014, 0.1619, 0.0734, 0.0914, 0.0912],\n",
                                    "        [0.1056, 0.0929, 0.0931, 0.1363, 0.0934, 0.1368, 0.0923, 0.1317, 0.1179],\n",
                                    "        [0.1043, 0.0977, 0.0924, 0.1523, 0.1010, 0.1228, 0.0920, 0.1292, 0.1082],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.1100, 0.0915, 0.0918, 0.1386, 0.0948, 0.1376, 0.0950, 0.1261, 0.1146],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.0801, 0.1035, 0.0688, 0.2149, 0.0996, 0.1689, 0.0738, 0.0933, 0.0971],\n",
                                    "        [0.1129, 0.1044, 0.1047, 0.1239, 0.1094, 0.1254, 0.0920, 0.1200, 0.1074]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.0610, 0.0888, 0.0599, 0.3034, 0.0834, 0.1707, 0.0575, 0.0884, 0.0868],\n",
                                    "        [0.0801, 0.1035, 0.0688, 0.2149, 0.0996, 0.1689, 0.0738, 0.0933, 0.0971],\n",
                                    "        [0.0789, 0.1072, 0.0734, 0.2333, 0.0982, 0.1528, 0.0728, 0.0908, 0.0926],\n",
                                    "        [0.1147, 0.0993, 0.1039, 0.1140, 0.0997, 0.1293, 0.0981, 0.1264, 0.1145],\n",
                                    "        [0.1098, 0.0910, 0.0903, 0.1484, 0.1038, 0.1401, 0.0933, 0.1123, 0.1110],\n",
                                    "        [0.1113, 0.1005, 0.1030, 0.1192, 0.1061, 0.1269, 0.0977, 0.1234, 0.1119],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.1108, 0.1012, 0.1010, 0.1237, 0.1108, 0.1290, 0.0956, 0.1198, 0.1081],\n",
                                    "        [0.0826, 0.1063, 0.0729, 0.2189, 0.1014, 0.1619, 0.0734, 0.0914, 0.0912],\n",
                                    "        [0.1056, 0.0929, 0.0931, 0.1363, 0.0934, 0.1368, 0.0923, 0.1317, 0.1179],\n",
                                    "        [0.1043, 0.0977, 0.0924, 0.1523, 0.1010, 0.1228, 0.0920, 0.1292, 0.1082],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.1100, 0.0915, 0.0918, 0.1386, 0.0948, 0.1376, 0.0950, 0.1261, 0.1146],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.0801, 0.1035, 0.0688, 0.2149, 0.0996, 0.1689, 0.0738, 0.0933, 0.0971],\n",
                                    "        [0.1129, 0.1044, 0.1047, 0.1239, 0.1094, 0.1254, 0.0920, 0.1200, 0.1074]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.0610, 0.0888, 0.0599, 0.3034, 0.0834, 0.1707, 0.0575, 0.0884, 0.0868],\n",
                                    "        [0.0801, 0.1035, 0.0688, 0.2149, 0.0996, 0.1689, 0.0738, 0.0933, 0.0971],\n",
                                    "        [0.0789, 0.1072, 0.0734, 0.2333, 0.0982, 0.1528, 0.0728, 0.0908, 0.0926],\n",
                                    "        [0.1147, 0.0993, 0.1039, 0.1140, 0.0997, 0.1293, 0.0981, 0.1264, 0.1145],\n",
                                    "        [0.1098, 0.0910, 0.0903, 0.1484, 0.1038, 0.1401, 0.0933, 0.1123, 0.1110],\n",
                                    "        [0.1113, 0.1005, 0.1030, 0.1192, 0.1061, 0.1269, 0.0977, 0.1234, 0.1119],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.1108, 0.1012, 0.1010, 0.1237, 0.1108, 0.1290, 0.0956, 0.1198, 0.1081],\n",
                                    "        [0.0826, 0.1063, 0.0729, 0.2189, 0.1014, 0.1619, 0.0734, 0.0914, 0.0912],\n",
                                    "        [0.1056, 0.0929, 0.0931, 0.1363, 0.0934, 0.1368, 0.0923, 0.1317, 0.1179],\n",
                                    "        [0.1043, 0.0977, 0.0924, 0.1523, 0.1010, 0.1228, 0.0920, 0.1292, 0.1082],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.1100, 0.0915, 0.0918, 0.1386, 0.0948, 0.1376, 0.0950, 0.1261, 0.1146],\n",
                                    "        [0.1143, 0.1041, 0.1083, 0.1171, 0.1108, 0.1257, 0.0940, 0.1170, 0.1088],\n",
                                    "        [0.0801, 0.1035, 0.0688, 0.2149, 0.0996, 0.1689, 0.0738, 0.0933, 0.0971],\n",
                                    "        [0.1129, 0.1044, 0.1047, 0.1239, 0.1094, 0.1254, 0.0920, 0.1200, 0.1074]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.7963, -2.4219, -2.8146, -1.1926, -2.4836, -1.7679, -2.8560, -2.4258,\n",
                                    "         -2.4438],\n",
                                    "        [-2.5246, -2.2686, -2.6758, -1.5378, -2.3063, -1.7786, -2.6059, -2.3721,\n",
                                    "         -2.3318],\n",
                                    "        [-2.5400, -2.2329, -2.6115, -1.4553, -2.3209, -1.8789, -2.6195, -2.3995,\n",
                                    "         -2.3794],\n",
                                    "        [-2.1651, -2.3093, -2.2645, -2.1714, -2.3054, -2.0455, -2.3216, -2.0682,\n",
                                    "         -2.1674],\n",
                                    "        [-2.2092, -2.3968, -2.4045, -1.9079, -2.2655, -1.9652, -2.3718, -2.1869,\n",
                                    "         -2.1980],\n",
                                    "        [-2.1953, -2.2980, -2.2732, -2.1270, -2.2431, -2.0646, -2.3259, -2.0921,\n",
                                    "         -2.1900],\n",
                                    "        [-2.1693, -2.2622, -2.2231, -2.1445, -2.2001, -2.0739, -2.3650, -2.1456,\n",
                                    "         -2.2184],\n",
                                    "        [-2.1999, -2.2902, -2.2928, -2.0900, -2.2003, -2.0476, -2.3480, -2.1221,\n",
                                    "         -2.2247],\n",
                                    "        [-2.4932, -2.2419, -2.6184, -1.5192, -2.2886, -1.8209, -2.6117, -2.3922,\n",
                                    "         -2.3952],\n",
                                    "        [-2.2478, -2.3764, -2.3741, -1.9931, -2.3707, -1.9891, -2.3830, -2.0273,\n",
                                    "         -2.1377],\n",
                                    "        [-2.2609, -2.3254, -2.3817, -1.8818, -2.2922, -2.0970, -2.3857, -2.0467,\n",
                                    "         -2.2235],\n",
                                    "        [-2.1693, -2.2622, -2.2231, -2.1445, -2.2001, -2.0739, -2.3650, -2.1456,\n",
                                    "         -2.2184],\n",
                                    "        [-2.2069, -2.3916, -2.3887, -1.9760, -2.3560, -1.9837, -2.3537, -2.0704,\n",
                                    "         -2.1665],\n",
                                    "        [-2.1693, -2.2622, -2.2231, -2.1445, -2.2001, -2.0739, -2.3650, -2.1456,\n",
                                    "         -2.2184],\n",
                                    "        [-2.5246, -2.2686, -2.6758, -1.5378, -2.3063, -1.7786, -2.6059, -2.3721,\n",
                                    "         -2.3318],\n",
                                    "        [-2.1812, -2.2598, -2.2566, -2.0881, -2.2125, -2.0764, -2.3864, -2.1204,\n",
                                    "         -2.2317]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.23467308 2.1795204 2.4141932\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "score:  0\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_6/videos/alphazero/6/alphazero-episode-29.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_6/videos/alphazero/6/alphazero-episode-29.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                           \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_6/videos/alphazero/6/alphazero-episode-29.mp4\n",
                                    "score:  0\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.1175     0.11       0.08125    0.0775     0.24124999 0.0975\n",
                                    " 0.11       0.05375    0.11125   ]\n",
                                    "Temperature Policy  [0.1175  0.11    0.08125 0.0775  0.24125 0.0975  0.11    0.05375 0.11125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.07875 0.0825  0.11    0.3475  0.07875 0.14125 0.07875 0.0825 ]\n",
                                    "Temperature Policy  [0.07875 0.0825  0.11    0.3475  0.07875 0.14125 0.07875 0.0825 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.47499999 0.0775     0.16125    0.         0.06125\n",
                                    " 0.1025     0.0575     0.065     ]\n",
                                    "Temperature Policy  [0.475   0.0775  0.16125 0.06125 0.1025  0.0575  0.065  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.89625001 0.0275     0.         0.01875\n",
                                    " 0.0175     0.02       0.02      ]\n",
                                    "Temperature Policy  [0.89625 0.0275  0.01875 0.0175  0.02    0.02   ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.0275     0.         0.02125\n",
                                    " 0.91000003 0.02125    0.02      ]\n",
                                    "Temperature Policy  [6.3520372e-16 4.8214254e-17 1.0000000e+00 4.8214254e-17 2.6295781e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.93624997 0.         0.02875\n",
                                    " 0.         0.01875    0.01625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.4552155e-16 1.0377541e-17 2.4809349e-18]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.94875002\n",
                                    " 0.         0.0275     0.02375   ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.1860948e-16 9.6631398e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.         0.46125001 0.53874999]\n",
                                    "Temperature Policy  [0.17463689 0.8253631 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.1125  0.105   0.08875 0.08125 0.2375  0.09    0.1125  0.065   0.1075 ]\n",
                                    "Temperature Policy  [0.1125  0.105   0.08875 0.08125 0.2375  0.09    0.1125  0.065   0.1075 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0975  0.      0.09875 0.0725  0.34875 0.08375 0.09875 0.09625 0.10375]\n",
                                    "Temperature Policy  [0.0975  0.09875 0.0725  0.34875 0.08375 0.09875 0.09625 0.10375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01625    0.         0.01875    0.02125    0.89499998 0.\n",
                                    " 0.015      0.0175     0.01625   ]\n",
                                    "Temperature Policy  [0.01625 0.01875 0.02125 0.895   0.015   0.0175  0.01625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.10875    0.         0.11625    0.10375    0.         0.\n",
                                    " 0.10375    0.46125001 0.10625   ]\n",
                                    "Temperature Policy  [0.10875 0.11625 0.10375 0.10375 0.46125 0.10625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.91250002 0.         0.02       0.0225     0.         0.\n",
                                    " 0.02125    0.         0.02375   ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.5584162e-17 8.3079989e-17 4.6909479e-17 1.4266170e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.36500001 0.1        0.         0.\n",
                                    " 0.1525     0.         0.38249999]\n",
                                    "Temperature Policy  [3.8499126e-01 9.1732284e-07 6.2405183e-05 6.1494547e-01]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.0175     0.         0.\n",
                                    " 0.0175     0.         0.96499997]\n",
                                    "Temperature Policy  [3.8468688e-18 3.8468688e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.1125  0.12    0.08    0.07875 0.245   0.08875 0.11125 0.05625 0.1075 ]\n",
                                    "Temperature Policy  [0.1125  0.12    0.08    0.07875 0.245   0.08875 0.11125 0.05625 0.1075 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.1425  0.0975  0.      0.07    0.2825  0.09375 0.09375 0.0975  0.1225 ]\n",
                                    "Temperature Policy  [0.1425  0.0975  0.07    0.2825  0.09375 0.09375 0.0975  0.1225 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01875    0.0175     0.         0.0175     0.01625    0.01625\n",
                                    " 0.015      0.         0.89875001]\n",
                                    "Temperature Policy  [0.01875 0.0175  0.0175  0.01625 0.01625 0.015   0.89875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.08375    0.1025     0.         0.09625    0.10875    0.52125001\n",
                                    " 0.0875     0.         0.        ]\n",
                                    "Temperature Policy  [0.08375 0.1025  0.09625 0.10875 0.52125 0.0875 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.88999999 0.05       0.         0.02       0.0225     0.\n",
                                    " 0.0175     0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.1318429e-13 3.2839752e-17 1.0664122e-16 8.6393374e-18]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.375      0.         0.1525     0.38874999 0.\n",
                                    " 0.08375    0.         0.        ]\n",
                                    "Temperature Policy  [4.1091335e-01 5.0831863e-05 5.8903563e-01 1.2684862e-07]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.96749997 0.         0.0175     0.         0.\n",
                                    " 0.015      0.         0.        ]\n",
                                    "Temperature Policy  [1.000000e+00 3.748614e-18 8.024220e-19]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.1125  0.11375 0.06375 0.08    0.25375 0.09375 0.11375 0.06125 0.1075 ]\n",
                                    "Temperature Policy  [0.1125  0.11375 0.06375 0.08    0.25375 0.09375 0.11375 0.06125 0.1075 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.15375 0.13    0.1225  0.13    0.      0.14375 0.115   0.0825  0.1225 ]\n",
                                    "Temperature Policy  [0.15375 0.13    0.1225  0.13    0.14375 0.115   0.0825  0.1225 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.065      0.05       0.0325     0.38499999 0.         0.06625\n",
                                    " 0.         0.35499999 0.04625   ]\n",
                                    "Temperature Policy  [0.065   0.05    0.0325  0.385   0.06625 0.355   0.04625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02125    0.02625    0.02       0.         0.         0.89375001\n",
                                    " 0.         0.01875    0.02      ]\n",
                                    "Temperature Policy  [0.02125 0.02625 0.02    0.89375 0.01875 0.02   ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.03375    0.105      0.0375     0.         0.         0.\n",
                                    " 0.         0.4075     0.41624999]\n",
                                    "Temperature Policy  [6.7897311e-12 5.7677681e-07 1.9472758e-11 4.4708586e-01 5.5291361e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.92874998 0.02375    0.02375    0.         0.         0.\n",
                                    " 0.         0.02375    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.1957707e-16 1.1957707e-16 1.1957707e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.47999999 0.11375    0.         0.         0.\n",
                                    " 0.         0.40625    0.        ]\n",
                                    "Temperature Policy  [8.4133184e-01 4.6996874e-07 1.5866776e-01]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0275     0.         0.         0.\n",
                                    " 0.         0.97250003 0.        ]\n",
                                    "Temperature Policy  [3.2691197e-16 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.11875 0.1125  0.07    0.08375 0.2375  0.09625 0.1125  0.065   0.10375]\n",
                                    "Temperature Policy  [0.11875 0.1125  0.07    0.08375 0.2375  0.09625 0.1125  0.065   0.10375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.14749999 0.13124999 0.1225     0.13       0.         0.145\n",
                                    " 0.115      0.0825     0.12625   ]\n",
                                    "Temperature Policy  [0.1475  0.13125 0.1225  0.13    0.145   0.115   0.0825  0.12625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01625    0.89875001 0.01375    0.02125    0.         0.\n",
                                    " 0.01625    0.0175     0.01625   ]\n",
                                    "Temperature Policy  [0.01625 0.89875 0.01375 0.02125 0.01625 0.0175  0.01625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.105      0.45249999 0.12375    0.1075     0.         0.\n",
                                    " 0.10375    0.         0.1075    ]\n",
                                    "Temperature Policy  [0.105   0.4525  0.12375 0.1075  0.10375 0.1075 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.02375    0.         0.02       0.0225     0.         0.\n",
                                    " 0.02375    0.         0.91000003]\n",
                                    "Temperature Policy  [1.466298e-16 2.629578e-17 8.539084e-17 1.466298e-16 1.000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.38999999 0.         0.205      0.10875    0.         0.\n",
                                    " 0.29624999 0.         0.        ]\n",
                                    "Temperature Policy  [9.3845838e-01 1.5111499e-03 2.6672369e-06 6.0027815e-02]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.02125    0.02125    0.         0.\n",
                                    " 0.95749998 0.         0.        ]\n",
                                    "Temperature Policy  [2.8986884e-17 2.8986884e-17 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.11375 0.11375 0.06375 0.085   0.24625 0.095   0.11    0.06875 0.10375]\n",
                                    "Temperature Policy  [0.11375 0.11375 0.06375 0.085   0.24625 0.095   0.11    0.06875 0.10375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0975     0.         0.10125    0.07375    0.35499999 0.085\n",
                                    " 0.0975     0.085      0.105     ]\n",
                                    "Temperature Policy  [0.0975  0.10125 0.07375 0.355   0.085   0.0975  0.085   0.105  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.07375    0.         0.52999997 0.0525     0.17874999 0.055\n",
                                    " 0.0575     0.0525     0.        ]\n",
                                    "Temperature Policy  [0.07375 0.53    0.0525  0.17875 0.055   0.0575  0.0525 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.93124998 0.01875    0.0125     0.01375\n",
                                    " 0.01125    0.0125     0.        ]\n",
                                    "Temperature Policy  [0.93125 0.01875 0.0125  0.01375 0.01125 0.0125 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.1375     0.1575     0.44\n",
                                    " 0.13375001 0.13124999 0.        ]\n",
                                    "Temperature Policy  [8.8812894e-06 3.4534612e-05 9.9994427e-01 6.7357651e-06 5.5775331e-06]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.      0.      0.02375 0.0175  0.      0.9375  0.02125 0.     ]\n",
                                    "Temperature Policy  [1.0887380e-16 5.1365050e-18 1.0000000e+00 3.5799467e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.11125    0.42875001 0.\n",
                                    " 0.         0.46000001 0.        ]\n",
                                    "Temperature Policy  [4.5796000e-07 3.3103102e-01 6.6896850e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.         0.0225     0.97750002 0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.1750102e-17 1.0000000e+00]\n",
                                    "Action  4\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [-1, 1]]\n",
                                    "Updated Rewards [-1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.1125  0.115   0.085   0.08375 0.2375  0.095   0.11125 0.05375 0.10625]\n",
                                    "Temperature Policy  [0.1125  0.115   0.085   0.08375 0.2375  0.095   0.11125 0.05375 0.10625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0975  0.      0.09875 0.07375 0.3475  0.08375 0.10375 0.09125 0.10375]\n",
                                    "Temperature Policy  [0.0975  0.09875 0.07375 0.3475  0.08375 0.10375 0.09125 0.10375]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0525  0.      0.      0.0425  0.71875 0.0425  0.03875 0.05625 0.04875]\n",
                                    "Temperature Policy  [0.0525  0.0425  0.71875 0.0425  0.03875 0.05625 0.04875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.02625    0.         0.         0.025      0.         0.02125\n",
                                    " 0.02       0.88499999 0.0225    ]\n",
                                    "Temperature Policy  [0.02625 0.025   0.02125 0.02    0.885   0.0225 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.03375 0.      0.      0.0975  0.      0.40125 0.065   0.      0.4025 ]\n",
                                    "Temperature Policy  [8.7246989e-12 3.5323100e-07 4.9222437e-01 6.1255663e-09 5.0777525e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.93124998 0.         0.         0.0275     0.         0.0225\n",
                                    " 0.01875    0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 5.0427111e-16 6.7789489e-17 1.0948381e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.         0.47874999 0.         0.38249999\n",
                                    " 0.13875    0.         0.        ]\n",
                                    "Temperature Policy  [9.0417153e-01 9.5824674e-02 3.7800096e-06]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.97624999\n",
                                    " 0.02375    0.         0.        ]\n",
                                    "Temperature Policy  [1.000000e+00 7.261489e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.1125  0.1125  0.07875 0.0825  0.23625 0.09375 0.1125  0.065   0.10625]\n",
                                    "Temperature Policy  [0.1125  0.1125  0.07875 0.0825  0.23625 0.09375 0.1125  0.065   0.10625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0625     0.075      0.06625    0.         0.47       0.13500001\n",
                                    " 0.0675     0.06       0.06375   ]\n",
                                    "Temperature Policy  [0.0625  0.075   0.06625 0.47    0.135   0.0675  0.06    0.06375]\n",
                                    "Action  2\n",
                                    "Target Policy [0.77249998 0.03125    0.         0.         0.06875    0.0625\n",
                                    " 0.0225     0.0175     0.025     ]\n",
                                    "Temperature Policy  [0.7725  0.03125 0.06875 0.0625  0.0225  0.0175  0.025  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.095      0.         0.         0.0925     0.1125\n",
                                    " 0.50749999 0.09125    0.10125   ]\n",
                                    "Temperature Policy  [0.095   0.0925  0.1125  0.5075  0.09125 0.10125]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.035      0.0275\n",
                                    " 0.90249997 0.02       0.015     ]\n",
                                    "Temperature Policy  [7.6949953e-15 6.9000913e-16 1.0000000e+00 2.8564581e-17 1.6085719e-18]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.12875 0.1125  0.06375 0.0775  0.2475  0.09125 0.11    0.065   0.10375]\n",
                                    "Temperature Policy  [0.12875 0.1125  0.06375 0.0775  0.2475  0.09125 0.11    0.065   0.10375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.15625    0.13124999 0.1225     0.12       0.         0.145\n",
                                    " 0.11625    0.0825     0.12625   ]\n",
                                    "Temperature Policy  [0.15625 0.13125 0.1225  0.12    0.145   0.11625 0.0825  0.12625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.44       0.02625    0.36750001 0.         0.04375\n",
                                    " 0.03125    0.0575     0.03375   ]\n",
                                    "Temperature Policy  [0.44    0.02625 0.3675  0.04375 0.03125 0.0575  0.03375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.025      0.01875    0.         0.         0.89249998\n",
                                    " 0.01875    0.025      0.02      ]\n",
                                    "Temperature Policy  [0.025   0.01875 0.8925  0.01875 0.025   0.02   ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.41125    0.42250001 0.         0.         0.\n",
                                    " 0.0325     0.0925     0.04125   ]\n",
                                    "Temperature Policy  [4.3293616e-01 5.6706369e-01 4.1133754e-12 1.4347692e-07 4.4627677e-11]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.01875 0.      0.      0.      0.02125 0.9325  0.0275 ]\n",
                                    "Temperature Policy  [1.08025015e-17 3.77659963e-17 1.00000000e+00 4.97552077e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.4075     0.         0.         0.\n",
                                    " 0.45875001 0.         0.13375001]\n",
                                    "Temperature Policy  [2.3421729e-01 7.6577926e-01 3.3984488e-06]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.97250003 0.         0.         0.\n",
                                    " 0.         0.         0.0275    ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.2691197e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.11375 0.115   0.08125 0.08125 0.245   0.08625 0.115   0.05375 0.10875]\n",
                                    "Temperature Policy  [0.11375 0.115   0.08125 0.08125 0.245   0.08625 0.115   0.05375 0.10875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.0975  0.      0.09875 0.0725  0.345   0.08375 0.09625 0.1025  0.10375]\n",
                                    "Temperature Policy  [0.0975  0.09875 0.0725  0.345   0.08375 0.09625 0.1025  0.10375]\n",
                                    "Action  5\n",
                                    "Target Policy [0.015      0.         0.015      0.02375    0.89499998 0.\n",
                                    " 0.0175     0.01625    0.0175    ]\n",
                                    "Temperature Policy  [0.015   0.015   0.02375 0.895   0.0175  0.01625 0.0175 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.0975  0.      0.1175  0.10125 0.      0.      0.1025  0.46625 0.115  ]\n",
                                    "Temperature Policy  [0.0975  0.1175  0.10125 0.1025  0.46625 0.115  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.89749998 0.         0.04375    0.02       0.         0.\n",
                                    " 0.0175     0.         0.02125   ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.5759311e-14 3.0196414e-17 7.9439399e-18 5.5366206e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.36625001 0.1        0.         0.\n",
                                    " 0.155      0.         0.37875   ]\n",
                                    "Temperature Policy  [4.1684577e-01 9.5984046e-07 7.6827382e-05 5.8307642e-01]\n",
                                    "Action  2\n",
                                    "Target Policy [0.      0.      0.      0.0175  0.      0.      0.01625 0.      0.96625]\n",
                                    "Temperature Policy  [3.7973922e-18 1.8098333e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.11375    0.1125     0.0725     0.07875    0.25624999 0.09625\n",
                                    " 0.11       0.0525     0.1075    ]\n",
                                    "Temperature Policy  [0.11375 0.1125  0.0725  0.07875 0.25625 0.09625 0.11    0.0525  0.1075 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0625  0.07    0.0675  0.      0.4725  0.13625 0.0675  0.06    0.06375]\n",
                                    "Temperature Policy  [0.0625  0.07    0.0675  0.4725  0.13625 0.0675  0.06    0.06375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.16125    0.09       0.0575     0.         0.         0.06125\n",
                                    " 0.50625002 0.0675     0.05625   ]\n",
                                    "Temperature Policy  [0.16125 0.09    0.0575  0.06125 0.50625 0.0675  0.05625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.05625    0.63       0.         0.         0.         0.05125\n",
                                    " 0.05125    0.16625001 0.045     ]\n",
                                    "Temperature Policy  [0.05625 0.63    0.05125 0.05125 0.16625 0.045  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.025      0.         0.         0.         0.         0.0225\n",
                                    " 0.02125    0.91374999 0.0175    ]\n",
                                    "Temperature Policy  [2.3503158e-16 8.1950444e-17 4.6271703e-17 1.0000000e+00 6.6390604e-18]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02625    0.         0.         0.         0.         0.02875\n",
                                    " 0.06125    0.         0.88375002]\n",
                                    "Temperature Policy  [5.3456292e-16 1.3276519e-15 2.5572116e-12 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.95249999 0.         0.         0.         0.         0.02625\n",
                                    " 0.02125    0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.5272210e-16 3.0544954e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.02875 0.97125 0.      0.     ]\n",
                                    "Temperature Policy  [5.164974e-16 1.000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.115   0.105   0.07875 0.08625 0.255   0.08625 0.11375 0.0525  0.1075 ]\n",
                                    "Temperature Policy  [0.115   0.105   0.07875 0.08625 0.255   0.08625 0.11375 0.0525  0.1075 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0625  0.07    0.065   0.      0.4725  0.13625 0.0675  0.0625  0.06375]\n",
                                    "Temperature Policy  [0.0625  0.07    0.065   0.4725  0.13625 0.0675  0.0625  0.06375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.16125    0.09       0.05625    0.         0.         0.05125\n",
                                    " 0.50875002 0.075      0.0575    ]\n",
                                    "Temperature Policy  [0.16125 0.09    0.05625 0.05125 0.50875 0.075   0.0575 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.67374998 0.         0.05       0.         0.         0.05875\n",
                                    " 0.1225     0.0475     0.0475    ]\n",
                                    "Temperature Policy  [0.67375 0.05    0.05875 0.1225  0.0475  0.0475 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.01875    0.         0.         0.025\n",
                                    " 0.01875    0.02375    0.91374999]\n",
                                    "Temperature Policy  [1.3235455e-17 2.3503158e-16 1.3235455e-17 1.4072208e-16 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.27625    0.         0.         0.07875\n",
                                    " 0.58749998 0.0575     0.        ]\n",
                                    "Temperature Policy  [5.2809826e-04 1.8715332e-09 9.9947190e-01 8.0606549e-11]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.94499999 0.         0.         0.025\n",
                                    " 0.         0.03       0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.6791211e-16 1.0396675e-15]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.97000003\n",
                                    " 0.         0.03       0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 8.0074683e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.1125  0.105   0.07625 0.08375 0.25    0.09625 0.11125 0.055   0.11   ]\n",
                                    "Temperature Policy  [0.1125  0.105   0.07625 0.08375 0.25    0.09625 0.11125 0.055   0.11   ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.16374999 0.08125    0.0775     0.11625    0.32624999 0.07875\n",
                                    " 0.         0.07625    0.08      ]\n",
                                    "Temperature Policy  [0.16375 0.08125 0.0775  0.11625 0.32625 0.07875 0.07625 0.08   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.025      0.01625    0.02       0.0275     0.0175\n",
                                    " 0.         0.0175     0.87625003]\n",
                                    "Temperature Policy  [0.025   0.01625 0.02    0.0275  0.0175  0.0175  0.87625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.12125    0.14375    0.0975     0.10875    0.1075\n",
                                    " 0.         0.42124999 0.        ]\n",
                                    "Temperature Policy  [0.12125 0.14375 0.0975  0.10875 0.1075  0.42125]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.01875    0.91874999 0.0225     0.0175     0.0225\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.2532543e-17 1.0000000e+00 7.7598204e-17 6.2864722e-18 7.7598204e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.185      0.         0.10625    0.39625001 0.3125\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [4.4996681e-04 1.7569224e-06 9.1444165e-01 8.5106641e-02]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.02125    0.         0.0175     0.         0.96125001\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [2.7875702e-17 3.9996040e-18 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.12125 0.1125  0.065   0.0825  0.24625 0.08625 0.1125  0.065   0.10875]\n",
                                    "Temperature Policy  [0.12125 0.1125  0.065   0.0825  0.24625 0.08625 0.1125  0.065   0.10875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.1425  0.09875 0.      0.07    0.2825  0.095   0.09375 0.095   0.1225 ]\n",
                                    "Temperature Policy  [0.1425  0.09875 0.07    0.2825  0.095   0.09375 0.095   0.1225 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.07875    0.54750001 0.         0.05875    0.         0.14875001\n",
                                    " 0.04875    0.04875    0.06875   ]\n",
                                    "Temperature Policy  [0.07875 0.5475  0.05875 0.14875 0.04875 0.04875 0.06875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.89875001 0.         0.         0.02625    0.         0.0225\n",
                                    " 0.015      0.02       0.0175    ]\n",
                                    "Temperature Policy  [0.89875 0.02625 0.0225  0.015   0.02    0.0175 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.      0.      0.02375 0.      0.02375 0.02125 0.0225  0.90875]\n",
                                    "Temperature Policy  [1.4865925e-16 1.4865925e-16 4.8881570e-17 8.6572703e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.         0.02875    0.         0.93124998\n",
                                    " 0.02       0.02       0.        ]\n",
                                    "Temperature Policy  [7.8653065e-16 1.0000000e+00 2.0875511e-17 2.0875511e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.95249999 0.         0.\n",
                                    " 0.02625    0.02125    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.5272210e-16 3.0544954e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.55000001 0.44999999 0.        ]\n",
                                    "Temperature Policy  [0.88149947 0.11850054]\n",
                                    "Action  6\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.1125     0.115      0.09375    0.08       0.24124999 0.09\n",
                                    " 0.10875    0.0525     0.10625   ]\n",
                                    "Temperature Policy  [0.1125  0.115   0.09375 0.08    0.24125 0.09    0.10875 0.0525  0.10625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.15375 0.13    0.11875 0.13    0.      0.14375 0.115   0.0825  0.12625]\n",
                                    "Temperature Policy  [0.15375 0.13    0.11875 0.13    0.14375 0.115   0.0825  0.12625]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01625 0.01875 0.025   0.88    0.      0.02875 0.01625 0.      0.015  ]\n",
                                    "Temperature Policy  [0.01625 0.01875 0.025   0.88    0.02875 0.01625 0.015  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.09625 0.10375 0.09625 0.      0.      0.46875 0.11875 0.      0.11625]\n",
                                    "Temperature Policy  [0.09625 0.10375 0.09625 0.46875 0.11875 0.11625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.90125 0.02125 0.01625 0.      0.      0.      0.04125 0.      0.02   ]\n",
                                    "Temperature Policy  [1.0000000e+00 5.3105141e-17 3.6314569e-18 4.0344725e-14 2.8963243e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.105      0.13375001 0.         0.         0.\n",
                                    " 0.38249999 0.         0.37875   ]\n",
                                    "Temperature Policy  [1.2747032e-06 1.4336859e-05 5.2460265e-01 4.7538176e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.01875    0.01375    0.         0.         0.\n",
                                    " 0.96749997 0.         0.        ]\n",
                                    "Temperature Policy  [7.4731376e-18 3.3613771e-19 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.11375 0.11625 0.0775  0.08125 0.2475  0.08625 0.11    0.05625 0.11125]\n",
                                    "Temperature Policy  [0.11375 0.11625 0.0775  0.08125 0.2475  0.08625 0.11    0.05625 0.11125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.14749999 0.13124999 0.1225     0.13       0.         0.145\n",
                                    " 0.115      0.0825     0.12625   ]\n",
                                    "Temperature Policy  [0.1475  0.13125 0.1225  0.13    0.145   0.115   0.0825  0.12625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0175     0.89249998 0.0175     0.         0.         0.02\n",
                                    " 0.01625    0.0175     0.01875   ]\n",
                                    "Temperature Policy  [0.0175  0.8925  0.0175  0.02    0.01625 0.0175  0.01875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.12625    0.         0.1        0.         0.         0.1025\n",
                                    " 0.13249999 0.4325     0.10625   ]\n",
                                    "Temperature Policy  [0.12625 0.1     0.1025  0.1325  0.4325  0.10625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0275     0.         0.015      0.         0.         0.01375\n",
                                    " 0.         0.93124998 0.0125    ]\n",
                                    "Temperature Policy  [5.0427111e-16 1.1755734e-18 4.9245226e-19 1.0000000e+00 1.8986167e-19]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x3321c8d60>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c1480>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c3820>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32ec381c0>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x107e219f0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4610>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x3321b5660>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x3321cd450>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x107dd0ac0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32d8bc400>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x107dbf0d0>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x107e21990>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x31a517e50>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x3321cfc10>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x33217e590>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x32d847e50>, 4)]\n",
                                    "Observations [array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 1.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]])]\n",
                                    "Policies [array([0.        , 0.        , 0.0275    , 0.        , 0.        ,\n",
                                    "       0.        , 0.        , 0.97250003, 0.        ]), array([0.0375    , 0.03625   , 0.0425    , 0.0425    , 0.71125001,\n",
                                    "       0.04625   , 0.03875   , 0.        , 0.045     ]), array([0.17749999, 0.1175    , 0.1025    , 0.08875   , 0.10875   ,\n",
                                    "       0.09375   , 0.105     , 0.09625   , 0.11      ]), array([0.09625   , 0.        , 0.        , 0.14875001, 0.375     ,\n",
                                    "       0.        , 0.        , 0.38      , 0.        ]), array([0.05375   , 0.        , 0.05125   , 0.055     , 0.065     ,\n",
                                    "       0.0575    , 0.57875001, 0.        , 0.13875   ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
                                    "       0.18875   , 0.1175    , 0.31874999, 0.375     ]), array([0., 0., 0., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0.03625, 0.     , 0.     , 0.095  , 0.     , 0.40875, 0.05   ,\n",
                                    "       0.     , 0.41   ]), array([0.145     , 0.14125   , 0.09375   , 0.09625   , 0.21250001,\n",
                                    "       0.10375   , 0.09625   , 0.        , 0.11125   ]), array([0.17749999, 0.1175    , 0.10125   , 0.08875   , 0.1075    ,\n",
                                    "       0.09375   , 0.11      , 0.0975    , 0.10625   ]), array([0.09      , 0.07625   , 0.07      , 0.17749999, 0.        ,\n",
                                    "       0.08      , 0.        , 0.27250001, 0.23375   ]), array([0.935 , 0.    , 0.    , 0.    , 0.    , 0.025 , 0.0175, 0.    ,\n",
                                    "       0.0225]), array([0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0.        , 0.        , 0.        , 0.        , 0.035     ,\n",
                                    "       0.0275    , 0.90249997, 0.02      , 0.015     ])]\n",
                                    "NP Array Policies [[0.         0.         0.0275     0.         0.         0.\n",
                                    "  0.         0.97250003 0.        ]\n",
                                    " [0.0375     0.03625    0.0425     0.0425     0.71125001 0.04625\n",
                                    "  0.03875    0.         0.045     ]\n",
                                    " [0.17749999 0.1175     0.1025     0.08875    0.10875    0.09375\n",
                                    "  0.105      0.09625    0.11      ]\n",
                                    " [0.09625    0.         0.         0.14875001 0.375      0.\n",
                                    "  0.         0.38       0.        ]\n",
                                    " [0.05375    0.         0.05125    0.055      0.065      0.0575\n",
                                    "  0.57875001 0.         0.13875   ]\n",
                                    " [0.         0.         0.         0.         0.         0.18875\n",
                                    "  0.1175     0.31874999 0.375     ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         1.         0.        ]\n",
                                    " [0.         0.         0.         1.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.03625    0.         0.         0.095      0.         0.40875\n",
                                    "  0.05       0.         0.41      ]\n",
                                    " [0.145      0.14125    0.09375    0.09625    0.21250001 0.10375\n",
                                    "  0.09625    0.         0.11125   ]\n",
                                    " [0.17749999 0.1175     0.10125    0.08875    0.1075     0.09375\n",
                                    "  0.11       0.0975     0.10625   ]\n",
                                    " [0.09       0.07625    0.07       0.17749999 0.         0.08\n",
                                    "  0.         0.27250001 0.23375   ]\n",
                                    " [0.935      0.         0.         0.         0.         0.025\n",
                                    "  0.0175     0.         0.0225    ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         0.         1.        ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         0.         1.        ]\n",
                                    " [0.         0.         0.         0.         0.035      0.0275\n",
                                    "  0.90249997 0.02       0.015     ]]\n",
                                    "Predicted: tensor([[0.0854, 0.1494, 0.0671, 0.1881, 0.1005, 0.1435, 0.0820, 0.1037, 0.0805],\n",
                                    "        [0.0925, 0.1488, 0.0755, 0.1748, 0.0994, 0.1349, 0.0915, 0.1005, 0.0820],\n",
                                    "        [0.1125, 0.1076, 0.1043, 0.1182, 0.1187, 0.1183, 0.1024, 0.1178, 0.1002],\n",
                                    "        [0.0857, 0.1507, 0.0685, 0.1871, 0.1019, 0.1388, 0.0842, 0.1001, 0.0831],\n",
                                    "        [0.1119, 0.1045, 0.1003, 0.1245, 0.1108, 0.1217, 0.1022, 0.1239, 0.1002],\n",
                                    "        [0.0735, 0.1548, 0.0578, 0.2376, 0.0913, 0.1318, 0.0741, 0.1048, 0.0744],\n",
                                    "        [0.0982, 0.1128, 0.0851, 0.1519, 0.1068, 0.1195, 0.0989, 0.1292, 0.0975],\n",
                                    "        [0.0997, 0.1061, 0.0843, 0.1662, 0.0975, 0.1123, 0.0986, 0.1333, 0.1020],\n",
                                    "        [0.1179, 0.1016, 0.1023, 0.1190, 0.1064, 0.1265, 0.1031, 0.1161, 0.1070],\n",
                                    "        [0.0925, 0.1488, 0.0755, 0.1748, 0.0994, 0.1349, 0.0915, 0.1005, 0.0820],\n",
                                    "        [0.1125, 0.1076, 0.1043, 0.1182, 0.1187, 0.1183, 0.1024, 0.1178, 0.1002],\n",
                                    "        [0.1090, 0.1065, 0.0944, 0.1317, 0.1120, 0.1209, 0.1027, 0.1215, 0.1012],\n",
                                    "        [0.0880, 0.1519, 0.0651, 0.1946, 0.0958, 0.1451, 0.0786, 0.1021, 0.0789],\n",
                                    "        [0.0992, 0.1072, 0.0862, 0.1550, 0.0997, 0.1211, 0.0921, 0.1353, 0.1042],\n",
                                    "        [0.1000, 0.1073, 0.0860, 0.1557, 0.1012, 0.1154, 0.0995, 0.1349, 0.0999],\n",
                                    "        [0.1124, 0.1019, 0.1026, 0.1200, 0.1091, 0.1209, 0.1069, 0.1232, 0.1030]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.0854, 0.1494, 0.0671, 0.1881, 0.1005, 0.1435, 0.0820, 0.1037, 0.0805],\n",
                                    "        [0.0925, 0.1488, 0.0755, 0.1748, 0.0994, 0.1349, 0.0915, 0.1005, 0.0820],\n",
                                    "        [0.1125, 0.1076, 0.1043, 0.1182, 0.1187, 0.1183, 0.1024, 0.1178, 0.1002],\n",
                                    "        [0.0857, 0.1507, 0.0685, 0.1871, 0.1019, 0.1388, 0.0842, 0.1001, 0.0831],\n",
                                    "        [0.1119, 0.1045, 0.1003, 0.1245, 0.1108, 0.1217, 0.1022, 0.1239, 0.1002],\n",
                                    "        [0.0735, 0.1548, 0.0578, 0.2376, 0.0913, 0.1318, 0.0741, 0.1048, 0.0744],\n",
                                    "        [0.0982, 0.1128, 0.0851, 0.1519, 0.1068, 0.1195, 0.0989, 0.1292, 0.0975],\n",
                                    "        [0.0997, 0.1061, 0.0843, 0.1662, 0.0975, 0.1123, 0.0986, 0.1333, 0.1020],\n",
                                    "        [0.1179, 0.1016, 0.1023, 0.1190, 0.1064, 0.1265, 0.1031, 0.1161, 0.1070],\n",
                                    "        [0.0925, 0.1488, 0.0755, 0.1748, 0.0994, 0.1349, 0.0915, 0.1005, 0.0820],\n",
                                    "        [0.1125, 0.1076, 0.1043, 0.1182, 0.1187, 0.1183, 0.1024, 0.1178, 0.1002],\n",
                                    "        [0.1090, 0.1065, 0.0944, 0.1317, 0.1120, 0.1209, 0.1027, 0.1215, 0.1012],\n",
                                    "        [0.0880, 0.1519, 0.0651, 0.1946, 0.0958, 0.1451, 0.0786, 0.1021, 0.0789],\n",
                                    "        [0.0992, 0.1072, 0.0862, 0.1550, 0.0997, 0.1211, 0.0921, 0.1353, 0.1042],\n",
                                    "        [0.1000, 0.1073, 0.0860, 0.1557, 0.1012, 0.1154, 0.0995, 0.1349, 0.0999],\n",
                                    "        [0.1124, 0.1019, 0.1026, 0.1200, 0.1091, 0.1209, 0.1069, 0.1232, 0.1030]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.0854, 0.1494, 0.0671, 0.1881, 0.1005, 0.1435, 0.0820, 0.1037, 0.0805],\n",
                                    "        [0.0925, 0.1488, 0.0755, 0.1748, 0.0994, 0.1349, 0.0915, 0.1005, 0.0820],\n",
                                    "        [0.1125, 0.1076, 0.1043, 0.1182, 0.1187, 0.1183, 0.1024, 0.1178, 0.1002],\n",
                                    "        [0.0857, 0.1507, 0.0685, 0.1871, 0.1019, 0.1388, 0.0842, 0.1001, 0.0831],\n",
                                    "        [0.1119, 0.1045, 0.1003, 0.1245, 0.1108, 0.1217, 0.1022, 0.1239, 0.1002],\n",
                                    "        [0.0735, 0.1548, 0.0578, 0.2376, 0.0913, 0.1318, 0.0741, 0.1048, 0.0744],\n",
                                    "        [0.0982, 0.1128, 0.0851, 0.1519, 0.1068, 0.1195, 0.0989, 0.1292, 0.0975],\n",
                                    "        [0.0997, 0.1061, 0.0843, 0.1662, 0.0975, 0.1123, 0.0986, 0.1333, 0.1020],\n",
                                    "        [0.1179, 0.1016, 0.1023, 0.1190, 0.1064, 0.1265, 0.1031, 0.1161, 0.1070],\n",
                                    "        [0.0925, 0.1488, 0.0755, 0.1748, 0.0994, 0.1349, 0.0915, 0.1005, 0.0820],\n",
                                    "        [0.1125, 0.1076, 0.1043, 0.1182, 0.1187, 0.1183, 0.1024, 0.1178, 0.1002],\n",
                                    "        [0.1090, 0.1065, 0.0944, 0.1317, 0.1120, 0.1209, 0.1027, 0.1215, 0.1012],\n",
                                    "        [0.0880, 0.1519, 0.0651, 0.1946, 0.0958, 0.1451, 0.0786, 0.1021, 0.0789],\n",
                                    "        [0.0992, 0.1072, 0.0862, 0.1550, 0.0997, 0.1211, 0.0921, 0.1353, 0.1042],\n",
                                    "        [0.1000, 0.1073, 0.0860, 0.1557, 0.1012, 0.1154, 0.0995, 0.1349, 0.0999],\n",
                                    "        [0.1124, 0.1019, 0.1026, 0.1200, 0.1091, 0.1209, 0.1069, 0.1232, 0.1030]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.4608, -1.9013, -2.7018, -1.6709, -2.2980, -1.9417, -2.5016, -2.2663,\n",
                                    "         -2.5192],\n",
                                    "        [-2.3801, -1.9049, -2.5839, -1.7442, -2.3083, -2.0030, -2.3913, -2.2978,\n",
                                    "         -2.5009],\n",
                                    "        [-2.1852, -2.2292, -2.2601, -2.1355, -2.1316, -2.1343, -2.2791, -2.1385,\n",
                                    "         -2.3005],\n",
                                    "        [-2.4570, -1.8926, -2.6807, -1.6762, -2.2842, -1.9747, -2.4748, -2.3017,\n",
                                    "         -2.4875],\n",
                                    "        [-2.1903, -2.2581, -2.2993, -2.0838, -2.2002, -2.1063, -2.2807, -2.0882,\n",
                                    "         -2.3007],\n",
                                    "        [-2.6103, -1.8655, -2.8510, -1.4372, -2.3938, -2.0268, -2.6029, -2.2561,\n",
                                    "         -2.5979],\n",
                                    "        [-2.3207, -2.1821, -2.4642, -1.8845, -2.2363, -2.1241, -2.3132, -2.0465,\n",
                                    "         -2.3280],\n",
                                    "        [-2.3060, -2.2431, -2.4730, -1.7946, -2.3274, -2.1868, -2.3171, -2.0153,\n",
                                    "         -2.2825],\n",
                                    "        [-2.1382, -2.2863, -2.2795, -2.1286, -2.2410, -2.0674, -2.2717, -2.1531,\n",
                                    "         -2.2347],\n",
                                    "        [-2.3801, -1.9049, -2.5839, -1.7442, -2.3083, -2.0030, -2.3913, -2.2978,\n",
                                    "         -2.5009],\n",
                                    "        [-2.1852, -2.2292, -2.2601, -2.1355, -2.1316, -2.1343, -2.2791, -2.1385,\n",
                                    "         -2.3005],\n",
                                    "        [-2.2165, -2.2397, -2.3601, -2.0272, -2.1893, -2.1124, -2.2755, -2.1076,\n",
                                    "         -2.2907],\n",
                                    "        [-2.4309, -1.8846, -2.7318, -1.6367, -2.3457, -1.9306, -2.5428, -2.2821,\n",
                                    "         -2.5399],\n",
                                    "        [-2.3107, -2.2330, -2.4511, -1.8644, -2.3055, -2.1107, -2.3853, -2.0001,\n",
                                    "         -2.2618],\n",
                                    "        [-2.3022, -2.2318, -2.4529, -1.8596, -2.2909, -2.1592, -2.3079, -2.0035,\n",
                                    "         -2.3033],\n",
                                    "        [-2.1854, -2.2839, -2.2773, -2.1203, -2.2154, -2.1127, -2.2357, -2.0938,\n",
                                    "         -2.2732]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.17645206 2.2145324 2.3909845\n",
                                    "score:  1\n",
                                    "score:  0\n",
                                    "score:  0\n",
                                    "score:  0\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_7/videos/alphazero/7/alphazero-episode-34.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_7/videos/alphazero/7/alphazero-episode-34.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                           \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_7/videos/alphazero/7/alphazero-episode-34.mp4\n",
                                    "score:  0\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.07375    0.05375    0.04       0.0525     0.57499999 0.05\n",
                                    " 0.05625    0.045      0.05375   ]\n",
                                    "Temperature Policy  [0.07375 0.05375 0.04    0.0525  0.575   0.05    0.05625 0.045   0.05375]\n",
                                    "Action  4\n",
                                    "Target Policy [0.22125    0.09375    0.0775     0.11125    0.         0.15000001\n",
                                    " 0.16       0.09       0.09625   ]\n",
                                    "Temperature Policy  [0.22125 0.09375 0.0775  0.11125 0.15    0.16    0.09    0.09625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.49250001 0.02875    0.3075     0.         0.05125\n",
                                    " 0.02875    0.065      0.02625   ]\n",
                                    "Temperature Policy  [0.4925  0.02875 0.3075  0.05125 0.02875 0.065   0.02625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02125    0.02       0.         0.         0.89749998\n",
                                    " 0.02       0.0225     0.01875   ]\n",
                                    "Temperature Policy  [0.02125 0.02    0.8975  0.02    0.0225  0.01875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.0275     0.01375    0.         0.         0.91374999\n",
                                    " 0.02125    0.02375    0.        ]\n",
                                    "Temperature Policy  [6.0961136e-16 5.9532359e-19 1.0000000e+00 4.6271703e-17 1.4072208e-16]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.07375    0.04875    0.04375    0.0475     0.57125002 0.05\n",
                                    " 0.065      0.0425     0.0575    ]\n",
                                    "Temperature Policy  [0.07375 0.04875 0.04375 0.0475  0.57125 0.05    0.065   0.0425  0.0575 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.21375    0.09375    0.09125    0.10875    0.         0.15000001\n",
                                    " 0.16       0.08625    0.09625   ]\n",
                                    "Temperature Policy  [0.21375 0.09375 0.09125 0.10875 0.15    0.16    0.08625 0.09625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.015      0.89625001 0.02125    0.01875    0.         0.\n",
                                    " 0.01625    0.0175     0.015     ]\n",
                                    "Temperature Policy  [0.015   0.89625 0.02125 0.01875 0.01625 0.0175  0.015  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.10125    0.         0.10875    0.09625    0.         0.\n",
                                    " 0.09875    0.49000001 0.105     ]\n",
                                    "Temperature Policy  [0.10125 0.10875 0.09625 0.09875 0.49    0.105  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0225     0.         0.0225     0.02375    0.         0.\n",
                                    " 0.         0.91000003 0.02125   ]\n",
                                    "Temperature Policy  [8.5390841e-17 8.5390841e-17 1.4662979e-16 1.0000000e+00 4.8214254e-17]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.06875    0.045      0.0375     0.04125    0.61374998 0.0425\n",
                                    " 0.0575     0.04125    0.0525    ]\n",
                                    "Temperature Policy  [0.06875 0.045   0.0375  0.04125 0.61375 0.0425  0.0575  0.04125 0.0525 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.04625    0.04125    0.0325     0.         0.73000002 0.04125\n",
                                    " 0.04125    0.0325     0.035     ]\n",
                                    "Temperature Policy  [0.04625 0.04125 0.0325  0.73    0.04125 0.04125 0.0325  0.035  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.59875    0.0425     0.03375    0.         0.         0.055\n",
                                    " 0.17749999 0.0425     0.05      ]\n",
                                    "Temperature Policy  [0.59875 0.0425  0.03375 0.055   0.1775  0.0425  0.05   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.02875    0.01625    0.         0.         0.01875\n",
                                    " 0.89875001 0.02       0.0175    ]\n",
                                    "Temperature Policy  [0.02875 0.01625 0.01875 0.89875 0.02    0.0175 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.0225     0.91500002 0.         0.         0.02\n",
                                    " 0.         0.02125    0.02125   ]\n",
                                    "Temperature Policy  [8.0837761e-17 1.0000000e+00 2.4893675e-17 4.5643446e-17 4.5643446e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.93000001 0.         0.         0.         0.02375\n",
                                    " 0.         0.025      0.02125   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.1797953e-16 1.9704734e-16 3.8793580e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.02375\n",
                                    " 0.         0.95499998 0.02125   ]\n",
                                    "Temperature Policy  [9.0490383e-17 1.0000000e+00 2.9754702e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.  0.  0.  0.  0.  0.5 0.  0.  0.5]\n",
                                    "Temperature Policy  [0.5 0.5]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.07       0.0625     0.04       0.045      0.57999998 0.0475\n",
                                    " 0.055      0.04875    0.05125   ]\n",
                                    "Temperature Policy  [0.07    0.0625  0.04    0.045   0.58    0.0475  0.055   0.04875 0.05125]\n",
                                    "Action  2\n",
                                    "Target Policy [0.04       0.03625    0.         0.03625    0.72874999 0.045\n",
                                    " 0.035      0.03375    0.045     ]\n",
                                    "Temperature Policy  [0.04    0.03625 0.03625 0.72875 0.045   0.035   0.03375 0.045  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.84750003 0.06       0.         0.         0.02375    0.01625\n",
                                    " 0.01625    0.015      0.02125   ]\n",
                                    "Temperature Policy  [0.8475  0.06    0.02375 0.01625 0.01625 0.015   0.02125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.4725  0.      0.      0.12625 0.115   0.0975  0.09375 0.095  ]\n",
                                    "Temperature Policy  [0.4725  0.12625 0.115   0.0975  0.09375 0.095  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.01875    0.0225\n",
                                    " 0.02       0.01875    0.92000002]\n",
                                    "Temperature Policy  [1.2363301e-17 7.6550305e-17 2.3573370e-17 1.2363301e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.         0.         0.375      0.39250001\n",
                                    " 0.08375    0.14875001 0.        ]\n",
                                    "Temperature Policy  [3.8789564e-01 6.1206681e-01 1.1974306e-07 3.7408645e-05]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.         0.96249998 0.\n",
                                    " 0.0175     0.02       0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.9479635e-18 1.5006956e-17]\n",
                                    "Action  4\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.06       0.04625    0.03875    0.0425     0.61624998 0.0425\n",
                                    " 0.0575     0.04375    0.0525    ]\n",
                                    "Temperature Policy  [0.06    0.04625 0.03875 0.0425  0.61625 0.0425  0.0575  0.04375 0.0525 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.2225     0.09375    0.0775     0.11125    0.         0.15000001\n",
                                    " 0.16       0.08875    0.09625   ]\n",
                                    "Temperature Policy  [0.2225  0.09375 0.0775  0.11125 0.15    0.16    0.08875 0.09625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.49250001 0.02375    0.31125    0.         0.05375\n",
                                    " 0.02625    0.065      0.0275    ]\n",
                                    "Temperature Policy  [0.4925  0.02375 0.31125 0.05375 0.02625 0.065   0.0275 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02375    0.01875    0.         0.         0.89249998\n",
                                    " 0.01875    0.02       0.02625   ]\n",
                                    "Temperature Policy  [0.02375 0.01875 0.8925  0.01875 0.02    0.02625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.41749999 0.39500001 0.         0.         0.\n",
                                    " 0.03125    0.09625    0.06      ]\n",
                                    "Temperature Policy  [6.3506013e-01 3.6493963e-01 3.5055027e-12 2.6931309e-07 2.3865063e-09]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0225     0.         0.         0.\n",
                                    " 0.02375    0.93374997 0.02      ]\n",
                                    "Temperature Policy  [6.5996215e-17 1.1332611e-16 1.0000000e+00 2.0323280e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.41749999 0.         0.         0.\n",
                                    " 0.45625001 0.         0.12625   ]\n",
                                    "Temperature Policy  [2.9161230e-01 7.0838577e-01 1.8644680e-06]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.97250003 0.         0.0275    ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.2691197e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.07       0.05       0.0425     0.04875    0.57999998 0.05375\n",
                                    " 0.0575     0.04       0.0575    ]\n",
                                    "Temperature Policy  [0.07    0.05    0.0425  0.04875 0.58    0.05375 0.0575  0.04    0.0575 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.22624999 0.095      0.0775     0.1125     0.         0.15125\n",
                                    " 0.16125    0.07875    0.0975    ]\n",
                                    "Temperature Policy  [0.22625 0.095   0.0775  0.1125  0.15125 0.16125 0.07875 0.0975 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.49250001 0.03125    0.30250001 0.         0.05125\n",
                                    " 0.03125    0.06375    0.0275    ]\n",
                                    "Temperature Policy  [0.4925  0.03125 0.3025  0.05125 0.03125 0.06375 0.0275 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.01875    0.015      0.90499997 0.         0.\n",
                                    " 0.0175     0.01875    0.025     ]\n",
                                    "Temperature Policy  [0.01875 0.015   0.905   0.0175  0.01875 0.025  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02125    0.0225     0.         0.         0.\n",
                                    " 0.91750002 0.02125    0.0175    ]\n",
                                    "Temperature Policy  [4.4414891e-17 7.8661905e-17 1.0000000e+00 4.4414891e-17 6.3726453e-18]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.02875    0.92624998 0.         0.         0.\n",
                                    " 0.         0.02375    0.02125   ]\n",
                                    "Temperature Policy  [8.3003474e-16 1.0000000e+00 1.2284400e-16 4.0393095e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.95249999 0.         0.         0.         0.\n",
                                    " 0.         0.02375    0.02375   ]\n",
                                    "Temperature Policy  [1.0000000e+00 9.2893706e-17 9.2893706e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.      0.      0.      0.      0.      0.97375 0.02625]\n",
                                    "Temperature Policy  [1.0000000e+00 2.0268348e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.57875001 0.0525     0.0375     0.0475     0.07625    0.05\n",
                                    " 0.055      0.04125    0.06125   ]\n",
                                    "Temperature Policy  [0.57875 0.0525  0.0375  0.0475  0.07625 0.05    0.055   0.04125 0.06125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.06       0.045      0.05375    0.0925     0.0775\n",
                                    " 0.51749998 0.0775     0.07625   ]\n",
                                    "Temperature Policy  [0.06    0.045   0.05375 0.0925  0.0775  0.5175  0.0775  0.07625]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.0175     0.01625    0.015      0.0175\n",
                                    " 0.89749998 0.0175     0.01875   ]\n",
                                    "Temperature Policy  [0.0175  0.01625 0.015   0.0175  0.8975  0.0175  0.01875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.      0.09375 0.46875 0.1225  0.1     0.      0.12625 0.08875]\n",
                                    "Temperature Policy  [0.09375 0.46875 0.1225  0.1     0.12625 0.08875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.      0.      0.02125 0.90875 0.025   0.02    0.      0.025   0.     ]\n",
                                    "Temperature Policy  [4.888157e-17 1.000000e+00 2.482881e-16 2.665973e-17 2.482881e-16]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.61374998 0.045      0.04       0.04       0.07625    0.045\n",
                                    " 0.0525     0.04       0.0475    ]\n",
                                    "Temperature Policy  [0.61375 0.045   0.04    0.04    0.07625 0.045   0.0525  0.04    0.0475 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.065      0.045      0.05625    0.0925     0.07875\n",
                                    " 0.53500003 0.0525     0.075     ]\n",
                                    "Temperature Policy  [0.065   0.045   0.05625 0.0925  0.07875 0.535   0.0525  0.075  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.01625    0.02       0.01625    0.0175\n",
                                    " 0.89875001 0.0175     0.01375   ]\n",
                                    "Temperature Policy  [0.01625 0.02    0.01625 0.0175  0.89875 0.0175  0.01375]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.09       0.47874999 0.1075     0.1\n",
                                    " 0.         0.125      0.09875   ]\n",
                                    "Temperature Policy  [0.09    0.47875 0.1075  0.1     0.125   0.09875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.      0.02375 0.8775  0.03375 0.      0.      0.0325  0.0325 ]\n",
                                    "Temperature Policy  [2.1094289e-16 1.0000000e+00 7.0838039e-15 4.8569360e-15 4.8569360e-15]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.06875 0.05375 0.0475  0.0425  0.59125 0.045   0.055   0.04375 0.0525 ]\n",
                                    "Temperature Policy  [0.06875 0.05375 0.0475  0.0425  0.59125 0.045   0.055   0.04375 0.0525 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.13       0.05375    0.04375    0.0525     0.51249999 0.08125\n",
                                    " 0.         0.04875    0.0775    ]\n",
                                    "Temperature Policy  [0.13    0.05375 0.04375 0.0525  0.5125  0.08125 0.04875 0.0775 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.08125    0.06375    0.035      0.18125001 0.         0.0475\n",
                                    " 0.         0.52249998 0.06875   ]\n",
                                    "Temperature Policy  [0.08125 0.06375 0.035   0.18125 0.0475  0.5225  0.06875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.01875    0.01875    0.91000003 0.         0.01875\n",
                                    " 0.         0.0175     0.01625   ]\n",
                                    "Temperature Policy  [0.01875 0.01875 0.91    0.01875 0.0175  0.01625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02375    0.01625    0.         0.         0.91500002\n",
                                    " 0.         0.02125    0.02375   ]\n",
                                    "Temperature Policy  [1.3881143e-16 3.1212083e-18 1.0000000e+00 4.5643446e-17 1.3881143e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.52249998 0.0425     0.         0.         0.\n",
                                    " 0.         0.38749999 0.0475    ]\n",
                                    "Temperature Policy  [9.5207894e-01 1.2069778e-11 4.7921132e-02 3.6706766e-11]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.02125    0.         0.         0.\n",
                                    " 0.         0.95375001 0.025     ]\n",
                                    "Temperature Policy  [3.0146981e-17 1.0000000e+00 1.5312798e-16]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.         0.025      0.         0.         0.\n",
                                    " 0.         0.         0.97500002]\n",
                                    "Temperature Policy  [1.22844e-16 1.00000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.58125001 0.055      0.04125    0.0425     0.075      0.05875\n",
                                    " 0.05375    0.04125    0.05125   ]\n",
                                    "Temperature Policy  [0.58125 0.055   0.04125 0.0425  0.075   0.05875 0.05375 0.04125 0.05125]\n",
                                    "Action  6\n",
                                    "Target Policy [0.1275  0.0575  0.05875 0.0525  0.495   0.08125 0.      0.05125 0.07625]\n",
                                    "Temperature Policy  [0.1275  0.0575  0.05875 0.0525  0.495   0.08125 0.05125 0.07625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.08125    0.05875    0.03625    0.18125001 0.         0.0475\n",
                                    " 0.         0.52625    0.06875   ]\n",
                                    "Temperature Policy  [0.08125 0.05875 0.03625 0.18125 0.0475  0.52625 0.06875]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01875    0.0175     0.0175     0.0275     0.         0.01875\n",
                                    " 0.         0.         0.89999998]\n",
                                    "Temperature Policy  [0.01875 0.0175  0.0175  0.0275  0.01875 0.9    ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91500002 0.02125    0.015      0.0275     0.         0.02125\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.5643446e-17 1.4018504e-18 6.0133431e-16 4.5643446e-17]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.02125 0.0225  0.9325  0.      0.02375 0.      0.      0.     ]\n",
                                    "Temperature Policy  [3.7765996e-17 6.6886241e-17 1.0000000e+00 1.1485443e-16]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02625    0.025      0.         0.         0.94875002\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [2.6289068e-16 1.6139207e-16 1.0000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.48625001 0.51375002 0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.36583224 0.6341678 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.06125    0.05125    0.04       0.04       0.61124998 0.05\n",
                                    " 0.0575     0.04       0.04875   ]\n",
                                    "Temperature Policy  [0.06125 0.05125 0.04    0.04    0.61125 0.05    0.0575  0.04    0.04875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.22750001 0.095      0.0775     0.11125    0.         0.15125\n",
                                    " 0.16125    0.07875    0.0975    ]\n",
                                    "Temperature Policy  [0.2275  0.095   0.0775  0.11125 0.15125 0.16125 0.07875 0.0975 ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0175     0.015      0.015      0.02125    0.         0.89875001\n",
                                    " 0.0175     0.         0.015     ]\n",
                                    "Temperature Policy  [0.0175  0.015   0.015   0.02125 0.89875 0.0175  0.015  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.0975     0.11625    0.09375    0.45750001 0.         0.\n",
                                    " 0.11875    0.         0.11625   ]\n",
                                    "Temperature Policy  [0.0975  0.11625 0.09375 0.4575  0.11875 0.11625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02       0.02125    0.02625    0.         0.         0.\n",
                                    " 0.0175     0.         0.91500002]\n",
                                    "Temperature Policy  [2.4893675e-17 4.5643446e-17 3.7764360e-16 6.5489184e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.43625 0.09875 0.2775  0.      0.      0.      0.1875  0.      0.     ]\n",
                                    "Temperature Policy  [9.8905939e-01 3.4933251e-07 1.0727484e-02 2.1275546e-04]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.01875    0.95749998 0.         0.         0.\n",
                                    " 0.02375    0.         0.        ]\n",
                                    "Temperature Policy  [8.291344e-18 1.000000e+00 8.815528e-17]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.07375 0.0475  0.04125 0.045   0.59125 0.0475  0.05875 0.04375 0.05125]\n",
                                    "Temperature Policy  [0.07375 0.0475  0.04125 0.045   0.59125 0.0475  0.05875 0.04375 0.05125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.22624999 0.09625    0.0775     0.11125    0.         0.15125\n",
                                    " 0.16125    0.08       0.09625   ]\n",
                                    "Temperature Policy  [0.22625 0.09625 0.0775  0.11125 0.15125 0.16125 0.08    0.09625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.0275     0.0725     0.025      0.39375001 0.         0.06\n",
                                    " 0.         0.38124999 0.04      ]\n",
                                    "Temperature Policy  [0.0275  0.0725  0.025   0.39375 0.06    0.38125 0.04   ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02       0.89499998 0.01625    0.03       0.         0.02\n",
                                    " 0.         0.         0.01875   ]\n",
                                    "Temperature Policy  [0.02    0.895   0.01625 0.03    0.02    0.01875]\n",
                                    "Action  1\n",
                                    "Target Policy [0.42750001 0.         0.03375    0.41125    0.         0.09625\n",
                                    " 0.         0.         0.03125   ]\n",
                                    "Temperature Policy  [5.9568781e-01 5.6026733e-12 4.0431198e-01 1.9937234e-07 2.5951218e-12]\n",
                                    "Action  3\n",
                                    "Target Policy [0.025   0.      0.01875 0.      0.      0.935   0.      0.      0.02125]\n",
                                    "Temperature Policy  [1.8676004e-16 1.0517114e-17 1.0000000e+00 3.6768271e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.45124999 0.         0.12       0.         0.         0.\n",
                                    " 0.         0.         0.42875001]\n",
                                    "Temperature Policy  [6.2515152e-01 1.1056702e-06 3.7484738e-01]\n",
                                    "Action  8\n",
                                    "Target Policy [0.97125 0.      0.02875 0.      0.      0.      0.      0.      0.     ]\n",
                                    "Temperature Policy  [1.000000e+00 5.164974e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.58249998 0.05125    0.04125    0.04125    0.075      0.045\n",
                                    " 0.06125    0.0425     0.06      ]\n",
                                    "Temperature Policy  [0.5825  0.05125 0.04125 0.04125 0.075   0.045   0.06125 0.0425  0.06   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0625     0.045      0.05625    0.095      0.0775\n",
                                    " 0.53750002 0.05125    0.075     ]\n",
                                    "Temperature Policy  [0.0625  0.045   0.05625 0.095   0.0775  0.5375  0.05125 0.075  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.03       0.40125    0.03125    0.45124999 0.03\n",
                                    " 0.         0.02875    0.0275    ]\n",
                                    "Temperature Policy  [0.03    0.40125 0.03125 0.45125 0.03    0.02875 0.0275 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.0225     0.01625    0.02       0.         0.02125\n",
                                    " 0.         0.0225     0.89749998]\n",
                                    "Temperature Policy  [0.0225  0.01625 0.02    0.02125 0.0225  0.8975 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.02125    0.015      0.025      0.         0.02125\n",
                                    " 0.         0.91750002 0.        ]\n",
                                    "Temperature Policy  [4.4414891e-17 1.3641177e-18 2.2560013e-16 4.4414891e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.92624998 0.02625    0.02625    0.         0.02125\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 3.3420341e-16 3.3420341e-16 4.0393095e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.1275     0.46000001 0.         0.41249999\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [2.0027892e-06 7.4836200e-01 2.5163609e-01]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.02375    0.97624999 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [7.261489e-17 1.000000e+00]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.06125    0.05125    0.045      0.04       0.61374998 0.04375\n",
                                    " 0.055      0.0375     0.0525    ]\n",
                                    "Temperature Policy  [0.06125 0.05125 0.045   0.04    0.61375 0.04375 0.055   0.0375  0.0525 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.045      0.0375     0.03125    0.0375     0.73124999 0.\n",
                                    " 0.035      0.04375    0.03875   ]\n",
                                    "Temperature Policy  [0.045   0.0375  0.03125 0.0375  0.73125 0.035   0.04375 0.03875]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0325     0.03       0.03375    0.02875    0.81875002 0.\n",
                                    " 0.0275     0.02875    0.        ]\n",
                                    "Temperature Policy  [0.0325  0.03    0.03375 0.02875 0.81875 0.0275  0.02875]\n",
                                    "Action  4\n",
                                    "Target Policy [0.01875    0.0225     0.01875    0.89625001 0.         0.\n",
                                    " 0.01875    0.025      0.        ]\n",
                                    "Temperature Policy  [0.01875 0.0225  0.01875 0.89625 0.01875 0.025  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.0575     0.10375    0.0325     0.         0.         0.\n",
                                    " 0.38749999 0.41874999 0.        ]\n",
                                    "Temperature Policy  [1.6317273e-09 5.9682935e-07 5.4300366e-12 3.1527257e-01 6.8472683e-01]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02375 0.935   0.01875 0.      0.      0.      0.0225  0.      0.     ]\n",
                                    "Temperature Policy  [1.1182014e-16 1.0000000e+00 1.0517114e-17 6.5119205e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.1275  0.      0.40875 0.      0.      0.      0.46375 0.      0.     ]\n",
                                    "Temperature Policy  [1.9232941e-06 2.2055733e-01 7.7944070e-01]\n",
                                    "Action  6\n",
                                    "Target Policy [0.03       0.         0.97000003 0.         0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [8.0074683e-16 1.0000000e+00]\n",
                                    "Action  2\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.06375    0.04875    0.0375     0.04625    0.61624998 0.045\n",
                                    " 0.05125    0.03875    0.0525    ]\n",
                                    "Temperature Policy  [0.06375 0.04875 0.0375  0.04625 0.61625 0.045   0.05125 0.03875 0.0525 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.04       0.04125    0.03125    0.035      0.74374998 0.03875\n",
                                    " 0.0375     0.0325     0.        ]\n",
                                    "Temperature Policy  [0.04    0.04125 0.03125 0.035   0.74375 0.03875 0.0375  0.0325 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.0475     0.0425     0.05375    0.0525     0.         0.14749999\n",
                                    " 0.06875    0.58749998 0.        ]\n",
                                    "Temperature Policy  [0.0475  0.0425  0.05375 0.0525  0.1475  0.06875 0.5875 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01625    0.0275     0.89749998 0.0225     0.         0.\n",
                                    " 0.01625    0.02       0.        ]\n",
                                    "Temperature Policy  [0.01625 0.0275  0.8975  0.0225  0.01625 0.02   ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0175     0.02125    0.         0.0275     0.         0.\n",
                                    " 0.91250002 0.02125    0.        ]\n",
                                    "Temperature Policy  [6.730569e-18 4.690948e-17 6.180138e-16 1.000000e+00 4.690948e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.025   0.02125 0.      0.02125 0.      0.      0.      0.9325  0.     ]\n",
                                    "Temperature Policy  [1.9182787e-16 3.7765996e-17 3.7765996e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.0225     0.94999999 0.         0.0275     0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [5.5537859e-17 1.0000000e+00 4.1313394e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.51999998 0.         0.         0.47999999 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.69006586 0.30993417]\n",
                                    "Action  0\n",
                                    "Target Policy [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.065      0.045      0.03875    0.045      0.60874999 0.04625\n",
                                    " 0.05375    0.0425     0.055     ]\n",
                                    "Temperature Policy  [0.065   0.045   0.03875 0.045   0.60875 0.04625 0.05375 0.0425  0.055  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.22       0.09375    0.09       0.11125    0.         0.15000001\n",
                                    " 0.16       0.07875    0.09625   ]\n",
                                    "Temperature Policy  [0.22    0.09375 0.09    0.11125 0.15    0.16    0.07875 0.09625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.02875    0.06       0.02375    0.05875    0.         0.41874999\n",
                                    " 0.035      0.375      0.        ]\n",
                                    "Temperature Policy  [0.02875 0.06    0.02375 0.05875 0.41875 0.035   0.375  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01875    0.89375001 0.01875    0.02125    0.         0.02375\n",
                                    " 0.02375    0.         0.        ]\n",
                                    "Temperature Policy  [0.01875 0.89375 0.01875 0.02125 0.02375 0.02375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.06375 0.      0.4025  0.095   0.      0.405   0.03375 0.      0.     ]\n",
                                    "Temperature Policy  [4.8134794e-09 4.8452491e-01 2.5995209e-07 5.1547486e-01 8.3252068e-12]\n",
                                    "Action  2\n",
                                    "Target Policy [0.025      0.         0.         0.02625    0.         0.02125\n",
                                    " 0.92750001 0.         0.        ]\n",
                                    "Temperature Policy  [2.0242348e-16 3.2972653e-16 3.9852004e-17 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Target Policy [0.11       0.         0.         0.48249999 0.         0.4075\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [3.2016268e-07 8.4414500e-01 1.5585473e-01]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02875 0.      0.      0.      0.      0.97125 0.      0.      0.     ]\n",
                                    "Temperature Policy  [5.164974e-16 1.000000e+00]\n",
                                    "Action  5\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x32c6fdc60>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x107dbd780>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x330a1cb80>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32c608280>, 6), (<replay_buffers.base_replay_buffer.Game object at 0x3321cfc10>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x328773610>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32878d630>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x107e20040>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x107dbd180>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x3337cebf0>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x107e23640>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32ecfbb20>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x330a1e980>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x32d8bc400>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x107dd0ac0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4550>, 1)]\n",
                                    "Observations [array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 1., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 1.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 1.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 1., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 1., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]])]\n",
                                    "Policies [array([0.1125    , 0.0575    , 0.0525    , 0.05      , 0.47874999,\n",
                                    "       0.0575    , 0.06      , 0.045     , 0.08625   ]), array([0.07125   , 0.        , 0.05625   , 0.0575    , 0.55500001,\n",
                                    "       0.0625    , 0.05625   , 0.07      , 0.07125   ]), array([0.22750001, 0.095     , 0.0775    , 0.11125   , 0.        ,\n",
                                    "       0.15125   , 0.16125   , 0.07875   , 0.0975    ]), array([0.        , 0.02      , 0.02125   , 0.        , 0.        ,\n",
                                    "       0.95875001, 0.        , 0.        , 0.        ]), array([0.115     , 0.115     , 0.1025    , 0.0975    , 0.14749999,\n",
                                    "       0.105     , 0.1125    , 0.09625   , 0.10875   ]), array([0.        , 0.01875   , 0.91374999, 0.        , 0.        ,\n",
                                    "       0.0225    , 0.        , 0.02      , 0.025     ]), array([0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0.        , 0.375     , 0.41624999, 0.        , 0.        ,\n",
                                    "       0.        , 0.02375   , 0.12375   , 0.06125   ]), array([0.12625, 0.15625, 0.     , 0.115  , 0.11125, 0.1275 , 0.155  ,\n",
                                    "       0.125  , 0.08375]), array([0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0.12625   , 0.12625   , 0.        , 0.07875   , 0.22750001,\n",
                                    "       0.14875001, 0.1025    , 0.07875   , 0.11125   ]), array([0.        , 0.92374998, 0.        , 0.        , 0.        ,\n",
                                    "       0.02375   , 0.        , 0.02375   , 0.02875   ]), array([0.        , 0.52249998, 0.0425    , 0.        , 0.        ,\n",
                                    "       0.        , 0.        , 0.38749999, 0.0475    ]), array([0.        , 0.0225    , 0.        , 0.97750002, 0.        ,\n",
                                    "       0.        , 0.        , 0.        , 0.        ]), array([0.03625, 0.     , 0.     , 0.095  , 0.     , 0.40875, 0.05   ,\n",
                                    "       0.     , 0.41   ]), array([0.15875, 0.     , 0.06   , 0.14   , 0.14375, 0.0575 , 0.1075 ,\n",
                                    "       0.1875 , 0.145  ])]\n",
                                    "NP Array Policies [[0.1125     0.0575     0.0525     0.05       0.47874999 0.0575\n",
                                    "  0.06       0.045      0.08625   ]\n",
                                    " [0.07125    0.         0.05625    0.0575     0.55500001 0.0625\n",
                                    "  0.05625    0.07       0.07125   ]\n",
                                    " [0.22750001 0.095      0.0775     0.11125    0.         0.15125\n",
                                    "  0.16125    0.07875    0.0975    ]\n",
                                    " [0.         0.02       0.02125    0.         0.         0.95875001\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.115      0.115      0.1025     0.0975     0.14749999 0.105\n",
                                    "  0.1125     0.09625    0.10875   ]\n",
                                    " [0.         0.01875    0.91374999 0.         0.         0.0225\n",
                                    "  0.         0.02       0.025     ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  1.         0.         0.        ]\n",
                                    " [0.         0.375      0.41624999 0.         0.         0.\n",
                                    "  0.02375    0.12375    0.06125   ]\n",
                                    " [0.12625    0.15625    0.         0.115      0.11125    0.1275\n",
                                    "  0.155      0.125      0.08375   ]\n",
                                    " [0.         0.         0.         1.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.12625    0.12625    0.         0.07875    0.22750001 0.14875001\n",
                                    "  0.1025     0.07875    0.11125   ]\n",
                                    " [0.         0.92374998 0.         0.         0.         0.02375\n",
                                    "  0.         0.02375    0.02875   ]\n",
                                    " [0.         0.52249998 0.0425     0.         0.         0.\n",
                                    "  0.         0.38749999 0.0475    ]\n",
                                    " [0.         0.0225     0.         0.97750002 0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.03625    0.         0.         0.095      0.         0.40875\n",
                                    "  0.05       0.         0.41      ]\n",
                                    " [0.15875    0.         0.06       0.14       0.14375    0.0575\n",
                                    "  0.1075     0.1875     0.145     ]]\n",
                                    "Predicted: tensor([[0.1133, 0.1026, 0.1001, 0.1209, 0.1116, 0.1143, 0.1136, 0.1188, 0.1048],\n",
                                    "        [0.1052, 0.1333, 0.0887, 0.1451, 0.1001, 0.1192, 0.0928, 0.1300, 0.0857],\n",
                                    "        [0.1002, 0.1322, 0.0830, 0.1536, 0.1038, 0.1145, 0.0936, 0.1313, 0.0878],\n",
                                    "        [0.1063, 0.1018, 0.0766, 0.1520, 0.0960, 0.1188, 0.1112, 0.1331, 0.1041],\n",
                                    "        [0.1133, 0.1026, 0.1001, 0.1209, 0.1116, 0.1143, 0.1136, 0.1188, 0.1048],\n",
                                    "        [0.1030, 0.1008, 0.0871, 0.1415, 0.1029, 0.1110, 0.1128, 0.1394, 0.1014],\n",
                                    "        [0.1133, 0.0970, 0.0872, 0.1385, 0.0965, 0.1188, 0.1098, 0.1281, 0.1108],\n",
                                    "        [0.1100, 0.0994, 0.0876, 0.1298, 0.1009, 0.1152, 0.1082, 0.1387, 0.1102],\n",
                                    "        [0.1078, 0.1305, 0.0882, 0.1414, 0.1035, 0.1200, 0.0962, 0.1219, 0.0905],\n",
                                    "        [0.1056, 0.1007, 0.0770, 0.1626, 0.0967, 0.1172, 0.1087, 0.1255, 0.1060],\n",
                                    "        [0.1078, 0.1305, 0.0882, 0.1414, 0.1035, 0.1200, 0.0962, 0.1219, 0.0905],\n",
                                    "        [0.1092, 0.1336, 0.0826, 0.1409, 0.1012, 0.1210, 0.0931, 0.1237, 0.0947],\n",
                                    "        [0.0982, 0.1336, 0.0757, 0.1676, 0.0961, 0.1140, 0.0894, 0.1351, 0.0903],\n",
                                    "        [0.0988, 0.1312, 0.0728, 0.1752, 0.0964, 0.1094, 0.0914, 0.1312, 0.0937],\n",
                                    "        [0.1192, 0.0994, 0.0934, 0.1221, 0.1004, 0.1202, 0.1100, 0.1207, 0.1146],\n",
                                    "        [0.1052, 0.1333, 0.0887, 0.1451, 0.1001, 0.1192, 0.0928, 0.1300, 0.0857]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.1133, 0.1026, 0.1001, 0.1209, 0.1116, 0.1143, 0.1136, 0.1188, 0.1048],\n",
                                    "        [0.1052, 0.1333, 0.0887, 0.1451, 0.1001, 0.1192, 0.0928, 0.1300, 0.0857],\n",
                                    "        [0.1002, 0.1322, 0.0830, 0.1536, 0.1038, 0.1145, 0.0936, 0.1313, 0.0878],\n",
                                    "        [0.1063, 0.1018, 0.0766, 0.1520, 0.0960, 0.1188, 0.1112, 0.1331, 0.1041],\n",
                                    "        [0.1133, 0.1026, 0.1001, 0.1209, 0.1116, 0.1143, 0.1136, 0.1188, 0.1048],\n",
                                    "        [0.1030, 0.1008, 0.0871, 0.1415, 0.1029, 0.1110, 0.1128, 0.1394, 0.1014],\n",
                                    "        [0.1133, 0.0970, 0.0872, 0.1385, 0.0965, 0.1188, 0.1098, 0.1281, 0.1108],\n",
                                    "        [0.1100, 0.0994, 0.0876, 0.1298, 0.1009, 0.1152, 0.1082, 0.1387, 0.1102],\n",
                                    "        [0.1078, 0.1305, 0.0882, 0.1414, 0.1035, 0.1200, 0.0962, 0.1219, 0.0905],\n",
                                    "        [0.1056, 0.1007, 0.0770, 0.1626, 0.0967, 0.1172, 0.1087, 0.1255, 0.1060],\n",
                                    "        [0.1078, 0.1305, 0.0882, 0.1414, 0.1035, 0.1200, 0.0962, 0.1219, 0.0905],\n",
                                    "        [0.1092, 0.1336, 0.0826, 0.1409, 0.1012, 0.1210, 0.0931, 0.1237, 0.0947],\n",
                                    "        [0.0982, 0.1336, 0.0757, 0.1676, 0.0961, 0.1140, 0.0894, 0.1351, 0.0903],\n",
                                    "        [0.0988, 0.1312, 0.0728, 0.1752, 0.0964, 0.1094, 0.0914, 0.1312, 0.0937],\n",
                                    "        [0.1192, 0.0994, 0.0934, 0.1221, 0.1004, 0.1202, 0.1100, 0.1207, 0.1146],\n",
                                    "        [0.1052, 0.1333, 0.0887, 0.1451, 0.1001, 0.1192, 0.0928, 0.1300, 0.0857]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.1133, 0.1026, 0.1001, 0.1209, 0.1116, 0.1143, 0.1136, 0.1188, 0.1048],\n",
                                    "        [0.1052, 0.1333, 0.0887, 0.1451, 0.1001, 0.1192, 0.0928, 0.1300, 0.0857],\n",
                                    "        [0.1002, 0.1322, 0.0830, 0.1536, 0.1038, 0.1145, 0.0936, 0.1313, 0.0878],\n",
                                    "        [0.1063, 0.1018, 0.0766, 0.1520, 0.0960, 0.1188, 0.1112, 0.1331, 0.1041],\n",
                                    "        [0.1133, 0.1026, 0.1001, 0.1209, 0.1116, 0.1143, 0.1136, 0.1188, 0.1048],\n",
                                    "        [0.1030, 0.1008, 0.0871, 0.1415, 0.1029, 0.1110, 0.1128, 0.1394, 0.1014],\n",
                                    "        [0.1133, 0.0970, 0.0872, 0.1385, 0.0965, 0.1188, 0.1098, 0.1281, 0.1108],\n",
                                    "        [0.1100, 0.0994, 0.0876, 0.1298, 0.1009, 0.1152, 0.1082, 0.1387, 0.1102],\n",
                                    "        [0.1078, 0.1305, 0.0882, 0.1414, 0.1035, 0.1200, 0.0962, 0.1219, 0.0905],\n",
                                    "        [0.1056, 0.1007, 0.0770, 0.1626, 0.0967, 0.1172, 0.1087, 0.1255, 0.1060],\n",
                                    "        [0.1078, 0.1305, 0.0882, 0.1414, 0.1035, 0.1200, 0.0962, 0.1219, 0.0905],\n",
                                    "        [0.1092, 0.1336, 0.0826, 0.1409, 0.1012, 0.1210, 0.0931, 0.1237, 0.0947],\n",
                                    "        [0.0982, 0.1336, 0.0757, 0.1676, 0.0961, 0.1140, 0.0894, 0.1351, 0.0903],\n",
                                    "        [0.0988, 0.1312, 0.0728, 0.1752, 0.0964, 0.1094, 0.0914, 0.1312, 0.0937],\n",
                                    "        [0.1192, 0.0994, 0.0934, 0.1221, 0.1004, 0.1202, 0.1100, 0.1207, 0.1146],\n",
                                    "        [0.1052, 0.1333, 0.0887, 0.1451, 0.1001, 0.1192, 0.0928, 0.1300, 0.0857]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.1778, -2.2766, -2.3012, -2.1131, -2.1928, -2.1692, -2.1753, -2.1305,\n",
                                    "         -2.2554],\n",
                                    "        [-2.2519, -2.0150, -2.4229, -1.9301, -2.3021, -2.1271, -2.3777, -2.0405,\n",
                                    "         -2.4567],\n",
                                    "        [-2.3006, -2.0235, -2.4886, -1.8733, -2.2656, -2.1669, -2.3688, -2.0304,\n",
                                    "         -2.4329],\n",
                                    "        [-2.2419, -2.2843, -2.5686, -1.8837, -2.3432, -2.1300, -2.1961, -2.0168,\n",
                                    "         -2.2628],\n",
                                    "        [-2.1778, -2.2766, -2.3012, -2.1131, -2.1928, -2.1692, -2.1753, -2.1305,\n",
                                    "         -2.2554],\n",
                                    "        [-2.2726, -2.2945, -2.4403, -1.9555, -2.2738, -2.1983, -2.1818, -1.9707,\n",
                                    "         -2.2886],\n",
                                    "        [-2.1775, -2.3327, -2.4398, -1.9772, -2.3383, -2.1302, -2.2094, -2.0549,\n",
                                    "         -2.1997],\n",
                                    "        [-2.2076, -2.3085, -2.4345, -2.0419, -2.2937, -2.1607, -2.2242, -1.9754,\n",
                                    "         -2.2054],\n",
                                    "        [-2.2275, -2.0364, -2.4283, -1.9563, -2.2682, -2.1203, -2.3410, -2.1046,\n",
                                    "         -2.4022],\n",
                                    "        [-2.2483, -2.2957, -2.5634, -1.8163, -2.3366, -2.1436, -2.2188, -2.0756,\n",
                                    "         -2.2447],\n",
                                    "        [-2.2275, -2.0364, -2.4283, -1.9563, -2.2682, -2.1203, -2.3410, -2.1046,\n",
                                    "         -2.4022],\n",
                                    "        [-2.2147, -2.0131, -2.4934, -1.9594, -2.2906, -2.1123, -2.3741, -2.0896,\n",
                                    "         -2.3574],\n",
                                    "        [-2.3210, -2.0129, -2.5805, -1.7860, -2.3422, -2.1715, -2.4147, -2.0020,\n",
                                    "         -2.4047],\n",
                                    "        [-2.3150, -2.0307, -2.6206, -1.7416, -2.3392, -2.2131, -2.3930, -2.0311,\n",
                                    "         -2.3679],\n",
                                    "        [-2.1267, -2.3082, -2.3708, -2.1032, -2.2982, -2.1185, -2.2075, -2.1148,\n",
                                    "         -2.1664],\n",
                                    "        [-2.2519, -2.0150, -2.4229, -1.9301, -2.3021, -2.1271, -2.3777, -2.0405,\n",
                                    "         -2.4567]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.19349761 2.1468785 2.3403761\n",
                                    "score:  0\n",
                                    "score:  0\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_8/videos/alphazero/8/alphazero-episode-39.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_8/videos/alphazero/8/alphazero-episode-39.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                          \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_8/videos/alphazero/8/alphazero-episode-39.mp4\n",
                                    "score:  1\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "Training Game  1\n",
                                    "Target Policy [0.61624998 0.04375    0.03625    0.04125    0.06875    0.04375\n",
                                    " 0.05       0.05375    0.04625   ]\n",
                                    "Temperature Policy  [0.61625 0.04375 0.03625 0.04125 0.06875 0.04375 0.05    0.05375 0.04625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.18875    0.105      0.09875    0.0975     0.         0.15000001\n",
                                    " 0.15625    0.09875    0.105     ]\n",
                                    "Temperature Policy  [0.18875 0.105   0.09875 0.0975  0.15    0.15625 0.09875 0.105  ]\n",
                                    "Action  7\n",
                                    "Target Policy [0.01625    0.0175     0.0175     0.0225     0.         0.89125001\n",
                                    " 0.01625    0.         0.01875   ]\n",
                                    "Temperature Policy  [0.01625 0.0175  0.0175  0.0225  0.89125 0.01625 0.01875]\n",
                                    "Action  5\n",
                                    "Target Policy [0.09875    0.10375    0.1        0.45875001 0.         0.\n",
                                    " 0.12125    0.         0.1175    ]\n",
                                    "Temperature Policy  [0.09875 0.10375 0.1     0.45875 0.12125 0.1175 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.02875    0.         0.02375    0.88499999 0.         0.\n",
                                    " 0.03375    0.         0.02875   ]\n",
                                    "Temperature Policy  [1.3090185e-15 1.9373293e-16 1.0000000e+00 6.5058653e-15 1.3090185e-15]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Target Policy [0.075      0.04875    0.03       0.035      0.62625003 0.03875\n",
                                    " 0.0625     0.0375     0.04625   ]\n",
                                    "Temperature Policy  [0.075   0.04875 0.03    0.035   0.62625 0.03875 0.0625  0.0375  0.04625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.18875 0.09875 0.10125 0.0975  0.      0.16    0.1525  0.09625 0.105  ]\n",
                                    "Temperature Policy  [0.18875 0.09875 0.10125 0.0975  0.16    0.1525  0.09625 0.105  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.02       0.89499998 0.01625    0.02       0.         0.\n",
                                    " 0.01625    0.0175     0.015     ]\n",
                                    "Temperature Policy  [0.02    0.895   0.01625 0.02    0.01625 0.0175  0.015  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.08125    0.         0.09375    0.09       0.         0.\n",
                                    " 0.09625    0.53250003 0.10625   ]\n",
                                    "Temperature Policy  [0.08125 0.09375 0.09    0.09625 0.5325  0.10625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.01375    0.         0.         0.0125     0.         0.\n",
                                    " 0.015      0.93624997 0.0225    ]\n",
                                    "Temperature Policy  [4.6677616e-19 1.7996242e-19 1.1142799e-18 1.0000000e+00 6.4254993e-17]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  3\n",
                                    "Target Policy [0.60874999 0.04625    0.03875    0.055      0.06125    0.04625\n",
                                    " 0.055      0.04125    0.0475    ]\n",
                                    "Temperature Policy  [0.60875 0.04625 0.03875 0.055   0.06125 0.04625 0.055   0.04125 0.0475 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0725     0.075      0.155      0.21875    0.075\n",
                                    " 0.25874999 0.06875    0.07625   ]\n",
                                    "Temperature Policy  [0.0725  0.075   0.155   0.21875 0.075   0.25875 0.06875 0.07625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.02875    0.44624999 0.0325     0.35249999 0.03375\n",
                                    " 0.         0.0275     0.07875   ]\n",
                                    "Temperature Policy  [0.02875 0.44625 0.0325  0.3525  0.03375 0.0275  0.07875]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.44999999 0.         0.10125    0.0925     0.10625\n",
                                    " 0.         0.10625    0.14375   ]\n",
                                    "Temperature Policy  [0.45    0.10125 0.0925  0.10625 0.10625 0.14375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.02125    0.0175     0.025\n",
                                    " 0.         0.01625    0.92000002]\n",
                                    "Temperature Policy  [4.3222619e-17 6.2015782e-18 2.1954412e-16 2.9556663e-18 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.         0.         0.1075     0.39625001 0.31125\n",
                                    " 0.         0.185      0.        ]\n",
                                    "Temperature Policy  [1.9815377e-06 9.1750926e-01 8.2037285e-02 4.5147631e-04]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.         0.         0.02       0.         0.95625001\n",
                                    " 0.         0.02375    0.        ]\n",
                                    "Temperature Policy  [1.6017158e-17 1.0000000e+00 8.9314436e-17]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Target Policy [0.60000002 0.04375    0.04375    0.0425     0.07125    0.04625\n",
                                    " 0.05       0.03875    0.06375   ]\n",
                                    "Temperature Policy  [0.6     0.04375 0.04375 0.0425  0.07125 0.04625 0.05    0.03875 0.06375]\n",
                                    "Action  8\n",
                                    "Target Policy [0.0725     0.04       0.035      0.0375     0.69625002 0.03875\n",
                                    " 0.0425     0.0375     0.        ]\n",
                                    "Temperature Policy  [0.0725  0.04    0.035   0.0375  0.69625 0.03875 0.0425  0.0375 ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.01875    0.         0.01625    0.01875    0.0175     0.015\n",
                                    " 0.89625001 0.0175     0.        ]\n",
                                    "Temperature Policy  [0.01875 0.01625 0.01875 0.0175  0.015   0.89625 0.0175 ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.08625    0.         0.10375    0.08875    0.13500001 0.09\n",
                                    " 0.         0.49625    0.        ]\n",
                                    "Temperature Policy  [0.08625 0.10375 0.08875 0.135   0.09    0.49625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.         0.0325     0.01375    0.01375    0.0125\n",
                                    " 0.         0.92750001 0.        ]\n",
                                    "Temperature Policy  [2.7905799e-15 5.1272888e-19 5.1272888e-19 1.9767918e-19 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  5\n",
                                    "Target Policy [0.59375 0.04875 0.04125 0.05    0.07    0.0475  0.05625 0.0375  0.055  ]\n",
                                    "Temperature Policy  [0.59375 0.04875 0.04125 0.05    0.07    0.0475  0.05625 0.0375  0.055  ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.07375    0.075      0.1575     0.21625    0.075\n",
                                    " 0.25874999 0.06875    0.075     ]\n",
                                    "Temperature Policy  [0.07375 0.075   0.1575  0.21625 0.075   0.25875 0.06875 0.075  ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.015      0.0125     0.03375    0.015      0.015\n",
                                    " 0.88499999 0.02375    0.        ]\n",
                                    "Temperature Policy  [0.015   0.0125  0.03375 0.015   0.015   0.885   0.02375]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.09125 0.0725  0.56    0.09625 0.085   0.      0.095   0.     ]\n",
                                    "Temperature Policy  [0.09125 0.0725  0.56    0.09625 0.085   0.095  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02125    0.92124999 0.         0.02125    0.0175\n",
                                    " 0.         0.01875    0.        ]\n",
                                    "Temperature Policy  [4.26397180e-17 1.00000000e+00 4.26397180e-17 6.11794403e-18\n",
                                    " 1.21965705e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.29499999 0.         0.         0.40625    0.19750001\n",
                                    " 0.         0.10125    0.        ]\n",
                                    "Temperature Policy  [3.9140444e-02 9.6015054e-01 7.0807064e-04 8.8788329e-07]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.96125001 0.         0.         0.         0.0225\n",
                                    " 0.         0.01625    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 4.9369835e-17 1.9062073e-18]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  6\n",
                                    "Target Policy [0.60500002 0.04375    0.0375     0.0575     0.06375    0.06\n",
                                    " 0.05       0.03875    0.04375   ]\n",
                                    "Temperature Policy  [0.605   0.04375 0.0375  0.0575  0.06375 0.06    0.05    0.03875 0.04375]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.07375    0.075      0.155      0.21875    0.07625\n",
                                    " 0.25749999 0.06875    0.075     ]\n",
                                    "Temperature Policy  [0.07375 0.075   0.155   0.21875 0.07625 0.2575  0.06875 0.075  ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.25125    0.0625     0.19125    0.         0.0725\n",
                                    " 0.29499999 0.06125    0.06625   ]\n",
                                    "Temperature Policy  [0.25125 0.0625  0.19125 0.0725  0.295   0.06125 0.06625]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02       0.01625    0.         0.         0.02125\n",
                                    " 0.89999998 0.0225     0.02      ]\n",
                                    "Temperature Policy  [0.02    0.01625 0.02125 0.9     0.0225  0.02   ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.0225  0.91125 0.      0.      0.02125 0.      0.02375 0.02125]\n",
                                    "Temperature Policy  [8.4226700e-17 1.0000000e+00 4.7556943e-17 1.4463078e-16 4.7556943e-17]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.93000001 0.         0.         0.         0.02\n",
                                    " 0.         0.02625    0.02375   ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.1157798e-17 3.2096937e-16 1.1797953e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.025\n",
                                    " 0.         0.95375001 0.02125   ]\n",
                                    "Temperature Policy  [1.5312798e-16 1.0000000e+00 3.0146981e-17]\n",
                                    "Action  7\n",
                                    "Target Policy [0.     0.     0.     0.     0.     0.5025 0.     0.     0.4975]\n",
                                    "Temperature Policy  [0.5249794 0.4750206]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  7\n",
                                    "Target Policy [0.60500002 0.0425     0.0375     0.04625    0.065      0.04375\n",
                                    " 0.06375    0.04625    0.05      ]\n",
                                    "Temperature Policy  [0.605   0.0425  0.0375  0.04625 0.065   0.04375 0.06375 0.04625 0.05   ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.10875    0.0725     0.07375    0.06625    0.44749999 0.08\n",
                                    " 0.         0.07375    0.0775    ]\n",
                                    "Temperature Policy  [0.10875 0.0725  0.07375 0.06625 0.4475  0.08    0.07375 0.0775 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.29499999 0.0625     0.05375    0.175      0.         0.0525\n",
                                    " 0.         0.29249999 0.06875   ]\n",
                                    "Temperature Policy  [0.295   0.0625  0.05375 0.175   0.0525  0.2925  0.06875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.01875    0.01625    0.91000003 0.         0.0175\n",
                                    " 0.         0.02       0.0175    ]\n",
                                    "Temperature Policy  [0.01875 0.01625 0.91    0.0175  0.02    0.0175 ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.02125    0.025      0.         0.         0.91000003\n",
                                    " 0.         0.0225     0.02125   ]\n",
                                    "Temperature Policy  [4.8214254e-17 2.4489853e-16 1.0000000e+00 8.5390841e-17 4.8214254e-17]\n",
                                    "Action  5\n",
                                    "Target Policy [0.      0.4375  0.0425  0.      0.      0.      0.      0.47375 0.04625]\n",
                                    "Temperature Policy  [3.1087530e-01 2.3264566e-11 6.8912476e-01 5.4190478e-11]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.95749998 0.02125    0.         0.         0.\n",
                                    " 0.         0.         0.02125   ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.8986884e-17 2.8986884e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.      0.      0.97125 0.      0.      0.      0.      0.      0.02875]\n",
                                    "Temperature Policy  [1.000000e+00 5.164974e-16]\n",
                                    "Action  2\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  8\n",
                                    "Target Policy [0.07875 0.04    0.0375  0.035   0.63    0.045   0.05    0.0375  0.04625]\n",
                                    "Temperature Policy  [0.07875 0.04    0.0375  0.035   0.63    0.045   0.05    0.0375  0.04625]\n",
                                    "Action  8\n",
                                    "Target Policy [0.08875    0.0475     0.0325     0.04       0.64999998 0.04375\n",
                                    " 0.05625    0.04125    0.        ]\n",
                                    "Temperature Policy  [0.08875 0.0475  0.0325  0.04    0.65    0.04375 0.05625 0.04125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.06625 0.06    0.06125 0.0525  0.      0.59625 0.06875 0.095   0.     ]\n",
                                    "Temperature Policy  [0.06625 0.06    0.06125 0.0525  0.59625 0.06875 0.095  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.01625    0.02625    0.89999998 0.02       0.         0.\n",
                                    " 0.01625    0.02125    0.        ]\n",
                                    "Temperature Policy  [0.01625 0.02625 0.9     0.02    0.01625 0.02125]\n",
                                    "Action  2\n",
                                    "Target Policy [0.0225  0.02    0.      0.025   0.      0.      0.90875 0.02375 0.     ]\n",
                                    "Temperature Policy  [8.6572703e-17 2.6659730e-17 2.4828809e-16 1.0000000e+00 1.4865925e-16]\n",
                                    "Action  6\n",
                                    "Target Policy [0.02       0.0225     0.         0.02375    0.         0.\n",
                                    " 0.         0.93374997 0.        ]\n",
                                    "Temperature Policy  [2.0323280e-17 6.5996215e-17 1.1332611e-16 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.02125    0.95375001 0.         0.025      0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [3.0146981e-17 1.0000000e+00 1.5312798e-16]\n",
                                    "Action  1\n",
                                    "Target Policy [0.51875001 0.         0.         0.48124999 0.         0.\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.67925537 0.32074463]\n",
                                    "Action  3\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  9\n",
                                    "Target Policy [0.59249997 0.05       0.03625    0.04125    0.07375    0.0425\n",
                                    " 0.05       0.04125    0.0725    ]\n",
                                    "Temperature Policy  [0.5925  0.05    0.03625 0.04125 0.07375 0.0425  0.05    0.04125 0.0725 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.0725     0.07625    0.1575     0.21375    0.07625\n",
                                    " 0.25874999 0.06875    0.07625   ]\n",
                                    "Temperature Policy  [0.0725  0.07625 0.1575  0.21375 0.07625 0.25875 0.06875 0.07625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.03       0.37125    0.0375     0.41874999 0.03875\n",
                                    " 0.         0.03       0.07375   ]\n",
                                    "Temperature Policy  [0.03    0.37125 0.0375  0.41875 0.03875 0.03    0.07375]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.44999999 0.         0.09625    0.10375    0.1125\n",
                                    " 0.         0.10625    0.13124999]\n",
                                    "Temperature Policy  [0.45    0.09625 0.10375 0.1125  0.10625 0.13125]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.91624999 0.         0.0225     0.02375    0.\n",
                                    " 0.         0.02125    0.01625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 7.9741667e-17 1.3692926e-16 4.5024559e-17 3.0788872e-18]\n",
                                    "Action  1\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  10\n",
                                    "Target Policy [0.59625 0.04625 0.04    0.04625 0.0775  0.0525  0.055   0.0375  0.04875]\n",
                                    "Temperature Policy  [0.59625 0.04625 0.04    0.04625 0.0775  0.0525  0.055   0.0375  0.04875]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.07375    0.075      0.155      0.215      0.07625\n",
                                    " 0.25874999 0.07       0.07625   ]\n",
                                    "Temperature Policy  [0.07375 0.075   0.155   0.215   0.07625 0.25875 0.07    0.07625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.01625    0.         0.02625    0.015      0.01625\n",
                                    " 0.89499998 0.01625    0.015     ]\n",
                                    "Temperature Policy  [0.01625 0.02625 0.015   0.01625 0.895   0.01625 0.015  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.1025  0.      0.46375 0.1025  0.11    0.      0.10625 0.115  ]\n",
                                    "Temperature Policy  [0.1025  0.46375 0.1025  0.11    0.10625 0.115  ]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.01875    0.         0.         0.01625    0.02\n",
                                    " 0.         0.02125    0.92374998]\n",
                                    "Temperature Policy  [1.18704785e-17 2.83784823e-18 2.26336948e-17 4.14996877e-17\n",
                                    " 1.00000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.         0.09875    0.         0.         0.39750001 0.19499999\n",
                                    " 0.         0.30875    0.        ]\n",
                                    "Temperature Policy  [8.2847629e-07 9.2529535e-01 7.4688747e-04 7.3956907e-02]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.02       0.         0.         0.         0.02125\n",
                                    " 0.         0.95875001 0.        ]\n",
                                    "Temperature Policy  [1.5604369e-17 2.8611168e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Target Policy [0.60000002 0.0475     0.045      0.0425     0.0775     0.0525\n",
                                    " 0.05       0.0375     0.0475    ]\n",
                                    "Temperature Policy  [0.6    0.0475 0.045  0.0425 0.0775 0.0525 0.05   0.0375 0.0475]\n",
                                    "Action  3\n",
                                    "Target Policy [0.1175  0.09375 0.04875 0.      0.40625 0.08875 0.115   0.0425  0.0875 ]\n",
                                    "Temperature Policy  [0.1175  0.09375 0.04875 0.40625 0.08875 0.115   0.0425  0.0875 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.14749999 0.0525     0.0525     0.         0.         0.055\n",
                                    " 0.58749998 0.05625    0.04875   ]\n",
                                    "Temperature Policy  [0.1475  0.0525  0.0525  0.055   0.5875  0.05625 0.04875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.90375 0.025   0.01625 0.      0.      0.02    0.      0.01875 0.01625]\n",
                                    "Temperature Policy  [0.90375 0.025   0.01625 0.02    0.01875 0.01625]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.02375    0.01625    0.         0.         0.02125\n",
                                    " 0.         0.0225     0.91624999]\n",
                                    "Temperature Policy  [1.3692926e-16 3.0788872e-18 4.5024559e-17 7.9741667e-17 1.0000000e+00]\n",
                                    "Action  8\n",
                                    "Target Policy [0.      0.02375 0.02    0.      0.      0.02125 0.      0.935   0.     ]\n",
                                    "Temperature Policy  [1.1182014e-16 2.0053207e-17 3.6768271e-17 1.0000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.         0.94999999 0.02625    0.         0.         0.02375\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [1.000000e+00 2.594520e-16 9.536743e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.44374999 0.         0.         0.55624998\n",
                                    " 0.         0.         0.        ]\n",
                                    "Temperature Policy  [0.09452761 0.9054724 ]\n",
                                    "Action  5\n",
                                    "Target Policy [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Target Policy [0.61500001 0.0475     0.05125    0.0425     0.055      0.04375\n",
                                    " 0.055      0.0425     0.0475    ]\n",
                                    "Temperature Policy  [0.615   0.0475  0.05125 0.0425  0.055   0.04375 0.055   0.0425  0.0475 ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.075      0.07625    0.16       0.18125001 0.075\n",
                                    " 0.28625    0.07       0.07625   ]\n",
                                    "Temperature Policy  [0.075   0.07625 0.16    0.18125 0.075   0.28625 0.07    0.07625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.015      0.         0.0225     0.01625    0.015\n",
                                    " 0.89749998 0.01875    0.015     ]\n",
                                    "Temperature Policy  [0.015   0.0225  0.01625 0.015   0.8975  0.01875 0.015  ]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.1075  0.      0.4675  0.1025  0.1075  0.      0.10125 0.11375]\n",
                                    "Temperature Policy  [0.1075  0.4675  0.1025  0.1075  0.10125 0.11375]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.89249998 0.03       0.0175\n",
                                    " 0.         0.03375    0.02625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.8413245e-15 8.4003674e-18 5.9793716e-15 4.8440750e-16]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  13\n",
                                    "Target Policy [0.58875 0.05    0.04125 0.04375 0.0725  0.0575  0.055   0.04    0.05125]\n",
                                    "Temperature Policy  [0.58875 0.05    0.04125 0.04375 0.0725  0.0575  0.055   0.04    0.05125]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.07375 0.075   0.16    0.18375 0.075   0.28625 0.07    0.07625]\n",
                                    "Temperature Policy  [0.07375 0.075   0.16    0.18375 0.075   0.28625 0.07    0.07625]\n",
                                    "Action  2\n",
                                    "Target Policy [0.         0.015      0.         0.01625    0.01875    0.01625\n",
                                    " 0.89999998 0.0175     0.01625   ]\n",
                                    "Temperature Policy  [0.015   0.01625 0.01875 0.01625 0.9     0.0175  0.01625]\n",
                                    "Action  6\n",
                                    "Target Policy [0.      0.105   0.      0.46375 0.10375 0.10875 0.      0.1025  0.11625]\n",
                                    "Temperature Policy  [0.105   0.46375 0.10375 0.10875 0.1025  0.11625]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.015      0.         0.92500001 0.01375    0.\n",
                                    " 0.         0.01375    0.0325    ]\n",
                                    "Temperature Policy  [1.2574632e-18 1.0000000e+00 5.2675621e-19 5.2675621e-19 2.8669249e-15]\n",
                                    "Action  3\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  14\n",
                                    "Target Policy [0.60624999 0.05       0.035      0.0425     0.065      0.0575\n",
                                    " 0.05375    0.04       0.05      ]\n",
                                    "Temperature Policy  [0.60625 0.05    0.035   0.0425  0.065   0.0575  0.05375 0.04    0.05   ]\n",
                                    "Action  0\n",
                                    "Target Policy [0.         0.07375    0.075      0.1575     0.21250001 0.07625\n",
                                    " 0.25874999 0.07       0.07625   ]\n",
                                    "Temperature Policy  [0.07375 0.075   0.1575  0.2125  0.07625 0.25875 0.07    0.07625]\n",
                                    "Action  4\n",
                                    "Target Policy [0.         0.51749998 0.0625     0.14749999 0.         0.0725\n",
                                    " 0.085      0.0625     0.0525    ]\n",
                                    "Temperature Policy  [0.5175 0.0625 0.1475 0.0725 0.085  0.0625 0.0525]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.89499998 0.0225     0.         0.01875\n",
                                    " 0.0225     0.02375    0.0175    ]\n",
                                    "Temperature Policy  [0.895   0.0225  0.01875 0.0225  0.02375 0.0175 ]\n",
                                    "Action  2\n",
                                    "Target Policy [0.      0.      0.      0.0225  0.      0.02375 0.91125 0.0225  0.02   ]\n",
                                    "Temperature Policy  [8.4226700e-17 1.4463078e-16 1.0000000e+00 8.4226700e-17 2.5937287e-17]\n",
                                    "Action  6\n",
                                    "Target Policy [0.         0.         0.         0.93374997 0.         0.02125\n",
                                    " 0.         0.025      0.02      ]\n",
                                    "Temperature Policy  [1.000000e+00 3.726346e-17 1.892753e-16 2.032328e-17]\n",
                                    "Action  3\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.94999999\n",
                                    " 0.         0.02375    0.02625   ]\n",
                                    "Temperature Policy  [1.000000e+00 9.536743e-17 2.594520e-16]\n",
                                    "Action  5\n",
                                    "Target Policy [0.         0.         0.         0.         0.         0.\n",
                                    " 0.         0.53874999 0.46125001]\n",
                                    "Temperature Policy  [0.8253631  0.17463689]\n",
                                    "Action  7\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  8\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  15\n",
                                    "Target Policy [0.60874999 0.05       0.03875    0.04875    0.06125    0.05\n",
                                    " 0.05       0.0375     0.055     ]\n",
                                    "Temperature Policy  [0.60875 0.05    0.03875 0.04875 0.06125 0.05    0.05    0.0375  0.055  ]\n",
                                    "Action  1\n",
                                    "Target Policy [0.08875    0.         0.0475     0.05       0.56999999 0.06375\n",
                                    " 0.06125    0.055      0.06375   ]\n",
                                    "Temperature Policy  [0.08875 0.0475  0.05    0.57    0.06375 0.06125 0.055   0.06375]\n",
                                    "Action  3\n",
                                    "Target Policy [0.02       0.         0.01375    0.         0.89999998 0.015\n",
                                    " 0.01625    0.0175     0.0175    ]\n",
                                    "Temperature Policy  [0.02    0.01375 0.9     0.015   0.01625 0.0175  0.0175 ]\n",
                                    "Action  4\n",
                                    "Target Policy [0.12375    0.         0.09       0.         0.         0.095\n",
                                    " 0.1225     0.47999999 0.08875   ]\n",
                                    "Temperature Policy  [0.12375 0.09    0.095   0.1225  0.48    0.08875]\n",
                                    "Action  6\n",
                                    "Target Policy [0.03       0.         0.01375    0.         0.         0.01375\n",
                                    " 0.         0.92874998 0.01375   ]\n",
                                    "Temperature Policy  [1.2365859e-15 5.0586975e-19 5.0586975e-19 1.0000000e+00 5.0586975e-19]\n",
                                    "Action  7\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1]\n",
                                    "Training Game  16\n",
                                    "Target Policy [0.58875 0.0475  0.04125 0.04375 0.075   0.05    0.06125 0.0475  0.045  ]\n",
                                    "Temperature Policy  [0.58875 0.0475  0.04125 0.04375 0.075   0.05    0.06125 0.0475  0.045  ]\n",
                                    "Action  5\n",
                                    "Target Policy [0.07       0.0475     0.03125    0.04375    0.66500002 0.\n",
                                    " 0.04375    0.0575     0.04125   ]\n",
                                    "Temperature Policy  [0.07    0.0475  0.03125 0.04375 0.665   0.04375 0.0575  0.04125]\n",
                                    "Action  4\n",
                                    "Target Policy [0.0625     0.09375    0.41       0.05625    0.         0.\n",
                                    " 0.0525     0.08375    0.24124999]\n",
                                    "Temperature Policy  [0.0625  0.09375 0.41    0.05625 0.0525  0.08375 0.24125]\n",
                                    "Action  2\n",
                                    "Target Policy [0.01625    0.02       0.         0.025      0.         0.\n",
                                    " 0.02125    0.02       0.89749998]\n",
                                    "Temperature Policy  [0.01625 0.02    0.025   0.02125 0.02    0.8975 ]\n",
                                    "Action  8\n",
                                    "Target Policy [0.91374999 0.02       0.         0.025      0.         0.\n",
                                    " 0.0175     0.02375    0.        ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.5236323e-17 2.3503158e-16 6.6390604e-18 1.4072208e-16]\n",
                                    "Action  0\n",
                                    "Target Policy [0.      0.9325  0.      0.02375 0.      0.      0.02125 0.0225  0.     ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.1485443e-16 3.7765996e-17 6.6886241e-17]\n",
                                    "Action  1\n",
                                    "Target Policy [0.         0.         0.         0.02625    0.         0.\n",
                                    " 0.02375    0.94999999 0.        ]\n",
                                    "Temperature Policy  [2.594520e-16 9.536743e-17 1.000000e+00]\n",
                                    "Action  7\n",
                                    "Target Policy [0.     0.     0.     0.5675 0.     0.     0.4325 0.     0.    ]\n",
                                    "Temperature Policy  [0.9379982  0.06200182]\n",
                                    "Action  3\n",
                                    "Target Policy [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Game Indices [(<replay_buffers.base_replay_buffer.Game object at 0x3321ca6b0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c2ce0>, 4), (<replay_buffers.base_replay_buffer.Game object at 0x32c608280>, 7), (<replay_buffers.base_replay_buffer.Game object at 0x32878c850>, 8), (<replay_buffers.base_replay_buffer.Game object at 0x32d8c1480>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32ec69b40>, 5), (<replay_buffers.base_replay_buffer.Game object at 0x325b831c0>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32d844850>, 1), (<replay_buffers.base_replay_buffer.Game object at 0x32878c850>, 3), (<replay_buffers.base_replay_buffer.Game object at 0x32c6fdd50>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32d856290>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3286f4310>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x31a517760>, 0), (<replay_buffers.base_replay_buffer.Game object at 0x32d855d50>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x3287247c0>, 2), (<replay_buffers.base_replay_buffer.Game object at 0x33217f4c0>, 3)]\n",
                                    "Observations [array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 1., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[1., 1., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 1.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [1., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 1.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 1., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 1., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [1., 0., 0.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[0., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [0., 0., 0.]]]), array([[[0., 0., 0.],\n",
                                    "        [0., 0., 1.],\n",
                                    "        [0., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 0., 0.],\n",
                                    "        [0., 0., 0.],\n",
                                    "        [1., 0., 0.]],\n",
                                    "\n",
                                    "       [[1., 1., 1.],\n",
                                    "        [1., 1., 1.],\n",
                                    "        [1., 1., 1.]]])]\n",
                                    "Policies [array([0.        , 0.0425    , 0.10625   , 0.64625001, 0.        ,\n",
                                    "       0.04375   , 0.085     , 0.03125   , 0.045     ]), array([0.0575    , 0.10375   , 0.0325    , 0.        , 0.        ,\n",
                                    "       0.        , 0.38749999, 0.41874999, 0.        ]), array([0.        , 0.46250001, 0.53750002, 0.        , 0.        ,\n",
                                    "       0.        , 0.        , 0.        , 0.        ]), array([0., 0., 0., 0., 0., 0., 0., 1., 0.]), array([0.52499998, 0.045     , 0.05875   , 0.05875   , 0.06625   ,\n",
                                    "       0.05875   , 0.06625   , 0.05875   , 0.0625    ]), array([0.        , 0.        , 0.02125   , 0.        , 0.        ,\n",
                                    "       0.        , 0.02      , 0.93374997, 0.025     ]), array([0.27500001, 0.08      , 0.0675    , 0.19      , 0.0825    ,\n",
                                    "       0.06      , 0.1175    , 0.06      , 0.0675    ]), array([0.03375   , 0.03375   , 0.        , 0.04125   , 0.72874999,\n",
                                    "       0.05125   , 0.03625   , 0.03375   , 0.04125   ]), array([0.        , 0.        , 0.89875001, 0.025     , 0.        ,\n",
                                    "       0.02125   , 0.01625   , 0.01875   , 0.02      ]), array([0.115  , 0.105  , 0.07875, 0.08625, 0.255  , 0.08625, 0.11375,\n",
                                    "       0.0525 , 0.1075 ]), array([0.        , 0.        , 0.01625   , 0.02      , 0.01625   ,\n",
                                    "       0.0175    , 0.89875001, 0.0175    , 0.01375   ]), array([0.045     , 0.61874998, 0.0525    , 0.05      , 0.06      ,\n",
                                    "       0.04375   , 0.045     , 0.04      , 0.045     ]), array([0.05125   , 0.60500002, 0.05125   , 0.05      , 0.0575    ,\n",
                                    "       0.04375   , 0.04875   , 0.0425    , 0.05      ]), array([0.02      , 0.89499998, 0.01625   , 0.02      , 0.        ,\n",
                                    "       0.        , 0.01625   , 0.0175    , 0.015     ]), array([0.27875   , 0.05375   , 0.0525    , 0.        , 0.0625    ,\n",
                                    "       0.        , 0.44999999, 0.0475    , 0.055     ]), array([0.        , 0.0875    , 0.1       , 0.49125001, 0.11      ,\n",
                                    "       0.        , 0.        , 0.09375   , 0.1175    ])]\n",
                                    "NP Array Policies [[0.         0.0425     0.10625    0.64625001 0.         0.04375\n",
                                    "  0.085      0.03125    0.045     ]\n",
                                    " [0.0575     0.10375    0.0325     0.         0.         0.\n",
                                    "  0.38749999 0.41874999 0.        ]\n",
                                    " [0.         0.46250001 0.53750002 0.         0.         0.\n",
                                    "  0.         0.         0.        ]\n",
                                    " [0.         0.         0.         0.         0.         0.\n",
                                    "  0.         1.         0.        ]\n",
                                    " [0.52499998 0.045      0.05875    0.05875    0.06625    0.05875\n",
                                    "  0.06625    0.05875    0.0625    ]\n",
                                    " [0.         0.         0.02125    0.         0.         0.\n",
                                    "  0.02       0.93374997 0.025     ]\n",
                                    " [0.27500001 0.08       0.0675     0.19       0.0825     0.06\n",
                                    "  0.1175     0.06       0.0675    ]\n",
                                    " [0.03375    0.03375    0.         0.04125    0.72874999 0.05125\n",
                                    "  0.03625    0.03375    0.04125   ]\n",
                                    " [0.         0.         0.89875001 0.025      0.         0.02125\n",
                                    "  0.01625    0.01875    0.02      ]\n",
                                    " [0.115      0.105      0.07875    0.08625    0.255      0.08625\n",
                                    "  0.11375    0.0525     0.1075    ]\n",
                                    " [0.         0.         0.01625    0.02       0.01625    0.0175\n",
                                    "  0.89875001 0.0175     0.01375   ]\n",
                                    " [0.045      0.61874998 0.0525     0.05       0.06       0.04375\n",
                                    "  0.045      0.04       0.045     ]\n",
                                    " [0.05125    0.60500002 0.05125    0.05       0.0575     0.04375\n",
                                    "  0.04875    0.0425     0.05      ]\n",
                                    " [0.02       0.89499998 0.01625    0.02       0.         0.\n",
                                    "  0.01625    0.0175     0.015     ]\n",
                                    " [0.27875    0.05375    0.0525     0.         0.0625     0.\n",
                                    "  0.44999999 0.0475     0.055     ]\n",
                                    " [0.         0.0875     0.1        0.49125001 0.11       0.\n",
                                    "  0.         0.09375    0.1175    ]]\n",
                                    "Predicted: tensor([[0.1003, 0.1037, 0.0877, 0.1480, 0.1017, 0.1079, 0.1168, 0.1328, 0.1010],\n",
                                    "        [0.1186, 0.0977, 0.0812, 0.1252, 0.0905, 0.1157, 0.1204, 0.1293, 0.1214],\n",
                                    "        [0.1059, 0.1352, 0.0658, 0.1757, 0.0973, 0.1020, 0.0905, 0.1399, 0.0878],\n",
                                    "        [0.1011, 0.1025, 0.0659, 0.1666, 0.0934, 0.1161, 0.1115, 0.1346, 0.1082],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1042, 0.1316, 0.0731, 0.1597, 0.0940, 0.1075, 0.0931, 0.1549, 0.0820],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1138, 0.1386, 0.0783, 0.1472, 0.1100, 0.1089, 0.0916, 0.1271, 0.0846],\n",
                                    "        [0.1098, 0.1452, 0.0720, 0.1542, 0.0989, 0.1065, 0.0861, 0.1431, 0.0842],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1137, 0.0969, 0.1003, 0.1239, 0.0993, 0.1102, 0.1208, 0.1249, 0.1099],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1094, 0.0942, 0.0927, 0.1321, 0.0952, 0.1090, 0.1168, 0.1332, 0.1175],\n",
                                    "        [0.1063, 0.1018, 0.0955, 0.1317, 0.1017, 0.1080, 0.1164, 0.1266, 0.1120],\n",
                                    "        [0.1062, 0.1361, 0.0776, 0.1554, 0.1035, 0.1052, 0.0908, 0.1414, 0.0837]],\n",
                                    "       grad_fn=<SoftmaxBackward0>)\n",
                                    "Normalized Predicted: tensor([[0.1003, 0.1037, 0.0877, 0.1480, 0.1017, 0.1079, 0.1168, 0.1328, 0.1010],\n",
                                    "        [0.1186, 0.0977, 0.0812, 0.1252, 0.0905, 0.1157, 0.1204, 0.1293, 0.1214],\n",
                                    "        [0.1059, 0.1352, 0.0658, 0.1757, 0.0973, 0.1020, 0.0905, 0.1399, 0.0878],\n",
                                    "        [0.1011, 0.1025, 0.0659, 0.1666, 0.0934, 0.1161, 0.1115, 0.1346, 0.1082],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1042, 0.1316, 0.0731, 0.1597, 0.0940, 0.1075, 0.0931, 0.1549, 0.0820],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1138, 0.1386, 0.0783, 0.1472, 0.1100, 0.1089, 0.0916, 0.1271, 0.0846],\n",
                                    "        [0.1098, 0.1452, 0.0720, 0.1542, 0.0989, 0.1065, 0.0861, 0.1431, 0.0842],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1137, 0.0969, 0.1003, 0.1239, 0.0993, 0.1102, 0.1208, 0.1249, 0.1099],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1094, 0.0942, 0.0927, 0.1321, 0.0952, 0.1090, 0.1168, 0.1332, 0.1175],\n",
                                    "        [0.1063, 0.1018, 0.0955, 0.1317, 0.1017, 0.1080, 0.1164, 0.1266, 0.1120],\n",
                                    "        [0.1062, 0.1361, 0.0776, 0.1554, 0.1035, 0.1052, 0.0908, 0.1414, 0.0837]],\n",
                                    "       grad_fn=<DivBackward0>)\n",
                                    "Clamped Predicted: tensor([[0.1003, 0.1037, 0.0877, 0.1480, 0.1017, 0.1079, 0.1168, 0.1328, 0.1010],\n",
                                    "        [0.1186, 0.0977, 0.0812, 0.1252, 0.0905, 0.1157, 0.1204, 0.1293, 0.1214],\n",
                                    "        [0.1059, 0.1352, 0.0658, 0.1757, 0.0973, 0.1020, 0.0905, 0.1399, 0.0878],\n",
                                    "        [0.1011, 0.1025, 0.0659, 0.1666, 0.0934, 0.1161, 0.1115, 0.1346, 0.1082],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1042, 0.1316, 0.0731, 0.1597, 0.0940, 0.1075, 0.0931, 0.1549, 0.0820],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1138, 0.1386, 0.0783, 0.1472, 0.1100, 0.1089, 0.0916, 0.1271, 0.0846],\n",
                                    "        [0.1098, 0.1452, 0.0720, 0.1542, 0.0989, 0.1065, 0.0861, 0.1431, 0.0842],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1137, 0.0969, 0.1003, 0.1239, 0.0993, 0.1102, 0.1208, 0.1249, 0.1099],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1120, 0.1013, 0.1030, 0.1226, 0.1093, 0.1096, 0.1202, 0.1148, 0.1073],\n",
                                    "        [0.1094, 0.0942, 0.0927, 0.1321, 0.0952, 0.1090, 0.1168, 0.1332, 0.1175],\n",
                                    "        [0.1063, 0.1018, 0.0955, 0.1317, 0.1017, 0.1080, 0.1164, 0.1266, 0.1120],\n",
                                    "        [0.1062, 0.1361, 0.0776, 0.1554, 0.1035, 0.1052, 0.0908, 0.1414, 0.0837]],\n",
                                    "       grad_fn=<ClampBackward1>)\n",
                                    "Log Prob: tensor([[-2.3001, -2.2660, -2.4334, -1.9103, -2.2857, -2.2266, -2.1469, -2.0188,\n",
                                    "         -2.2928],\n",
                                    "        [-2.1320, -2.3262, -2.5111, -2.0777, -2.4026, -2.1565, -2.1168, -2.0452,\n",
                                    "         -2.1090],\n",
                                    "        [-2.2452, -2.0014, -2.7206, -1.7389, -2.3302, -2.2833, -2.4028, -1.9667,\n",
                                    "         -2.4329],\n",
                                    "        [-2.2913, -2.2774, -2.7193, -1.7921, -2.3710, -2.1537, -2.1933, -2.0054,\n",
                                    "         -2.2238],\n",
                                    "        [-2.1894, -2.2895, -2.2733, -2.0988, -2.2136, -2.2113, -2.1189, -2.1646,\n",
                                    "         -2.2321],\n",
                                    "        [-2.2618, -2.0281, -2.6162, -1.8345, -2.3642, -2.2303, -2.3744, -1.8649,\n",
                                    "         -2.5012],\n",
                                    "        [-2.1894, -2.2895, -2.2733, -2.0988, -2.2136, -2.2113, -2.1189, -2.1646,\n",
                                    "         -2.2321],\n",
                                    "        [-2.1734, -1.9761, -2.5474, -1.9161, -2.2074, -2.2176, -2.3907, -2.0627,\n",
                                    "         -2.4697],\n",
                                    "        [-2.2095, -1.9298, -2.6312, -1.8692, -2.3136, -2.2395, -2.4521, -1.9441,\n",
                                    "         -2.4748],\n",
                                    "        [-2.1894, -2.2895, -2.2733, -2.0988, -2.2136, -2.2113, -2.1189, -2.1646,\n",
                                    "         -2.2321],\n",
                                    "        [-2.1742, -2.3337, -2.2991, -2.0881, -2.3094, -2.2056, -2.1138, -2.0804,\n",
                                    "         -2.2079],\n",
                                    "        [-2.1894, -2.2895, -2.2733, -2.0988, -2.2136, -2.2113, -2.1189, -2.1646,\n",
                                    "         -2.2321],\n",
                                    "        [-2.1894, -2.2895, -2.2733, -2.0988, -2.2136, -2.2113, -2.1189, -2.1646,\n",
                                    "         -2.2321],\n",
                                    "        [-2.2129, -2.3622, -2.3785, -2.0243, -2.3514, -2.2164, -2.1477, -2.0163,\n",
                                    "         -2.1414],\n",
                                    "        [-2.2413, -2.2844, -2.3490, -2.0269, -2.2861, -2.2256, -2.1505, -2.0668,\n",
                                    "         -2.1896],\n",
                                    "        [-2.2424, -1.9942, -2.5567, -1.8615, -2.2685, -2.2516, -2.3992, -1.9560,\n",
                                    "         -2.4801]], grad_fn=<LogBackward0>)\n",
                                    "Losses 0.2214407 2.1907241 2.412165\n",
                                    "score:  1\n",
                                    "score:  1\n",
                                    "score:  0\n",
                                    "score:  1\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_9/videos/alphazero/9/alphazero-episode-44.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_9/videos/alphazero/9/alphazero-episode-44.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                           \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_9/videos/alphazero/9/alphazero-episode-44.mp4\n",
                                    "score:  0\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n",
                                    "score:  0\n",
                                    "score:  0\n",
                                    "score:  1\n",
                                    "score:  0\n",
                                    "Moviepy - Building video checkpoints/alphazero/step_10/videos/alphazero/10/alphazero-episode-49.mp4.\n",
                                    "Moviepy - Writing video checkpoints/alphazero/step_10/videos/alphazero/10/alphazero-episode-49.mp4\n",
                                    "\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "                                                           \r"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Moviepy - Done !\n",
                                    "Moviepy - video ready checkpoints/alphazero/step_10/videos/alphazero/10/alphazero-episode-49.mp4\n",
                                    "score:  0\n",
                                    "Plotting score...\n",
                                    "Plotting policy_loss...\n",
                                    "Plotting value_loss...\n",
                                    "Plotting loss...\n",
                                    "Plotting test_score...\n"
                              ]
                        },
                        {
                              "ename": "",
                              "evalue": "",
                              "output_type": "error",
                              "traceback": [
                                    "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                                    "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                                    "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                                    "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                              ]
                        }
                  ],
                  "source": [
                        "agent.checkpoint_interval = 1\n",
                        "agent.train()"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.10.14"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
