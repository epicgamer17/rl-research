{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "MODE = enum.Enum(\"mode\", \"best_response average_policy\")\n",
    "\n",
    "\n",
    "def evaluatebotnfsp(agent, num_of_eval_games, mini_env, config):\n",
    "    modelselect = CFRAgent(env=mini_env, config=config)\n",
    "    eval_games = num_of_eval_games\n",
    "    rewards_player_1 = []\n",
    "    rewards_player_2 = []\n",
    "    for i in range(eval_games):\n",
    "        # FOR EACH EVAL GAME, RESET ENVIRONEMENT (DEBATABLE STEP) BUT RESET WITH SET SEED FOR RECREATION\n",
    "        if i % 1000 == 0:\n",
    "            print(\"EVAL GAME: \", i)\n",
    "            print(\"REWARDS P1: \", np.mean(rewards_player_1))\n",
    "            print(\"REWARDS P2: \", np.mean(rewards_player_2))\n",
    "        modelselect.env.reset()\n",
    "        observation, reward, termination, truncation, infos = modelselect.env.obs()\n",
    "        init_starting_player = np.random.randint(0, 2)\n",
    "        modelselect.active_player_obj.set_active_player(init_starting_player)\n",
    "        while not termination and not truncation:\n",
    "            # GET CURRENT STATE\n",
    "            observation, reward, termination, truncation, infos = modelselect.env.obs()\n",
    "            # print(\"STATE\", observation)\n",
    "            # print(\"ACTIVE PLAYER: \", modelselect.active_player_obj.get_active_player())\n",
    "            # print(\"ACTION: \", action)\n",
    "            # print(\"REWARD: \", reward)\n",
    "            # print(\"TERMINATION: \", termination)\n",
    "            # print(\"TRUNCATION: \", truncation)\n",
    "            # print(\"INFO: \", infos)\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            active_player = modelselect.active_player_obj.get_active_player()\n",
    "            if active_player == 0:\n",
    "                agent._mode = MODE.best_response\n",
    "\n",
    "                action, probs = agent._rl_agent._epsilon_greedy(\n",
    "                    observation[\"observation\"], observation[\"action_mask\"], 0\n",
    "                )\n",
    "            else:\n",
    "                agent._mode = MODE.average_policy\n",
    "                # if active player, branch off and traverse\n",
    "                action, probs = agent._act(\n",
    "                    observation[\"observation\"], observation[\"action_mask\"]\n",
    "                )\n",
    "            modelselect.env.step(action)\n",
    "            # print(\"STATE\", observation)\n",
    "            # print(\"ACTIVE PLAYER: \", active_player)\n",
    "            # print(\"ACTION: \", sample)\n",
    "            # print(\"REWARD: \", reward)\n",
    "            # print(\"TERMINATION: \", termination)\n",
    "            # print(\"TRUNCATION: \", truncation)\n",
    "            # print(\"INFO: \", infos)\n",
    "            # print(\"NEXT STATE: \", modelselect.env.state)\n",
    "            # print(\"REWAS\", modelselect.env.state.rewards())\n",
    "            modelselect.active_player_obj.next()\n",
    "        if init_starting_player == 0:\n",
    "            final_rewards_p_1 = modelselect.env.state.rewards()[0]\n",
    "            final_rewards_p_2 = modelselect.env.state.rewards()[1]\n",
    "        else:\n",
    "            final_rewards_p_1 = modelselect.env.state.rewards()[1]\n",
    "            final_rewards_p_2 = modelselect.env.state.rewards()[0]\n",
    "        rewards_player_1.append(final_rewards_p_1)\n",
    "        rewards_player_2.append(final_rewards_p_2)\n",
    "    return rewards_player_1, rewards_player_2\n",
    "    print(\"PLAYER 1 REW MEAN: \", np.mean(rewards_player_1))\n",
    "    print(\"PLAYER 1 REW STD: \", np.std(rewards_player_1))\n",
    "    print(\"PLAYER 2 REW MEAN: \", np.mean(rewards_player_2))\n",
    "    print(\"PLAYER 2 REW STD: \", np.std(rewards_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [108, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/1000002/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/1000002/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/1000002/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/1000002/avg_network_pid0\n",
      "2025-04-25 19:33:07.275363: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_16/Assign' id:18018 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_16/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_16, zeros_379)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL GAME:  1000\n",
      "REWARDS P1:  -2.95\n",
      "REWARDS P2:  2.95\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  -7.95\n",
      "REWARDS P2:  7.95\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  4.566666666666666\n",
      "REWARDS P2:  -4.566666666666666\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  10.9125\n",
      "REWARDS P2:  -10.9125\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  8.45\n",
      "REWARDS P2:  -8.45\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  9.283333333333333\n",
      "REWARDS P2:  -9.283333333333333\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  11.485714285714286\n",
      "REWARDS P2:  -11.485714285714286\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  9.29375\n",
      "REWARDS P2:  -9.29375\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  5.6\n",
      "REWARDS P2:  -5.6\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  4.9\n",
      "REWARDS P2:  -4.9\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  4.422727272727273\n",
      "REWARDS P2:  -4.422727272727273\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  4.879166666666666\n",
      "REWARDS P2:  -4.879166666666666\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  4.8961538461538465\n",
      "REWARDS P2:  -4.8961538461538465\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  4.314285714285714\n",
      "REWARDS P2:  -4.314285714285714\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  2.6733333333333333\n",
      "REWARDS P2:  -2.6733333333333333\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  2.74375\n",
      "REWARDS P2:  -2.74375\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  2.6941176470588237\n",
      "REWARDS P2:  -2.6941176470588237\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  4.166666666666667\n",
      "REWARDS P2:  -4.166666666666667\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  4.707894736842105\n",
      "REWARDS P2:  -4.707894736842105\n",
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [108, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/4000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/4000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/4000001/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/4000001/avg_network_pid0\n",
      "2025-04-25 19:34:29.557879: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_17/Assign' id:18968 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_17/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_17, zeros_399)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  0.2\n",
      "REWARDS P2:  -0.2\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  -24.325\n",
      "REWARDS P2:  24.325\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  -31.816666666666666\n",
      "REWARDS P2:  31.816666666666666\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  -31.3125\n",
      "REWARDS P2:  31.3125\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  -24.19\n",
      "REWARDS P2:  24.19\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  -22.0\n",
      "REWARDS P2:  22.0\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  -20.72142857142857\n",
      "REWARDS P2:  20.72142857142857\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  -28.61875\n",
      "REWARDS P2:  28.61875\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  -28.272222222222222\n",
      "REWARDS P2:  28.272222222222222\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  -30.84\n",
      "REWARDS P2:  30.84\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  -32.70909090909091\n",
      "REWARDS P2:  32.70909090909091\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  -30.5625\n",
      "REWARDS P2:  30.5625\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  -34.426923076923075\n",
      "REWARDS P2:  34.426923076923075\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  -31.810714285714287\n",
      "REWARDS P2:  31.810714285714287\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  -31.503333333333334\n",
      "REWARDS P2:  31.503333333333334\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  -30.471875\n",
      "REWARDS P2:  30.471875\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  -30.529411764705884\n",
      "REWARDS P2:  30.529411764705884\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  -31.594444444444445\n",
      "REWARDS P2:  31.594444444444445\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  -30.99736842105263\n",
      "REWARDS P2:  30.99736842105263\n",
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [108, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/7000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/7000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/7000001/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/7000001/avg_network_pid0\n",
      "2025-04-25 19:35:10.764742: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_18/Assign' id:19918 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_18/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_18, zeros_419)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  28.35\n",
      "REWARDS P2:  -28.35\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  21.575\n",
      "REWARDS P2:  -21.575\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  -4.333333333333333\n",
      "REWARDS P2:  4.333333333333333\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  -15.3375\n",
      "REWARDS P2:  15.3375\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  -5.91\n",
      "REWARDS P2:  5.91\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  2.1416666666666666\n",
      "REWARDS P2:  -2.1416666666666666\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  0.4357142857142857\n",
      "REWARDS P2:  -0.4357142857142857\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  -5.5875\n",
      "REWARDS P2:  5.5875\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  -6.377777777777778\n",
      "REWARDS P2:  6.377777777777778\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  -4.425\n",
      "REWARDS P2:  4.425\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  -7.990909090909091\n",
      "REWARDS P2:  7.990909090909091\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  -11.208333333333334\n",
      "REWARDS P2:  11.208333333333334\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  -11.180769230769231\n",
      "REWARDS P2:  11.180769230769231\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  -9.064285714285715\n",
      "REWARDS P2:  9.064285714285715\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  -8.656666666666666\n",
      "REWARDS P2:  8.656666666666666\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  -2.4125\n",
      "REWARDS P2:  2.4125\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  -3.0205882352941176\n",
      "REWARDS P2:  3.0205882352941176\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  -1.4333333333333333\n",
      "REWARDS P2:  1.4333333333333333\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  1.8105263157894738\n",
      "REWARDS P2:  -1.8105263157894738\n",
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [108, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/8000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/8000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/8000000/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/8000000/avg_network_pid0\n",
      "2025-04-25 19:36:33.012621: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_19/Assign' id:20868 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_19/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_19, zeros_439)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  76.55\n",
      "REWARDS P2:  -76.55\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  56.225\n",
      "REWARDS P2:  -56.225\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  57.15\n",
      "REWARDS P2:  -57.15\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  66.075\n",
      "REWARDS P2:  -66.075\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  70.5\n",
      "REWARDS P2:  -70.5\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  62.19166666666667\n",
      "REWARDS P2:  -62.19166666666667\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  60.55714285714286\n",
      "REWARDS P2:  -60.55714285714286\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  55.51875\n",
      "REWARDS P2:  -55.51875\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  57.53333333333333\n",
      "REWARDS P2:  -57.53333333333333\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  61.2\n",
      "REWARDS P2:  -61.2\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  60.07272727272727\n",
      "REWARDS P2:  -60.07272727272727\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  60.4\n",
      "REWARDS P2:  -60.4\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  60.3\n",
      "REWARDS P2:  -60.3\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  60.40357142857143\n",
      "REWARDS P2:  -60.40357142857143\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  59.88666666666666\n",
      "REWARDS P2:  -59.88666666666666\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  58.86875\n",
      "REWARDS P2:  -58.86875\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  61.71764705882353\n",
      "REWARDS P2:  -61.71764705882353\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  59.330555555555556\n",
      "REWARDS P2:  -59.330555555555556\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  58.305263157894736\n",
      "REWARDS P2:  -58.305263157894736\n",
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [108, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/10000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/10000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/10000000/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/fhp/nfsp/0/10000000/avg_network_pid0\n",
      "2025-04-25 19:37:18.969478: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_20/Assign' id:21818 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_20/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_20, zeros_459)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  -108.9\n",
      "REWARDS P2:  108.9\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  -90.2\n",
      "REWARDS P2:  90.2\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  -87.13333333333334\n",
      "REWARDS P2:  87.13333333333334\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  -83.9125\n",
      "REWARDS P2:  83.9125\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  -90.85\n",
      "REWARDS P2:  90.85\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  -93.26666666666667\n",
      "REWARDS P2:  93.26666666666667\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  -92.30714285714286\n",
      "REWARDS P2:  92.30714285714286\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  -93.75625\n",
      "REWARDS P2:  93.75625\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  -93.71666666666667\n",
      "REWARDS P2:  93.71666666666667\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  -94.965\n",
      "REWARDS P2:  94.965\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  -95.0\n",
      "REWARDS P2:  95.0\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  -94.70833333333333\n",
      "REWARDS P2:  94.70833333333333\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  -91.2\n",
      "REWARDS P2:  91.2\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  -94.56785714285714\n",
      "REWARDS P2:  94.56785714285714\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  -93.01\n",
      "REWARDS P2:  93.01\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  -93.40625\n",
      "REWARDS P2:  93.40625\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  -91.7\n",
      "REWARDS P2:  91.7\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  -90.80277777777778\n",
      "REWARDS P2:  90.80277777777778\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  -92.98157894736842\n",
      "REWARDS P2:  92.98157894736842\n",
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [16, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/1000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/1000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/1000000/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/1000000/avg_network_pid0\n",
      "2025-04-25 19:38:13.820356: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_21/Assign' id:22768 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_21/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_21, zeros_479)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  -197.8\n",
      "REWARDS P2:  197.8\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  -197.7\n",
      "REWARDS P2:  197.7\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  -197.53333333333333\n",
      "REWARDS P2:  197.53333333333333\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  -197.8\n",
      "REWARDS P2:  197.8\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  -198.2\n",
      "REWARDS P2:  198.2\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  -197.93333333333334\n",
      "REWARDS P2:  197.93333333333334\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  -198.5142857142857\n",
      "REWARDS P2:  198.5142857142857\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  -198.1\n",
      "REWARDS P2:  198.1\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  -198.22222222222223\n",
      "REWARDS P2:  198.22222222222223\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  -197.98\n",
      "REWARDS P2:  197.98\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  -198.03636363636363\n",
      "REWARDS P2:  198.03636363636363\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  -198.2\n",
      "REWARDS P2:  198.2\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  -198.29615384615386\n",
      "REWARDS P2:  198.29615384615386\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  -198.24642857142857\n",
      "REWARDS P2:  198.24642857142857\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  -198.11\n",
      "REWARDS P2:  198.11\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  -198.390625\n",
      "REWARDS P2:  198.390625\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  -198.77941176470588\n",
      "REWARDS P2:  198.77941176470588\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  -198.99166666666667\n",
      "REWARDS P2:  198.99166666666667\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  -198.88684210526316\n",
      "REWARDS P2:  198.88684210526316\n",
      "('mlp/bias', [1024])\n",
      "('mlp/bias_1', [512])\n",
      "('mlp/bias_2', [1024])\n",
      "('mlp/bias_3', [512])\n",
      "('mlp/bias_4', [4])\n",
      "('mlp/weights', [16, 1024])\n",
      "('mlp/weights_1', [1024, 512])\n",
      "('mlp/weights_2', [512, 1024])\n",
      "('mlp/weights_3', [1024, 512])\n",
      "('mlp/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/4000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/4000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/4000001/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/4000001/avg_network_pid0\n",
      "2025-04-25 19:39:02.851484: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_2/weights_4_22/Assign' id:23718 op device:{requested: '', assigned: ''} def:{{{node mlp_2/weights_4_22/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_2/weights_4_22, zeros_499)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  -75.5\n",
      "REWARDS P2:  75.5\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  -75.725\n",
      "REWARDS P2:  75.725\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  -75.3\n",
      "REWARDS P2:  75.3\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  -75.1125\n",
      "REWARDS P2:  75.1125\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  -75.11\n",
      "REWARDS P2:  75.11\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  -75.01666666666667\n",
      "REWARDS P2:  75.01666666666667\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  -74.89285714285714\n",
      "REWARDS P2:  74.89285714285714\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  -75.04375\n",
      "REWARDS P2:  75.04375\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  -75.0111111111111\n",
      "REWARDS P2:  75.0111111111111\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  -75.125\n",
      "REWARDS P2:  75.125\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  -75.1\n",
      "REWARDS P2:  75.1\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  -75.15\n",
      "REWARDS P2:  75.15\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  -75.18076923076923\n",
      "REWARDS P2:  75.18076923076923\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  -75.13571428571429\n",
      "REWARDS P2:  75.13571428571429\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  -75.1\n",
      "REWARDS P2:  75.1\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  -75.1125\n",
      "REWARDS P2:  75.1125\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  -75.0264705882353\n",
      "REWARDS P2:  75.0264705882353\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  -74.99444444444444\n",
      "REWARDS P2:  74.99444444444444\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  -74.96578947368421\n",
      "REWARDS P2:  74.96578947368421\n",
      "('mlp_6/bias', [1024])\n",
      "('mlp_6/bias_1', [512])\n",
      "('mlp_6/bias_2', [1024])\n",
      "('mlp_6/bias_3', [512])\n",
      "('mlp_6/bias_4', [4])\n",
      "('mlp_6/weights', [16, 1024])\n",
      "('mlp_6/weights_1', [1024, 512])\n",
      "('mlp_6/weights_2', [512, 1024])\n",
      "('mlp_6/weights_3', [1024, 512])\n",
      "('mlp_6/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/7000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/7000001/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/7000001/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/7000001/avg_network_pid0\n",
      "2025-04-25 19:39:37.776839: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_8/weights_4_4/Assign' id:24668 op device:{requested: '', assigned: ''} def:{{{node mlp_8/weights_4_4/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_8/weights_4_4, zeros_519)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  57.1\n",
      "REWARDS P2:  -57.1\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  48.225\n",
      "REWARDS P2:  -48.225\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  53.45\n",
      "REWARDS P2:  -53.45\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  50.4375\n",
      "REWARDS P2:  -50.4375\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  57.7\n",
      "REWARDS P2:  -57.7\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  58.94166666666667\n",
      "REWARDS P2:  -58.94166666666667\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  51.121428571428574\n",
      "REWARDS P2:  -51.121428571428574\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  48.34375\n",
      "REWARDS P2:  -48.34375\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  52.794444444444444\n",
      "REWARDS P2:  -52.794444444444444\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  47.92\n",
      "REWARDS P2:  -47.92\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  49.763636363636365\n",
      "REWARDS P2:  -49.763636363636365\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  48.520833333333336\n",
      "REWARDS P2:  -48.520833333333336\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  50.94615384615385\n",
      "REWARDS P2:  -50.94615384615385\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  50.725\n",
      "REWARDS P2:  -50.725\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  48.806666666666665\n",
      "REWARDS P2:  -48.806666666666665\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  48.41875\n",
      "REWARDS P2:  -48.41875\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  46.502941176470586\n",
      "REWARDS P2:  -46.502941176470586\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  43.42777777777778\n",
      "REWARDS P2:  -43.42777777777778\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  42.05263157894737\n",
      "REWARDS P2:  -42.05263157894737\n",
      "('mlp_6/bias', [1024])\n",
      "('mlp_6/bias_1', [512])\n",
      "('mlp_6/bias_2', [1024])\n",
      "('mlp_6/bias_3', [512])\n",
      "('mlp_6/bias_4', [4])\n",
      "('mlp_6/weights', [16, 1024])\n",
      "('mlp_6/weights_1', [1024, 512])\n",
      "('mlp_6/weights_2', [512, 1024])\n",
      "('mlp_6/weights_3', [1024, 512])\n",
      "('mlp_6/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/8000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/8000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/8000000/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/8000000/avg_network_pid0\n",
      "2025-04-25 19:40:22.581550: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_8/weights_4_5/Assign' id:25618 op device:{requested: '', assigned: ''} def:{{{node mlp_8/weights_4_5/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_8/weights_4_5, zeros_539)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  22.8\n",
      "REWARDS P2:  -22.8\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  20.4\n",
      "REWARDS P2:  -20.4\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  -4.8\n",
      "REWARDS P2:  4.8\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  0.875\n",
      "REWARDS P2:  -0.875\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  1.64\n",
      "REWARDS P2:  -1.64\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  1.1333333333333333\n",
      "REWARDS P2:  -1.1333333333333333\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  -0.24285714285714285\n",
      "REWARDS P2:  0.24285714285714285\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  -3.6875\n",
      "REWARDS P2:  3.6875\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  -3.022222222222222\n",
      "REWARDS P2:  3.022222222222222\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  -6.2\n",
      "REWARDS P2:  6.2\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  -0.6227272727272727\n",
      "REWARDS P2:  0.6227272727272727\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  2.529166666666667\n",
      "REWARDS P2:  -2.529166666666667\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  0.7653846153846153\n",
      "REWARDS P2:  -0.7653846153846153\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  2.932142857142857\n",
      "REWARDS P2:  -2.932142857142857\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  2.9766666666666666\n",
      "REWARDS P2:  -2.9766666666666666\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  4.884375\n",
      "REWARDS P2:  -4.884375\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  3.961764705882353\n",
      "REWARDS P2:  -3.961764705882353\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  3.263888888888889\n",
      "REWARDS P2:  -3.263888888888889\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  6.371052631578947\n",
      "REWARDS P2:  -6.371052631578947\n",
      "('mlp_6/bias', [1024])\n",
      "('mlp_6/bias_1', [512])\n",
      "('mlp_6/bias_2', [1024])\n",
      "('mlp_6/bias_3', [512])\n",
      "('mlp_6/bias_4', [4])\n",
      "('mlp_6/weights', [16, 1024])\n",
      "('mlp_6/weights_1', [1024, 512])\n",
      "('mlp_6/weights_2', [512, 1024])\n",
      "('mlp_6/weights_3', [1024, 512])\n",
      "('mlp_6/weights_4', [512, 4])\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/10000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/10000000/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/10000000/avg_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/leduc/nfsp/0/10000000/avg_network_pid0\n",
      "2025-04-25 19:41:14.530295: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_8/weights_4_6/Assign' id:26568 op device:{requested: '', assigned: ''} def:{{{node mlp_8/weights_4_6/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mlp_8/weights_4_6, zeros_559)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "EVAL GAME:  0\n",
      "REWARDS P1:  nan\n",
      "REWARDS P2:  nan\n",
      "EVAL GAME:  1000\n",
      "REWARDS P1:  18.2\n",
      "REWARDS P2:  -18.2\n",
      "EVAL GAME:  2000\n",
      "REWARDS P1:  17.2\n",
      "REWARDS P2:  -17.2\n",
      "EVAL GAME:  3000\n",
      "REWARDS P1:  17.2\n",
      "REWARDS P2:  -17.2\n",
      "EVAL GAME:  4000\n",
      "REWARDS P1:  18.1625\n",
      "REWARDS P2:  -18.1625\n",
      "EVAL GAME:  5000\n",
      "REWARDS P1:  18.4\n",
      "REWARDS P2:  -18.4\n",
      "EVAL GAME:  6000\n",
      "REWARDS P1:  19.241666666666667\n",
      "REWARDS P2:  -19.241666666666667\n",
      "EVAL GAME:  7000\n",
      "REWARDS P1:  18.5\n",
      "REWARDS P2:  -18.5\n",
      "EVAL GAME:  8000\n",
      "REWARDS P1:  18.5375\n",
      "REWARDS P2:  -18.5375\n",
      "EVAL GAME:  9000\n",
      "REWARDS P1:  18.766666666666666\n",
      "REWARDS P2:  -18.766666666666666\n",
      "EVAL GAME:  10000\n",
      "REWARDS P1:  18.845\n",
      "REWARDS P2:  -18.845\n",
      "EVAL GAME:  11000\n",
      "REWARDS P1:  18.70909090909091\n",
      "REWARDS P2:  -18.70909090909091\n",
      "EVAL GAME:  12000\n",
      "REWARDS P1:  18.879166666666666\n",
      "REWARDS P2:  -18.879166666666666\n",
      "EVAL GAME:  13000\n",
      "REWARDS P1:  18.619230769230768\n",
      "REWARDS P2:  -18.619230769230768\n",
      "EVAL GAME:  14000\n",
      "REWARDS P1:  18.392857142857142\n",
      "REWARDS P2:  -18.392857142857142\n",
      "EVAL GAME:  15000\n",
      "REWARDS P1:  18.303333333333335\n",
      "REWARDS P2:  -18.303333333333335\n",
      "EVAL GAME:  16000\n",
      "REWARDS P1:  18.165625\n",
      "REWARDS P2:  -18.165625\n",
      "EVAL GAME:  17000\n",
      "REWARDS P1:  18.147058823529413\n",
      "REWARDS P2:  -18.147058823529413\n",
      "EVAL GAME:  18000\n",
      "REWARDS P1:  18.155555555555555\n",
      "REWARDS P2:  -18.155555555555555\n",
      "EVAL GAME:  19000\n",
      "REWARDS P1:  18.121052631578948\n",
      "REWARDS P2:  -18.121052631578948\n"
     ]
    }
   ],
   "source": [
    "from agent_configs import RainbowConfig\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "from utils.utils import HuberLoss\n",
    "from cfr_utils import (\n",
    "    EvalWrapper,\n",
    "    evaluatebots,\n",
    "    WrapperEnv,\n",
    "    load_agents,\n",
    "    EmptyConf,\n",
    "    NFSPWrapper,\n",
    "    NFSPEvalWrapper,\n",
    "    LoadNFSPAgent,\n",
    ")\n",
    "import pyspiel\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "from cfr_network import CFRNetwork\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import open_spiel.python.algorithms.nfsp\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "num_players = 2\n",
    "max_nodes = 10000000\n",
    "\n",
    "fhp = pyspiel.load_game(\n",
    "    \"universal_poker\",\n",
    "    {\n",
    "        \"numPlayers\": 2,\n",
    "        \"numSuits\": 4,\n",
    "        \"numRanks\": 13,\n",
    "        \"numHoleCards\": 2,\n",
    "        \"numBoardCards\": \"0 3\",\n",
    "        \"bettingAbstraction\": \"fcpa\",\n",
    "        \"numRounds\": 2,\n",
    "        \"blind\": \"50 100\",\n",
    "    },\n",
    ")\n",
    "leduc = pyspiel.load_game(\n",
    "    \"universal_poker\",\n",
    "    {\n",
    "        \"numPlayers\": 2,\n",
    "        \"numSuits\": 2,\n",
    "        \"numRanks\": 3,\n",
    "        \"numHoleCards\": 1,\n",
    "        \"numBoardCards\": \"0 1\",\n",
    "        \"bettingAbstraction\": \"fcpa\",\n",
    "        \"numRounds\": 2,\n",
    "        \"blind\": \"50 100\",\n",
    "    },\n",
    ")\n",
    "leducconfig = {\"state_representation_size\": 16}\n",
    "fhpconfig = {\"state_representation_size\": 108}\n",
    "leducgame = NFSPWrapper(leduc)\n",
    "fhpgame = NFSPWrapper(fhp)\n",
    "\n",
    "active_player_obj = ActivePlayer(2)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "\n",
    "mainpath1 = \"./checkpoints/fhp/nfsp/0/1000002/\"\n",
    "mainpath2 = \"./checkpoints/fhp/nfsp/0/4000001/\"\n",
    "mainpath3 = \"./checkpoints/fhp/nfsp/0/7000001/\"\n",
    "mainpath4 = \"./checkpoints/fhp/nfsp/0/8000000/\"\n",
    "mainpath5 = \"./checkpoints/fhp/nfsp/0/10000000/\"\n",
    "mainpath6 = \"./checkpoints/leduc/nfsp/0/1000000/\"\n",
    "mainpath7 = \"./checkpoints/leduc/nfsp/0/4000001/\"\n",
    "mainpath8 = \"./checkpoints/leduc/nfsp/0/7000001/\"\n",
    "mainpath9 = \"./checkpoints/leduc/nfsp/0/8000000/\"\n",
    "mainpath10 = \"./checkpoints/leduc/nfsp/0/10000000/\"\n",
    "\n",
    "leduc_agent_paths = [\n",
    "    mainpath6,\n",
    "    mainpath7,\n",
    "    mainpath8,\n",
    "    mainpath9,\n",
    "    mainpath10,\n",
    "]\n",
    "fhp_agent_paths = [\n",
    "    mainpath1,\n",
    "    mainpath2,\n",
    "    mainpath3,\n",
    "    mainpath4,\n",
    "    mainpath5,\n",
    "]\n",
    "\n",
    "nodes = 0\n",
    "games = [fhpgame, leducgame]\n",
    "results = []\n",
    "for i in games:\n",
    "    if i == leducgame:\n",
    "        agent_paths = leduc_agent_paths\n",
    "        game_string = \"leduc\"\n",
    "    else:\n",
    "        agent_paths = fhp_agent_paths\n",
    "        game_string = \"fhp\"\n",
    "    for number in range(len(agent_paths)):\n",
    "        i.reset()\n",
    "\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            agent = open_spiel.python.algorithms.nfsp.NFSP(\n",
    "                session=sess,\n",
    "                player_id=0,\n",
    "                state_representation_size=(\n",
    "                    leducconfig[\"state_representation_size\"]\n",
    "                    if i == leducgame\n",
    "                    else fhpconfig[\"state_representation_size\"]\n",
    "                ),\n",
    "                num_actions=4,\n",
    "                hidden_layers_sizes=[1024, 512, 1024, 512],\n",
    "                reservoir_buffer_capacity=30000000,\n",
    "                anticipatory_param=0,\n",
    "                batch_size=256,\n",
    "                rl_learning_rate=0.1,\n",
    "                sl_learning_rate=0.01,\n",
    "                min_buffer_size_to_learn=1000,\n",
    "                learn_every=256,\n",
    "                optimizer_str=\"sgd\",\n",
    "                replay_buffer_capacity=600000,\n",
    "                epsilon_start=0.08,\n",
    "                epsilon_end=0,\n",
    "            )\n",
    "            LoadNFSPAgent(agent_paths[number], agent, 0)\n",
    "            agent.restore(agent_paths[number])  # IF YOU HAVE A NFSP AGENT PATH\n",
    "            # agent.restore(path1) # IF YOU HAVE A NFSP AGENT PATH\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "            wrapped = NFSPEvalWrapper(\n",
    "                i,\n",
    "                agent,\n",
    "                (\n",
    "                    leducconfig[\"state_representation_size\"]\n",
    "                    if i == leducgame\n",
    "                    else fhpconfig[\"state_representation_size\"]\n",
    "                ),\n",
    "                4,\n",
    "            )\n",
    "\n",
    "            config = CFRConfig(\n",
    "                config_dict={\n",
    "                    \"network\": {\n",
    "                        \"policy\": p_v_networks,\n",
    "                        \"value\": p_v_networks,\n",
    "                        \"num_players\": num_players,\n",
    "                    },\n",
    "                    \"replay_buffer_size\": replay_buffer_size,\n",
    "                    \"minibatch_size\": minibatch_size,\n",
    "                    \"steps_per_epoch\": steps_per_epoch,\n",
    "                    \"traversals\": traversals,\n",
    "                    \"training_steps\": training_steps,\n",
    "                    \"active_player_obj\": active_player_obj,\n",
    "                },\n",
    "                game_config={\n",
    "                    \"num_players\": num_players,\n",
    "                    \"observation_space\": (\n",
    "                        leducconfig[\"state_representation_size\"]\n",
    "                        if i == leducgame\n",
    "                        else fhpconfig[\"state_representation_size\"]\n",
    "                    ),\n",
    "                    \"action_space\": 4,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            p1, p2 = evaluatebotnfsp(agent, 20000, i, config)\n",
    "            results.append(np.mean(p1) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:  [0.040525000000000005, -0.30017499999999997, 0.033624999999999995, 0.60315, -0.93965, -1.9902250000000001, -0.7497, 0.42305, 0.083925, 0.18170000000000003]\n"
     ]
    }
   ],
   "source": [
    "print(\"RESULTS: \", results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
