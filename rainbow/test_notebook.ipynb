{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 04:33:33.348970: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-03-01 04:33:33.348995: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-03-01 04:33:33.349003: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-03-01 04:33:33.349265: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-01 04:33:33.349676: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = f\"{1}\"\n",
    "# os.environ['TF_NUM_INTEROP_THREADS'] = f\"{1}\"\n",
    "# os.environ['TF_NUM_INTRAOP_THREADS'] = f\"{1}\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# import search\n",
    "from collections import deque\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "import gymnasium as gym\n",
    "from time import time\n",
    "# import moviepy\n",
    "\n",
    "# from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Segment tree for Prioritized Replay Buffer.\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class SegmentTree:\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int)\n",
    "        tree (list)\n",
    "        operation (function)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, operation: Callable, init_value: float):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "            operation (function)\n",
    "            init_value (float)\n",
    "\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            capacity > 0 and capacity & (capacity - 1) == 0\n",
    "        ), \"capacity must be positive and a power of 2.\"\n",
    "        self.capacity = capacity\n",
    "        self.tree = [init_value for _ in range(2 * capacity)]\n",
    "        self.operation = operation\n",
    "\n",
    "    def _operate_helper(\n",
    "        self, start: int, end: int, node: int, node_start: int, node_end: int\n",
    "    ) -> float:\n",
    "        \"\"\"Returns result of operation in segment.\"\"\"\n",
    "        if start == node_start and end == node_end:\n",
    "            return self.tree[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._operate_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self.operation(\n",
    "                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n",
    "                )\n",
    "\n",
    "    def operate(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns result of applying `self.operation`.\"\"\"\n",
    "        if end <= 0:\n",
    "            end += self.capacity\n",
    "        end -= 1\n",
    "\n",
    "        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        idx += self.capacity\n",
    "        self.tree[idx] = val\n",
    "\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SumSegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=operator.add, init_value=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n",
    "        return super(SumSegmentTree, self).operate(start, end)\n",
    "\n",
    "    def retrieve(self, upperbound: float) -> int:\n",
    "        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n",
    "        # TODO: Check assert case and fix bug\n",
    "        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n",
    "\n",
    "        idx = 1\n",
    "\n",
    "        while idx < self.capacity:  # while non-leaf\n",
    "            left = 2 * idx\n",
    "            right = left + 1\n",
    "            if self.tree[left] > upperbound:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                upperbound -= self.tree[left]\n",
    "                idx = right\n",
    "        return idx - self.capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=min, init_value=float(\"inf\")\n",
    "        )\n",
    "\n",
    "    def min(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n",
    "        return super(MinSegmentTree, self).operate(start, end)\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, observation_dimensions, max_size: int, batch_size = 32, n_step = 1, gamma = 0.99):\n",
    "        # self.observation_buffer = np.zeros((max_size,) + observation_dimensions, dtype=np.float32)\n",
    "        # self.next_observation_buffer = np.zeros((max_size,) + observation_dimensions, dtype=np.float32)\n",
    "        observation_buffer_shape = []\n",
    "        observation_buffer_shape += [max_size]\n",
    "        observation_buffer_shape += list(observation_dimensions)\n",
    "        observation_buffer_shape = list(observation_buffer_shape)\n",
    "        self.observation_buffer = np.zeros(observation_buffer_shape, dtype=np.float32)\n",
    "        self.next_observation_buffer = np.zeros(observation_buffer_shape, dtype=np.float32)\n",
    "        self.action_buffer = np.zeros(max_size, dtype=np.int32)\n",
    "        self.reward_buffer = np.zeros(max_size, dtype=np.float32)\n",
    "        self.done_buffer = np.zeros(max_size)\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.pointer = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # n-step learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        # print(\"Storing in Buffer\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        transition = (observation, action, reward, next_observation, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            # print(\"Buffer Storage Time \", time() - time1)\n",
    "            return ()\n",
    "\n",
    "        # compute n-step return and store\n",
    "        reward, next_observation, done = self._get_n_step_info()\n",
    "        observation, action = self.n_step_buffer[0][:2]\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.next_observation_buffer[self.pointer] = next_observation\n",
    "        self.done_buffer[self.pointer] = done\n",
    "\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        # print(\"Buffer Storage Time \", time() - time1)\n",
    "        return self.n_step_buffer[0]\n",
    "\n",
    "    def sample(self):\n",
    "        # print(\"Sampling From Buffer\")\n",
    "        # time1 = time()\n",
    "        idx = np.random.choice(self.size, self.batch_size, replace=False)\n",
    "\n",
    "        # print(\"Buffer Sampling Time \", time() - time1)\n",
    "        return dict(\n",
    "            observations=self.observation_buffer[idx],\n",
    "            next_observations=self.next_observation_buffer[idx],\n",
    "            actions=self.action_buffer[idx],\n",
    "            rewards=self.reward_buffer[idx],\n",
    "            dones=self.done_buffer[idx],\n",
    "        )\n",
    "\n",
    "    def sample_from_indices(self, indices):\n",
    "        # print(\"Sampling From Indices\")\n",
    "        return dict(\n",
    "            observations=self.observation_buffer[indices],\n",
    "            next_observations=self.next_observation_buffer[indices],\n",
    "            actions=self.action_buffer[indices],\n",
    "            rewards=self.reward_buffer[indices],\n",
    "            dones=self.done_buffer[indices],\n",
    "        )\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        reward, next_observation, done = self.n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(self.n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "            reward = r + self.gamma * reward * (1 - d)\n",
    "            next_observation, done = (n_o, d) if d else (next_observation, done)\n",
    "\n",
    "        return reward, next_observation, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_dimensions,\n",
    "            max_size,\n",
    "            batch_size=32,\n",
    "            max_priority=1.0,\n",
    "            alpha=0.6,\n",
    "            # epsilon=0.01,\n",
    "            n_step=1,\n",
    "            gamma=0.99,\n",
    "        ):\n",
    "        assert alpha >= 0\n",
    "\n",
    "        super(PrioritizedReplayBuffer, self).__init__(\n",
    "            observation_dimensions, max_size, batch_size, n_step=n_step, gamma=gamma\n",
    "        )\n",
    "\n",
    "        self.max_priority = max_priority  # (initial) priority\n",
    "        self.tree_pointer = 0\n",
    "\n",
    "        self.alpha = alpha  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "        # self.epsilon = epsilon\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        # print(\"Storing in PrioritizedReplayBuffer\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        transition = super().store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_pointer] = self.max_priority ** self.alpha\n",
    "            self.min_tree[self.tree_pointer] = self.max_priority ** self.alpha\n",
    "            self.tree_pointer = (self.tree_pointer + 1) % self.max_size\n",
    "\n",
    "        # print(\"Storing in PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return transition\n",
    "\n",
    "    def sample(self, beta=0.4):\n",
    "        # print(\"Sampling from PrioritizedReplayBuffer\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        indices = self._sample_proportional()\n",
    "        # print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays\")\n",
    "        # time2 = 0\n",
    "        # time2 = time()\n",
    "        observations = self.observation_buffer[indices]\n",
    "        next_observations = self.next_observation_buffer[indices]\n",
    "        actions = self.action_buffer[indices]\n",
    "        rewards = self.reward_buffer[indices]\n",
    "        dones = self.done_buffer[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        # print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays Time \", time() - time2)\n",
    "\n",
    "        # print(\"Sampling from PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return dict(\n",
    "            observations=observations,\n",
    "            next_observations=next_observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            dones=dones,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        assert len(indices) == len(priorities)\n",
    "        # priorities += self.self.epsilon\n",
    "        for index, priority in zip(indices, priorities):\n",
    "            # print(\"Priority\", priority)\n",
    "            assert priority > 0, \"Negative priority: {}\".format(priority)\n",
    "            assert 0 <= index < len(self)\n",
    "\n",
    "            self.sum_tree[index] = priority ** self.alpha\n",
    "            self.min_tree[index] = priority ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority) # could remove and clip priorities in experience replay isntead\n",
    "\n",
    "    def _sample_proportional(self):\n",
    "        # print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        indices = []\n",
    "        total_priority = self.sum_tree.sum(0, len(self) - 1)\n",
    "        priority_segment = total_priority / self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = priority_segment * i\n",
    "            b = priority_segment * (i + 1)\n",
    "            upperbound = np.random.uniform(a, b)\n",
    "            index = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(index)\n",
    "\n",
    "        # print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree Time \", time() - time1)\n",
    "        return indices\n",
    "\n",
    "    def _calculate_weight(self, index, beta):\n",
    "        min_priority = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (min_priority * len(self)) ** (-beta)\n",
    "        priority_sample = self.sum_tree[index] / self.sum_tree.sum()\n",
    "        weight = (priority_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "\n",
    "        return weight\n",
    "class FastSumTree(object):\n",
    "    # https://medium.com/free-code-camp/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = int(\n",
    "            capacity\n",
    "        )  # number of leaf nodes (final nodes) that contains experiences\n",
    "\n",
    "        self.tree = np.zeros(2 * self.capacity - 1)  # sub tree\n",
    "        # self.data = np.zeros(self.capacity, object)  # contains the experiences\n",
    "\n",
    "    def add(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        tree_index = idx + self.capacity - 1\n",
    "        # self.data[self.data_pointer] = data\n",
    "        self.update(tree_index, val)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "    def update(self, tree_index, val):\n",
    "        change = val - self.tree[tree_index]\n",
    "        # print(\"change\", change)\n",
    "        self.tree[tree_index] = val\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "            # print(\"new value\", self.tree[tree_index])\n",
    "\n",
    "\n",
    "    def retrieve(self, v):\n",
    "        parent_index = 0\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "class FastPrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_dimensions,\n",
    "            max_size,\n",
    "            batch_size=32,\n",
    "            max_priority=1.0,\n",
    "            alpha=0.6,\n",
    "            # epsilon=0.01,\n",
    "            n_step=1,\n",
    "            gamma=0.99,\n",
    "        ):\n",
    "        assert alpha >= 0\n",
    "\n",
    "        super(FastPrioritizedReplayBuffer, self).__init__(\n",
    "            observation_dimensions, max_size, batch_size, n_step=n_step, gamma=gamma\n",
    "        )\n",
    "\n",
    "        self.max_priority = max_priority  # (initial) priority\n",
    "        self.min_priority = max_priority\n",
    "        self.tree_pointer = 0\n",
    "\n",
    "        self.alpha = alpha  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "        # self.epsilon = epsilon\n",
    "\n",
    "        self.tree = FastSumTree(self.max_size)\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        # print(\"Storing in PrioritizedReplayBuffer\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        transition = super().store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        # max_priority = np.max(self.tree.tree[-self.tree.capacity :])\n",
    "        # if max_priority == 0:\n",
    "        #     max_priority = self.max_priority\n",
    "\n",
    "        if transition:\n",
    "            self.tree.add(self.tree_pointer, self.max_priority)\n",
    "            self.tree_pointer = (self.tree_pointer + 1) % self.max_size\n",
    "\n",
    "        # print(\"Storing in PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return transition\n",
    "\n",
    "    def sample(self, beta=0.4):\n",
    "        # print(\"Sampling from PrioritizedReplayBuffer\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        # indices = self._sample_proportional()\n",
    "        # print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        priority_segment = self.tree.total_priority / self.batch_size\n",
    "        indices, weights = np.empty((self.batch_size,), dtype=np.int32), np.empty(\n",
    "            (self.batch_size, 1), dtype=np.float32\n",
    "        )\n",
    "        # print(\"Total Priority\",self.tree.total_priority)\n",
    "        for i in range(self.batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            # print(a, b)\n",
    "            # print(\"a, b\", a, b)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority = self.tree.retrieve(value)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            # print(\"sampling probabilities\", sampling_probabilities)\n",
    "            # weights[i, 0] = np.power(\n",
    "            #     self.batch_size * sampling_probabilities, -beta\n",
    "            # )\n",
    "            weights[i, 0] = (len(self) * sampling_probabilities) ** -beta\n",
    "            indices[i] = index - self.tree.capacity + 1\n",
    "            indices[i] = index - self.tree.capacity + 1\n",
    "\n",
    "        max_weight = (len(self) * self.min_priority / self.tree.total_priority) ** -beta\n",
    "        weights = weights / max_weight\n",
    "\n",
    "        # print(weights)\n",
    "        # print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree Time \", time() - time1)\n",
    "        # print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays\")\n",
    "        # time2 = 0\n",
    "        # time2 = time()\n",
    "        observations = self.observation_buffer[indices]\n",
    "        next_observations = self.next_observation_buffer[indices]\n",
    "        actions = self.action_buffer[indices]\n",
    "        rewards = self.reward_buffer[indices]\n",
    "        dones = self.done_buffer[indices]\n",
    "        # weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        # print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays Time \", time() - time2)\n",
    "\n",
    "        # print(\"Sampling from PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return dict(\n",
    "            observations=observations,\n",
    "            next_observations=next_observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            dones=dones,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        assert len(indices) == len(priorities)\n",
    "        # priorities += self.epsilon\n",
    "\n",
    "        for index, priority in zip(indices, priorities):\n",
    "            assert priority > 0, \"Negative priority: {}\".format(priority)\n",
    "            # assert 0 <= index < len(self)\n",
    "            # self.tree[index] = priority ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority ** self.alpha)\n",
    "            self.min_priority = min(self.min_priority, priority ** self.alpha)\n",
    "            # priority = np.clip(priority, self.epsilon, self.max_priority)\n",
    "            self.tree.update(index + self.tree.capacity - 1, priority ** self.alpha)\n",
    "\n",
    "# From tensorflow_addons\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    activations,\n",
    "    initializers,\n",
    "    regularizers,\n",
    "    constraints,\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import InputSpec\n",
    "\n",
    "def _scaled_noise(size, dtype):\n",
    "    x = tf.random.normal(shape=size, dtype=dtype)\n",
    "    return tf.sign(x) * tf.sqrt(tf.abs(x))\n",
    "\n",
    "class NoisyDense(tf.keras.layers.Dense):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int,\n",
    "        sigma: float = 0.5, # might want to make sigma 0.1 for CPU's\n",
    "        use_factorised: bool = True,\n",
    "        activation = None,\n",
    "        use_bias: bool = True,\n",
    "        kernel_regularizer = None,\n",
    "        bias_regularizer = None,\n",
    "        activity_regularizer = None,\n",
    "        kernel_constraint = None,\n",
    "        bias_constraint = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            units=units,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs,\n",
    "        )\n",
    "        delattr(self, \"kernel_initializer\")\n",
    "        delattr(self, \"bias_initializer\")\n",
    "        self.sigma = sigma\n",
    "        self.use_factorised = use_factorised\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Make sure dtype is correct\n",
    "        dtype = tf.dtypes.as_dtype(self.dtype or K.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\n",
    "                \"Unable to build `Dense` layer with non-floating point \"\n",
    "                \"dtype %s\" % (dtype,)\n",
    "            )\n",
    "\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        sqrt_dim = self.last_dim ** (1 / 2)\n",
    "        if self.last_dim is None:\n",
    "            raise ValueError(\n",
    "                \"The last dimension of the inputs to `Dense` \"\n",
    "                \"should be defined. Found `None`.\"\n",
    "            )\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: self.last_dim})\n",
    "\n",
    "        # use factorising Gaussian variables\n",
    "        if self.use_factorised:\n",
    "            mu_init = 1.0 / sqrt_dim\n",
    "            sigma_init = self.sigma / sqrt_dim\n",
    "        # use independent Gaussian variables\n",
    "        else:\n",
    "            mu_init = (3.0 / self.last_dim) ** (1 / 2)\n",
    "            sigma_init = 0.017\n",
    "\n",
    "        sigma_init = initializers.Constant(value=sigma_init)\n",
    "        mu_init = initializers.RandomUniform(minval=-mu_init, maxval=mu_init)\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.sigma_kernel = self.add_weight(\n",
    "            \"sigma_kernel\",\n",
    "            shape=[self.last_dim, self.units],\n",
    "            initializer=sigma_init,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.mu_kernel = self.add_weight(\n",
    "            \"mu_kernel\",\n",
    "            shape=[self.last_dim, self.units],\n",
    "            initializer=mu_init,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.eps_kernel = self.add_weight(\n",
    "            \"eps_kernel\",\n",
    "            shape=[self.last_dim, self.units],\n",
    "            initializer=initializers.Zeros(),\n",
    "            regularizer=None,\n",
    "            constraint=None,\n",
    "            dtype=self.dtype,\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.sigma_bias = self.add_weight(\n",
    "                \"sigma_bias\",\n",
    "                shape=[\n",
    "                    self.units,\n",
    "                ],\n",
    "                initializer=sigma_init,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                dtype=self.dtype,\n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            self.mu_bias = self.add_weight(\n",
    "                \"mu_bias\",\n",
    "                shape=[\n",
    "                    self.units,\n",
    "                ],\n",
    "                initializer=mu_init,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                dtype=self.dtype,\n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            self.eps_bias = self.add_weight(\n",
    "                \"eps_bias\",\n",
    "                shape=[\n",
    "                    self.units,\n",
    "                ],\n",
    "                initializer=initializers.Zeros(),\n",
    "                regularizer=None,\n",
    "                constraint=None,\n",
    "                dtype=self.dtype,\n",
    "                trainable=False,\n",
    "            )\n",
    "        else:\n",
    "            self.sigma_bias = None\n",
    "            self.mu_bias = None\n",
    "            self.eps_bias = None\n",
    "        self.reset_noise()\n",
    "        self.built = True\n",
    "\n",
    "    @property\n",
    "    def kernel(self):\n",
    "        return self.mu_kernel + (self.sigma_kernel * self.eps_kernel)\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        if self.use_bias:\n",
    "            return self.mu_bias + (self.sigma_bias * self.eps_bias)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Create the factorised Gaussian noise.\"\"\"\n",
    "\n",
    "        if self.use_factorised:\n",
    "            # Generate random noise\n",
    "            in_eps = _scaled_noise([self.last_dim, 1], dtype=self.dtype)\n",
    "            out_eps = _scaled_noise([1, self.units], dtype=self.dtype)\n",
    "\n",
    "            # Scale the random noise\n",
    "            self.eps_kernel.assign(tf.matmul(in_eps, out_eps))\n",
    "            self.eps_bias.assign(out_eps[0])\n",
    "        else:\n",
    "            # generate independent variables\n",
    "            self.eps_kernel.assign(\n",
    "                tf.random.normal(shape=[self.last_dim, self.units], dtype=self.dtype)\n",
    "            )\n",
    "            self.eps_bias.assign(\n",
    "                tf.random.normal(\n",
    "                    shape=[\n",
    "                        self.units,\n",
    "                    ],\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def remove_noise(self):\n",
    "        \"\"\"Remove the factorised Gaussian noise.\"\"\"\n",
    "\n",
    "        self.eps_kernel.assign(tf.zeros([self.last_dim, self.units], dtype=self.dtype))\n",
    "        self.eps_bias.assign(tf.zeros([self.units], dtype=self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO(WindQAQ): Replace this with `dense()` once public.\n",
    "        return super().call(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        # TODO(WindQAQ): Get rid of this hacky way.\n",
    "        config = super(tf.keras.layers.Dense, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"units\": self.units,\n",
    "                \"sigma\": self.sigma,\n",
    "                \"use_factorised\": self.use_factorised,\n",
    "                \"activation\": activations.serialize(self.activation),\n",
    "                \"use_bias\": self.use_bias,\n",
    "                \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n",
    "                \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n",
    "                \"activity_regularizer\": regularizers.serialize(\n",
    "                    self.activity_regularizer\n",
    "                ),\n",
    "                \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n",
    "                \"bias_constraint\": constraints.serialize(self.bias_constraint),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, config, output_size, input_shape, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        kernel_initializers = []\n",
    "        for i in range(len(config['conv_layers']) + config['dense_layers'] + config['value_hidden_layers'] + config['advantage_hidden_layers'] + 2):\n",
    "            if config['kernel_initializer'] == 'glorot_uniform':\n",
    "                kernel_initializers.append(initializers.glorot_uniform(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'glorot_normal':\n",
    "                kernel_initializers.append(initializers.glorot_normal(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'he_normal':\n",
    "                kernel_initializers.append(initializers.he_normal(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'he_uniform':\n",
    "                kernel_initializers.append(initializers.he_uniform(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_baseline':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_0.1':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(scale=0.1, seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_0.3':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(scale=0.3, seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_0.8':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(scale=0.8, seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_3':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(scale=3, seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_5':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(scale=5, seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'variance_10':\n",
    "                kernel_initializers.append(initializers.VarianceScaling(scale=10, seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'lecun_uniform':\n",
    "                kernel_initializers.append(initializers.lecun_uniform(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'lecun_normal':\n",
    "                kernel_initializers.append(initializers.lecun_normal(seed=np.random.seed()))\n",
    "            elif config['kernel_initializer'] == 'orthogonal':\n",
    "                kernel_initializers.append(initializers.orthogonal(seed=np.random.seed()))\n",
    "\n",
    "        activation = None\n",
    "        if config['activation'] == 'linear':\n",
    "            activation = None\n",
    "        elif config['activation'] == 'relu':\n",
    "            activation = tf.keras.activations.relu\n",
    "        elif config['activation'] == 'relu6':\n",
    "            activation = tf.keras.activations.relu(max_value=6)\n",
    "        elif config['activation'] == 'sigmoid':\n",
    "            activation = tf.keras.activations.sigmoid\n",
    "        elif config['activation'] == 'softplus':\n",
    "            activation = tf.keras.activations.softplus\n",
    "        elif config['activation'] == 'soft_sign':\n",
    "            activation = tf.keras.activations.softsign\n",
    "        elif config['activation'] == 'silu':\n",
    "            activation = tf.nn.silu\n",
    "        elif config['activation'] == 'swish':\n",
    "            activation = tf.nn.swish\n",
    "        elif config['activation'] == 'log_sigmoid':\n",
    "            activation = tf.math.log_sigmoid\n",
    "        elif config['activation'] == 'hard_sigmoid':\n",
    "            activation = tf.keras.activations.hard_sigmoid\n",
    "        elif config['activation'] == 'hard_silu':\n",
    "            activation = tf.keras.activations.hard_silu\n",
    "        elif config['activation'] == 'hard_swish':\n",
    "            activation = tf.keras.activations.hard_swish\n",
    "        elif config['activation'] == 'hard_tanh':\n",
    "            activation = tf.keras.activations.hard_tanh\n",
    "        elif config['activation'] == 'elu':\n",
    "            activation = tf.keras.activations.elu\n",
    "        elif config['activation'] == 'celu':\n",
    "            activation = tf.keras.activations.celu\n",
    "        elif config['activation'] == 'selu':\n",
    "            activation = tf.keras.activations.selu\n",
    "        elif config['activation'] == 'gelu':\n",
    "            activation = tf.nn.gelu\n",
    "        elif config['activation'] == 'glu':\n",
    "            activation = tf.keras.activations.glu\n",
    "\n",
    "        self.inputs = tf.keras.layers.Input(shape=input_shape, name='my_input')\n",
    "        self.has_conv_layers = len(config['conv_layers']) > 0\n",
    "        self.has_dense_layers = config['dense_layers'] > 0\n",
    "        if self.has_conv_layers:\n",
    "            self.conv_layers = []\n",
    "            for i, (filters, kernel_size, strides) in enumerate(config['conv_layers']):\n",
    "                if config['conv_layers_noisy']:\n",
    "                    # if i == 0:\n",
    "                    #     self.conv_layers.append(NoisyConv2D(filters, kernel_size, strides=strides, kernel_initializer=kernel_initializers.pop(), activation=activation, input_shape=input_shape))\n",
    "                    # else:\n",
    "                    #     self.conv_layers.append(NoisyConv2D(filters, kernel_size, strides=strides, kernel_initializer=kernel_initializers.pop(), activation=activation))\n",
    "                    pass\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        self.conv_layers.append(tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, kernel_initializer=kernel_initializers.pop(), activation=activation, input_shape=input_shape, padding='same'))\n",
    "                    else:\n",
    "                        self.conv_layers.append(tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, kernel_initializer=kernel_initializers.pop(), activation=activation, padding='same'))\n",
    "            self.conv_layers.append(tf.keras.layers.Flatten())\n",
    "\n",
    "        if self.has_dense_layers:\n",
    "            self.dense_layers = []\n",
    "            for i in range(config['dense_layers']):\n",
    "                if config['dense_layers_noisy']:\n",
    "                    self.dense_layers.append(NoisyDense(config['width'], sigma=config['noisy_sigma'], kernel_initializer=kernel_initializers.pop(), activation=activation))\n",
    "                else:\n",
    "                    self.dense_layers.append(tf.keras.layers.Dense(config['width'], kernel_initializer=kernel_initializers.pop(), activation=activation))\n",
    "\n",
    "        self.has_value_hidden_layers = config['value_hidden_layers'] > 0\n",
    "        if self.has_value_hidden_layers:\n",
    "            self.value_hidden_layers = []\n",
    "            for i in range(config['value_hidden_layers']):\n",
    "                self.value_hidden_layers.append(NoisyDense(config['width'], sigma=config['noisy_sigma'], kernel_initializer=kernel_initializers.pop(), activation=activation))\n",
    "\n",
    "        self.value = NoisyDense(\n",
    "            config[\"atom_size\"], sigma=config['noisy_sigma'], kernel_initializer=kernel_initializers.pop(), activation=\"linear\", name=\"HiddenV\"\n",
    "        )\n",
    "\n",
    "        self.has_advantage_hidden_layers = config['advantage_hidden_layers'] > 0\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            self.advantage_hidden_layers = []\n",
    "            for i in range(config['advantage_hidden_layers']):\n",
    "                self.advantage_hidden_layers.append(NoisyDense(config['width'], sigma=config['noisy_sigma'], kernel_initializer=kernel_initializers.pop(), activation=activation))\n",
    "\n",
    "        self.advantage = NoisyDense(config[\"atom_size\"] * output_size, sigma=config['noisy_sigma'], kernel_initializer=kernel_initializers.pop(), activation=\"linear\", name=\"A\")\n",
    "        self.advantage_reduced_mean = tf.keras.layers.Lambda(\n",
    "            lambda a: a - tf.reduce_mean(a, axis=1, keepdims=True), name=\"Ao\"\n",
    "        )\n",
    "\n",
    "        self.advantage_reshaped = tf.keras.layers.Reshape((output_size, config[\"atom_size\"]), name=\"ReshapeAo\")\n",
    "        self.value_reshaped = tf.keras.layers.Reshape((1, config[\"atom_size\"]), name=\"ReshapeV\")\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        # self.softmax = tf.keras.activations.softmax(self.add, axis=-1)\n",
    "        # ONLY CLIP FOR CATEGORICAL CROSS ENTROPY LOSS TO PREVENT NAN\n",
    "        self.clip_qs = tf.keras.layers.Lambda(\n",
    "            lambda q: tf.clip_by_value(q, 1e-3, 1), name=\"ClippedQ\"\n",
    "        )\n",
    "        self.outputs = tf.keras.layers.Lambda(\n",
    "            lambda q: tf.reduce_sum(q * config['support'], axis=2), name=\"Q\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        if self.has_conv_layers:\n",
    "            for layer in self.conv_layers:\n",
    "                x = layer(x)\n",
    "        if self.has_dense_layers:\n",
    "            for layer in self.dense_layers:\n",
    "                x = layer(x)\n",
    "        if self.has_value_hidden_layers:\n",
    "            for layer in self.value_hidden_layers:\n",
    "                x = layer(x)\n",
    "        value = self.value(x)\n",
    "        value = self.value_reshaped(value)\n",
    "\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            for layer in self.advantage_hidden_layers:\n",
    "                x = layer(x)\n",
    "        advantage = self.advantage(x)\n",
    "        advantage = self.advantage_reduced_mean(advantage)\n",
    "        advantage = self.advantage_reshaped(advantage)\n",
    "\n",
    "        q = self.add([value, advantage])\n",
    "        q = tf.keras.activations.softmax(q, axis=-1)\n",
    "        # MIGHT BE ABLE TO REMOVE CLIPPING ENTIRELY SINCE I DONT THINK THE TENSORFLOW LOSSES CAN RETURN NaN\n",
    "        # q = self.clip_qs(q)\n",
    "        # q = self.outputs(q)\n",
    "        return q\n",
    "\n",
    "    def reset_noise(self):\n",
    "        if self.has_dense_layers and self.config['conv_layers_noisy']:\n",
    "            for layer in self.conv_layers:\n",
    "                layer.reset_noise()\n",
    "        if self.has_dense_layers and self.config['dense_layers_noisy']:\n",
    "            for layer in self.dense_layers:\n",
    "                layer.reset_noise()\n",
    "        if self.has_value_hidden_layers:\n",
    "            for layer in self.value_hidden_layers:\n",
    "                layer.reset_noise()\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            for layer in self.advantage_hidden_layers:\n",
    "                layer.reset_noise()\n",
    "        self.value.reset_noise()\n",
    "        self.advantage.reset_noise()\n",
    "\n",
    "class RainbowDQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        model_name=datetime.datetime.now().timestamp(),\n",
    "        config=None,\n",
    "        start_episode=0,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        self.env = env\n",
    "        self.test_env = copy.deepcopy(env)\n",
    "        self.observation_dimensions = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.model = Network(config, self.num_actions, input_shape=self.observation_dimensions)\n",
    "\n",
    "        self.target_model = Network(config, self.num_actions, input_shape=self.observation_dimensions)\n",
    "\n",
    "        self.optimizer = config[\"optimizer_function\"]\n",
    "        self.adam_epsilon=config[\"adam_epsilon\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.loss_function = config[\"loss_function\"]\n",
    "        self.clipnorm = 10.0\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer(learning_rate=self.learning_rate, epsilon=self.adam_epsilon, clipnorm=self.clipnorm),\n",
    "            loss=config[\"loss_function\"],\n",
    "        )\n",
    "\n",
    "        self.target_model.compile(\n",
    "            optimizer=self.optimizer(learning_rate=self.learning_rate, epsilon=self.adam_epsilon, clipnorm=self.clipnorm),\n",
    "            loss=config[\"loss_function\"],\n",
    "        )\n",
    "\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.num_training_steps = int(config[\"num_training_steps\"])\n",
    "        self.start_episode = start_episode\n",
    "\n",
    "        self.discount_factor = config[\"discount_factor\"]\n",
    "\n",
    "        self.replay_batch_size = int(config[\"replay_batch_size\"])\n",
    "        self.replay_period = int(config[\"replay_period\"])\n",
    "        self.memory_size = max(int(config[\"memory_size\"]), self.replay_batch_size)\n",
    "        self.min_memory_size = int(config[\"min_memory_size\"])\n",
    "\n",
    "        self.soft_update = config[\"soft_update\"]\n",
    "        self.transfer_frequency = int(config[\"transfer_frequency\"])\n",
    "        self.ema_beta = config[\"ema_beta\"]\n",
    "\n",
    "        self.per_beta = config[\"per_beta\"]\n",
    "        # self.per_beta_increase = config[\"per_beta_increase\"]\n",
    "        self.per_beta_increase = (1 - self.per_beta) / self.num_training_steps\n",
    "        self.per_epsilon = config[\"per_epsilon\"]\n",
    "        # TESTING WITH FAST PRIORITIZED EXPERIENCE REPLAY\n",
    "        # it is an approximation but should be much faster computationally\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            observation_dimensions=self.observation_dimensions,\n",
    "            max_size=self.memory_size,\n",
    "            batch_size=self.replay_batch_size,\n",
    "            max_priority=1.0,\n",
    "            alpha=config[\"per_alpha\"],\n",
    "            # epsilon=config[\"per_epsilon\"],\n",
    "            n_step=config[\"n_step\"],\n",
    "            gamma=config[\"discount_factor\"],\n",
    "        )\n",
    "\n",
    "        self.use_n_step = config[\"n_step\"] > 1\n",
    "\n",
    "        self.n_step = config[\"n_step\"]\n",
    "\n",
    "        if self.use_n_step:\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                observation_dimensions=self.observation_dimensions,\n",
    "                max_size=self.memory_size,\n",
    "                batch_size=self.replay_batch_size,\n",
    "                n_step=self.n_step,\n",
    "                gamma=config[\"discount_factor\"],\n",
    "            )\n",
    "\n",
    "        self.v_min = config[\"v_min\"]\n",
    "        self.v_max = config[\"v_max\"]\n",
    "\n",
    "        self.atom_size = config[\"atom_size\"]\n",
    "        self.support = np.linspace(self.v_min, self.v_max, self.atom_size)\n",
    "\n",
    "        self.transition = list()\n",
    "        self.is_test = True\n",
    "        # self.search = search.Search(\n",
    "        #     scoring_function=self.score_state,\n",
    "        #     max_depth=config[\"search_max_depth\"],\n",
    "        #     max_time=config[\"search_max_time\"],\n",
    "        #     transposition_table=search.TranspositionTable(\n",
    "        #         buckets=config[\"search_transposition_table_buckets\"],\n",
    "        #         bucket_size=config[\"search_transposition_table_bucket_size\"],\n",
    "        #         replacement_strategy=search.TranspositionTable.replacement_strategies[\n",
    "        #             config[\"search_transposition_table_replacement_strategy\"]\n",
    "        #         ],\n",
    "        #     ),\n",
    "        #     debug=False,\n",
    "        # )\n",
    "\n",
    "    def export(self, episode=-1, best_model=False):\n",
    "        if episode != -1:\n",
    "            path = \"./{}_{}_episodes.keras\".format(\n",
    "                self.model_name, episode + self.start_episode\n",
    "            )\n",
    "        else:\n",
    "            path = \"./{}.keras\".format(self.model_name)\n",
    "\n",
    "        if best_model:\n",
    "            path = \"./best_model.keras\"\n",
    "\n",
    "        self.model.save(path)\n",
    "\n",
    "    def prepare_states(self, state):\n",
    "        if (self.env.observation_space.high == 255).all():\n",
    "            state = np.array(state)/255\n",
    "        # print(state.shape)\n",
    "        if state.shape == self.observation_dimensions:\n",
    "            new_shape = (1,) + state.shape\n",
    "            state_input = state.reshape(new_shape)\n",
    "        else:\n",
    "            state_input = state\n",
    "        # print(state_input.shape)\n",
    "        # observation_high = self.env.observation_space.high\n",
    "        # observation_low = self.env.observation_space.low\n",
    "        # for s in state_input:\n",
    "        #     for i in range(len(s)):\n",
    "        #         s[i] = s[i] - observation_low[i]\n",
    "        #         s[i] = s[i] / (observation_high[i] - observation_low[i])\n",
    "        # print(state_input)\n",
    "        # NORMALIZE VALUES\n",
    "        return state_input\n",
    "\n",
    "    def predict_single(self, state):\n",
    "        state_input = self.prepare_states(state)\n",
    "        # print(state_input)\n",
    "        q_values = self.model(inputs=state_input).numpy()\n",
    "        return q_values\n",
    "\n",
    "    def select_action(self, state):\n",
    "        q_values = np.sum(np.multiply(self.predict_single(state), np.array(self.support)), axis=2)\n",
    "        # print(q_values)\n",
    "        selected_action = np.argmax(q_values)\n",
    "        # selected_action = np.argmax(self.predict_single(state))\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action):\n",
    "        if not self.is_test:\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            self.transition += [reward, next_state, done]\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "\n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "        else:\n",
    "            next_state, reward, terminated, truncated, _ = self.test_env.step(action)\n",
    "\n",
    "        return next_state, reward, terminated, truncated\n",
    "\n",
    "    def experience_replay(self):\n",
    "        # print(\"Experience Replay\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            # print(\"One Step Learning\")\n",
    "            # time2 = 0\n",
    "            # time2 = time()\n",
    "            elementwise_loss = 0\n",
    "            samples = self.memory.sample(self.per_beta)\n",
    "            actions = samples[\"actions\"]\n",
    "            observations = samples[\"observations\"]\n",
    "            inputs = self.prepare_states(observations)\n",
    "            weights = samples[\"weights\"].reshape(-1, 1)\n",
    "            # print(\"weights\", weights)\n",
    "            indices = samples[\"indices\"]\n",
    "            discount_factor = self.discount_factor\n",
    "            target_ditributions = self.compute_target_distributions(samples, discount_factor)\n",
    "            self.model.loss.actions = samples[\"actions\"]\n",
    "            initial_distributions = self.model(inputs)\n",
    "            distributions_to_train = tf.gather_nd(initial_distributions, list(zip(range(initial_distributions.shape[0]), actions)))\n",
    "            elementwise_loss = self.model.loss.call(y_pred=distributions_to_train, y_true=tf.convert_to_tensor\n",
    "            (target_ditributions))\n",
    "            assert np.all(elementwise_loss) >= 0, \"Elementwise Loss: {}\".format(elementwise_loss)\n",
    "            # print(\"One Step Learning Time \", time() - time2)\n",
    "            if self.use_n_step:\n",
    "                # print(\"N-Step Learning\")\n",
    "                # time2 = time()\n",
    "                discount_factor = self.discount_factor ** self.n_step\n",
    "                n_step_samples = self.memory_n.sample_from_indices(indices)\n",
    "                actions = n_step_samples[\"actions\"]\n",
    "                n_step_observations = n_step_samples[\"observations\"]\n",
    "                observations = n_step_observations\n",
    "                inputs = self.prepare_states(observations)\n",
    "                target_ditributions = self.compute_target_distributions(n_step_samples, discount_factor)\n",
    "                self.model.loss.actions = n_step_samples[\"actions\"]\n",
    "                initial_distributions = self.model(inputs)\n",
    "                distributions_to_train = tf.gather_nd(initial_distributions, list(zip(range(initial_distributions.shape[0]), actions)))\n",
    "                elementwise_loss_n_step = self.model.loss.call(y_pred=distributions_to_train, y_true=tf.convert_to_tensor(target_ditributions))\n",
    "                # add the losses together to reduce variance (original paper just uses n_step loss)\n",
    "                elementwise_loss += elementwise_loss_n_step\n",
    "                assert np.all(elementwise_loss) >= 0, \"Elementwise Loss: {}\".format(elementwise_loss)\n",
    "                # print(\"Elementwise Loss N-Step Shape\", elementwise_loss_n_step.shape)\n",
    "                # print(\"N-Step Learning Time \", time() - time2)\n",
    "\n",
    "            # print(weights)\n",
    "            loss = tf.reduce_mean(elementwise_loss * weights)\n",
    "\n",
    "        #TRAINING WITH GRADIENT TAPE\n",
    "        # print(\"Computing Gradients\")\n",
    "        # time2 = time()\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        # print(\"Computing Gradients Time \", time() - time2)\n",
    "        # print(\"Applying Gradients\")\n",
    "        # time2 = time()\n",
    "        self.optimizer(learning_rate=self.learning_rate, epsilon=self.adam_epsilon, clipnorm=self.clipnorm).apply_gradients(grads_and_vars=zip(gradients, self.model.trainable_variables))\n",
    "        # print(\"Applying Gradients Time \", time() - time2)\n",
    "\n",
    "        # TRAINING WITH tf.train_on_batch\n",
    "        # print(\"Training Model on Batch\")\n",
    "        # loss = self.model.train_on_batch(samples[\"observations\"], target_ditributions, sample_weight=weights)\n",
    "\n",
    "        # print(\"Updating Priorities\")\n",
    "        # time2 = time()\n",
    "        prioritized_loss = elementwise_loss + self.per_epsilon\n",
    "        self.memory.update_priorities(indices, prioritized_loss)\n",
    "        # print(\"Updating Priorities Time \", time() - time2)\n",
    "\n",
    "        # print(\"Resetting Noise\")\n",
    "        # time2 = time()\n",
    "        self.model.reset_noise()\n",
    "        self.target_model.reset_noise()\n",
    "        # print(\"Resetting Noise Time \", time() - time2)\n",
    "\n",
    "        loss = loss.numpy()\n",
    "        # print(\"Experience Replay Time \", time() - time1)\n",
    "        return loss\n",
    "\n",
    "    def compute_target_distributions(self, samples, discount_factor):\n",
    "        # print(\"Computing Target Distributions\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        observations = samples[\"observations\"]\n",
    "        inputs = self.prepare_states(observations)\n",
    "        next_observations = samples[\"next_observations\"]\n",
    "        next_inputs = self.prepare_states(next_observations)\n",
    "        rewards = samples[\"rewards\"].reshape(-1,1)\n",
    "        dones = samples[\"dones\"].reshape(-1,1)\n",
    "\n",
    "        # print(rewards.shape, dones.shape)\n",
    "\n",
    "        next_actions = np.argmax(np.sum(self.model(inputs).numpy(), axis=2), axis=1)\n",
    "        target_network_distributions = self.target_model(next_inputs).numpy()\n",
    "\n",
    "        target_distributions = target_network_distributions[range(self.replay_batch_size), next_actions]\n",
    "        target_z = rewards + (1 - dones) * (discount_factor) * self.support\n",
    "        # print(\"Target Z\", target_z.shape)\n",
    "        target_z = np.clip(target_z, self.v_min, self.v_max)\n",
    "\n",
    "        b = ((target_z - self.v_min) / (self.v_max - self.v_min)) * (self.atom_size - 1)\n",
    "        # print(b)\n",
    "        l, u = tf.cast(tf.math.floor(b), tf.int32), tf.cast(tf.math.ceil(b), tf.int32)\n",
    "        # print(l, u)\n",
    "        m = np.zeros_like(target_distributions)\n",
    "        assert m.shape == l.shape\n",
    "        lower_distributions = target_distributions * (tf.cast(u, tf.float64) - b)\n",
    "        upper_distributions = target_distributions * (b - tf.cast(l, tf.float64))\n",
    "\n",
    "        for i in range(self.replay_batch_size):\n",
    "            np.add.at(m[i], np.asarray(l)[i], lower_distributions[i])\n",
    "            np.add.at(m[i], np.asarray(u)[i], upper_distributions[i])\n",
    "            # print(m[i])\n",
    "        # target_distributions = np.clip(m, 1e-3, 1)\n",
    "        target_distributions = m\n",
    "        # print(\"Computing Target Distributions Time \", time() - time1)\n",
    "        return target_distributions\n",
    "\n",
    "    # def score_state(self, state, turn):\n",
    "    #     state_input = self.prepare_state(state)\n",
    "    #     q = self.predict(state_input)\n",
    "\n",
    "    #     if (turn % 2) == 0:\n",
    "    #         return q.max(), q.argmax()\n",
    "\n",
    "    #     return q.min(), q.argmin()\n",
    "\n",
    "    # def play_optimal_move(\n",
    "    #     self, state: bb.Bitboard, turn: int, max_depth: int, with_output=True\n",
    "    # ):\n",
    "    #     # q_value, action = self.alpha_beta_pruning(state, turn, max_depth=max_depth)\n",
    "    #     q_value, action = self.search.iterative_deepening(state, turn, max_depth)\n",
    "    #     if with_output:\n",
    "    #         print(\"Evaluation: {}\".format(q_value))\n",
    "    #         print(\"Action: {}\".format(action + 1))\n",
    "    #     state.move(turn % 2, action)\n",
    "    #     winner, _ = state.check_victory()\n",
    "\n",
    "    #     if winner == 0:\n",
    "    #         return False\n",
    "    #     else:\n",
    "    #         return True\n",
    "\n",
    "    def action_mask(self, q, state, turn):\n",
    "        q_copy = copy.deepcopy(q)\n",
    "        for i in range(len(q_copy)):\n",
    "            if not state.is_valid_move(i):\n",
    "                if turn % 2 == 0:\n",
    "                    q_copy[i] = float(\"-inf\")\n",
    "                else:\n",
    "                    q_copy[i] = float(\"inf\")\n",
    "        return q_copy\n",
    "\n",
    "    def fill_memory(self):\n",
    "        state, _ = self.env.reset()\n",
    "        # print(state)\n",
    "        for experience in range(self.min_memory_size):\n",
    "            # clear_output(wait=False)\n",
    "            # print(\"Filling Memory\")\n",
    "            print(\"Memory Size: {}/{}\".format(experience, self.min_memory_size))\n",
    "            # state_input = self.prepare_state(state)\n",
    "            action = self.env.action_space.sample()\n",
    "            self.transition = [state, action]\n",
    "\n",
    "            next_state, reward, terminated, truncated = self.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "\n",
    "    def update_target_model(self, step):\n",
    "        # print(\"Updating Target Model\")\n",
    "        # time1 = 0\n",
    "        # time1 = time()\n",
    "        if self.soft_update:\n",
    "            new_weights = self.target_model.get_weights()\n",
    "\n",
    "            counter = 0\n",
    "            for wt, wp in zip(\n",
    "                self.target_model.get_weights(),\n",
    "                self.model.get_weights(),\n",
    "            ):\n",
    "                wt = (self.ema_beta * wt) + ((1 - self.ema_beta) * wp)\n",
    "                new_weights[counter] = wt\n",
    "                counter += 1\n",
    "            self.target_model.set_weights(new_weights)\n",
    "        else:\n",
    "            if step % self.transfer_frequency == 0 and (len(self.memory) >= self.replay_batch_size):\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "        # print(\"Updating Target Model Time \", time() - time1)\n",
    "\n",
    "    def train(self, graph_interval=200):\n",
    "        self.is_test = False\n",
    "        stat_score = [] # make these num trials divided by graph interval so i dont need to append (to make it faster?)\n",
    "        stat_test_score = []\n",
    "        stat_loss = []\n",
    "        self.fill_memory()\n",
    "        num_trials_truncated = 0\n",
    "        state, _ = self.env.reset()\n",
    "        model_update_count = 0\n",
    "        score = 0\n",
    "        for step in range(self.num_training_steps):\n",
    "            # state_input = self.prepare_state(state)\n",
    "            # clear_output(wait=False)\n",
    "            print(\"{} Step: {}/{}\".format(self.model_name, step, self.num_training_steps))\n",
    "            # print(\"Last Training Score: \", stat_score[-1] if len(stat_score) > 0 else 0)\n",
    "            # print(\"Last Training Loss: \", stat_loss[-1] if len(stat_loss) > 0 else 0)\n",
    "            action = self.select_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated = self.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if truncated:\n",
    "                num_trials_truncated += 1\n",
    "            #     if num_trials_truncated > 100:\n",
    "            #         num_trials_truncated += self.num_training_steps - step\n",
    "            #         break\n",
    "            self.per_beta = min(1.0, self.per_beta + self.per_beta_increase)\n",
    "\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                stat_score.append(score)\n",
    "                if score >= self.env.spec.reward_threshold:\n",
    "                    print(\"Your DQN agent has achieved the env's reward threshold.\")\n",
    "                    # test_score = self.test()\n",
    "                    # if test_score >= self.env.spec.reward_threshold:\n",
    "                    #     print(\"Congratulations!\")\n",
    "                    #     break\n",
    "                    # else:\n",
    "                    #     print(\"It was a fluke!\")\n",
    "                score = 0\n",
    "\n",
    "            if (step % self.replay_period) == 0 and (len(self.memory) >= self.replay_batch_size):\n",
    "                model_update_count += 1\n",
    "                loss = self.experience_replay()\n",
    "                stat_loss.append(loss)\n",
    "\n",
    "                self.update_target_model(model_update_count)\n",
    "\n",
    "\n",
    "            if step % graph_interval == 0 and step > 0:\n",
    "                self.export()\n",
    "                # stat_test_score.append(self.test())\n",
    "                self.plot_graph(stat_score, stat_loss, stat_test_score, step)\n",
    "\n",
    "        self.plot_graph(stat_score, stat_loss, stat_test_score, step)\n",
    "        self.export()\n",
    "        self.env.close()\n",
    "        return num_trials_truncated / self.num_training_steps\n",
    "\n",
    "    def plot_graph(self, score, loss, test_score, step):\n",
    "            fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(30, 5))\n",
    "            ax1.plot(score, linestyle=\"solid\")\n",
    "            ax1.set_title('Frame {}. Score: {}'.format(step, np.mean(score[-10:])))\n",
    "            ax2.plot(loss, linestyle=\"solid\")\n",
    "            ax2.set_title('Frame {}. Loss: {}'.format(step, np.mean(loss[-10:])))\n",
    "            ax3.plot(test_score, linestyle=\"solid\")\n",
    "            ax3.axhline(y=self.env.spec.reward_threshold, color='r', linestyle='-')\n",
    "            ax3.set_title('Frame {}. Test Score: {}'.format(step, np.mean(test_score[-10:])))\n",
    "            plt.savefig(\"./{}.png\".format(self.model_name))\n",
    "            plt.close(fig)\n",
    "\n",
    "    def test(self, video_folder = '', num_trials=100) -> None:\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        average_score = 0\n",
    "        for trials in range(num_trials - 1):\n",
    "            state, _ = self.test_env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated = self.step(action)\n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "\n",
    "                score += reward\n",
    "            average_score += score\n",
    "            print(\"score: \", score)\n",
    "\n",
    "        if video_folder == '':\n",
    "            video_folder = \"./videos/{}\".format(self.model_name)\n",
    "        # for recording a video\n",
    "        self.test_env = gym.wrappers.RecordVideo(self.test_env, video_folder)\n",
    "        state, _ = self.test_env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated = self.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        average_score += score\n",
    "        self.test_env.close()\n",
    "\n",
    "        # reset\n",
    "        self.is_test = False\n",
    "        average_score /= num_trials\n",
    "        return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeZeroToOne(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_high = self.env.observation_space.high\n",
    "        self.observation_low = self.env.observation_space.low\n",
    "\n",
    "    def observation(self, obs):\n",
    "        print(obs)\n",
    "        print((obs - self.observation_low) / (self.observation_high - self.observation_low))\n",
    "        return (obs - self.observation_low) / (self.observation_high - self.observation_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online with frame stackign though\n",
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online\n",
    "# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "# env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
    "# env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'conv_layers': [(32, 8, (4, 4)), (64, 4, (2, 2)), (64, 3, (1, 1))],\n",
    "#     'conv_layers_noisy': False,\n",
    "#     'dense_layers': [512],\n",
    "#     'dense_layers_noisy': True,\n",
    "#     'noisy_sigma': 0.5,\n",
    "#     'activation': 'relu',\n",
    "#     'kernel_initializer': initializers.VarianceScaling(scale=1.0/np.sqrt(3.0), mode='fan_in', distribution='uniform'),\n",
    "#     'optimizer_function': tf.keras.optimizers.legacy.Adam,\n",
    "#     'adam_epsilon': 1.5e-4,\n",
    "#     'learning_rate': 0.0000625,\n",
    "#     'loss_function': tf.keras.losses.CategoricalCrossentropy(), #? KL Divergence?\n",
    "#     'dueling': True,\n",
    "#     'advantage_hidden_layers': [],\n",
    "#     'value_hidden_layers': [],\n",
    "#     'num_training_steps': 1000000, #\n",
    "#     'discount_factor': 0.99,\n",
    "#     'soft_update': False,\n",
    "#     'ema_beta': 0.99,\n",
    "#     'transfer_frequency': 32000,\n",
    "#     'per_epsilon': 1e-6, #\n",
    "#     'per_alpha': 0.5,\n",
    "#     'per_beta': 0.4,\n",
    "#     'per_beta_increase': 1/1000000,\n",
    "#     'replay_batch_size': 32,\n",
    "#     'replay_period': 4,\n",
    "#     'memory_size': 1000000,\n",
    "#     'min_memory_size': 80000,\n",
    "#     'n_step': 3,\n",
    "#     'v_min': -10.0,\n",
    "#     'v_max': 10.0,\n",
    "#     'atom_size': 51,\n",
    "#     'search_max_depth': 5,\n",
    "#     'search_max_time': 10,\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    'conv_layers': [],\n",
    "    'conv_layers_noisy': False,\n",
    "    'dense_layers': [128],\n",
    "    'dense_layers_noisy': False,\n",
    "    'noisy_sigma': 0.5,\n",
    "    'activation': 'relu',\n",
    "    'kernel_initializer': tf.keras.initializers.GlorotUniform(),\n",
    "    'optimizer_function': tf.keras.optimizers.legacy.Adam,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'learning_rate': 1e-3,\n",
    "    'loss_function': tf.keras.losses.CategoricalCrossentropy(), #? KL Divergence?\n",
    "    'dueling': True,\n",
    "    'advantage_hidden_layers': [128],\n",
    "    'value_hidden_layers': [128],\n",
    "    'num_training_steps': 3000, #\n",
    "    'discount_factor': 0.99,\n",
    "    'soft_update': False,\n",
    "    'ema_beta': 0.99,\n",
    "    'transfer_frequency': 100,\n",
    "    'per_epsilon': 1e-6, #\n",
    "    'per_alpha': 0.2,\n",
    "    'per_beta': 0.6,\n",
    "    'per_beta_increase': 1/10000,\n",
    "    'replay_batch_size': 128,\n",
    "    'replay_period': 1,\n",
    "    'memory_size': 10000,\n",
    "    'min_memory_size': 0,\n",
    "    'n_step': 3,\n",
    "    'v_min': 0.0,\n",
    "    'v_max': 200.0,\n",
    "    'atom_size': 51,\n",
    "    'search_max_depth': 5,\n",
    "    'search_max_time': 10,\n",
    "}\n",
    "\n",
    "# train\n",
    "# agent = RainbowDQN(env, \"RainbowDQN-{}\".format(env.unwrapped.spec.id), config=config)\n",
    "# agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def ipython_show_video(path: str) -> None:\n",
    "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise NameError(\"Cannot access: {}\".format(path))\n",
    "\n",
    "    video = io.open(path, \"r+b\").read()\n",
    "    encoded = base64.b64encode(video)\n",
    "\n",
    "    display(HTML(\n",
    "        data=\"\"\"\n",
    "        <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
    "        </video>\n",
    "        \"\"\".format(encoded.decode(\"ascii\"))\n",
    "    ))\n",
    "\n",
    "\n",
    "def show_latest_video(video_folder: str) -> str:\n",
    "    \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
    "    list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    ipython_show_video(latest_file)\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "# latest_file = show_latest_video(video_folder='./video')\n",
    "# print(\"Played:\", latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def list_scores(trials):\n",
    "    print(\"{} Scores\".format(env.unwrapped.spec.id))\n",
    "    trials = pickle.load(open(\"./{}_trials.p\".format(env.unwrapped.spec.id), \"rb\"))\n",
    "\n",
    "    total_score = 0\n",
    "    scores = []\n",
    "    for trial in trials.trials:\n",
    "        total_score += -1 * trial['result']['loss']\n",
    "        scores.append((trial['tid'] + 1, int(-1 * trial['result']['loss'])))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1])\n",
    "    scores.reverse()\n",
    "    for score in scores:\n",
    "        print(\"Model {} | Score: {}\".format(score[0], score[1]))\n",
    "\n",
    "    average_score = int(total_score / len(trials.trials))\n",
    "    print(\"Average Score: \", average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "  0%|          | 0/3 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-4:\n",
      "Process SpawnPoolWorker-1:\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute '/var/folders/by/wwwbwc016yxfhlbgfph7_xmc0000gn/T/ipykernel_70697/3000282594py\\x0045' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute '/var/folders/by/wwwbwc016yxfhlbgfph7_xmc0000gn/T/ipykernel_70697/3000282594py\\x0045' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute '/var/folders/by/wwwbwc016yxfhlbgfph7_xmc0000gn/T/ipykernel_70697/3000282594py\\x0045' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [02:18<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 256\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# create a new trials object and start searching\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# trials = Trials()\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Objective Function to optimize\u001b[39;49;00m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Hyperparameter's Search Space\u001b[39;49;00m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Optimization algorithm (representative TPE)\u001b[39;49;00m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of optimization attempts\u001b[39;49;00m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Record the results\u001b[39;49;00m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# early_stop_fn=no_progress_loss(5, 1),\u001b[39;49;00m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./classiccontrol_trials.p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_best_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n\u001b[1;32m    268\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m space_eval(search_space, best)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mglobalize.<locals>.result\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     80\u001b[0m args_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[params \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m environments_list], environments_list, [name \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m environments_list]])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(\u001b[38;5;241m8\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# print(pool.map(func1, range(10)))\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(\u001b[38;5;241m8\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# print(list(executor.map(func1, range(10))))\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(func1, (args \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list))))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = f\"{1}\"\n",
    "# os.environ['TF_NUM_INTEROP_THREADS'] = f\"{1}\"\n",
    "# os.environ['TF_NUM_INTRAOP_THREADS'] = f\"{1}\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "from rainbow_dqn import RainbowDQN\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "from hyperopt import tpe, hp, fmin, space_eval\n",
    "\n",
    "\n",
    "# MAGIC CODE DO NOT TOUCH\n",
    "def globalize(func):\n",
    "    def result(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    result.__name__ = result.__qualname__ = (\n",
    "        os.path.abspath(func.__code__.co_filename).replace('.', '') + '\\0' +\n",
    "        str(func.__code__.co_firstlineno))\n",
    "    setattr(sys.modules[result.__module__], result.__name__, result)\n",
    "    return result\n",
    "\n",
    "def make_func():\n",
    "    def run_training(args):\n",
    "        m = RainbowDQN(\n",
    "            env=args[1],\n",
    "            model_name=\"{}_{}\".format(args[2], args[1].unwrapped.spec.id),\n",
    "            config=args[0]\n",
    "        )\n",
    "        m.train()\n",
    "        print(\"Training complete\")\n",
    "        return m.test()\n",
    "    return run_training\n",
    "\n",
    "func1 = globalize(make_func())\n",
    "\n",
    "def objective(params):\n",
    "    environments_list = [gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\"), gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\"), ]\n",
    "    \n",
    "    if os.path.exists(\"./classiccontrol_trials.p\"):\n",
    "        trials = pickle.load(open(\"./classiccontrol_trials.p\", \"rb\"))\n",
    "        name = \"classiccontrol_{}\".format(len(trials.trials) + 1)\n",
    "    else:\n",
    "        name = \"classiccontrol_1\"\n",
    "    # name = datetime.datetime.now().timestamp()\n",
    "    params[\"model_name\"] = name\n",
    "    entry = pandas.DataFrame.from_dict(\n",
    "        params,\n",
    "        orient=\"index\",\n",
    "    ).T\n",
    "\n",
    "    entry.to_csv(\n",
    "        \"classiccontrol_results.csv\",\n",
    "        mode=\"a\",\n",
    "        header=False,\n",
    "    )\n",
    "    \n",
    "    num_workers = len(environments_list)\n",
    "    args_list = np.array([[params for env in environments_list], environments_list, [name for env in environments_list]]).T\n",
    "    with multiprocessing.Pool(8) as pool:\n",
    "        # print(pool.map(func1, range(10)))\n",
    "        print(pool.map(func1, (args for args in args_list)))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(8) as executor:\n",
    "        # print(list(executor.map(func1, range(10))))\n",
    "        print(list(executor.map(func1, (args for args in args_list))))\n",
    "\n",
    "\n",
    "func2 = globalize(objective)\n",
    "\n",
    "from hyperopt import hp\n",
    "import tensorflow as tf\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "def create_search_space():\n",
    "    search_space = {\n",
    "        'activation': hp.choice('activation', [\n",
    "            'linear',\n",
    "            'relu',\n",
    "            # 'relu6',\n",
    "            'sigmoid',\n",
    "            'softplus',\n",
    "            'soft_sign',\n",
    "            'silu',\n",
    "            'swish',\n",
    "            'log_sigmoid',\n",
    "            'hard_sigmoid',\n",
    "            # 'hard_silu',\n",
    "            # 'hard_swish',\n",
    "            # 'hard_tanh',\n",
    "            'elu',\n",
    "            # 'celu',\n",
    "            'selu',\n",
    "            'gelu',\n",
    "            'glu'\n",
    "        ]),\n",
    "        'kernel_initializer': hp.choice('kernel_initializer', ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal', 'lecun_uniform', 'lecun_normal', 'orthogonal', 'variance_baseline', 'variance_0.1', 'variance_0.3', 'variance_0.8', 'variance_3', 'variance_5', 'variance_10']),\n",
    "        'optimizer_function': hp.choice('optimizer_function', [tf.keras.optimizers.legacy.Adam]), # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "        'learning_rate': hp.choice('learning_rate', [10, 5, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]), #\n",
    "        'adam_epsilon': hp.choice('adam_epsilon', [1, 0.5, 0.3125, 0.03125, 0.003125, 0.0003125, 0.00003125, 0.000003125]),\n",
    "        # NORMALIZATION?\n",
    "        'soft_update': hp.choice('soft_update', [False]), # seems to always be false, we can try it with tru\n",
    "        'ema_beta': hp.uniform('ema_beta', 0.95, 0.999),\n",
    "        'transfer_frequency': hp.choice('transfer_frequency', [10, 25, 50, 100, 200, 400, 800, 1600, 2000]),\n",
    "        'replay_period': hp.choice('replay_period', [1, 2, 3, 4, 5, 8, 10, 12]),\n",
    "        'replay_batch_size': hp.choice('replay_batch_size', [2 ** i for i in range(0, 8)]), ###########\n",
    "        'memory_size': hp.choice('memory_size', [2000, 3000, 5000, 7500, 10000, 15000, 20000, 25000]), #############\n",
    "        'min_memory_size': hp.choice('min_memory_size', [125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000]),\n",
    "        'n_step': hp.choice('n_step', [1, 2, 3, 4, 5, 8, 10]),\n",
    "        'discount_factor': hp.choice('discount_factor', [0.1, 0.5, 0.9, 0.99, 0.995, 0.999]),\n",
    "        'atom_size': hp.choice('atom_size', [11, 21, 31, 41, 51, 61, 71, 81]), #\n",
    "        'conv_layers': hp.choice('conv_layers', [[]]),\n",
    "        'conv_layers_noisy': hp.choice('conv_layers_noisy', [False]),\n",
    "        'width': hp.choice('width', [32, 64, 128, 256, 512, 1024]),\n",
    "        'dense_layers': hp.choice('dense_layers', [0, 1, 2, 3, 4]),\n",
    "        'dense_layers_noisy': hp.choice('dense_layers_noisy', [True]), # i think this is always true for rainbow\n",
    "        # REWARD CLIPPING\n",
    "        'noisy_sigma': hp.choice('noisy_sigma', [0.5]), #\n",
    "        'loss_function': hp.choice('loss_function', [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()]),\n",
    "        'dueling': hp.choice('dueling', [True]),\n",
    "        'advantage_hidden_layers': hp.choice('advantage_hidden_layers', [0, 1, 2, 3, 4]), #\n",
    "        'value_hidden_layers': hp.choice('value_hidden_layers', [0, 1, 2, 3, 4]), #\n",
    "        'num_training_steps': hp.choice('num_training_steps', [25000]),\n",
    "        'per_epsilon': hp.choice('per_epsilon', [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]),\n",
    "        'per_alpha': hp.choice('per_alpha', [0.05 * i for i in range(0, 21)]),\n",
    "        'per_beta': hp.choice('per_beta', [0.05 * i for i in range(1, 21)]),\n",
    "        # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        'v_min': hp.choice('v_min', [-500.0]), # MIN GAME SCORE\n",
    "        'v_max': hp.choice('v_max', [500.0]), # MAX GAME SCORE\n",
    "        # 'search_max_depth': 5,\n",
    "        # 'search_max_time': 10,\n",
    "    }\n",
    "\n",
    "    # search_space = {\n",
    "    #     'conv_layers': hp.choice('conv_layers', [[]]),\n",
    "    #     'conv_layers_noisy': hp.choice('conv_layers_noisy', [False]), #\n",
    "    #     'dense_layers': hp.choice('dense_layers', [\n",
    "    #         [], [32], [32, 32], [32, 32, 32], [64], [64, 64], [64, 64, 64], [128], [128, 128], [128, 128, 128], [256], [256, 256], [256, 256, 256], [512], [512, 512], [512, 512, 512], [1024], [1024, 1024], [1024, 1024, 1024]\n",
    "    #     ]),\n",
    "    #     'dense_layers_noisy': hp.choice('dense_layers_noisy', [True, False]), #\n",
    "    #     'noisy_sigma': hp.uniform('noisy_sigma', 0.1, 1.0), #\n",
    "    #     'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
    "    #     'kernel_initializer': hp.choice('kernel_initializer', ['he_uniform', 'he_normal', 'glorot_uniform', 'glorot_normal', 'lecun_uniform', 'lecun_normal']),\n",
    "    #     'optimizer_function': hp.choice('optimizer_function', [tf.keras.optimizers.legacy.Adam, tf.keras.optimizers.legacy.SGD]),\n",
    "    #     'adam_epsilon': hp.uniform('adam_epsilon', 0.00001, 1.0), #\n",
    "    #     'learning_rate': hp.uniform('learning_rate', 0.000001, 0.0025),\n",
    "    #     'loss_function': hp.choice('loss_function', [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()]),\n",
    "    #     'dueling': hp.choice('dueling', [True]),\n",
    "    #     'advantage_hidden_layers': [], #\n",
    "    #     'value_hidden_layers': [], #\n",
    "    #     'num_training_steps': scope.int(hp.quniform('num_training_steps', 5000, 10000, 100)), #\n",
    "    #     'discount_factor': hp.uniform('discount_factor', 0.85, 0.999),\n",
    "    #     'soft_update': hp.choice('soft_update', [True, False]),\n",
    "    #     'ema_beta': hp.uniform('ema_beta', 0.95, 0.999),\n",
    "    #     'transfer_frequency': scope.int(hp.uniform('transfer_frequency', 0, 200)), #\n",
    "    #     'per_epsilon': hp.uniform('per_epsilon', 0.000001, 0.1),\n",
    "    #     'per_alpha': hp.choice('per_alpha', [0.05 * i for i in range(0, 21)]),\n",
    "    #     'per_beta': hp.choice('per_beta', [0.05 * i for i in range(0, 21)]),\n",
    "    #     'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "    #     'replay_batch_size': hp.choice('replay_batch_size', [2 ** i for i in range(0, 8)]),\n",
    "    #     'replay_period': scope.int(hp.quniform('replay_period', 1, 10, 1)),\n",
    "    #     'memory_size': scope.int(hp.quniform('memory_size', 1, 100000, 1)), #\n",
    "    #     'min_memory_size': hp.uniform('min_memory_size', 0, 3000), #\n",
    "    #     'n_step': scope.int(hp.quniform('n_step', 1, 5, 1)), #\n",
    "    #     'v_min': hp.choice('v_min', [0.0]), #\n",
    "    #     'v_max': scope.int(hp.quniform('v_max', 100.0, 1000.0, 100.0)),\n",
    "    #     'atom_size': hp.choice('atom_size', [51]), #\n",
    "    #     # 'search_max_depth': 5,\n",
    "    #     # 'search_max_time': 10,\n",
    "    # }\n",
    "    # Current best setting\n",
    "    # For hp.uniform specify the exact value\n",
    "    # For hp.choice specify the index (0 based indexing) in the array\n",
    "    initial_best_config = []\n",
    "\n",
    "    return search_space, initial_best_config\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = {\n",
    "        'activation': 'relu',\n",
    "        'kernel_initializer': 'he_uniform',\n",
    "        'optimizer_function': tf.keras.optimizers.legacy.Adam, # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "        'learning_rate': 0.001, #\n",
    "        'adam_epsilon': 0.00003125,\n",
    "        # NORMALIZATION?\n",
    "        'soft_update': False, # seems to always be false, we can try it with tru\n",
    "        'ema_beta': 0.95,\n",
    "        'transfer_frequency': 100,\n",
    "        'replay_period': 1,\n",
    "        'replay_batch_size': 128,\n",
    "        'memory_size': 10000, #############\n",
    "        'min_memory_size': 500,\n",
    "        'n_step': 3,\n",
    "        'discount_factor': 0.99,\n",
    "        'atom_size': 51, #\n",
    "        'conv_layers': [],\n",
    "        'conv_layers_noisy': False,\n",
    "        'width': 512,\n",
    "        'dense_layers': 2,\n",
    "        'dense_layers_noisy': True, # i think this is always true for rainbow\n",
    "        # REWARD CLIPPING\n",
    "        'noisy_sigma': 0.5, #\n",
    "        'loss_function': tf.keras.losses.KLDivergence(),\n",
    "        'dueling': True,\n",
    "        'advantage_hidden_layers': 1, #\n",
    "        'value_hidden_layers': 1, #\n",
    "        'num_training_steps': 25000,\n",
    "        'per_epsilon': 0.001,\n",
    "        'per_alpha': 0.5,\n",
    "        'per_beta': 0.5,\n",
    "        # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        'v_min': -500.0, # MIN GAME SCORE\n",
    "        'v_max': 500.0, # MAX GAME SCORE\n",
    "        # 'search_max_depth': 5,\n",
    "        # 'search_max_time': 10,\n",
    "    }\n",
    "    \n",
    "    search_space, initial_best_config = create_search_space()\n",
    "\n",
    "\n",
    "    max_trials = 3\n",
    "    trials_step = 10  # how many additional trials to do after loading the last ones\n",
    "\n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"./classiccontrol_trials.p\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        # trials = Trials()\n",
    "        trials = None\n",
    "\n",
    "    best = fmin(\n",
    "        fn=func2, # Objective Function to optimize\n",
    "        space=search_space, # Hyperparameter's Search Space\n",
    "        algo=tpe.suggest, # Optimization algorithm (representative TPE)\n",
    "        max_evals=max_trials, # Number of optimization attempts\n",
    "        trials=trials, # Record the results\n",
    "        # early_stop_fn=no_progress_loss(5, 1),\n",
    "        trials_save_file=\"./classiccontrol_trials.p\",\n",
    "        points_to_evaluate=initial_best_config,\n",
    "    )\n",
    "\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
