{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prepare_activations' from 'utils' (/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_activations\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'prepare_activations' from 'utils' (/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import prepare_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_plots = list(map(list, zip(*[1, 2, 3, 4])))\n",
    "print(len(score_plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.empty((1, 1), dtype=np.int32)\n",
    "print(arr)  \n",
    "arr = np.append(arr, [[1, 1, 1],[1, 1, 1],[1, 1, 1]], axis=0)\n",
    "print(arr)\n",
    "arr = np.append(arr, [[1, 1, 1],[1, 1, 1],[1, 1, 1]], axis=0)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "import random\n",
    "env = gym.make('custom_gym_envs/MississippiMarbles-v0', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = random.choice(info['legal_moves'])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym_envs\n",
    "# import gymnasium as gym\n",
    "# env = gym.make('gym_envs/TicTacToe-v0')\n",
    "\n",
    "# state, info = env.reset()\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(0)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(4)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(3)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(6)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(2)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(1)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(7)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(8)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(5)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# print(\"Truncated:\", truncated)\n",
    "# env.render()\n",
    "\n",
    "\n",
    "# env.reset()\n",
    "# state, reward, terminated, truncated, info = env.step(0)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(3)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(7)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(4)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(2)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(6)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(1)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# print(\"Truncated:\", truncated)\n",
    "# print(\"Reward:\", reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeZeroToOne(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_high = self.env.observation_space.high\n",
    "        self.observation_low = self.env.observation_space.low\n",
    "\n",
    "    def observation(self, obs):\n",
    "        print(obs)\n",
    "        print((obs - self.observation_low) / (self.observation_high - self.observation_low))\n",
    "        return (obs - self.observation_low) / (self.observation_high - self.observation_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online with frame stackign though\n",
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online\n",
    "env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'utils' from 'utils' (/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrainbow_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RainbowAgent\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/rainbow/rainbow_agent.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magent_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RainbowConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/configs/agent_configs/agent_configs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malphazero_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlphaZeroConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mape_x_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     ApeXLearnerConfig,\n\u001b[1;32m      4\u001b[0m     ApeXActorConfig,\n\u001b[1;32m      5\u001b[0m     ApeXConfig,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     DistributedLearnerConfig,\n\u001b[1;32m      9\u001b[0m     DistributedActorConfig,\n\u001b[1;32m     10\u001b[0m     DistributedConfig,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/configs/agent_configs/agent_configs/alphazero_config.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlphaZeroConfig\u001b[39;00m(Config):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_dict, game_config):\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/configs/agent_configs/agent_configs/base_config.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfigBase\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'utils' from 'utils' (/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_space():\n",
    "    search_space = {\n",
    "        \"activation\": hp.choice(\n",
    "            \"activation\",\n",
    "            [\n",
    "                \"linear\",\n",
    "                \"relu\",\n",
    "                # 'relu6',\n",
    "                \"sigmoid\",\n",
    "                \"softplus\",\n",
    "                \"soft_sign\",\n",
    "                \"silu\",\n",
    "                \"swish\",\n",
    "                \"log_sigmoid\",\n",
    "                \"hard_sigmoid\",\n",
    "                # 'hard_silu',\n",
    "                # 'hard_swish',\n",
    "                # 'hard_tanh',\n",
    "                \"elu\",\n",
    "                # 'celu',\n",
    "                \"selu\",\n",
    "                \"gelu\",\n",
    "                # 'glu'\n",
    "            ],\n",
    "        ),\n",
    "        \"kernel_initializer\": hp.choice(\n",
    "            \"kernel_initializer\",\n",
    "            [\n",
    "                \"he_uniform\",\n",
    "                \"he_normal\",\n",
    "                \"glorot_uniform\",\n",
    "                \"glorot_normal\",\n",
    "                \"lecun_uniform\",\n",
    "                \"lecun_normal\",\n",
    "                \"orthogonal\",\n",
    "                \"variance_baseline\",\n",
    "                \"variance_0.1\",\n",
    "                \"variance_0.3\",\n",
    "                \"variance_0.8\",\n",
    "                \"variance_3\",\n",
    "                \"variance_5\",\n",
    "                \"variance_10\",\n",
    "            ],\n",
    "        ),\n",
    "        \"optimizer\": hp.choice(\n",
    "            \"optimizer\", [tf.keras.optimizers.legacy.Adam]\n",
    "        ),  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "        \"learning_rate\": hp.choice(\n",
    "            \"learning_rate\", [10, 5, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "        ),  #\n",
    "        \"adam_epsilon\": hp.choice(\n",
    "            \"adam_epsilon\",\n",
    "            [1, 0.5, 0.3125, 0.03125, 0.003125, 0.0003125, 0.00003125, 0.000003125],\n",
    "        ),\n",
    "        \"clipnorm\": hp.choice(\"clipnorm\", [None]),\n",
    "        # NORMALIZATION?\n",
    "        \"soft_update\": hp.choice(\n",
    "            \"soft_update\", [False]\n",
    "        ),  # seems to always be false, we can try it with tru\n",
    "        \"ema_beta\": hp.uniform(\"ema_beta\", 0.95, 0.999),\n",
    "        \"transfer_interval\": hp.choice(\n",
    "            \"transfer_interval\", [10, 25, 50, 100, 200, 400, 800, 1600, 2000]\n",
    "        ),\n",
    "        \"replay_interval\": hp.choice(\"replay_interval\", [1, 2, 3, 4, 5, 8, 10, 12, 350]),\n",
    "        \"minibatch_size\": hp.choice(\n",
    "            \"minibatch_size\", [2**i for i in range(0, 8)]\n",
    "        ),  ###########\n",
    "        \"replay_buffer_size\": hp.choice(\n",
    "            \"replay_buffer_size\", [2000, 3000, 5000, 7500, 10000, 15000, 20000, 25000, 50000]\n",
    "        ),  #############\n",
    "        \"min_replay_buffer_size\": hp.choice(\n",
    "            \"min_replay_buffer_size\", [0, 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000]\n",
    "        ),  # 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000\n",
    "        \"n_step\": hp.choice(\"n_step\", [1, 2, 3, 4, 5, 8, 10]),\n",
    "        \"discount_factor\": hp.choice(\n",
    "            \"discount_factor\", [0.1, 0.5, 0.9, 0.99, 0.995, 0.999]\n",
    "        ),\n",
    "        \"atom_size\": hp.choice(\"atom_size\", [11, 21, 31, 41, 51, 61, 71, 81]),  #\n",
    "        \"conv_layers\": hp.choice(\"conv_layers\", [[], [(32, 8, 4), (64, 4, 2), (64, 3, 1)]]),\n",
    "        \"conv_layers_noisy\": hp.choice(\"conv_layers_noisy\", [False]),\n",
    "        \"width\": hp.choice(\"width\", [32, 64, 128, 256, 512, 1024]),\n",
    "        \"dense_layers\": hp.choice(\"dense_layers\", [0, 1, 2, 3, 4]),\n",
    "        \"dense_layers_noisy\": hp.choice(\n",
    "            \"dense_layers_noisy\", [True]\n",
    "        ),  # i think this is always true for rainbow\n",
    "        # REWARD CLIPPING\n",
    "        \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.5]),  #\n",
    "        \"loss_function\": hp.choice(\n",
    "            \"loss_function\",\n",
    "            [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()],\n",
    "        ),\n",
    "        \"dueling\": hp.choice(\"dueling\", [True]),\n",
    "        \"advantage_hidden_layers\": hp.choice(\n",
    "            \"advantage_hidden_layers\", [0, 1, 2, 3, 4]\n",
    "        ),  #\n",
    "        \"value_hidden_layers\": hp.choice(\"value_hidden_layers\", [0, 1, 2, 3, 4]),  #\n",
    "        \"training_steps\": hp.choice(\"training_steps\", [30000]),\n",
    "        \"per_epsilon\": hp.choice(\n",
    "            \"per_epsilon\", [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "        ),\n",
    "        \"per_alpha\": hp.choice(\"per_alpha\", [0.05 * i for i in range(0, 21)]),\n",
    "        \"per_beta\": hp.choice(\"per_beta\", [0.05 * i for i in range(1, 21)]),\n",
    "        # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        # 'search_max_depth': 5,\n",
    "        # 'search_max_time': 10,\n",
    "        \"training_iterations\": hp.choice(\"training_iterations\", [1, 2, 3, 4, 5]),\n",
    "        \"num_minibatches\": hp.choice(\"num_minibatches\", [1, 2, 3, 4, 5]),\n",
    "    }\n",
    "    initial_best_config = [\n",
    "        {\n",
    "            \"activation\": 1,\n",
    "            \"kernel_initializer\": 6,\n",
    "            \"optimizer\": 0,  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "            \"learning_rate\": 5,  #\n",
    "            \"adam_epsilon\": 5,\n",
    "            \"clipnorm\": 0,\n",
    "            # NORMALIZATION?\n",
    "            \"soft_update\": 0,  # seems to always be false, we can try it with tru\n",
    "            \"ema_beta\": 0.95,\n",
    "            \"transfer_interval\": 3,\n",
    "            \"replay_interval\": 1,\n",
    "            \"minibatch_size\": 7,\n",
    "            \"replay_buffer_size\": 8,  \n",
    "            \"min_replay_buffer_size\": 4,\n",
    "            \"n_step\": 2,\n",
    "            \"discount_factor\": 3,\n",
    "            \"atom_size\": 4,  #\n",
    "            \"conv_layers\": 0,\n",
    "            \"conv_layers_noisy\": 0,\n",
    "            \"width\": 4,\n",
    "            \"dense_layers\": 2,\n",
    "            \"dense_layers_noisy\": 0,  # i think this is always true for rainbow\n",
    "            # REWARD CLIPPING\n",
    "            \"noisy_sigma\": 0,  #\n",
    "            \"loss_function\": 0,\n",
    "            \"dueling\": 0,\n",
    "            \"advantage_hidden_layers\": 0,  #\n",
    "            \"value_hidden_layers\": 0,  #\n",
    "            \"training_steps\": 0,\n",
    "            \"per_epsilon\": 3,\n",
    "            \"per_alpha\": 10,\n",
    "            \"per_beta\": 7,\n",
    "            # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "            # 'search_max_depth': 5,\n",
    "            # 'search_max_time': 10,\n",
    "            \"training_iterations\": 0,\n",
    "            \"num_minibatches\": 0,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return search_space, initial_best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'adam_epsilon': 0.0003125, 'advantage_hidden_layers': 0, 'atom_size': 51, 'clipnorm': None, 'conv_layers': (), 'conv_layers_noisy': False, 'dense_layers': 2, 'dense_layers_noisy': True, 'discount_factor': 0.99, 'dueling': True, 'ema_beta': 0.95, 'kernel_initializer': 'orthogonal', 'learning_rate': 0.01, 'loss_function': <keras.src.losses.CategoricalCrossentropy object at 0x29f2d0160>, 'min_replay_buffer_size': 500, 'minibatch_size': 128, 'n_step': 3, 'noisy_sigma': 0.5, 'num_minibatches': 1, 'optimizer': <class 'keras.src.optimizers.legacy.adam.Adam'>, 'per_alpha': 0.5, 'per_beta': 0.4, 'per_epsilon': 0.001, 'replay_buffer_size': 50000, 'replay_interval': 2, 'soft_update': False, 'training_iterations': 1, 'training_steps': 30000, 'transfer_interval': 100, 'value_hidden_layers': 0, 'width': 512}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import space_eval\n",
    "\n",
    "search_sapce, initial_best_config = create_search_space()\n",
    "config = space_eval(search_sapce, initial_best_config[0])\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magent_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RainbowConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgame_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CartPoleConfig\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m RainbowConfig(config, CartPoleConfig())\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/configs/agent_configs/agent_configs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malphazero_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlphaZeroConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mape_x_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     ApeXLearnerConfig,\n\u001b[1;32m      4\u001b[0m     ApeXActorConfig,\n\u001b[1;32m      5\u001b[0m     ApeXConfig,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     DistributedLearnerConfig,\n\u001b[1;32m      9\u001b[0m     DistributedActorConfig,\n\u001b[1;32m     10\u001b[0m     DistributedConfig,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/configs/agent_configs/agent_configs/alphazero_config.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlphaZeroConfig\u001b[39;00m(Config):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_dict, game_config):\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/configs/agent_configs/agent_configs/base_config.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_activations\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfigBase\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.utils'"
     ]
    }
   ],
   "source": [
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "config = RainbowConfig(config, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_anytrading\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('forex-v0')\n",
    "# env = gym.make('stocks-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "custom_env = gym.make(\n",
    "    'forex-v0',\n",
    "    df=FOREX_EURUSD_1H_ASK,\n",
    "    window_size=10,\n",
    "    frame_bound=(10, 300),\n",
    "    unit_side='right'\n",
    ")\n",
    "\n",
    "# custom_env = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=STOCKS_GOOGL,\n",
    "#     window_size=10,\n",
    "#     frame_bound=(10, 300)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env.unwrapped.shape)\n",
    "print(\"> df.shape:\", env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env.unwrapped.max_possible_profit())\n",
    "\n",
    "print()\n",
    "print(\"custom_env information:\")\n",
    "print(\"> shape:\", custom_env.unwrapped.shape)\n",
    "print(\"> df.shape:\", custom_env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", custom_env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", custom_env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", custom_env.unwrapped.max_possible_profit())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "env = custom_env\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "config_dict = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"loss_function\": tf.keras.losses.KLDivergence(),\n",
    "    \"learning_rate\": 0.000001,\n",
    "}\n",
    "config = RainbowConfig(config_dict, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
