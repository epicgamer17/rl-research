{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import prepare_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_plots = list(map(list, zip(*[1, 2, 3, 4])))\n",
    "print(len(score_plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.empty((1, 1), dtype=np.int32)\n",
    "print(arr)  \n",
    "arr = np.append(arr, [[1, 1, 1],[1, 1, 1],[1, 1, 1]], axis=0)\n",
    "print(arr)\n",
    "arr = np.append(arr, [[1, 1, 1],[1, 1, 1],[1, 1, 1]], axis=0)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "import random\n",
    "env = gym.make('custom_gym_envs/MississippiMarbles-v0', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = random.choice(info['legal_moves'])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym_envs\n",
    "# import gymnasium as gym\n",
    "# env = gym.make('gym_envs/TicTacToe-v0')\n",
    "\n",
    "# state, info = env.reset()\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(0)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(4)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(3)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(6)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(2)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(1)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(7)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(8)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(5)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# print(\"Truncated:\", truncated)\n",
    "# env.render()\n",
    "\n",
    "\n",
    "# env.reset()\n",
    "# state, reward, terminated, truncated, info = env.step(0)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(3)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(7)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(4)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(2)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(6)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(1)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# print(\"Truncated:\", truncated)\n",
    "# print(\"Reward:\", reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeZeroToOne(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_high = self.env.observation_space.high\n",
    "        self.observation_low = self.env.observation_space.low\n",
    "\n",
    "    def observation(self, obs):\n",
    "        print(obs)\n",
    "        print((obs - self.observation_low) / (self.observation_high - self.observation_low))\n",
    "        return (obs - self.observation_low) / (self.observation_high - self.observation_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online with frame stackign though\n",
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online\n",
    "env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 00:55:13.415707: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-05-22 00:55:13.415732: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-22 00:55:13.415737: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-22 00:55:13.415770: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-22 00:55:13.415789: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_space():\n",
    "    search_space = {\n",
    "        \"activation\": hp.choice(\n",
    "            \"activation\",\n",
    "            [\n",
    "                \"linear\",\n",
    "                \"relu\",\n",
    "                # 'relu6',\n",
    "                \"sigmoid\",\n",
    "                \"softplus\",\n",
    "                \"soft_sign\",\n",
    "                \"silu\",\n",
    "                \"swish\",\n",
    "                \"log_sigmoid\",\n",
    "                \"hard_sigmoid\",\n",
    "                # 'hard_silu',\n",
    "                # 'hard_swish',\n",
    "                # 'hard_tanh',\n",
    "                \"elu\",\n",
    "                # 'celu',\n",
    "                \"selu\",\n",
    "                \"gelu\",\n",
    "                # 'glu'\n",
    "            ],\n",
    "        ),\n",
    "        \"kernel_initializer\": hp.choice(\n",
    "            \"kernel_initializer\",\n",
    "            [\n",
    "                \"he_uniform\",\n",
    "                \"he_normal\",\n",
    "                \"glorot_uniform\",\n",
    "                \"glorot_normal\",\n",
    "                \"lecun_uniform\",\n",
    "                \"lecun_normal\",\n",
    "                \"orthogonal\",\n",
    "                \"variance_baseline\",\n",
    "                \"variance_0.1\",\n",
    "                \"variance_0.3\",\n",
    "                \"variance_0.8\",\n",
    "                \"variance_3\",\n",
    "                \"variance_5\",\n",
    "                \"variance_10\",\n",
    "            ],\n",
    "        ),\n",
    "        \"optimizer\": hp.choice(\n",
    "            \"optimizer\", [tf.keras.optimizers.legacy.Adam]\n",
    "        ),  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "        \"learning_rate\": hp.choice(\n",
    "            \"learning_rate\", [10, 5, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "        ),  #\n",
    "        \"adam_epsilon\": hp.choice(\n",
    "            \"adam_epsilon\",\n",
    "            [1, 0.5, 0.3125, 0.03125, 0.003125, 0.0003125, 0.00003125, 0.000003125],\n",
    "        ),\n",
    "        \"clipnorm\": hp.choice(\"clipnorm\", [None]),\n",
    "        # NORMALIZATION?\n",
    "        \"soft_update\": hp.choice(\n",
    "            \"soft_update\", [False]\n",
    "        ),  # seems to always be false, we can try it with tru\n",
    "        \"ema_beta\": hp.uniform(\"ema_beta\", 0.95, 0.999),\n",
    "        \"transfer_interval\": hp.choice(\n",
    "            \"transfer_interval\", [10, 25, 50, 100, 200, 400, 800, 1600, 2000]\n",
    "        ),\n",
    "        \"replay_interval\": hp.choice(\"replay_interval\", [1, 2, 3, 4, 5, 8, 10, 12, 350]),\n",
    "        \"minibatch_size\": hp.choice(\n",
    "            \"minibatch_size\", [2**i for i in range(0, 8)]\n",
    "        ),  ###########\n",
    "        \"replay_buffer_size\": hp.choice(\n",
    "            \"replay_buffer_size\", [2000, 3000, 5000, 7500, 10000, 15000, 20000, 25000, 50000]\n",
    "        ),  #############\n",
    "        \"min_replay_buffer_size\": hp.choice(\n",
    "            \"min_replay_buffer_size\", [0, 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000]\n",
    "        ),  # 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000\n",
    "        \"n_step\": hp.choice(\"n_step\", [1, 2, 3, 4, 5, 8, 10]),\n",
    "        \"discount_factor\": hp.choice(\n",
    "            \"discount_factor\", [0.1, 0.5, 0.9, 0.99, 0.995, 0.999]\n",
    "        ),\n",
    "        \"atom_size\": hp.choice(\"atom_size\", [11, 21, 31, 41, 51, 61, 71, 81]),  #\n",
    "        \"conv_layers\": hp.choice(\"conv_layers\", [[], [(32, 8, 4), (64, 4, 2), (64, 3, 1)]]),\n",
    "        \"conv_layers_noisy\": hp.choice(\"conv_layers_noisy\", [False]),\n",
    "        \"width\": hp.choice(\"width\", [32, 64, 128, 256, 512, 1024]),\n",
    "        \"dense_layers\": hp.choice(\"dense_layers\", [0, 1, 2, 3, 4]),\n",
    "        \"dense_layers_noisy\": hp.choice(\n",
    "            \"dense_layers_noisy\", [True]\n",
    "        ),  # i think this is always true for rainbow\n",
    "        # REWARD CLIPPING\n",
    "        \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.5]),  #\n",
    "        \"loss_function\": hp.choice(\n",
    "            \"loss_function\",\n",
    "            [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()],\n",
    "        ),\n",
    "        \"dueling\": hp.choice(\"dueling\", [True]),\n",
    "        \"advantage_hidden_layers\": hp.choice(\n",
    "            \"advantage_hidden_layers\", [0, 1, 2, 3, 4]\n",
    "        ),  #\n",
    "        \"value_hidden_layers\": hp.choice(\"value_hidden_layers\", [0, 1, 2, 3, 4]),  #\n",
    "        \"training_steps\": hp.choice(\"training_steps\", [30000]),\n",
    "        \"per_epsilon\": hp.choice(\n",
    "            \"per_epsilon\", [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "        ),\n",
    "        \"per_alpha\": hp.choice(\"per_alpha\", [0.05 * i for i in range(0, 21)]),\n",
    "        \"per_beta\": hp.choice(\"per_beta\", [0.05 * i for i in range(1, 21)]),\n",
    "        # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        # 'search_max_depth': 5,\n",
    "        # 'search_max_time': 10,\n",
    "        \"training_iterations\": hp.choice(\"training_iterations\", [1, 2, 3, 4, 5]),\n",
    "        \"num_minibatches\": hp.choice(\"num_minibatches\", [1, 2, 3, 4, 5]),\n",
    "    }\n",
    "    initial_best_config = [\n",
    "        {\n",
    "            \"activation\": 1,\n",
    "            \"kernel_initializer\": 6,\n",
    "            \"optimizer\": 0,  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "            \"learning_rate\": 5,  #\n",
    "            \"adam_epsilon\": 5,\n",
    "            \"clipnorm\": 0,\n",
    "            # NORMALIZATION?\n",
    "            \"soft_update\": 0,  # seems to always be false, we can try it with tru\n",
    "            \"ema_beta\": 0.95,\n",
    "            \"transfer_interval\": 3,\n",
    "            \"replay_interval\": 1,\n",
    "            \"minibatch_size\": 7,\n",
    "            \"replay_buffer_size\": 8,  \n",
    "            \"min_replay_buffer_size\": 4,\n",
    "            \"n_step\": 2,\n",
    "            \"discount_factor\": 3,\n",
    "            \"atom_size\": 4,  #\n",
    "            \"conv_layers\": 0,\n",
    "            \"conv_layers_noisy\": 0,\n",
    "            \"width\": 4,\n",
    "            \"dense_layers\": 2,\n",
    "            \"dense_layers_noisy\": 0,  # i think this is always true for rainbow\n",
    "            # REWARD CLIPPING\n",
    "            \"noisy_sigma\": 0,  #\n",
    "            \"loss_function\": 0,\n",
    "            \"dueling\": 0,\n",
    "            \"advantage_hidden_layers\": 0,  #\n",
    "            \"value_hidden_layers\": 0,  #\n",
    "            \"training_steps\": 0,\n",
    "            \"per_epsilon\": 3,\n",
    "            \"per_alpha\": 10,\n",
    "            \"per_beta\": 7,\n",
    "            # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "            # 'search_max_depth': 5,\n",
    "            # 'search_max_time': 10,\n",
    "            \"training_iterations\": 0,\n",
    "            \"num_minibatches\": 0,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return search_space, initial_best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'adam_epsilon': 0.0003125, 'advantage_hidden_layers': 0, 'atom_size': 51, 'clipnorm': None, 'conv_layers': (), 'conv_layers_noisy': False, 'dense_layers': 2, 'dense_layers_noisy': True, 'discount_factor': 0.99, 'dueling': True, 'ema_beta': 0.95, 'kernel_initializer': 'orthogonal', 'learning_rate': 0.01, 'loss_function': <keras.src.losses.CategoricalCrossentropy object at 0x29ea28370>, 'min_replay_buffer_size': 500, 'minibatch_size': 128, 'n_step': 3, 'noisy_sigma': 0.5, 'num_minibatches': 1, 'optimizer': <class 'keras.src.optimizers.legacy.adam.Adam'>, 'per_alpha': 0.5, 'per_beta': 0.4, 'per_epsilon': 0.001, 'replay_buffer_size': 50000, 'replay_interval': 2, 'soft_update': False, 'training_iterations': 1, 'training_steps': 30000, 'transfer_interval': 100, 'value_hidden_layers': 0, 'width': 512}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import space_eval\n",
    "\n",
    "search_sapce, initial_best_config = create_search_space()\n",
    "config = space_eval(search_sapce, initial_best_config[0])\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights: True\n",
      "Using adam_epsilon: 0.0003125\n",
      "Using learning_rate: 0.01\n",
      "Using clipnorm: None\n",
      "Using optimizer: <class 'keras.src.optimizers.legacy.adam.Adam'>\n",
      "Using loss_function: <keras.src.losses.CategoricalCrossentropy object at 0x29ea28370>\n",
      "Using training_iterations: 1\n",
      "Using num_minibatches: 1\n",
      "Using minibatch_size: 128\n",
      "Using replay_buffer_size: 50000\n",
      "Using min_replay_buffer_size: 500\n",
      "Using training_steps: 30000\n",
      "Using activation: relu\n",
      "Using kernel_initializer: orthogonal\n",
      "Using width: 512\n",
      "Using noisy_sigma: 0.5\n",
      "Using conv_layers: ()\n",
      "Using conv_layers_noisy: False\n",
      "Using dense_layers: 2\n",
      "Using dense_layers_noisy: True\n",
      "Using value_hidden_layers: 0\n",
      "Using advantage_hidden_layers: 0\n",
      "Using discount_factor: 0.99\n",
      "Using soft_update: False\n",
      "Using transfer_interval: 100\n",
      "Using ema_beta: 0.95\n",
      "Using replay_interval: 2\n",
      "Using per_alpha: 0.5\n",
      "Using per_beta: 0.4\n",
      "Using per_epsilon: 0.001\n",
      "Using n_step: 3\n",
      "Using atom_size: 51\n",
      "observation_dimensions:  (4,)\n",
      "num_actions:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/rainbow/videos/RainbowDQN-CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  163.0\n"
     ]
    }
   ],
   "source": [
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "config = RainbowConfig(config, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.checkpoint_interval = 10\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_anytrading\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('forex-v0')\n",
    "# env = gym.make('stocks-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "custom_env = gym.make(\n",
    "    'forex-v0',\n",
    "    df=FOREX_EURUSD_1H_ASK,\n",
    "    window_size=10,\n",
    "    frame_bound=(10, 300),\n",
    "    unit_side='right'\n",
    ")\n",
    "\n",
    "# custom_env = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=STOCKS_GOOGL,\n",
    "#     window_size=10,\n",
    "#     frame_bound=(10, 300)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env.unwrapped.shape)\n",
    "print(\"> df.shape:\", env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env.unwrapped.max_possible_profit())\n",
    "\n",
    "print()\n",
    "print(\"custom_env information:\")\n",
    "print(\"> shape:\", custom_env.unwrapped.shape)\n",
    "print(\"> df.shape:\", custom_env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", custom_env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", custom_env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", custom_env.unwrapped.max_possible_profit())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "env = custom_env\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "config_dict = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"loss_function\": tf.keras.losses.KLDivergence(),\n",
    "    \"learning_rate\": 0.000001,\n",
    "}\n",
    "config = RainbowConfig(config_dict, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
