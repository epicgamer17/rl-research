{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     ActionMaskInInfoWrapper,\n\u001b[1;32m      5\u001b[0m     ChannelLastToFirstWrapper,\n\u001b[1;32m      6\u001b[0m     FrameStackWrapper,\n\u001b[1;32m      7\u001b[0m     TwoPlayerPlayerPlaneWrapper,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magent_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MuZeroConfig\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wrappers'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [0, 500],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [512, 64],\n",
    "    \"dynamics_dense_layer_widths\": [512, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [512],  # ???\n",
    "    \"critic_dense_layer_widths\": [512],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 50,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.5, 0.25],\n",
    "    \"temperature_updates\": [30000, 60000],\n",
    "    \"temperature_with_training_steps\": True,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.005,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"discount_factor\": 0.997,\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 50000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 10,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    # \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": 31,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = CartPoleConfig().make_env()\n",
    "game_config = CartPoleConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_cartpole\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(24, 3, 1)] * 1,  # increase num layers\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layers\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # try larger\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [8],  # change this\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # slightly increase this, maybe 0.002 or 0.003\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,  # try lower\n",
    "    \"value_loss_function\": MSELoss(),  #  MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),  # MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"min_replay_buffer_size\": 4000,  # try lower (or just different)\n",
    "    \"replay_buffer_size\": 100000,  # try lower, 50k or 20k or 10k\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"glorot_normal\",  # try different\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,  # 0.0 was original\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,  # None\n",
    "}\n",
    "\n",
    "# REANALYZE NOT IMPLEMENTED BUT NEVER USED IN THING I SAW ONLINE (like it is computed but never ends up used since bootstrap index always > than len of game history)\n",
    "\n",
    "# add a max depth to the tree\n",
    "# game priority is max of position priorities of the game\n",
    "# with these two changes should be an exact match with online github implementation\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-hyperopt-best\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 50\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 300\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [256, 64],\n",
    "    \"dynamics_dense_layer_widths\": [256, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [256],  # ???\n",
    "    \"critic_dense_layer_widths\": [256],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.002,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 20000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    # \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-dense-5moves_1\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tested the ones i found on the github and they worked good? but i think i am gonna do less workers because they did 480k episodes after 32k training steps, i got 250k after like 5k steps, so i want to change that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"critic_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 1.8,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [2],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": SGD,\n",
    "    \"learning_rate\": 0.1,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 4000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 10000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 1.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 2 moves left one losing one drawing, it finds the drawing move, but evaluates the winning position after tree search as 0 instead of 1 (though predicts it as 1). shown in paper 2m-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from packages.utils.utils.utils import process_petting_zoo_obs\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get action from average strategy\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "\n",
    "random_vs_expert_table = StandingsTable([agent, TicTacToeBestAgent()], start_elo=1400)\n",
    "random_vs_expert_table.play_1v1_tournament(1000, play_game)\n",
    "print(random_vs_expert_table.bayes_elo())\n",
    "print(random_vs_expert_table.get_win_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToeBestAgent:\n",
    "    def __init__(self, model_name=\"tictactoe_expert\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def predict(self, observation, info, env=None):\n",
    "        return observation, info\n",
    "\n",
    "    def select_actions(self, prediction, info):\n",
    "        # Reconstruct board: +1 for current player, -1 for opponent, 0 otherwise\n",
    "        board = prediction[0][0] - prediction[0][1]\n",
    "        print(board)\n",
    "        # Default: random legal move\n",
    "        action = np.random.choice(info[\"legal_moves\"])\n",
    "\n",
    "        # Horizontal and vertical checks\n",
    "        for i in range(3):\n",
    "            # Row\n",
    "            if np.sum(board[i, :]) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                return np.ravel_multi_index((i, ind), (3, 3))\n",
    "            elif abs(np.sum(board[i, :])) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((i, ind), (3, 3))\n",
    "\n",
    "            # Column\n",
    "            if np.sum(board[:, i]) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                return np.ravel_multi_index((ind, i), (3, 3))\n",
    "            elif abs(np.sum(board[:, i])) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((ind, i), (3, 3))\n",
    "\n",
    "        # Diagonals\n",
    "        diag = board.diagonal()\n",
    "        if np.sum(diag) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, ind), (3, 3))\n",
    "        elif abs(np.sum(diag)) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, ind), (3, 3))\n",
    "\n",
    "        anti_diag = np.fliplr(board).diagonal()\n",
    "        if np.sum(anti_diag) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "        elif abs(np.sum(anti_diag)) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 2.0,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 33000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 40000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt-2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],  # RandomAgent(),\n",
    ")\n",
    "\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 500\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.training_steps += 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    "    FrameStackWrapper,\n",
    ")\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig\n",
    "\n",
    "# from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# from agents.random import RandomAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "best_agent = TicTacToeBestAgent()\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "env.reset()\n",
    "env.step(0)\n",
    "env.step(6)\n",
    "env.step(2)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "\n",
    "selected_actions = {i: 0 for i in range(agent.num_actions)}\n",
    "for i in range(100):\n",
    "    selected_actions[agent.select_actions(prediction).item()] += 1\n",
    "print(selected_actions)\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "# env.step(action)\n",
    "# state, reward, terminated, truncated, info = env.last()\n",
    "# prediction = agent.predict(state, info, env)\n",
    "# print(\"MCTS Prediction\", prediction)\n",
    "# initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "# print(\"Initial Value\", initial_inference[0])\n",
    "# print(\"Initial Policy\", initial_inference[1])\n",
    "# action = agent.select_actions(prediction).item()\n",
    "# print(\"Action\", action)\n",
    "# env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from math import log, sqrt, inf\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior_policy):\n",
    "        self.visits = 0\n",
    "        self.to_play = -1\n",
    "        self.prior_policy = prior_policy\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expand(self, legal_moves, to_play, policy, hidden_state, reward):\n",
    "        self.to_play = to_play\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        # print(legal_moves)\n",
    "        policy = {a: policy[a] for a in legal_moves}\n",
    "        policy_sum = sum(policy.values())\n",
    "\n",
    "        for action, p in policy.items():\n",
    "            self.children[action] = Node((p / (policy_sum + 1e-10)).item())\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visits == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visits\n",
    "\n",
    "    def add_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior_policy = (1 - frac) * self.children[\n",
    "                a\n",
    "            ].prior_policy + frac * n\n",
    "\n",
    "    def select_child(self, min_max_stats, pb_c_base, pb_c_init, discount, num_players):\n",
    "        # Select the child with the highest UCB\n",
    "        child_ucbs = [\n",
    "            self.child_ucb_score(\n",
    "                child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "            )\n",
    "            for action, child in self.children.items()\n",
    "        ]\n",
    "        print(\"Child UCBs\", child_ucbs)\n",
    "        action_index = np.random.choice(\n",
    "            np.where(np.isclose(child_ucbs, max(child_ucbs)))[0]\n",
    "        )\n",
    "        action = list(self.children.keys())[action_index]\n",
    "        return action, self.children[action]\n",
    "\n",
    "    def child_ucb_score(\n",
    "        self, child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "    ):\n",
    "        pb_c = log((self.visits + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
    "        pb_c *= sqrt(self.visits) / (child.visits + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior_policy\n",
    "        if child.visits > 0:\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward\n",
    "                + discount\n",
    "                * (\n",
    "                    child.value() if num_players == 1 else -child.value()\n",
    "                )  # (or if on the same team)\n",
    "            )\n",
    "        else:\n",
    "            value_score = 0.0\n",
    "\n",
    "        # check if value_score is nan\n",
    "        assert (\n",
    "            value_score == value_score\n",
    "        ), \"value_score is nan, child value is {}, and reward is {},\".format(\n",
    "            child.value(),\n",
    "            child.reward,\n",
    "        )\n",
    "        assert prior_score == prior_score, \"prior_score is nan\"\n",
    "        print(\"Prior Score\", prior_score)\n",
    "        print(\"Value Score\", value_score)\n",
    "        return prior_score + value_score\n",
    "        # return value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Optional\n",
    "\n",
    "from pyparsing import List\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float(\"inf\")\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "    def __init__(\n",
    "        self, known_bounds: Optional[List[float]]\n",
    "    ):  # might need to say known_bounds=None\n",
    "        self.max = known_bounds[1] if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "        self.min = known_bounds[0] if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.max = max(self.max, value)\n",
    "        self.min = min(self.min, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        print(\"Initial value\", value)\n",
    "        if self.max > self.min:\n",
    "            # We normalize only when we have at a max and min value\n",
    "            print(\"normalized value\", (value - self.min) / (self.max - self.min))\n",
    "            return (value - self.min) / (self.max - self.min)\n",
    "        return value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"min: {self.min}, max: {self.max}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_legal_moves\n",
    "\n",
    "# from muzero.muzero_mcts import Node\n",
    "# from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "root = Node(0.0)\n",
    "_, policy, hidden_state = agent.predict_single_initial_inference(\n",
    "    state,\n",
    "    info,\n",
    ")\n",
    "print(\"root policy\", policy)\n",
    "legal_moves = get_legal_moves(info)[0]\n",
    "to_play = env.agents.index(env.agent_selection)\n",
    "root.expand(legal_moves, to_play, policy, hidden_state, 0.0)\n",
    "print(\"expanded root\")\n",
    "min_max_stats = MinMaxStats(agent.config.known_bounds)\n",
    "\n",
    "for _ in range(agent.config.num_simulations):\n",
    "    print(\"at root\")\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "    to_play = env.agents.index(env.agent_selection)\n",
    "\n",
    "    # GO UNTIL A LEAF NODE IS REACHED\n",
    "    while node.expanded():\n",
    "        print(\"selecting child\")\n",
    "        action, node = node.select_child(\n",
    "            min_max_stats,\n",
    "            agent.config.pb_c_base,\n",
    "            agent.config.pb_c_init,\n",
    "            agent.config.discount_factor,\n",
    "            agent.config.game.num_players,\n",
    "        )\n",
    "        print(\"Selected action\", action)\n",
    "        # THIS NEEDS TO BE CHANGED FOR GAMES WHERE PLAYER COUNT DECREASES AS PLAYERS GET ELIMINATED, USE agent_selector.next() (clone of the current one)\n",
    "        to_play = (to_play + 1) % agent.config.game.num_players\n",
    "        search_path.append(node)\n",
    "    parent = search_path[-2]\n",
    "    reward, hidden_state, value, policy = agent.predict_single_recurrent_inference(\n",
    "        parent.hidden_state,\n",
    "        action,  # model=model\n",
    "    )\n",
    "    reward = reward.item()\n",
    "    value = value.item()\n",
    "    print(\"leaf value\", value)\n",
    "    print(\"leaf reward\", reward)\n",
    "\n",
    "    node.expand(\n",
    "        list(range(agent.num_actions)),\n",
    "        to_play,\n",
    "        policy,\n",
    "        hidden_state,\n",
    "        (\n",
    "            reward  # if self.config.game.has_intermediate_rewards else 0.0\n",
    "        ),  # for board games and games with no intermediate rewards\n",
    "    )\n",
    "\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "        min_max_stats.update(\n",
    "            node.reward\n",
    "            + agent.config.discount_factor\n",
    "            * (node.value() if agent.config.game.num_players == 1 else -node.value())\n",
    "        )\n",
    "        value = (\n",
    "            -node.reward\n",
    "            if node.to_play == to_play and agent.config.game.num_players > 1\n",
    "            else node.reward\n",
    "        ) + agent.config.discount_factor * value\n",
    "\n",
    "    visit_counts = [(child.visits, action) for action, child in root.children.items()]\n",
    "\n",
    "print(visit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf value 1.0\n",
      "leaf reward 0.0\n",
      "leaf to_play 0\n",
      "leaf value 1.0\n",
      "leaf reward 0.0\n",
      "leaf to_play 0\n",
      "<muzero.muzero_mcts.Node object at 0x12ff21ae0>\n",
      "node 1 to_play 0\n",
      "init value sum 1 0\n",
      "new value sum 1 1.0\n",
      "min max update 1 1.0\n",
      "<muzero.muzero_mcts.Node object at 0x12ff204f0>\n",
      "node to_play 0\n",
      "init value sum 0\n",
      "new value sum 1.0\n",
      "min max update 2 1.0\n",
      "True\n",
      "True\n",
      "True\n",
      "next value to be added 1.0\n",
      "method 1 value 1.0\n",
      "next value to be added 1.0\n",
      "method 2 value 1.0\n",
      "True\n",
      "<muzero.muzero_mcts.Node object at 0x12ff23e20>\n",
      "node 1 to_play 1\n",
      "init value sum 1 0\n",
      "new value sum 1 -1.0\n",
      "min max update 1 2.0\n",
      "<muzero.muzero_mcts.Node object at 0x12ff207f0>\n",
      "node to_play 1\n",
      "init value sum 0\n",
      "new value sum -1.0\n",
      "min max update 2 2.0\n",
      "True\n",
      "True\n",
      "True\n",
      "next value to be added 2.0\n",
      "method 1 value -1.0\n",
      "next value to be added 2.0\n",
      "method 2 value -1.0\n",
      "True\n",
      "<muzero.muzero_mcts.Node object at 0x12ff21060>\n",
      "node 1 to_play 1\n",
      "init value sum 1 0\n",
      "new value sum 1 -2.0\n",
      "min max update 1 2.0\n",
      "<muzero.muzero_mcts.Node object at 0x12ff23340>\n",
      "node to_play 1\n",
      "init value sum 0\n",
      "new value sum -2.0\n",
      "min max update 2 2.0\n",
      "True\n",
      "True\n",
      "True\n",
      "next value to be added 2.0\n",
      "method 1 value -2.0\n",
      "next value to be added 2.0\n",
      "method 2 value -2.0\n",
      "True\n",
      "<muzero.muzero_mcts.Node object at 0x12ff23220>\n",
      "node 1 to_play 0\n",
      "init value sum 1 0\n",
      "new value sum 1 2.0\n",
      "min max update 1 2.0\n",
      "<muzero.muzero_mcts.Node object at 0x12ff21ab0>\n",
      "node to_play 0\n",
      "init value sum 0\n",
      "new value sum 2.0\n",
      "min max update 2 2.0\n",
      "True\n",
      "True\n",
      "True\n",
      "next value to be added 2.0\n",
      "method 1 value 2.0\n",
      "next value to be added 2.0\n",
      "method 2 value 2.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "import torch\n",
    "\n",
    "num_players = 2\n",
    "min_max_stats = MinMaxStats([-1, 1])\n",
    "\n",
    "\n",
    "def make_search_path():\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "    root.expand(legal_moves, 0, policy, hidden_state, 0.0)\n",
    "\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "    search_path.append(node)\n",
    "    node.expand(legal_moves, 1, policy, hidden_state, 0.0)\n",
    "    node = node.children[0]\n",
    "    search_path.append(node)\n",
    "    node.expand(legal_moves, 1, policy, hidden_state, 1.0)\n",
    "    node = node.children[0]\n",
    "    search_path.append(node)\n",
    "\n",
    "    value = 0.0\n",
    "    reward = 0.0\n",
    "    to_play = 0\n",
    "    node.expand(legal_moves, to_play, policy, hidden_state, reward)\n",
    "    print(\"leaf value\", value)\n",
    "    print(\"leaf reward\", reward)\n",
    "    print(\"leaf to_play\", to_play)\n",
    "\n",
    "    return search_path, to_play, value\n",
    "\n",
    "\n",
    "search_path_1, to_play_1, value_1 = make_search_path()\n",
    "search_path_2, to_play_2, value_2 = make_search_path()\n",
    "\n",
    "for _ in range(1):\n",
    "    for node_1, node_2 in zip(reversed(search_path_1), reversed(search_path_2)):\n",
    "        print(node_1)\n",
    "        print(\"node 1 to_play\", node_1.to_play)\n",
    "        print(\"init value sum 1\", node_1.value_sum)\n",
    "        node_1.value_sum += value_1 if node_1.to_play == to_play_1 else -value_1\n",
    "        print(\"new value sum 1\", node_1.value_sum)\n",
    "        node_1.visits += 1\n",
    "        min_max_update_1 = node_1.reward + 1.0 * (\n",
    "            node_1.value() if node_1.to_play == to_play_1 else -node_1.value()\n",
    "        )\n",
    "        print(\"min max update 1\", min_max_update_1)\n",
    "        min_max_stats.update(min_max_update_1)\n",
    "\n",
    "        print(node_2)\n",
    "        print(\"node to_play\", node_2.to_play)\n",
    "        print(\"init value sum\", node_2.value_sum)\n",
    "        node_2.value_sum += value_2 if node_2.to_play == to_play_2 else -value_2\n",
    "        print(\"new value sum\", node_2.value_sum)\n",
    "        node_2.visits += 1\n",
    "        if num_players == 1:\n",
    "            min_max_update_2 = node_2.reward + 1.0 * node_2.value()\n",
    "        elif node_2.to_play == to_play_2:\n",
    "            min_max_update_2 = node_2.reward + 1.0 * node_2.value()\n",
    "        else:\n",
    "            min_max_update_2 = node_2.reward + 1.0 * (-node_2.value())\n",
    "        print(\"min max update 2\", min_max_update_2)\n",
    "        min_max_stats.update(min_max_update_2)\n",
    "\n",
    "        print(node_1.value_sum == node_2.value_sum)\n",
    "        print(node_1.visits == node_2.visits)\n",
    "        print(min_max_update_1 == min_max_update_2)\n",
    "\n",
    "        #  METHOD 1 (baseline)\n",
    "        if node_1.to_play == to_play_1 and num_players > 1:\n",
    "            value_1 = -node_1.reward + 1.0 * value_1\n",
    "        else:\n",
    "            value_1 = node_1.reward + 1.0 * value_1\n",
    "\n",
    "        print(\"next value to be added\", value_1)\n",
    "        print(\"method 1 value\", node_1.value())\n",
    "\n",
    "        #  METHOD 2 (generalized version)\n",
    "        if num_players == 1:\n",
    "            value_2 = node_2.reward + 1.0 * value_2\n",
    "        elif node_2.to_play == to_play_2:\n",
    "            value_2 = -node_2.reward + 1.0 * value_2\n",
    "        else:\n",
    "            value_2 = node_2.reward + 1.0 * value_2\n",
    "        print(\"next value to be added\", value_2)\n",
    "        print(\"method 2 value\", node_2.value())\n",
    "\n",
    "        print(value_1 == value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MuZero Value Backpropagation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Test Case 1: 2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\n",
      "Path config: [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 2: 2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 3: 2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\n",
      "Path config: [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-2.0, 2.0, 1.0, 0.0]\n",
      "Method 1 result: [0.0, 0.0, 1.0, 0.0] \n",
      "Method 2 result: [-2.0, 2.0, 1.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 4: 2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-2.0, 2.0, 1.0, 0.0]\n",
      "Method 1 result: [2.0, -2.0, -1.0, 0.0] \n",
      "Method 2 result: [-2.0, 2.0, 1.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 5: 2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\n",
      "Path config: [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [0.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [2.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [0.0, 1.0, 0.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 6: 2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\n",
      "Path config: [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [0.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [2.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [0.0, 1.0, 0.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 7: 2-player: alternating game, player 1 wins on there first move\n",
      "Path config: [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 8: 2-player: alternating game, player 1 wins on there first move\n",
      "Path config: [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 9: 2-player: alternating game, player 0 wins\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [1.0, -1.0, 1.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 1.0, 0.0] \n",
      "Method 2 result: [1.0, -1.0, 1.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 10: 2-player: alternating game, player 1 wins\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, -1.0, 1.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, -1.0, 1.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, -1.0, 1.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 11: 2-player: alternating game with a leaf value\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, -1.0, 1.0]\n",
      "Method 1 result: [-1.0, 1.0, -1.0, 1.0] \n",
      "Method 2 result: [-1.0, 1.0, -1.0, 1.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 12: 2-player: alternating game with a leaf value\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)]\n",
      "Num players: 2\n",
      "Expected node values: [1.0, -1.0, 1.0, -1.0, 1.0]\n",
      "Method 1 result: [1.0, -1.0, 1.0, -1.0, 1.0] \n",
      "Method 2 result: [1.0, -1.0, 1.0, -1.0, 1.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 13: 2-player: All player 0 turns\n",
      "Path config: [(0, 1.0), (0, 1.0), (0, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [3.0, 2.0, 1.0, 0.0]\n",
      "Method 1 result: [-3.0, -2.0, -1.0, 0.0] \n",
      "Method 2 result: [3.0, 2.0, 1.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 14: 2-player: All player 0 turns with leaf value\n",
      "Path config: [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [4.0, 4.0, 4.0, 4.0]\n",
      "Method 1 result: [4.0, 4.0, 4.0, 4.0] \n",
      "Method 2 result: [4.0, 4.0, 4.0, 4.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 15: 2-player: Two player 1 turns with leaf value\n",
      "Path config: [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [3.0, -3.0, -4.0, 4.0]\n",
      "Method 1 result: [5.0, -5.0, -4.0, 4.0] \n",
      "Method 2 result: [3.0, -3.0, -4.0, 4.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 16: 2-player: Two player 1 turns with leaf value\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-5.0, 5.0, 4.0, 4.0]\n",
      "Method 1 result: [-3.0, 3.0, 4.0, 4.0] \n",
      "Method 2 result: [-5.0, 5.0, 4.0, 4.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 17: 2-player: Two player 1 turns with leaf value\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [3.0, -3.0, -4.0, -4.0, 4.0]\n",
      "Method 1 result: [5.0, -5.0, -4.0, -4.0, 4.0] \n",
      "Method 2 result: [3.0, -3.0, -4.0, -4.0, 4.0] \n",
      "Methods match: \n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 18: 1-player: All rewards sum up\n",
      "Path config: [(0, 1.0), (0, 2.0), (0, 3.0, 0.0)]\n",
      "Num players: 1\n",
      "Expected node values: [6.0, 5.0, 3.0, 0.0]\n",
      "Method 1 result: [6.0, 5.0, 3.0, 0.0] \n",
      "Method 2 result: [6.0, 5.0, 3.0, 0.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "Test Case 19: 1-player: Rewards + leaf value\n",
      "Path config: [(0, 1.0), (0, 0.0), (0, 0.0, 5.0)]\n",
      "Num players: 1\n",
      "Expected node values: [6.0, 5.0, 5.0, 5.0]\n",
      "Method 1 result: [6.0, 5.0, 5.0, 5.0] \n",
      "Method 2 result: [6.0, 5.0, 5.0, 5.0] \n",
      "Methods match: \n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      "\n",
      "================================================================================\n",
      " All tests passed!\n",
      "Method 1 got 9\n",
      "Method 2 got 19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "def make_search_path(path_config):\n",
    "    \"\"\"\n",
    "    path_config is a list of tuples: (to_play, reward)\n",
    "    Last element has the leaf value and to_play\n",
    "    \"\"\"\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "\n",
    "    # Initialize root (player 0)\n",
    "    root.expand(legal_moves, 0, policy, hidden_state, 0.0)\n",
    "\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "\n",
    "    # Build path according to config\n",
    "    for i, (to_play, reward) in enumerate(path_config[:-1]):\n",
    "        search_path.append(node)\n",
    "        node.expand(legal_moves, to_play, policy, hidden_state, reward)\n",
    "        node = node.children[0]\n",
    "\n",
    "    # Last node\n",
    "    search_path.append(node)\n",
    "    last_config = path_config[-1]\n",
    "    leaf_to_play = last_config[0]\n",
    "    leaf_reward = last_config[1]\n",
    "    leaf_value = last_config[2] if len(last_config) > 2 else 0.0\n",
    "    node.expand(legal_moves, leaf_to_play, policy, hidden_state, leaf_reward)\n",
    "\n",
    "    return search_path, leaf_to_play, leaf_value\n",
    "\n",
    "\n",
    "def backpropagate_method1(search_path, to_play, value, num_players):\n",
    "    \"\"\"Original Method 1\"\"\"\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "\n",
    "        if node.to_play == to_play and num_players > 1:\n",
    "            value = -node.reward + 1.0 * value\n",
    "        else:\n",
    "            value = node.reward + 1.0 * value\n",
    "\n",
    "    return [n.value() for n in search_path]\n",
    "\n",
    "\n",
    "def backpropagate_method2(search_path, to_play, value, num_players):\n",
    "    \"\"\"\n",
    "    Robust method 2: compute, for each node in the search_path, the total return\n",
    "    from that node according to that node's player's perspective, then add it\n",
    "    to node.value_sum and increment visits.\n",
    "\n",
    "    - `to_play` is the leaf player (owner of `value`).\n",
    "    - `value` is the leaf value (scalar).\n",
    "    - reward is stored on nodes so that search_path[j].reward is the reward\n",
    "      for the parent at j-1 (i.e. it's the reward received when moving into j).\n",
    "    \"\"\"\n",
    "    leaf_to_play = to_play\n",
    "    leaf_value = value\n",
    "\n",
    "    # For each node in the path compute the exact return from that node's perspective.\n",
    "    # Complexity O(n^2) in path length  fine for typical MCTS path lengths.\n",
    "    for i, node in enumerate(search_path):\n",
    "        total = 0.0\n",
    "        # sum future rewards (reward at search_path[j] belongs to acting_player = search_path[j-1].to_play)\n",
    "        for j in range(i + 1, len(search_path)):\n",
    "            acting_player = search_path[j - 1].to_play\n",
    "            r = search_path[j].reward\n",
    "            if acting_player == node.to_play:\n",
    "                total += r\n",
    "            else:\n",
    "                total -= r\n",
    "\n",
    "        # add the leaf value (belongs to leaf_to_play)\n",
    "        if leaf_to_play == node.to_play:\n",
    "            total += leaf_value\n",
    "        else:\n",
    "            total -= leaf_value\n",
    "\n",
    "        # update node stats (value_sum / visits semantics preserved)\n",
    "        node.value_sum += total\n",
    "        node.visits += 1\n",
    "\n",
    "    return [n.value() for n in search_path]\n",
    "\n",
    "\n",
    "# Test cases: (path_config, expected_root_value, num_players, description)\n",
    "test_cases = [\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 1.0, 0.0)],\n",
    "        [1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)],\n",
    "        [1.0, -1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 1.0), (0, 1.0, 0.0)],\n",
    "        [3.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)],\n",
    "        [4.0, 4.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)],\n",
    "        [-5.0, 5.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    # Single player test cases\n",
    "    (\n",
    "        [(0, 1.0), (0, 2.0), (0, 3.0, 0.0)],\n",
    "        [6.0, 5.0, 3.0, 0.0],\n",
    "        1,\n",
    "        \"1-player: All rewards sum up\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 0.0), (0, 0.0, 5.0)],\n",
    "        [6.0, 5.0, 5.0, 5.0],\n",
    "        1,\n",
    "        \"1-player: Rewards + leaf value\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Testing MuZero Value Backpropagation\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def are_lists_roughly_equal(list1, list2):\n",
    "    \"\"\"\n",
    "    Checks if two lists of floats are roughly equal in Python 2.\n",
    "\n",
    "    Args:\n",
    "        list1: The first list of floats.\n",
    "        list2: The second list of floats.\n",
    "        tolerance: The maximum allowed absolute difference between corresponding\n",
    "                   elements for them to be considered roughly equal.\n",
    "\n",
    "    Returns:\n",
    "        True if the lists are roughly equal, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        # Compare elements using absolute tolerance\n",
    "        if not math.isclose(list1[i], list2[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "all_passed = True\n",
    "method1_correct = 0\n",
    "method2_correct = 0\n",
    "total_tests = 0\n",
    "for i, (path_config, expected, num_players, description) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest Case {i}: {description}\")\n",
    "    print(f\"Path config: {path_config}\")\n",
    "    print(f\"Num players: {num_players}\")\n",
    "    print(f\"Expected node values: {expected}\")\n",
    "\n",
    "    # Test Method 1\n",
    "    search_path_1, to_play_1, value_1 = make_search_path(path_config)\n",
    "    result_1 = backpropagate_method1(search_path_1, to_play_1, value_1, num_players)\n",
    "\n",
    "    # Test Method 2\n",
    "    search_path_2, to_play_2, value_2 = make_search_path(path_config)\n",
    "    result_2 = backpropagate_method2(search_path_2, to_play_2, value_2, num_players)\n",
    "\n",
    "    # Check results\n",
    "    method1_pass = are_lists_roughly_equal(result_1, expected)\n",
    "    method2_pass = are_lists_roughly_equal(result_2, expected)\n",
    "    match = are_lists_roughly_equal(result_1, result_2)\n",
    "\n",
    "    print(f\"Method 1 result: {result_1} {'' if method1_pass else ''}\")\n",
    "    print(f\"Method 2 result: {result_2} {'' if method2_pass else ''}\")\n",
    "    print(f\"Methods match: {'' if match else ''}\")\n",
    "    if method1_pass:\n",
    "        method1_correct += 1\n",
    "    if method2_pass:\n",
    "        method2_correct += 1\n",
    "    if not method1_pass:\n",
    "        print(\" METHOD 1 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 1 PASSED\")\n",
    "    if not method2_pass:\n",
    "        print(\" METHOD 2 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 2 PASSED\")\n",
    "\n",
    "    # if not (method1_pass and method2_pass and match):\n",
    "    #     all_passed = False\n",
    "    #     print(\" FAILED\")\n",
    "    # else:\n",
    "    #     print(\" PASSED\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_passed:\n",
    "    print(\" All tests passed!\")\n",
    "else:\n",
    "    print(\" Some tests failed\")\n",
    "\n",
    "print(\"Method 1 got\", method1_correct)\n",
    "print(\"Method 2 got\", method2_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MuZero Value Backpropagation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Test Case 1: 2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\n",
      "Path config: [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Method 3 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 2: 2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Method 3 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 3: 2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\n",
      "Path config: [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-2.0, 2.0, 1.0, 0.0]\n",
      "Method 1 result: [0.0, 0.0, 1.0, 0.0] \n",
      "Method 2 result: [-2.0, 2.0, 1.0, 0.0] \n",
      "Method 3 result: [-2.0, 2.0, 1.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 2.0\n",
      "MinMaxStats mins match:   -1 = -2.0\n",
      "MinMaxStats maxes match:   2.0 = 2.0\n",
      "MinMaxStats mins match:   -2.0 = -2.0\n",
      "MinMaxStats maxes match:   1 = 2.0\n",
      "MinMaxStats mins match:   -1 = -2.0\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 4: 2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-2.0, 2.0, 1.0, 0.0]\n",
      "Method 1 result: [2.0, -2.0, -1.0, 0.0] \n",
      "Method 2 result: [-2.0, 2.0, 1.0, 0.0] \n",
      "Method 3 result: [-2.0, 2.0, 1.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   2.0 = 2.0\n",
      "MinMaxStats mins match:   -2.0 = -2.0\n",
      "MinMaxStats maxes match:   2.0 = 2.0\n",
      "MinMaxStats mins match:   -2.0 = -2.0\n",
      "MinMaxStats maxes match:   2.0 = 2.0\n",
      "MinMaxStats mins match:   -2.0 = -2.0\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 5: 2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\n",
      "Path config: [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [0.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [2.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [0.0, 1.0, 0.0, 0.0] \n",
      "Method 3 result: [0.0, 1.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   2.0 = 1\n",
      "MinMaxStats mins match:   -2.0 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   2.0 = 1\n",
      "MinMaxStats mins match:   -2.0 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 6: 2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\n",
      "Path config: [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [0.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [2.0, -1.0, 0.0, 0.0] \n",
      "Method 2 result: [0.0, 1.0, 0.0, 0.0] \n",
      "Method 3 result: [0.0, 1.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   2.0 = 1\n",
      "MinMaxStats mins match:   -2.0 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   2.0 = 1\n",
      "MinMaxStats mins match:   -2.0 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 7: 2-player: alternating game, player 1 wins on there first move\n",
      "Path config: [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Method 3 result: [-1.0, 1.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 8: 2-player: alternating game, player 1 wins on there first move\n",
      "Path config: [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Method 3 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 9: 2-player: alternating game, player 0 wins\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [1.0, -1.0, 1.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 1.0, 0.0] \n",
      "Method 2 result: [1.0, -1.0, 1.0, 0.0] \n",
      "Method 3 result: [1.0, -1.0, 1.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 10: 2-player: alternating game, player 1 wins\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, -1.0, 1.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, -1.0, 1.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, -1.0, 1.0, 0.0] \n",
      "Method 3 result: [-1.0, 1.0, -1.0, 1.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 11: 2-player: alternating game with a leaf value\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-1.0, 1.0, -1.0, 1.0]\n",
      "Method 1 result: [-1.0, 1.0, -1.0, 1.0] \n",
      "Method 2 result: [-1.0, 1.0, -1.0, 1.0] \n",
      "Method 3 result: [-1.0, 1.0, -1.0, 1.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 12: 2-player: alternating game with a leaf value\n",
      "Path config: [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)]\n",
      "Num players: 2\n",
      "Expected node values: [1.0, -1.0, 1.0, -1.0, 1.0]\n",
      "Method 1 result: [1.0, -1.0, 1.0, -1.0, 1.0] \n",
      "Method 2 result: [1.0, -1.0, 1.0, -1.0, 1.0] \n",
      "Method 3 result: [1.0, -1.0, 1.0, -1.0, 1.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 13: 2-player: All player 0 turns\n",
      "Path config: [(0, 1.0), (0, 1.0), (0, 1.0, 0.0)]\n",
      "Num players: 2\n",
      "Expected node values: [3.0, 2.0, 1.0, 0.0]\n",
      "Method 1 result: [-3.0, -2.0, -1.0, 0.0] \n",
      "Method 2 result: [3.0, 2.0, 1.0, 0.0] \n",
      "Method 3 result: [3.0, 2.0, 1.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   3.0 = 3.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   3.0 = 3.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   3.0 = 3.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 14: 2-player: All player 0 turns with leaf value\n",
      "Path config: [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [4.0, 4.0, 4.0, 4.0]\n",
      "Method 1 result: [4.0, 4.0, 4.0, 4.0] \n",
      "Method 2 result: [4.0, 4.0, 4.0, 4.0] \n",
      "Method 3 result: [4.0, 4.0, 4.0, 4.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 4.0\n",
      "MinMaxStats mins match:   -4.0 = -1\n",
      "MinMaxStats maxes match:   4.0 = 4.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 4.0\n",
      "MinMaxStats mins match:   -4.0 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 15: 2-player: Two player 1 turns with leaf value\n",
      "Path config: [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [3.0, -3.0, -4.0, 4.0]\n",
      "Method 1 result: [5.0, -5.0, -4.0, 4.0] \n",
      "Method 2 result: [3.0, -3.0, -4.0, 4.0] \n",
      "Method 3 result: [3.0, -3.0, -4.0, 4.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   5.0 = 3.0\n",
      "MinMaxStats mins match:   -5.0 = -4.0\n",
      "MinMaxStats maxes match:   3.0 = 3.0\n",
      "MinMaxStats mins match:   -4.0 = -4.0\n",
      "MinMaxStats maxes match:   5.0 = 3.0\n",
      "MinMaxStats mins match:   -5.0 = -4.0\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 16: 2-player: Two player 1 turns with leaf value\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [-5.0, 5.0, 4.0, 4.0]\n",
      "Method 1 result: [-3.0, 3.0, 4.0, 4.0] \n",
      "Method 2 result: [-5.0, 5.0, 4.0, 4.0] \n",
      "Method 3 result: [-5.0, 5.0, 4.0, 4.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   3.0 = 5.0\n",
      "MinMaxStats mins match:   -4.0 = -5.0\n",
      "MinMaxStats maxes match:   5.0 = 5.0\n",
      "MinMaxStats mins match:   -5.0 = -5.0\n",
      "MinMaxStats maxes match:   3.0 = 5.0\n",
      "MinMaxStats mins match:   -4.0 = -5.0\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 17: 2-player: Two player 1 turns with leaf value\n",
      "Path config: [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)]\n",
      "Num players: 2\n",
      "Expected node values: [3.0, -3.0, -4.0, -4.0, 4.0]\n",
      "Method 1 result: [5.0, -5.0, -4.0, -4.0, 4.0] \n",
      "Method 2 result: [3.0, -3.0, -4.0, -4.0, 4.0] \n",
      "Method 3 result: [3.0, -3.0, -4.0, -4.0, 4.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   5.0 = 3.0\n",
      "MinMaxStats mins match:   -5.0 = -4.0\n",
      "MinMaxStats maxes match:   3.0 = 3.0\n",
      "MinMaxStats mins match:   -4.0 = -4.0\n",
      "MinMaxStats maxes match:   5.0 = 3.0\n",
      "MinMaxStats mins match:   -5.0 = -4.0\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 18: 1-player: All rewards sum up\n",
      "Path config: [(0, 1.0), (0, 2.0), (0, 3.0, 0.0)]\n",
      "Num players: 1\n",
      "Expected node values: [6.0, 5.0, 3.0, 0.0]\n",
      "Method 1 result: [6.0, 5.0, 3.0, 0.0] \n",
      "Method 2 result: [6.0, 5.0, 3.0, 0.0] \n",
      "Method 3 result: [6.0, 5.0, 3.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   6.0 = 6.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   6.0 = 6.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   6.0 = 6.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 19: 1-player: Rewards + leaf value\n",
      "Path config: [(0, 1.0), (0, 0.0), (0, 0.0, 5.0)]\n",
      "Num players: 1\n",
      "Expected node values: [6.0, 5.0, 5.0, 5.0]\n",
      "Method 1 result: [6.0, 5.0, 5.0, 5.0] \n",
      "Method 2 result: [6.0, 5.0, 5.0, 5.0] \n",
      "Method 3 result: [6.0, 5.0, 5.0, 5.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   6.0 = 6.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   6.0 = 6.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   6.0 = 6.0\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 20: 3-player: Player 2 wins\n",
      "Path config: [(1, 0.0), (2, 0.0), (0, 1.0), (0, 0.0, 0.0)]\n",
      "Num players: 3\n",
      "Expected node values: [-1.0, -1.0, 1.0, 0.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, 1.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, -1.0, 1.0, 0.0, 0.0] \n",
      "Method 3 result: [-1.0, -1.0, 1.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 21: 3-player: Player 0 wins\n",
      "Path config: [(1, 1.0), (2, 0.0), (0, 0.0), (0, 0.0, 0.0)]\n",
      "Num players: 3\n",
      "Expected node values: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Method 1 result: [1.0, 0.0, 0.0, 0.0, 0.0] \n",
      "Method 2 result: [1.0, 0.0, 0.0, 0.0, 0.0] \n",
      "Method 3 result: [1.0, 0.0, 0.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 22: 3-player: Player 1 wins\n",
      "Path config: [(1, 0.0), (2, 1.0), (0, 0.0), (0, 0.0, 0.0)]\n",
      "Num players: 3\n",
      "Expected node values: [-1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Method 1 result: [1.0, -1.0, 0.0, 0.0, 0.0] \n",
      "Method 2 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Method 3 result: [-1.0, 1.0, 0.0, 0.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 23: 3-player: Player 0 wins\n",
      "Path config: [(1, 0.0), (2, 0.0), (0, 0.0), (0, 1.0, 0.0)]\n",
      "Num players: 3\n",
      "Expected node values: [1.0, -1.0, -1.0, 1.0, 0.0]\n",
      "Method 1 result: [-1.0, 1.0, 1.0, -1.0, 0.0] \n",
      "Method 2 result: [1.0, -1.0, -1.0, 1.0, 0.0] \n",
      "Method 3 result: [1.0, -1.0, -1.0, 1.0, 0.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 FAILED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "Test Case 24: 3-player: player 1 ends with a value prediction\n",
      "Path config: [(1, 0.0), (2, 0.0), (0, 0.0), (1, 0.0, 1.0)]\n",
      "Num players: 3\n",
      "Expected node values: [-1.0, 1.0, -1.0, -1.0, 1.0]\n",
      "Method 1 result: [-1.0, 1.0, -1.0, -1.0, 1.0] \n",
      "Method 2 result: [-1.0, 1.0, -1.0, -1.0, 1.0] \n",
      "Method 3 result: [-1.0, 1.0, -1.0, -1.0, 1.0] \n",
      "Methods 1 and 2 match: \n",
      "Methods 2 and 3 match: \n",
      "Methods 1 and 3 match: \n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      "MinMaxStats maxes match:   1 = 1\n",
      "MinMaxStats mins match:   -1 = -1\n",
      " METHOD 1 PASSED\n",
      " METHOD 2 PASSED\n",
      " METHOD 3 PASSED\n",
      "\n",
      "================================================================================\n",
      " All tests passed!\n",
      "Method 1 got 11/24\n",
      "Method 2 got 24/24\n",
      "Method 3 got 24/24\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "def make_search_path(path_config):\n",
    "    \"\"\"\n",
    "    path_config is a list of tuples: (to_play, reward)\n",
    "    Last element has the leaf value and to_play\n",
    "    \"\"\"\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "\n",
    "    # Initialize root (player 0)\n",
    "    root.expand(legal_moves, 0, policy, hidden_state, 0.0)\n",
    "\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "\n",
    "    # Build path according to config\n",
    "    for i, (to_play, reward) in enumerate(path_config[:-1]):\n",
    "        search_path.append(node)\n",
    "        node.expand(legal_moves, to_play, policy, hidden_state, reward)\n",
    "        node = node.children[0]\n",
    "\n",
    "    # Last node\n",
    "    search_path.append(node)\n",
    "    last_config = path_config[-1]\n",
    "    leaf_to_play = last_config[0]\n",
    "    leaf_reward = last_config[1]\n",
    "    leaf_value = last_config[2] if len(last_config) > 2 else 0.0\n",
    "    node.expand(legal_moves, leaf_to_play, policy, hidden_state, leaf_reward)\n",
    "\n",
    "    return search_path, leaf_to_play, leaf_value\n",
    "\n",
    "\n",
    "def backpropagate_method1(search_path, to_play, value, num_players, min_max_stats):\n",
    "    \"\"\"Original Method 1\"\"\"\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "        min_max_stats.update(\n",
    "            node.reward + 1.0 * (node.value() if num_players == 1 else -node.value())\n",
    "        )\n",
    "\n",
    "        if node.to_play == to_play and num_players > 1:\n",
    "            value = -node.reward + 1.0 * value\n",
    "        else:\n",
    "            value = node.reward + 1.0 * value\n",
    "\n",
    "    return [n.value() for n in search_path], min_max_stats\n",
    "\n",
    "\n",
    "def backpropagate_method2(\n",
    "    search_path, leaf_to_play, leaf_value, num_players, min_max_stats\n",
    "):\n",
    "\n",
    "    n = len(search_path)\n",
    "\n",
    "    # 1) Compute exact total return for each node from that node's player perspective.\n",
    "    #    totals[i] is the scalar to add to search_path[i].value_sum.\n",
    "    totals = [0.0] * n\n",
    "    for i, node in enumerate(search_path):\n",
    "        total = 0.0\n",
    "        # Sum future rewards: reward at search_path[j] belongs to acting_player = search_path[j-1].to_play\n",
    "        for j in range(i + 1, n):\n",
    "            acting_player = search_path[j - 1].to_play\n",
    "            r = search_path[j].reward\n",
    "            total += r if acting_player == node.to_play else -r\n",
    "\n",
    "        # Add leaf value (owned by leaf_to_play)\n",
    "        total += leaf_value if leaf_to_play == node.to_play else -leaf_value\n",
    "        totals[i] = total\n",
    "\n",
    "    # 2) Apply updates in reverse order and update MinMaxStats using parent-perspective value.\n",
    "    #    For node at index i, its parent is search_path[i-1] (if i>0).\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        node = search_path[i]\n",
    "\n",
    "        # Update node stats (so node.value() reflects totals after update)\n",
    "        node.value_sum += totals[i]\n",
    "        node.visits += 1\n",
    "\n",
    "        # Compute the scalar used to update MinMaxStats: it must be the value of this child\n",
    "        # from its parent's perspective:\n",
    "        # parent_value_contrib = child.reward + discount * (sign * child.value())\n",
    "        # sign = +1 if child.to_play == parent.to_play (same player acts again), else -1.\n",
    "        if i > 0:\n",
    "            parent_to_play = search_path[i - 1].to_play\n",
    "            # For single-player games, child.value() is always added (no sign flip).\n",
    "            if num_players == 1:\n",
    "                sign = 1.0\n",
    "            else:\n",
    "                sign = 1.0 if node.to_play == parent_to_play else -1.0\n",
    "        else:\n",
    "            # For root (no parent) we must still pass something to MinMaxStats.\n",
    "            # Use sign = +1 (treat root as its own parent's perspective) for a consistent convention.\n",
    "            # This is harmless because root's parent doesn't exist  MinMaxStats is just tracking\n",
    "            # global min/max of these scalars for normalization.\n",
    "            sign = 1.0 if num_players == 1 else 1.0\n",
    "\n",
    "        parent_value_contrib = node.reward + 1.0 * (sign * node.value())\n",
    "        min_max_stats.update(parent_value_contrib)\n",
    "\n",
    "    return [n.value() for n in search_path], min_max_stats\n",
    "\n",
    "\n",
    "def backpropagate_method3(\n",
    "    search_path,  # list of nodes from root .. leaf\n",
    "    leaf_to_play,  # player id that owns `leaf_value`\n",
    "    leaf_value,  # scalar leaf value\n",
    "    num_players,\n",
    "    min_max_stats,\n",
    "):\n",
    "    \"\"\"\n",
    "    O(n) discounted backpropagation with correct sign handling for repeated same-player turns.\n",
    "    - search_path: list of Nodes, index 0 is root, last is leaf.\n",
    "      Each Node must expose: .reward (reward stored on the node),\n",
    "      .to_play (player id who acts at that node),\n",
    "      .value_sum, .visits, and .value() == value_sum/visits.\n",
    "    - leaf_to_play, leaf_value: ownership and scalar of the leaf bootstrap.\n",
    "    - num_players: number of players (1 for single-player).\n",
    "    - discount: gamma\n",
    "    - min_max_stats: MinMaxStats instance with .update(x) and .normalize(x)\n",
    "    Returns: list of node.value() after updates.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(search_path)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    # --- 1) Build per-player accumulator array acc[p] = Acc_p(i) for current i (starting from i = n-1) ---\n",
    "    # Acc_p(i) definition: discounted return from node i for a node whose player is p:\n",
    "    # Acc_p(i) = sum_{j=i+1..n-1} discount^{j-i-1} * sign(p, j) * reward_j\n",
    "    #            + discount^{n-1-i} * sign(p, leaf) * leaf_value\n",
    "    # Where sign(p, j) = +1 if acting_player_at_j (which is search_path[j-1].to_play) == p else -1.\n",
    "    #\n",
    "    # We compute Acc_p(n-1) = sign(p, leaf) * leaf_value as base, then iterate backward:\n",
    "    # Acc_p(i-1) = s(p, i) * reward_i + discount * Acc_p(i)\n",
    "\n",
    "    # Initialize acc for i = n-1 (base: discounted exponent 0 for leaf value)\n",
    "    # acc is a Python list of floats length num_players\n",
    "    acc = [0.0] * num_players\n",
    "    for p in range(num_players):\n",
    "        acc[p] = leaf_value if leaf_to_play == p else -leaf_value\n",
    "\n",
    "    # totals[i] will hold Acc_{node_player}(i)\n",
    "    totals = [0.0] * n\n",
    "\n",
    "    # Iterate from i = n-1 down to 0\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        node = search_path[i]\n",
    "        node_player = node.to_play\n",
    "        # totals for this node = acc[node_player] (current Acc_p(i))\n",
    "        totals[i] = acc[node_player]\n",
    "\n",
    "        # Prepare acc for i-1 (if any)\n",
    "        if i > 0:\n",
    "            # reward at index i belongs to acting_player = search_path[i-1].to_play\n",
    "            r_i = search_path[i].reward\n",
    "            acting_player = search_path[i - 1].to_play\n",
    "\n",
    "            # Update per-player accumulators in O(num_players)\n",
    "            # Acc_p(i-1) = sign(p, i) * r_i + discount * Acc_p(i)\n",
    "            # sign(p, i) = +1 if acting_player == p else -1\n",
    "            # We overwrite acc[p] in-place to be Acc_p(i-1)\n",
    "            for p in range(num_players):\n",
    "                sign = 1.0 if acting_player == p else -1.0\n",
    "                acc[p] = sign * r_i + 1.0 * acc[p]\n",
    "\n",
    "    # --- 2) Apply totals to nodes in reverse order and update MinMaxStats (parent-perspective scalar) ---\n",
    "    # We must update nodes (value_sum, visits) from the leaf back to the root so that when\n",
    "    # computing parent-perspective scalars we can use child.value() (which should reflect the\n",
    "    # just-updated child totals).\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        node = search_path[i]\n",
    "\n",
    "        # apply computed discounted total for this node's player\n",
    "        node.value_sum += totals[i]\n",
    "        node.visits += 1\n",
    "\n",
    "        # compute scalar that MinMaxStats expects for this child from its parent's perspective:\n",
    "        # parent_value_contrib = child.reward + discount * (sign * child.value())\n",
    "        # sign = +1 if single-player OR child.to_play == parent.to_play else -1\n",
    "        if i > 0:\n",
    "            parent = search_path[i - 1]\n",
    "            if num_players == 1:\n",
    "                sign = 1.0\n",
    "            else:\n",
    "                sign = 1.0 if node.to_play == parent.to_play else -1.0\n",
    "        else:\n",
    "            # root: choose sign = +1 convention (root has no parent)\n",
    "            sign = 1.0\n",
    "\n",
    "        parent_value_contrib = node.reward + 1.0 * (sign * node.value())\n",
    "        min_max_stats.update(parent_value_contrib)\n",
    "\n",
    "    # Return updated node values\n",
    "    return [node.value() for node in search_path], min_max_stats\n",
    "\n",
    "\n",
    "# Test cases: (path_config, expected_root_value, num_players, description)\n",
    "test_cases = [\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 1.0, 0.0)],\n",
    "        [1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)],\n",
    "        [1.0, -1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 1.0), (0, 1.0, 0.0)],\n",
    "        [3.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)],\n",
    "        [4.0, 4.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)],\n",
    "        [-5.0, 5.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    # Single player test cases\n",
    "    (\n",
    "        [(0, 1.0), (0, 2.0), (0, 3.0, 0.0)],\n",
    "        [6.0, 5.0, 3.0, 0.0],\n",
    "        1,\n",
    "        \"1-player: All rewards sum up\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 0.0), (0, 0.0, 5.0)],\n",
    "        [6.0, 5.0, 5.0, 5.0],\n",
    "        1,\n",
    "        \"1-player: Rewards + leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 1.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, -1.0, 1.0, 0.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 2 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (2, 0.0), (0, 0.0), (0, 0.0, 0.0)],\n",
    "        [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 1.0), (0, 0.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 0.0), (0, 1.0, 0.0)],\n",
    "        [1.0, -1.0, -1.0, 1.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        [-1.0, 1.0, -1.0, -1.0, 1.0],\n",
    "        3,\n",
    "        \"3-player: player 1 ends with a value prediction\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Testing MuZero Value Backpropagation\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def are_lists_roughly_equal(list1, list2):\n",
    "    \"\"\"\n",
    "    Checks if two lists of floats are roughly equal in Python 2.\n",
    "\n",
    "    Args:\n",
    "        list1: The first list of floats.\n",
    "        list2: The second list of floats.\n",
    "        tolerance: The maximum allowed absolute difference between corresponding\n",
    "                   elements for them to be considered roughly equal.\n",
    "\n",
    "    Returns:\n",
    "        True if the lists are roughly equal, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        # Compare elements using absolute tolerance\n",
    "        if not math.isclose(list1[i], list2[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "all_passed = True\n",
    "method1_correct = 0\n",
    "method2_correct = 0\n",
    "method3_correct = 0\n",
    "for i, (path_config, expected, num_players, description) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest Case {i}: {description}\")\n",
    "    print(f\"Path config: {path_config}\")\n",
    "    print(f\"Num players: {num_players}\")\n",
    "    print(f\"Expected node values: {expected}\")\n",
    "\n",
    "    # Test Method 1\n",
    "    search_path_1, to_play_1, value_1 = make_search_path(path_config)\n",
    "    result_1, min_max_stats_1 = backpropagate_method1(\n",
    "        search_path_1,\n",
    "        to_play_1,\n",
    "        value_1,\n",
    "        num_players,\n",
    "        min_max_stats=MinMaxStats(known_bounds=[-1, 1]),\n",
    "    )\n",
    "\n",
    "    # Test Method 2\n",
    "    search_path_2, to_play_2, value_2 = make_search_path(path_config)\n",
    "    result_2, min_max_stats_2 = backpropagate_method2(\n",
    "        search_path_2,\n",
    "        to_play_2,\n",
    "        value_2,\n",
    "        num_players,\n",
    "        min_max_stats=MinMaxStats(known_bounds=[-1, 1]),\n",
    "    )\n",
    "\n",
    "    # Test Method 3\n",
    "    search_path_3, to_play_3, value_3 = make_search_path(path_config)\n",
    "    result_3, min_max_stats_3 = backpropagate_method3(\n",
    "        search_path_3,\n",
    "        to_play_3,\n",
    "        value_3,\n",
    "        num_players,\n",
    "        min_max_stats=MinMaxStats(known_bounds=[-1, 1]),\n",
    "    )\n",
    "\n",
    "    # Check results\n",
    "    method1_pass = are_lists_roughly_equal(result_1, expected)\n",
    "    method2_pass = are_lists_roughly_equal(result_2, expected)\n",
    "    method3_pass = are_lists_roughly_equal(result_3, expected)\n",
    "    match1 = are_lists_roughly_equal(result_1, result_2)\n",
    "    match2 = are_lists_roughly_equal(result_2, result_3)\n",
    "    match3 = are_lists_roughly_equal(result_1, result_3)\n",
    "\n",
    "    print(f\"Method 1 result: {result_1} {'' if method1_pass else ''}\")\n",
    "    print(f\"Method 2 result: {result_2} {'' if method2_pass else ''}\")\n",
    "    print(f\"Method 3 result: {result_3} {'' if method3_pass else ''}\")\n",
    "    print(f\"Methods 1 and 2 match: {'' if match1 else ''}\")\n",
    "    print(f\"Methods 2 and 3 match: {'' if match2 else ''}\")\n",
    "    print(f\"Methods 1 and 3 match: {'' if match3 else ''}\")\n",
    "    print(\n",
    "        f\"MinMaxStats maxes match:  {'' if min_max_stats_1.max == min_max_stats_2.max else ''} {min_max_stats_1.max} = {min_max_stats_2.max}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats mins match:  {'' if min_max_stats_1.min == min_max_stats_2.min else ''} {min_max_stats_1.min} = {min_max_stats_2.min}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats maxes match:  {'' if min_max_stats_2.max == min_max_stats_3.max else ''} {min_max_stats_2.max} = {min_max_stats_3.max}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats mins match:  {'' if min_max_stats_2.min == min_max_stats_3.min else ''} {min_max_stats_2.min} = {min_max_stats_3.min}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats maxes match:  {'' if min_max_stats_1.max == min_max_stats_3.max else ''} {min_max_stats_1.max} = {min_max_stats_3.max}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats mins match:  {'' if min_max_stats_1.min == min_max_stats_3.min else ''} {min_max_stats_1.min} = {min_max_stats_3.min}\"\n",
    "    )\n",
    "    if method1_pass:\n",
    "        method1_correct += 1\n",
    "    if method2_pass:\n",
    "        method2_correct += 1\n",
    "    if method3_pass:\n",
    "        method3_correct += 1\n",
    "    if not method1_pass:\n",
    "        print(\" METHOD 1 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 1 PASSED\")\n",
    "    if not method2_pass:\n",
    "        print(\" METHOD 2 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 2 PASSED\")\n",
    "    if not method3_pass:\n",
    "        print(\" METHOD 3 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 3 PASSED\")\n",
    "\n",
    "    # if not (method1_pass and method2_pass and match):\n",
    "    #     all_passed = False\n",
    "    #     print(\" FAILED\")\n",
    "    # else:\n",
    "    #     print(\" PASSED\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_passed:\n",
    "    print(\" All tests passed!\")\n",
    "else:\n",
    "    print(\" Some tests failed\")\n",
    "\n",
    "print(f\"Method 1 got {method1_correct}/{len(test_cases)}\")\n",
    "print(f\"Method 2 got {method2_correct}/{len(test_cases)}\")\n",
    "print(f\"Method 3 got {method3_correct}/{len(test_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_fn(\n",
    "    index: int,\n",
    "    values: list,\n",
    "    policies: list,\n",
    "    rewards: list,\n",
    "    actions: list,\n",
    "    infos: list,\n",
    "    num_unroll_steps: int,\n",
    "    n_step: int,\n",
    "):\n",
    "    n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_policies = torch.zeros(\n",
    "        (num_unroll_steps + 1, num_actions), dtype=torch.float32\n",
    "    )\n",
    "    n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.int16)\n",
    "    for current_index in range(index, index + num_unroll_steps + 1):\n",
    "        unroll_step = current_index - index\n",
    "        bootstrap_index = current_index + n_step\n",
    "        # print(\"bootstrapping\")\n",
    "        # value of current position is the value at the position n_steps away + rewards to get to the n_step position\n",
    "        if bootstrap_index < len(values):\n",
    "            if (\n",
    "                \"player\" not in infos[current_index]\n",
    "                or infos[current_index][\"player\"] == infos[bootstrap_index][\"player\"]\n",
    "            ):\n",
    "                value = values[bootstrap_index] * gamma**n_step\n",
    "            else:\n",
    "                value = -values[bootstrap_index] * gamma**n_step\n",
    "        else:\n",
    "            value = 0\n",
    "\n",
    "        # the rewards at this index to the bootstrap index should be added to the value\n",
    "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
    "            # WHAT IS current_index + i + 1 when current index is the last frame?? IS THIS AN ERROR?\n",
    "            if (\n",
    "                \"player\" not in infos[current_index]\n",
    "                or infos[current_index][\"player\"]\n",
    "                == infos[current_index + i][\n",
    "                    \"player\"\n",
    "                ]  # + 1 if doing my og thing and i want to go back\n",
    "            ):\n",
    "                value += reward * gamma**i\n",
    "            else:\n",
    "                value -= reward * gamma**i\n",
    "\n",
    "        # target reward is the reward before the ones added to the value\n",
    "        if current_index > 0 and current_index <= len(rewards):\n",
    "            last_reward = rewards[current_index - 1]\n",
    "            # if self.has_intermediate_rewards:\n",
    "            #     last_reward = rewards[current_index - 1]\n",
    "            # else:\n",
    "            #     value += (\n",
    "            #         rewards[current_index - 1]\n",
    "            #         if infos[current_index][\"player\"]\n",
    "            #         == infos[current_index - 1][\"player\"]\n",
    "            #         else -rewards[current_index - 1]\n",
    "            #     )\n",
    "            #     last_reward = rewards[current_index - 1]  # reward not used\n",
    "        else:\n",
    "            last_reward = 0  # self absorbing state 0 reward\n",
    "\n",
    "        if current_index < len(values):\n",
    "            n_step_values[unroll_step] = value\n",
    "            n_step_rewards[unroll_step] = last_reward\n",
    "            n_step_policies[unroll_step] = policies[current_index]\n",
    "            if unroll_step < num_unroll_steps:\n",
    "                # no action for last unroll step (since you dont act on that state)\n",
    "                n_step_actions[unroll_step] = actions[current_index]\n",
    "        else:\n",
    "            n_step_values[unroll_step] = (\n",
    "                value  # should be value or 0, maybe broken for single player\n",
    "            )\n",
    "            n_step_rewards[unroll_step] = last_reward\n",
    "            n_step_policies[unroll_step] = (\n",
    "                torch.ones(num_actions) / num_actions\n",
    "            )  # self absorbing state\n",
    "            if unroll_step < num_unroll_steps:\n",
    "                # no action for last unroll step (since you dont act on that state)\n",
    "                n_step_actions[unroll_step] = -1  # self absorbing state\n",
    "\n",
    "    return (\n",
    "        n_step_values,  # [initial value, recurrent values]\n",
    "        n_step_policies,  # [initial policy, recurrent policies]\n",
    "        n_step_rewards,  # [initial reward (0), recurrent rewards] initial reward is useless like the first last action, but we ignore it in the learn function\n",
    "        n_step_actions,  # [recurrent actions, extra action]\n",
    "    )  # remove the last actions, as there should be one less action than other stuff\n",
    "\n",
    "\n",
    "def new_fn(\n",
    "    index: int,\n",
    "    values: list,\n",
    "    policies: list,\n",
    "    rewards: list,\n",
    "    actions: list,\n",
    "    infos: list,\n",
    "    num_unroll_steps: int,\n",
    "    n_step: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        n_step_values: tensor shape (num_unroll_steps+1,)\n",
    "        n_step_policies: tensor shape (num_unroll_steps+1, num_actions)\n",
    "        n_step_rewards: tensor shape (num_unroll_steps+1,)\n",
    "        n_step_actions: tensor shape (num_unroll_steps,)\n",
    "    Conventions:\n",
    "        - rewards[t] is the reward from taking action at state t (transition t  t+1)\n",
    "        - infos[t][\"player\"] is the player who acted at state t\n",
    "        - n_step_rewards[0] = 0 (no reward leading into root)\n",
    "    \"\"\"\n",
    "    n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_policies = torch.zeros(\n",
    "        (num_unroll_steps + 1, num_actions), dtype=torch.float32\n",
    "    )\n",
    "    n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.int16)\n",
    "\n",
    "    max_index = len(values)\n",
    "\n",
    "    for u in range(0, num_unroll_steps + 1):\n",
    "        current_index = index + u\n",
    "\n",
    "        # 1. discounted n-step value from current_index\n",
    "        value = 0.0\n",
    "        for k in range(n_step):\n",
    "            r_idx = current_index + k\n",
    "            if r_idx < len(rewards):\n",
    "                r = rewards[r_idx]\n",
    "                node_player = (\n",
    "                    infos[current_index].get(\"player\", None)\n",
    "                    if current_index < len(infos)\n",
    "                    else None\n",
    "                )\n",
    "                acting_player = (\n",
    "                    infos[r_idx].get(\"player\", None) if r_idx < len(infos) else None\n",
    "                )\n",
    "                sign = (\n",
    "                    1.0\n",
    "                    if (\n",
    "                        node_player is None\n",
    "                        or acting_player is None\n",
    "                        or node_player == acting_player\n",
    "                    )\n",
    "                    else -1.0\n",
    "                )\n",
    "                value += (gamma**k) * (sign * r)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        boot_idx = current_index + n_step\n",
    "        if boot_idx < len(values):\n",
    "            v_boot = values[boot_idx]\n",
    "            node_player = (\n",
    "                infos[current_index].get(\"player\", None)\n",
    "                if current_index < len(infos)\n",
    "                else None\n",
    "            )\n",
    "            boot_player = (\n",
    "                infos[boot_idx].get(\"player\", None) if boot_idx < len(infos) else None\n",
    "            )\n",
    "            sign_leaf = (\n",
    "                1.0\n",
    "                if (\n",
    "                    node_player is None\n",
    "                    or boot_player is None\n",
    "                    or node_player == boot_player\n",
    "                )\n",
    "                else -1.0\n",
    "            )\n",
    "            value += (gamma**n_step) * (sign_leaf * v_boot)\n",
    "\n",
    "        n_step_values[u] = value\n",
    "\n",
    "        # 2. reward target\n",
    "        if u == 0:\n",
    "            n_step_rewards[u] = 0.0  # root has no preceding reward\n",
    "        else:\n",
    "            reward_idx = current_index - 1\n",
    "            n_step_rewards[u] = (\n",
    "                rewards[reward_idx] if reward_idx < len(rewards) else 0.0\n",
    "            )\n",
    "\n",
    "        # 3. policy\n",
    "        if current_index < len(policies):\n",
    "            n_step_policies[u] = policies[current_index]\n",
    "        else:\n",
    "            n_step_policies[u] = torch.ones(num_actions) / num_actions\n",
    "\n",
    "        # 4. action\n",
    "        if u < num_unroll_steps:\n",
    "            n_step_actions[u] = (\n",
    "                actions[current_index] if current_index < len(actions) else -1\n",
    "            )\n",
    "\n",
    "    return n_step_values, n_step_policies, n_step_rewards, n_step_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "def compare_get_n_step_info(\n",
    "    old_fn,\n",
    "    new_fn,\n",
    "    *,\n",
    "    index,\n",
    "    values,\n",
    "    policies,\n",
    "    rewards,\n",
    "    actions,\n",
    "    infos,\n",
    "    num_unroll_steps,\n",
    "    n_step,\n",
    "    num_actions,\n",
    "    gamma=0.997,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares old and new _get_n_step_info outputs and checks correctness.\n",
    "    Arguments:\n",
    "        old_fn: callable implementing the old logic\n",
    "        new_fn: callable implementing the new logic\n",
    "        (both should take arguments in same order as MuZeroReplayBuffer._get_n_step_info)\n",
    "    \"\"\"\n",
    "\n",
    "    # Run both implementations\n",
    "    old_vals, old_pols, old_rews, old_acts = old_fn(\n",
    "        index, values, policies, rewards, actions, infos, num_unroll_steps, n_step\n",
    "    )\n",
    "\n",
    "    new_vals, new_pols, new_rews, new_acts = new_fn(\n",
    "        index, values, policies, rewards, actions, infos, num_unroll_steps, n_step\n",
    "    )\n",
    "\n",
    "    # --- compute expected mathematically correct discounted values ---\n",
    "    def expected_nstep_value(t):\n",
    "        \"\"\"Return correct discounted n-step bootstrap value from index t\"\"\"\n",
    "        v = 0.0\n",
    "        for k in range(n_step):\n",
    "            idx = t + k\n",
    "            if idx >= len(rewards):\n",
    "                break\n",
    "            # who acted for reward idx\n",
    "            r_player = infos[idx][\"player\"]\n",
    "            node_player = infos[t][\"player\"]\n",
    "            sign = 1 if r_player == node_player else -1\n",
    "            v += (gamma**k) * (sign * rewards[idx])\n",
    "        boot_idx = t + n_step\n",
    "        if boot_idx < len(values):\n",
    "            node_player = infos[t][\"player\"]\n",
    "            leaf_player = infos[boot_idx][\"player\"]\n",
    "            sign_leaf = 1 if leaf_player == node_player else -1\n",
    "            v += (gamma**n_step) * (sign_leaf * values[boot_idx])\n",
    "        return v\n",
    "\n",
    "    expected_vals = torch.tensor(\n",
    "        [expected_nstep_value(index + u) for u in range(num_unroll_steps + 1)],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    # --- compare ---\n",
    "    def close(a, b, tol=1e-5):\n",
    "        return torch.allclose(a, b, atol=tol, rtol=tol)\n",
    "\n",
    "    ok_vals = close(new_vals, expected_vals)\n",
    "    ok_rews = close(\n",
    "        new_rews,\n",
    "        torch.tensor(\n",
    "            [\n",
    "                rewards[index + u] if index + u < len(rewards) else 0\n",
    "                for u in range(num_unroll_steps + 1)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    ok_acts = close(\n",
    "        new_acts,\n",
    "        torch.tensor(\n",
    "            [\n",
    "                actions[index + u] if index + u < len(actions) else -1\n",
    "                for u in range(num_unroll_steps)\n",
    "            ],\n",
    "            dtype=torch.int16,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # --- print diagnostics ---\n",
    "    if verbose:\n",
    "        print(\"====== N-STEP COMPARISON ======\")\n",
    "        for name, old, new in zip(\n",
    "            [\"values\", \"rewards\", \"actions\"],\n",
    "            [old_vals, old_rews, old_acts],\n",
    "            [new_vals, new_rews, new_acts],\n",
    "        ):\n",
    "            print(f\"\\n{name.upper()}:\")\n",
    "            print(f\" old: {old.tolist()}\")\n",
    "            print(f\" new: {new.tolist()}\")\n",
    "\n",
    "        print(\"\\nEXPECTED CORRECT VALUES:\", expected_vals.tolist())\n",
    "        print(\"\\nChecks:\")\n",
    "        print(f\"  New matches expected values: {ok_vals}\")\n",
    "        print(f\"  Immediate rewards correct:   {ok_rews}\")\n",
    "        print(f\"  Actions aligned:             {ok_acts}\")\n",
    "\n",
    "        # Highlight mismatches\n",
    "        if not ok_vals:\n",
    "            diffs = (new_vals - expected_vals).abs()\n",
    "            print(\"  Value diffs:\", diffs.tolist())\n",
    "        if not close(new_vals, old_vals):\n",
    "            print(\n",
    "                \"  Old and new disagree on n-step values (expected if old lacked proper discounting or sign logic).\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"ok_values\": ok_vals,\n",
    "        \"ok_rewards\": ok_rews,\n",
    "        \"ok_actions\": ok_acts,\n",
    "        \"old_vals\": old_vals,\n",
    "        \"new_vals\": new_vals,\n",
    "        \"expected_vals\": expected_vals,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== N-STEP COMPARISON ======\n",
      "\n",
      "VALUES:\n",
      " old: [3.372040033340454, -4.479949951171875, 3.0]\n",
      " new: [3.372040033340454, -4.479949951171875, 3.0]\n",
      "\n",
      "REWARDS:\n",
      " old: [0.0, 1.0, -2.0]\n",
      " new: [0.0, 1.0, -2.0]\n",
      "\n",
      "ACTIONS:\n",
      " old: [0, 1]\n",
      " new: [0, 1]\n",
      "\n",
      "EXPECTED CORRECT VALUES: [3.391603708267212, -4.493995666503906, 3.0]\n",
      "\n",
      "Checks:\n",
      "  New matches expected values: False\n",
      "  Immediate rewards correct:   False\n",
      "  Actions aligned:             True\n",
      "  Value diffs: [0.019563674926757812, 0.01404571533203125, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Example episode\n",
    "values = [0.2, -0.1, 0.4, 0.5]\n",
    "rewards = [1.0, -2.0, 3.0]\n",
    "actions = [0, 1, 2]\n",
    "policies = [\n",
    "    torch.tensor([0.7, 0.3]),\n",
    "    torch.tensor([0.2, 0.8]),\n",
    "    torch.tensor([0.5, 0.5]),\n",
    "    torch.tensor([0.5, 0.5]),\n",
    "]\n",
    "infos = [\n",
    "    {\"player\": 0},\n",
    "    {\"player\": 1},\n",
    "    {\"player\": 0},\n",
    "    {\"player\": 1},\n",
    "]\n",
    "index = 0\n",
    "num_unroll_steps = 2\n",
    "n_step = 2\n",
    "num_actions = 2\n",
    "gamma = 0.99\n",
    "\n",
    "results = compare_get_n_step_info(\n",
    "    old_fn=old_fn,\n",
    "    new_fn=new_fn,\n",
    "    index=index,\n",
    "    values=values,\n",
    "    policies=policies,\n",
    "    rewards=rewards,\n",
    "    actions=actions,\n",
    "    infos=infos,\n",
    "    num_unroll_steps=num_unroll_steps,\n",
    "    n_step=n_step,\n",
    "    num_actions=num_actions,\n",
    "    gamma=0.997,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
