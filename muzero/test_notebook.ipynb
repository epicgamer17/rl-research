{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [0, 500],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [512, 64],\n",
    "    \"dynamics_dense_layer_widths\": [512, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [512],  # ???\n",
    "    \"critic_dense_layer_widths\": [512],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 50,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.5, 0.25],\n",
    "    \"temperature_updates\": [30000, 60000],\n",
    "    \"temperature_with_training_steps\": True,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.005,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"discount_factor\": 0.997,\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 50000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 10,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    # \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": 31,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = CartPoleConfig().make_env()\n",
    "game_config = CartPoleConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_cartpole\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(24, 3, 1)] * 1,  # increase num layers\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layers\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # try larger\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [8],  # change this\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # slightly increase this, maybe 0.002 or 0.003\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,  # try lower\n",
    "    \"value_loss_function\": MSELoss(),  #  MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),  # MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"min_replay_buffer_size\": 4000,  # try lower (or just different)\n",
    "    \"replay_buffer_size\": 100000,  # try lower, 50k or 20k or 10k\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"glorot_normal\",  # try different\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,  # 0.0 was original\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,  # None\n",
    "}\n",
    "\n",
    "# REANALYZE NOT IMPLEMENTED BUT NEVER USED IN THING I SAW ONLINE (like it is computed but never ends up used since bootstrap index always > than len of game history)\n",
    "\n",
    "# add a max depth to the tree\n",
    "# game priority is max of position priorities of the game\n",
    "# with these two changes should be an exact match with online github implementation\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-hyperopt-best\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 50\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 300\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [256, 64],\n",
    "    \"dynamics_dense_layer_widths\": [256, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [256],  # ???\n",
    "    \"critic_dense_layer_widths\": [256],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.002,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 20000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    # \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-dense-5moves_1\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tested the ones i found on the github and they worked good? but i think i am gonna do less workers because they did 480k episodes after 32k training steps, i got 250k after like 5k steps, so i want to change that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"critic_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 1.8,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [2],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": SGD,\n",
    "    \"learning_rate\": 0.1,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 4000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 10000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 1.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 2 moves left one losing one drawing, it finds the drawing move, but evaluates the winning position after tree search as 0 instead of 1 (though predicts it as 1). shown in paper 2m-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from packages.utils.utils.utils import process_petting_zoo_obs\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get action from average strategy\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "\n",
    "random_vs_expert_table = StandingsTable([agent, TicTacToeBestAgent()], start_elo=1400)\n",
    "random_vs_expert_table.play_1v1_tournament(1000, play_game)\n",
    "print(random_vs_expert_table.bayes_elo())\n",
    "print(random_vs_expert_table.get_win_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToeBestAgent:\n",
    "    def __init__(self, model_name=\"tictactoe_expert\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def predict(self, observation, info, env=None):\n",
    "        return observation, info\n",
    "\n",
    "    def select_actions(self, prediction, info):\n",
    "        # Reconstruct board: +1 for current player, -1 for opponent, 0 otherwise\n",
    "        board = prediction[0][0] - prediction[0][1]\n",
    "        print(board)\n",
    "        # Default: random legal move\n",
    "        action = np.random.choice(info[\"legal_moves\"])\n",
    "\n",
    "        # Horizontal and vertical checks\n",
    "        for i in range(3):\n",
    "            # Row\n",
    "            if np.sum(board[i, :]) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                return np.ravel_multi_index((i, ind), (3, 3))\n",
    "            elif abs(np.sum(board[i, :])) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((i, ind), (3, 3))\n",
    "\n",
    "            # Column\n",
    "            if np.sum(board[:, i]) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                return np.ravel_multi_index((ind, i), (3, 3))\n",
    "            elif abs(np.sum(board[:, i])) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((ind, i), (3, 3))\n",
    "\n",
    "        # Diagonals\n",
    "        diag = board.diagonal()\n",
    "        if np.sum(diag) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, ind), (3, 3))\n",
    "        elif abs(np.sum(diag)) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, ind), (3, 3))\n",
    "\n",
    "        anti_diag = np.fliplr(board).diagonal()\n",
    "        if np.sum(anti_diag) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "        elif abs(np.sum(anti_diag)) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 2.0,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 33000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 40000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt-2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],  # RandomAgent(),\n",
    ")\n",
    "\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 500\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.training_steps += 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    "    FrameStackWrapper,\n",
    ")\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig\n",
    "\n",
    "# from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# from agents.random import RandomAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "best_agent = TicTacToeBestAgent()\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "env.reset()\n",
    "env.step(0)\n",
    "env.step(6)\n",
    "env.step(2)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "\n",
    "selected_actions = {i: 0 for i in range(agent.num_actions)}\n",
    "for i in range(100):\n",
    "    selected_actions[agent.select_actions(prediction).item()] += 1\n",
    "print(selected_actions)\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "# env.step(action)\n",
    "# state, reward, terminated, truncated, info = env.last()\n",
    "# prediction = agent.predict(state, info, env)\n",
    "# print(\"MCTS Prediction\", prediction)\n",
    "# initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "# print(\"Initial Value\", initial_inference[0])\n",
    "# print(\"Initial Policy\", initial_inference[1])\n",
    "# action = agent.select_actions(prediction).item()\n",
    "# print(\"Action\", action)\n",
    "# env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from math import log, sqrt, inf\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior_policy):\n",
    "        self.visits = 0\n",
    "        self.to_play = -1\n",
    "        self.prior_policy = prior_policy\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expand(self, legal_moves, to_play, policy, hidden_state, reward):\n",
    "        self.to_play = to_play\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        # print(legal_moves)\n",
    "        policy = {a: policy[a] for a in legal_moves}\n",
    "        policy_sum = sum(policy.values())\n",
    "\n",
    "        for action, p in policy.items():\n",
    "            self.children[action] = Node((p / (policy_sum + 1e-10)).item())\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visits == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visits\n",
    "\n",
    "    def add_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior_policy = (1 - frac) * self.children[\n",
    "                a\n",
    "            ].prior_policy + frac * n\n",
    "\n",
    "    def select_child(self, min_max_stats, pb_c_base, pb_c_init, discount, num_players):\n",
    "        # Select the child with the highest UCB\n",
    "        child_ucbs = [\n",
    "            self.child_ucb_score(\n",
    "                child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "            )\n",
    "            for action, child in self.children.items()\n",
    "        ]\n",
    "        print(\"Child UCBs\", child_ucbs)\n",
    "        action_index = np.random.choice(\n",
    "            np.where(np.isclose(child_ucbs, max(child_ucbs)))[0]\n",
    "        )\n",
    "        action = list(self.children.keys())[action_index]\n",
    "        return action, self.children[action]\n",
    "\n",
    "    def child_ucb_score(\n",
    "        self, child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "    ):\n",
    "        pb_c = log((self.visits + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
    "        pb_c *= sqrt(self.visits) / (child.visits + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior_policy\n",
    "        if child.visits > 0:\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward\n",
    "                + discount\n",
    "                * (\n",
    "                    child.value() if num_players == 1 else -child.value()\n",
    "                )  # (or if on the same team)\n",
    "            )\n",
    "        else:\n",
    "            value_score = 0.0\n",
    "\n",
    "        # check if value_score is nan\n",
    "        assert (\n",
    "            value_score == value_score\n",
    "        ), \"value_score is nan, child value is {}, and reward is {},\".format(\n",
    "            child.value(),\n",
    "            child.reward,\n",
    "        )\n",
    "        assert prior_score == prior_score, \"prior_score is nan\"\n",
    "        print(\"Prior Score\", prior_score)\n",
    "        print(\"Value Score\", value_score)\n",
    "        return prior_score + value_score\n",
    "        # return value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Optional\n",
    "\n",
    "from pyparsing import List\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float(\"inf\")\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "    def __init__(\n",
    "        self, known_bounds: Optional[List[float]]\n",
    "    ):  # might need to say known_bounds=None\n",
    "        self.max = known_bounds[1] if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "        self.min = known_bounds[0] if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.max = max(self.max, value)\n",
    "        self.min = min(self.min, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        print(\"Initial value\", value)\n",
    "        if self.max > self.min:\n",
    "            # We normalize only when we have at a max and min value\n",
    "            print(\"normalized value\", (value - self.min) / (self.max - self.min))\n",
    "            return (value - self.min) / (self.max - self.min)\n",
    "        return value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"min: {self.min}, max: {self.max}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_legal_moves\n",
    "\n",
    "# from muzero.muzero_mcts import Node\n",
    "# from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "root = Node(0.0)\n",
    "_, policy, hidden_state = agent.predict_single_initial_inference(\n",
    "    state,\n",
    "    info,\n",
    ")\n",
    "print(\"root policy\", policy)\n",
    "legal_moves = get_legal_moves(info)[0]\n",
    "to_play = env.agents.index(env.agent_selection)\n",
    "root.expand(legal_moves, to_play, policy, hidden_state, 0.0)\n",
    "print(\"expanded root\")\n",
    "min_max_stats = MinMaxStats(agent.config.known_bounds)\n",
    "\n",
    "for _ in range(agent.config.num_simulations):\n",
    "    print(\"at root\")\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "    to_play = env.agents.index(env.agent_selection)\n",
    "\n",
    "    # GO UNTIL A LEAF NODE IS REACHED\n",
    "    while node.expanded():\n",
    "        print(\"selecting child\")\n",
    "        action, node = node.select_child(\n",
    "            min_max_stats,\n",
    "            agent.config.pb_c_base,\n",
    "            agent.config.pb_c_init,\n",
    "            agent.config.discount_factor,\n",
    "            agent.config.game.num_players,\n",
    "        )\n",
    "        print(\"Selected action\", action)\n",
    "        # THIS NEEDS TO BE CHANGED FOR GAMES WHERE PLAYER COUNT DECREASES AS PLAYERS GET ELIMINATED, USE agent_selector.next() (clone of the current one)\n",
    "        to_play = (to_play + 1) % agent.config.game.num_players\n",
    "        search_path.append(node)\n",
    "    parent = search_path[-2]\n",
    "    reward, hidden_state, value, policy = agent.predict_single_recurrent_inference(\n",
    "        parent.hidden_state,\n",
    "        action,  # model=model\n",
    "    )\n",
    "    reward = reward.item()\n",
    "    value = value.item()\n",
    "    print(\"leaf value\", value)\n",
    "    print(\"leaf reward\", reward)\n",
    "\n",
    "    node.expand(\n",
    "        list(range(agent.num_actions)),\n",
    "        to_play,\n",
    "        policy,\n",
    "        hidden_state,\n",
    "        (\n",
    "            reward  # if self.config.game.has_intermediate_rewards else 0.0\n",
    "        ),  # for board games and games with no intermediate rewards\n",
    "    )\n",
    "\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "        min_max_stats.update(\n",
    "            node.reward\n",
    "            + agent.config.discount_factor\n",
    "            * (node.value() if agent.config.game.num_players == 1 else -node.value())\n",
    "        )\n",
    "        value = (\n",
    "            -node.reward\n",
    "            if node.to_play == to_play and agent.config.game.num_players > 1\n",
    "            else node.reward\n",
    "        ) + agent.config.discount_factor * value\n",
    "\n",
    "    visit_counts = [(child.visits, action) for action, child in root.children.items()]\n",
    "\n",
    "print(visit_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
