{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [0, 500],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [512, 64],\n",
    "    \"dynamics_dense_layer_widths\": [512, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [512],  # ???\n",
    "    \"critic_dense_layer_widths\": [512],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 50,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.5, 0.25],\n",
    "    \"temperature_updates\": [30000, 60000],\n",
    "    \"temperature_with_training_steps\": True,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.005,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"discount_factor\": 0.997,\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 50000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 10,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    # \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": 31,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = CartPoleConfig().make_env()\n",
    "game_config = CartPoleConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_cartpole\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)] * 3,\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layers\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [8],\n",
    "    \"critic_dense_layer_widths\": [8],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    # \"games_per_generation\": 64,  # 32\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"root_dirichlet_alpha\": 0.1,\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # try larger\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,  # Adam, SGD\n",
    "    \"learning_rate\": 0.003,\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,  # try lower\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),  #  MSELoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),  # MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 64,\n",
    "    \"min_replay_buffer_size\": 64,  # try larger\n",
    "    \"replay_buffer_size\": 27000,  # try larger\n",
    "    \"unroll_steps\": 20,  # 20\n",
    "    \"n_step\": 20,  # 20\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # try he_normal\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 1.0,\n",
    "    \"per_beta_final\": 1.0,  # 0.0 was original\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 1,\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": 10,  # None\n",
    "}\n",
    "\n",
    "# REANALYZE NOT IMPLEMENTED BUT NEVER USED IN THING I SAW ONLINE (like it is computed but never ends up used since bootstrap index always > than len of game history)\n",
    "\n",
    "# add a max depth to the tree\n",
    "# game priority is max of position priorities of the game\n",
    "# with these two changes should be an exact match with online github implementation\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "game_config = TicTacToeConfig(tictactoe_v3.env)\n",
    "game_config.has_intermediate_rewards = True\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-test-github\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 50\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 1000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [256, 64],\n",
    "    \"dynamics_dense_layer_widths\": [256, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [256],  # ???\n",
    "    \"critic_dense_layer_widths\": [256],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.002,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 20000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    # \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-dense-5moves_1\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tested the ones i found on the github and they worked good? but i think i am gonna do less workers because they did 480k episodes after 32k training steps, i got 250k after like 5k steps, so i want to change that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"critic_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 1.8,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [2],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": SGD,\n",
    "    \"learning_rate\": 0.1,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 4000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 10000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 1.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 2 moves left one losing one drawing, it finds the drawing move, but evaluates the winning position after tree search as 0 instead of 1 (though predicts it as 1). shown in paper 2m-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from packages.utils.utils.utils import process_petting_zoo_obs\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get action from average strategy\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "\n",
    "random_vs_expert_table = StandingsTable([agent, TicTacToeBestAgent()], start_elo=1400)\n",
    "random_vs_expert_table.play_1v1_tournament(1000, play_game)\n",
    "print(random_vs_expert_table.bayes_elo())\n",
    "print(random_vs_expert_table.get_win_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToeBestAgent:\n",
    "    def __init__(self, model_name=\"tictactoe_expert\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def predict(self, observation, info, env=None):\n",
    "        return observation, info\n",
    "\n",
    "    def select_actions(self, prediction, info):\n",
    "        # Reconstruct board: +1 for current player, -1 for opponent, 0 otherwise\n",
    "        board = prediction[0][0] - prediction[0][1]\n",
    "        print(board)\n",
    "        # Default: random legal move\n",
    "        action = np.random.choice(info[\"legal_moves\"])\n",
    "\n",
    "        # Horizontal and vertical checks\n",
    "        for i in range(3):\n",
    "            # Row\n",
    "            if np.sum(board[i, :]) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                return np.ravel_multi_index((i, ind), (3, 3))\n",
    "            elif abs(np.sum(board[i, :])) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((i, ind), (3, 3))\n",
    "\n",
    "            # Column\n",
    "            if np.sum(board[:, i]) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                return np.ravel_multi_index((ind, i), (3, 3))\n",
    "            elif abs(np.sum(board[:, i])) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((ind, i), (3, 3))\n",
    "\n",
    "        # Diagonals\n",
    "        diag = board.diagonal()\n",
    "        if np.sum(diag) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, ind), (3, 3))\n",
    "        elif abs(np.sum(diag)) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, ind), (3, 3))\n",
    "\n",
    "        anti_diag = np.fliplr(board).diagonal()\n",
    "        if np.sum(anti_diag) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "        elif abs(np.sum(anti_diag)) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 402/500 [02:07<00:32,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 0.9789805334761468 8.047132734420302 0.04376915918994026 9.069881439208984\n",
      "Training Step: 19496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 404/500 [02:08<00:27,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.382751777095169 7.666744708432816 0.034938833933484514 9.084436416625977\n",
      "Training Step: 19497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 406/500 [02:08<00:22,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.5402220708115308 8.675712044889224 0.023078601911809105 10.23901081085205\n",
      "Training Step: 19498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 408/500 [02:09<00:21,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.705008872465616 7.504435320966877 0.05348969953879745 9.262932777404785\n",
      "Training Step: 19499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 409/500 [02:09<00:20,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.7297336760951083 8.347355518300901 0.015066018676199555 10.092151641845703\n",
      "Training Step: 19500\n",
      "Saving Checkpoint\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 420/500 [02:12<00:22,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unroll step 0\n",
      "predicted value tensor([0.2296], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([1.6884e-01, 1.4176e-01, 6.3912e-01, 5.2639e-03, 1.0492e-04, 4.9950e-03,\n",
      "        6.5236e-03, 3.6114e-03, 2.9786e-02], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0800, 0.1600, 0.7200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400])\n",
      "sample losses tensor([0.5934], grad_fn=<MulBackward0>) tensor(0.) tensor(0.9178, grad_fn=<NegBackward0>)\n",
      "unroll step 1\n",
      "predicted value tensor([-0.5842], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.0008], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([5.3861e-02, 2.1185e-02, 2.7938e-04, 6.4275e-03, 9.5829e-06, 1.6870e-02,\n",
      "        1.0514e-01, 1.8217e-01, 6.1406e-01], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0800, 0.0000, 0.0000, 0.0000, 0.0000, 0.1200, 0.1600, 0.6400])\n",
      "sample losses tensor([0.1729], grad_fn=<MulBackward0>) tensor([5.7282e-07], grad_fn=<PowBackward0>) tensor(1.1632, grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "predicted value tensor([0.6693], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([-0.0145], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([7.1685e-01, 2.5575e-01, 1.7471e-04, 5.3910e-03, 3.3010e-05, 9.7835e-03,\n",
      "        6.1773e-03, 5.8347e-03, 1.1935e-05], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.7200, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.0000])\n",
      "sample losses tensor([0.1094], grad_fn=<MulBackward0>) tensor([0.0002], grad_fn=<PowBackward0>) tensor(0.7727, grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "predicted value tensor([-0.9969], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([-0.0188], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([5.6978e-05, 2.9862e-02, 3.8625e-05, 4.3187e-01, 1.5112e-05, 2.1617e-01,\n",
      "        5.5424e-02, 2.6574e-01, 8.2342e-04], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.4800, 0.0000, 0.1200, 0.1200, 0.2800, 0.0000])\n",
      "sample losses tensor([9.4894e-06], grad_fn=<MulBackward0>) tensor([0.0004], grad_fn=<PowBackward0>) tensor(1.3050, grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "predicted value tensor([1.0047], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([-0.0228], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([5.5029e-05, 8.7646e-01, 2.1815e-05, 2.3675e-04, 1.1286e-03, 9.1962e-02,\n",
      "        1.2559e-02, 1.6861e-02, 7.1640e-04], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.8800, 0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000])\n",
      "sample losses tensor([2.1758e-05], grad_fn=<MulBackward0>) tensor([0.0005], grad_fn=<PowBackward0>) tensor(0.4024, grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "predicted value tensor([0.0865], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([1.0210], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(1.)\n",
      "predicted policy tensor([0.1041, 0.1056, 0.1123, 0.1078, 0.1112, 0.1069, 0.1232, 0.1186, 0.1103],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111])\n",
      "sample losses tensor([0.0075], grad_fn=<MulBackward0>) tensor([0.0004], grad_fn=<PowBackward0>) tensor(2.1986, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 421/500 [02:12<00:23,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.428133685464445 8.509321720535809 0.01954444478828421 9.957001686096191\n",
      "Training Step: 19501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 423/500 [02:13<00:20,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.310933444682492 8.12679902146192 0.036966388467649436 9.474699020385742\n",
      "Training Step: 19502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 425/500 [02:13<00:20,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.760165622836331 7.817286049292306 0.01064504792875498 9.58809757232666\n",
      "Training Step: 19503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 427/500 [02:14<00:20,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.2802201423065283 7.887582034112711 0.01442713857574085 9.182226181030273\n",
      "Training Step: 19504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 428/500 [02:14<00:21,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.663477180571178 8.356820088163659 0.026165115016080875 10.046463012695312\n",
      "Training Step: 19505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 430/500 [02:15<00:18,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.34631562793771 7.3633041919601965 0.005520519280871841 8.715140342712402\n",
      "Training Step: 19506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 432/500 [02:15<00:19,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.8985915506513251 7.4575026708698715 0.034641120688465366 9.390734672546387\n",
      "Training Step: 19507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 433/500 [02:16<00:23,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.5090985537870543 8.970156170420523 0.06892091919571869 10.548171043395996\n",
      "Training Step: 19508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 435/500 [02:16<00:19,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.1485258037751418 7.786399624754267 0.006573351108879549 8.941497802734375\n",
      "Training Step: 19509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 437/500 [02:17<00:16,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.2056949288485157 7.984938715839235 0.05122756575469967 9.241859436035156\n",
      "Training Step: 19510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 438/500 [02:17<00:17,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 0.7435094367952644 8.018978505991981 0.007485334164910599 8.769974708557129\n",
      "Training Step: 19511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 441/500 [02:18<00:14,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 0.8993861633763114 7.596389985534188 0.00797521245677979 8.503751754760742\n",
      "Training Step: 19512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 443/500 [02:18<00:13,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.1284538021735107 7.533998463652097 0.013016028025860528 8.675469398498535\n",
      "Training Step: 19513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 444/500 [02:19<00:13,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.4010892530125432 6.857556072522129 0.0019284472928834395 8.260573387145996\n",
      "Training Step: 19514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 446/500 [02:19<00:11,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.5493259182755323 7.947292871576792 0.06534046078688957 9.561958312988281\n",
      "Training Step: 19515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 448/500 [02:20<00:13,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 2.0945289943220686 8.03136102372082 0.030503526638396186 10.156394004821777\n",
      "Training Step: 19516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 450/500 [02:20<00:10,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.2180150658082902 7.846633883447794 0.025503144754411622 9.090147972106934\n",
      "Training Step: 19517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 452/500 [02:21<00:12,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.9895737983611603 7.48112910753116 0.054936359336215906 9.52563762664795\n",
      "Training Step: 19518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 454/500 [02:21<00:11,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.1202673303109854 7.664214143442223 0.009897077563127007 8.794381141662598\n",
      "Training Step: 19519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 456/500 [02:22<00:11,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.9816745429520157 7.896648661240761 0.020692259632124677 9.899015426635742\n",
      "Training Step: 19520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 457/500 [02:22<00:11,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.811029830079626 7.409418554379954 0.016808992654667194 9.237258911132812\n",
      "Training Step: 19521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 459/500 [02:22<00:10,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.7051222954308844 8.038682541504386 0.062390846761004704 9.806194305419922\n",
      "Training Step: 19522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 461/500 [02:23<00:10,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.2127873419487294 8.187517616373952 0.0027069884676789446 9.403010368347168\n",
      "Training Step: 19523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 462/500 [02:23<00:10,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.7045460001894737 8.224431575086783 0.033382396207151555 9.962362289428711\n",
      "Training Step: 19524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 464/500 [02:24<00:09,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.004788455064606 8.020301043703512 0.01800532235126795 9.043097496032715\n",
      "Training Step: 19525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 466/500 [02:24<00:09,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 2.6536475722423276 7.3547227378658135 0.007509188979110432 10.015878677368164\n",
      "Training Step: 19526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 467/500 [02:25<00:09,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.7275052344957147 7.854047432549123 0.03360947224014765 9.615161895751953\n",
      "Training Step: 19527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 470/500 [02:25<00:07,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 2.3147399547048257 8.17988294755196 0.018464241146797233 10.51308822631836\n",
      "Training Step: 19528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 471/500 [02:26<00:07,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 0.8711543622763387 8.430681710633507 0.03367738865958081 9.335514068603516\n",
      "Training Step: 19529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 472/500 [02:26<00:07,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.6898273275639113 8.904097436869051 0.06030814002408036 10.654229164123535\n",
      "Training Step: 19530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 474/500 [02:26<00:06,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.268007149443136 8.665311318407475 0.009079448896493064 9.94239616394043\n",
      "Training Step: 19531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 476/500 [02:27<00:06,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.4478558649117033 8.671462070851703 0.018363487935148082 10.13768482208252\n",
      "Training Step: 19532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 478/500 [02:28<00:06,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.3909509338238353 7.960506363735476 0.05644484780921433 9.407902717590332\n",
      "Training Step: 19533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 480/500 [02:28<00:05,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.8304960899197456 8.880853796108568 0.06880919616357695 10.780160903930664\n",
      "Training Step: 19534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 481/500 [02:28<00:05,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.525561789276277 7.369323395221727 0.012033973655082353 8.906919479370117\n",
      "Training Step: 19535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 483/500 [02:29<00:04,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.3450949434594683 8.051947015803307 0.006566448964477445 9.403608322143555\n",
      "Training Step: 19536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 485/500 [02:30<00:04,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.4173133054303695 7.664755324309226 0.00358671246947687 9.085653305053711\n",
      "Training Step: 19537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 486/500 [02:30<00:04,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.453832037197374 8.183243490275345 0.032111970872141904 9.669191360473633\n",
      "Training Step: 19538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 488/500 [02:31<00:03,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.6959041533510404 7.863974294916261 0.05824682188366648 9.618128776550293\n",
      "Training Step: 19539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 490/500 [02:31<00:02,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.2044289101439856 7.885773060130305 0.007407651349758848 9.097612380981445\n",
      "Training Step: 19540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 492/500 [02:32<00:02,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.088523434264279 8.675466101696657 0.03222390624375193 9.796215057373047\n",
      "Training Step: 19541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 494/500 [02:32<00:01,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.342499735072115 7.836370058124885 0.014007017057358087 9.192876815795898\n",
      "Training Step: 19542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 496/500 [02:33<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.603873225494271 7.290694295981666 0.030021609552143502 8.924586296081543\n",
      "Training Step: 19543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 498/500 [02:33<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.5613351436037064 7.80715021368087 0.027043398036646682 9.395528793334961\n",
      "Training Step: 19544\n",
      "Started recording episode 1499 to checkpoints/muzero_tictactoe_hyperopt-2/step_19000/videos/muzero_tictactoe_hyperopt-2/episode_001499.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [02:34<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped recording episode 1499. Recorded 6 frames.\n",
      "average score: 0.156\n",
      "Test score {'score': 0.156, 'max_score': 1, 'min_score': -1}\n",
      "Losses 1.9430123336031742 8.122265110825538 0.02560769626367254 10.090887069702148\n",
      "Training Step: 19545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:34<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 1.6567725476548052 7.810082039679401 0.007249742786176849 9.474103927612305\n",
      "Training Step: 19546\n",
      "Losses 0.7640155943672085 8.037580729680485 0.01581317637899249 8.81740951538086\n",
      "Training Step: 19547\n",
      "Losses 1.2171300614391498 7.935810570095782 0.005974214149103152 9.158920288085938\n",
      "Training Step: 19548\n",
      "Losses 1.4163666904385082 8.222698607176426 0.042307989141465774 9.681368827819824\n",
      "Training Step: 19549\n",
      "Losses 1.5870062337871005 7.58452379135997 0.058396954217055264 9.229926109313965\n",
      "Training Step: 19550\n",
      "Losses 1.2253188036916114 7.540393577270152 0.006637683252957771 8.77235221862793\n",
      "Training Step: 19551\n",
      "Losses 0.9857457185960641 8.57183543722931 0.007664591939475329 9.565242767333984\n",
      "Training Step: 19552\n",
      "Losses 1.422819176508077 8.16500855977938 0.029776635807755025 9.617602348327637\n",
      "Training Step: 19553\n",
      "Losses 1.5804467356653298 8.519526532065356 0.06718832353329732 10.167158126831055\n",
      "Training Step: 19554\n",
      "Losses 1.7865356309619704 7.400916676611814 0.0033578358584652745 9.19080924987793\n",
      "Training Step: 19555\n",
      "Losses 1.221056440679999 7.869966605634545 0.020567518968085827 9.111588478088379\n",
      "Training Step: 19556\n",
      "Losses 1.3471108790740054 7.876893747379654 0.011920517211619241 9.235926628112793\n",
      "Training Step: 19557\n",
      "Losses 1.638111739472328 7.918636532929668 0.07261682257372937 9.629365921020508\n",
      "Training Step: 19558\n",
      "Losses 1.0258480656400675 8.104334037001536 0.012624900436515762 9.14280891418457\n",
      "Training Step: 19559\n",
      "Losses 1.3677140698638866 7.767747177465935 0.008720000370389336 9.144180297851562\n",
      "Training Step: 19560\n",
      "Losses 1.143997393835368 8.000076248834375 0.03839709419768589 9.182470321655273\n",
      "Training Step: 19561\n",
      "Losses 1.5416901380487382 7.876099407912989 0.015349240992004853 9.433137893676758\n",
      "Training Step: 19562\n",
      "Losses 1.195491057564393 7.693533783101884 0.02342151327887132 8.912446975708008\n",
      "Training Step: 19563\n",
      "Losses 1.0601570996486487 8.214044215870672 0.058993162747049244 9.333197593688965\n",
      "Training Step: 19564\n",
      "Losses 0.7456917629369286 7.793887904426811 0.03982414784845767 8.579404830932617\n",
      "Training Step: 19565\n",
      "Losses 1.2891413608552975 8.312603538124677 0.007058452933268487 9.608800888061523\n",
      "Training Step: 19566\n",
      "Losses 1.3250246588227697 7.638695405170438 0.01346354642850045 8.977185249328613\n",
      "Training Step: 19567\n",
      "Losses 1.8070729759781496 8.19887282406853 0.0054721583022847575 10.011422157287598\n",
      "Training Step: 19568\n",
      "Losses 1.0762770301521343 7.802524789745803 0.016736314682328335 8.895538330078125\n",
      "Training Step: 19569\n",
      "Losses 1.6139860715521657 8.33404479123783 0.013152815445723748 9.96118450164795\n",
      "Training Step: 19570\n",
      "Losses 2.2298839272440603 7.834964474524895 0.02807933163703119 10.092931747436523\n",
      "Training Step: 19571\n",
      "Losses 1.2379517150715742 8.108696183506254 0.015327191743911561 9.361976623535156\n",
      "Training Step: 19572\n",
      "Losses 1.4402781952023282 7.769766649362282 0.009086928217011288 9.21912956237793\n",
      "Training Step: 19573\n",
      "Losses 1.35030623735247 7.941040890400473 0.009783519464118484 9.301129341125488\n",
      "Training Step: 19574\n",
      "Losses 1.7090980243567628 7.9958592500770465 0.026195025521802418 9.73115062713623\n",
      "Training Step: 19575\n",
      "Losses 1.088259594761098 7.933479070059548 0.0038650070699226546 9.025602340698242\n",
      "Training Step: 19576\n",
      "Losses 2.1328610493541778 8.159783180868544 0.0332326135232186 10.325873374938965\n",
      "Training Step: 19577\n",
      "Losses 1.140904417550594 8.767267969873501 0.021889563302961587 9.93006420135498\n",
      "Training Step: 19578\n",
      "Losses 0.874018506621254 8.035799366582069 0.015200171537704948 8.925017356872559\n",
      "Training Step: 19579\n",
      "Losses 1.3619158505904245 7.148791890642315 0.027426730113531583 8.53813362121582\n",
      "Training Step: 19580\n",
      "Losses 2.073333777892426 7.716738885290397 0.03376386926877658 9.823837280273438\n",
      "Training Step: 19581\n",
      "Losses 1.1989866421076925 8.19967955998436 0.03268974075662236 9.431354522705078\n",
      "Training Step: 19582\n",
      "Losses 1.5157170615322686 7.959159259538865 0.01438294442242427 9.4892578125\n",
      "Training Step: 19583\n",
      "Losses 1.48247362752624 8.414301228054683 0.025338445906841825 9.922109603881836\n",
      "Training Step: 19584\n",
      "Losses 1.5720263122323885 7.725721208727919 0.0243724778986234 9.32211971282959\n",
      "Training Step: 19585\n",
      "Losses 1.3060377079196854 7.01050770139409 0.006794458095358849 8.323338508605957\n",
      "Training Step: 19586\n",
      "Losses 2.0238760388095423 7.788033486947825 0.0072796449498326865 9.81919002532959\n",
      "Training Step: 19587\n",
      "Losses 1.8241615892607723 7.514590973718441 0.006691288443439525 9.345443725585938\n",
      "Training Step: 19588\n",
      "Losses 2.5155769527577077 7.884728151009767 0.023417390278244632 10.423726081848145\n",
      "Training Step: 19589\n",
      "Losses 1.4921995805675503 7.354686615995888 0.07135135888128041 8.918238639831543\n",
      "Training Step: 19590\n",
      "Losses 1.9203616661608047 7.245713433971105 0.037731270608533 9.20380687713623\n",
      "Training Step: 19591\n",
      "Losses 1.042463429853517 7.913502443880134 0.028934750066085257 8.984905242919922\n",
      "Training Step: 19592\n",
      "Losses 1.4587652972239762 8.787368120531028 0.04396551094420542 10.290103912353516\n",
      "Training Step: 19593\n",
      "Losses 1.6281767692262994 7.767887008740217 0.004546052482407303 9.400607109069824\n",
      "Training Step: 19594\n",
      "Losses 2.027673355592298 7.683951869432349 0.026282404829669925 9.737909317016602\n",
      "Training Step: 19595\n",
      "Losses 0.9452435162022622 8.50794373681856 0.004288798436532321 9.457473754882812\n",
      "Training Step: 19596\n",
      "Losses 1.0833862940737582 8.058361557858007 0.017764703163423157 9.159512519836426\n",
      "Training Step: 19597\n",
      "Losses 1.8770529258618152 7.755717794367229 0.009564017768855869 9.642333030700684\n",
      "Training Step: 19598\n",
      "Losses 1.2522377224811714 7.248755936481757 0.026220888897919115 8.527215957641602\n",
      "Training Step: 19599\n",
      "Losses 1.2391380149495355 8.462208432545594 0.005484152251867047 9.706830024719238\n",
      "Training Step: 19600\n",
      "Saving Checkpoint\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "unroll step 0\n",
      "predicted value tensor([-0.9265], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([4.4257e-04, 4.8504e-04, 5.0749e-01, 1.5821e-01, 6.1313e-04, 2.1371e-02,\n",
      "        4.7376e-03, 4.2194e-03, 3.0242e-01], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.6000, 0.0800, 0.0000, 0.0000, 0.0000, 0.0000, 0.3200])\n",
      "sample losses tensor([0.0054], grad_fn=<MulBackward0>) tensor(0.) tensor(0.9372, grad_fn=<NegBackward0>)\n",
      "unroll step 1\n",
      "predicted value tensor([0.9791], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.0100], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([1.8037e-05, 6.0812e-05, 9.6319e-01, 1.3431e-02, 1.9978e-05, 9.4875e-03,\n",
      "        6.1139e-03, 7.4576e-03, 2.1871e-04], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.9600, 0.0000, 0.0000, 0.0400, 0.0000, 0.0000, 0.0000])\n",
      "sample losses tensor([0.0004], grad_fn=<MulBackward0>) tensor([9.9444e-05], grad_fn=<PowBackward0>) tensor(0.2223, grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "predicted value tensor([-0.7157], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([-5.8498e-05], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([6.3423e-04, 5.5108e-03, 2.1410e-04, 4.6043e-01, 1.0633e-04, 2.2497e-01,\n",
      "        1.4771e-01, 1.5934e-01, 1.0754e-03], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.2400, 0.1200, 0.2400, 0.0000])\n",
      "sample losses tensor([0.0808], grad_fn=<MulBackward0>) tensor([3.4220e-09], grad_fn=<PowBackward0>) tensor(1.3386, grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "predicted value tensor([1.0246], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([-0.0021], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([7.7907e-05, 7.6281e-04, 4.0561e-05, 4.4307e-04, 3.8306e-05, 2.9247e-02,\n",
      "        9.4192e-01, 2.7388e-02, 7.9378e-05], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "sample losses tensor([0.0006], grad_fn=<MulBackward0>) tensor([4.3940e-06], grad_fn=<PowBackward0>) tensor(0.0598, grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "predicted value tensor([-0.0020], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.9769], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(1.)\n",
      "predicted policy tensor([0.0948, 0.1074, 0.1038, 0.1216, 0.1248, 0.1200, 0.1089, 0.1073, 0.1113],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111])\n",
      "sample losses tensor([3.8203e-06], grad_fn=<MulBackward0>) tensor([0.0005], grad_fn=<PowBackward0>) tensor(2.2005, grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "predicted value tensor([0.0135], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([-0.0108], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0997, 0.1096, 0.1100, 0.1178, 0.1128, 0.1164, 0.1114, 0.1061, 0.1162],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111])\n",
      "sample losses tensor([0.0002], grad_fn=<MulBackward0>) tensor([0.0001], grad_fn=<PowBackward0>) tensor(2.1984, grad_fn=<NegBackward0>)\n",
      "Losses 2.0314823603048264 9.084618763103208 0.017085274046596273 11.133188247680664\n",
      "Training Step: 19601\n",
      "Losses 0.7524672814415405 7.779311884296476 0.008666746252607993 8.540446281433105\n",
      "Training Step: 19602\n",
      "Losses 1.5359777068099418 7.260089625284309 0.011404529608654707 8.807473182678223\n",
      "Training Step: 19603\n",
      "Losses 0.8377020951500131 8.813626071838371 0.062959868972843 9.714287757873535\n",
      "Training Step: 19604\n",
      "Losses 0.7152556661929287 7.923128444439499 0.006685024855211129 8.645071983337402\n",
      "Training Step: 19605\n",
      "Losses 2.1390398289671397 8.319579497881932 0.006723955736728904 10.465344429016113\n",
      "Training Step: 19606\n",
      "Losses 1.603399690050809 7.958118889167963 0.0659517567455905 9.627467155456543\n",
      "Training Step: 19607\n",
      "Losses 1.2215204101376287 8.443454948552244 0.01168994613444152 9.676661491394043\n",
      "Training Step: 19608\n",
      "Losses 2.4166176641590926 7.580051268450916 0.0069762614243242815 10.003645896911621\n",
      "Training Step: 19609\n",
      "Losses 1.5485223044362306 7.157083335900097 0.007775974335195723 8.713382720947266\n",
      "Training Step: 19610\n",
      "Losses 1.0009990211368427 8.094511092600442 0.023612772357103518 9.119121551513672\n",
      "Training Step: 19611\n",
      "Losses 1.0882758211954044 8.09914107197983 0.013289536105926203 9.20070743560791\n",
      "Training Step: 19612\n",
      "Losses 1.5273789284208845 7.212090106215328 0.011262104060338451 8.750730514526367\n",
      "Training Step: 19613\n",
      "Losses 1.849208782537815 7.62094768955285 0.04170368079828091 9.511861801147461\n",
      "Training Step: 19614\n",
      "Losses 1.3812862277316267 8.400891937235428 0.01238976985564283 9.794564247131348\n",
      "Training Step: 19615\n",
      "Losses 1.219480402724804 7.52993007830446 0.037527397824196174 8.78693962097168\n",
      "Training Step: 19616\n",
      "Losses 2.3984279616074753 7.270343070056697 0.02687046185891262 9.695638656616211\n",
      "Training Step: 19617\n",
      "Losses 1.0178393342266745 8.286660241567006 0.0063795387082521415 9.31087875366211\n",
      "Training Step: 19618\n",
      "Losses 1.8425590602232176 8.56207386762253 0.014817868337871937 10.419449806213379\n",
      "Training Step: 19619\n",
      "Losses 1.328165049178121 7.948024763572903 0.025104674785608294 9.301295280456543\n",
      "Training Step: 19620\n",
      "Losses 1.854773280705249 7.818623194310931 0.034265171858515076 9.707662582397461\n",
      "Training Step: 19621\n",
      "Losses 2.208659924989007 7.493982737134502 0.04077481446834552 9.743416786193848\n",
      "Training Step: 19622\n",
      "Losses 1.5230280806866006 8.016399159867433 0.01378333283160793 9.553211212158203\n",
      "Training Step: 19623\n",
      "Losses 1.424246379023483 8.46079204716807 0.0060553792647465195 9.891096115112305\n",
      "Training Step: 19624\n",
      "Losses 1.126512991588985 8.059010235592723 0.015964966138634296 9.20148754119873\n",
      "Training Step: 19625\n",
      "Losses 1.5115515202872984 8.069226326217176 0.004567786438261168 9.585346221923828\n",
      "Training Step: 19626\n",
      "Losses 0.9693690369558823 7.68969817981997 0.014496022290740984 8.67356014251709\n",
      "Training Step: 19627\n",
      "Losses 2.00789224377235 7.861649300353747 0.028666841317089964 9.898207664489746\n",
      "Training Step: 19628\n",
      "Losses 1.1855681304245214 8.377416784889647 0.026883280881404792 9.58987045288086\n",
      "Training Step: 19629\n",
      "Losses 1.8748627886568272 7.6219306761049666 0.01876107953676512 9.515554428100586\n",
      "Training Step: 19630\n",
      "Losses 0.8292021969517871 8.42187752207974 0.02559913140247949 9.276676177978516\n",
      "Training Step: 19631\n",
      "Losses 1.7045875366802554 7.79032196715707 0.004797245285554107 9.499709129333496\n",
      "Training Step: 19632\n",
      "Losses 0.995414711456279 8.061909851967357 0.01326012930370548 9.07058334350586\n",
      "Training Step: 19633\n",
      "Losses 1.0294170770924884 7.879389633075334 0.0020808193841492795 8.910888671875\n",
      "Training Step: 19634\n",
      "Losses 1.7459114657715085 7.767555580219778 0.007801742112128474 9.521268844604492\n",
      "Training Step: 19635\n",
      "Losses 1.3096207491169816 8.005239501595497 0.003240360658250605 9.318099021911621\n",
      "Training Step: 19636\n",
      "Losses 1.9665307533315284 8.03272619213385 0.061903974971549425 10.061162948608398\n",
      "Training Step: 19637\n",
      "Losses 1.652111035865822 8.307594190460804 0.008991027255474175 9.968701362609863\n",
      "Training Step: 19638\n",
      "Losses 1.3246627850585533 7.876628183301364 0.030416339758843502 9.231708526611328\n",
      "Training Step: 19639\n",
      "Losses 1.629851538907701 8.173514758760575 0.0048651653419998775 9.808231353759766\n",
      "Training Step: 19640\n",
      "Losses 1.414208406532964 8.062011675683607 0.016152373961423105 9.492372512817383\n",
      "Training Step: 19641\n",
      "Losses 1.581593096702413 8.113932829335681 0.03770276595891492 9.733224868774414\n",
      "Training Step: 19642\n",
      "Losses 1.173644212511857 8.8749408354779 0.020862076541970337 10.069446563720703\n",
      "Training Step: 19643\n",
      "Losses 2.0482152025873006 7.550764158018865 0.01842568245861509 9.617405891418457\n",
      "Training Step: 19644\n",
      "Losses 1.4636005692048144 7.811998832905374 0.05639697084435946 9.331998825073242\n",
      "Training Step: 19645\n",
      "Losses 1.0512810590042871 7.965213197610865 0.008844893794652309 9.025339126586914\n",
      "Training Step: 19646\n",
      "Losses 0.7036520907303228 8.02950542216422 0.011287118879240887 8.744443893432617\n",
      "Training Step: 19647\n",
      "Losses 1.363773462339374 7.831906251951295 0.03287146390339163 9.22854995727539\n",
      "Training Step: 19648\n",
      "Losses 1.1809052792569867 8.611528995221306 0.011330675414370855 9.803766250610352\n",
      "Training Step: 19649\n",
      "Losses 1.5261827618871493 8.062902548495913 0.003003790093861425 9.592093467712402\n",
      "Training Step: 19650\n",
      "Losses 1.1390578148660673 7.858478425107023 0.005962302048861279 9.003494262695312\n",
      "Training Step: 19651\n",
      "Losses 0.5712468786687528 8.385150807356695 0.024916209151895963 8.981315612792969\n",
      "Training Step: 19652\n",
      "Losses 0.9539399895206149 8.467767364301835 0.09454987254010483 9.51625919342041\n",
      "Training Step: 19653\n",
      "Losses 1.3024871193795793 8.07650118098536 0.00796910084725596 9.386958122253418\n",
      "Training Step: 19654\n",
      "Losses 1.3274967753084583 7.651498580788029 0.0027514486875532818 8.9817476272583\n",
      "Training Step: 19655\n",
      "Losses 1.8387187257820217 7.984994218400971 0.057337124569651676 9.88105297088623\n",
      "Training Step: 19656\n",
      "Losses 0.6774494855864552 7.3264381971239345 0.03437942953525275 8.038267135620117\n",
      "Training Step: 19657\n",
      "Losses 1.1707747157650217 7.8084459607853205 0.004621253811316284 8.983842849731445\n",
      "Training Step: 19658\n",
      "Losses 1.445076002421212 8.068680186392157 0.002868578730893523 9.516624450683594\n",
      "Training Step: 19659\n",
      "Losses 0.7864501043203358 8.378092418788583 0.0028818667641626883 9.167424201965332\n",
      "Training Step: 19660\n",
      "Losses 0.6849928421956235 8.28022826227243 0.014172416687513678 8.97939395904541\n",
      "Training Step: 19661\n",
      "Losses 1.56873863554334 8.465471599316515 0.01526384928267982 10.049477577209473\n",
      "Training Step: 19662\n",
      "Losses 1.727438418758943 8.225912442088884 0.03122715372339571 9.984580993652344\n",
      "Training Step: 19663\n",
      "Losses 1.1850363680156484 8.264658019092167 0.00373194393544396 9.4534273147583\n",
      "Training Step: 19664\n",
      "Losses 1.7596293782207875 8.80923817398434 0.03088366504116996 10.599747657775879\n",
      "Training Step: 19665\n",
      "Losses 0.9649823567869316 8.108213602623437 0.013187344852037583 9.086384773254395\n",
      "Training Step: 19666\n",
      "Losses 1.0672083160946144 8.301765034892014 0.012405722679345699 9.381377220153809\n",
      "Training Step: 19667\n",
      "Losses 1.3310888236155733 7.987670667571365 0.017773044070779773 9.336531639099121\n",
      "Training Step: 19668\n",
      "Losses 1.8947243318673375 8.051215432606114 0.05723472868209001 10.00317668914795\n",
      "Training Step: 19669\n",
      "Losses 0.7870504045928596 7.914937367102539 0.00512969074862113 8.707118034362793\n",
      "Training Step: 19670\n",
      "Losses 1.5976707073780063 7.600345418228244 0.006588646875156758 9.204607009887695\n",
      "Training Step: 19671\n",
      "Losses 1.1783980982500388 7.5175641974346945 0.006904381176754515 8.702866554260254\n",
      "Training Step: 19672\n",
      "Losses 1.6917567957116937 8.630075899680378 0.032782992053942195 10.354613304138184\n",
      "Training Step: 19673\n",
      "Losses 1.2796111969240274 8.192934413076728 0.029731705013734122 9.502276420593262\n",
      "Training Step: 19674\n",
      "Losses 1.9780964407456025 8.298521186014113 0.021920817326069292 10.298537254333496\n",
      "Training Step: 19675\n",
      "Losses 1.9811594852550272 7.035813583090203 0.030022981535080318 9.046998977661133\n",
      "Training Step: 19676\n",
      "Losses 1.4631715482078578 8.963783414263162 0.024504178326829607 10.451456069946289\n",
      "Training Step: 19677\n",
      "Losses 1.1130719142309036 7.933295709684899 0.023796643745272128 9.07016372680664\n",
      "Training Step: 19678\n",
      "Losses 1.7834382541438423 7.831758639295003 0.008610983515433546 9.623808860778809\n",
      "Training Step: 19679\n",
      "Losses 0.9728865190853222 8.773135685609304 0.005008828870109561 9.751030921936035\n",
      "Training Step: 19680\n",
      "Losses 0.9171830003346493 8.135683548891393 0.009862742063917657 9.062727928161621\n",
      "Training Step: 19681\n",
      "Losses 1.4270053241528657 8.445453590727993 0.007756890857479881 9.880217552185059\n",
      "Training Step: 19682\n",
      "Losses 1.4858881705107492 7.860519424335507 0.009820746923749922 9.356230735778809\n",
      "Training Step: 19683\n",
      "Losses 1.438812492861885 7.413289917036309 0.0026866055712245207 8.854788780212402\n",
      "Training Step: 19684\n",
      "Losses 1.574533611954564 8.06713326220779 0.00683287933850496 9.648497581481934\n",
      "Training Step: 19685\n",
      "Losses 1.4004586043663179 8.935093120271631 0.06086677131753743 10.396417617797852\n",
      "Training Step: 19686\n",
      "Losses 1.3776314427685632 8.340586099140637 0.07511056641049221 9.793327331542969\n",
      "Training Step: 19687\n",
      "Losses 1.7899470310452905 8.294585187759367 0.06029811899119919 10.144830703735352\n",
      "Training Step: 19688\n",
      "Losses 1.414103171157762 7.538645490974886 0.010848130387906794 8.963593482971191\n",
      "Training Step: 19689\n",
      "Losses 1.306235098359644 8.611809683170577 0.03422601129379377 9.952268600463867\n",
      "Training Step: 19690\n",
      "Losses 1.340029485713473 8.283176328259287 0.07857321402506906 9.701781272888184\n",
      "Training Step: 19691\n",
      "Losses 1.0137504729617681 8.322196087676275 0.01849826244865027 9.354442596435547\n",
      "Training Step: 19692\n",
      "Losses 1.4061962715034397 8.483392707596067 0.008135747957809358 9.897722244262695\n",
      "Training Step: 19693\n",
      "Losses 1.376065612479966 7.70683436771651 0.028874267703278278 9.111775398254395\n",
      "Training Step: 19694\n",
      "Losses 1.3368950098971988 7.3517659205135715 0.014092894000410958 8.702753067016602\n",
      "Training Step: 19695\n",
      "Losses 2.2900276422392043 8.000569826341234 0.046181169311514864 10.336777687072754\n",
      "Training Step: 19696\n",
      "Losses 1.638900440427573 7.828784376892145 0.015549271354997285 9.483233451843262\n",
      "Training Step: 19697\n",
      "Losses 1.145794916561682 7.972982635059452 0.011085209118304462 9.129865646362305\n",
      "Training Step: 19698\n",
      "Losses 0.7173171697790579 8.381793544991524 0.03853529017356472 9.13764762878418\n",
      "Training Step: 19699\n",
      "Losses 1.4434898525793187 7.820056286669569 0.019961931121833087 9.28350830078125\n",
      "Training Step: 19700\n",
      "Saving Checkpoint\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "unroll step 0\n",
      "predicted value tensor([0.2997], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([1.3652e-01, 1.0566e-01, 7.1632e-01, 3.8624e-03, 5.6543e-05, 4.4798e-03,\n",
      "        5.1107e-03, 3.8332e-03, 2.4158e-02], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0800, 0.0800, 0.8000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400])\n",
      "sample losses tensor([1.6891], grad_fn=<MulBackward0>) tensor(0.) tensor(0.7549, grad_fn=<NegBackward0>)\n",
      "unroll step 1\n",
      "predicted value tensor([-0.5689], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.0154], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([6.7530e-02, 2.7478e-02, 3.9819e-04, 1.1588e-02, 5.2266e-06, 1.2375e-02,\n",
      "        1.1793e-01, 1.8636e-01, 5.7633e-01], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.6000])\n",
      "sample losses tensor([2.4613], grad_fn=<MulBackward0>) tensor([0.0002], grad_fn=<PowBackward0>) tensor(1.0942, grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "predicted value tensor([0.5241], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([-0.0086], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([5.9656e-01, 3.6939e-01, 2.6584e-05, 1.0392e-02, 8.8191e-06, 7.1028e-03,\n",
      "        1.3615e-04, 8.7725e-03, 7.6203e-03], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.6800, 0.2800, 0.0000, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "sample losses tensor([2.3229], grad_fn=<MulBackward0>) tensor([7.3718e-05], grad_fn=<PowBackward0>) tensor(0.8128, grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "predicted value tensor([0.4622], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([-0.0102], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([2.3649e-01, 8.5871e-02, 2.3122e-04, 2.5451e-04, 1.3320e-03, 3.7388e-02,\n",
      "        1.4725e-02, 3.3135e-01, 2.9236e-01], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.4800, 0.0800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2800, 0.1600])\n",
      "sample losses tensor([0.2893], grad_fn=<MulBackward0>) tensor([0.0001], grad_fn=<PowBackward0>) tensor(1.3945, grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "predicted value tensor([-0.6066], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.0448], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([7.2155e-04, 4.1398e-01, 5.9983e-03, 5.4391e-03, 2.8611e-04, 1.7663e-01,\n",
      "        2.4250e-03, 2.3482e-02, 3.7103e-01], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.0000, 0.4000])\n",
      "sample losses tensor([0.1548], grad_fn=<MulBackward0>) tensor([0.0020], grad_fn=<PowBackward0>) tensor(1.1302, grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "predicted value tensor([0.6658], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([-0.0463], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([5.2492e-04, 2.3965e-04, 1.9600e-03, 5.2076e-04, 1.1659e-05, 4.7828e-02,\n",
      "        1.2726e-02, 4.8599e-02, 8.8759e-01], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.8800])\n",
      "sample losses tensor([0.1117], grad_fn=<MulBackward0>) tensor([0.0021], grad_fn=<PowBackward0>) tensor(0.4698, grad_fn=<NegBackward0>)\n",
      "Losses 1.5785898495429012 7.5807822762435535 0.004497395921773117 9.16386890411377\n",
      "Training Step: 19701\n",
      "Losses 1.4798865629970956 7.284104650381778 0.01172078926340625 8.775710105895996\n",
      "Training Step: 19702\n",
      "Losses 2.09784149121793 8.117322971760586 0.043317472958501524 10.258482933044434\n",
      "Training Step: 19703\n",
      "Losses 2.0461300494429704 7.684923281500232 0.08316539258078473 9.814215660095215\n",
      "Training Step: 19704\n",
      "Losses 1.9886538617340608 7.973874299845193 0.05045635828665829 10.012983322143555\n",
      "Training Step: 19705\n",
      "Losses 1.3313639903919299 8.172392142514582 0.044221679611226694 9.54797649383545\n",
      "Training Step: 19706\n",
      "Losses 1.8911424161974697 7.535519949517038 0.0035177129486982928 9.43017864227295\n",
      "Training Step: 19707\n",
      "Losses 1.0607053275126384 8.323616317014967 0.03021606295188492 9.41454029083252\n",
      "Training Step: 19708\n",
      "Losses 1.0847370433460928 7.802288576698629 0.024261239917434452 8.911286354064941\n",
      "Training Step: 19709\n",
      "Losses 1.636196412149586 8.276772670200444 0.021227224375110376 9.934197425842285\n",
      "Training Step: 19710\n",
      "Losses 1.7822972912349784 9.184405927240732 0.043791753901375685 11.010493278503418\n",
      "Training Step: 19711\n",
      "Losses 1.3351503345433273 8.074574314960046 0.03830152763740141 9.448028564453125\n",
      "Training Step: 19712\n",
      "Losses 1.609342017483917 8.61911943287123 0.01024540027037342 10.238709449768066\n",
      "Training Step: 19713\n",
      "Losses 1.390140611583277 8.068647170504846 0.008354535005788533 9.467142105102539\n",
      "Training Step: 19714\n",
      "Losses 1.74584581738995 7.276218118990073 0.018200940489549455 9.040266990661621\n",
      "Training Step: 19715\n",
      "Losses 0.8848387123007377 8.435630673244304 0.009254545068236891 9.32972240447998\n",
      "Training Step: 19716\n",
      "Losses 2.049969942473774 7.493349631018646 0.007114392736672748 9.550433158874512\n",
      "Training Step: 19717\n",
      "Losses 1.1880065068260537 8.649660439506988 0.04100024248868363 9.878667831420898\n",
      "Training Step: 19718\n",
      "Losses 0.871994005795159 8.062675060478796 0.010691127096585873 8.945357322692871\n",
      "Training Step: 19719\n",
      "Losses 1.9661674830598628 7.635398888247437 0.0342927921031313 9.635856628417969\n",
      "Training Step: 19720\n",
      "Losses 1.5252250652586916 7.9895468751346925 0.041217398444959086 9.555989265441895\n",
      "Training Step: 19721\n",
      "Losses 1.4443461669463993 8.87728049976431 0.03011388899380496 10.351738929748535\n",
      "Training Step: 19722\n",
      "Losses 1.1710471977306862 8.742880272417096 0.17297838866760518 10.086905479431152\n",
      "Training Step: 19723\n",
      "Losses 1.151902697195133 8.42131271972903 0.026200127171204568 9.599414825439453\n",
      "Training Step: 19724\n",
      "Losses 1.1038323674251322 7.497289564053062 0.008519131208021058 8.609642028808594\n",
      "Training Step: 19725\n",
      "Losses 1.2417596295026176 7.87158862031356 0.01568842952321875 9.12903881072998\n",
      "Training Step: 19726\n",
      "Losses 1.0834196197769046 7.357233833477949 0.015847140748888933 8.456501007080078\n",
      "Training Step: 19727\n",
      "Losses 1.8364758264864065 7.574772550156922 0.007842717121924109 9.419092178344727\n",
      "Training Step: 19728\n",
      "Losses 1.7495860477343843 7.8131187097824295 0.00925686731137243 9.5719633102417\n",
      "Training Step: 19729\n",
      "Losses 2.3898752136880717 7.276145962285227 0.0054285588332585455 9.671451568603516\n",
      "Training Step: 19730\n",
      "Losses 1.6614400198761499 8.770432405814063 0.02601581653901608 10.457890510559082\n",
      "Training Step: 19731\n",
      "Losses 1.1322305035316775 8.109031138010323 0.034775405160235834 9.27603816986084\n",
      "Training Step: 19732\n",
      "Losses 1.064135091423239 8.379222870920785 0.028953204794200438 9.472311973571777\n",
      "Training Step: 19733\n",
      "Losses 1.1775919953372807 7.673539447008807 0.00861432042730817 8.859748840332031\n",
      "Training Step: 19734\n",
      "Losses 1.5283568236649785 8.522309922271234 0.013374005218455706 10.064043045043945\n",
      "Training Step: 19735\n",
      "Losses 1.452850187493541 7.450101417467522 0.051221061853335303 8.954171180725098\n",
      "Training Step: 19736\n",
      "Losses 1.4835750585859468 7.541729701417353 0.022106019214894523 9.047410011291504\n",
      "Training Step: 19737\n",
      "Losses 1.217866448732968 8.349880397148809 0.02233495935149596 9.590078353881836\n",
      "Training Step: 19738\n",
      "Losses 1.2501984493307021 8.262824953417294 0.00948463589885895 9.522504806518555\n",
      "Training Step: 19739\n",
      "Losses 0.7352933531628469 8.183873228685115 0.03254727934508722 8.951715469360352\n",
      "Training Step: 19740\n",
      "Losses 1.2832129984482807 8.12821754347533 0.02634551025650052 9.437775611877441\n",
      "Training Step: 19741\n",
      "Losses 1.3534557790889004 8.140720295283245 0.007940673500031803 9.502119064331055\n",
      "Training Step: 19742\n",
      "Losses 1.197667847701176 8.121291947376449 0.015053300343765275 9.334013938903809\n",
      "Training Step: 19743\n",
      "Losses 0.8642189987286468 8.731920146292396 0.050142787514490195 9.646282196044922\n",
      "Training Step: 19744\n",
      "Losses 1.4247822276598825 8.133992468305223 0.03446510154623006 9.593241691589355\n",
      "Training Step: 19745\n",
      "Losses 1.7129823234617056 7.8922472083431785 0.042512447739149015 9.647741317749023\n",
      "Training Step: 19746\n",
      "Losses 1.4971612268577568 7.4200251262918755 0.018449934847341733 8.935637474060059\n",
      "Training Step: 19747\n",
      "Losses 0.7736490663357032 7.631439616823627 0.02958433649597092 8.434672355651855\n",
      "Training Step: 19748\n",
      "Losses 1.2425976614867935 7.1725680906711204 0.013489675734075834 8.428657531738281\n",
      "Training Step: 19749\n",
      "Losses 1.2229007311743372 8.748891034527333 0.029170717033139157 10.000959396362305\n",
      "Training Step: 19750\n",
      "Losses 1.9051076601324581 8.16585132246837 0.03748061675809744 10.108443260192871\n",
      "Training Step: 19751\n",
      "Losses 1.254990378113263 8.10812693353364 0.042548569191978025 9.40566349029541\n",
      "Training Step: 19752\n",
      "Losses 0.9706280595558425 7.676725944940699 0.037427273061015454 8.684782028198242\n",
      "Training Step: 19753\n",
      "Losses 2.589171175260713 7.497805856903142 0.022056058570866233 10.109030723571777\n",
      "Training Step: 19754\n",
      "Losses 1.6929360987529005 8.434044796995295 0.009422879671125817 10.136404991149902\n",
      "Training Step: 19755\n",
      "Losses 1.6017790065005801 8.418630275547912 0.06470631558755136 10.085111618041992\n",
      "Training Step: 19756\n",
      "Losses 1.6827117897810524 7.699672861846921 0.010512353231102445 9.39289665222168\n",
      "Training Step: 19757\n",
      "Losses 1.5851260773690217 7.9861328650440555 0.043409299903480525 9.614663124084473\n",
      "Training Step: 19758\n",
      "Losses 1.678367488907421 7.51038639764738 0.00953420639413638 9.198288917541504\n",
      "Training Step: 19759\n",
      "Losses 1.633802120540464 8.05406950791803 0.030491879615727413 9.718361854553223\n",
      "Training Step: 19760\n",
      "Losses 1.6116828837574833 8.553246633200615 0.012690555078212928 10.177618026733398\n",
      "Training Step: 19761\n",
      "Losses 1.0454554788877601 7.899989191297209 0.03490330224734717 8.980350494384766\n",
      "Training Step: 19762\n",
      "Losses 1.3711115380668168 7.999970131291775 0.00788100009697884 9.378963470458984\n",
      "Training Step: 19763\n",
      "Losses 1.2983327355300673 8.00984603408142 0.0034497092209023056 9.311628341674805\n",
      "Training Step: 19764\n",
      "Losses 1.212655430180955 7.849813826978789 0.008774977507517434 9.071240425109863\n",
      "Training Step: 19765\n",
      "Losses 1.7786684001370645 7.540018548294029 0.011267716844504116 9.329954147338867\n",
      "Training Step: 19766\n",
      "Losses 1.3194645778257836 7.9799695009569405 0.013005059714630562 9.312438011169434\n",
      "Training Step: 19767\n",
      "Losses 1.0058950303042593 7.9413325123096 0.018594037532686958 8.96582317352295\n",
      "Training Step: 19768\n",
      "Losses 1.797184695936029 8.222170028238907 0.03240816162517657 10.051763534545898\n",
      "Training Step: 19769\n",
      "Losses 0.9620252233615152 7.870367249499395 0.040253417043174755 8.872642517089844\n",
      "Training Step: 19770\n",
      "Losses 1.4544292562777497 8.42184584903589 0.03227931692572583 9.908549308776855\n",
      "Training Step: 19771\n",
      "Losses 1.7163999704193558 7.599685270615737 0.006683367826819416 9.322771072387695\n",
      "Training Step: 19772\n",
      "Losses 1.4004807219687299 8.557699000019056 0.02114975838601385 9.979328155517578\n",
      "Training Step: 19773\n",
      "Losses 1.73007641940785 7.281991489500797 0.09589577709866637 9.107965469360352\n",
      "Training Step: 19774\n",
      "Losses 0.8423674470511925 7.7246659458724025 0.004458675802313017 8.571491241455078\n",
      "Training Step: 19775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 707, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 643, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 392, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 629, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../muzero/muzero_network.py\", line 809, in recurrent_inference\n",
      "    reward, hidden_state = self.dynamics(nn_input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../muzero/muzero_network.py\", line 356, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../modules/residual.py\", line 151, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 707, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 643, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 392, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py\", line 629, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../muzero/muzero_network.py\", line 809, in recurrent_inference\n",
      "    reward, hidden_state = self.dynamics(nn_input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../muzero/muzero_network.py\", line 356, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/muzero/../modules/residual.py\", line 148, in forward\n",
      "    residual = self.downsample(inputs) if self.downsample else inputs\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    118\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py:287\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_replay_buffer_size:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[0;32m--> 287\u001b[0m         value_loss, policy_loss, reward_loss, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/muzero/muzero_agent_torch.py:599\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    597\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 599\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    601\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 2.0,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 33000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 40000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt-2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],  # RandomAgent(),\n",
    ")\n",
    "\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 500\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.training_steps += 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Box(0, 1, (3, 3, 2), int8)\n",
      "Box(0, 1, (3, 3, 8), int8)\n",
      "Box(0, 1, (3, 3, 9), int8)\n",
      "Box(0, 1, (9, 3, 3), int8)\n",
      "MCTS Prediction (tensor([0.0000, 0.1600, 0.8400, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.1600, 0.8400, 0.0000, 0.0000, 0.0000, 0.0000]), [1, 3, 4, 5, 7, 8], 0.1852449732180685)\n",
      "Initial Value tensor([-0.5853], grad_fn=<SelectBackward0>)\n",
      "Initial Policy tensor([1.0743e-04, 7.6787e-02, 2.1165e-04, 4.6148e-01, 3.6692e-01, 6.7124e-02,\n",
      "        7.2198e-04, 9.0125e-03, 1.7639e-02], grad_fn=<SelectBackward0>)\n",
      "Move 1\n",
      "Reccurent Value tensor([0.4830], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([0.0614], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([1.0179e-05, 1.3826e-04, 3.9588e-04, 1.3460e-01, 5.3352e-01, 2.8216e-01,\n",
      "        1.0824e-04, 3.7477e-03, 4.5313e-02], grad_fn=<SelectBackward0>)\n",
      "Move 3\n",
      "Reccurent Value tensor([0.8411], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([0.0061], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([9.2868e-06, 6.9540e-03, 2.2366e-05, 7.0267e-05, 8.4098e-01, 1.1484e-01,\n",
      "        2.2204e-04, 1.2128e-02, 2.4779e-02], grad_fn=<SelectBackward0>)\n",
      "Move 4\n",
      "Reccurent Value tensor([0.1026], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0765], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([6.7234e-05, 4.4744e-02, 1.0117e-03, 8.1864e-01, 1.0513e-03, 3.9506e-02,\n",
      "        8.4139e-05, 1.5267e-02, 7.9628e-02], grad_fn=<SelectBackward0>)\n",
      "Move 5\n",
      "Reccurent Value tensor([0.4335], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0176], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([3.6313e-06, 1.4360e-02, 1.6887e-04, 8.3490e-01, 4.5672e-02, 2.2155e-04,\n",
      "        1.7460e-03, 7.8795e-02, 2.4129e-02], grad_fn=<SelectBackward0>)\n",
      "Move 7\n",
      "Reccurent Value tensor([0.5162], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([0.0665], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([7.7177e-05, 3.4801e-02, 1.3421e-04, 5.5638e-01, 1.0394e-01, 2.7490e-01,\n",
      "        1.2527e-03, 5.8474e-04, 2.7932e-02], grad_fn=<SelectBackward0>)\n",
      "Move 8\n",
      "Reccurent Value tensor([0.7309], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0224], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([8.5037e-05, 9.6053e-02, 1.2601e-04, 1.9913e-02, 4.3379e-02, 7.9121e-01,\n",
      "        6.4972e-04, 4.8574e-02, 6.4352e-06], grad_fn=<SelectBackward0>)\n",
      "{0: 0, 1: 0, 2: 0, 3: 20, 4: 80, 5: 0, 6: 0, 7: 0, 8: 0}\n",
      "Action 4\n",
      "MCTS Prediction (tensor([0.0000, 0.1200, 0.2000, 0.4800, 0.2000]), tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.2000, 0.0000, 0.4800, 0.2000]), [1, 3, 5, 7, 8], 0.3695822191890329)\n",
      "Initial Value tensor([0.4775], grad_fn=<SelectBackward0>)\n",
      "Initial Policy tensor([1.0696e-04, 3.0497e-02, 1.4053e-04, 3.5398e-01, 1.4733e-06, 3.0001e-01,\n",
      "        2.9484e-04, 2.5103e-02, 2.8986e-01], grad_fn=<SelectBackward0>)\n",
      "Move 1\n",
      "Reccurent Value tensor([-0.4233], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([0.0865], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([0.0522, 0.0024, 0.0207, 0.5387, 0.0136, 0.1127, 0.0105, 0.0789, 0.1703],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Move 3\n",
      "Reccurent Value tensor([0.5195], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.1297], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([6.4008e-04, 1.0476e-01, 4.9490e-05, 2.6465e-04, 1.9262e-05, 8.7899e-02,\n",
      "        3.3027e-03, 2.3824e-01, 5.6482e-01], grad_fn=<SelectBackward0>)\n",
      "Move 5\n",
      "Reccurent Value tensor([-0.1210], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0286], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([0.0033, 0.1193, 0.0025, 0.5264, 0.0030, 0.0092, 0.0081, 0.1745, 0.1537],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Move 7\n",
      "Reccurent Value tensor([-0.6407], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0457], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([1.2681e-04, 9.2962e-02, 3.5591e-05, 7.9219e-01, 3.0715e-06, 6.2346e-02,\n",
      "        2.7924e-04, 9.9557e-04, 5.1064e-02], grad_fn=<SelectBackward0>)\n",
      "Move 8\n",
      "Reccurent Value tensor([-0.6129], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0506], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([4.8544e-04, 1.9454e-01, 2.0301e-03, 4.1997e-01, 3.2133e-04, 2.8308e-01,\n",
      "        9.5923e-04, 9.8382e-02, 2.3102e-04], grad_fn=<SelectBackward0>)\n",
      "Action 7\n",
      "MCTS Prediction (tensor([0.4800, 0.4400, 0.0000, 0.0800]), tensor([0.0000, 0.4800, 0.0000, 0.4400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0800]), [1, 3, 5, 8], -0.4091256488114595)\n",
      "Initial Value tensor([-0.5532], grad_fn=<SelectBackward0>)\n",
      "Initial Policy tensor([3.8667e-05, 1.0536e-01, 8.9040e-05, 7.6352e-01, 2.9112e-05, 2.5028e-02,\n",
      "        6.0007e-04, 1.5519e-03, 1.0378e-01], grad_fn=<SelectBackward0>)\n",
      "Move 1\n",
      "Reccurent Value tensor([0.1523], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([0.0686], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([0.0072, 0.0207, 0.0030, 0.3222, 0.0296, 0.4268, 0.0053, 0.0102, 0.1749],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Move 3\n",
      "Reccurent Value tensor([0.5102], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([0.0281], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([0.0309, 0.3889, 0.0206, 0.0308, 0.0160, 0.1055, 0.0091, 0.0129, 0.3853],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Move 5\n",
      "Reccurent Value tensor([-0.2277], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.1239], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([3.3545e-04, 3.6483e-01, 3.2476e-03, 5.5928e-01, 6.2428e-04, 1.7320e-04,\n",
      "        1.7273e-03, 4.0492e-03, 6.5740e-02], grad_fn=<SelectBackward0>)\n",
      "Move 8\n",
      "Reccurent Value tensor([0.4844], grad_fn=<SelectBackward0>)\n",
      "Reccurent Reward tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "Reccurent Policy tensor([0.0006, 0.4381, 0.0065, 0.3537, 0.0020, 0.1736, 0.0022, 0.0227, 0.0005],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Action 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    "    FrameStackWrapper,\n",
    ")\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig\n",
    "\n",
    "# from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# from agents.random import RandomAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "best_agent = TicTacToeBestAgent()\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "env.reset()\n",
    "env.step(0)\n",
    "env.step(6)\n",
    "env.step(2)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "\n",
    "selected_actions = {i: 0 for i in range(agent.num_actions)}\n",
    "for i in range(100):\n",
    "    selected_actions[agent.select_actions(prediction).item()] += 1\n",
    "print(selected_actions)\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "# env.step(action)\n",
    "# state, reward, terminated, truncated, info = env.last()\n",
    "# prediction = agent.predict(state, info, env)\n",
    "# print(\"MCTS Prediction\", prediction)\n",
    "# initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "# print(\"Initial Value\", initial_inference[0])\n",
    "# print(\"Initial Policy\", initial_inference[1])\n",
    "# action = agent.select_actions(prediction).item()\n",
    "# print(\"Action\", action)\n",
    "# env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from math import log, sqrt, inf\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior_policy):\n",
    "        self.visits = 0\n",
    "        self.to_play = -1\n",
    "        self.prior_policy = prior_policy\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expand(self, legal_moves, to_play, policy, hidden_state, reward):\n",
    "        self.to_play = to_play\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        # print(legal_moves)\n",
    "        policy = {a: policy[a] for a in legal_moves}\n",
    "        policy_sum = sum(policy.values())\n",
    "\n",
    "        for action, p in policy.items():\n",
    "            self.children[action] = Node((p / (policy_sum + 1e-10)).item())\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visits == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visits\n",
    "\n",
    "    def add_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior_policy = (1 - frac) * self.children[\n",
    "                a\n",
    "            ].prior_policy + frac * n\n",
    "\n",
    "    def select_child(self, min_max_stats, pb_c_base, pb_c_init, discount, num_players):\n",
    "        # Select the child with the highest UCB\n",
    "        child_ucbs = [\n",
    "            self.child_ucb_score(\n",
    "                child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "            )\n",
    "            for action, child in self.children.items()\n",
    "        ]\n",
    "        print(\"Child UCBs\", child_ucbs)\n",
    "        action_index = np.random.choice(\n",
    "            np.where(np.isclose(child_ucbs, max(child_ucbs)))[0]\n",
    "        )\n",
    "        action = list(self.children.keys())[action_index]\n",
    "        return action, self.children[action]\n",
    "\n",
    "    def child_ucb_score(\n",
    "        self, child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "    ):\n",
    "        pb_c = log((self.visits + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
    "        pb_c *= sqrt(self.visits) / (child.visits + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior_policy\n",
    "        if child.visits > 0:\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward\n",
    "                + discount\n",
    "                * (\n",
    "                    child.value() if num_players == 1 else -child.value()\n",
    "                )  # (or if on the same team)\n",
    "            )\n",
    "        else:\n",
    "            value_score = 0.0\n",
    "\n",
    "        # check if value_score is nan\n",
    "        assert (\n",
    "            value_score == value_score\n",
    "        ), \"value_score is nan, child value is {}, and reward is {},\".format(\n",
    "            child.value(),\n",
    "            child.reward,\n",
    "        )\n",
    "        assert prior_score == prior_score, \"prior_score is nan\"\n",
    "        print(\"Prior Score\", prior_score)\n",
    "        print(\"Value Score\", value_score)\n",
    "        return prior_score + value_score\n",
    "        # return value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Optional\n",
    "\n",
    "from pyparsing import List\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float(\"inf\")\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "    def __init__(\n",
    "        self, known_bounds: Optional[List[float]]\n",
    "    ):  # might need to say known_bounds=None\n",
    "        self.max = known_bounds[1] if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "        self.min = known_bounds[0] if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.max = max(self.max, value)\n",
    "        self.min = min(self.min, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        print(\"Initial value\", value)\n",
    "        if self.max > self.min:\n",
    "            # We normalize only when we have at a max and min value\n",
    "            print(\"normalized value\", (value - self.min) / (self.max - self.min))\n",
    "            return (value - self.min) / (self.max - self.min)\n",
    "        return value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"min: {self.min}, max: {self.max}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root policy tensor([3.8667e-05, 1.0536e-01, 8.9040e-05, 7.6352e-01, 2.9112e-05, 2.5028e-02,\n",
      "        6.0007e-04, 1.5519e-03, 1.0378e-01], grad_fn=<SelectBackward0>)\n",
      "expanded root\n",
      "at root\n",
      "selecting child\n",
      "Prior Score 0.0\n",
      "Value Score 0.0\n",
      "Prior Score 0.0\n",
      "Value Score 0.0\n",
      "Prior Score 0.0\n",
      "Value Score 0.0\n",
      "Prior Score 0.0\n",
      "Value Score 0.0\n",
      "Child UCBs [0.0, 0.0, 0.0, 0.0]\n",
      "Selected action 1\n",
      "leaf value 0.15232999622821808\n",
      "leaf reward 0.06857515871524811\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.08375483751296997\n",
      "normalized value 0.458122581243515\n",
      "Prior Score 0.06600853030837159\n",
      "Value Score 0.458122581243515\n",
      "Prior Score 0.9566919723439027\n",
      "Value Score 0.0\n",
      "Prior Score 0.03136020727562555\n",
      "Value Score 0.0\n",
      "Prior Score 0.13003252074056998\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5241311115518866, 0.9566919723439027, 0.03136020727562555, 0.13003252074056998]\n",
      "Selected action 3\n",
      "leaf value 0.5102172493934631\n",
      "leaf reward 0.028090298175811768\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.08375483751296997\n",
      "normalized value 0.458122581243515\n",
      "Prior Score 0.09335395813032496\n",
      "Value Score 0.458122581243515\n",
      "Initial value -0.48212695121765137\n",
      "normalized value 0.2589365243911743\n",
      "Prior Score 0.6765109139120141\n",
      "Value Score 0.2589365243911743\n",
      "Prior Score 0.044351835486871376\n",
      "Value Score 0.0\n",
      "Prior Score 0.1839012388260406\n",
      "Value Score 0.0\n",
      "Child UCBs [0.55147653937384, 0.9354474383031884, 0.044351835486871376, 0.1839012388260406]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.038631560040710416\n",
      "Value Score 0.0\n",
      "Prior Score 0.4861986433038985\n",
      "Value Score 0.0\n",
      "Prior Score 0.02578250935368373\n",
      "Value Score 0.0\n",
      "Prior Score 0.03855975620987286\n",
      "Value Score 0.0\n",
      "Prior Score 0.01994146319228718\n",
      "Value Score 0.0\n",
      "Prior Score 0.13182511803531174\n",
      "Value Score 0.0\n",
      "Prior Score 0.011382272850650086\n",
      "Value Score 0.0\n",
      "Prior Score 0.016072318995079164\n",
      "Value Score 0.0\n",
      "Prior Score 0.4817081108456117\n",
      "Value Score 0.0\n",
      "Child UCBs [0.038631560040710416, 0.4861986433038985, 0.02578250935368373, 0.03855975620987286, 0.01994146319228718, 0.13182511803531174, 0.011382272850650086, 0.016072318995079164, 0.4817081108456117]\n",
      "Selected action 1\n",
      "leaf value -0.041411202400922775\n",
      "leaf reward 0.9761313796043396\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.08375483751296997\n",
      "normalized value 0.4541391942153517\n",
      "Prior Score 0.11433943442465283\n",
      "Value Score 0.4541391942153517\n",
      "Initial value -0.735789617523551\n",
      "normalized value 0.13095653337529403\n",
      "Prior Score 0.5523913274308743\n",
      "Value Score 0.13095653337529403\n",
      "Prior Score 0.05432189364895081\n",
      "Value Score 0.0\n",
      "Prior Score 0.22524126516422505\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5684786286400045, 0.6833478608061683, 0.05432189364895081, 0.22524126516422505]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.05463549971044014\n",
      "Value Score 0.0\n",
      "Initial value 1.0175425820052624\n",
      "normalized value 1.0\n",
      "Prior Score 0.34380835005696603\n",
      "Value Score 1.0\n",
      "Prior Score 0.03646345839627406\n",
      "Value Score 0.0\n",
      "Prior Score 0.05453394962613602\n",
      "Value Score 0.0\n",
      "Prior Score 0.0282026354959473\n",
      "Value Score 0.0\n",
      "Prior Score 0.1864364574109124\n",
      "Value Score 0.0\n",
      "Prior Score 0.016097619779799278\n",
      "Value Score 0.0\n",
      "Prior Score 0.02273061659628491\n",
      "Value Score 0.0\n",
      "Prior Score 0.6812658697419278\n",
      "Value Score 0.0\n",
      "Child UCBs [0.05463549971044014, 1.343808350056966, 0.03646345839627406, 0.05453394962613602, 0.0282026354959473, 0.1864364574109124, 0.016097619779799278, 0.02273061659628491, 0.6812658697419278]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.1354604218584958\n",
      "Value Score 0.0\n",
      "Prior Score 0.144834108454886\n",
      "Value Score 0.0\n",
      "Prior Score 0.13988789864537873\n",
      "Value Score 0.0\n",
      "Prior Score 0.132783666694474\n",
      "Value Score 0.0\n",
      "Prior Score 0.14736815475668477\n",
      "Value Score 0.0\n",
      "Prior Score 0.17278218893396616\n",
      "Value Score 0.0\n",
      "Prior Score 0.12352907801097025\n",
      "Value Score 0.0\n",
      "Prior Score 0.11734310242462023\n",
      "Value Score 0.0\n",
      "Prior Score 0.13611324830818092\n",
      "Value Score 0.0\n",
      "Child UCBs [0.1354604218584958, 0.144834108454886, 0.13988789864537873, 0.132783666694474, 0.14736815475668477, 0.17278218893396616, 0.12352907801097025, 0.11734310242462023, 0.13611324830818092]\n",
      "Selected action 5\n",
      "leaf value -0.015735747292637825\n",
      "leaf reward -0.03597383201122284\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.08375483751296997\n",
      "normalized value 0.4541391942153517\n",
      "Prior Score 0.13203317901380146\n",
      "Value Score 0.4541391942153517\n",
      "Initial value -0.8132861337314049\n",
      "normalized value 0.09254519232155076\n",
      "Prior Score 0.47840438900640575\n",
      "Value Score 0.09254519232155076\n",
      "Prior Score 0.06272807229291474\n",
      "Value Score 0.0\n",
      "Prior Score 0.2600967936772576\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5861723732291532, 0.5709495813279565, 0.06272807229291474, 0.2600967936772576]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.009055172794193059\n",
      "Value Score 0.0\n",
      "Prior Score 0.025902794799524967\n",
      "Value Score 0.0\n",
      "Prior Score 0.0037551087256654085\n",
      "Value Score 0.0\n",
      "Prior Score 0.4027391252880773\n",
      "Value Score 0.0\n",
      "Prior Score 0.03700522434512856\n",
      "Value Score 0.0\n",
      "Prior Score 0.5335994077250543\n",
      "Value Score 0.0\n",
      "Prior Score 0.006612136922403744\n",
      "Value Score 0.0\n",
      "Prior Score 0.012748309472275858\n",
      "Value Score 0.0\n",
      "Prior Score 0.21868440784795648\n",
      "Value Score 0.0\n",
      "Child UCBs [0.009055172794193059, 0.025902794799524967, 0.0037551087256654085, 0.4027391252880773, 0.03700522434512856, 0.5335994077250543, 0.006612136922403744, 0.012748309472275858, 0.21868440784795648]\n",
      "Selected action 5\n",
      "leaf value -0.03920288011431694\n",
      "leaf reward 0.08108663558959961\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.0677345972508192\n",
      "normalized value 0.4620796661563345\n",
      "Prior Score 0.0984157254272494\n",
      "Value Score 0.4620796661563345\n",
      "Initial value -0.8132861337314049\n",
      "normalized value 0.09254519232155076\n",
      "Prior Score 0.5348941305131025\n",
      "Value Score 0.09254519232155076\n",
      "Prior Score 0.07013497045369374\n",
      "Value Score 0.0\n",
      "Prior Score 0.29080888783689585\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5604953915835839, 0.6274393228346533, 0.07013497045369374, 0.29080888783689585]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.0669172712278468\n",
      "Value Score 0.0\n",
      "Initial value 1.0069560231640935\n",
      "normalized value 0.9947527457732036\n",
      "Prior Score 0.2807297664594461\n",
      "Value Score 0.9947527457732036\n",
      "Prior Score 0.04466025108840579\n",
      "Value Score 0.0\n",
      "Prior Score 0.06679289322141134\n",
      "Value Score 0.0\n",
      "Prior Score 0.03454243886894984\n",
      "Value Score 0.0\n",
      "Prior Score 0.22834638748514938\n",
      "Value Score 0.0\n",
      "Prior Score 0.019716279610081777\n",
      "Value Score 0.0\n",
      "Prior Score 0.027840339047162332\n",
      "Value Score 0.0\n",
      "Prior Score 0.8344108359108531\n",
      "Value Score 0.0\n",
      "Child UCBs [0.0669172712278468, 1.2754825122326496, 0.04466025108840579, 0.06679289322141134, 0.03454243886894984, 0.22834638748514938, 0.019716279610081777, 0.027840339047162332, 0.8344108359108531]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.1915777626227555\n",
      "Value Score 0.0\n",
      "Prior Score 0.2048346968698617\n",
      "Value Score 0.0\n",
      "Prior Score 0.19783941517970138\n",
      "Value Score 0.0\n",
      "Prior Score 0.18779210509728367\n",
      "Value Score 0.0\n",
      "Prior Score 0.2084185253728336\n",
      "Value Score 0.0\n",
      "Initial value -0.020238084718585014\n",
      "normalized value 0.48562143075444614\n",
      "Prior Score 0.12218042998422649\n",
      "Value Score 0.48562143075444614\n",
      "Prior Score 0.1747036075889\n",
      "Value Score 0.0\n",
      "Prior Score 0.16595496096420623\n",
      "Value Score 0.0\n",
      "Prior Score 0.1925010362173282\n",
      "Value Score 0.0\n",
      "Child UCBs [0.1915777626227555, 0.2048346968698617, 0.19783941517970138, 0.18779210509728367, 0.2084185253728336, 0.6078018607386726, 0.1747036075889, 0.16595496096420623, 0.1925010362173282]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.14467746587266\n",
      "Value Score 0.0\n",
      "Prior Score 0.12190158039588948\n",
      "Value Score 0.0\n",
      "Prior Score 0.13082268188373367\n",
      "Value Score 0.0\n",
      "Prior Score 0.14104756416017228\n",
      "Value Score 0.0\n",
      "Prior Score 0.14564791780332215\n",
      "Value Score 0.0\n",
      "Prior Score 0.15209114562662557\n",
      "Value Score 0.0\n",
      "Prior Score 0.1286638401117478\n",
      "Value Score 0.0\n",
      "Prior Score 0.13107086230031081\n",
      "Value Score 0.0\n",
      "Prior Score 0.15417871679335557\n",
      "Value Score 0.0\n",
      "Child UCBs [0.14467746587266, 0.12190158039588948, 0.13082268188373367, 0.14104756416017228, 0.14564791780332215, 0.15209114562662557, 0.1286638401117478, 0.13107086230031081, 0.15417871679335557]\n",
      "Selected action 8\n",
      "leaf value -0.029425809159874916\n",
      "leaf reward 0.028829261660575867\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.0677345972508192\n",
      "normalized value 0.4596763683678851\n",
      "Prior Score 0.10781341186203842\n",
      "Value Score 0.4596763683678851\n",
      "Initial value -0.8705320963636041\n",
      "normalized value 0.06383733171721424\n",
      "Prior Score 0.4687768012296028\n",
      "Value Score 0.06383733171721424\n",
      "Prior Score 0.07683213655774518\n",
      "Value Score 0.0\n",
      "Prior Score 0.31857813638408167\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5674897802299236, 0.532614132946817, 0.07683213655774518, 0.31857813638408167]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.012806469375136898\n",
      "Value Score 0.0\n",
      "Prior Score 0.03663357462855936\n",
      "Value Score 0.0\n",
      "Prior Score 0.005310741825532315\n",
      "Value Score 0.0\n",
      "Prior Score 0.5695823140425013\n",
      "Value Score 0.0\n",
      "Prior Score 0.05233542010372988\n",
      "Value Score 0.0\n",
      "Initial value 0.12028951570391655\n",
      "normalized value 0.5523862781787069\n",
      "Prior Score 0.37732711616526676\n",
      "Value Score 0.5523862781787069\n",
      "Prior Score 0.009351354295003432\n",
      "Value Score 0.0\n",
      "Prior Score 0.018029565923486854\n",
      "Value Score 0.0\n",
      "Prior Score 0.30927904255132127\n",
      "Value Score 0.0\n",
      "Child UCBs [0.012806469375136898, 0.03663357462855936, 0.005310741825532315, 0.5695823140425013, 0.05233542010372988, 0.9297133943439737, 0.009351354295003432, 0.018029565923486854, 0.30927904255132127]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.12699324669288775\n",
      "Value Score 0.0\n",
      "Prior Score 0.022706291388627283\n",
      "Value Score 0.0\n",
      "Prior Score 0.0860633819413782\n",
      "Value Score 0.0\n",
      "Prior Score 0.4844912410204317\n",
      "Value Score 0.0\n",
      "Prior Score 0.08199454942082868\n",
      "Value Score 0.0\n",
      "Prior Score 0.07991365638059746\n",
      "Value Score 0.0\n",
      "Prior Score 0.07314886326110223\n",
      "Value Score 0.0\n",
      "Prior Score 0.09326177827011058\n",
      "Value Score 0.0\n",
      "Prior Score 0.20152860823412616\n",
      "Value Score 0.0\n",
      "Child UCBs [0.12699324669288775, 0.022706291388627283, 0.0860633819413782, 0.4844912410204317, 0.08199454942082868, 0.07991365638059746, 0.07314886326110223, 0.09326177827011058, 0.20152860823412616]\n",
      "Selected action 3\n",
      "leaf value 0.08221598714590073\n",
      "leaf reward 0.04366469383239746\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.062177321563164384\n",
      "normalized value 0.4624165197224105\n",
      "Prior Score 0.08734239881050679\n",
      "Value Score 0.4624165197224105\n",
      "Initial value -0.8705320963636041\n",
      "normalized value 0.06383733171721424\n",
      "Prior Score 0.5063574140912144\n",
      "Value Score 0.06383733171721424\n",
      "Prior Score 0.0829915684488572\n",
      "Value Score 0.0\n",
      "Prior Score 0.34411771423482074\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5497589185329173, 0.5701947458084287, 0.0829915684488572, 0.34411771423482074]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.0772725533898252\n",
      "Value Score 0.0\n",
      "Initial value 1.0280907762547333\n",
      "normalized value 1.0\n",
      "Prior Score 0.2431290024475425\n",
      "Value Score 1.0\n",
      "Prior Score 0.0515713144500689\n",
      "Value Score 0.0\n",
      "Prior Score 0.07712892819461843\n",
      "Value Score 0.0\n",
      "Prior Score 0.03988779582220848\n",
      "Value Score 0.0\n",
      "Prior Score 0.2636824259949382\n",
      "Value Score 0.0\n",
      "Prior Score 0.02276732509954424\n",
      "Value Score 0.0\n",
      "Prior Score 0.032148562634715626\n",
      "Value Score 0.0\n",
      "Prior Score 0.9635338483458472\n",
      "Value Score 0.0\n",
      "Child UCBs [0.0772725533898252, 1.2431290024475425, 0.0515713144500689, 0.07712892819461843, 0.03988779582220848, 0.2636824259949382, 0.02276732509954424, 0.032148562634715626, 0.9635338483458472]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.23464343093033463\n",
      "Value Score 0.0\n",
      "Prior Score 0.25088045391657854\n",
      "Value Score 0.0\n",
      "Prior Score 0.24231266988135353\n",
      "Value Score 0.0\n",
      "Prior Score 0.2300067776051097\n",
      "Value Score 0.0\n",
      "Prior Score 0.25526990812196637\n",
      "Value Score 0.0\n",
      "Initial value -0.05723349377512932\n",
      "normalized value 0.46485419551380913\n",
      "Prior Score 0.0997639631780422\n",
      "Value Score 0.46485419551380913\n",
      "Prior Score 0.2139760550460527\n",
      "Value Score 0.0\n",
      "Prior Score 0.20326075890775544\n",
      "Value Score 0.0\n",
      "Prior Score 0.23577425154830228\n",
      "Value Score 0.0\n",
      "Child UCBs [0.23464343093033463, 0.25088045391657854, 0.24231266988135353, 0.2300067776051097, 0.25526990812196637, 0.5646181586918513, 0.2139760550460527, 0.20326075890775544, 0.23577425154830228]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.20461316178955863\n",
      "Value Score 0.0\n",
      "Prior Score 0.17240188471299794\n",
      "Value Score 0.0\n",
      "Prior Score 0.18501874091146056\n",
      "Value Score 0.0\n",
      "Prior Score 0.1994794966268638\n",
      "Value Score 0.0\n",
      "Prior Score 0.2059856439290532\n",
      "Value Score 0.0\n",
      "Prior Score 0.21509811496318754\n",
      "Value Score 0.0\n",
      "Prior Score 0.1819655533393324\n",
      "Value Score 0.0\n",
      "Prior Score 0.18536973530733142\n",
      "Value Score 0.0\n",
      "Initial value 0.05825507082045078\n",
      "normalized value 0.5217986705578959\n",
      "Prior Score 0.10902525328827625\n",
      "Value Score 0.5217986705578959\n",
      "Child UCBs [0.20461316178955863, 0.17240188471299794, 0.18501874091146056, 0.1994794966268638, 0.2059856439290532, 0.21509811496318754, 0.1819655533393324, 0.18536973530733142, 0.6308239238461721]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.12900593342858568\n",
      "Value Score 0.0\n",
      "Prior Score 0.1329991363993864\n",
      "Value Score 0.0\n",
      "Prior Score 0.1337477385461241\n",
      "Value Score 0.0\n",
      "Prior Score 0.14633809332868367\n",
      "Value Score 0.0\n",
      "Prior Score 0.1398680412315777\n",
      "Value Score 0.0\n",
      "Prior Score 0.16103408840685413\n",
      "Value Score 0.0\n",
      "Prior Score 0.1452104027208221\n",
      "Value Score 0.0\n",
      "Prior Score 0.13848179442900843\n",
      "Value Score 0.0\n",
      "Prior Score 0.12341652782880713\n",
      "Value Score 0.0\n",
      "Child UCBs [0.12900593342858568, 0.1329991363993864, 0.1337477385461241, 0.14633809332868367, 0.1398680412315777, 0.16103408840685413, 0.1452104027208221, 0.13848179442900843, 0.12341652782880713]\n",
      "Selected action 5\n",
      "leaf value -0.03905659541487694\n",
      "leaf reward 0.012838490307331085\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.062177321563164384\n",
      "normalized value 0.4624165197224105\n",
      "Prior Score 0.09337675008725256\n",
      "Value Score 0.4624165197224105\n",
      "Initial value -0.8886154949665069\n",
      "normalized value 0.05492086761480488\n",
      "Prior Score 0.4511173873775706\n",
      "Value Score 0.05492086761480488\n",
      "Prior Score 0.08872532758358175\n",
      "Value Score 0.0\n",
      "Prior Score 0.36789227500397076\n",
      "Value Score 0.0\n",
      "Child UCBs [0.555793269809663, 0.5060382549923755, 0.08872532758358175, 0.36789227500397076]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.015685295992330657\n",
      "Value Score 0.0\n",
      "Prior Score 0.04486860855043006\n",
      "Value Score 0.0\n",
      "Prior Score 0.00650456851394566\n",
      "Value Score 0.0\n",
      "Prior Score 0.6976214072785977\n",
      "Value Score 0.0\n",
      "Prior Score 0.06410014588436873\n",
      "Value Score 0.0\n",
      "Initial value 0.11996372230350971\n",
      "normalized value 0.552225637735872\n",
      "Prior Score 0.3080988381531177\n",
      "Value Score 0.552225637735872\n",
      "Prior Score 0.011453489306822587\n",
      "Value Score 0.0\n",
      "Prior Score 0.022082517034099157\n",
      "Value Score 0.0\n",
      "Prior Score 0.37880333638718006\n",
      "Value Score 0.0\n",
      "Child UCBs [0.015685295992330657, 0.04486860855043006, 0.00650456851394566, 0.6976214072785977, 0.06410014588436873, 0.8603244758889896, 0.011453489306822587, 0.022082517034099157, 0.37880333638718006]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.17960288131272498\n",
      "Value Score 0.0\n",
      "Prior Score 0.03211285216753317\n",
      "Value Score 0.0\n",
      "Prior Score 0.12171695562339498\n",
      "Value Score 0.0\n",
      "Initial value -0.038551293313503265\n",
      "normalized value 0.4740659135889371\n",
      "Prior Score 0.34260098518656407\n",
      "Value Score 0.4740659135889371\n",
      "Prior Score 0.11596252329490377\n",
      "Value Score 0.0\n",
      "Prior Score 0.11301957636298587\n",
      "Value Score 0.0\n",
      "Prior Score 0.10345232481705084\n",
      "Value Score 0.0\n",
      "Prior Score 0.13189743966596634\n",
      "Value Score 0.0\n",
      "Prior Score 0.28501609060617483\n",
      "Value Score 0.0\n",
      "Child UCBs [0.17960288131272498, 0.03211285216753317, 0.12171695562339498, 0.8166668987755012, 0.11596252329490377, 0.11301957636298587, 0.10345232481705084, 0.13189743966596634, 0.28501609060617483]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.13952063894388306\n",
      "Value Score 0.0\n",
      "Prior Score 0.09077695637353625\n",
      "Value Score 0.0\n",
      "Prior Score 0.14374633758227365\n",
      "Value Score 0.0\n",
      "Prior Score 0.17329721499097472\n",
      "Value Score 0.0\n",
      "Prior Score 0.1163915113114755\n",
      "Value Score 0.0\n",
      "Prior Score 0.14076930888939082\n",
      "Value Score 0.0\n",
      "Prior Score 0.08440215839038114\n",
      "Value Score 0.0\n",
      "Prior Score 0.16920014247106085\n",
      "Value Score 0.0\n",
      "Prior Score 0.1919975339367932\n",
      "Value Score 0.0\n",
      "Child UCBs [0.13952063894388306, 0.09077695637353625, 0.14374633758227365, 0.17329721499097472, 0.1163915113114755, 0.14076930888939082, 0.08440215839038114, 0.16920014247106085, 0.1919975339367932]\n",
      "Selected action 8\n",
      "leaf value -0.02376149781048298\n",
      "leaf reward 0.21901178359985352\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.09953800728544593\n",
      "normalized value 0.4439949154432986\n",
      "Prior Score 0.07923602252588165\n",
      "Value Score 0.4439949154432986\n",
      "Initial value -0.8886154949665069\n",
      "normalized value 0.05492086761480488\n",
      "Prior Score 0.4785017072593775\n",
      "Value Score 0.05492086761480488\n",
      "Prior Score 0.09411124889841091\n",
      "Value Score 0.0\n",
      "Prior Score 0.3902245548553842\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5232309379691803, 0.5334225748741824, 0.09411124889841091, 0.3902245548553842]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.08639685631609957\n",
      "Value Score 0.0\n",
      "Initial value 1.0183279290795326\n",
      "normalized value 0.9951861882665679\n",
      "Prior Score 0.21747003891296646\n",
      "Value Score 0.9951861882665679\n",
      "Prior Score 0.05766082844573996\n",
      "Value Score 0.0\n",
      "Prior Score 0.08623627193252097\n",
      "Value Score 0.0\n",
      "Prior Score 0.04459772601316726\n",
      "Value Score 0.0\n",
      "Prior Score 0.2948179097542925\n",
      "Value Score 0.0\n",
      "Prior Score 0.025455679009388832\n",
      "Value Score 0.0\n",
      "Prior Score 0.03594464819492277\n",
      "Value Score 0.0\n",
      "Prior Score 1.0773074241674538\n",
      "Value Score 0.0\n",
      "Child UCBs [0.08639685631609957, 1.2126562271795343, 0.05766082844573996, 0.08623627193252097, 0.04459772601316726, 0.2948179097542925, 0.025455679009388832, 0.03594464819492277, 1.0773074241674538]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.27095392133370266\n",
      "Value Score 0.0\n",
      "Prior Score 0.2897035834549254\n",
      "Value Score 0.0\n",
      "Prior Score 0.2798099560378691\n",
      "Value Score 0.0\n",
      "Prior Score 0.26559975737797853\n",
      "Value Score 0.0\n",
      "Prior Score 0.2947722948386149\n",
      "Value Score 0.0\n",
      "Initial value -0.042458331833283104\n",
      "normalized value 0.4721394522265937\n",
      "Prior Score 0.08640164223979678\n",
      "Value Score 0.4721394522265937\n",
      "Prior Score 0.2470883201646403\n",
      "Value Score 0.0\n",
      "Prior Score 0.2347148584597374\n",
      "Value Score 0.0\n",
      "Prior Score 0.2722597336445289\n",
      "Value Score 0.0\n",
      "Child UCBs [0.27095392133370266, 0.2897035834549254, 0.2798099560378691, 0.26559975737797853, 0.2947722948386149, 0.5585410944663904, 0.2470883201646403, 0.2347148584597374, 0.2722597336445289]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2506091189213155\n",
      "Value Score 0.0\n",
      "Prior Score 0.21115691703515524\n",
      "Value Score 0.0\n",
      "Prior Score 0.22660997581103995\n",
      "Value Score 0.0\n",
      "Prior Score 0.24432143296793976\n",
      "Value Score 0.0\n",
      "Prior Score 0.25229012779046994\n",
      "Value Score 0.0\n",
      "Prior Score 0.26345103414217924\n",
      "Value Score 0.0\n",
      "Prior Score 0.22287044781264315\n",
      "Value Score 0.0\n",
      "Prior Score 0.22703987189165467\n",
      "Value Score 0.0\n",
      "Initial value 0.017594623379409313\n",
      "normalized value 0.5017500376677404\n",
      "Prior Score 0.08902236926103889\n",
      "Value Score 0.5017500376677404\n",
      "Child UCBs [0.2506091189213155, 0.21115691703515524, 0.22660997581103995, 0.24432143296793976, 0.25229012779046994, 0.26345103414217924, 0.22287044781264315, 0.22703987189165467, 0.5907724069287793]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.18244936603789655\n",
      "Value Score 0.0\n",
      "Prior Score 0.18809683767831184\n",
      "Value Score 0.0\n",
      "Prior Score 0.18915556407528417\n",
      "Value Score 0.0\n",
      "Prior Score 0.2069617392427373\n",
      "Value Score 0.0\n",
      "Prior Score 0.1978113314128322\n",
      "Value Score 0.0\n",
      "Initial value 0.05189508572220802\n",
      "normalized value 0.5186627236009319\n",
      "Prior Score 0.11387293033535328\n",
      "Value Score 0.5186627236009319\n",
      "Prior Score 0.2053668789830336\n",
      "Value Score 0.0\n",
      "Prior Score 0.19585080259389354\n",
      "Value Score 0.0\n",
      "Prior Score 0.1745444311166451\n",
      "Value Score 0.0\n",
      "Child UCBs [0.18244936603789655, 0.18809683767831184, 0.18915556407528417, 0.2069617392427373, 0.1978113314128322, 0.6325356539362852, 0.2053668789830336, 0.19585080259389354, 0.1745444311166451]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.13730169403340842\n",
      "Value Score 0.0\n",
      "Prior Score 0.13390844202535634\n",
      "Value Score 0.0\n",
      "Prior Score 0.1479764883049967\n",
      "Value Score 0.0\n",
      "Prior Score 0.1332850105090465\n",
      "Value Score 0.0\n",
      "Prior Score 0.1436015517016277\n",
      "Value Score 0.0\n",
      "Prior Score 0.16409013693964689\n",
      "Value Score 0.0\n",
      "Prior Score 0.1390324930445808\n",
      "Value Score 0.0\n",
      "Prior Score 0.12872592712881992\n",
      "Value Score 0.0\n",
      "Prior Score 0.12218013371415762\n",
      "Value Score 0.0\n",
      "Child UCBs [0.13730169403340842, 0.13390844202535634, 0.1479764883049967, 0.1332850105090465, 0.1436015517016277, 0.16409013693964689, 0.1390324930445808, 0.12872592712881992, 0.12218013371415762]\n",
      "Selected action 5\n",
      "leaf value -0.03533702343702316\n",
      "leaf reward -0.001616060733795166\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.09953800728544593\n",
      "normalized value 0.4439949154432986\n",
      "Prior Score 0.0835254981514538\n",
      "Value Score 0.4439949154432986\n",
      "Initial value -0.912800687054793\n",
      "normalized value 0.04299576427552104\n",
      "Prior Score 0.4323476569414311\n",
      "Value Score 0.04299576427552104\n",
      "Prior Score 0.09920600120138051\n",
      "Value Score 0.0\n",
      "Prior Score 0.41134952634174526\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5275204135947524, 0.47534342121695217, 0.09920600120138051, 0.41134952634174526]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.018112556740033432\n",
      "Value Score 0.0\n",
      "Prior Score 0.05181191471384264\n",
      "Value Score 0.0\n",
      "Prior Score 0.007511134398476203\n",
      "Value Score 0.0\n",
      "Prior Score 0.8055765940645185\n",
      "Value Score 0.0\n",
      "Prior Score 0.07401948486931519\n",
      "Value Score 0.0\n",
      "Initial value 0.17337422259151936\n",
      "normalized value 0.5785609975300932\n",
      "Prior Score 0.2668322783157213\n",
      "Value Score 0.5785609975300932\n",
      "Prior Score 0.013225888439888173\n",
      "Value Score 0.0\n",
      "Prior Score 0.025499731910602258\n",
      "Value Score 0.0\n",
      "Prior Score 0.4374222154928738\n",
      "Value Score 0.0\n",
      "Child UCBs [0.018112556740033432, 0.05181191471384264, 0.007511134398476203, 0.8055765940645185, 0.07401948486931519, 0.8453932758458145, 0.013225888439888173, 0.025499731910602258, 0.4374222154928738]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2199766596041549\n",
      "Value Score 0.0\n",
      "Prior Score 0.039331651577883105\n",
      "Value Score 0.0\n",
      "Prior Score 0.14907828381996321\n",
      "Value Score 0.0\n",
      "Initial value -0.11882994044572115\n",
      "normalized value 0.434482553676188\n",
      "Prior Score 0.2797439170522309\n",
      "Value Score 0.434482553676188\n",
      "Prior Score 0.14203028552345723\n",
      "Value Score 0.0\n",
      "Prior Score 0.13842577967842917\n",
      "Value Score 0.0\n",
      "Prior Score 0.12670786055995475\n",
      "Value Score 0.0\n",
      "Prior Score 0.16154728685861094\n",
      "Value Score 0.0\n",
      "Prior Score 0.3490862011050564\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2199766596041549, 0.039331651577883105, 0.14907828381996321, 0.7142264707284189, 0.14203028552345723, 0.13842577967842917, 0.12670786055995475, 0.16154728685861094, 0.3490862011050564]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.19732001039010505\n",
      "Value Score 0.0\n",
      "Prior Score 0.12838322781773354\n",
      "Value Score 0.0\n",
      "Prior Score 0.20329629393886411\n",
      "Value Score 0.0\n",
      "Prior Score 0.24508924644725183\n",
      "Value Score 0.0\n",
      "Prior Score 0.164609153134238\n",
      "Value Score 0.0\n",
      "Prior Score 0.19908596823323463\n",
      "Value Score 0.0\n",
      "Prior Score 0.11936753513031027\n",
      "Value Score 0.0\n",
      "Prior Score 0.23929487510321307\n",
      "Value Score 0.0\n",
      "Initial value 0.2427732814103365\n",
      "normalized value 0.6127799090459652\n",
      "Prior Score 0.1357682837394415\n",
      "Value Score 0.6127799090459652\n",
      "Child UCBs [0.19732001039010505, 0.12838322781773354, 0.20329629393886411, 0.24508924644725183, 0.164609153134238, 0.19908596823323463, 0.11936753513031027, 0.23929487510321307, 0.7485481927854067]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.11811389979513272\n",
      "Value Score 0.0\n",
      "Prior Score 0.13862017228869045\n",
      "Value Score 0.0\n",
      "Prior Score 0.14967254615600126\n",
      "Value Score 0.0\n",
      "Prior Score 0.14441725178880754\n",
      "Value Score 0.0\n",
      "Prior Score 0.1381482327214772\n",
      "Value Score 0.0\n",
      "Prior Score 0.1646937948680113\n",
      "Value Score 0.0\n",
      "Prior Score 0.15028165276498182\n",
      "Value Score 0.0\n",
      "Prior Score 0.11725927656898733\n",
      "Value Score 0.0\n",
      "Prior Score 0.1288949293677598\n",
      "Value Score 0.0\n",
      "Child UCBs [0.11811389979513272, 0.13862017228869045, 0.14967254615600126, 0.14441725178880754, 0.1381482327214772, 0.1646937948680113, 0.15028165276498182, 0.11725927656898733, 0.1288949293677598]\n",
      "Selected action 5\n",
      "leaf value -0.06277843564748764\n",
      "leaf reward 0.020467281341552734\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.10055297575891017\n",
      "normalized value 0.4434944602933873\n",
      "Prior Score 0.07300486997565188\n",
      "Value Score 0.4434944602933873\n",
      "Initial value -0.912800687054793\n",
      "normalized value 0.04299576427552104\n",
      "Prior Score 0.4534684882269698\n",
      "Value Score 0.04299576427552104\n",
      "Prior Score 0.10405236310538674\n",
      "Value Score 0.0\n",
      "Prior Score 0.43144456746377263\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5164993302690392, 0.4964642525024908, 0.10405236310538674, 0.43144456746377263]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.02025127802224175\n",
      "Value Score 0.0\n",
      "Prior Score 0.05792983866355959\n",
      "Value Score 0.0\n",
      "Prior Score 0.008398045242821107\n",
      "Value Score 0.0\n",
      "Prior Score 0.9006986594307302\n",
      "Value Score 0.0\n",
      "Prior Score 0.08275966715612627\n",
      "Value Score 0.0\n",
      "Initial value 0.17332766903564334\n",
      "normalized value 0.5785380431552589\n",
      "Prior Score 0.2386717559994741\n",
      "Value Score 0.5785380431552589\n",
      "Prior Score 0.014787594470046872\n",
      "Value Score 0.0\n",
      "Prior Score 0.028510726995976992\n",
      "Value Score 0.0\n",
      "Prior Score 0.4890728032598439\n",
      "Value Score 0.0\n",
      "Child UCBs [0.02025127802224175, 0.05792983866355959, 0.008398045242821107, 0.9006986594307302, 0.08275966715612627, 0.817209799154733, 0.014787594470046872, 0.028510726995976992, 0.4890728032598439]\n",
      "Selected action 3\n",
      "leaf value 0.49441778659820557\n",
      "leaf reward -0.08700887858867645\n",
      "at root\n",
      "selecting child\n",
      "Initial value 0.024539490851263203\n",
      "normalized value 0.5051743752532005\n",
      "Prior Score 0.06536074319958596\n",
      "Value Score 0.5051743752532005\n",
      "Initial value -0.912800687054793\n",
      "normalized value 0.04299576427552104\n",
      "Prior Score 0.4736516480018097\n",
      "Value Score 0.04299576427552104\n",
      "Prior Score 0.10868356797194086\n",
      "Value Score 0.0\n",
      "Prior Score 0.45064747762221685\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5705351184527865, 0.5166474122773308, 0.10868356797194086, 0.45064747762221685]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.022185066143302214\n",
      "Value Score 0.0\n",
      "Prior Score 0.0634615406005685\n",
      "Value Score 0.0\n",
      "Prior Score 0.00919997192186129\n",
      "Value Score 0.0\n",
      "Initial value -0.581426665186882\n",
      "normalized value 0.20638786967223213\n",
      "Prior Score 0.4933530445018905\n",
      "Value Score 0.20638786967223213\n",
      "Prior Score 0.09066236154774268\n",
      "Value Score 0.0\n",
      "Initial value 0.17332766903564334\n",
      "normalized value 0.5785380431552589\n",
      "Prior Score 0.2614624463488693\n",
      "Value Score 0.5785380431552589\n",
      "Prior Score 0.01619965718005605\n",
      "Value Score 0.0\n",
      "Prior Score 0.03123320728226141\n",
      "Value Score 0.0\n",
      "Prior Score 0.5357742102643256\n",
      "Value Score 0.0\n",
      "Child UCBs [0.022185066143302214, 0.0634615406005685, 0.00919997192186129, 0.6997409141741227, 0.09066236154774268, 0.8400004895041282, 0.01619965718005605, 0.03123320728226141, 0.5357742102643256]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.25401750343196733\n",
      "Value Score 0.0\n",
      "Prior Score 0.0454181273488215\n",
      "Value Score 0.0\n",
      "Prior Score 0.17214777940538403\n",
      "Value Score 0.0\n",
      "Initial value -0.10992041788995266\n",
      "normalized value 0.43887561273453135\n",
      "Prior Score 0.24227519707462936\n",
      "Value Score 0.43887561273453135\n",
      "Prior Score 0.16400912081000002\n",
      "Value Score 0.0\n",
      "Prior Score 0.1598468266033893\n",
      "Value Score 0.0\n",
      "Prior Score 0.14631558849272397\n",
      "Value Score 0.0\n",
      "Prior Score 0.186546329814607\n",
      "Value Score 0.0\n",
      "Prior Score 0.4031064270492325\n",
      "Value Score 0.0\n",
      "Child UCBs [0.25401750343196733, 0.0454181273488215, 0.17214777940538403, 0.6811508098091608, 0.16400912081000002, 0.1598468266033893, 0.14631558849272397, 0.186546329814607, 0.4031064270492325]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.24167650564076512\n",
      "Value Score 0.0\n",
      "Prior Score 0.15724309876393572\n",
      "Value Score 0.0\n",
      "Prior Score 0.24899622613909178\n",
      "Value Score 0.0\n",
      "Prior Score 0.30018401344291856\n",
      "Value Score 0.0\n",
      "Prior Score 0.2016124205919015\n",
      "Value Score 0.0\n",
      "Prior Score 0.24383944147171652\n",
      "Value Score 0.0\n",
      "Prior Score 0.14620072601968254\n",
      "Value Score 0.0\n",
      "Prior Score 0.29308709805129796\n",
      "Value Score 0.0\n",
      "Initial value 0.18926967401057482\n",
      "normalized value 0.5863986405020757\n",
      "Prior Score 0.1108588508116747\n",
      "Value Score 0.5863986405020757\n",
      "Child UCBs [0.24167650564076512, 0.15724309876393572, 0.24899622613909178, 0.30018401344291856, 0.2016124205919015, 0.24383944147171652, 0.14620072601968254, 0.29308709805129796, 0.6972574913137504]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.16704507742517916\n",
      "Value Score 0.0\n",
      "Prior Score 0.19604650640457633\n",
      "Value Score 0.0\n",
      "Prior Score 0.2116775595795138\n",
      "Value Score 0.0\n",
      "Prior Score 0.20424514852558504\n",
      "Value Score 0.0\n",
      "Prior Score 0.19537905590398424\n",
      "Value Score 0.0\n",
      "Initial value 0.08324571698904037\n",
      "normalized value 0.5341209228264752\n",
      "Prior Score 0.11646083891435113\n",
      "Value Score 0.5341209228264752\n",
      "Prior Score 0.21253900146597957\n",
      "Value Score 0.0\n",
      "Prior Score 0.16583640847742256\n",
      "Value Score 0.0\n",
      "Prior Score 0.18229237620039793\n",
      "Value Score 0.0\n",
      "Child UCBs [0.16704507742517916, 0.19604650640457633, 0.2116775595795138, 0.20424514852558504, 0.19537905590398424, 0.6505817617408263, 0.21253900146597957, 0.16583640847742256, 0.18229237620039793]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.1356287162346548\n",
      "Value Score 0.0\n",
      "Prior Score 0.11653143529249474\n",
      "Value Score 0.0\n",
      "Prior Score 0.13354011121570536\n",
      "Value Score 0.0\n",
      "Prior Score 0.14618805436108473\n",
      "Value Score 0.0\n",
      "Prior Score 0.1515414622353048\n",
      "Value Score 0.0\n",
      "Prior Score 0.1588660723606684\n",
      "Value Score 0.0\n",
      "Prior Score 0.1554040272670936\n",
      "Value Score 0.0\n",
      "Prior Score 0.13107181232667467\n",
      "Value Score 0.0\n",
      "Prior Score 0.12133009296812017\n",
      "Value Score 0.0\n",
      "Child UCBs [0.1356287162346548, 0.11653143529249474, 0.13354011121570536, 0.14618805436108473, 0.1515414622353048, 0.1588660723606684, 0.1554040272670936, 0.13107181232667467, 0.12133009296812017]\n",
      "Selected action 5\n",
      "leaf value -0.0232807956635952\n",
      "leaf reward -0.0032347440719604492\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.005742913112044334\n",
      "normalized value 0.4902428917526296\n",
      "Prior Score 0.05952833636450917\n",
      "Value Score 0.4902428917526296\n",
      "Initial value -0.912800687054793\n",
      "normalized value 0.04299576427552104\n",
      "Prior Score 0.49301231012640595\n",
      "Value Score 0.04299576427552104\n",
      "Prior Score 0.11312604346395538\n",
      "Value Score 0.0\n",
      "Prior Score 0.46906783694821663\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5497712281171387, 0.536008074401927, 0.11312604346395538, 0.46906783694821663]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.02396358500313874\n",
      "Value Score 0.0\n",
      "Prior Score 0.06854908670493148\n",
      "Value Score 0.0\n",
      "Prior Score 0.009937509663119575\n",
      "Value Score 0.0\n",
      "Initial value -0.581426665186882\n",
      "normalized value 0.20638786967223213\n",
      "Prior Score 0.5329038706538911\n",
      "Value Score 0.20638786967223213\n",
      "Prior Score 0.0979305264857435\n",
      "Value Score 0.0\n",
      "Initial value 0.18986463434994222\n",
      "normalized value 0.5866920003192659\n",
      "Prior Score 0.2353526736460862\n",
      "Value Score 0.5866920003192659\n",
      "Prior Score 0.017498341422487945\n",
      "Value Score 0.0\n",
      "Prior Score 0.03373709200569984\n",
      "Value Score 0.0\n",
      "Prior Score 0.5787258305756674\n",
      "Value Score 0.0\n",
      "Child UCBs [0.02396358500313874, 0.06854908670493148, 0.009937509663119575, 0.7392917403261232, 0.0979305264857435, 0.8220446739653522, 0.017498341422487945, 0.03373709200569984, 0.5787258305756674]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2840117581603791\n",
      "Value Score 0.0\n",
      "Prior Score 0.05078107621093742\n",
      "Value Score 0.0\n",
      "Prior Score 0.19247489968903972\n",
      "Value Score 0.0\n",
      "Initial value -0.126171778421849\n",
      "normalized value 0.43086248002756855\n",
      "Prior Score 0.2167063410990391\n",
      "Value Score 0.43086248002756855\n",
      "Prior Score 0.18337523251842222\n",
      "Value Score 0.0\n",
      "Prior Score 0.1787214567760869\n",
      "Value Score 0.0\n",
      "Prior Score 0.16359245710490455\n",
      "Value Score 0.0\n",
      "Prior Score 0.20857362344403285\n",
      "Value Score 0.0\n",
      "Prior Score 0.4507050243593303\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2840117581603791, 0.05078107621093742, 0.19247489968903972, 0.6475688211266076, 0.18337523251842222, 0.1787214567760869, 0.16359245710490455, 0.20857362344403285, 0.4507050243593303]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.2790753469549885\n",
      "Value Score 0.0\n",
      "Prior Score 0.18157607926130537\n",
      "Value Score 0.0\n",
      "Prior Score 0.2875277760906549\n",
      "Value Score 0.0\n",
      "Prior Score 0.3466367468356541\n",
      "Value Score 0.0\n",
      "Prior Score 0.23281144386767147\n",
      "Value Score 0.0\n",
      "Prior Score 0.28157299175444356\n",
      "Value Score 0.0\n",
      "Prior Score 0.1688249266548976\n",
      "Value Score 0.0\n",
      "Prior Score 0.33844160134571266\n",
      "Value Score 0.0\n",
      "Initial value 0.19904330062369505\n",
      "normalized value 0.5912177673022918\n",
      "Prior Score 0.09601048777354007\n",
      "Value Score 0.5912177673022918\n",
      "Child UCBs [0.2790753469549885, 0.18157607926130537, 0.2875277760906549, 0.3466367468356541, 0.23281144386767147, 0.28157299175444356, 0.1688249266548976, 0.33844160134571266, 0.6872282550758319]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.20459592778651517\n",
      "Value Score 0.0\n",
      "Prior Score 0.24011672469136372\n",
      "Value Score 0.0\n",
      "Prior Score 0.2592615559902035\n",
      "Value Score 0.0\n",
      "Prior Score 0.2501583782210148\n",
      "Value Score 0.0\n",
      "Prior Score 0.23929923484655594\n",
      "Value Score 0.0\n",
      "Initial value 0.04183347336947918\n",
      "normalized value 0.5137015983542061\n",
      "Prior Score 0.09509374657328669\n",
      "Value Score 0.5137015983542061\n",
      "Prior Score 0.26031664545894034\n",
      "Value Score 0.0\n",
      "Prior Score 0.20311555644864215\n",
      "Value Score 0.0\n",
      "Prior Score 0.2232707387252053\n",
      "Value Score 0.0\n",
      "Child UCBs [0.20459592778651517, 0.24011672469136372, 0.2592615559902035, 0.2501583782210148, 0.23929923484655594, 0.6087953449274929, 0.26031664545894034, 0.20311555644864215, 0.2232707387252053]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.19181577649872153\n",
      "Value Score 0.0\n",
      "Prior Score 0.164807043579677\n",
      "Value Score 0.0\n",
      "Prior Score 0.18886192273801952\n",
      "Value Score 0.0\n",
      "Prior Score 0.20674954346389304\n",
      "Value Score 0.0\n",
      "Prior Score 0.21432071361735286\n",
      "Value Score 0.0\n",
      "Initial value 0.02004605159163475\n",
      "normalized value 0.502958774594572\n",
      "Prior Score 0.11233984909376242\n",
      "Value Score 0.502958774594572\n",
      "Prior Score 0.21978342779336504\n",
      "Value Score 0.0\n",
      "Prior Score 0.18537107890218163\n",
      "Value Score 0.0\n",
      "Prior Score 0.17159364654810097\n",
      "Value Score 0.0\n",
      "Child UCBs [0.19181577649872153, 0.164807043579677, 0.18886192273801952, 0.20674954346389304, 0.21432071361735286, 0.6152986236883343, 0.21978342779336504, 0.18537107890218163, 0.17159364654810097]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.14062509116176639\n",
      "Value Score 0.0\n",
      "Prior Score 0.1529726117545414\n",
      "Value Score 0.0\n",
      "Prior Score 0.15523037735015807\n",
      "Value Score 0.0\n",
      "Prior Score 0.12968943082744847\n",
      "Value Score 0.0\n",
      "Prior Score 0.13411717771986623\n",
      "Value Score 0.0\n",
      "Prior Score 0.16047485818197513\n",
      "Value Score 0.0\n",
      "Prior Score 0.13052226864511327\n",
      "Value Score 0.0\n",
      "Prior Score 0.12724360658171135\n",
      "Value Score 0.0\n",
      "Prior Score 0.11922622232946156\n",
      "Value Score 0.0\n",
      "Child UCBs [0.14062509116176639, 0.1529726117545414, 0.15523037735015807, 0.12968943082744847, 0.13411717771986623, 0.16047485818197513, 0.13052226864511327, 0.12724360658171135, 0.11922622232946156]\n",
      "Selected action 5\n",
      "leaf value -0.038428377360105515\n",
      "leaf reward 0.006441988050937653\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.019935820950195193\n",
      "normalized value 0.4832447297352662\n",
      "Prior Score 0.05491376498145607\n",
      "Value Score 0.4832447297352662\n",
      "Initial value -0.912800687054793\n",
      "normalized value 0.04299576427552104\n",
      "Prior Score 0.5116438532928771\n",
      "Value Score 0.04299576427552104\n",
      "Prior Score 0.11740121614982678\n",
      "Value Score 0.0\n",
      "Prior Score 0.486794488945736\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5381584947167223, 0.5546396175683981, 0.11740121614982678, 0.486794488945736]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.09464686474803884\n",
      "Value Score 0.0\n",
      "Initial value 1.0270257323980332\n",
      "normalized value 0.9994748539517215\n",
      "Prior Score 0.1985301533468602\n",
      "Value Score 0.9994748539517215\n",
      "Prior Score 0.063166842682294\n",
      "Value Score 0.0\n",
      "Prior Score 0.09447094621255875\n",
      "Value Score 0.0\n",
      "Prior Score 0.04885634873790852\n",
      "Value Score 0.0\n",
      "Prior Score 0.3229699785339801\n",
      "Value Score 0.0\n",
      "Prior Score 0.02788643373152637\n",
      "Value Score 0.0\n",
      "Prior Score 0.03937699126081212\n",
      "Value Score 0.0\n",
      "Prior Score 1.1801791687209195\n",
      "Value Score 0.0\n",
      "Child UCBs [0.09464686474803884, 1.1980050072985817, 0.063166842682294, 0.09447094621255875, 0.04885634873790852, 0.3229699785339801, 0.02788643373152637, 0.03937699126081212, 1.1801791687209195]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.30294801948183187\n",
      "Value Score 0.0\n",
      "Prior Score 0.32391163195740974\n",
      "Value Score 0.0\n",
      "Prior Score 0.31284977015915594\n",
      "Value Score 0.0\n",
      "Prior Score 0.29696163863012254\n",
      "Value Score 0.0\n",
      "Prior Score 0.3295788541458002\n",
      "Value Score 0.0\n",
      "Initial value -0.053265140391886234\n",
      "normalized value 0.46681088967646955\n",
      "Prior Score 0.07728312258463238\n",
      "Value Score 0.46681088967646955\n",
      "Prior Score 0.27626438053568675\n",
      "Value Score 0.0\n",
      "Prior Score 0.26242986690627146\n",
      "Value Score 0.0\n",
      "Prior Score 0.3044080214313611\n",
      "Value Score 0.0\n",
      "Child UCBs [0.30294801948183187, 0.32391163195740974, 0.31284977015915594, 0.29696163863012254, 0.3295788541458002, 0.544094012261102, 0.27626438053568675, 0.26242986690627146, 0.3044080214313611]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2893902600404574\n",
      "Value Score 0.0\n",
      "Prior Score 0.2438329275214073\n",
      "Value Score 0.0\n",
      "Prior Score 0.26167730891032964\n",
      "Value Score 0.0\n",
      "Prior Score 0.2821295702422085\n",
      "Value Score 0.0\n",
      "Prior Score 0.29133140087311665\n",
      "Value Score 0.0\n",
      "Prior Score 0.3042194298694695\n",
      "Value Score 0.0\n",
      "Prior Score 0.2573590982061757\n",
      "Value Score 0.0\n",
      "Prior Score 0.2621737303458103\n",
      "Value Score 0.0\n",
      "Initial value 0.02830032693843047\n",
      "normalized value 0.5070287479130439\n",
      "Prior Score 0.07709877048994665\n",
      "Value Score 0.5070287479130439\n",
      "Child UCBs [0.2893902600404574, 0.2438329275214073, 0.26167730891032964, 0.2821295702422085, 0.29133140087311665, 0.3042194298694695, 0.2573590982061757, 0.2621737303458103, 0.5841275184029905]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.22346301904828442\n",
      "Value Score 0.0\n",
      "Prior Score 0.23038001245945713\n",
      "Value Score 0.0\n",
      "Prior Score 0.23167673495376498\n",
      "Value Score 0.0\n",
      "Prior Score 0.25348564417077585\n",
      "Value Score 0.0\n",
      "Prior Score 0.24227827303215024\n",
      "Value Score 0.0\n",
      "Initial value 0.015506306663155556\n",
      "normalized value 0.5007203417878991\n",
      "Prior Score 0.09298064207515543\n",
      "Value Score 0.5007203417878991\n",
      "Prior Score 0.251532267755538\n",
      "Value Score 0.0\n",
      "Prior Score 0.23987702769858082\n",
      "Value Score 0.0\n",
      "Prior Score 0.21378109654428326\n",
      "Value Score 0.0\n",
      "Child UCBs [0.22346301904828442, 0.23038001245945713, 0.23167673495376498, 0.25348564417077585, 0.24227827303215024, 0.5937009838630546, 0.251532267755538, 0.23987702769858082, 0.21378109654428326]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.194181820684953\n",
      "Value Score 0.0\n",
      "Prior Score 0.18938284236494698\n",
      "Value Score 0.0\n",
      "Prior Score 0.20927887394192124\n",
      "Value Score 0.0\n",
      "Prior Score 0.18850114117573977\n",
      "Value Score 0.0\n",
      "Prior Score 0.20309152744919165\n",
      "Value Score 0.0\n",
      "Initial value 0.033720962703228\n",
      "normalized value 0.5097015255955142\n",
      "Prior Score 0.11603397092693872\n",
      "Value Score 0.5097015255955142\n",
      "Prior Score 0.19662963974206812\n",
      "Value Score 0.0\n",
      "Prior Score 0.1820533612145435\n",
      "Value Score 0.0\n",
      "Prior Score 0.17279583462658007\n",
      "Value Score 0.0\n",
      "Child UCBs [0.194181820684953, 0.18938284236494698, 0.20927887394192124, 0.18850114117573977, 0.20309152744919165, 0.6257354965224529, 0.19662963974206812, 0.1820533612145435, 0.17279583462658007]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.14114077851163606\n",
      "Value Score 0.0\n",
      "Prior Score 0.13768406101688593\n",
      "Value Score 0.0\n",
      "Prior Score 0.15107847340667632\n",
      "Value Score 0.0\n",
      "Prior Score 0.13206860420397346\n",
      "Value Score 0.0\n",
      "Prior Score 0.13188251080446842\n",
      "Value Score 0.0\n",
      "Prior Score 0.15703006258669278\n",
      "Value Score 0.0\n",
      "Prior Score 0.13633979234002083\n",
      "Value Score 0.0\n",
      "Prior Score 0.1323277471796706\n",
      "Value Score 0.0\n",
      "Prior Score 0.1305497542117769\n",
      "Value Score 0.0\n",
      "Child UCBs [0.14114077851163606, 0.13768406101688593, 0.15107847340667632, 0.13206860420397346, 0.13188251080446842, 0.15703006258669278, 0.13633979234002083, 0.1323277471796706, 0.1305497542117769]\n",
      "Selected action 5\n",
      "leaf value -0.03280239552259445\n",
      "leaf reward 0.0015182346105575562\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.019935820950195193\n",
      "normalized value 0.4832447297352662\n",
      "Prior Score 0.05684345759514635\n",
      "Value Score 0.4832447297352662\n",
      "Initial value -0.9201247308935437\n",
      "normalized value 0.039384464463647725\n",
      "Prior Score 0.4634203278040707\n",
      "Value Score 0.039384464463647725\n",
      "Prior Score 0.12152674386986348\n",
      "Value Score 0.0\n",
      "Prior Score 0.5039006503976222\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5400881873304125, 0.5028047922677185, 0.12152674386986348, 0.5039006503976222]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.025619192036245565\n",
      "Value Score 0.0\n",
      "Prior Score 0.07328503710829848\n",
      "Value Score 0.0\n",
      "Prior Score 0.01062407683942783\n",
      "Value Score 0.0\n",
      "Initial value -0.581426665186882\n",
      "normalized value 0.20638786967223213\n",
      "Prior Score 0.5697213750510367\n",
      "Value Score 0.20638786967223213\n",
      "Prior Score 0.10469639513120765\n",
      "Value Score 0.0\n",
      "Initial value 0.18953075104703504\n",
      "normalized value 0.5865273709511842\n",
      "Prior Score 0.21566813652321098\n",
      "Value Score 0.5865273709511842\n",
      "Prior Score 0.018707274773778287\n",
      "Value Score 0.0\n",
      "Prior Score 0.036067935524893346\n",
      "Value Score 0.0\n",
      "Prior Score 0.6187091033295633\n",
      "Value Score 0.0\n",
      "Child UCBs [0.025619192036245565, 0.07328503710829848, 0.01062407683942783, 0.7761092447232688, 0.10469639513120765, 0.8021955074743952, 0.018707274773778287, 0.036067935524893346, 0.6187091033295633]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.3111319509486484\n",
      "Value Score 0.0\n",
      "Prior Score 0.0556301450866659\n",
      "Value Score 0.0\n",
      "Prior Score 0.2108542668683446\n",
      "Value Score 0.0\n",
      "Initial value -0.12229236252605916\n",
      "normalized value 0.4327753213762946\n",
      "Prior Score 0.19783296745004647\n",
      "Value Score 0.4327753213762946\n",
      "Prior Score 0.20088567536313365\n",
      "Value Score 0.0\n",
      "Prior Score 0.19578751204986458\n",
      "Value Score 0.0\n",
      "Prior Score 0.17921384899420212\n",
      "Value Score 0.0\n",
      "Prior Score 0.228490252653292\n",
      "Value Score 0.0\n",
      "Prior Score 0.4937427043146943\n",
      "Value Score 0.0\n",
      "Child UCBs [0.3111319509486484, 0.0556301450866659, 0.2108542668683446, 0.6306082888263411, 0.20088567536313365, 0.19578751204986458, 0.17921384899420212, 0.228490252653292, 0.4937427043146943]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.31202841881772997\n",
      "Value Score 0.0\n",
      "Prior Score 0.20301648828968763\n",
      "Value Score 0.0\n",
      "Prior Score 0.32147890639088095\n",
      "Value Score 0.0\n",
      "Prior Score 0.3875674336676393\n",
      "Value Score 0.0\n",
      "Prior Score 0.26030169810885806\n",
      "Value Score 0.0\n",
      "Prior Score 0.31482098421645005\n",
      "Value Score 0.0\n",
      "Prior Score 0.1887596862134988\n",
      "Value Score 0.0\n",
      "Prior Score 0.3784046096593255\n",
      "Value Score 0.0\n",
      "Initial value 0.18689232366159558\n",
      "normalized value 0.5852264294862702\n",
      "Prior Score 0.085877884999218\n",
      "Value Score 0.5852264294862702\n",
      "Child UCBs [0.31202841881772997, 0.20301648828968763, 0.32147890639088095, 0.3875674336676393, 0.26030169810885806, 0.31482098421645005, 0.1887596862134988, 0.3784046096593255, 0.6711043144854882]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.23625664141913372\n",
      "Value Score 0.0\n",
      "Prior Score 0.27727419376273316\n",
      "Value Score 0.0\n",
      "Prior Score 0.29938164033869535\n",
      "Value Score 0.0\n",
      "Prior Score 0.28886976833196615\n",
      "Value Score 0.0\n",
      "Prior Score 0.27633019938699926\n",
      "Value Score 0.0\n",
      "Initial value 0.0507464458545049\n",
      "normalized value 0.5180963584849559\n",
      "Prior Score 0.08235695143750459\n",
      "Value Score 0.5180963584849559\n",
      "Prior Score 0.30060000229231426\n",
      "Value Score 0.0\n",
      "Prior Score 0.23454718627932286\n",
      "Value Score 0.0\n",
      "Prior Score 0.25782133314709405\n",
      "Value Score 0.0\n",
      "Child UCBs [0.23625664141913372, 0.27727419376273316, 0.29938164033869535, 0.28886976833196615, 0.27633019938699926, 0.6004533099224605, 0.30060000229231426, 0.23454718627932286, 0.25782133314709405]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2349349490674144\n",
      "Value Score 0.0\n",
      "Prior Score 0.2018547957633747\n",
      "Value Score 0.0\n",
      "Prior Score 0.23131708459614794\n",
      "Value Score 0.0\n",
      "Prior Score 0.25322574790256963\n",
      "Value Score 0.0\n",
      "Prior Score 0.26249887708334724\n",
      "Value Score 0.0\n",
      "Initial value -0.014029528945684433\n",
      "normalized value 0.48615697216231274\n",
      "Prior Score 0.09172883554153329\n",
      "Value Score 0.48615697216231274\n",
      "Prior Score 0.26918958052879516\n",
      "Value Score 0.0\n",
      "Prior Score 0.22704151751952462\n",
      "Value Score 0.0\n",
      "Prior Score 0.21016699120334717\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2349349490674144, 0.2018547957633747, 0.23131708459614794, 0.25322574790256963, 0.26249887708334724, 0.577885807703846, 0.26918958052879516, 0.22704151751952462, 0.21016699120334717]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.1988820052659726\n",
      "Value Score 0.0\n",
      "Prior Score 0.21634474705171197\n",
      "Value Score 0.0\n",
      "Prior Score 0.2195378397307434\n",
      "Value Score 0.0\n",
      "Prior Score 0.18341601667013357\n",
      "Value Score 0.0\n",
      "Prior Score 0.18967805122953688\n",
      "Value Score 0.0\n",
      "Initial value 0.04487036541104317\n",
      "normalized value 0.5151990126105701\n",
      "Prior Score 0.11347747875693855\n",
      "Value Score 0.5151990126105701\n",
      "Prior Score 0.18459387514382494\n",
      "Value Score 0.0\n",
      "Prior Score 0.1799569580732522\n",
      "Value Score 0.0\n",
      "Prior Score 0.16861820306230585\n",
      "Value Score 0.0\n",
      "Child UCBs [0.1988820052659726, 0.21634474705171197, 0.2195378397307434, 0.18341601667013357, 0.18967805122953688, 0.6286764913675087, 0.18459387514382494, 0.1799569580732522, 0.16861820306230585]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.13944069701956116\n",
      "Value Score 0.0\n",
      "Prior Score 0.14078917561717583\n",
      "Value Score 0.0\n",
      "Prior Score 0.14798833569259284\n",
      "Value Score 0.0\n",
      "Prior Score 0.1356257264458039\n",
      "Value Score 0.0\n",
      "Prior Score 0.1339988994375682\n",
      "Value Score 0.0\n",
      "Prior Score 0.15723694479839478\n",
      "Value Score 0.0\n",
      "Prior Score 0.1355824536763294\n",
      "Value Score 0.0\n",
      "Prior Score 0.13109554435780274\n",
      "Value Score 0.0\n",
      "Prior Score 0.12834399790258846\n",
      "Value Score 0.0\n",
      "Child UCBs [0.13944069701956116, 0.14078917561717583, 0.14798833569259284, 0.1356257264458039, 0.1339988994375682, 0.15723694479839478, 0.1355824536763294, 0.13109554435780274, 0.12834399790258846]\n",
      "Selected action 5\n",
      "leaf value -0.03400901332497597\n",
      "leaf reward 0.003530353307723999\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.03941560971240203\n",
      "normalized value 0.47363974114684615\n",
      "Prior Score 0.05283905120477583\n",
      "Value Score 0.47363974114684615\n",
      "Initial value -0.9201247308935437\n",
      "normalized value 0.039384464463647725\n",
      "Prior Score 0.4786379112453356\n",
      "Value Score 0.039384464463647725\n",
      "Prior Score 0.12551738315396227\n",
      "Value Score 0.0\n",
      "Prior Score 0.5204475080416733\n",
      "Value Score 0.0\n",
      "Child UCBs [0.526478792351622, 0.5180223757089834, 0.12551738315396227, 0.5204475080416733]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.027174361864463252\n",
      "Value Score 0.0\n",
      "Prior Score 0.07773368164046779\n",
      "Value Score 0.0\n",
      "Prior Score 0.011268993499171398\n",
      "Value Score 0.0\n",
      "Initial value -0.581426665186882\n",
      "normalized value 0.20638786967223213\n",
      "Prior Score 0.6043053498975717\n",
      "Value Score 0.20638786967223213\n",
      "Prior Score 0.11105181315535359\n",
      "Value Score 0.0\n",
      "Initial value 0.200144797829645\n",
      "normalized value 0.5917608875703027\n",
      "Prior Score 0.2001649185187645\n",
      "Value Score 0.5917608875703027\n",
      "Prior Score 0.019842868326268064\n",
      "Value Score 0.0\n",
      "Prior Score 0.03825737869761548\n",
      "Value Score 0.0\n",
      "Prior Score 0.656266795569836\n",
      "Value Score 0.0\n",
      "Child UCBs [0.027174361864463252, 0.07773368164046779, 0.011268993499171398, 0.8106932195698038, 0.11105181315535359, 0.7919258060890672, 0.019842868326268064, 0.03825737869761548, 0.656266795569836]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.13486292978751238\n",
      "Value Score 0.0\n",
      "Prior Score 0.019759495887676063\n",
      "Value Score 0.0\n",
      "Prior Score 0.030091686901804728\n",
      "Value Score 0.0\n",
      "Prior Score 0.10695336263210359\n",
      "Value Score 0.0\n",
      "Prior Score 0.0951326596000295\n",
      "Value Score 0.0\n",
      "Prior Score 0.29728534201535867\n",
      "Value Score 0.0\n",
      "Prior Score 0.07104558871741701\n",
      "Value Score 0.0\n",
      "Prior Score 0.19743472109672633\n",
      "Value Score 0.0\n",
      "Prior Score 0.2975359626957331\n",
      "Value Score 0.0\n",
      "Child UCBs [0.13486292978751238, 0.019759495887676063, 0.030091686901804728, 0.10695336263210359, 0.0951326596000295, 0.29728534201535867, 0.07104558871741701, 0.19743472109672633, 0.2975359626957331]\n",
      "Selected action 8\n",
      "leaf value -0.026487203314900398\n",
      "leaf reward 0.04618695378303528\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.012648229300975797\n",
      "normalized value 0.4868380558992348\n",
      "Prior Score 0.049515873639746605\n",
      "Value Score 0.4868380558992348\n",
      "Initial value -0.9201247308935437\n",
      "normalized value 0.039384464463647725\n",
      "Prior Score 0.4933887185942048\n",
      "Value Score 0.039384464463647725\n",
      "Prior Score 0.12938561568285045\n",
      "Value Score 0.0\n",
      "Prior Score 0.5364868161406651\n",
      "Value Score 0.0\n",
      "Child UCBs [0.5363539295389814, 0.5327731830578525, 0.12938561568285045, 0.5364868161406651]\n",
      "Selected action 8\n",
      "leaf value 0.4843778610229492\n",
      "leaf reward -0.007061757147312164\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.012648229300975797\n",
      "normalized value 0.4868380558992348\n",
      "Prior Score 0.05095348430724942\n",
      "Value Score 0.4868380558992348\n",
      "Initial value -0.9201247308935437\n",
      "normalized value 0.039384464463647725\n",
      "Prior Score 0.5077134357593931\n",
      "Value Score 0.039384464463647725\n",
      "Prior Score 0.13314211087629843\n",
      "Value Score 0.0\n",
      "Initial value -0.4914396181702614\n",
      "normalized value 0.25075819474357797\n",
      "Prior Score 0.2760314072831683\n",
      "Value Score 0.25075819474357797\n",
      "Child UCBs [0.5377915402064842, 0.5470979002230408, 0.13314211087629843, 0.5267896020267462]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.10223445690987713\n",
      "Value Score 0.0\n",
      "Initial value 1.0212146590153377\n",
      "normalized value 0.9966095614062733\n",
      "Prior Score 0.183810683715269\n",
      "Value Score 0.9966095614062733\n",
      "Prior Score 0.06823076362357566\n",
      "Value Score 0.0\n",
      "Prior Score 0.10204443544447449\n",
      "Value Score 0.0\n",
      "Prior Score 0.05277303472351032\n",
      "Value Score 0.0\n",
      "Prior Score 0.34886163890917793\n",
      "Value Score 0.0\n",
      "Prior Score 0.030122016352950042\n",
      "Value Score 0.0\n",
      "Prior Score 0.04253374189426091\n",
      "Value Score 0.0\n",
      "Prior Score 1.274791052949628\n",
      "Value Score 0.0\n",
      "Child UCBs [0.10223445690987713, 1.1804202451215424, 0.06823076362357566, 0.10204443544447449, 0.05277303472351032, 0.34886163890917793, 0.030122016352950042, 0.04253374189426091, 1.274791052949628]\n",
      "Selected action 8\n",
      "leaf value -0.04374909773468971\n",
      "leaf reward 0.005544833838939667\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.012648229300975797\n",
      "normalized value 0.4868380558992348\n",
      "Prior Score 0.052351856006623634\n",
      "Value Score 0.4868380558992348\n",
      "Initial value -0.807759593706578\n",
      "normalized value 0.09478885686193571\n",
      "Prior Score 0.46368636968709415\n",
      "Value Score 0.09478885686193571\n",
      "Prior Score 0.136796074140551\n",
      "Value Score 0.0\n",
      "Initial value -0.4914396181702614\n",
      "normalized value 0.25075819474357797\n",
      "Prior Score 0.2836068363893639\n",
      "Value Score 0.25075819474357797\n",
      "Child UCBs [0.5391899119058584, 0.5584752265490298, 0.136796074140551, 0.5343650311329419]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.10929767745320067\n",
      "Value Score 0.0\n",
      "Initial value 1.0212146590153377\n",
      "normalized value 0.9966095614062733\n",
      "Prior Score 0.19650987962770508\n",
      "Value Score 0.9966095614062733\n",
      "Prior Score 0.07294472157747302\n",
      "Value Score 0.0\n",
      "Prior Score 0.10909452769858251\n",
      "Value Score 0.0\n",
      "Prior Score 0.05641904208990356\n",
      "Value Score 0.0\n",
      "Prior Score 0.37296395009857464\n",
      "Value Score 0.0\n",
      "Prior Score 0.03220309988526672\n",
      "Value Score 0.0\n",
      "Prior Score 0.04547233235204364\n",
      "Value Score 0.0\n",
      "Initial value 0.04929393157362938\n",
      "normalized value 0.5173801606214866\n",
      "Prior Score 0.681432197797753\n",
      "Value Score 0.5173801606214866\n",
      "Child UCBs [0.10929767745320067, 1.1931194410339785, 0.07294472157747302, 0.10909452769858251, 0.05641904208990356, 0.37296395009857464, 0.03220309988526672, 0.04547233235204364, 1.1988123584192396]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.16317859595752676\n",
      "Value Score 0.0\n",
      "Prior Score 0.3353850502870382\n",
      "Value Score 0.0\n",
      "Prior Score 0.12780557511787435\n",
      "Value Score 0.0\n",
      "Prior Score 0.07610587620245671\n",
      "Value Score 0.0\n",
      "Prior Score 0.1023358433165169\n",
      "Value Score 0.0\n",
      "Prior Score 0.21134334922692086\n",
      "Value Score 0.0\n",
      "Prior Score 0.07809159895459271\n",
      "Value Score 0.0\n",
      "Prior Score 0.08448436361280515\n",
      "Value Score 0.0\n",
      "Prior Score 0.07137162006891723\n",
      "Value Score 0.0\n",
      "Child UCBs [0.16317859595752676, 0.3353850502870382, 0.12780557511787435, 0.07610587620245671, 0.1023358433165169, 0.21134334922692086, 0.07809159895459271, 0.08448436361280515, 0.07137162006891723]\n",
      "Selected action 1\n",
      "leaf value 0.05433216691017151\n",
      "leaf reward 0.06026466190814972\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.012648229300975797\n",
      "normalized value 0.4868380558992348\n",
      "Prior Score 0.05371405342008609\n",
      "Value Score 0.4868380558992348\n",
      "Initial value -0.7148443100353082\n",
      "normalized value 0.14060302098079042\n",
      "Prior Score 0.4281763570244472\n",
      "Value Score 0.14060302098079042\n",
      "Prior Score 0.14035551352971995\n",
      "Value Score 0.0\n",
      "Initial value -0.4914396181702614\n",
      "normalized value 0.25075819474357797\n",
      "Prior Score 0.29098629775785834\n",
      "Value Score 0.25075819474357797\n",
      "Child UCBs [0.5405521093193209, 0.5687793780052376, 0.14035551352971995, 0.5417444925014363]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.11593240855748399\n",
      "Value Score 0.0\n",
      "Initial value 1.0212146590153377\n",
      "normalized value 0.9966095614062733\n",
      "Prior Score 0.20843868032178356\n",
      "Value Score 0.9966095614062733\n",
      "Prior Score 0.07737270782951917\n",
      "Value Score 0.0\n",
      "Prior Score 0.11571692693976317\n",
      "Value Score 0.0\n",
      "Prior Score 0.05984386485055218\n",
      "Value Score 0.0\n",
      "Prior Score 0.39560409742974667\n",
      "Value Score 0.0\n",
      "Prior Score 0.03415793473118197\n",
      "Value Score 0.0\n",
      "Prior Score 0.048232653567191086\n",
      "Value Score 0.0\n",
      "Initial value 0.024453135207295418\n",
      "normalized value 0.5051317954806485\n",
      "Prior Score 0.4818649264414594\n",
      "Value Score 0.5051317954806485\n",
      "Child UCBs [0.11593240855748399, 1.2050482417280568, 0.07737270782951917, 0.11571692693976317, 0.05984386485055218, 0.39560409742974667, 0.03415793473118197, 0.048232653567191086, 0.9869967219221079]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.33187642986310956\n",
      "Value Score 0.0\n",
      "Prior Score 0.3548418510509704\n",
      "Value Score 0.0\n",
      "Prior Score 0.34272369557491605\n",
      "Value Score 0.0\n",
      "Prior Score 0.3253184114008521\n",
      "Value Score 0.0\n",
      "Prior Score 0.3610502344902845\n",
      "Value Score 0.0\n",
      "Initial value -0.04581769481301308\n",
      "normalized value 0.47048303574905626\n",
      "Prior Score 0.07055238622545004\n",
      "Value Score 0.47048303574905626\n",
      "Prior Score 0.3026447786895855\n",
      "Value Score 0.0\n",
      "Prior Score 0.287489211737618\n",
      "Value Score 0.0\n",
      "Prior Score 0.33347584693614973\n",
      "Value Score 0.0\n",
      "Child UCBs [0.33187642986310956, 0.3548418510509704, 0.34272369557491605, 0.3253184114008521, 0.3610502344902845, 0.5410354219745063, 0.3026447786895855, 0.287489211737618, 0.33347584693614973]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.3235613114770744\n",
      "Value Score 0.0\n",
      "Prior Score 0.27262459282178775\n",
      "Value Score 0.0\n",
      "Prior Score 0.2925760294868975\n",
      "Value Score 0.0\n",
      "Prior Score 0.31544328320265635\n",
      "Value Score 0.0\n",
      "Prior Score 0.32573166120995456\n",
      "Value Score 0.0\n",
      "Prior Score 0.340141501969044\n",
      "Value Score 0.0\n",
      "Prior Score 0.287747926708058\n",
      "Value Score 0.0\n",
      "Prior Score 0.2931310681073645\n",
      "Value Score 0.0\n",
      "Initial value 0.016238765325397253\n",
      "normalized value 0.501081498532369\n",
      "Prior Score 0.06896204257740968\n",
      "Value Score 0.501081498532369\n",
      "Child UCBs [0.3235613114770744, 0.27262459282178775, 0.2925760294868975, 0.31544328320265635, 0.32573166120995456, 0.340141501969044, 0.287747926708058, 0.2931310681073645, 0.5700435411097787]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.2580433683744474\n",
      "Value Score 0.0\n",
      "Prior Score 0.26603074940261295\n",
      "Value Score 0.0\n",
      "Prior Score 0.26752813649468393\n",
      "Value Score 0.0\n",
      "Prior Score 0.2927119204554387\n",
      "Value Score 0.0\n",
      "Prior Score 0.2797702363613532\n",
      "Value Score 0.0\n",
      "Initial value 0.026595931500196457\n",
      "normalized value 0.5061883538546568\n",
      "Prior Score 0.08052687479412775\n",
      "Value Score 0.5061883538546568\n",
      "Prior Score 0.2904562638728063\n",
      "Value Score 0.0\n",
      "Prior Score 0.2769974042533533\n",
      "Value Score 0.0\n",
      "Prior Score 0.24686319231707035\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2580433683744474, 0.26603074940261295, 0.26752813649468393, 0.2927119204554387, 0.2797702363613532, 0.5867152286487846, 0.2904562638728063, 0.2769974042533533, 0.24686319231707035]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.23783286747918408\n",
      "Value Score 0.0\n",
      "Prior Score 0.2319551042014917\n",
      "Value Score 0.0\n",
      "Prior Score 0.256323658501358\n",
      "Value Score 0.0\n",
      "Prior Score 0.23087520124585312\n",
      "Value Score 0.0\n",
      "Prior Score 0.24874542922499\n",
      "Value Score 0.0\n",
      "Initial value -0.0011078640818595886\n",
      "normalized value 0.4925283165888612\n",
      "Prior Score 0.09474519613698855\n",
      "Value Score 0.4925283165888612\n",
      "Prior Score 0.2408309433205288\n",
      "Value Score 0.0\n",
      "Prior Score 0.22297799443402638\n",
      "Value Score 0.0\n",
      "Prior Score 0.21163942480678866\n",
      "Value Score 0.0\n",
      "Child UCBs [0.23783286747918408, 0.2319551042014917, 0.256323658501358, 0.23087520124585312, 0.24874542922499, 0.5872735127258497, 0.2408309433205288, 0.22297799443402638, 0.21163942480678866]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.19961132699216722\n",
      "Value Score 0.0\n",
      "Prior Score 0.19472259126716746\n",
      "Value Score 0.0\n",
      "Prior Score 0.21366592188784958\n",
      "Value Score 0.0\n",
      "Prior Score 0.18678081286752335\n",
      "Value Score 0.0\n",
      "Prior Score 0.18651762634686359\n",
      "Value Score 0.0\n",
      "Initial value 0.03432063013315201\n",
      "normalized value 0.5099972063593857\n",
      "Prior Score 0.11104154129349883\n",
      "Value Score 0.5099972063593857\n",
      "Prior Score 0.19282143089911036\n",
      "Value Score 0.0\n",
      "Prior Score 0.1871473112941658\n",
      "Value Score 0.0\n",
      "Prior Score 0.18463274718699146\n",
      "Value Score 0.0\n",
      "Child UCBs [0.19961132699216722, 0.19472259126716746, 0.21366592188784958, 0.18678081286752335, 0.18651762634686359, 0.6210387476528846, 0.19282143089911036, 0.1871473112941658, 0.18463274718699146]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.14174031034510656\n",
      "Value Score 0.0\n",
      "Prior Score 0.13832187332442877\n",
      "Value Score 0.0\n",
      "Prior Score 0.15429138805731013\n",
      "Value Score 0.0\n",
      "Prior Score 0.13071728484125122\n",
      "Value Score 0.0\n",
      "Prior Score 0.13154317442688246\n",
      "Value Score 0.0\n",
      "Prior Score 0.155519716261851\n",
      "Value Score 0.0\n",
      "Prior Score 0.1339246204154935\n",
      "Value Score 0.0\n",
      "Prior Score 0.131493726486043\n",
      "Value Score 0.0\n",
      "Prior Score 0.1325495876496111\n",
      "Value Score 0.0\n",
      "Child UCBs [0.14174031034510656, 0.13832187332442877, 0.15429138805731013, 0.13071728484125122, 0.13154317442688246, 0.155519716261851, 0.1339246204154935, 0.131493726486043, 0.1325495876496111]\n",
      "Selected action 5\n",
      "leaf value -0.032458383589982986\n",
      "leaf reward 0.0002904459834098816\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.012648229300975797\n",
      "normalized value 0.4868380558992348\n",
      "Prior Score 0.05504276233187224\n",
      "Value Score 0.4868380558992348\n",
      "Initial value -0.746321900933981\n",
      "normalized value 0.12508222118858275\n",
      "Prior Score 0.3988800281761235\n",
      "Value Score 0.12508222118858275\n",
      "Prior Score 0.1438274470326107\n",
      "Value Score 0.0\n",
      "Initial value -0.4914396181702614\n",
      "normalized value 0.25075819474357797\n",
      "Prior Score 0.2981843411454002\n",
      "Value Score 0.25075819474357797\n",
      "Child UCBs [0.541880818231107, 0.5239622493647063, 0.1438274470326107, 0.5489425358889781]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.0007873060432526834\n",
      "Value Score 0.0\n",
      "Prior Score 0.5476881124211163\n",
      "Value Score 0.0\n",
      "Prior Score 0.008136349440864823\n",
      "Value Score 0.0\n",
      "Prior Score 0.44215579363565355\n",
      "Value Score 0.0\n",
      "Prior Score 0.002483836360646639\n",
      "Value Score 0.0\n",
      "Prior Score 0.2170233705766386\n",
      "Value Score 0.0\n",
      "Prior Score 0.0027394758230651277\n",
      "Value Score 0.0\n",
      "Prior Score 0.02840984917580993\n",
      "Value Score 0.0\n",
      "Prior Score 0.0006776415225104327\n",
      "Value Score 0.0\n",
      "Child UCBs [0.0007873060432526834, 0.5476881124211163, 0.008136349440864823, 0.44215579363565355, 0.002483836360646639, 0.2170233705766386, 0.0027394758230651277, 0.02840984917580993, 0.0006776415225104327]\n",
      "Selected action 1\n",
      "leaf value 0.06824377179145813\n",
      "leaf reward 0.9279288649559021\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.012648229300975797\n",
      "normalized value 0.4868380558992348\n",
      "Prior Score 0.05634035212794272\n",
      "Value Score 0.4868380558992348\n",
      "Initial value -0.746321900933981\n",
      "normalized value 0.12508222118858275\n",
      "Prior Score 0.4082833108692586\n",
      "Value Score 0.12508222118858275\n",
      "Prior Score 0.14721806588526076\n",
      "Value Score 0.0\n",
      "Initial value -0.6790932342410088\n",
      "normalized value 0.1582309675268128\n",
      "Prior Score 0.20347586818503596\n",
      "Value Score 0.1582309675268128\n",
      "Child UCBs [0.5431784080271775, 0.5333655320578414, 0.14721806588526076, 0.36170683571184875]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.02864545744879824\n",
      "Value Score 0.0\n",
      "Prior Score 0.08194182740616239\n",
      "Value Score 0.0\n",
      "Prior Score 0.011879045233199787\n",
      "Value Score 0.0\n",
      "Initial value -0.3705548504367471\n",
      "normalized value 0.31036340036299886\n",
      "Prior Score 0.42467978390591343\n",
      "Value Score 0.31036340036299886\n",
      "Prior Score 0.11706365007649491\n",
      "Value Score 0.0\n",
      "Initial value 0.200144797829645\n",
      "normalized value 0.5917608875703027\n",
      "Prior Score 0.21100093112654553\n",
      "Value Score 0.5917608875703027\n",
      "Prior Score 0.020917070403980408\n",
      "Value Score 0.0\n",
      "Prior Score 0.04032845809042702\n",
      "Value Score 0.0\n",
      "Prior Score 0.6917940763915057\n",
      "Value Score 0.0\n",
      "Child UCBs [0.02864545744879824, 0.08194182740616239, 0.011879045233199787, 0.7350431842689122, 0.11706365007649491, 0.8027618186968482, 0.020917070403980408, 0.04032845809042702, 0.6917940763915057]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.3360745875441658\n",
      "Value Score 0.0\n",
      "Prior Score 0.06008986864903841\n",
      "Value Score 0.0\n",
      "Prior Score 0.22775790321001815\n",
      "Value Score 0.0\n",
      "Initial value -0.1323673759276668\n",
      "normalized value 0.4278075884130723\n",
      "Prior Score 0.18316518874027607\n",
      "Value Score 0.4278075884130723\n",
      "Prior Score 0.21699015573729719\n",
      "Value Score 0.0\n",
      "Prior Score 0.21148328597507698\n",
      "Value Score 0.0\n",
      "Prior Score 0.19358095560191949\n",
      "Value Score 0.0\n",
      "Prior Score 0.24680771995349113\n",
      "Value Score 0.0\n",
      "Prior Score 0.5333247684770536\n",
      "Value Score 0.0\n",
      "Child UCBs [0.3360745875441658, 0.06008986864903841, 0.22775790321001815, 0.6109727771533484, 0.21699015573729719, 0.21148328597507698, 0.19358095560191949, 0.24680771995349113, 0.5333247684770536]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.3418239136541695\n",
      "Value Score 0.0\n",
      "Prior Score 0.2224024684240194\n",
      "Value Score 0.0\n",
      "Prior Score 0.3521768252909828\n",
      "Value Score 0.0\n",
      "Prior Score 0.4245761250950757\n",
      "Value Score 0.0\n",
      "Prior Score 0.28515782477611984\n",
      "Value Score 0.0\n",
      "Prior Score 0.3448831402378972\n",
      "Value Score 0.0\n",
      "Prior Score 0.20678428883531186\n",
      "Value Score 0.0\n",
      "Prior Score 0.41453834592575006\n",
      "Value Score 0.0\n",
      "Initial value 0.194795286282897\n",
      "normalized value 0.5891231794315048\n",
      "Prior Score 0.07839861418713447\n",
      "Value Score 0.5891231794315048\n",
      "Child UCBs [0.3418239136541695, 0.2224024684240194, 0.3521768252909828, 0.4245761250950757, 0.28515782477611984, 0.3448831402378972, 0.20678428883531186, 0.41453834592575006, 0.6675217936186393]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.2641537027958606\n",
      "Value Score 0.0\n",
      "Prior Score 0.3100145863930458\n",
      "Value Score 0.0\n",
      "Prior Score 0.334732468765893\n",
      "Value Score 0.0\n",
      "Prior Score 0.322979360378275\n",
      "Value Score 0.0\n",
      "Prior Score 0.30895912565225675\n",
      "Value Score 0.0\n",
      "Initial value 0.036210996098816395\n",
      "normalized value 0.5109292977567715\n",
      "Prior Score 0.07366529395328601\n",
      "Value Score 0.5109292977567715\n",
      "Prior Score 0.33609469426550587\n",
      "Value Score 0.0\n",
      "Prior Score 0.262242396081975\n",
      "Value Score 0.0\n",
      "Prior Score 0.28826474211046016\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2641537027958606, 0.3100145863930458, 0.334732468765893, 0.322979360378275, 0.30895912565225675, 0.5845945917100576, 0.33609469426550587, 0.262242396081975, 0.28826474211046016]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.27129055118124823\n",
      "Value Score 0.0\n",
      "Prior Score 0.23309132599726787\n",
      "Value Score 0.0\n",
      "Prior Score 0.2671128311340394\n",
      "Value Score 0.0\n",
      "Prior Score 0.2924118058827389\n",
      "Value Score 0.0\n",
      "Prior Score 0.3031199288615222\n",
      "Value Score 0.0\n",
      "Initial value -6.547446052233364e-05\n",
      "normalized value 0.4930422924096389\n",
      "Prior Score 0.07944273442093068\n",
      "Value Score 0.4930422924096389\n",
      "Prior Score 0.3108460021116328\n",
      "Value Score 0.0\n",
      "Prior Score 0.2621756306305217\n",
      "Value Score 0.0\n",
      "Prior Score 0.24268981311631\n",
      "Value Score 0.0\n",
      "Child UCBs [0.27129055118124823, 0.23309132599726787, 0.2671128311340394, 0.2924118058827389, 0.3031199288615222, 0.5724850268305696, 0.3108460021116328, 0.2621756306305217, 0.24268981311631]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2435896287076154\n",
      "Value Score 0.0\n",
      "Prior Score 0.2649779025341817\n",
      "Value Score 0.0\n",
      "Prior Score 0.2688887855679387\n",
      "Value Score 0.0\n",
      "Prior Score 0.22464696763268097\n",
      "Value Score 0.0\n",
      "Prior Score 0.23231667445829032\n",
      "Value Score 0.0\n",
      "Initial value 0.006886493414640427\n",
      "normalized value 0.49647013102345133\n",
      "Prior Score 0.09265774407330063\n",
      "Value Score 0.49647013102345133\n",
      "Prior Score 0.2260896024647911\n",
      "Value Score 0.0\n",
      "Prior Score 0.22041033094870657\n",
      "Value Score 0.0\n",
      "Prior Score 0.20652268375091554\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2435896287076154, 0.2649779025341817, 0.2688887855679387, 0.22464696763268097, 0.23231667445829032, 0.5891278750967519, 0.2260896024647911, 0.22041033094870657, 0.20652268375091554]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.19720695083520903\n",
      "Value Score 0.0\n",
      "Prior Score 0.19911406517260238\n",
      "Value Score 0.0\n",
      "Prior Score 0.20929562936005333\n",
      "Value Score 0.0\n",
      "Prior Score 0.1918115481266929\n",
      "Value Score 0.0\n",
      "Prior Score 0.1895107736706851\n",
      "Value Score 0.0\n",
      "Initial value 0.037539366632699966\n",
      "normalized value 0.5115842834948048\n",
      "Prior Score 0.11118783506218989\n",
      "Value Score 0.5115842834948048\n",
      "Prior Score 0.19175034869851545\n",
      "Value Score 0.0\n",
      "Prior Score 0.18540464242843996\n",
      "Value Score 0.0\n",
      "Prior Score 0.18151320973976\n",
      "Value Score 0.0\n",
      "Child UCBs [0.19720695083520903, 0.19911406517260238, 0.20929562936005333, 0.1918115481266929, 0.1895107736706851, 0.6227721185569947, 0.19175034869851545, 0.18540464242843996, 0.18151320973976]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.1417438403450271\n",
      "Value Score 0.0\n",
      "Prior Score 0.1421677476969627\n",
      "Value Score 0.0\n",
      "Prior Score 0.15600229239874566\n",
      "Value Score 0.0\n",
      "Prior Score 0.13059932323440782\n",
      "Value Score 0.0\n",
      "Prior Score 0.13143625920501467\n",
      "Value Score 0.0\n",
      "Prior Score 0.15615200537690602\n",
      "Value Score 0.0\n",
      "Prior Score 0.13202244409947161\n",
      "Value Score 0.0\n",
      "Prior Score 0.12935106310419428\n",
      "Value Score 0.0\n",
      "Prior Score 0.13062687399895917\n",
      "Value Score 0.0\n",
      "Child UCBs [0.1417438403450271, 0.1421677476969627, 0.15600229239874566, 0.13059932323440782, 0.13143625920501467, 0.15615200537690602, 0.13202244409947161, 0.12935106310419428, 0.13062687399895917]\n",
      "Selected action 5\n",
      "leaf value -0.03575228527188301\n",
      "leaf reward 0.003647223114967346\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.022575244646180756\n",
      "normalized value 0.4819432970149519\n",
      "Prior Score 0.05280818183184991\n",
      "Value Score 0.4819432970149519\n",
      "Initial value -0.746321900933981\n",
      "normalized value 0.12508222118858275\n",
      "Prior Score 0.417476320353158\n",
      "Value Score 0.12508222118858275\n",
      "Prior Score 0.1505328648002669\n",
      "Value Score 0.0\n",
      "Initial value -0.6790932342410088\n",
      "normalized value 0.1582309675268128\n",
      "Prior Score 0.20805738189419828\n",
      "Value Score 0.1582309675268128\n",
      "Child UCBs [0.5347514788468019, 0.5425585415417408, 0.1505328648002669, 0.3662883494210111]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.12220845857701601\n",
      "Value Score 0.0\n",
      "Initial value 1.0264283530414104\n",
      "normalized value 0.9991803013786233\n",
      "Prior Score 0.19225727196167744\n",
      "Value Score 0.9991803013786233\n",
      "Prior Score 0.08156131212513268\n",
      "Value Score 0.0\n",
      "Prior Score 0.12198131177069146\n",
      "Value Score 0.0\n",
      "Prior Score 0.06308353781031778\n",
      "Value Score 0.0\n",
      "Prior Score 0.4170202927309063\n",
      "Value Score 0.0\n",
      "Prior Score 0.03600708898929014\n",
      "Value Score 0.0\n",
      "Prior Score 0.05084374868829654\n",
      "Value Score 0.0\n",
      "Initial value 0.024453135207295418\n",
      "normalized value 0.5051317954806485\n",
      "Prior Score 0.5079508882413921\n",
      "Value Score 0.5051317954806485\n",
      "Child UCBs [0.12220845857701601, 1.1914375733403009, 0.08156131212513268, 0.12198131177069146, 0.06308353781031778, 0.4170202927309063, 0.03600708898929014, 0.05084374868829654, 1.0130826837220406]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.35848209719960084\n",
      "Value Score 0.0\n",
      "Prior Score 0.38328859627485085\n",
      "Value Score 0.0\n",
      "Prior Score 0.3701989598971224\n",
      "Value Score 0.0\n",
      "Prior Score 0.35139833951065186\n",
      "Value Score 0.0\n",
      "Prior Score 0.38999468961345457\n",
      "Value Score 0.0\n",
      "Initial value -0.05177793527642886\n",
      "normalized value 0.46754419270849845\n",
      "Prior Score 0.06532147450259751\n",
      "Value Score 0.46754419270849845\n",
      "Prior Score 0.3269070208327302\n",
      "Value Score 0.0\n",
      "Prior Score 0.31053647162731884\n",
      "Value Score 0.0\n",
      "Prior Score 0.3602097353656399\n",
      "Value Score 0.0\n",
      "Child UCBs [0.35848209719960084, 0.38328859627485085, 0.3701989598971224, 0.35139833951065186, 0.38999468961345457, 0.532865667211096, 0.3269070208327302, 0.31053647162731884, 0.3602097353656399]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.35445807857897843\n",
      "Value Score 0.0\n",
      "Prior Score 0.2986574288002728\n",
      "Value Score 0.0\n",
      "Prior Score 0.320514021830265\n",
      "Value Score 0.0\n",
      "Prior Score 0.3455648623571004\n",
      "Value Score 0.0\n",
      "Prior Score 0.35683567432010455\n",
      "Value Score 0.0\n",
      "Prior Score 0.3726215062070476\n",
      "Value Score 0.0\n",
      "Prior Score 0.3152248850470173\n",
      "Value Score 0.0\n",
      "Prior Score 0.3211220609127182\n",
      "Value Score 0.0\n",
      "Initial value 0.022112073376774788\n",
      "normalized value 0.503977477410703\n",
      "Prior Score 0.06295600514186292\n",
      "Value Score 0.503977477410703\n",
      "Child UCBs [0.35445807857897843, 0.2986574288002728, 0.320514021830265, 0.3455648623571004, 0.35683567432010455, 0.3726215062070476, 0.3152248850470173, 0.3211220609127182, 0.5669334825525659]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.28851299514201184\n",
      "Value Score 0.0\n",
      "Prior Score 0.29744352196893076\n",
      "Value Score 0.0\n",
      "Prior Score 0.2991177197502645\n",
      "Value Score 0.0\n",
      "Prior Score 0.3272751918267537\n",
      "Value Score 0.0\n",
      "Prior Score 0.3128053604038893\n",
      "Value Score 0.0\n",
      "Initial value 0.015752937644720078\n",
      "normalized value 0.5008419492546122\n",
      "Prior Score 0.07202835703978576\n",
      "Value Score 0.5008419492546122\n",
      "Prior Score 0.32475318848767654\n",
      "Value Score 0.0\n",
      "Prior Score 0.3097051137223155\n",
      "Value Score 0.0\n",
      "Prior Score 0.27601266970893135\n",
      "Value Score 0.0\n",
      "Child UCBs [0.28851299514201184, 0.29744352196893076, 0.2991177197502645, 0.3272751918267537, 0.3128053604038893, 0.572870306294398, 0.32475318848767654, 0.3097051137223155, 0.27601266970893135]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.27463691529747725\n",
      "Value Score 0.0\n",
      "Prior Score 0.26784958269478093\n",
      "Value Score 0.0\n",
      "Prior Score 0.29598911048212556\n",
      "Value Score 0.0\n",
      "Prior Score 0.26660256742855337\n",
      "Value Score 0.0\n",
      "Prior Score 0.2872381689745944\n",
      "Value Score 0.0\n",
      "Initial value 0.009132935355106989\n",
      "normalized value 0.4975777944312082\n",
      "Prior Score 0.0820550856220316\n",
      "Value Score 0.4975777944312082\n",
      "Prior Score 0.27809893595770774\n",
      "Value Score 0.0\n",
      "Prior Score 0.257483287401136\n",
      "Value Score 0.0\n",
      "Prior Score 0.24439010217692486\n",
      "Value Score 0.0\n",
      "Child UCBs [0.27463691529747725, 0.26784958269478093, 0.29598911048212556, 0.26660256742855337, 0.2872381689745944, 0.5796328800532398, 0.27809893595770774, 0.257483287401136, 0.24439010217692486]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.24448289810247373\n",
      "Value Score 0.0\n",
      "Prior Score 0.23849520042962602\n",
      "Value Score 0.0\n",
      "Prior Score 0.2616968916344514\n",
      "Value Score 0.0\n",
      "Prior Score 0.22876815222805374\n",
      "Value Score 0.0\n",
      "Prior Score 0.22844580276882226\n",
      "Value Score 0.0\n",
      "Initial value 0.001545017585158348\n",
      "normalized value 0.4938363850925388\n",
      "Prior Score 0.0906687285211538\n",
      "Value Score 0.4938363850925388\n",
      "Prior Score 0.23616666926085922\n",
      "Value Score 0.0\n",
      "Prior Score 0.22921703756359918\n",
      "Value Score 0.0\n",
      "Prior Score 0.22613721273777368\n",
      "Value Score 0.0\n",
      "Child UCBs [0.24448289810247373, 0.23849520042962602, 0.2616968916344514, 0.22876815222805374, 0.22844580276882226, 0.5845051136136926, 0.23616666926085922, 0.22921703756359918, 0.22613721273777368]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2004592275501426\n",
      "Value Score 0.0\n",
      "Prior Score 0.19562463079410736\n",
      "Value Score 0.0\n",
      "Prior Score 0.2182098542912883\n",
      "Value Score 0.0\n",
      "Prior Score 0.1848696809180779\n",
      "Value Score 0.0\n",
      "Prior Score 0.18603771270786476\n",
      "Value Score 0.0\n",
      "Initial value 0.03274882957339287\n",
      "normalized value 0.5092221914645092\n",
      "Prior Score 0.10997352169881253\n",
      "Value Score 0.5092221914645092\n",
      "Prior Score 0.18940572299489625\n",
      "Value Score 0.0\n",
      "Prior Score 0.18596777991316094\n",
      "Value Score 0.0\n",
      "Prior Score 0.18746105386418954\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2004592275501426, 0.19562463079410736, 0.2182098542912883, 0.1848696809180779, 0.18603771270786476, 0.6191957131633217, 0.18940572299489625, 0.18596777991316094, 0.18746105386418954]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.14207417941410846\n",
      "Value Score 0.0\n",
      "Prior Score 0.13665938307161324\n",
      "Value Score 0.0\n",
      "Prior Score 0.15557162309445566\n",
      "Value Score 0.0\n",
      "Prior Score 0.12965949568300356\n",
      "Value Score 0.0\n",
      "Prior Score 0.13074973476136512\n",
      "Value Score 0.0\n",
      "Prior Score 0.1535499204222852\n",
      "Value Score 0.0\n",
      "Prior Score 0.13406104233854405\n",
      "Value Score 0.0\n",
      "Prior Score 0.13209405000815003\n",
      "Value Score 0.0\n",
      "Prior Score 0.13568242066616362\n",
      "Value Score 0.0\n",
      "Child UCBs [0.14207417941410846, 0.13665938307161324, 0.15557162309445566, 0.12965949568300356, 0.13074973476136512, 0.1535499204222852, 0.13406104233854405, 0.13209405000815003, 0.13568242066616362]\n",
      "Selected action 2\n",
      "leaf value -0.01914280466735363\n",
      "leaf reward -0.008903414011001587\n",
      "at root\n",
      "selecting child\n",
      "Initial value -0.022575244646180756\n",
      "normalized value 0.4819432970149519\n",
      "Prior Score 0.053946162689267135\n",
      "Value Score 0.4819432970149519\n",
      "Initial value -0.7681946776468646\n",
      "normalized value 0.11429731108052736\n",
      "Prior Score 0.3909332668253224\n",
      "Value Score 0.11429731108052736\n",
      "Prior Score 0.15377674695285334\n",
      "Value Score 0.0\n",
      "Initial value -0.6790932342410088\n",
      "normalized value 0.1582309675268128\n",
      "Prior Score 0.2125408787620481\n",
      "Value Score 0.1582309675268128\n",
      "Child UCBs [0.5358894597042191, 0.5052305779058498, 0.15377674695285334, 0.3707718462888609]\n",
      "Selected action 1\n",
      "selecting child\n",
      "Prior Score 0.030044830994969914\n",
      "Value Score 0.0\n",
      "Prior Score 0.08594480853509386\n",
      "Value Score 0.0\n",
      "Prior Score 0.012459354403783965\n",
      "Value Score 0.0\n",
      "Initial value -0.3705548504367471\n",
      "normalized value 0.31036340036299886\n",
      "Prior Score 0.4454260280967796\n",
      "Value Score 0.31036340036299886\n",
      "Prior Score 0.12278238490306048\n",
      "Value Score 0.0\n",
      "Initial value 0.1989292677026242\n",
      "normalized value 0.5911615405680617\n",
      "Prior Score 0.196718793758246\n",
      "Value Score 0.5911615405680617\n",
      "Prior Score 0.021938900655393223\n",
      "Value Score 0.0\n",
      "Prior Score 0.04229856373494358\n",
      "Value Score 0.0\n",
      "Prior Score 0.725589254270264\n",
      "Value Score 0.0\n",
      "Child UCBs [0.030044830994969914, 0.08594480853509386, 0.012459354403783965, 0.7557894284597785, 0.12278238490306048, 0.7878803343263077, 0.021938900655393223, 0.04229856373494358, 0.725589254270264]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.35929346112730115\n",
      "Value Score 0.0\n",
      "Prior Score 0.06424138475736624\n",
      "Value Score 0.0\n",
      "Prior Score 0.24349334456200203\n",
      "Value Score 0.0\n",
      "Initial value -0.12907688239855425\n",
      "normalized value 0.4294300471154333\n",
      "Prior Score 0.17134231485263832\n",
      "Value Score 0.4294300471154333\n",
      "Prior Score 0.23198167006649964\n",
      "Value Score 0.0\n",
      "Prior Score 0.22609433918765012\n",
      "Value Score 0.0\n",
      "Prior Score 0.20695516449128643\n",
      "Value Score 0.0\n",
      "Prior Score 0.26385928368765443\n",
      "Value Score 0.0\n",
      "Prior Score 0.5701713520539695\n",
      "Value Score 0.0\n",
      "Child UCBs [0.35929346112730115, 0.06424138475736624, 0.24349334456200203, 0.6007723619680716, 0.23198167006649964, 0.22609433918765012, 0.20695516449128643, 0.26385928368765443, 0.5701713520539695]\n",
      "Selected action 3\n",
      "selecting child\n",
      "Prior Score 0.3692270448078087\n",
      "Value Score 0.0\n",
      "Prior Score 0.24023189394888914\n",
      "Value Score 0.0\n",
      "Prior Score 0.3804099223541829\n",
      "Value Score 0.0\n",
      "Prior Score 0.4586132850945232\n",
      "Value Score 0.0\n",
      "Prior Score 0.3080181834569707\n",
      "Value Score 0.0\n",
      "Prior Score 0.3725315216035722\n",
      "Value Score 0.0\n",
      "Prior Score 0.22336164565885766\n",
      "Value Score 0.0\n",
      "Prior Score 0.44777080336320374\n",
      "Value Score 0.0\n",
      "Initial value 0.18782917441179356\n",
      "normalized value 0.5856883667728882\n",
      "Prior Score 0.07258596557314693\n",
      "Value Score 0.5856883667728882\n",
      "Child UCBs [0.3692270448078087, 0.24023189394888914, 0.3804099223541829, 0.4586132850945232, 0.3080181834569707, 0.3725315216035722, 0.22336164565885766, 0.44777080336320374, 0.658274332346035]\n",
      "Selected action 8\n",
      "selecting child\n",
      "Prior Score 0.2893776561700499\n",
      "Value Score 0.0\n",
      "Prior Score 0.33961778100940127\n",
      "Value Score 0.0\n",
      "Prior Score 0.3666959661373578\n",
      "Value Score 0.0\n",
      "Prior Score 0.353820557154165\n",
      "Value Score 0.0\n",
      "Prior Score 0.3384615346569317\n",
      "Value Score 0.0\n",
      "Initial value 0.04217143058776855\n",
      "normalized value 0.5138682364663885\n",
      "Prior Score 0.06724964127467867\n",
      "Value Score 0.5138682364663885\n",
      "Prior Score 0.3681882701182627\n",
      "Value Score 0.0\n",
      "Prior Score 0.28728383938371577\n",
      "Value Score 0.0\n",
      "Prior Score 0.3157910509884249\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2893776561700499, 0.33961778100940127, 0.3666959661373578, 0.353820557154165, 0.3384615346569317, 0.5811178777410672, 0.3681882701182627, 0.28728383938371577, 0.3157910509884249]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.3033243984067444\n",
      "Value Score 0.0\n",
      "Prior Score 0.260614628574792\n",
      "Value Score 0.0\n",
      "Prior Score 0.2986533753485738\n",
      "Value Score 0.0\n",
      "Prior Score 0.3269396398813546\n",
      "Value Score 0.0\n",
      "Prior Score 0.3389121724537658\n",
      "Value Score 0.0\n",
      "Initial value -0.011435577645897865\n",
      "normalized value 0.4874359836001424\n",
      "Prior Score 0.07105863295597493\n",
      "Value Score 0.4874359836001424\n",
      "Prior Score 0.3475505364160645\n",
      "Value Score 0.0\n",
      "Prior Score 0.2931331927767065\n",
      "Value Score 0.0\n",
      "Prior Score 0.27134650006210087\n",
      "Value Score 0.0\n",
      "Child UCBs [0.3033243984067444, 0.260614628574792, 0.2986533753485738, 0.3269396398813546, 0.3389121724537658, 0.5584946165561173, 0.3475505364160645, 0.2931331927767065, 0.27134650006210087]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.2812845210831615\n",
      "Value Score 0.0\n",
      "Prior Score 0.30598257736749757\n",
      "Value Score 0.0\n",
      "Prior Score 0.310498659874783\n",
      "Value Score 0.0\n",
      "Prior Score 0.25941053007306314\n",
      "Value Score 0.0\n",
      "Prior Score 0.2682671050542554\n",
      "Value Score 0.0\n",
      "Initial value 0.018694709986448288\n",
      "normalized value 0.5022924624052911\n",
      "Prior Score 0.08024722554255968\n",
      "Value Score 0.5022924624052911\n",
      "Prior Score 0.26107640907620844\n",
      "Value Score 0.0\n",
      "Prior Score 0.2545182843441388\n",
      "Value Score 0.0\n",
      "Prior Score 0.23848155810202343\n",
      "Value Score 0.0\n",
      "Child UCBs [0.2812845210831615, 0.30598257736749757, 0.310498659874783, 0.25941053007306314, 0.2682671050542554, 0.5825396879478508, 0.26107640907620844, 0.2545182843441388, 0.23848155810202343]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.24153803089559076\n",
      "Value Score 0.0\n",
      "Prior Score 0.24387385445452636\n",
      "Value Score 0.0\n",
      "Prior Score 0.25634418044891266\n",
      "Value Score 0.0\n",
      "Prior Score 0.23492977017970604\n",
      "Value Score 0.0\n",
      "Prior Score 0.23211179378848149\n",
      "Value Score 0.0\n",
      "Initial value 0.0008351057767868042\n",
      "normalized value 0.4934863456284856\n",
      "Prior Score 0.09078818174418432\n",
      "Value Score 0.4934863456284856\n",
      "Prior Score 0.23485481344359044\n",
      "Value Score 0.0\n",
      "Prior Score 0.22708262594906023\n",
      "Value Score 0.0\n",
      "Prior Score 0.22231641976309313\n",
      "Value Score 0.0\n",
      "Child UCBs [0.24153803089559076, 0.24387385445452636, 0.25634418044891266, 0.23492977017970604, 0.23211179378848149, 0.5842745273726699, 0.23485481344359044, 0.22708262594906023, 0.22231641976309313]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.20046421992708602\n",
      "Value Score 0.0\n",
      "Prior Score 0.20106373985275106\n",
      "Value Score 0.0\n",
      "Prior Score 0.2206295368915401\n",
      "Value Score 0.0\n",
      "Prior Score 0.1847028512241762\n",
      "Value Score 0.0\n",
      "Prior Score 0.18588650559722145\n",
      "Value Score 0.0\n",
      "Initial value 0.03939950838685036\n",
      "normalized value 0.5125014721019072\n",
      "Prior Score 0.11042063581646788\n",
      "Value Score 0.5125014721019072\n",
      "Prior Score 0.18671552996480115\n",
      "Value Score 0.0\n",
      "Prior Score 0.182937472970982\n",
      "Value Score 0.0\n",
      "Prior Score 0.18474181547483243\n",
      "Value Score 0.0\n",
      "Child UCBs [0.20046421992708602, 0.20106373985275106, 0.2206295368915401, 0.1847028512241762, 0.18588650559722145, 0.6229221079183751, 0.18671552996480115, 0.182937472970982, 0.18474181547483243]\n",
      "Selected action 5\n",
      "selecting child\n",
      "Prior Score 0.14154898248661643\n",
      "Value Score 0.0\n",
      "Prior Score 0.13612337260874458\n",
      "Value Score 0.0\n",
      "Prior Score 0.15466916324669577\n",
      "Value Score 0.0\n",
      "Prior Score 0.1313583570431798\n",
      "Value Score 0.0\n",
      "Prior Score 0.13035159920304468\n",
      "Value Score 0.0\n",
      "Prior Score 0.15334101707606407\n",
      "Value Score 0.0\n",
      "Prior Score 0.13479416464394153\n",
      "Value Score 0.0\n",
      "Prior Score 0.13332431552135438\n",
      "Value Score 0.0\n",
      "Prior Score 0.13459085900207984\n",
      "Value Score 0.0\n",
      "Child UCBs [0.14154898248661643, 0.13612337260874458, 0.15466916324669577, 0.1313583570431798, 0.13035159920304468, 0.15334101707606407, 0.13479416464394153, 0.13332431552135438, 0.13459085900207984]\n",
      "Selected action 2\n",
      "leaf value -0.020871955901384354\n",
      "leaf reward -0.006984032690525055\n",
      "[(12, 1), (11, 3), (0, 5), (2, 8)]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils.utils import get_legal_moves\n",
    "\n",
    "# from muzero.muzero_mcts import Node\n",
    "# from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "root = Node(0.0)\n",
    "_, policy, hidden_state = agent.predict_single_initial_inference(\n",
    "    state,\n",
    "    info,\n",
    ")\n",
    "print(\"root policy\", policy)\n",
    "legal_moves = get_legal_moves(info)[0]\n",
    "to_play = env.agents.index(env.agent_selection)\n",
    "root.expand(legal_moves, to_play, policy, hidden_state, 0.0)\n",
    "print(\"expanded root\")\n",
    "min_max_stats = MinMaxStats(agent.config.known_bounds)\n",
    "\n",
    "for _ in range(agent.config.num_simulations):\n",
    "    print(\"at root\")\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "    to_play = env.agents.index(env.agent_selection)\n",
    "\n",
    "    # GO UNTIL A LEAF NODE IS REACHED\n",
    "    while node.expanded():\n",
    "        print(\"selecting child\")\n",
    "        action, node = node.select_child(\n",
    "            min_max_stats,\n",
    "            agent.config.pb_c_base,\n",
    "            agent.config.pb_c_init,\n",
    "            agent.config.discount_factor,\n",
    "            agent.config.game.num_players,\n",
    "        )\n",
    "        print(\"Selected action\", action)\n",
    "        # THIS NEEDS TO BE CHANGED FOR GAMES WHERE PLAYER COUNT DECREASES AS PLAYERS GET ELIMINATED, USE agent_selector.next() (clone of the current one)\n",
    "        to_play = (to_play + 1) % agent.config.game.num_players\n",
    "        search_path.append(node)\n",
    "    parent = search_path[-2]\n",
    "    reward, hidden_state, value, policy = agent.predict_single_recurrent_inference(\n",
    "        parent.hidden_state,\n",
    "        action,  # model=model\n",
    "    )\n",
    "    reward = reward.item()\n",
    "    value = value.item()\n",
    "    print(\"leaf value\", value)\n",
    "    print(\"leaf reward\", reward)\n",
    "\n",
    "    node.expand(\n",
    "        list(range(agent.num_actions)),\n",
    "        to_play,\n",
    "        policy,\n",
    "        hidden_state,\n",
    "        (\n",
    "            reward  # if self.config.game.has_intermediate_rewards else 0.0\n",
    "        ),  # for board games and games with no intermediate rewards\n",
    "    )\n",
    "\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "        min_max_stats.update(\n",
    "            node.reward\n",
    "            + agent.config.discount_factor\n",
    "            * (node.value() if agent.config.game.num_players == 1 else -node.value())\n",
    "        )\n",
    "        value = (\n",
    "            -node.reward\n",
    "            if node.to_play == to_play and agent.config.game.num_players > 1\n",
    "            else node.reward\n",
    "        ) + agent.config.discount_factor * value\n",
    "\n",
    "    visit_counts = [(child.visits, action) for action, child in root.children.items()]\n",
    "\n",
    "print(visit_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
