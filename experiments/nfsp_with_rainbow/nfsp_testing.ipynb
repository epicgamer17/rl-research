{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4073872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig, RainbowConfig\n",
    "from game_configs import MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 2,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": HuberLoss(),\n",
    "    \"min_replay_buffer_size\": 500,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 1000,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 500,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 20000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=MatchingPenniesConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f187c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "# env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=False)\n",
    "\n",
    "env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-MatchingPennies\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 500\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fde57c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1037a1ed0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1037a1ed0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1037a1ed0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1037a02e0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1037a02e0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0091e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-Standard\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96240cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/50000 [00:00<17:54, 46.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 65/200000\n",
      "   Player 0 SL buffer: 8/2000000\n",
      "   Player 1 RL buffer: 62/200000\n",
      "   Player 1 SL buffer: 7/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1005/50000 [00:28<23:02, 35.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0019 â†’ 0.0019\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 63883/200000\n",
      "   Player 0 SL buffer: 7051/2000000\n",
      "   Player 1 RL buffer: 64245/200000\n",
      "   Player 1 SL buffer: 7171/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1999/50000 [01:07<28:35, 27.97it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0013 â†’ 0.0013\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 127674/200000\n",
      "   Player 0 SL buffer: 13775/2000000\n",
      "   Player 1 RL buffer: 128452/200000\n",
      "   Player 1 SL buffer: 13827/2000000\n",
      "P1 SL Buffer Size:  13775\n",
      "P1 SL buffer distribution [4330. 7703.  629. 1113.]\n",
      "P1 actions distribution [0.31433757 0.55920145 0.04566243 0.08079855]\n",
      "P2 SL Buffer Size:  13827\n",
      "P2 SL buffer distribution [4532. 7118.  874. 1303.]\n",
      "P2 actions distribution [0.32776452 0.5147899  0.06320966 0.09423592]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.9876,  2.5510, -0.5471,  1.6790]])\n",
      "Player 0 Prediction: tensor([[0.1569, 0.8225, 0.0207, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.6371,  3.4609, -2.0920,  2.5180]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7888, 0.0563, 0.1549]])\n",
      "Player 1 Prediction: tensor([[ 1.2185,  2.0621, -2.9521,  1.1818]])\n",
      "Player 0 Prediction: tensor([[0.9204, 0.0000, 0.0796, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1999/50000 [01:20<28:35, 27.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53909\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5440/10000 (54.4%)\n",
      "    Average reward: -0.990\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4560/10000 (45.6%)\n",
      "    Average reward: +0.990\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8532 (32.0%)\n",
      "    Action 1: 15794 (59.2%)\n",
      "    Action 2: 1247 (4.7%)\n",
      "    Action 3: 1104 (4.1%)\n",
      "  Player 1:\n",
      "    Action 0: 7128 (26.2%)\n",
      "    Action 1: 14693 (54.0%)\n",
      "    Action 2: 3154 (11.6%)\n",
      "    Action 3: 2257 (8.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-9898.5, 9898.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.974 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.986 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.980\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.9899\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.2766, 0.7017, 0.0217, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8591, 0.0493, 0.0915]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48366\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5519/10000 (55.2%)\n",
      "    Average reward: -0.094\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4481/10000 (44.8%)\n",
      "    Average reward: +0.094\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2608 (11.5%)\n",
      "    Action 1: 17400 (77.0%)\n",
      "    Action 2: 1040 (4.6%)\n",
      "    Action 3: 1547 (6.8%)\n",
      "  Player 1:\n",
      "    Action 0: 22351 (86.7%)\n",
      "    Action 1: 3420 (13.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-938.5, 938.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.650 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.565 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.607\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0939\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4635, 0.4952, 0.0412, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.4337,  1.5382, -1.0009,  0.9534]])\n",
      "Player 1 Prediction: tensor([[0.9141, 0.0000, 0.0859, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55667\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6108/10000 (61.1%)\n",
      "    Average reward: +1.064\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3892/10000 (38.9%)\n",
      "    Average reward: -1.064\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8509 (30.3%)\n",
      "    Action 1: 14248 (50.8%)\n",
      "    Action 2: 2548 (9.1%)\n",
      "    Action 3: 2752 (9.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9229 (33.4%)\n",
      "    Action 1: 14939 (54.1%)\n",
      "    Action 2: 1863 (6.7%)\n",
      "    Action 3: 1579 (5.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [10643.5, -10643.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.018 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.008 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.013\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.0643\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9091, 0.0305, 0.0604]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8165, 0.0635, 0.1200]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48740\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6558/10000 (65.6%)\n",
      "    Average reward: +0.243\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3442/10000 (34.4%)\n",
      "    Average reward: -0.243\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21359 (83.0%)\n",
      "    Action 1: 4360 (17.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3490 (15.2%)\n",
      "    Action 1: 16296 (70.8%)\n",
      "    Action 2: 1376 (6.0%)\n",
      "    Action 3: 1859 (8.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2432.0, -2432.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.657 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.765 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.711\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.2432\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:315: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:379: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:245: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:265: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "  6%|â–Œ         | 3003/50000 [02:21<28:40, 27.31it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0011 â†’ 0.0011\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 191321/200000\n",
      "   Player 0 SL buffer: 20269/2000000\n",
      "   Player 1 RL buffer: 192805/200000\n",
      "   Player 1 SL buffer: 20218/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4000/50000 [02:55<27:05, 28.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0009 â†’ 0.0009\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 26832/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 26913/2000000\n",
      "P1 SL Buffer Size:  26832\n",
      "P1 SL buffer distribution [ 7979. 14519.  1859.  2475.]\n",
      "P1 actions distribution [0.29736881 0.54110763 0.06928295 0.09224061]\n",
      "P2 SL Buffer Size:  26913\n",
      "P2 SL buffer distribution [ 9034. 12955.  2229.  2695.]\n",
      "P2 actions distribution [0.33567421 0.48136588 0.08282243 0.10013748]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.8158,  3.4494, -0.5353,  2.5612]])\n",
      "Player 0 Prediction: tensor([[0.0451, 0.9454, 0.0094, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.8776,  3.9054, -2.0001,  2.8937]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7997, 0.0640, 0.1362]])\n",
      "Player 1 Prediction: tensor([[ 0.5948,  1.3128, -2.9548,  0.7965]])\n",
      "Player 0 Prediction: tensor([[0.9735, 0.0000, 0.0265, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4000/50000 [03:10<27:05, 28.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56531\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5222/10000 (52.2%)\n",
      "    Average reward: -0.899\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4778/10000 (47.8%)\n",
      "    Average reward: +0.899\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8874 (31.4%)\n",
      "    Action 1: 15582 (55.2%)\n",
      "    Action 2: 1875 (6.6%)\n",
      "    Action 3: 1920 (6.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7388 (26.1%)\n",
      "    Action 1: 14145 (50.0%)\n",
      "    Action 2: 2717 (9.6%)\n",
      "    Action 3: 4030 (14.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-8987.5, 8987.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.998 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.8988\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9776, 0.0100, 0.0123]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8915, 0.0451, 0.0634]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47851\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5066/10000 (50.7%)\n",
      "    Average reward: -0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4934/10000 (49.3%)\n",
      "    Average reward: +0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2212 (9.7%)\n",
      "    Action 1: 16707 (73.6%)\n",
      "    Action 2: 1728 (7.6%)\n",
      "    Action 3: 2041 (9.0%)\n",
      "  Player 1:\n",
      "    Action 0: 21752 (86.4%)\n",
      "    Action 1: 3411 (13.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2544.0, 2544.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.653 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.572 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.613\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.2544\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.2911,  1.5763, -0.5860,  1.1163]])\n",
      "Player 1 Prediction: tensor([[0.1922, 0.7942, 0.0136, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-2.5333, -1.9197, -2.0952, -1.7527]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8865, 0.0449, 0.0686]])\n",
      "Player 0 Prediction: tensor([[-3.6671, -2.8501, -2.0127, -2.3585]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55800\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6235/10000 (62.4%)\n",
      "    Average reward: +0.903\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3765/10000 (37.6%)\n",
      "    Average reward: -0.903\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7196 (25.9%)\n",
      "    Action 1: 15442 (55.5%)\n",
      "    Action 2: 2760 (9.9%)\n",
      "    Action 3: 2424 (8.7%)\n",
      "  Player 1:\n",
      "    Action 0: 9528 (34.1%)\n",
      "    Action 1: 14150 (50.6%)\n",
      "    Action 2: 2620 (9.4%)\n",
      "    Action 3: 1680 (6.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9031.0, -9031.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.976 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.001\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9031\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4856, 0.4854, 0.0290, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.2678, 0.6902, 0.0420, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.4082, 0.4502, 0.1416, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48977\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6644/10000 (66.4%)\n",
      "    Average reward: +0.272\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3356/10000 (33.6%)\n",
      "    Average reward: -0.272\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20934 (81.8%)\n",
      "    Action 1: 4670 (18.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3644 (15.6%)\n",
      "    Action 1: 15892 (68.0%)\n",
      "    Action 2: 1996 (8.5%)\n",
      "    Action 3: 1841 (7.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2717.0, -2717.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.685 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.796 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.741\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.2717\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5005/50000 [04:01<24:41, 30.37it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 33465/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 33411/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5999/50000 [04:40<35:30, 20.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 39924/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 40167/2000000\n",
      "P1 SL Buffer Size:  39924\n",
      "P1 SL buffer distribution [12419. 20646.  3245.  3614.]\n",
      "P1 actions distribution [0.31106603 0.51713255 0.08127943 0.09052199]\n",
      "P2 SL Buffer Size:  40167\n",
      "P2 SL buffer distribution [13138. 19512.  3533.  3984.]\n",
      "P2 actions distribution [0.32708442 0.4857719  0.08795778 0.0991859 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5999/50000 [04:51<35:30, 20.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.2065, 0.7743, 0.0192, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.1156,  0.2856, -0.9552,  0.2312]])\n",
      "Player 0 Prediction: tensor([[0.9813, 0.0000, 0.0187, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56057\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4966/10000 (49.7%)\n",
      "    Average reward: -0.922\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5034/10000 (50.3%)\n",
      "    Average reward: +0.922\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8956 (31.9%)\n",
      "    Action 1: 14898 (53.0%)\n",
      "    Action 2: 2372 (8.4%)\n",
      "    Action 3: 1874 (6.7%)\n",
      "  Player 1:\n",
      "    Action 0: 7009 (25.1%)\n",
      "    Action 1: 14468 (51.8%)\n",
      "    Action 2: 2521 (9.0%)\n",
      "    Action 3: 3959 (14.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-9224.5, 9224.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.992 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.9224\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7630, 0.1007, 0.1363]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3164, 0.2560, 0.4276]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48044\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4912/10000 (49.1%)\n",
      "    Average reward: -0.273\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5088/10000 (50.9%)\n",
      "    Average reward: +0.273\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2262 (9.8%)\n",
      "    Action 1: 16130 (70.0%)\n",
      "    Action 2: 2233 (9.7%)\n",
      "    Action 3: 2414 (10.5%)\n",
      "  Player 1:\n",
      "    Action 0: 21127 (84.5%)\n",
      "    Action 1: 3878 (15.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2729.5, 2729.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.689 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.622 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.656\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.2730\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.9150,  1.2141, -0.6180,  1.0244]])\n",
      "Player 1 Prediction: tensor([[0.1504, 0.8460, 0.0037, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.8002,  1.2981, -1.9182,  1.0417]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8538, 0.0300, 0.1161]])\n",
      "Player 0 Prediction: tensor([[-3.9760, -4.5988, -2.9019, -3.2106]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53086\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6580/10000 (65.8%)\n",
      "    Average reward: +1.117\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3420/10000 (34.2%)\n",
      "    Average reward: -1.117\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9442 (35.9%)\n",
      "    Action 1: 13231 (50.3%)\n",
      "    Action 2: 2475 (9.4%)\n",
      "    Action 3: 1171 (4.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7199 (26.9%)\n",
      "    Action 1: 14506 (54.2%)\n",
      "    Action 2: 3233 (12.1%)\n",
      "    Action 3: 1829 (6.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [11172.0, -11172.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.989 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.009\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.1172\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6537, 0.3179, 0.0285, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.2837, 0.6715, 0.0448, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2220, 0.1794, 0.5986]])\n",
      "Player 1 Prediction: tensor([[0.0537, 0.3021, 0.6442, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48622\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6542/10000 (65.4%)\n",
      "    Average reward: +0.207\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3458/10000 (34.6%)\n",
      "    Average reward: -0.207\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21020 (82.7%)\n",
      "    Action 1: 4394 (17.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3079 (13.3%)\n",
      "    Action 1: 16074 (69.3%)\n",
      "    Action 2: 2198 (9.5%)\n",
      "    Action 3: 1857 (8.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2067.0, -2067.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.664 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.754 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.709\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.2067\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7004/50000 [05:54<26:36, 26.93it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 46371/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 46622/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [06:32<25:38, 27.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 52896/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 53291/2000000\n",
      "P1 SL Buffer Size:  52896\n",
      "P1 SL buffer distribution [16936. 26870.  4439.  4651.]\n",
      "P1 actions distribution [0.32017544 0.50797792 0.08391939 0.08792725]\n",
      "P2 SL Buffer Size:  53291\n",
      "P2 SL buffer distribution [17136. 26299.  4782.  5074.]\n",
      "P2 actions distribution [0.32155523 0.49349796 0.08973373 0.09521308]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.6025, 0.3543, 0.0432, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.5172,  1.2355, -0.9843,  1.0505]])\n",
      "Player 0 Prediction: tensor([[0.3522, 0.5874, 0.0604, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-1.5048, -1.6517, -2.1242, -1.6446]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3136, 0.2567, 0.4297]])\n",
      "Player 1 Prediction: tensor([[-3.1221, -4.3363, -1.9980, -3.2287]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56735\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4945/10000 (49.5%)\n",
      "    Average reward: -0.774\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5055/10000 (50.5%)\n",
      "    Average reward: +0.774\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9094 (32.0%)\n",
      "    Action 1: 14816 (52.2%)\n",
      "    Action 2: 2775 (9.8%)\n",
      "    Action 3: 1699 (6.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7415 (26.2%)\n",
      "    Action 1: 15456 (54.5%)\n",
      "    Action 2: 2527 (8.9%)\n",
      "    Action 3: 2953 (10.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7737.0, 7737.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.983 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.999\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7737\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1175, 0.8805, 0.0019, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9356, 0.0187, 0.0458]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [06:51<25:38, 27.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48490\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4982/10000 (49.8%)\n",
      "    Average reward: -0.123\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5018/10000 (50.2%)\n",
      "    Average reward: +0.123\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2491 (10.7%)\n",
      "    Action 1: 15812 (67.8%)\n",
      "    Action 2: 2465 (10.6%)\n",
      "    Action 3: 2538 (10.9%)\n",
      "  Player 1:\n",
      "    Action 0: 20742 (82.4%)\n",
      "    Action 1: 4442 (17.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1232.0, 1232.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.725 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.672 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.698\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1232\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7282, 0.2506, 0.0213, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.4871,  0.9073, -1.0018,  0.7390]])\n",
      "Player 1 Prediction: tensor([[0.3024, 0.6655, 0.0321, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.8192,  1.4275, -1.9569,  1.0364]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2162, 0.1205, 0.6633]])\n",
      "Player 0 Prediction: tensor([[ 0.9068,  1.1751, -2.9670,  1.2240]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56709\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6510/10000 (65.1%)\n",
      "    Average reward: +0.832\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3490/10000 (34.9%)\n",
      "    Average reward: -0.832\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7445 (26.3%)\n",
      "    Action 1: 15037 (53.1%)\n",
      "    Action 2: 2366 (8.4%)\n",
      "    Action 3: 3465 (12.2%)\n",
      "  Player 1:\n",
      "    Action 0: 8855 (31.2%)\n",
      "    Action 1: 14277 (50.3%)\n",
      "    Action 2: 3003 (10.6%)\n",
      "    Action 3: 2261 (8.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8318.5, -8318.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.992 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.007\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8318\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7115, 0.0643, 0.2242]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4528, 0.2966, 0.2506]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48426\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6449/10000 (64.5%)\n",
      "    Average reward: +0.095\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3551/10000 (35.5%)\n",
      "    Average reward: -0.095\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21046 (83.0%)\n",
      "    Action 1: 4307 (17.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2803 (12.1%)\n",
      "    Action 1: 16046 (69.5%)\n",
      "    Action 2: 2330 (10.1%)\n",
      "    Action 3: 1894 (8.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [952.0, -952.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.657 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.734 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.696\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0952\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9004/50000 [07:38<22:54, 29.83it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 59250/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 59870/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 10000/50000 [08:19<28:03, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 65727/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 66440/2000000\n",
      "P1 SL Buffer Size:  65727\n",
      "P1 SL buffer distribution [21169. 33483.  5622.  5453.]\n",
      "P1 actions distribution [0.32207464 0.50942535 0.08553562 0.08296438]\n",
      "P2 SL Buffer Size:  66440\n",
      "P2 SL buffer distribution [20768. 33505.  5998.  6169.]\n",
      "P2 actions distribution [0.31258278 0.50428958 0.09027694 0.09285069]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.5055,  1.3937, -0.6673,  1.7244]])\n",
      "Player 0 Prediction: tensor([[0.4191, 0.5412, 0.0397, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.4750,  1.4670, -1.8893,  0.9966]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3120, 0.0995, 0.5885]])\n",
      "Player 1 Prediction: tensor([[ 0.7111,  1.2275, -3.0509,  0.8566]])\n",
      "Player 0 Prediction: tensor([[0.1007, 0.1986, 0.7007, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 10000/50000 [08:31<28:03, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55768\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4368/10000 (43.7%)\n",
      "    Average reward: -0.596\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5632/10000 (56.3%)\n",
      "    Average reward: +0.596\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9598 (33.9%)\n",
      "    Action 1: 13842 (48.9%)\n",
      "    Action 2: 3571 (12.6%)\n",
      "    Action 3: 1306 (4.6%)\n",
      "  Player 1:\n",
      "    Action 0: 6317 (23.0%)\n",
      "    Action 1: 17630 (64.2%)\n",
      "    Action 2: 2134 (7.8%)\n",
      "    Action 3: 1370 (5.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5961.5, 5961.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.898 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.966\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5961\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.6563, 0.3090, 0.0347, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.4191, 0.5412, 0.0397, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3120, 0.0995, 0.5885]])\n",
      "Player 0 Prediction: tensor([[0.1007, 0.1986, 0.7007, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48666\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5006/10000 (50.1%)\n",
      "    Average reward: -0.092\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4994/10000 (49.9%)\n",
      "    Average reward: +0.092\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2519 (10.8%)\n",
      "    Action 1: 15778 (67.6%)\n",
      "    Action 2: 2509 (10.7%)\n",
      "    Action 3: 2545 (10.9%)\n",
      "  Player 1:\n",
      "    Action 0: 20684 (81.7%)\n",
      "    Action 1: 4631 (18.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-918.0, 918.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.729 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.686 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.708\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0918\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.7950,  1.4640, -0.6184,  0.9693]])\n",
      "Player 1 Prediction: tensor([[1.1235e-01, 8.8710e-01, 5.4386e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.4135,  1.2989, -1.8094,  0.8168]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9573, 0.0024, 0.0402]])\n",
      "Player 0 Prediction: tensor([[ 5.7442,  6.7842, -3.1340,  4.9810]])\n",
      "Player 1 Prediction: tensor([[0.9920, 0.0000, 0.0080, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55301\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6495/10000 (65.0%)\n",
      "    Average reward: +0.655\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3505/10000 (35.0%)\n",
      "    Average reward: -0.655\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7016 (25.6%)\n",
      "    Action 1: 17232 (62.8%)\n",
      "    Action 2: 2567 (9.4%)\n",
      "    Action 3: 609 (2.2%)\n",
      "  Player 1:\n",
      "    Action 0: 8048 (28.9%)\n",
      "    Action 1: 14446 (51.8%)\n",
      "    Action 2: 3560 (12.8%)\n",
      "    Action 3: 1823 (6.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6545.5, -6545.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.009 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.967\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6545\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6326, 0.0619, 0.3055]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4843, 0.2232, 0.2925]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48979\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6427/10000 (64.3%)\n",
      "    Average reward: +0.103\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3573/10000 (35.7%)\n",
      "    Average reward: -0.103\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21140 (82.2%)\n",
      "    Action 1: 4565 (17.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2712 (11.7%)\n",
      "    Action 1: 16078 (69.1%)\n",
      "    Action 2: 2281 (9.8%)\n",
      "    Action 3: 2203 (9.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1034.0, -1034.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.675 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.730 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.702\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1034\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11003/50000 [09:29<24:30, 26.52it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 72080/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 72683/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11998/50000 [10:09<25:29, 24.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 78339/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 78774/2000000\n",
      "P1 SL Buffer Size:  78339\n",
      "P1 SL buffer distribution [25125. 39893.  6771.  6550.]\n",
      "P1 actions distribution [0.32072148 0.5092355  0.08643205 0.08361097]\n",
      "P2 SL Buffer Size:  78774\n",
      "P2 SL buffer distribution [24609. 39605.  7154.  7406.]\n",
      "P2 actions distribution [0.31240003 0.50276741 0.09081677 0.09401579]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.6880, 0.2840, 0.0280, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.6141,  1.0103, -1.0565,  1.4589]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4803, 0.1040, 0.4157]])\n",
      "Player 1 Prediction: tensor([[ 2.1422,  3.3378, -0.8530,  3.9413]])\n",
      "Player 0 Prediction: tensor([[0.0900, 0.0000, 0.9100, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11998/50000 [10:21<25:29, 24.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50708\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4813/10000 (48.1%)\n",
      "    Average reward: -0.752\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5187/10000 (51.9%)\n",
      "    Average reward: +0.752\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6129 (24.3%)\n",
      "    Action 1: 14250 (56.4%)\n",
      "    Action 2: 2963 (11.7%)\n",
      "    Action 3: 1929 (7.6%)\n",
      "  Player 1:\n",
      "    Action 0: 8888 (34.9%)\n",
      "    Action 1: 11208 (44.1%)\n",
      "    Action 2: 2680 (10.5%)\n",
      "    Action 3: 2661 (10.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7515.5, 7515.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.962 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.006\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7516\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.6880, 0.2840, 0.0280, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.4499, 0.5241, 0.0260, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0613, 0.1404, 0.7983, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48764\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5141/10000 (51.4%)\n",
      "    Average reward: -0.006\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4859/10000 (48.6%)\n",
      "    Average reward: +0.006\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2396 (10.3%)\n",
      "    Action 1: 15738 (67.7%)\n",
      "    Action 2: 2357 (10.1%)\n",
      "    Action 3: 2760 (11.9%)\n",
      "  Player 1:\n",
      "    Action 0: 20742 (81.3%)\n",
      "    Action 1: 4771 (18.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-55.5, 55.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.719 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.695 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.707\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0056\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.7476,  1.0272, -0.6992,  0.8981]])\n",
      "Player 1 Prediction: tensor([[1.2265e-01, 8.7703e-01, 3.2012e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.1684,  0.9274, -1.6868,  0.5964]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9453, 0.0015, 0.0532]])\n",
      "Player 0 Prediction: tensor([[-3.2422, -5.6483, -3.0302, -4.0701]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53076\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6369/10000 (63.7%)\n",
      "    Average reward: +0.841\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3631/10000 (36.3%)\n",
      "    Average reward: -0.841\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9251 (35.0%)\n",
      "    Action 1: 12255 (46.3%)\n",
      "    Action 2: 2618 (9.9%)\n",
      "    Action 3: 2325 (8.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7122 (26.7%)\n",
      "    Action 1: 14596 (54.8%)\n",
      "    Action 2: 2644 (9.9%)\n",
      "    Action 3: 2265 (8.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8405.5, -8405.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.984 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.014\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8406\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4846, 0.0646, 0.4508]])\n",
      "Player 1 Prediction: tensor([[0.1942, 0.7243, 0.0815, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48860\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6336/10000 (63.4%)\n",
      "    Average reward: +0.053\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3664/10000 (36.6%)\n",
      "    Average reward: -0.053\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20912 (81.2%)\n",
      "    Action 1: 4846 (18.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2786 (12.1%)\n",
      "    Action 1: 15897 (68.8%)\n",
      "    Action 2: 1982 (8.6%)\n",
      "    Action 3: 2437 (10.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [526.0, -526.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.698 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.739 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.718\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0526\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13002/50000 [11:20<31:35, 19.52it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 84465/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 84765/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13998/50000 [12:01<24:58, 24.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 90932/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 90863/2000000\n",
      "P1 SL Buffer Size:  90932\n",
      "P1 SL buffer distribution [29287. 45831.  8008.  7806.]\n",
      "P1 actions distribution [0.32207584 0.50401399 0.08806581 0.08584437]\n",
      "P2 SL Buffer Size:  90863\n",
      "P2 SL buffer distribution [28944. 45004.  8361.  8554.]\n",
      "P2 actions distribution [0.3185455  0.49529511 0.09201765 0.09414173]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[8.7769e-02, 9.1197e-01, 2.5842e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.0493, -0.4834, -1.0713,  0.1920]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9378, 0.0054, 0.0569]])\n",
      "Player 1 Prediction: tensor([[ 4.4674,  5.8208, -2.0227,  3.7841]])\n",
      "Player 0 Prediction: tensor([[0.9856, 0.0000, 0.0144, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51755\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5081/10000 (50.8%)\n",
      "    Average reward: -0.677\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4919/10000 (49.2%)\n",
      "    Average reward: +0.677\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6463 (25.1%)\n",
      "    Action 1: 14326 (55.7%)\n",
      "    Action 2: 2737 (10.6%)\n",
      "    Action 3: 2211 (8.6%)\n",
      "  Player 1:\n",
      "    Action 0: 9286 (35.7%)\n",
      "    Action 1: 11760 (45.2%)\n",
      "    Action 2: 2719 (10.5%)\n",
      "    Action 3: 2253 (8.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6774.0, 6774.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.010\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6774\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9009, 0.0050, 0.0941]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7310, 0.0222, 0.2467]])\n",
      "Player 0 Prediction: tensor([[0.1145, 0.5609, 0.3246, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13998/50000 [12:21<24:58, 24.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48663\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5111/10000 (51.1%)\n",
      "    Average reward: -0.043\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4889/10000 (48.9%)\n",
      "    Average reward: +0.043\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2456 (10.6%)\n",
      "    Action 1: 15501 (67.1%)\n",
      "    Action 2: 2235 (9.7%)\n",
      "    Action 3: 2895 (12.5%)\n",
      "  Player 1:\n",
      "    Action 0: 20586 (80.5%)\n",
      "    Action 1: 4990 (19.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-427.0, 427.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.730 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.712 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.721\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0427\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.8695,  2.3509, -0.7221,  2.3382]])\n",
      "Player 1 Prediction: tensor([[0.3756, 0.6134, 0.0110, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.0272,  1.7573, -2.3484,  1.2048]])\n",
      "Player 1 Prediction: tensor([[0.0354, 0.0813, 0.8833, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53817\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6299/10000 (63.0%)\n",
      "    Average reward: +0.754\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3701/10000 (37.0%)\n",
      "    Average reward: -0.754\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8735 (33.1%)\n",
      "    Action 1: 13510 (51.2%)\n",
      "    Action 2: 2561 (9.7%)\n",
      "    Action 3: 1587 (6.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7635 (27.8%)\n",
      "    Action 1: 14152 (51.6%)\n",
      "    Action 2: 3207 (11.7%)\n",
      "    Action 3: 2430 (8.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7538.5, -7538.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.014\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7539\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8684, 0.1220, 0.0096, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.3756, 0.6134, 0.0110, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7471, 0.0243, 0.2287]])\n",
      "Player 1 Prediction: tensor([[0.0876, 0.1806, 0.7319, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48755\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6260/10000 (62.6%)\n",
      "    Average reward: +0.029\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3740/10000 (37.4%)\n",
      "    Average reward: -0.029\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20319 (79.0%)\n",
      "    Action 1: 5404 (21.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3116 (13.5%)\n",
      "    Action 1: 15341 (66.6%)\n",
      "    Action 2: 1842 (8.0%)\n",
      "    Action 3: 2733 (11.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [287.0, -287.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.742 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.781 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.761\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0287\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15004/50000 [13:14<23:33, 24.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 96972/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 97096/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16000/50000 [13:57<23:20, 24.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 103451/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 103494/2000000\n",
      "P1 SL Buffer Size:  103451\n",
      "P1 SL buffer distribution [33258. 51816.  9270.  9107.]\n",
      "P1 actions distribution [0.32148553 0.50087481 0.08960764 0.08803202]\n",
      "P2 SL Buffer Size:  103494\n",
      "P2 SL buffer distribution [33903. 49807.  9574. 10210.]\n",
      "P2 actions distribution [0.32758421 0.48125495 0.09250778 0.09865306]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[8.0737e-02, 9.1909e-01, 1.6934e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.4182,  2.3985, -1.3833,  1.8471]])\n",
      "Player 0 Prediction: tensor([[9.9966e-01, 0.0000e+00, 3.3776e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.3881,  0.2094, -2.9431,  0.5407]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.8287e-01, 8.5027e-04, 1.1628e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16000/50000 [14:11<23:20, 24.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53889\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4958/10000 (49.6%)\n",
      "    Average reward: -0.563\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5042/10000 (50.4%)\n",
      "    Average reward: +0.563\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7625 (28.5%)\n",
      "    Action 1: 13483 (50.3%)\n",
      "    Action 2: 2311 (8.6%)\n",
      "    Action 3: 3367 (12.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10293 (38.0%)\n",
      "    Action 1: 10408 (38.4%)\n",
      "    Action 2: 2393 (8.8%)\n",
      "    Action 3: 4009 (14.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5628.5, 5628.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.014 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5628\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8707, 0.0039, 0.1254]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6253, 0.0603, 0.3144]])\n",
      "Player 0 Prediction: tensor([[0.0234, 0.3771, 0.5996, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48967\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5219/10000 (52.2%)\n",
      "    Average reward: +0.000\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4781/10000 (47.8%)\n",
      "    Average reward: -0.000\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2749 (11.8%)\n",
      "    Action 1: 15286 (65.8%)\n",
      "    Action 2: 2134 (9.2%)\n",
      "    Action 3: 3061 (13.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20287 (78.8%)\n",
      "    Action 1: 5450 (21.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1.0, -1.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.762 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.745 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.753\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.0001\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.8342e-01, 8.1642e-01, 1.5774e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.4976,  2.3240, -1.3311,  1.9714]])\n",
      "Player 1 Prediction: tensor([[9.9975e-01, 0.0000e+00, 2.4742e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.2931,  3.1111, -3.2962,  1.7606]])\n",
      "Player 1 Prediction: tensor([[0.1362, 0.8233, 0.0405, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.7449,  3.4867, -5.0051,  2.2267]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52395\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6110/10000 (61.1%)\n",
      "    Average reward: +0.717\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3890/10000 (38.9%)\n",
      "    Average reward: -0.717\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9239 (35.1%)\n",
      "    Action 1: 12438 (47.3%)\n",
      "    Action 2: 2819 (10.7%)\n",
      "    Action 3: 1810 (6.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7137 (27.4%)\n",
      "    Action 1: 13591 (52.1%)\n",
      "    Action 2: 2702 (10.4%)\n",
      "    Action 3: 2659 (10.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7171.0, -7171.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.002 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7171\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2742, 0.0481, 0.6777]])\n",
      "Player 1 Prediction: tensor([[0.2162, 0.6390, 0.1448, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49220\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6212/10000 (62.1%)\n",
      "    Average reward: -0.018\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3788/10000 (37.9%)\n",
      "    Average reward: +0.018\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19688 (75.9%)\n",
      "    Action 1: 6257 (24.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3648 (15.7%)\n",
      "    Action 1: 14755 (63.4%)\n",
      "    Action 2: 1751 (7.5%)\n",
      "    Action 3: 3121 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-185.0, 185.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.797 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.836 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.816\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0185\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17005/50000 [15:08<22:36, 24.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 109804/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 109796/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17999/50000 [15:50<22:20, 23.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 116141/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 116337/2000000\n",
      "P1 SL Buffer Size:  116141\n",
      "P1 SL buffer distribution [37558. 57432. 10562. 10589.]\n",
      "P1 actions distribution [0.32338278 0.49450237 0.09094118 0.09117366]\n",
      "P2 SL Buffer Size:  116337\n",
      "P2 SL buffer distribution [38579. 54754. 10754. 12250.]\n",
      "P2 actions distribution [0.33161419 0.47064992 0.09243835 0.10529754]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17999/50000 [16:01<22:20, 23.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[7.8044e-02, 9.2184e-01, 1.1283e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.1848,  0.5862, -1.1555,  0.6548]])\n",
      "Player 0 Prediction: tensor([[9.9977e-01, 0.0000e+00, 2.3173e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.1852, -0.2986, -2.9423,  0.2629]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.5002e-01, 4.2567e-04, 1.4956e-01]])\n",
      "Player 1 Prediction: tensor([[-1.0349, -1.9569, -2.9438, -0.6092]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53070\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5073/10000 (50.7%)\n",
      "    Average reward: -0.553\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4927/10000 (49.3%)\n",
      "    Average reward: +0.553\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7317 (27.6%)\n",
      "    Action 1: 13516 (51.0%)\n",
      "    Action 2: 2485 (9.4%)\n",
      "    Action 3: 3203 (12.1%)\n",
      "  Player 1:\n",
      "    Action 0: 9321 (35.1%)\n",
      "    Action 1: 11109 (41.8%)\n",
      "    Action 2: 2411 (9.1%)\n",
      "    Action 3: 3708 (14.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5533.5, 5533.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.008 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.032\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5534\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8608, 0.0029, 0.1363]])\n",
      "Player 0 Prediction: tensor([[0.0291, 0.9410, 0.0299, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49189\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5150/10000 (51.5%)\n",
      "    Average reward: -0.039\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4850/10000 (48.5%)\n",
      "    Average reward: +0.039\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2760 (11.8%)\n",
      "    Action 1: 15094 (64.8%)\n",
      "    Action 2: 2130 (9.1%)\n",
      "    Action 3: 3315 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20158 (77.9%)\n",
      "    Action 1: 5732 (22.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-391.5, 391.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.770 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.763 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.767\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0391\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.2197, -0.8199, -0.5176,  0.2093]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2293, 0.0385, 0.7322]])\n",
      "Player 0 Prediction: tensor([[-2.0485, -2.4078, -0.8209, -0.6004]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1033, 0.0553, 0.8413]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51880\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6014/10000 (60.1%)\n",
      "    Average reward: +0.661\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3986/10000 (39.9%)\n",
      "    Average reward: -0.661\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9046 (34.4%)\n",
      "    Action 1: 11346 (43.2%)\n",
      "    Action 2: 2739 (10.4%)\n",
      "    Action 3: 3132 (11.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7149 (27.9%)\n",
      "    Action 1: 13222 (51.6%)\n",
      "    Action 2: 2140 (8.4%)\n",
      "    Action 3: 3106 (12.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6614.0, -6614.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.030\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6614\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.9249, 0.0709, 0.0042, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.4805, 0.5151, 0.0044, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0318, 0.0819, 0.8864, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49755\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6153/10000 (61.5%)\n",
      "    Average reward: -0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3847/10000 (38.5%)\n",
      "    Average reward: +0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19468 (73.9%)\n",
      "    Action 1: 6860 (26.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3879 (16.6%)\n",
      "    Action 1: 14425 (61.6%)\n",
      "    Action 2: 1656 (7.1%)\n",
      "    Action 3: 3467 (14.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-757.5, 757.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.828 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.860 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.844\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0757\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19003/50000 [17:05<21:09, 24.41it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 122289/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 122741/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19998/50000 [17:47<20:43, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 128750/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 128998/2000000\n",
      "P1 SL Buffer Size:  128750\n",
      "P1 SL buffer distribution [42116. 62548. 11811. 12275.]\n",
      "P1 actions distribution [0.32711456 0.48580971 0.09173592 0.09533981]\n",
      "P2 SL Buffer Size:  128998\n",
      "P2 SL buffer distribution [43195. 59842. 11935. 14026.]\n",
      "P2 actions distribution [0.33485015 0.46389867 0.09252081 0.10873037]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8274, 0.1629, 0.0097, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.1278, -0.0700, -1.2387,  0.0555]])\n",
      "Player 0 Prediction: tensor([[0.9881, 0.0000, 0.0119, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-2.4140, -3.3377, -2.8360, -2.8283]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2664, 0.0522, 0.6814]])\n",
      "Player 1 Prediction: tensor([[-4.5218, -6.4190, -2.8915, -3.2278]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19998/50000 [18:01<20:43, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56195\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5328/10000 (53.3%)\n",
      "    Average reward: -0.316\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4672/10000 (46.7%)\n",
      "    Average reward: +0.316\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9734 (34.8%)\n",
      "    Action 1: 12236 (43.7%)\n",
      "    Action 2: 2108 (7.5%)\n",
      "    Action 3: 3928 (14.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7708 (27.3%)\n",
      "    Action 1: 12229 (43.4%)\n",
      "    Action 2: 2134 (7.6%)\n",
      "    Action 3: 6118 (21.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3160.0, 3160.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.043\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3160\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2415, 0.0578, 0.7007]])\n",
      "Player 0 Prediction: tensor([[0.1486, 0.6770, 0.1744, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49353\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5229/10000 (52.3%)\n",
      "    Average reward: +0.073\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4771/10000 (47.7%)\n",
      "    Average reward: -0.073\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3006 (12.8%)\n",
      "    Action 1: 14880 (63.4%)\n",
      "    Action 2: 2215 (9.4%)\n",
      "    Action 3: 3362 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 19868 (76.7%)\n",
      "    Action 1: 6022 (23.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [728.0, -728.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.796 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.783 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.789\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.0728\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.8045e-01, 7.1945e-01, 9.7656e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.8252,  1.0566, -1.1401,  0.6302]])\n",
      "Player 1 Prediction: tensor([[1.0160e-01, 8.9835e-01, 4.8451e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.2401,  0.3049, -1.7478,  0.4811]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.3109e-01, 1.9558e-04, 6.8716e-02]])\n",
      "Player 0 Prediction: tensor([[-4.0895, -4.8671, -3.0638, -4.6026]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51496\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5845/10000 (58.5%)\n",
      "    Average reward: +0.619\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4155/10000 (41.5%)\n",
      "    Average reward: -0.619\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9230 (35.6%)\n",
      "    Action 1: 10503 (40.5%)\n",
      "    Action 2: 2781 (10.7%)\n",
      "    Action 3: 3436 (13.2%)\n",
      "  Player 1:\n",
      "    Action 0: 6935 (27.1%)\n",
      "    Action 1: 13200 (51.7%)\n",
      "    Action 2: 2102 (8.2%)\n",
      "    Action 3: 3309 (13.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6192.5, -6192.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.003 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.031\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6192\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3897, 0.6080, 0.0023, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9026, 0.0146, 0.0828]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50066\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6182/10000 (61.8%)\n",
      "    Average reward: -0.066\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3818/10000 (38.2%)\n",
      "    Average reward: +0.066\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18991 (72.0%)\n",
      "    Action 1: 7402 (28.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4245 (17.9%)\n",
      "    Action 1: 14058 (59.4%)\n",
      "    Action 2: 1724 (7.3%)\n",
      "    Action 3: 3646 (15.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-655.0, 655.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.856 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.891 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.874\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0655\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21004/50000 [19:00<21:31, 22.46it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 135285/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 135658/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22000/50000 [19:46<20:35, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 141703/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 142118/2000000\n",
      "P1 SL Buffer Size:  141703\n",
      "P1 SL buffer distribution [47040. 67481. 13071. 14111.]\n",
      "P1 actions distribution [0.33196192 0.47621434 0.09224222 0.09958152]\n",
      "P2 SL Buffer Size:  142118\n",
      "P2 SL buffer distribution [47852. 64902. 13178. 16186.]\n",
      "P2 actions distribution [0.33670612 0.45667685 0.09272576 0.11389127]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.0228, -0.6662, -0.6670,  0.1856]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8741, 0.0020, 0.1240]])\n",
      "Player 1 Prediction: tensor([[-1.1808, -1.8932, -0.9603, -0.2088]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8035, 0.0019, 0.1946]])\n",
      "Player 1 Prediction: tensor([[-2.1459, -2.0982, -1.2196, -0.8950]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22000/50000 [20:01<20:35, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53078\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5256/10000 (52.6%)\n",
      "    Average reward: -0.332\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4744/10000 (47.4%)\n",
      "    Average reward: +0.332\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7427 (28.2%)\n",
      "    Action 1: 12637 (47.9%)\n",
      "    Action 2: 2044 (7.8%)\n",
      "    Action 3: 4265 (16.2%)\n",
      "  Player 1:\n",
      "    Action 0: 10009 (37.5%)\n",
      "    Action 1: 9800 (36.7%)\n",
      "    Action 2: 2586 (9.7%)\n",
      "    Action 3: 4310 (16.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3317.0, 3317.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3317\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2141, 0.0533, 0.7326]])\n",
      "Player 0 Prediction: tensor([[0.1782, 0.4615, 0.3603, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49762\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5263/10000 (52.6%)\n",
      "    Average reward: +0.160\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4737/10000 (47.4%)\n",
      "    Average reward: -0.160\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3220 (13.6%)\n",
      "    Action 1: 14661 (62.0%)\n",
      "    Action 2: 2248 (9.5%)\n",
      "    Action 3: 3526 (14.9%)\n",
      "  Player 1:\n",
      "    Action 0: 19679 (75.4%)\n",
      "    Action 1: 6428 (24.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1603.0, -1603.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.819 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.805 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.812\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1603\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.2082e-01, 6.7911e-01, 7.1207e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.8430,  0.2807, -1.1540,  0.7640]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.5126e-01, 4.1032e-04, 4.8333e-02]])\n",
      "Player 0 Prediction: tensor([[-4.0232, -4.3298, -2.0205, -2.7981]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51228\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5785/10000 (57.9%)\n",
      "    Average reward: +0.582\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4215/10000 (42.1%)\n",
      "    Average reward: -0.582\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11672 (44.9%)\n",
      "    Action 1: 8004 (30.8%)\n",
      "    Action 2: 2626 (10.1%)\n",
      "    Action 3: 3675 (14.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6315 (25.0%)\n",
      "    Action 1: 11935 (47.3%)\n",
      "    Action 2: 1629 (6.5%)\n",
      "    Action 3: 5372 (21.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5824.5, -5824.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.026\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5825\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.2082e-01, 6.7911e-01, 7.1207e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.5126e-01, 4.1032e-04, 4.8333e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50405\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6124/10000 (61.2%)\n",
      "    Average reward: -0.128\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3876/10000 (38.8%)\n",
      "    Average reward: +0.128\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18839 (70.9%)\n",
      "    Action 1: 7749 (29.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4441 (18.6%)\n",
      "    Action 1: 13879 (58.3%)\n",
      "    Action 2: 1704 (7.2%)\n",
      "    Action 3: 3793 (15.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1283.0, 1283.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.871 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.906 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.888\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1283\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23003/50000 [20:58<18:41, 24.06it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 148274/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 148466/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23998/50000 [21:42<19:30, 22.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 154619/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 154906/2000000\n",
      "P1 SL Buffer Size:  154619\n",
      "P1 SL buffer distribution [51820. 72265. 14282. 16252.]\n",
      "P1 actions distribution [0.33514639 0.46737464 0.09236898 0.10510998]\n",
      "P2 SL Buffer Size:  154906\n",
      "P2 SL buffer distribution [52577. 69519. 14514. 18296.]\n",
      "P2 actions distribution [0.33941229 0.44878184 0.09369553 0.11811034]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23998/50000 [21:52<19:30, 22.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8712, 0.1228, 0.0060, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.0368,  0.4037, -1.2790,  0.5799]])\n",
      "Player 0 Prediction: tensor([[0.9857, 0.0000, 0.0143, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-2.2191, -1.7212, -2.8877, -2.4732]])\n",
      "Player 0 Prediction: tensor([[0.2374, 0.4485, 0.3140, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51831\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5460/10000 (54.6%)\n",
      "    Average reward: -0.380\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4540/10000 (45.4%)\n",
      "    Average reward: +0.380\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7437 (28.9%)\n",
      "    Action 1: 12325 (47.9%)\n",
      "    Action 2: 2032 (7.9%)\n",
      "    Action 3: 3962 (15.4%)\n",
      "  Player 1:\n",
      "    Action 0: 8791 (33.7%)\n",
      "    Action 1: 10155 (38.9%)\n",
      "    Action 2: 2548 (9.8%)\n",
      "    Action 3: 4581 (17.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3799.0, 3799.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3799\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.2218e-01, 1.5544e-04, 7.7668e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9098, 0.0011, 0.0891]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50101\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5339/10000 (53.4%)\n",
      "    Average reward: +0.266\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4661/10000 (46.6%)\n",
      "    Average reward: -0.266\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3155 (13.2%)\n",
      "    Action 1: 14633 (61.4%)\n",
      "    Action 2: 2342 (9.8%)\n",
      "    Action 3: 3704 (15.5%)\n",
      "  Player 1:\n",
      "    Action 0: 19676 (74.9%)\n",
      "    Action 1: 6591 (25.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2661.5, -2661.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.818 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.813 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.816\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2661\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.9561, 0.0425, 0.0014, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.7458,  0.9939, -1.1547,  0.8337]])\n",
      "Player 1 Prediction: tensor([[0.5987, 0.3996, 0.0017, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-1.5836, -1.2564, -1.9285, -1.7311]])\n",
      "Player 1 Prediction: tensor([[0.0259, 0.7762, 0.1980, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-5.0611, -4.4095, -3.8895, -2.4029]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51553\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5786/10000 (57.9%)\n",
      "    Average reward: +0.545\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4214/10000 (42.1%)\n",
      "    Average reward: -0.545\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10232 (38.9%)\n",
      "    Action 1: 9820 (37.4%)\n",
      "    Action 2: 2658 (10.1%)\n",
      "    Action 3: 3570 (13.6%)\n",
      "  Player 1:\n",
      "    Action 0: 6888 (27.3%)\n",
      "    Action 1: 12128 (48.0%)\n",
      "    Action 2: 1915 (7.6%)\n",
      "    Action 3: 4342 (17.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5448.5, -5448.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.040\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5448\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.5229e-01, 6.4766e-01, 4.7424e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[8.2010e-02, 9.1797e-01, 2.3012e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.3927e-01, 1.1222e-04, 6.0623e-02]])\n",
      "Player 1 Prediction: tensor([[0.1636, 0.8109, 0.0255, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50770\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6045/10000 (60.5%)\n",
      "    Average reward: -0.168\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3955/10000 (39.6%)\n",
      "    Average reward: +0.168\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18619 (69.7%)\n",
      "    Action 1: 8090 (30.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4787 (19.9%)\n",
      "    Action 1: 13630 (56.6%)\n",
      "    Action 2: 1789 (7.4%)\n",
      "    Action 3: 3855 (16.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1677.5, 1677.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.928 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.906\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1678\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25003/50000 [22:58<17:48, 23.40it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 160975/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 161552/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25999/50000 [23:43<17:23, 23.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 167280/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 168043/2000000\n",
      "P1 SL Buffer Size:  167280\n",
      "P1 SL buffer distribution [56103. 77000. 15443. 18734.]\n",
      "P1 actions distribution [0.33538379 0.46030607 0.09231827 0.11199187]\n",
      "P2 SL Buffer Size:  168043\n",
      "P2 SL buffer distribution [57137. 74479. 15882. 20545.]\n",
      "P2 actions distribution [0.34001416 0.44321394 0.09451152 0.12226037]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8815, 0.1135, 0.0050, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.2553,  1.8741, -1.4469,  1.2331]])\n",
      "Player 0 Prediction: tensor([[0.7192, 0.2763, 0.0045, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.7551,  2.6467, -1.6116,  3.1129]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2881, 0.0534, 0.6585]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52156\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5596/10000 (56.0%)\n",
      "    Average reward: -0.253\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4404/10000 (44.0%)\n",
      "    Average reward: +0.253\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7395 (28.7%)\n",
      "    Action 1: 11993 (46.5%)\n",
      "    Action 2: 1807 (7.0%)\n",
      "    Action 3: 4580 (17.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9748 (37.0%)\n",
      "    Action 1: 9146 (34.7%)\n",
      "    Action 2: 2599 (9.9%)\n",
      "    Action 3: 4888 (18.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2533.5, 2533.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.045\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2534\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.1991e-01, 1.1810e-04, 7.9974e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.8842e-01, 3.0402e-04, 2.1128e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25999/50000 [24:02<17:23, 23.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50217\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5296/10000 (53.0%)\n",
      "    Average reward: +0.335\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4704/10000 (47.0%)\n",
      "    Average reward: -0.335\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3407 (14.2%)\n",
      "    Action 1: 14394 (59.9%)\n",
      "    Action 2: 2502 (10.4%)\n",
      "    Action 3: 3708 (15.4%)\n",
      "  Player 1:\n",
      "    Action 0: 19351 (73.8%)\n",
      "    Action 1: 6855 (26.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3352.0, -3352.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.842 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.829 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.836\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3352\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.8646,  0.7793, -0.6004,  1.1442]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1628, 0.0312, 0.8060]])\n",
      "Player 0 Prediction: tensor([[ 0.3593,  0.4389, -1.1551,  0.4331]])\n",
      "Player 1 Prediction: tensor([[0.9933, 0.0000, 0.0067, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.3692,  0.3541, -2.5874,  0.5475]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3220, 0.0203, 0.6577]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50571\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6000/10000 (60.0%)\n",
      "    Average reward: +0.492\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4000/10000 (40.0%)\n",
      "    Average reward: -0.492\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9642 (37.4%)\n",
      "    Action 1: 8056 (31.2%)\n",
      "    Action 2: 2063 (8.0%)\n",
      "    Action 3: 6021 (23.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7159 (28.9%)\n",
      "    Action 1: 10030 (40.5%)\n",
      "    Action 2: 1544 (6.2%)\n",
      "    Action 3: 6056 (24.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4924.0, -4924.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4924\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5099, 0.4887, 0.0014, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6976, 0.0404, 0.2620]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50817\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6065/10000 (60.7%)\n",
      "    Average reward: -0.218\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3935/10000 (39.4%)\n",
      "    Average reward: +0.218\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18402 (68.9%)\n",
      "    Action 1: 8290 (31.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4774 (19.8%)\n",
      "    Action 1: 13405 (55.6%)\n",
      "    Action 2: 1907 (7.9%)\n",
      "    Action 3: 4039 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2175.5, 2175.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.894 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.934 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.914\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2175\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27003/50000 [24:56<17:07, 22.38it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 173633/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 174397/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28000/50000 [25:43<17:09, 21.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 179943/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 180747/2000000\n",
      "P1 SL Buffer Size:  179943\n",
      "P1 SL buffer distribution [60295. 81684. 16643. 21321.]\n",
      "P1 actions distribution [0.33507833 0.45394375 0.0924904  0.11848752]\n",
      "P2 SL Buffer Size:  180747\n",
      "P2 SL buffer distribution [61678. 79043. 17101. 22925.]\n",
      "P2 actions distribution [0.34123941 0.43731293 0.09461291 0.12683475]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[-0.0124, -1.0357, -0.7435,  0.1161]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8211, 0.0017, 0.1772]])\n",
      "Player 1 Prediction: tensor([[-0.0646, -0.8831, -1.1111, -0.1230]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7379, 0.0034, 0.2587]])\n",
      "Player 1 Prediction: tensor([[-1.7281, -1.6744, -2.0075, -1.6134]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48866\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5899/10000 (59.0%)\n",
      "    Average reward: -0.323\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4101/10000 (41.0%)\n",
      "    Average reward: +0.323\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5474 (22.9%)\n",
      "    Action 1: 11794 (49.3%)\n",
      "    Action 2: 1640 (6.9%)\n",
      "    Action 3: 5001 (20.9%)\n",
      "  Player 1:\n",
      "    Action 0: 11475 (46.0%)\n",
      "    Action 1: 6222 (24.9%)\n",
      "    Action 2: 2646 (10.6%)\n",
      "    Action 3: 4614 (18.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3233.0, 3233.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.990 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.015 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3233\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8924, 0.1038, 0.0038, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.7290, 0.2674, 0.0037, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0135, 0.8123, 0.1742, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28000/50000 [26:03<17:09, 21.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50200\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5261/10000 (52.6%)\n",
      "    Average reward: +0.337\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4739/10000 (47.4%)\n",
      "    Average reward: -0.337\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3471 (14.5%)\n",
      "    Action 1: 14037 (58.5%)\n",
      "    Action 2: 2552 (10.6%)\n",
      "    Action 3: 3936 (16.4%)\n",
      "  Player 1:\n",
      "    Action 0: 19071 (72.8%)\n",
      "    Action 1: 7133 (27.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3371.0, -3371.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.856 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.845 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.850\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3371\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.8343,  2.3612, -0.6064,  1.5424]])\n",
      "Player 1 Prediction: tensor([[0.6543, 0.3443, 0.0014, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.1887,  2.3995, -2.1768,  3.5379]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6478, 0.0513, 0.3009]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51724\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5835/10000 (58.4%)\n",
      "    Average reward: +0.598\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4165/10000 (41.6%)\n",
      "    Average reward: -0.598\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8547 (32.8%)\n",
      "    Action 1: 10626 (40.8%)\n",
      "    Action 2: 2764 (10.6%)\n",
      "    Action 3: 4125 (15.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7268 (28.3%)\n",
      "    Action 1: 12296 (47.9%)\n",
      "    Action 2: 2285 (8.9%)\n",
      "    Action 3: 3813 (14.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5976.5, -5976.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.040\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5977\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.8077e-01, 6.1921e-01, 2.2299e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.9490e-01, 2.6363e-04, 1.0483e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50967\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6041/10000 (60.4%)\n",
      "    Average reward: -0.284\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3959/10000 (39.6%)\n",
      "    Average reward: +0.284\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18112 (68.0%)\n",
      "    Action 1: 8513 (32.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5053 (20.8%)\n",
      "    Action 1: 13145 (54.0%)\n",
      "    Action 2: 2072 (8.5%)\n",
      "    Action 3: 4072 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2844.5, 2844.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.904 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.927\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2844\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29003/50000 [26:58<15:04, 23.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 186123/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 187011/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29998/50000 [27:44<14:45, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 192292/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 193313/2000000\n",
      "P1 SL Buffer Size:  192292\n",
      "P1 SL buffer distribution [64668. 86016. 17893. 23715.]\n",
      "P1 actions distribution [0.33630104 0.4473197  0.09305119 0.12332806]\n",
      "P2 SL Buffer Size:  193313\n",
      "P2 SL buffer distribution [66540. 83123. 18311. 25339.]\n",
      "P2 actions distribution [0.34420862 0.42999177 0.09472203 0.13107758]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[-0.0067, -0.9224, -0.8240,  0.1210]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.1790e-01, 7.0311e-05, 8.2027e-02]])\n",
      "Player 1 Prediction: tensor([[-0.1721, -0.8329, -1.2005, -0.1157]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.3572e-01, 1.4757e-04, 2.6413e-01]])\n",
      "Player 1 Prediction: tensor([[-4.1660, -5.3212, -1.9571, -2.8022]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51387\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5787/10000 (57.9%)\n",
      "    Average reward: -0.189\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4213/10000 (42.1%)\n",
      "    Average reward: +0.189\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7260 (28.6%)\n",
      "    Action 1: 11245 (44.4%)\n",
      "    Action 2: 1743 (6.9%)\n",
      "    Action 3: 5096 (20.1%)\n",
      "  Player 1:\n",
      "    Action 0: 9667 (37.1%)\n",
      "    Action 1: 8607 (33.0%)\n",
      "    Action 2: 2315 (8.9%)\n",
      "    Action 3: 5454 (20.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1892.0, 1892.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.048\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1892\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.1790e-01, 7.0311e-05, 8.2027e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.6525e-01, 5.9834e-04, 1.3415e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29998/50000 [28:03<14:45, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50632\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5226/10000 (52.3%)\n",
      "    Average reward: +0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4774/10000 (47.7%)\n",
      "    Average reward: -0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3739 (15.4%)\n",
      "    Action 1: 13838 (56.9%)\n",
      "    Action 2: 2693 (11.1%)\n",
      "    Action 3: 4036 (16.6%)\n",
      "  Player 1:\n",
      "    Action 0: 18824 (71.5%)\n",
      "    Action 1: 7502 (28.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3587.0, -3587.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.878 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.862 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.870\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3587\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.9905e-01, 6.0093e-01, 1.5260e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 2.0376,  1.9211, -0.8312,  1.8818]])\n",
      "Player 1 Prediction: tensor([[6.3966e-02, 9.3602e-01, 1.2977e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 2.0708,  1.3662, -1.7716,  2.3301]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.5188e-01, 8.1918e-05, 4.8034e-02]])\n",
      "Player 0 Prediction: tensor([[ 1.0056,  1.1570, -2.8787,  1.9686]])\n",
      "Player 1 Prediction: tensor([[0.9847, 0.0000, 0.0153, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49431\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5821/10000 (58.2%)\n",
      "    Average reward: +0.514\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4179/10000 (41.8%)\n",
      "    Average reward: -0.514\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9978 (39.9%)\n",
      "    Action 1: 5744 (23.0%)\n",
      "    Action 2: 2290 (9.2%)\n",
      "    Action 3: 6977 (27.9%)\n",
      "  Player 1:\n",
      "    Action 0: 6254 (25.6%)\n",
      "    Action 1: 10060 (41.2%)\n",
      "    Action 2: 1320 (5.4%)\n",
      "    Action 3: 6808 (27.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5142.0, -5142.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.023\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5142\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 7.8116e-01, 3.1004e-04, 2.1853e-01]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.3575e-01, 2.6750e-04, 1.6398e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51295\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6148/10000 (61.5%)\n",
      "    Average reward: -0.306\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3852/10000 (38.5%)\n",
      "    Average reward: +0.306\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17789 (66.8%)\n",
      "    Action 1: 8847 (33.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5297 (21.5%)\n",
      "    Action 1: 12854 (52.1%)\n",
      "    Action 2: 2265 (9.2%)\n",
      "    Action 3: 4243 (17.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3056.5, 3056.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.917 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.967 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.942\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3056\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31003/50000 [28:56<12:59, 24.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 198569/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 199897/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [29:40<13:03, 22.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 205033/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 206164/2000000\n",
      "P1 SL Buffer Size:  205033\n",
      "P1 SL buffer distribution [69139. 90320. 19106. 26468.]\n",
      "P1 actions distribution [0.33720913 0.44051445 0.093185   0.12909141]\n",
      "P2 SL Buffer Size:  206164\n",
      "P2 SL buffer distribution [71104. 87595. 19500. 27965.]\n",
      "P2 actions distribution [0.34489048 0.42488019 0.09458489 0.13564444]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[1.8132e-01, 8.1867e-01, 1.0096e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.0033,  0.3038, -1.1784,  0.4330]])\n",
      "Player 0 Prediction: tensor([[9.9995e-01, 0.0000e+00, 4.6203e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 5.0903,  5.6133, -3.0150,  4.4055]])\n",
      "Player 0 Prediction: tensor([[0.5481, 0.4493, 0.0025, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [29:54<13:03, 22.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51576\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5667/10000 (56.7%)\n",
      "    Average reward: -0.329\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4333/10000 (43.3%)\n",
      "    Average reward: +0.329\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7976 (30.9%)\n",
      "    Action 1: 11365 (44.1%)\n",
      "    Action 2: 2286 (8.9%)\n",
      "    Action 3: 4159 (16.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8609 (33.4%)\n",
      "    Action 1: 10571 (41.0%)\n",
      "    Action 2: 2249 (8.7%)\n",
      "    Action 3: 4361 (16.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3290.5, 3290.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3291\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9033, 0.0942, 0.0026, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.7499, 0.2474, 0.0027, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0031, 0.0568, 0.9401, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50967\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5211/10000 (52.1%)\n",
      "    Average reward: +0.365\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4789/10000 (47.9%)\n",
      "    Average reward: -0.365\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3731 (15.2%)\n",
      "    Action 1: 13817 (56.4%)\n",
      "    Action 2: 2665 (10.9%)\n",
      "    Action 3: 4270 (17.4%)\n",
      "  Player 1:\n",
      "    Action 0: 18802 (71.0%)\n",
      "    Action 1: 7682 (29.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3651.0, -3651.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.879 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.869 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.874\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3651\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.6659e-01, 3.2968e-02, 4.4383e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 2.0483,  2.0944, -0.8664,  1.7375]])\n",
      "Player 1 Prediction: tensor([[0.6959, 0.3030, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.2800,  1.3993, -1.8107,  2.0568]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3418, 0.0320, 0.6262]])\n",
      "Player 0 Prediction: tensor([[-1.0079, -2.9796, -3.3384,  0.7182]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51227\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5627/10000 (56.3%)\n",
      "    Average reward: +0.483\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4373/10000 (43.7%)\n",
      "    Average reward: -0.483\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8590 (33.2%)\n",
      "    Action 1: 9763 (37.8%)\n",
      "    Action 2: 2835 (11.0%)\n",
      "    Action 3: 4667 (18.1%)\n",
      "  Player 1:\n",
      "    Action 0: 7193 (28.4%)\n",
      "    Action 1: 12144 (47.9%)\n",
      "    Action 2: 2023 (8.0%)\n",
      "    Action 3: 4012 (15.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4827.5, -4827.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4828\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6125, 0.3867, 0.0008, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.2035, 0.7947, 0.0017, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4538, 0.0014, 0.5448]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51489\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6049/10000 (60.5%)\n",
      "    Average reward: -0.330\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3951/10000 (39.5%)\n",
      "    Average reward: +0.330\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17801 (66.5%)\n",
      "    Action 1: 8961 (33.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5331 (21.6%)\n",
      "    Action 1: 12774 (51.7%)\n",
      "    Action 2: 2243 (9.1%)\n",
      "    Action 3: 4379 (17.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3304.5, 3304.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.969 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.945\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3305\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33004/50000 [30:54<13:39, 20.74it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 211256/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 212495/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33998/50000 [31:38<11:34, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 217549/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 218782/2000000\n",
      "P1 SL Buffer Size:  217549\n",
      "P1 SL buffer distribution [73293. 94933. 20269. 29054.]\n",
      "P1 actions distribution [0.33690341 0.43637525 0.09316981 0.13355152]\n",
      "P2 SL Buffer Size:  218782\n",
      "P2 SL buffer distribution [75648. 92110. 20546. 30478.]\n",
      "P2 actions distribution [0.34576885 0.4210127  0.09391083 0.13930762]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9100, 0.0879, 0.0022, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.2277,  0.7392, -0.9901,  0.8660]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6906, 0.0097, 0.2997]])\n",
      "Player 1 Prediction: tensor([[-3.1477, -3.1967, -0.9992, -1.6388]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33998/50000 [31:54<11:34, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50821\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5656/10000 (56.6%)\n",
      "    Average reward: -0.280\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4344/10000 (43.4%)\n",
      "    Average reward: +0.280\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7575 (29.9%)\n",
      "    Action 1: 11068 (43.7%)\n",
      "    Action 2: 2066 (8.1%)\n",
      "    Action 3: 4641 (18.3%)\n",
      "  Player 1:\n",
      "    Action 0: 8138 (32.0%)\n",
      "    Action 1: 9691 (38.0%)\n",
      "    Action 2: 2086 (8.2%)\n",
      "    Action 3: 5556 (21.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2795.5, 2795.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2796\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.7984e-01, 8.1980e-01, 3.6374e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6323, 0.0016, 0.3661]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51042\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5199/10000 (52.0%)\n",
      "    Average reward: +0.446\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4801/10000 (48.0%)\n",
      "    Average reward: -0.446\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3813 (15.5%)\n",
      "    Action 1: 13617 (55.3%)\n",
      "    Action 2: 2794 (11.3%)\n",
      "    Action 3: 4404 (17.9%)\n",
      "  Player 1:\n",
      "    Action 0: 18517 (70.1%)\n",
      "    Action 1: 7897 (29.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4455.0, -4455.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.889 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.880 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.885\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4455\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.5939,  0.6547, -0.7135,  0.8091]])\n",
      "Player 1 Prediction: tensor([[0.6919, 0.3070, 0.0010, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.1615,  1.5620, -2.0871,  0.8707]])\n",
      "Player 1 Prediction: tensor([[0.0082, 0.0363, 0.9555, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50465\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5880/10000 (58.8%)\n",
      "    Average reward: +0.496\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4120/10000 (41.2%)\n",
      "    Average reward: -0.496\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7235 (28.3%)\n",
      "    Action 1: 9649 (37.8%)\n",
      "    Action 2: 2373 (9.3%)\n",
      "    Action 3: 6268 (24.6%)\n",
      "  Player 1:\n",
      "    Action 0: 7438 (29.8%)\n",
      "    Action 1: 10925 (43.8%)\n",
      "    Action 2: 1903 (7.6%)\n",
      "    Action 3: 4674 (18.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4957.0, -4957.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4957\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.8330e-01, 6.1670e-01, 6.6167e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.8208e-01, 1.9269e-04, 3.1773e-01]])\n",
      "Player 1 Prediction: tensor([[0.0345, 0.9458, 0.0197, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51719\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6178/10000 (61.8%)\n",
      "    Average reward: -0.304\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3822/10000 (38.2%)\n",
      "    Average reward: +0.304\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17378 (65.1%)\n",
      "    Action 1: 9327 (34.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5508 (22.0%)\n",
      "    Action 1: 12422 (49.7%)\n",
      "    Action 2: 2497 (10.0%)\n",
      "    Action 3: 4587 (18.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3037.5, 3037.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.933 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.982 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.958\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3038\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35003/50000 [32:52<11:20, 22.03it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 223744/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 225031/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35999/50000 [33:37<10:03, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 230114/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 231205/2000000\n",
      "P1 SL Buffer Size:  230114\n",
      "P1 SL buffer distribution [77631. 99076. 21443. 31964.]\n",
      "P1 actions distribution [0.33735887 0.43055181 0.09318425 0.13890506]\n",
      "P2 SL Buffer Size:  231205\n",
      "P2 SL buffer distribution [80197. 96394. 21550. 33064.]\n",
      "P2 actions distribution [0.34686534 0.41692005 0.09320733 0.14300729]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.5106,  0.1816, -0.6669,  0.3396]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1173, 0.0198, 0.8629]])\n",
      "Player 1 Prediction: tensor([[ 0.3033,  0.4891, -0.8745,  0.8136]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0312, 0.0123, 0.9565]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35999/50000 [33:54<10:03, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49424\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5882/10000 (58.8%)\n",
      "    Average reward: -0.282\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4118/10000 (41.2%)\n",
      "    Average reward: +0.282\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5400 (22.3%)\n",
      "    Action 1: 11144 (46.0%)\n",
      "    Action 2: 1725 (7.1%)\n",
      "    Action 3: 5958 (24.6%)\n",
      "  Player 1:\n",
      "    Action 0: 11481 (45.6%)\n",
      "    Action 1: 6313 (25.1%)\n",
      "    Action 2: 2133 (8.5%)\n",
      "    Action 3: 5270 (20.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2817.5, 2817.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.998 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.017 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.008\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2818\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1173, 0.0198, 0.8629]])\n",
      "Player 0 Prediction: tensor([[0.0569, 0.1968, 0.7464, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51180\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5265/10000 (52.6%)\n",
      "    Average reward: +0.500\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4735/10000 (47.3%)\n",
      "    Average reward: -0.500\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3860 (15.7%)\n",
      "    Action 1: 13571 (55.1%)\n",
      "    Action 2: 2759 (11.2%)\n",
      "    Action 3: 4432 (18.0%)\n",
      "  Player 1:\n",
      "    Action 0: 18592 (70.0%)\n",
      "    Action 1: 7966 (30.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4997.0, -4997.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.893 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.881 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.887\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4997\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[6.4071e-01, 3.5871e-01, 5.8720e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.0539, -0.5934, -1.0192,  0.1282]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1258, 0.0034, 0.8708]])\n",
      "Player 0 Prediction: tensor([[-3.1415, -4.0856, -0.9918, -1.7688]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51926\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5865/10000 (58.7%)\n",
      "    Average reward: +0.502\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4135/10000 (41.3%)\n",
      "    Average reward: -0.502\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9440 (36.4%)\n",
      "    Action 1: 8474 (32.6%)\n",
      "    Action 2: 2464 (9.5%)\n",
      "    Action 3: 5587 (21.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7003 (27.0%)\n",
      "    Action 1: 11330 (43.6%)\n",
      "    Action 2: 1855 (7.1%)\n",
      "    Action 3: 5773 (22.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5020.5, -5020.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.045\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5020\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.2806e-01, 1.4134e-04, 1.7180e-01]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 5.8891e-01, 1.6543e-04, 4.1092e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52078\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6026/10000 (60.3%)\n",
      "    Average reward: -0.401\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3974/10000 (39.7%)\n",
      "    Average reward: +0.401\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17233 (64.2%)\n",
      "    Action 1: 9595 (35.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5722 (22.7%)\n",
      "    Action 1: 12324 (48.8%)\n",
      "    Action 2: 2542 (10.1%)\n",
      "    Action 3: 4662 (18.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4009.0, 4009.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.941 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.990 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.966\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4009\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37003/50000 [34:51<09:32, 22.70it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 236527/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 237593/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37998/50000 [35:37<08:48, 22.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 242828/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 243703/2000000\n",
      "P1 SL Buffer Size:  242828\n",
      "P1 SL buffer distribution [ 82291. 102922.  22609.  35006.]\n",
      "P1 actions distribution [0.33888596 0.42384733 0.09310706 0.14415965]\n",
      "P2 SL Buffer Size:  243703\n",
      "P2 SL buffer distribution [ 84875. 100648.  22671.  35509.]\n",
      "P2 actions distribution [0.34827228 0.41299451 0.09302717 0.14570604]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[2.2840e-01, 7.7132e-01, 2.8320e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.3105,  0.8412, -0.9737,  0.6680]])\n",
      "Player 0 Prediction: tensor([[4.2671e-01, 5.7284e-01, 4.4181e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.9826,  0.7490, -1.7669,  0.5912]])\n",
      "Player 0 Prediction: tensor([[0.0794, 0.8229, 0.0977, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-1.9392, -1.8803, -4.0232, -1.1185]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50147\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5717/10000 (57.2%)\n",
      "    Average reward: -0.338\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4283/10000 (42.8%)\n",
      "    Average reward: +0.338\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6900 (27.7%)\n",
      "    Action 1: 11594 (46.6%)\n",
      "    Action 2: 2570 (10.3%)\n",
      "    Action 3: 3830 (15.4%)\n",
      "  Player 1:\n",
      "    Action 0: 8613 (34.1%)\n",
      "    Action 1: 10587 (41.9%)\n",
      "    Action 2: 2747 (10.9%)\n",
      "    Action 3: 3306 (13.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3375.5, 3375.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.041\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3376\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[2.2840e-01, 7.7132e-01, 2.8320e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5805, 0.0410, 0.3785]])\n",
      "Player 0 Prediction: tensor([[0.0019, 0.0917, 0.9064, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37998/50000 [35:56<08:48, 22.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51590\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5184/10000 (51.8%)\n",
      "    Average reward: +0.483\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4816/10000 (48.2%)\n",
      "    Average reward: -0.483\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3878 (15.6%)\n",
      "    Action 1: 13454 (54.1%)\n",
      "    Action 2: 2838 (11.4%)\n",
      "    Action 3: 4693 (18.9%)\n",
      "  Player 1:\n",
      "    Action 0: 18504 (69.2%)\n",
      "    Action 1: 8223 (30.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4833.5, -4833.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.898 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.890 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.894\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4834\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.5477,  0.6262, -0.6637,  0.7675]])\n",
      "Player 1 Prediction: tensor([[0.7483, 0.2509, 0.0008, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.4384,  2.4312, -2.5199,  3.3395]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5171, 0.0478, 0.4351]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49639\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5805/10000 (58.1%)\n",
      "    Average reward: +0.455\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4195/10000 (41.9%)\n",
      "    Average reward: -0.455\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7710 (30.5%)\n",
      "    Action 1: 8781 (34.7%)\n",
      "    Action 2: 2536 (10.0%)\n",
      "    Action 3: 6267 (24.8%)\n",
      "  Player 1:\n",
      "    Action 0: 6926 (28.4%)\n",
      "    Action 1: 10764 (44.2%)\n",
      "    Action 2: 1730 (7.1%)\n",
      "    Action 3: 4925 (20.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4545.5, -4545.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4546\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.7135e-01, 2.8416e-02, 2.2960e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.7483, 0.2509, 0.0008, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0166, 0.9043, 0.0790, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52115\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6141/10000 (61.4%)\n",
      "    Average reward: -0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3859/10000 (38.6%)\n",
      "    Average reward: +0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17090 (63.7%)\n",
      "    Action 1: 9725 (36.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5626 (22.2%)\n",
      "    Action 1: 12040 (47.6%)\n",
      "    Action 2: 2728 (10.8%)\n",
      "    Action 3: 4906 (19.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3727.5, 3727.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.945 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.992 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.969\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3728\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39005/50000 [36:50<07:54, 23.20it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 249226/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 250147/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40000/50000 [37:35<10:20, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 255708/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 256460/2000000\n",
      "P1 SL Buffer Size:  255708\n",
      "P1 SL buffer distribution [ 86934. 106984.  23881.  37909.]\n",
      "P1 actions distribution [0.33997372 0.41838347 0.09339168 0.14825113]\n",
      "P2 SL Buffer Size:  256460\n",
      "P2 SL buffer distribution [ 89486. 105646.  23736.  37592.]\n",
      "P2 actions distribution [0.34892771 0.41193948 0.09255244 0.14658036]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40000/50000 [37:46<10:20, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[2.7623e-01, 7.2350e-01, 2.7276e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.3464, -0.7075, -1.0231,  0.0444]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2148, 0.0251, 0.7601]])\n",
      "Player 1 Prediction: tensor([[ 1.9876,  2.7584, -0.8679,  1.8158]])\n",
      "Player 0 Prediction: tensor([[0.0320, 0.2791, 0.6889, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50672\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6000/10000 (60.0%)\n",
      "    Average reward: -0.199\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4000/10000 (40.0%)\n",
      "    Average reward: +0.199\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5969 (23.9%)\n",
      "    Action 1: 11235 (45.0%)\n",
      "    Action 2: 1983 (7.9%)\n",
      "    Action 3: 5791 (23.2%)\n",
      "  Player 1:\n",
      "    Action 0: 11568 (45.0%)\n",
      "    Action 1: 7332 (28.5%)\n",
      "    Action 2: 2172 (8.5%)\n",
      "    Action 3: 4622 (18.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1988.5, 1988.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.012 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.023\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1988\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[2.7623e-01, 7.2350e-01, 2.7276e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6776, 0.0011, 0.3213]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51735\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5208/10000 (52.1%)\n",
      "    Average reward: +0.516\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4792/10000 (47.9%)\n",
      "    Average reward: -0.516\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4282 (17.1%)\n",
      "    Action 1: 13175 (52.7%)\n",
      "    Action 2: 2879 (11.5%)\n",
      "    Action 3: 4644 (18.6%)\n",
      "  Player 1:\n",
      "    Action 0: 18207 (68.1%)\n",
      "    Action 1: 8548 (31.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5158.5, -5158.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.904 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.913\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.5159\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[6.2420e-01, 3.7543e-01, 3.6640e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.0985, -0.5208, -1.2147,  0.0780]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1234, 0.0023, 0.8743]])\n",
      "Player 0 Prediction: tensor([[-1.2673, -1.7064, -1.0927, -0.5458]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51298\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5854/10000 (58.5%)\n",
      "    Average reward: +0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4146/10000 (41.5%)\n",
      "    Average reward: -0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11007 (42.5%)\n",
      "    Action 1: 7658 (29.5%)\n",
      "    Action 2: 2383 (9.2%)\n",
      "    Action 3: 4878 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 6533 (25.7%)\n",
      "    Action 1: 10608 (41.8%)\n",
      "    Action 2: 1884 (7.4%)\n",
      "    Action 3: 6347 (25.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3727.0, -3727.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.037\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3727\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3147, 0.0100, 0.6753]])\n",
      "Player 1 Prediction: tensor([[0.0489, 0.4783, 0.4728, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51989\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6042/10000 (60.4%)\n",
      "    Average reward: -0.469\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3958/10000 (39.6%)\n",
      "    Average reward: +0.469\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17165 (64.1%)\n",
      "    Action 1: 9627 (35.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5561 (22.1%)\n",
      "    Action 1: 12148 (48.2%)\n",
      "    Action 2: 2636 (10.5%)\n",
      "    Action 3: 4852 (19.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4686.5, 4686.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.942 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.989 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.965\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4687\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}, {'exploitability': 0.285775}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41003/50000 [38:54<06:55, 21.64it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 262137/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 262951/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [39:47<06:33, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 268472/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 269138/2000000\n",
      "P1 SL Buffer Size:  268472\n",
      "P1 SL buffer distribution [ 91552. 110948.  25179.  40793.]\n",
      "P1 actions distribution [0.34101135 0.41325725 0.09378632 0.15194508]\n",
      "P2 SL Buffer Size:  269138\n",
      "P2 SL buffer distribution [ 94022. 110376.  24815.  39925.]\n",
      "P2 actions distribution [0.34934495 0.41010931 0.09220177 0.14834397]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9313, 0.0672, 0.0015, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.9838,  1.6198, -1.5109,  1.0142]])\n",
      "Player 0 Prediction: tensor([[0.8091, 0.1895, 0.0014, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.0829,  0.2305, -1.9180,  0.0481]])\n",
      "Player 0 Prediction: tensor([[0.0110, 0.8567, 0.1323, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-5.1738, -3.5649, -3.9715, -2.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50778\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5616/10000 (56.2%)\n",
      "    Average reward: -0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4384/10000 (43.8%)\n",
      "    Average reward: +0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7645 (30.3%)\n",
      "    Action 1: 10669 (42.3%)\n",
      "    Action 2: 2638 (10.5%)\n",
      "    Action 3: 4260 (16.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8703 (34.0%)\n",
      "    Action 1: 10910 (42.7%)\n",
      "    Action 2: 2287 (8.9%)\n",
      "    Action 3: 3666 (14.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2396.5, 2396.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2397\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[3.0381e-01, 6.9596e-01, 2.2894e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6861, 0.0011, 0.3128]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [40:07<06:33, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51787\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5269/10000 (52.7%)\n",
      "    Average reward: +0.586\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4731/10000 (47.3%)\n",
      "    Average reward: -0.586\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4319 (17.3%)\n",
      "    Action 1: 13130 (52.5%)\n",
      "    Action 2: 2860 (11.4%)\n",
      "    Action 3: 4717 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 18144 (67.8%)\n",
      "    Action 1: 8617 (32.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5860.0, -5860.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.926 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.907 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.916\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.5860\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[6.2626e-01, 3.7343e-01, 3.1154e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.8566,  1.7766, -0.9206,  1.3473]])\n",
      "Player 1 Prediction: tensor([[0.2538, 0.7449, 0.0013, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.0287,  1.2809, -2.2061,  2.3127]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8038, 0.0163, 0.1799]])\n",
      "Player 0 Prediction: tensor([[ 1.2905,  2.4084, -3.0869,  2.7741]])\n",
      "Player 1 Prediction: tensor([[0.0729, 0.0000, 0.9271, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51789\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5810/10000 (58.1%)\n",
      "    Average reward: +0.317\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4190/10000 (41.9%)\n",
      "    Average reward: -0.317\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9157 (35.4%)\n",
      "    Action 1: 8765 (33.9%)\n",
      "    Action 2: 2640 (10.2%)\n",
      "    Action 3: 5305 (20.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7219 (27.8%)\n",
      "    Action 1: 11029 (42.5%)\n",
      "    Action 2: 1888 (7.3%)\n",
      "    Action 3: 5786 (22.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3170.5, -3170.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.049\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3170\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[4.0670e-01, 5.9330e-01, 1.8495e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 3.9219e-01, 9.1591e-05, 6.0771e-01]])\n",
      "Player 1 Prediction: tensor([[0.0327, 0.9522, 0.0151, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51847\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6059/10000 (60.6%)\n",
      "    Average reward: -0.469\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3941/10000 (39.4%)\n",
      "    Average reward: +0.469\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 16855 (63.4%)\n",
      "    Action 1: 9744 (36.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5826 (23.1%)\n",
      "    Action 1: 11839 (46.9%)\n",
      "    Action 2: 2805 (11.1%)\n",
      "    Action 3: 4778 (18.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4689.0, 4689.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.948 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.974\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4689\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}, {'exploitability': 0.285775}, {'exploitability': 0.27835}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43003/50000 [41:06<05:26, 21.40it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 274733/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 275409/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44000/50000 [41:56<04:50, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 281037/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 281513/2000000\n",
      "P1 SL Buffer Size:  281037\n",
      "P1 SL buffer distribution [ 96032. 114802.  26401.  43802.]\n",
      "P1 actions distribution [0.3417059  0.40849426 0.09394137 0.15585848]\n",
      "P2 SL Buffer Size:  281513\n",
      "P2 SL buffer distribution [ 98173. 115453.  25847.  42040.]\n",
      "P2 actions distribution [0.34873345 0.41011605 0.09181459 0.14933591]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44000/50000 [42:07<04:50, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[3.1224e-01, 6.8755e-01, 2.0675e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.8420,  1.6756, -1.4897,  1.0445]])\n",
      "Player 0 Prediction: tensor([[5.2182e-01, 4.7787e-01, 3.0168e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 2.3808,  2.8859, -1.4689,  3.0824]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7046, 0.0007, 0.2947]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50458\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5618/10000 (56.2%)\n",
      "    Average reward: -0.284\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4382/10000 (43.8%)\n",
      "    Average reward: +0.284\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7783 (31.1%)\n",
      "    Action 1: 10944 (43.7%)\n",
      "    Action 2: 2809 (11.2%)\n",
      "    Action 3: 3512 (14.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7480 (29.4%)\n",
      "    Action 1: 11921 (46.9%)\n",
      "    Action 2: 2533 (10.0%)\n",
      "    Action 3: 3476 (13.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2837.0, 2837.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2837\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.5599e-01, 2.1202e-05, 4.3986e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 3.7987e-01, 3.2468e-05, 6.2010e-01]])\n",
      "Player 0 Prediction: tensor([[0.0611, 0.9283, 0.0105, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51781\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5063/10000 (50.6%)\n",
      "    Average reward: +0.471\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4937/10000 (49.4%)\n",
      "    Average reward: -0.471\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4549 (18.1%)\n",
      "    Action 1: 12810 (51.0%)\n",
      "    Action 2: 3050 (12.1%)\n",
      "    Action 3: 4730 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 17797 (66.8%)\n",
      "    Action 1: 8845 (33.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4715.0, -4715.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.942 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.917 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.929\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4715\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[4.0960e-01, 5.9040e-01, 1.5558e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.7749, -0.0434, -1.0975,  0.1595]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 3.4726e-01, 7.9669e-05, 6.5266e-01]])\n",
      "Player 0 Prediction: tensor([[-0.8928, -1.0746, -1.9289, -0.9303]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48966\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5917/10000 (59.2%)\n",
      "    Average reward: +0.353\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4083/10000 (40.8%)\n",
      "    Average reward: -0.353\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9957 (40.0%)\n",
      "    Action 1: 6119 (24.6%)\n",
      "    Action 2: 2069 (8.3%)\n",
      "    Action 3: 6720 (27.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6115 (25.4%)\n",
      "    Action 1: 9286 (38.5%)\n",
      "    Action 2: 1777 (7.4%)\n",
      "    Action 3: 6923 (28.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3526.0, -3526.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.029\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3526\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.8729e-01, 5.7630e-05, 1.1266e-01]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.7422e-01, 6.7695e-05, 3.2571e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52212\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6029/10000 (60.3%)\n",
      "    Average reward: -0.520\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3971/10000 (39.7%)\n",
      "    Average reward: +0.520\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17058 (63.6%)\n",
      "    Action 1: 9758 (36.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5647 (22.2%)\n",
      "    Action 1: 12015 (47.3%)\n",
      "    Action 2: 2812 (11.1%)\n",
      "    Action 3: 4922 (19.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5196.5, 5196.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.946 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.993 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.970\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5196\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}, {'exploitability': 0.285775}, {'exploitability': 0.27835}, {'exploitability': 0.31815000000000004}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45003/50000 [43:15<03:57, 21.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 287345/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 287753/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [44:04<03:16, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 293562/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 294081/2000000\n",
      "P1 SL Buffer Size:  293562\n",
      "P1 SL buffer distribution [100389. 118757.  27646.  46770.]\n",
      "P1 actions distribution [0.34196865 0.40453805 0.09417431 0.15931899]\n",
      "P2 SL Buffer Size:  294081\n",
      "P2 SL buffer distribution [102556. 120031.  26979.  44515.]\n",
      "P2 actions distribution [0.34873385 0.40815626 0.09174003 0.15136986]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[3.4688e-01, 6.5293e-01, 1.8080e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.1552, -0.6875, -1.0257,  0.0465]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 2.4313e-01, 3.0114e-04, 7.5657e-01]])\n",
      "Player 1 Prediction: tensor([[-0.2299, -0.7200, -1.0319, -0.4568]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [44:17<03:16, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49291\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5761/10000 (57.6%)\n",
      "    Average reward: -0.379\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4239/10000 (42.4%)\n",
      "    Average reward: +0.379\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5909 (24.2%)\n",
      "    Action 1: 10757 (44.1%)\n",
      "    Action 2: 2050 (8.4%)\n",
      "    Action 3: 5655 (23.2%)\n",
      "  Player 1:\n",
      "    Action 0: 9502 (38.1%)\n",
      "    Action 1: 7321 (29.4%)\n",
      "    Action 2: 2257 (9.1%)\n",
      "    Action 3: 5840 (23.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3787.5, 3787.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3787\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[3.4688e-01, 6.5293e-01, 1.8080e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[5.4540e-01, 4.5434e-01, 2.6223e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0524, 0.8746, 0.0730, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51643\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5030/10000 (50.3%)\n",
      "    Average reward: +0.496\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4970/10000 (49.7%)\n",
      "    Average reward: -0.496\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4609 (18.3%)\n",
      "    Action 1: 12680 (50.4%)\n",
      "    Action 2: 3137 (12.5%)\n",
      "    Action 3: 4717 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 17625 (66.5%)\n",
      "    Action 1: 8875 (33.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4956.0, -4956.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.947 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.933\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4956\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.8936,  1.9957, -0.7520,  1.5129]])\n",
      "Player 1 Prediction: tensor([[7.9447e-01, 2.0495e-01, 5.7904e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.8936,  1.0589, -2.0360,  2.2709]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2819, 0.0187, 0.6993]])\n",
      "Player 0 Prediction: tensor([[-2.6540, -3.4589, -3.4683,  0.9176]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51281\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5797/10000 (58.0%)\n",
      "    Average reward: +0.266\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4203/10000 (42.0%)\n",
      "    Average reward: -0.266\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8112 (31.5%)\n",
      "    Action 1: 11116 (43.1%)\n",
      "    Action 2: 2677 (10.4%)\n",
      "    Action 3: 3863 (15.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7753 (30.4%)\n",
      "    Action 1: 11180 (43.8%)\n",
      "    Action 2: 2408 (9.4%)\n",
      "    Action 3: 4172 (16.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2661.5, -2661.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.046\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2661\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3782, 0.0080, 0.6139]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8133, 0.0010, 0.1857]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52108\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6102/10000 (61.0%)\n",
      "    Average reward: -0.491\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3898/10000 (39.0%)\n",
      "    Average reward: +0.491\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 16841 (63.2%)\n",
      "    Action 1: 9816 (36.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5741 (22.6%)\n",
      "    Action 1: 11894 (46.7%)\n",
      "    Action 2: 2947 (11.6%)\n",
      "    Action 3: 4869 (19.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4909.5, 4909.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.949 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.997 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.973\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4909\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}, {'exploitability': 0.285775}, {'exploitability': 0.27835}, {'exploitability': 0.31815000000000004}, {'exploitability': 0.32245}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47003/50000 [45:27<03:22, 14.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 299836/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 300339/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47998/50000 [46:18<01:39, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 306157/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 306586/2000000\n",
      "P1 SL Buffer Size:  306157\n",
      "P1 SL buffer distribution [104877. 122627.  28916.  49737.]\n",
      "P1 actions distribution [0.34255954 0.40053633 0.09444827 0.16245586]\n",
      "P2 SL Buffer Size:  306586\n",
      "P2 SL buffer distribution [107117. 124514.  27915.  47040.]\n",
      "P2 actions distribution [0.34938647 0.40613074 0.09105112 0.15343166]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.6699,  1.3992, -1.1013,  0.7372]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6679, 0.0009, 0.3312]])\n",
      "Player 1 Prediction: tensor([[ 0.8588,  1.5757, -1.4352,  1.9783]])\n",
      "Player 0 Prediction: tensor([[9.9936e-01, 0.0000e+00, 6.4345e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.5229,  4.9582, -3.1189,  3.9750]])\n",
      "Player 0 Prediction: tensor([[0.7896, 0.2004, 0.0100, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49401\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5706/10000 (57.1%)\n",
      "    Average reward: -0.251\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4294/10000 (42.9%)\n",
      "    Average reward: +0.251\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5577 (22.8%)\n",
      "    Action 1: 10233 (41.9%)\n",
      "    Action 2: 1991 (8.2%)\n",
      "    Action 3: 6614 (27.1%)\n",
      "  Player 1:\n",
      "    Action 0: 11050 (44.2%)\n",
      "    Action 1: 6449 (25.8%)\n",
      "    Action 2: 1562 (6.3%)\n",
      "    Action 3: 5925 (23.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2513.0, 2513.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.012 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.019\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2513\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9399, 0.0588, 0.0012, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.8250, 0.1739, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0015, 0.0250, 0.9735, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47998/50000 [46:38<01:39, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51726\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5043/10000 (50.4%)\n",
      "    Average reward: +0.501\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4957/10000 (49.6%)\n",
      "    Average reward: -0.501\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4461 (17.8%)\n",
      "    Action 1: 12648 (50.3%)\n",
      "    Action 2: 3126 (12.4%)\n",
      "    Action 3: 4895 (19.5%)\n",
      "  Player 1:\n",
      "    Action 0: 17724 (66.6%)\n",
      "    Action 1: 8872 (33.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5015.0, -5015.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.941 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.919 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.930\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.5015\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.1020,  2.1848, -0.6558,  1.5457]])\n",
      "Player 1 Prediction: tensor([[0.3012, 0.6978, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.3584,  0.8793, -2.4914,  0.7746]])\n",
      "Player 1 Prediction: tensor([[0.0052, 0.0827, 0.9121, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-3.4212, -1.5122, -3.9329,  0.4070]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48612\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5913/10000 (59.1%)\n",
      "    Average reward: +0.302\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4087/10000 (40.9%)\n",
      "    Average reward: -0.302\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8867 (36.2%)\n",
      "    Action 1: 7430 (30.3%)\n",
      "    Action 2: 1949 (8.0%)\n",
      "    Action 3: 6248 (25.5%)\n",
      "  Player 1:\n",
      "    Action 0: 6623 (27.5%)\n",
      "    Action 1: 9539 (39.6%)\n",
      "    Action 2: 1954 (8.1%)\n",
      "    Action 3: 6002 (24.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3017.0, -3017.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.047\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3017\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[6.0736e-01, 3.9243e-01, 2.0854e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.3012, 0.6978, 0.0011, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0052, 0.0827, 0.9121, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52398\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6024/10000 (60.2%)\n",
      "    Average reward: -0.525\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3976/10000 (39.8%)\n",
      "    Average reward: +0.525\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 16897 (63.0%)\n",
      "    Action 1: 9916 (37.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5809 (22.7%)\n",
      "    Action 1: 11945 (46.7%)\n",
      "    Action 2: 2911 (11.4%)\n",
      "    Action 3: 4920 (19.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5245.5, 5245.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.999 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.975\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5245\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}, {'exploitability': 0.285775}, {'exploitability': 0.27835}, {'exploitability': 0.31815000000000004}, {'exploitability': 0.32245}, {'exploitability': 0.2765}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49004/50000 [47:50<00:49, 20.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 312331/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 312977/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [48:40<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[1.9943e-01, 8.0056e-01, 4.1785e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.4926,  0.4304, -1.1620,  0.2424]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 3.0523e-01, 2.4477e-05, 6.9475e-01]])\n",
      "Player 1 Prediction: tensor([[ 0.4147,  0.9067, -1.6984,  0.3878]])\n",
      "Player 0 Prediction: tensor([[0.0768, 0.9119, 0.0114, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-2.3630, -1.8097, -3.9533, -1.2207]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50481\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5198/10000 (52.0%)\n",
      "    Average reward: -0.275\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4802/10000 (48.0%)\n",
      "    Average reward: +0.275\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7585 (29.9%)\n",
      "    Action 1: 10607 (41.8%)\n",
      "    Action 2: 3533 (13.9%)\n",
      "    Action 3: 3625 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 7993 (31.8%)\n",
      "    Action 1: 12044 (47.9%)\n",
      "    Action 2: 2094 (8.3%)\n",
      "    Action 3: 3000 (11.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2748.5, 2748.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.040\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2748\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[3.8372e-01, 6.1612e-01, 1.6024e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6359, 0.0273, 0.3368]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51780\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5063/10000 (50.6%)\n",
      "    Average reward: +0.568\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4937/10000 (49.4%)\n",
      "    Average reward: -0.568\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4747 (18.8%)\n",
      "    Action 1: 12501 (49.5%)\n",
      "    Action 2: 3157 (12.5%)\n",
      "    Action 3: 4867 (19.3%)\n",
      "  Player 1:\n",
      "    Action 0: 17384 (65.6%)\n",
      "    Action 1: 9124 (34.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5683.5, -5683.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.955 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.929 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.942\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.5684\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.6513,  1.8298, -0.7250,  1.3571]])\n",
      "Player 1 Prediction: tensor([[8.1300e-01, 1.8651e-01, 4.8696e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 2.0078,  2.6052, -2.6586,  2.9462]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3402, 0.0200, 0.6398]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51315\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5801/10000 (58.0%)\n",
      "    Average reward: +0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4199/10000 (42.0%)\n",
      "    Average reward: -0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9761 (37.9%)\n",
      "    Action 1: 8650 (33.6%)\n",
      "    Action 2: 2393 (9.3%)\n",
      "    Action 3: 4939 (19.2%)\n",
      "  Player 1:\n",
      "    Action 0: 7076 (27.7%)\n",
      "    Action 1: 10939 (42.8%)\n",
      "    Action 2: 1982 (7.8%)\n",
      "    Action 3: 5575 (21.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2539.0, -2539.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.048\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2539\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.7638e-01, 2.3518e-02, 1.0464e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[8.1300e-01, 1.8651e-01, 4.8696e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0024, 0.0187, 0.9789, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52166\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6000/10000 (60.0%)\n",
      "    Average reward: -0.550\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4000/10000 (40.0%)\n",
      "    Average reward: +0.550\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 16844 (63.0%)\n",
      "    Action 1: 9885 (37.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5910 (23.2%)\n",
      "    Action 1: 11777 (46.3%)\n",
      "    Action 2: 2929 (11.5%)\n",
      "    Action 3: 4821 (19.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5502.0, 5502.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.004 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.977\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5502\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.0271}, {'exploitability': 0.900925}, {'exploitability': 1.019825}, {'exploitability': 0.802775}, {'exploitability': 0.62535}, {'exploitability': 0.79605}, {'exploitability': 0.715625}, {'exploitability': 0.639975}, {'exploitability': 0.607375}, {'exploitability': 0.46762499999999996}, {'exploitability': 0.457075}, {'exploitability': 0.462375}, {'exploitability': 0.372875}, {'exploitability': 0.46047499999999997}, {'exploitability': 0.3517}, {'exploitability': 0.40590000000000004}, {'exploitability': 0.387625}, {'exploitability': 0.3919}, {'exploitability': 0.39605}, {'exploitability': 0.285775}, {'exploitability': 0.27835}, {'exploitability': 0.31815000000000004}, {'exploitability': 0.32245}, {'exploitability': 0.2765}, {'exploitability': 0.264375}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1a87c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1037a0130>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1037a0130>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : True\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1037a0130>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : True\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1037a12d0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1037a12d0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4e1b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-Dueling\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aeaf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/50000 [00:00<17:26, 47.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 61/200000\n",
      "   Player 0 SL buffer: 13/2000000\n",
      "   Player 1 RL buffer: 66/200000\n",
      "   Player 1 SL buffer: 6/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1006/50000 [00:33<30:00, 27.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0019 â†’ 0.0019\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 63985/200000\n",
      "   Player 0 SL buffer: 7262/2000000\n",
      "   Player 1 RL buffer: 64141/200000\n",
      "   Player 1 SL buffer: 7441/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:12<30:47, 25.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0013 â†’ 0.0013\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 127781/200000\n",
      "   Player 0 SL buffer: 14123/2000000\n",
      "   Player 1 RL buffer: 128345/200000\n",
      "   Player 1 SL buffer: 14479/2000000\n",
      "P1 SL Buffer Size:  14123\n",
      "P1 SL buffer distribution [4247. 7418.  719. 1739.]\n",
      "P1 actions distribution [0.30071515 0.52524251 0.05090986 0.12313248]\n",
      "P2 SL Buffer Size:  14479\n",
      "P2 SL buffer distribution [4405. 7647.  853. 1574.]\n",
      "P2 actions distribution [0.30423372 0.52814421 0.05891291 0.10870916]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.3075, -0.0023, -0.4798,  0.3222]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7841, 0.0405, 0.1754]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:24<30:47, 25.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56808\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5087/10000 (50.9%)\n",
      "    Average reward: -1.136\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4913/10000 (49.1%)\n",
      "    Average reward: +1.136\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10126 (35.4%)\n",
      "    Action 1: 15295 (53.4%)\n",
      "    Action 2: 1527 (5.3%)\n",
      "    Action 3: 1676 (5.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7907 (28.1%)\n",
      "    Action 1: 14750 (52.3%)\n",
      "    Action 2: 2273 (8.1%)\n",
      "    Action 3: 3254 (11.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-11356.5, 11356.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.013 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.003 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.008\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.1357\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.5221, 0.4310, 0.0469, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7534, 0.0669, 0.1797]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50005\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5425/10000 (54.2%)\n",
      "    Average reward: -0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4575/10000 (45.8%)\n",
      "    Average reward: +0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3754 (16.0%)\n",
      "    Action 1: 16458 (70.1%)\n",
      "    Action 2: 1059 (4.5%)\n",
      "    Action 3: 2200 (9.4%)\n",
      "  Player 1:\n",
      "    Action 0: 21459 (80.9%)\n",
      "    Action 1: 5075 (19.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1078.0, 1078.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.782 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.704 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.743\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1078\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3830, 0.5487, 0.0683, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.1645, -0.0112, -0.9729,  0.1582]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7519, 0.0810, 0.1671]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55149\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6042/10000 (60.4%)\n",
      "    Average reward: +1.134\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3958/10000 (39.6%)\n",
      "    Average reward: -1.134\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7567 (27.4%)\n",
      "    Action 1: 14187 (51.5%)\n",
      "    Action 2: 2632 (9.5%)\n",
      "    Action 3: 3183 (11.5%)\n",
      "  Player 1:\n",
      "    Action 0: 9100 (33.0%)\n",
      "    Action 1: 15023 (54.5%)\n",
      "    Action 2: 1865 (6.8%)\n",
      "    Action 3: 1592 (5.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [11340.0, -11340.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.005 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.005 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.1340\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7324, 0.0860, 0.1815]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7313, 0.0818, 0.1868]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47758\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6385/10000 (63.8%)\n",
      "    Average reward: +0.116\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3615/10000 (36.1%)\n",
      "    Average reward: -0.116\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21451 (84.5%)\n",
      "    Action 1: 3926 (15.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2754 (12.3%)\n",
      "    Action 1: 16511 (73.8%)\n",
      "    Action 2: 1217 (5.4%)\n",
      "    Action 3: 1899 (8.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1156.0, -1156.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.621 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.696 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.659\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1156\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3005/50000 [02:19<29:41, 26.39it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0011 â†’ 0.0011\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 191680/200000\n",
      "   Player 0 SL buffer: 20605/2000000\n",
      "   Player 1 RL buffer: 192446/200000\n",
      "   Player 1 SL buffer: 21142/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4000/50000 [02:55<33:02, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0009 â†’ 0.0009\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 27171/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 27477/2000000\n",
      "P1 SL Buffer Size:  27171\n",
      "P1 SL buffer distribution [ 7922. 14010.  1890.  3349.]\n",
      "P1 actions distribution [0.29156086 0.51562327 0.06955946 0.12325641]\n",
      "P2 SL Buffer Size:  27477\n",
      "P2 SL buffer distribution [ 8907. 13605.  2204.  2761.]\n",
      "P2 actions distribution [0.32416203 0.49514139 0.08021254 0.10048404]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 3.3656,  3.7764, -0.2920,  2.8262]])\n",
      "Player 0 Prediction: tensor([[0.0275, 0.9647, 0.0078, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 3.4237,  3.9113, -1.9008,  2.9533]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7853, 0.0512, 0.1635]])\n",
      "Player 1 Prediction: tensor([[ 1.2736,  1.9811, -2.7428,  1.4753]])\n",
      "Player 0 Prediction: tensor([[0.9700, 0.0000, 0.0300, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53961\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5295/10000 (52.9%)\n",
      "    Average reward: -1.121\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4705/10000 (47.0%)\n",
      "    Average reward: +1.121\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6546 (24.5%)\n",
      "    Action 1: 16063 (60.1%)\n",
      "    Action 2: 1754 (6.6%)\n",
      "    Action 3: 2351 (8.8%)\n",
      "  Player 1:\n",
      "    Action 0: 10614 (39.0%)\n",
      "    Action 1: 11932 (43.8%)\n",
      "    Action 2: 3014 (11.1%)\n",
      "    Action 3: 1687 (6.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-11207.0, 11207.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.938 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.995\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.1207\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6787, 0.0711, 0.2502]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4000/50000 [03:14<33:02, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49225\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5371/10000 (53.7%)\n",
      "    Average reward: -0.165\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4629/10000 (46.3%)\n",
      "    Average reward: +0.165\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2862 (12.4%)\n",
      "    Action 1: 16784 (72.5%)\n",
      "    Action 2: 1332 (5.8%)\n",
      "    Action 3: 2169 (9.4%)\n",
      "  Player 1:\n",
      "    Action 0: 21758 (83.4%)\n",
      "    Action 1: 4320 (16.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1651.0, 1651.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.709 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.648 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.678\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1651\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.7913,  3.4705, -0.4044,  2.4122]])\n",
      "Player 1 Prediction: tensor([[0.0649, 0.9114, 0.0237, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.3640,  3.6166, -1.9896,  2.2346]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6448, 0.1212, 0.2340]])\n",
      "Player 0 Prediction: tensor([[ 5.2139,  6.7252, -3.0779,  4.5250]])\n",
      "Player 1 Prediction: tensor([[0.9418, 0.0000, 0.0582, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55222\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6107/10000 (61.1%)\n",
      "    Average reward: +0.968\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3893/10000 (38.9%)\n",
      "    Average reward: -0.968\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8251 (29.7%)\n",
      "    Action 1: 13668 (49.1%)\n",
      "    Action 2: 2523 (9.1%)\n",
      "    Action 3: 3384 (12.2%)\n",
      "  Player 1:\n",
      "    Action 0: 8178 (29.9%)\n",
      "    Action 1: 14775 (53.9%)\n",
      "    Action 2: 2350 (8.6%)\n",
      "    Action 3: 2093 (7.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9683.0, -9683.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.012\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9683\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5973, 0.1493, 0.2534]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5541, 0.1616, 0.2844]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47746\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6437/10000 (64.4%)\n",
      "    Average reward: +0.105\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3563/10000 (35.6%)\n",
      "    Average reward: -0.105\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21090 (83.8%)\n",
      "    Action 1: 4066 (16.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2795 (12.4%)\n",
      "    Action 1: 15998 (70.8%)\n",
      "    Action 2: 1823 (8.1%)\n",
      "    Action 3: 1974 (8.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1054.0, -1054.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.638 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.726 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.682\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1054\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5003/50000 [04:03<24:37, 30.46it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 33410/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 34078/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5997/50000 [04:36<24:03, 30.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 39818/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 40906/2000000\n",
      "P1 SL Buffer Size:  39818\n",
      "P1 SL buffer distribution [11506. 20107.  2948.  5257.]\n",
      "P1 actions distribution [0.28896479 0.50497263 0.07403687 0.13202572]\n",
      "P2 SL Buffer Size:  40906\n",
      "P2 SL buffer distribution [13366. 19829.  3399.  4312.]\n",
      "P2 actions distribution [0.32674913 0.48474551 0.08309294 0.10541241]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.3417,  2.8479, -0.6317,  2.1675]])\n",
      "Player 0 Prediction: tensor([[0.0179, 0.9725, 0.0096, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.2373,  2.9142, -2.1355,  2.1613]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4424, 0.0618, 0.4958]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53515\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5024/10000 (50.2%)\n",
      "    Average reward: -1.074\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4976/10000 (49.8%)\n",
      "    Average reward: +1.074\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5491 (20.9%)\n",
      "    Action 1: 15490 (58.9%)\n",
      "    Action 2: 2276 (8.6%)\n",
      "    Action 3: 3064 (11.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10952 (40.3%)\n",
      "    Action 1: 11101 (40.8%)\n",
      "    Action 2: 2524 (9.3%)\n",
      "    Action 3: 2617 (9.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-10743.5, 10743.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.922 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.989\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.0743\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1768, 0.8186, 0.0046, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9707, 0.0154, 0.0139]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5997/50000 [04:55<24:03, 30.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49234\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5320/10000 (53.2%)\n",
      "    Average reward: -0.138\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4680/10000 (46.8%)\n",
      "    Average reward: +0.138\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2603 (11.2%)\n",
      "    Action 1: 16727 (72.0%)\n",
      "    Action 2: 1588 (6.8%)\n",
      "    Action 3: 2306 (9.9%)\n",
      "  Player 1:\n",
      "    Action 0: 21696 (83.4%)\n",
      "    Action 1: 4314 (16.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1384.5, 1384.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.695 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.648 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.672\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1384\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.1716,  2.5687, -0.5691,  1.8781]])\n",
      "Player 1 Prediction: tensor([[0.0667, 0.9213, 0.0121, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.8844,  3.3712, -2.0561,  2.3725]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4620, 0.1216, 0.4164]])\n",
      "Player 0 Prediction: tensor([[ 1.5352,  2.6845, -2.9928,  1.3071]])\n",
      "Player 1 Prediction: tensor([[0.9710, 0.0000, 0.0290, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51820\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6328/10000 (63.3%)\n",
      "    Average reward: +0.984\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3672/10000 (36.7%)\n",
      "    Average reward: -0.984\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9496 (36.7%)\n",
      "    Action 1: 11382 (43.9%)\n",
      "    Action 2: 2580 (10.0%)\n",
      "    Action 3: 2443 (9.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6568 (25.3%)\n",
      "    Action 1: 14687 (56.7%)\n",
      "    Action 2: 2603 (10.0%)\n",
      "    Action 3: 2061 (8.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9836.5, -9836.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.966 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.009\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9837\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7175, 0.2390, 0.0436, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.3015, 0.6229, 0.0756, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2055, 0.2381, 0.5564]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47689\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6490/10000 (64.9%)\n",
      "    Average reward: +0.146\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3510/10000 (35.1%)\n",
      "    Average reward: -0.146\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20711 (82.7%)\n",
      "    Action 1: 4328 (17.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2823 (12.5%)\n",
      "    Action 1: 15688 (69.3%)\n",
      "    Action 2: 2097 (9.3%)\n",
      "    Action 3: 2042 (9.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1456.5, -1456.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.664 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.741 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.703\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1457\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7003/50000 [05:41<23:21, 30.69it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 46129/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 47599/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 7999/50000 [06:14<22:50, 30.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 52596/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 54174/2000000\n",
      "P1 SL Buffer Size:  52596\n",
      "P1 SL buffer distribution [15140. 26241.  4044.  7171.]\n",
      "P1 actions distribution [0.28785459 0.49891627 0.07688798 0.13634117]\n",
      "P2 SL Buffer Size:  54174\n",
      "P2 SL buffer distribution [17642. 26515.  4446.  5571.]\n",
      "P2 actions distribution [0.32565437 0.48944143 0.08206889 0.10283531]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.0956,  2.7956, -0.6683,  2.6024]])\n",
      "Player 0 Prediction: tensor([[0.0380, 0.9413, 0.0207, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.9828,  2.7846, -2.1717,  2.1021]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1403, 0.0638, 0.7959]])\n",
      "Player 1 Prediction: tensor([[ 0.3440,  1.5855, -2.8761,  0.8359]])\n",
      "Player 0 Prediction: tensor([[0.0996, 0.1376, 0.7628, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 7999/50000 [06:25<22:50, 30.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53755\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4709/10000 (47.1%)\n",
      "    Average reward: -1.015\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5291/10000 (52.9%)\n",
      "    Average reward: +1.015\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5888 (21.8%)\n",
      "    Action 1: 14999 (55.7%)\n",
      "    Action 2: 3079 (11.4%)\n",
      "    Action 3: 2985 (11.1%)\n",
      "  Player 1:\n",
      "    Action 0: 9870 (36.8%)\n",
      "    Action 1: 12458 (46.5%)\n",
      "    Action 2: 2594 (9.7%)\n",
      "    Action 3: 1882 (7.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-10147.5, 10147.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.950 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.997\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.0148\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9716, 0.0031, 0.0254]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9770, 0.0074, 0.0155]])\n",
      "Player 0 Prediction: tensor([[0.3546, 0.6127, 0.0326, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49282\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5395/10000 (53.9%)\n",
      "    Average reward: -0.136\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4605/10000 (46.1%)\n",
      "    Average reward: +0.136\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2488 (10.7%)\n",
      "    Action 1: 16695 (72.0%)\n",
      "    Action 2: 1515 (6.5%)\n",
      "    Action 3: 2498 (10.8%)\n",
      "  Player 1:\n",
      "    Action 0: 21690 (83.1%)\n",
      "    Action 1: 4396 (16.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1361.5, 1361.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.687 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.654 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.671\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1361\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7735, 0.1976, 0.0289, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.1858,  0.5438, -1.1990,  0.6615]])\n",
      "Player 1 Prediction: tensor([[0.9764, 0.0000, 0.0236, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.1903,  0.3472, -2.8232,  0.5864]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2358, 0.1633, 0.6009]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55037\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6368/10000 (63.7%)\n",
      "    Average reward: +0.758\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3632/10000 (36.3%)\n",
      "    Average reward: -0.758\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7257 (26.2%)\n",
      "    Action 1: 13162 (47.6%)\n",
      "    Action 2: 2305 (8.3%)\n",
      "    Action 3: 4952 (17.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8154 (29.8%)\n",
      "    Action 1: 13959 (51.0%)\n",
      "    Action 2: 2819 (10.3%)\n",
      "    Action 3: 2429 (8.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7582.5, -7582.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.016\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7582\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4184, 0.1533, 0.4283]])\n",
      "Player 1 Prediction: tensor([[0.2370, 0.4601, 0.3029, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47718\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6513/10000 (65.1%)\n",
      "    Average reward: +0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3487/10000 (34.9%)\n",
      "    Average reward: -0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20611 (82.3%)\n",
      "    Action 1: 4427 (17.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2660 (11.7%)\n",
      "    Action 1: 15652 (69.0%)\n",
      "    Action 2: 2255 (9.9%)\n",
      "    Action 3: 2113 (9.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1374.0, -1374.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.673 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.732 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.702\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1374\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9004/50000 [07:19<22:27, 30.42it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 58683/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 60389/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9998/50000 [07:52<22:29, 29.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 64709/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 66555/2000000\n",
      "P1 SL Buffer Size:  64709\n",
      "P1 SL buffer distribution [19524. 31100.  5214.  8871.]\n",
      "P1 actions distribution [0.30172001 0.48061321 0.08057612 0.13709067]\n",
      "P2 SL Buffer Size:  66555\n",
      "P2 SL buffer distribution [21712. 32556.  5511.  6776.]\n",
      "P2 actions distribution [0.32622643 0.48915934 0.0828037  0.10181053]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.9105, -0.5653, -0.5459,  1.5045]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7673, 0.0114, 0.2213]])\n",
      "Player 1 Prediction: tensor([[-0.2932, -0.5084, -0.9638,  0.7269]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8590, 0.0377, 0.1033]])\n",
      "Player 1 Prediction: tensor([[-3.3951, -4.8629, -1.8539, -2.9551]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9998/50000 [08:05<22:29, 29.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52419\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4996/10000 (50.0%)\n",
      "    Average reward: -0.814\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5004/10000 (50.0%)\n",
      "    Average reward: +0.814\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5910 (22.7%)\n",
      "    Action 1: 14196 (54.5%)\n",
      "    Action 2: 2603 (10.0%)\n",
      "    Action 3: 3347 (12.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9616 (36.5%)\n",
      "    Action 1: 11401 (43.2%)\n",
      "    Action 2: 2530 (9.6%)\n",
      "    Action 3: 2816 (10.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-8143.5, 8143.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.963 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.008\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.8144\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8377, 0.1466, 0.0156, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1311, 0.8439, 0.0250, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3995, 0.0804, 0.5201]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49149\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5209/10000 (52.1%)\n",
      "    Average reward: -0.161\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4791/10000 (47.9%)\n",
      "    Average reward: +0.161\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2921 (12.5%)\n",
      "    Action 1: 15600 (66.7%)\n",
      "    Action 2: 1839 (7.9%)\n",
      "    Action 3: 3027 (12.9%)\n",
      "  Player 1:\n",
      "    Action 0: 20585 (79.9%)\n",
      "    Action 1: 5177 (20.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1608.5, 1608.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.764 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.724 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.744\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1608\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.3313,  1.8119, -0.9026,  2.1715]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3293, 0.1265, 0.5442]])\n",
      "Player 0 Prediction: tensor([[ 0.4985,  0.2721, -1.5551,  1.1875]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2652, 0.1594, 0.5754]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49288\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6278/10000 (62.8%)\n",
      "    Average reward: +0.717\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3722/10000 (37.2%)\n",
      "    Average reward: -0.717\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10565 (42.1%)\n",
      "    Action 1: 7917 (31.6%)\n",
      "    Action 2: 2682 (10.7%)\n",
      "    Action 3: 3902 (15.6%)\n",
      "  Player 1:\n",
      "    Action 0: 5320 (22.0%)\n",
      "    Action 1: 13661 (56.4%)\n",
      "    Action 2: 2579 (10.6%)\n",
      "    Action 3: 2662 (11.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7165.0, -7165.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.946 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.998\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7165\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.1947e-02, 9.0751e-01, 5.4149e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9774, 0.0061, 0.0165]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47639\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6359/10000 (63.6%)\n",
      "    Average reward: +0.085\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3641/10000 (36.4%)\n",
      "    Average reward: -0.085\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20389 (81.2%)\n",
      "    Action 1: 4715 (18.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2663 (11.8%)\n",
      "    Action 1: 15383 (68.3%)\n",
      "    Action 2: 2137 (9.5%)\n",
      "    Action 3: 2352 (10.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [848.5, -848.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.697 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.740 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.718\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0848\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11006/50000 [08:58<21:23, 30.37it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 70789/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 72572/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11999/50000 [09:31<21:33, 29.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 76892/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 78800/2000000\n",
      "P1 SL Buffer Size:  76892\n",
      "P1 SL buffer distribution [24373. 35376.  6435. 10708.]\n",
      "P1 actions distribution [0.31697706 0.46007387 0.08368881 0.13926026]\n",
      "P2 SL Buffer Size:  78800\n",
      "P2 SL buffer distribution [25657. 38400.  6448.  8295.]\n",
      "P2 actions distribution [0.32559645 0.48730964 0.08182741 0.1052665 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[2.2047e-01, 7.7897e-01, 5.6152e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.4173, -0.2833, -0.9900,  0.3369]])\n",
      "Player 0 Prediction: tensor([[9.9917e-01, 0.0000e+00, 8.3414e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-2.0388, -2.7416, -2.9573, -2.6345]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7152, 0.0024, 0.2825]])\n",
      "Player 1 Prediction: tensor([[-4.3782, -7.0923, -3.0317, -3.9117]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11999/50000 [09:45<21:33, 29.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56852\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5096/10000 (51.0%)\n",
      "    Average reward: -0.558\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4904/10000 (49.0%)\n",
      "    Average reward: +0.558\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8918 (30.8%)\n",
      "    Action 1: 12895 (44.5%)\n",
      "    Action 2: 3068 (10.6%)\n",
      "    Action 3: 4078 (14.1%)\n",
      "  Player 1:\n",
      "    Action 0: 7477 (26.8%)\n",
      "    Action 1: 15409 (55.2%)\n",
      "    Action 2: 1857 (6.7%)\n",
      "    Action 3: 3150 (11.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5577.5, 5577.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.982 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.013\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5577\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.2079, 0.7892, 0.0029, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8705, 0.0288, 0.1007]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49225\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5347/10000 (53.5%)\n",
      "    Average reward: -0.072\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4653/10000 (46.5%)\n",
      "    Average reward: +0.072\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3335 (14.3%)\n",
      "    Action 1: 14947 (64.0%)\n",
      "    Action 2: 1795 (7.7%)\n",
      "    Action 3: 3288 (14.1%)\n",
      "  Player 1:\n",
      "    Action 0: 19965 (77.2%)\n",
      "    Action 1: 5895 (22.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-718.0, 718.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.813 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.774 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.794\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0718\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1346, 0.8612, 0.0042, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.3323,  0.0992, -1.2374,  0.4248]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6740, 0.0785, 0.2476]])\n",
      "Player 0 Prediction: tensor([[-1.7515, -1.4505, -1.8318, -1.3841]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48548\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6309/10000 (63.1%)\n",
      "    Average reward: +0.638\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3691/10000 (36.9%)\n",
      "    Average reward: -0.638\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11803 (47.9%)\n",
      "    Action 1: 6592 (26.8%)\n",
      "    Action 2: 2490 (10.1%)\n",
      "    Action 3: 3751 (15.2%)\n",
      "  Player 1:\n",
      "    Action 0: 4914 (20.6%)\n",
      "    Action 1: 12803 (53.5%)\n",
      "    Action 2: 2340 (9.8%)\n",
      "    Action 3: 3855 (16.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6377.0, -6377.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.018 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.952 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.985\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6377\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[8.7444e-02, 9.1222e-01, 3.3649e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9783, 0.0045, 0.0172]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47836\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6314/10000 (63.1%)\n",
      "    Average reward: +0.024\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3686/10000 (36.9%)\n",
      "    Average reward: -0.024\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20153 (79.7%)\n",
      "    Action 1: 5144 (20.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2696 (12.0%)\n",
      "    Action 1: 15024 (66.7%)\n",
      "    Action 2: 2051 (9.1%)\n",
      "    Action 3: 2768 (12.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [240.5, -240.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.729 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.756 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.743\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0240\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13005/50000 [10:37<21:41, 28.42it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 82770/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 85185/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [11:12<30:32, 19.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 88926/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 91482/2000000\n",
      "P1 SL Buffer Size:  88926\n",
      "P1 SL buffer distribution [29295. 39601.  7492. 12538.]\n",
      "P1 actions distribution [0.32943121 0.44532533 0.08424983 0.14099364]\n",
      "P2 SL Buffer Size:  91482\n",
      "P2 SL buffer distribution [29775. 44551.  7568.  9588.]\n",
      "P2 actions distribution [0.32547386 0.48699198 0.08272666 0.1048075 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.9660,  2.3294, -0.9484,  2.0236]])\n",
      "Player 0 Prediction: tensor([[0.3612, 0.6246, 0.0142, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.7948,  2.1149, -2.1794,  1.6464]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6803, 0.0480, 0.2717]])\n",
      "Player 1 Prediction: tensor([[-0.4352, -0.2842, -2.8693,  0.3548]])\n",
      "Player 0 Prediction: tensor([[0.9695, 0.0000, 0.0305, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [11:25<30:32, 19.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52729\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5355/10000 (53.5%)\n",
      "    Average reward: -0.709\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4645/10000 (46.5%)\n",
      "    Average reward: +0.709\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7292 (27.7%)\n",
      "    Action 1: 12986 (49.4%)\n",
      "    Action 2: 2363 (9.0%)\n",
      "    Action 3: 3649 (13.9%)\n",
      "  Player 1:\n",
      "    Action 0: 9065 (34.3%)\n",
      "    Action 1: 11912 (45.1%)\n",
      "    Action 2: 2721 (10.3%)\n",
      "    Action 3: 2741 (10.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7090.0, 7090.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.032\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7090\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2198, 0.0442, 0.7360]])\n",
      "Player 0 Prediction: tensor([[0.1248, 0.8292, 0.0459, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50009\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5466/10000 (54.7%)\n",
      "    Average reward: +0.082\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4534/10000 (45.3%)\n",
      "    Average reward: -0.082\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3933 (16.6%)\n",
      "    Action 1: 14469 (60.9%)\n",
      "    Action 2: 1733 (7.3%)\n",
      "    Action 3: 3617 (15.2%)\n",
      "  Player 1:\n",
      "    Action 0: 19418 (74.0%)\n",
      "    Action 1: 6839 (26.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [815.5, -815.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.865 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.827 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.846\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0815\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[7.9873e-02, 9.1993e-01, 1.9315e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.0661, -0.4083, -1.0778,  0.1034]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9893, 0.0016, 0.0090]])\n",
      "Player 0 Prediction: tensor([[-3.6005, -5.2382, -1.9785, -2.4967]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52061\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6373/10000 (63.7%)\n",
      "    Average reward: +0.630\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3627/10000 (36.3%)\n",
      "    Average reward: -0.630\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9606 (36.6%)\n",
      "    Action 1: 10501 (40.1%)\n",
      "    Action 2: 2335 (8.9%)\n",
      "    Action 3: 3770 (14.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6976 (27.0%)\n",
      "    Action 1: 13167 (50.9%)\n",
      "    Action 2: 2307 (8.9%)\n",
      "    Action 3: 3399 (13.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6297.5, -6297.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6298\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6257, 0.0155, 0.3587]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8229, 0.0324, 0.1447]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48175\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6290/10000 (62.9%)\n",
      "    Average reward: +0.030\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3710/10000 (37.1%)\n",
      "    Average reward: -0.030\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19900 (78.3%)\n",
      "    Action 1: 5515 (21.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2843 (12.5%)\n",
      "    Action 1: 14973 (65.8%)\n",
      "    Action 2: 1994 (8.8%)\n",
      "    Action 3: 2950 (13.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [302.5, -302.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.755 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.772 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.763\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0302\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15003/50000 [12:25<22:49, 25.55it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 95215/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 97814/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15999/50000 [13:04<21:39, 26.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 101539/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 104042/2000000\n",
      "P1 SL Buffer Size:  101539\n",
      "P1 SL buffer distribution [34026. 44362.  8580. 14571.]\n",
      "P1 actions distribution [0.33510277 0.43689617 0.08449955 0.14350151]\n",
      "P2 SL Buffer Size:  104042\n",
      "P2 SL buffer distribution [34159. 50016.  8841. 11026.]\n",
      "P2 actions distribution [0.32831933 0.48072894 0.0849753  0.10597643]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.3564, 0.6417, 0.0019, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.2120, -1.3182, -0.8646,  0.5995]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8867, 0.0192, 0.0941]])\n",
      "Player 1 Prediction: tensor([[-4.3428, -5.6993, -1.9690, -4.1966]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15999/50000 [13:15<21:39, 26.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51326\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5380/10000 (53.8%)\n",
      "    Average reward: -0.667\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4620/10000 (46.2%)\n",
      "    Average reward: +0.667\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7198 (28.2%)\n",
      "    Action 1: 12092 (47.4%)\n",
      "    Action 2: 2196 (8.6%)\n",
      "    Action 3: 4010 (15.7%)\n",
      "  Player 1:\n",
      "    Action 0: 8359 (32.4%)\n",
      "    Action 1: 11024 (42.7%)\n",
      "    Action 2: 2407 (9.3%)\n",
      "    Action 3: 4040 (15.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6668.5, 6668.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6669\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9429, 0.0507, 0.0064, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50520\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5348/10000 (53.5%)\n",
      "    Average reward: -0.012\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4652/10000 (46.5%)\n",
      "    Average reward: +0.012\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4180 (17.5%)\n",
      "    Action 1: 14156 (59.1%)\n",
      "    Action 2: 1815 (7.6%)\n",
      "    Action 3: 3794 (15.8%)\n",
      "  Player 1:\n",
      "    Action 0: 19196 (72.2%)\n",
      "    Action 1: 7379 (27.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-123.5, 123.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.888 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.852 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.870\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0123\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.3255, -0.4509, -0.3958,  0.7619]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1924, 0.0585, 0.7491]])\n",
      "Player 0 Prediction: tensor([[-0.0541, -0.3724, -1.2315,  0.0018]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3082, 0.2620, 0.4298]])\n",
      "Player 0 Prediction: tensor([[-0.5114, -0.1222, -1.7805, -0.3848]])\n",
      "Player 1 Prediction: tensor([[0.0187, 0.0727, 0.9086, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52016\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6163/10000 (61.6%)\n",
      "    Average reward: +0.459\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3837/10000 (38.4%)\n",
      "    Average reward: -0.459\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10025 (37.7%)\n",
      "    Action 1: 7786 (29.3%)\n",
      "    Action 2: 2530 (9.5%)\n",
      "    Action 3: 6218 (23.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6694 (26.3%)\n",
      "    Action 1: 12530 (49.2%)\n",
      "    Action 2: 1609 (6.3%)\n",
      "    Action 3: 4624 (18.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4592.5, -4592.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.010 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.030\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4592\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6406, 0.0109, 0.3485]])\n",
      "Player 1 Prediction: tensor([[0.0388, 0.9455, 0.0157, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48438\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6245/10000 (62.5%)\n",
      "    Average reward: -0.027\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3755/10000 (37.5%)\n",
      "    Average reward: +0.027\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19744 (77.3%)\n",
      "    Action 1: 5792 (22.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2917 (12.7%)\n",
      "    Action 1: 14792 (64.6%)\n",
      "    Action 2: 2088 (9.1%)\n",
      "    Action 3: 3105 (13.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-266.5, 266.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.772 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.786 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.779\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0267\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17003/50000 [14:10<18:49, 29.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 107879/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 110113/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17999/50000 [14:47<21:18, 25.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 114026/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 116595/2000000\n",
      "P1 SL Buffer Size:  114026\n",
      "P1 SL buffer distribution [38801. 48807.  9797. 16621.]\n",
      "P1 actions distribution [0.34028204 0.42803396 0.085919   0.145765  ]\n",
      "P2 SL Buffer Size:  116595\n",
      "P2 SL buffer distribution [38204. 55443.  9874. 13074.]\n",
      "P2 actions distribution [0.32766414 0.47551782 0.08468631 0.11213174]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.4569,  2.4981, -0.5468,  3.0162]])\n",
      "Player 0 Prediction: tensor([[0.1797, 0.8180, 0.0023, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.1378,  2.1885, -2.0082,  2.7019]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5591, 0.0035, 0.4374]])\n",
      "Player 1 Prediction: tensor([[ 0.2963, -1.5062, -3.1928,  0.6306]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51770\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5524/10000 (55.2%)\n",
      "    Average reward: -0.698\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4476/10000 (44.8%)\n",
      "    Average reward: +0.698\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6864 (26.8%)\n",
      "    Action 1: 12900 (50.4%)\n",
      "    Action 2: 2010 (7.9%)\n",
      "    Action 3: 3824 (14.9%)\n",
      "  Player 1:\n",
      "    Action 0: 9715 (37.1%)\n",
      "    Action 1: 10352 (39.6%)\n",
      "    Action 2: 2719 (10.4%)\n",
      "    Action 3: 3386 (12.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6984.5, 6984.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6985\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.8624e-01, 7.5354e-04, 2.1300e-01]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9689, 0.0016, 0.0295]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17999/50000 [15:06<21:18, 25.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50522\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5364/10000 (53.6%)\n",
      "    Average reward: +0.009\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4636/10000 (46.4%)\n",
      "    Average reward: -0.009\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4382 (18.3%)\n",
      "    Action 1: 13853 (58.0%)\n",
      "    Action 2: 1769 (7.4%)\n",
      "    Action 3: 3901 (16.3%)\n",
      "  Player 1:\n",
      "    Action 0: 18928 (71.1%)\n",
      "    Action 1: 7689 (28.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [89.0, -89.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.905 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.867 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.886\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0089\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.8807e-02, 9.0110e-01, 9.4509e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.0911, -0.1408, -1.3506, -0.0452]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9570, 0.0028, 0.0402]])\n",
      "Player 0 Prediction: tensor([[-3.6190, -3.7666, -2.1162, -2.2742]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49682\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6046/10000 (60.5%)\n",
      "    Average reward: +0.445\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3954/10000 (39.5%)\n",
      "    Average reward: -0.445\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11819 (47.1%)\n",
      "    Action 1: 7525 (30.0%)\n",
      "    Action 2: 2650 (10.6%)\n",
      "    Action 3: 3087 (12.3%)\n",
      "  Player 1:\n",
      "    Action 0: 5486 (22.3%)\n",
      "    Action 1: 12553 (51.0%)\n",
      "    Action 2: 2003 (8.1%)\n",
      "    Action 3: 4559 (18.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4450.0, -4450.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.033 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.978 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4450\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.9302, 0.0638, 0.0060, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.5673, 0.4246, 0.0081, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0219, 0.6736, 0.3046, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48728\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6292/10000 (62.9%)\n",
      "    Average reward: -0.052\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3708/10000 (37.1%)\n",
      "    Average reward: +0.052\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19433 (75.9%)\n",
      "    Action 1: 6174 (24.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3051 (13.2%)\n",
      "    Action 1: 14522 (62.8%)\n",
      "    Action 2: 2172 (9.4%)\n",
      "    Action 3: 3376 (14.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-515.5, 515.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.797 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.807 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.802\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0515\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19003/50000 [15:55<19:48, 26.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 120171/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 123000/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19999/50000 [16:34<19:55, 25.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 126310/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 129394/2000000\n",
      "P1 SL Buffer Size:  126310\n",
      "P1 SL buffer distribution [43679. 53168. 10995. 18468.]\n",
      "P1 actions distribution [0.34580793 0.42093263 0.08704774 0.1462117 ]\n",
      "P2 SL Buffer Size:  129394\n",
      "P2 SL buffer distribution [42487. 60855. 10997. 15055.]\n",
      "P2 actions distribution [0.32835371 0.47030774 0.08498848 0.11635006]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.7363,  0.5437, -0.8813,  1.1995]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1441, 0.0320, 0.8240]])\n",
      "Player 1 Prediction: tensor([[ 0.0151,  0.1999, -1.3014,  0.9210]])\n",
      "Player 0 Prediction: tensor([[0.9800, 0.0000, 0.0200, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.3383,  0.3828, -2.7269,  0.5760]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1498, 0.0220, 0.8282]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19999/50000 [16:46<19:55, 25.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51354\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5435/10000 (54.4%)\n",
      "    Average reward: -0.592\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4565/10000 (45.6%)\n",
      "    Average reward: +0.592\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7764 (30.2%)\n",
      "    Action 1: 10769 (41.9%)\n",
      "    Action 2: 2012 (7.8%)\n",
      "    Action 3: 5175 (20.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8474 (33.1%)\n",
      "    Action 1: 9992 (39.0%)\n",
      "    Action 2: 2115 (8.3%)\n",
      "    Action 3: 5053 (19.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5924.5, 5924.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.053\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5925\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4024, 0.0045, 0.5931]])\n",
      "Player 0 Prediction: tensor([[0.1219, 0.8714, 0.0067, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50744\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5370/10000 (53.7%)\n",
      "    Average reward: +0.027\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4630/10000 (46.3%)\n",
      "    Average reward: -0.027\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4635 (19.3%)\n",
      "    Action 1: 13704 (56.9%)\n",
      "    Action 2: 1797 (7.5%)\n",
      "    Action 3: 3932 (16.3%)\n",
      "  Player 1:\n",
      "    Action 0: 18706 (70.1%)\n",
      "    Action 1: 7970 (29.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [267.0, -267.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.880 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.900\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0267\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.8568,  2.0207, -0.6495,  1.9525]])\n",
      "Player 1 Prediction: tensor([[0.6110, 0.3819, 0.0071, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 3.0031,  2.5391, -1.9590,  2.4488]])\n",
      "Player 1 Prediction: tensor([[0.0082, 0.0529, 0.9389, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51674\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6167/10000 (61.7%)\n",
      "    Average reward: +0.411\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3833/10000 (38.3%)\n",
      "    Average reward: -0.411\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9091 (35.2%)\n",
      "    Action 1: 10650 (41.3%)\n",
      "    Action 2: 2151 (8.3%)\n",
      "    Action 3: 3904 (15.1%)\n",
      "  Player 1:\n",
      "    Action 0: 7312 (28.3%)\n",
      "    Action 1: 12266 (47.4%)\n",
      "    Action 2: 2397 (9.3%)\n",
      "    Action 3: 3903 (15.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4112.0, -4112.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.041\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4112\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.4103e-01, 6.2444e-04, 5.8341e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9164, 0.0030, 0.0806]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49286\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6257/10000 (62.6%)\n",
      "    Average reward: -0.089\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3743/10000 (37.4%)\n",
      "    Average reward: +0.089\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19403 (75.0%)\n",
      "    Action 1: 6475 (25.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2993 (12.8%)\n",
      "    Action 1: 14381 (61.4%)\n",
      "    Action 2: 2355 (10.1%)\n",
      "    Action 3: 3679 (15.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-890.5, 890.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.812 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.811 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.811\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0891\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21005/50000 [17:42<16:36, 29.09it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 132996/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 135658/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21998/50000 [18:20<19:01, 24.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 139273/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 142078/2000000\n",
      "P1 SL Buffer Size:  139273\n",
      "P1 SL buffer distribution [48960. 57343. 12246. 20724.]\n",
      "P1 actions distribution [0.35153978 0.41173092 0.08792803 0.14880128]\n",
      "P2 SL Buffer Size:  142078\n",
      "P2 SL buffer distribution [46567. 66103. 12089. 17319.]\n",
      "P2 actions distribution [0.32775658 0.46525852 0.08508706 0.12189783]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.2224,  1.1267, -0.7008,  1.1610]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.0307e-01, 4.8160e-04, 1.9645e-01]])\n",
      "Player 1 Prediction: tensor([[ 0.7384,  0.7237, -0.9970,  1.3240]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9888, 0.0018, 0.0094]])\n",
      "Player 1 Prediction: tensor([[-4.1178, -3.6905, -2.0890, -1.9676]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21998/50000 [18:36<19:01, 24.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52306\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5583/10000 (55.8%)\n",
      "    Average reward: -0.652\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4417/10000 (44.2%)\n",
      "    Average reward: +0.652\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7857 (29.5%)\n",
      "    Action 1: 11622 (43.7%)\n",
      "    Action 2: 2057 (7.7%)\n",
      "    Action 3: 5056 (19.0%)\n",
      "  Player 1:\n",
      "    Action 0: 9467 (36.8%)\n",
      "    Action 1: 9654 (37.5%)\n",
      "    Action 2: 2358 (9.2%)\n",
      "    Action 3: 4235 (16.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6523.5, 6523.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.051\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6523\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.0307e-01, 4.8160e-04, 1.9645e-01]])\n",
      "Player 0 Prediction: tensor([[4.8071e-02, 9.5169e-01, 2.4263e-04, 0.0000e+00]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51101\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5451/10000 (54.5%)\n",
      "    Average reward: +0.136\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4549/10000 (45.5%)\n",
      "    Average reward: -0.136\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4938 (20.3%)\n",
      "    Action 1: 13451 (55.4%)\n",
      "    Action 2: 1865 (7.7%)\n",
      "    Action 3: 4045 (16.6%)\n",
      "  Player 1:\n",
      "    Action 0: 18457 (68.9%)\n",
      "    Action 1: 8345 (31.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1360.0, -1360.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.939 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.895 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.917\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1360\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1200, 0.8791, 0.0009, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.1743, -0.5525, -1.3421, -0.2647]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5804, 0.0141, 0.4055]])\n",
      "Player 0 Prediction: tensor([[-4.0549, -5.3064, -2.1486, -2.4940]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52099\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6078/10000 (60.8%)\n",
      "    Average reward: +0.295\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3922/10000 (39.2%)\n",
      "    Average reward: -0.295\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9947 (37.7%)\n",
      "    Action 1: 8924 (33.8%)\n",
      "    Action 2: 2502 (9.5%)\n",
      "    Action 3: 5020 (19.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7205 (28.0%)\n",
      "    Action 1: 11760 (45.7%)\n",
      "    Action 2: 1686 (6.6%)\n",
      "    Action 3: 5055 (19.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2949.0, -2949.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.045\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2949\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.9596, 0.0377, 0.0028, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6273, 0.3666, 0.0061, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2073, 0.0650, 0.7277]])\n",
      "Player 1 Prediction: tensor([[0.1095, 0.0580, 0.8325, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50031\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6298/10000 (63.0%)\n",
      "    Average reward: -0.139\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3702/10000 (37.0%)\n",
      "    Average reward: +0.139\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19034 (72.8%)\n",
      "    Action 1: 7117 (27.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3210 (13.4%)\n",
      "    Action 1: 14059 (58.9%)\n",
      "    Action 2: 2510 (10.5%)\n",
      "    Action 3: 4101 (17.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1388.0, 1388.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.845 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.839 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.842\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1388\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23003/50000 [19:28<17:01, 26.42it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 145445/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 148451/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [20:08<17:35, 24.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 151685/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 154633/2000000\n",
      "P1 SL Buffer Size:  151685\n",
      "P1 SL buffer distribution [54138. 61329. 13518. 22700.]\n",
      "P1 actions distribution [0.3569107  0.40431816 0.0891189  0.14965224]\n",
      "P2 SL Buffer Size:  154633\n",
      "P2 SL buffer distribution [50868. 70864. 13218. 19683.]\n",
      "P2 actions distribution [0.32895954 0.45827217 0.08547981 0.12728848]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.5384,  2.4208, -0.5284,  2.4960]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.1779e-01, 4.3623e-04, 1.8178e-01]])\n",
      "Player 1 Prediction: tensor([[-0.1901,  0.4463, -1.3249,  0.2941]])\n",
      "Player 0 Prediction: tensor([[6.3819e-02, 9.3555e-01, 6.3097e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-1.2967, -1.6362, -3.5983,  0.3722]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51968\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5496/10000 (55.0%)\n",
      "    Average reward: -0.492\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4504/10000 (45.0%)\n",
      "    Average reward: +0.492\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7535 (28.9%)\n",
      "    Action 1: 11257 (43.1%)\n",
      "    Action 2: 1597 (6.1%)\n",
      "    Action 3: 5720 (21.9%)\n",
      "  Player 1:\n",
      "    Action 0: 10994 (42.5%)\n",
      "    Action 1: 8555 (33.1%)\n",
      "    Action 2: 1790 (6.9%)\n",
      "    Action 3: 4520 (17.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4920.0, 4920.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.047\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4920\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.5516, 0.4475, 0.0009, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2974, 0.7006, 0.0020, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0175, 0.1287, 0.8538, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [20:27<17:35, 24.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50973\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5305/10000 (53.0%)\n",
      "    Average reward: +0.084\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4695/10000 (46.9%)\n",
      "    Average reward: -0.084\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5159 (21.2%)\n",
      "    Action 1: 13040 (53.5%)\n",
      "    Action 2: 2100 (8.6%)\n",
      "    Action 3: 4073 (16.7%)\n",
      "  Player 1:\n",
      "    Action 0: 18006 (67.7%)\n",
      "    Action 1: 8595 (32.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [844.0, -844.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.957 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.908 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.932\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0844\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.9629, 0.0349, 0.0022, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.2162, -0.7990, -1.0225, -0.0075]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0387, 0.0509, 0.9104]])\n",
      "Player 0 Prediction: tensor([[-1.4682, -1.2385, -0.9271, -0.2706]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51661\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6022/10000 (60.2%)\n",
      "    Average reward: +0.375\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3978/10000 (39.8%)\n",
      "    Average reward: -0.375\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9356 (36.0%)\n",
      "    Action 1: 11210 (43.2%)\n",
      "    Action 2: 2341 (9.0%)\n",
      "    Action 3: 3053 (11.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7469 (29.1%)\n",
      "    Action 1: 11726 (45.6%)\n",
      "    Action 2: 2475 (9.6%)\n",
      "    Action 3: 4031 (15.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3754.5, -3754.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3755\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6591, 0.0043, 0.3365]])\n",
      "Player 1 Prediction: tensor([[0.0339, 0.9509, 0.0151, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51198\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6215/10000 (62.2%)\n",
      "    Average reward: -0.257\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3785/10000 (37.9%)\n",
      "    Average reward: +0.257\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18845 (70.8%)\n",
      "    Action 1: 7769 (29.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3384 (13.8%)\n",
      "    Action 1: 13839 (56.3%)\n",
      "    Action 2: 2734 (11.1%)\n",
      "    Action 3: 4627 (18.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2566.0, 2566.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.871 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.860 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.866\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2566\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25003/50000 [21:17<15:47, 26.38it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 157903/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 161093/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [22:00<16:54, 23.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 164339/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 167278/2000000\n",
      "P1 SL Buffer Size:  164339\n",
      "P1 SL buffer distribution [59367. 65282. 14740. 24950.]\n",
      "P1 actions distribution [0.36124718 0.39723985 0.08969265 0.15182032]\n",
      "P2 SL Buffer Size:  167278\n",
      "P2 SL buffer distribution [55338. 75370. 14423. 22147.]\n",
      "P2 actions distribution [0.33081457 0.45056732 0.08622174 0.13239637]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.1983,  0.9842, -0.6681,  1.3891]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3504, 0.0056, 0.6440]])\n",
      "Player 1 Prediction: tensor([[ 0.4601,  0.1338, -0.8060,  0.8198]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1405, 0.0016, 0.8579]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48506\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5887/10000 (58.9%)\n",
      "    Average reward: -0.517\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4113/10000 (41.1%)\n",
      "    Average reward: +0.517\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6387 (26.8%)\n",
      "    Action 1: 10382 (43.6%)\n",
      "    Action 2: 1300 (5.5%)\n",
      "    Action 3: 5758 (24.2%)\n",
      "  Player 1:\n",
      "    Action 0: 9239 (37.4%)\n",
      "    Action 1: 6534 (26.5%)\n",
      "    Action 2: 2599 (10.5%)\n",
      "    Action 3: 6307 (25.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5172.0, 5172.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.035\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5172\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1078, 0.0328, 0.8593]])\n",
      "Player 0 Prediction: tensor([[0.1496, 0.6181, 0.2324, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [22:19<16:54, 23.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50895\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5348/10000 (53.5%)\n",
      "    Average reward: +0.178\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4652/10000 (46.5%)\n",
      "    Average reward: -0.178\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5420 (22.2%)\n",
      "    Action 1: 12790 (52.5%)\n",
      "    Action 2: 2180 (8.9%)\n",
      "    Action 3: 3988 (16.4%)\n",
      "  Player 1:\n",
      "    Action 0: 17786 (67.1%)\n",
      "    Action 1: 8731 (32.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1782.0, -1782.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.914 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.942\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1782\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.5820,  1.6140, -0.6555,  1.5751]])\n",
      "Player 1 Prediction: tensor([[0.6826, 0.3132, 0.0042, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.2381,  1.1798, -1.4064,  1.5290]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1665, 0.0549, 0.7786]])\n",
      "Player 0 Prediction: tensor([[-0.3889, -0.6550, -1.8945, -0.3576]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48786\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5939/10000 (59.4%)\n",
      "    Average reward: +0.479\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4061/10000 (40.6%)\n",
      "    Average reward: -0.479\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10867 (43.6%)\n",
      "    Action 1: 7201 (28.9%)\n",
      "    Action 2: 1951 (7.8%)\n",
      "    Action 3: 4912 (19.7%)\n",
      "  Player 1:\n",
      "    Action 0: 5070 (21.3%)\n",
      "    Action 1: 11661 (48.9%)\n",
      "    Action 2: 1982 (8.3%)\n",
      "    Action 3: 5142 (21.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4789.5, -4789.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.040 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.980 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.010\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4789\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.2870e-01, 2.4167e-04, 7.1058e-02]])\n",
      "Player 1 Prediction: tensor([[0.0403, 0.9581, 0.0016, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51693\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6185/10000 (61.9%)\n",
      "    Average reward: -0.324\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3815/10000 (38.1%)\n",
      "    Average reward: +0.324\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18770 (69.9%)\n",
      "    Action 1: 8095 (30.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3433 (13.8%)\n",
      "    Action 1: 13812 (55.6%)\n",
      "    Action 2: 2699 (10.9%)\n",
      "    Action 3: 4884 (19.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3240.0, 3240.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.883 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.865 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.874\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3240\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27003/50000 [23:09<15:16, 25.09it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 170612/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 173462/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27999/50000 [23:51<14:51, 24.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 176837/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 180015/2000000\n",
      "P1 SL Buffer Size:  176837\n",
      "P1 SL buffer distribution [64838. 69136. 15947. 26916.]\n",
      "P1 actions distribution [0.36665404 0.39095891 0.09017909 0.15220797]\n",
      "P2 SL Buffer Size:  180015\n",
      "P2 SL buffer distribution [59601. 80313. 15719. 24382.]\n",
      "P2 actions distribution [0.33108908 0.44614615 0.0873205  0.13544427]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.7807,  2.2795, -0.7669,  1.9964]])\n",
      "Player 0 Prediction: tensor([[6.1455e-02, 9.3852e-01, 2.4197e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 2.2042,  1.6254, -2.0545,  2.6628]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.7901e-01, 3.2333e-04, 2.0666e-02]])\n",
      "Player 1 Prediction: tensor([[ 0.7382,  0.7720, -2.7645,  1.6331]])\n",
      "Player 0 Prediction: tensor([[0.9739, 0.0000, 0.0261, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51026\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5556/10000 (55.6%)\n",
      "    Average reward: -0.444\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4444/10000 (44.4%)\n",
      "    Average reward: +0.444\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8148 (31.8%)\n",
      "    Action 1: 11013 (42.9%)\n",
      "    Action 2: 2585 (10.1%)\n",
      "    Action 3: 3905 (15.2%)\n",
      "  Player 1:\n",
      "    Action 0: 7948 (31.3%)\n",
      "    Action 1: 11442 (45.1%)\n",
      "    Action 2: 2657 (10.5%)\n",
      "    Action 3: 3328 (13.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4441.0, 4441.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.046\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4441\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9699, 0.0273, 0.0028, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.6958, 0.2991, 0.0051, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1091, 0.0235, 0.8673]])\n",
      "Player 0 Prediction: tensor([[0.1007, 0.0434, 0.8559, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27999/50000 [24:10<14:51, 24.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50866\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5247/10000 (52.5%)\n",
      "    Average reward: +0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4753/10000 (47.5%)\n",
      "    Average reward: -0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5520 (22.6%)\n",
      "    Action 1: 12505 (51.1%)\n",
      "    Action 2: 2425 (9.9%)\n",
      "    Action 3: 4018 (16.4%)\n",
      "  Player 1:\n",
      "    Action 0: 17557 (66.5%)\n",
      "    Action 1: 8841 (33.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1904.5, -1904.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.980 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.950\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1905\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.4403e-01, 7.5531e-01, 6.5809e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.9239,  1.9022, -1.1746,  0.6549]])\n",
      "Player 1 Prediction: tensor([[0.9978, 0.0000, 0.0022, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.1254, -0.3077, -1.7267, -0.1083]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8180, 0.0024, 0.1796]])\n",
      "Player 0 Prediction: tensor([[-2.0721, -2.5654, -2.9481, -1.3545]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51281\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5829/10000 (58.3%)\n",
      "    Average reward: +0.353\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4171/10000 (41.7%)\n",
      "    Average reward: -0.353\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11823 (45.3%)\n",
      "    Action 1: 7642 (29.3%)\n",
      "    Action 2: 2633 (10.1%)\n",
      "    Action 3: 4015 (15.4%)\n",
      "  Player 1:\n",
      "    Action 0: 5680 (22.6%)\n",
      "    Action 1: 11899 (47.3%)\n",
      "    Action 2: 1981 (7.9%)\n",
      "    Action 3: 5608 (22.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3529.0, -3529.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.036 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.996 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.016\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3529\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.0498e-01, 8.9499e-01, 2.4200e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5610, 0.0010, 0.4380]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51712\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6198/10000 (62.0%)\n",
      "    Average reward: -0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3802/10000 (38.0%)\n",
      "    Average reward: +0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18664 (69.5%)\n",
      "    Action 1: 8209 (30.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3696 (14.9%)\n",
      "    Action 1: 13727 (55.3%)\n",
      "    Action 2: 2669 (10.7%)\n",
      "    Action 3: 4747 (19.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3588.0, 3588.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.888 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.882 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.885\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3588\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29006/50000 [25:00<12:38, 27.69it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 183273/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 186197/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [25:41<14:07, 23.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 189756/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 192611/2000000\n",
      "P1 SL Buffer Size:  189756\n",
      "P1 SL buffer distribution [70464. 73165. 17171. 28956.]\n",
      "P1 actions distribution [0.37134004 0.38557411 0.09048989 0.15259597]\n",
      "P2 SL Buffer Size:  192611\n",
      "P2 SL buffer distribution [63817. 85363. 17015. 26416.]\n",
      "P2 actions distribution [0.33132583 0.4431886  0.08833867 0.13714689]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.7682,  0.9931, -1.0171,  0.5568]])\n",
      "Player 0 Prediction: tensor([[0.7285, 0.2670, 0.0045, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.0104, -0.2811, -1.4862,  0.5077]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1083, 0.0242, 0.8675]])\n",
      "Player 1 Prediction: tensor([[-0.0588,  0.0315, -2.5678,  0.4878]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50605\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5577/10000 (55.8%)\n",
      "    Average reward: -0.343\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4423/10000 (44.2%)\n",
      "    Average reward: +0.343\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8196 (32.8%)\n",
      "    Action 1: 10555 (42.2%)\n",
      "    Action 2: 2326 (9.3%)\n",
      "    Action 3: 3938 (15.7%)\n",
      "  Player 1:\n",
      "    Action 0: 8123 (31.7%)\n",
      "    Action 1: 11220 (43.8%)\n",
      "    Action 2: 2319 (9.1%)\n",
      "    Action 3: 3928 (15.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3426.0, 3426.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3426\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[6.4769e-01, 3.5186e-01, 4.4164e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.4314, 0.5671, 0.0015, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2907, 0.0025, 0.7068]])\n",
      "Player 0 Prediction: tensor([[0.7744, 0.1754, 0.0502, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [26:00<14:07, 23.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50984\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5282/10000 (52.8%)\n",
      "    Average reward: +0.289\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4718/10000 (47.2%)\n",
      "    Average reward: -0.289\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5779 (23.5%)\n",
      "    Action 1: 12190 (49.5%)\n",
      "    Action 2: 2543 (10.3%)\n",
      "    Action 3: 4127 (16.7%)\n",
      "  Player 1:\n",
      "    Action 0: 17233 (65.4%)\n",
      "    Action 1: 9112 (34.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2889.5, -2889.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.993 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.930 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.962\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2889\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.7393,  0.5151, -0.6024,  0.7643]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.3587e-01, 1.5945e-04, 6.3967e-02]])\n",
      "Player 0 Prediction: tensor([[ 0.4824,  0.0290, -1.1134,  0.0911]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5843, 0.0008, 0.4149]])\n",
      "Player 0 Prediction: tensor([[ 4.4676,  6.0456, -1.9967,  4.5742]])\n",
      "Player 1 Prediction: tensor([[0.9873, 0.0000, 0.0127, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50868\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5830/10000 (58.3%)\n",
      "    Average reward: +0.264\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4170/10000 (41.7%)\n",
      "    Average reward: -0.264\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11718 (45.6%)\n",
      "    Action 1: 8423 (32.8%)\n",
      "    Action 2: 2261 (8.8%)\n",
      "    Action 3: 3280 (12.8%)\n",
      "  Player 1:\n",
      "    Action 0: 6129 (24.3%)\n",
      "    Action 1: 11510 (45.7%)\n",
      "    Action 2: 2183 (8.7%)\n",
      "    Action 3: 5364 (21.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2636.5, -2636.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.012 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2636\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.0460e-01, 8.9538e-01, 2.0060e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5843, 0.0008, 0.4149]])\n",
      "Player 1 Prediction: tensor([[0.0559, 0.9130, 0.0311, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51909\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6162/10000 (61.6%)\n",
      "    Average reward: -0.402\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3838/10000 (38.4%)\n",
      "    Average reward: +0.402\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18674 (69.3%)\n",
      "    Action 1: 8290 (30.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3754 (15.0%)\n",
      "    Action 1: 13710 (55.0%)\n",
      "    Action 2: 2711 (10.9%)\n",
      "    Action 3: 4770 (19.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4021.0, 4021.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.890 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.886 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.888\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4021\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31003/50000 [26:51<12:45, 24.81it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 196058/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 199180/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31999/50000 [27:33<12:35, 23.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 202359/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 205578/2000000\n",
      "P1 SL Buffer Size:  202359\n",
      "P1 SL buffer distribution [76007. 77044. 18293. 31015.]\n",
      "P1 actions distribution [0.37560474 0.3807293  0.09039875 0.15326721]\n",
      "P2 SL Buffer Size:  205578\n",
      "P2 SL buffer distribution [68268. 90411. 18299. 28600.]\n",
      "P2 actions distribution [0.33207834 0.43978928 0.08901244 0.13911994]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.5465,  0.8816, -1.0451,  0.5784]])\n",
      "Player 0 Prediction: tensor([[0.7293, 0.2667, 0.0040, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.4700,  1.7493, -2.2605,  1.2679]])\n",
      "Player 0 Prediction: tensor([[0.0147, 0.0370, 0.9483, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53043\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5651/10000 (56.5%)\n",
      "    Average reward: -0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4349/10000 (43.5%)\n",
      "    Average reward: +0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10155 (37.9%)\n",
      "    Action 1: 9888 (36.9%)\n",
      "    Action 2: 2543 (9.5%)\n",
      "    Action 3: 4202 (15.7%)\n",
      "  Player 1:\n",
      "    Action 0: 5629 (21.4%)\n",
      "    Action 1: 13433 (51.2%)\n",
      "    Action 2: 2530 (9.6%)\n",
      "    Action 3: 4663 (17.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2778.5, 2778.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.016\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2778\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3817, 0.0062, 0.6121]])\n",
      "Player 0 Prediction: tensor([[0.3130, 0.6514, 0.0357, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31999/50000 [27:51<12:35, 23.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50931\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5217/10000 (52.2%)\n",
      "    Average reward: +0.331\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4783/10000 (47.8%)\n",
      "    Average reward: -0.331\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5897 (23.9%)\n",
      "    Action 1: 11963 (48.6%)\n",
      "    Action 2: 2705 (11.0%)\n",
      "    Action 3: 4075 (16.5%)\n",
      "  Player 1:\n",
      "    Action 0: 17040 (64.8%)\n",
      "    Action 1: 9251 (35.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3312.0, -3312.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.000 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.936 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.968\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3312\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.6811e-02, 9.0317e-01, 1.6581e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.4085,  0.8176, -0.7749,  0.8536]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.8509e-01, 7.3794e-04, 1.1417e-01]])\n",
      "Player 0 Prediction: tensor([[-2.9241, -2.9531, -0.8938, -1.5599]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49616\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5932/10000 (59.3%)\n",
      "    Average reward: +0.382\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4068/10000 (40.7%)\n",
      "    Average reward: -0.382\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10541 (41.9%)\n",
      "    Action 1: 7708 (30.6%)\n",
      "    Action 2: 2153 (8.6%)\n",
      "    Action 3: 4773 (19.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5409 (22.1%)\n",
      "    Action 1: 11679 (47.8%)\n",
      "    Action 2: 2231 (9.1%)\n",
      "    Action 3: 5122 (21.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3818.5, -3818.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.991 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.020\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3819\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.6811e-02, 9.0317e-01, 1.6581e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5609, 0.0006, 0.4385]])\n",
      "Player 1 Prediction: tensor([[0.0652, 0.9056, 0.0292, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51462\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6085/10000 (60.9%)\n",
      "    Average reward: -0.475\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3915/10000 (39.1%)\n",
      "    Average reward: +0.475\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18720 (69.9%)\n",
      "    Action 1: 8058 (30.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3690 (14.9%)\n",
      "    Action 1: 13727 (55.6%)\n",
      "    Action 2: 2680 (10.9%)\n",
      "    Action 3: 4587 (18.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4748.5, 4748.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.882 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.881 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.882\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4748\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33003/50000 [28:45<11:36, 24.40it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 208945/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 211752/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [29:27<11:18, 23.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 215509/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 218157/2000000\n",
      "P1 SL Buffer Size:  215509\n",
      "P1 SL buffer distribution [81583. 81674. 19228. 33024.]\n",
      "P1 actions distribution [0.3785596  0.37898185 0.08922133 0.15323722]\n",
      "P2 SL Buffer Size:  218157\n",
      "P2 SL buffer distribution [72451. 95413. 19661. 30632.]\n",
      "P2 actions distribution [0.33210486 0.43735933 0.09012317 0.14041264]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9776, 0.0210, 0.0014, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.9528,  2.1714, -1.1542,  1.4840]])\n",
      "Player 0 Prediction: tensor([[0.7529, 0.2437, 0.0034, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.1450,  1.2263, -1.8644,  0.9222]])\n",
      "Player 0 Prediction: tensor([[0.0233, 0.8539, 0.1227, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-3.8371, -0.2022, -3.6425,  0.4981]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [29:42<11:18, 23.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50437\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5741/10000 (57.4%)\n",
      "    Average reward: -0.339\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4259/10000 (42.6%)\n",
      "    Average reward: +0.339\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7827 (30.9%)\n",
      "    Action 1: 10424 (41.1%)\n",
      "    Action 2: 2171 (8.6%)\n",
      "    Action 3: 4914 (19.4%)\n",
      "  Player 1:\n",
      "    Action 0: 8944 (35.6%)\n",
      "    Action 1: 9218 (36.7%)\n",
      "    Action 2: 2740 (10.9%)\n",
      "    Action 3: 4199 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3388.0, 3388.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.056\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3388\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.6811e-01, 2.1545e-04, 1.3167e-01]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.3232e-01, 8.3036e-05, 6.7594e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51180\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5253/10000 (52.5%)\n",
      "    Average reward: +0.349\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4747/10000 (47.5%)\n",
      "    Average reward: -0.349\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6055 (24.4%)\n",
      "    Action 1: 12085 (48.8%)\n",
      "    Action 2: 2648 (10.7%)\n",
      "    Action 3: 3989 (16.1%)\n",
      "  Player 1:\n",
      "    Action 0: 17119 (64.8%)\n",
      "    Action 1: 9284 (35.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3490.5, -3490.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.002 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.936 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.969\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3491\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.1195, -1.0763, -0.4771,  0.3599]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.4903e-01, 1.1281e-04, 5.0856e-02]])\n",
      "Player 0 Prediction: tensor([[ 0.0268, -0.2855, -1.3212,  0.0216]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7909, 0.0013, 0.2078]])\n",
      "Player 0 Prediction: tensor([[ 3.8529,  5.1594, -2.1385,  2.3976]])\n",
      "Player 1 Prediction: tensor([[0.7360, 0.0000, 0.2640, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51379\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6164/10000 (61.6%)\n",
      "    Average reward: +0.370\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3836/10000 (38.4%)\n",
      "    Average reward: -0.370\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10658 (41.9%)\n",
      "    Action 1: 9966 (39.2%)\n",
      "    Action 2: 1627 (6.4%)\n",
      "    Action 3: 3170 (12.5%)\n",
      "  Player 1:\n",
      "    Action 0: 6578 (25.3%)\n",
      "    Action 1: 12046 (46.4%)\n",
      "    Action 2: 2883 (11.1%)\n",
      "    Action 3: 4451 (17.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3698.0, -3698.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.036\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3698\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.7247e-01, 2.6685e-02, 8.4272e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.7341, 0.2639, 0.0020, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1210, 0.0361, 0.8429]])\n",
      "Player 1 Prediction: tensor([[0.1062, 0.0534, 0.8404, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51418\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6143/10000 (61.4%)\n",
      "    Average reward: -0.400\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3857/10000 (38.6%)\n",
      "    Average reward: +0.400\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18624 (69.7%)\n",
      "    Action 1: 8080 (30.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3925 (15.9%)\n",
      "    Action 1: 13671 (55.3%)\n",
      "    Action 2: 2675 (10.8%)\n",
      "    Action 3: 4443 (18.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4003.5, 4003.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.884 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.894 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.889\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4003\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35003/50000 [30:38<09:55, 25.18it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 221903/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 224677/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35999/50000 [31:20<09:55, 23.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 228329/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 231214/2000000\n",
      "P1 SL Buffer Size:  228329\n",
      "P1 SL buffer distribution [87048. 86143. 20099. 35039.]\n",
      "P1 actions distribution [0.38123935 0.37727577 0.08802649 0.15345839]\n",
      "P2 SL Buffer Size:  231214\n",
      "P2 SL buffer distribution [ 76986. 100411.  20927.  32890.]\n",
      "P2 actions distribution [0.33296427 0.43427734 0.09050923 0.14224917]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9797, 0.0191, 0.0012, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.7807,  1.9282, -1.3161,  1.1067]])\n",
      "Player 0 Prediction: tensor([[0.7763, 0.2207, 0.0031, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.7014,  1.5152, -2.1002,  2.0770]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0428, 0.0220, 0.9352]])\n",
      "Player 1 Prediction: tensor([[ 0.1385,  0.9153, -2.2198,  0.8606]])\n",
      "Player 0 Prediction: tensor([[0.0668, 0.0276, 0.9056, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35999/50000 [31:32<09:55, 23.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51097\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5645/10000 (56.5%)\n",
      "    Average reward: -0.246\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4355/10000 (43.5%)\n",
      "    Average reward: +0.246\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9230 (36.0%)\n",
      "    Action 1: 10236 (39.9%)\n",
      "    Action 2: 2514 (9.8%)\n",
      "    Action 3: 3668 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 7400 (29.1%)\n",
      "    Action 1: 12136 (47.7%)\n",
      "    Action 2: 2392 (9.4%)\n",
      "    Action 3: 3521 (13.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2465.0, 2465.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2465\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0879, 0.0214, 0.8907]])\n",
      "Player 0 Prediction: tensor([[0.0775, 0.5021, 0.4204, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51275\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5151/10000 (51.5%)\n",
      "    Average reward: +0.350\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4849/10000 (48.5%)\n",
      "    Average reward: -0.350\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6387 (25.6%)\n",
      "    Action 1: 11733 (47.1%)\n",
      "    Action 2: 2769 (11.1%)\n",
      "    Action 3: 4021 (16.1%)\n",
      "  Player 1:\n",
      "    Action 0: 16765 (63.6%)\n",
      "    Action 1: 9600 (36.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3498.0, -3498.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.015 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.946 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.981\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3498\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.8115e-01, 7.1844e-01, 4.1028e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.2007,  1.7511, -0.8739,  1.3482]])\n",
      "Player 1 Prediction: tensor([[0.3422, 0.6571, 0.0007, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.6293,  1.6806, -1.5198,  2.1531]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5241, 0.0017, 0.4742]])\n",
      "Player 0 Prediction: tensor([[ 4.0533,  5.4027, -2.8192,  4.1428]])\n",
      "Player 1 Prediction: tensor([[0.7179, 0.2561, 0.0260, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 7.1536,  7.0902, -4.5578,  4.2523]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51810\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6168/10000 (61.7%)\n",
      "    Average reward: +0.352\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3832/10000 (38.3%)\n",
      "    Average reward: -0.352\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11841 (45.6%)\n",
      "    Action 1: 8282 (31.9%)\n",
      "    Action 2: 1645 (6.3%)\n",
      "    Action 3: 4202 (16.2%)\n",
      "  Player 1:\n",
      "    Action 0: 6256 (24.2%)\n",
      "    Action 1: 11725 (45.4%)\n",
      "    Action 2: 2264 (8.8%)\n",
      "    Action 3: 5595 (21.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3520.0, -3520.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.013 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3520\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.8115e-01, 7.1844e-01, 4.1028e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6067, 0.0039, 0.3895]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51511\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6172/10000 (61.7%)\n",
      "    Average reward: -0.428\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3828/10000 (38.3%)\n",
      "    Average reward: +0.428\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18780 (70.0%)\n",
      "    Action 1: 8058 (30.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3859 (15.6%)\n",
      "    Action 1: 13654 (55.3%)\n",
      "    Action 2: 2685 (10.9%)\n",
      "    Action 3: 4475 (18.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4280.0, 4280.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.882 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.891 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.886\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4280\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37003/50000 [32:33<09:01, 24.02it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 234601/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 237496/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37999/50000 [33:16<08:30, 23.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 240975/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 243925/2000000\n",
      "P1 SL Buffer Size:  240975\n",
      "P1 SL buffer distribution [92457. 90466. 21000. 37052.]\n",
      "P1 actions distribution [0.3836788  0.37541654 0.08714597 0.15375869]\n",
      "P2 SL Buffer Size:  243925\n",
      "P2 SL buffer distribution [ 80954. 105462.  22160.  35349.]\n",
      "P2 actions distribution [0.3318807  0.43235421 0.0908476  0.1449175 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9808, 0.0182, 0.0010, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.2786,  1.1739, -0.9079,  0.8461]])\n",
      "Player 0 Prediction: tensor([[0.7889, 0.2081, 0.0031, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.6794,  2.1547, -2.2265,  1.6711]])\n",
      "Player 0 Prediction: tensor([[0.0155, 0.0360, 0.9484, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37999/50000 [33:32<08:30, 23.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51531\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5756/10000 (57.6%)\n",
      "    Average reward: -0.180\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4244/10000 (42.4%)\n",
      "    Average reward: +0.180\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9014 (35.0%)\n",
      "    Action 1: 9809 (38.1%)\n",
      "    Action 2: 2094 (8.1%)\n",
      "    Action 3: 4802 (18.7%)\n",
      "  Player 1:\n",
      "    Action 0: 8011 (31.0%)\n",
      "    Action 1: 10645 (41.2%)\n",
      "    Action 2: 2456 (9.5%)\n",
      "    Action 3: 4700 (18.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1802.5, 1802.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.056\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1802\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9808, 0.0182, 0.0010, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.7889, 0.2081, 0.0031, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0381, 0.0205, 0.9414]])\n",
      "Player 0 Prediction: tensor([[0.0621, 0.0271, 0.9109, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51249\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5248/10000 (52.5%)\n",
      "    Average reward: +0.440\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4752/10000 (47.5%)\n",
      "    Average reward: -0.440\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6302 (25.3%)\n",
      "    Action 1: 11770 (47.2%)\n",
      "    Action 2: 2847 (11.4%)\n",
      "    Action 3: 4012 (16.1%)\n",
      "  Player 1:\n",
      "    Action 0: 16785 (63.8%)\n",
      "    Action 1: 9533 (36.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4396.0, -4396.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.013 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.945 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.979\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4396\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.5926,  0.1523, -0.8044,  0.3418]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7657, 0.0020, 0.2323]])\n",
      "Player 0 Prediction: tensor([[ 0.0239,  0.1459, -1.0547,  0.7914]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2719, 0.0010, 0.7270]])\n",
      "Player 0 Prediction: tensor([[-0.6811, -1.2282, -1.2360, -0.0016]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50388\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6192/10000 (61.9%)\n",
      "    Average reward: +0.320\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3808/10000 (38.1%)\n",
      "    Average reward: -0.320\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11238 (44.4%)\n",
      "    Action 1: 7110 (28.1%)\n",
      "    Action 2: 1612 (6.4%)\n",
      "    Action 3: 5373 (21.2%)\n",
      "  Player 1:\n",
      "    Action 0: 5693 (22.7%)\n",
      "    Action 1: 11050 (44.1%)\n",
      "    Action 2: 2278 (9.1%)\n",
      "    Action 3: 6034 (24.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3203.0, -3203.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3203\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.2332e-01, 8.7667e-01, 1.2109e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 5.4763e-01, 3.1043e-04, 4.5206e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51428\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6067/10000 (60.7%)\n",
      "    Average reward: -0.535\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3933/10000 (39.3%)\n",
      "    Average reward: +0.535\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18602 (69.6%)\n",
      "    Action 1: 8138 (30.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4098 (16.6%)\n",
      "    Action 1: 13542 (54.9%)\n",
      "    Action 2: 2671 (10.8%)\n",
      "    Action 3: 4377 (17.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5352.0, 5352.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.887 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.905 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.896\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5352\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39005/50000 [34:29<07:33, 24.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 247494/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 249934/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39999/50000 [35:12<09:16, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 253907/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 256386/2000000\n",
      "P1 SL Buffer Size:  253907\n",
      "P1 SL buffer distribution [98065. 94806. 21900. 39136.]\n",
      "P1 actions distribution [0.38622409 0.37338868 0.08625205 0.15413518]\n",
      "P2 SL Buffer Size:  256386\n",
      "P2 SL buffer distribution [ 84829. 110420.  23440.  37697.]\n",
      "P2 actions distribution [0.3308644  0.43067874 0.09142465 0.14703221]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.9583,  2.2157, -0.7832,  2.1076]])\n",
      "Player 0 Prediction: tensor([[0.7990, 0.1979, 0.0031, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.2314,  1.4309, -1.9316,  1.3015]])\n",
      "Player 0 Prediction: tensor([[0.0077, 0.0296, 0.9627, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50499\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5736/10000 (57.4%)\n",
      "    Average reward: -0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4264/10000 (42.6%)\n",
      "    Average reward: +0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8198 (32.4%)\n",
      "    Action 1: 10078 (39.8%)\n",
      "    Action 2: 2161 (8.5%)\n",
      "    Action 3: 4857 (19.2%)\n",
      "  Player 1:\n",
      "    Action 0: 8860 (35.2%)\n",
      "    Action 1: 9440 (37.5%)\n",
      "    Action 2: 2640 (10.5%)\n",
      "    Action 3: 4265 (16.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2777.5, 2777.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.058\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2777\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[5.0530e-01, 4.9469e-01, 9.2279e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.4025e-01, 5.9430e-04, 2.5915e-01]])\n",
      "Player 0 Prediction: tensor([[0.0213, 0.9598, 0.0188, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39999/50000 [35:32<09:16, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51287\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5256/10000 (52.6%)\n",
      "    Average reward: +0.451\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4744/10000 (47.4%)\n",
      "    Average reward: -0.451\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6607 (26.4%)\n",
      "    Action 1: 11688 (46.7%)\n",
      "    Action 2: 2839 (11.4%)\n",
      "    Action 3: 3877 (15.5%)\n",
      "  Player 1:\n",
      "    Action 0: 16626 (63.3%)\n",
      "    Action 1: 9650 (36.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4514.0, -4514.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.949 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.984\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4514\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.8097e-01, 1.8482e-02, 5.5007e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.8214,  1.5866, -0.8362,  1.1664]])\n",
      "Player 1 Prediction: tensor([[0.7672, 0.2315, 0.0013, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.2892,  1.4445, -1.6451,  1.8776]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1029, 0.0278, 0.8693]])\n",
      "Player 0 Prediction: tensor([[-0.5541, -0.5308, -1.9358, -0.2190]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53601\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6319/10000 (63.2%)\n",
      "    Average reward: +0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3681/10000 (36.8%)\n",
      "    Average reward: -0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10547 (40.2%)\n",
      "    Action 1: 9210 (35.1%)\n",
      "    Action 2: 1583 (6.0%)\n",
      "    Action 3: 4926 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7368 (27.0%)\n",
      "    Action 1: 11924 (43.6%)\n",
      "    Action 2: 2581 (9.4%)\n",
      "    Action 3: 5462 (20.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3731.0, -3731.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.045\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3731\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.6382e-01, 6.9127e-05, 3.6114e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 5.6566e-01, 2.4321e-04, 4.3410e-01]])\n",
      "Player 1 Prediction: tensor([[0.1311, 0.8477, 0.0212, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51388\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6115/10000 (61.2%)\n",
      "    Average reward: -0.492\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3885/10000 (38.9%)\n",
      "    Average reward: +0.492\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18469 (69.3%)\n",
      "    Action 1: 8183 (30.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4207 (17.0%)\n",
      "    Action 1: 13520 (54.7%)\n",
      "    Action 2: 2743 (11.1%)\n",
      "    Action 3: 4266 (17.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4924.0, 4924.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.890 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.911 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.900\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4924\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}, {'exploitability': 0.32542499999999996}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41003/50000 [36:25<06:17, 23.86it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 260443/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 262703/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [37:09<06:36, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 266744/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 269172/2000000\n",
      "P1 SL Buffer Size:  266744\n",
      "P1 SL buffer distribution [103438.  99444.  22779.  41083.]\n",
      "P1 actions distribution [0.38778004 0.37280689 0.08539649 0.15401659]\n",
      "P2 SL Buffer Size:  269172\n",
      "P2 SL buffer distribution [ 89307. 115126.  24831.  39908.]\n",
      "P2 actions distribution [0.33178414 0.42770422 0.09224957 0.14826208]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.8396e-01, 1.5276e-02, 7.6693e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.1039, -0.2668, -1.2337,  0.5500]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0157, 0.0102, 0.9741]])\n",
      "Player 1 Prediction: tensor([[-1.6651, -1.9627, -0.9541, -0.4802]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [37:22<06:36, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51527\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5711/10000 (57.1%)\n",
      "    Average reward: -0.231\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4289/10000 (42.9%)\n",
      "    Average reward: +0.231\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9168 (35.6%)\n",
      "    Action 1: 9649 (37.4%)\n",
      "    Action 2: 2097 (8.1%)\n",
      "    Action 3: 4873 (18.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7982 (31.0%)\n",
      "    Action 1: 10696 (41.6%)\n",
      "    Action 2: 2254 (8.8%)\n",
      "    Action 3: 4808 (18.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2314.0, 2314.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.056\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2314\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[4.9217e-01, 5.0782e-01, 7.9900e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[4.0082e-02, 9.5991e-01, 1.0596e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.2543e-01, 3.2751e-05, 7.4540e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51555\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5287/10000 (52.9%)\n",
      "    Average reward: +0.448\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4713/10000 (47.1%)\n",
      "    Average reward: -0.448\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6557 (26.2%)\n",
      "    Action 1: 11753 (46.9%)\n",
      "    Action 2: 2797 (11.2%)\n",
      "    Action 3: 3947 (15.8%)\n",
      "  Player 1:\n",
      "    Action 0: 16800 (63.4%)\n",
      "    Action 1: 9701 (36.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4476.5, -4476.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.018 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.948 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.983\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4476\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.1595, -1.5782, -0.6949, -0.0541]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7814, 0.0018, 0.2168]])\n",
      "Player 0 Prediction: tensor([[-0.2520, -1.2550, -1.0257, -0.1524]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6867, 0.0025, 0.3108]])\n",
      "Player 0 Prediction: tensor([[-3.2226, -3.4980, -2.0317, -2.2800]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51020\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6220/10000 (62.2%)\n",
      "    Average reward: +0.411\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3780/10000 (37.8%)\n",
      "    Average reward: -0.411\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10465 (41.4%)\n",
      "    Action 1: 9254 (36.6%)\n",
      "    Action 2: 2206 (8.7%)\n",
      "    Action 3: 3382 (13.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6570 (25.6%)\n",
      "    Action 1: 11660 (45.3%)\n",
      "    Action 2: 2888 (11.2%)\n",
      "    Action 3: 4595 (17.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4109.5, -4109.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4109\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.8200e-01, 1.7490e-02, 5.0645e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.7794, 0.2196, 0.0010, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0207, 0.9237, 0.0556, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51307\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6103/10000 (61.0%)\n",
      "    Average reward: -0.503\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3897/10000 (39.0%)\n",
      "    Average reward: +0.503\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18536 (69.7%)\n",
      "    Action 1: 8067 (30.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4224 (17.1%)\n",
      "    Action 1: 13613 (55.1%)\n",
      "    Action 2: 2716 (11.0%)\n",
      "    Action 3: 4151 (16.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5027.5, 5027.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.909 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.897\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5028\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}, {'exploitability': 0.32542499999999996}, {'exploitability': 0.321175}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43005/50000 [38:22<04:54, 23.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 273016/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 275744/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43998/50000 [39:06<04:24, 22.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 279365/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 282273/2000000\n",
      "P1 SL Buffer Size:  279365\n",
      "P1 SL buffer distribution [108553. 104155.  23682.  42975.]\n",
      "P1 actions distribution [0.38857051 0.37282766 0.08477082 0.15383101]\n",
      "P2 SL Buffer Size:  282273\n",
      "P2 SL buffer distribution [ 93472. 120385.  26257.  42159.]\n",
      "P2 actions distribution [0.33114042 0.42648429 0.09301988 0.14935541]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.8479e-01, 1.4486e-02, 7.2250e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.1781,  2.1392, -1.3146,  1.8375]])\n",
      "Player 0 Prediction: tensor([[0.9608, 0.0000, 0.0392, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 3.1533,  5.4121, -3.3887,  3.6832]])\n",
      "Player 0 Prediction: tensor([[0.0716, 0.0371, 0.8913, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43998/50000 [39:22<04:24, 22.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51370\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5691/10000 (56.9%)\n",
      "    Average reward: -0.233\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4309/10000 (43.1%)\n",
      "    Average reward: +0.233\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9186 (35.7%)\n",
      "    Action 1: 9536 (37.1%)\n",
      "    Action 2: 2127 (8.3%)\n",
      "    Action 3: 4868 (18.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7813 (30.5%)\n",
      "    Action 1: 10738 (41.9%)\n",
      "    Action 2: 2365 (9.2%)\n",
      "    Action 3: 4737 (18.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2332.5, 2332.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2333\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5105, 0.0039, 0.4856]])\n",
      "Player 0 Prediction: tensor([[0.4377, 0.5078, 0.0544, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51421\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5217/10000 (52.2%)\n",
      "    Average reward: +0.445\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4783/10000 (47.8%)\n",
      "    Average reward: -0.445\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6505 (26.0%)\n",
      "    Action 1: 11640 (46.6%)\n",
      "    Action 2: 2878 (11.5%)\n",
      "    Action 3: 3976 (15.9%)\n",
      "  Player 1:\n",
      "    Action 0: 16726 (63.3%)\n",
      "    Action 1: 9696 (36.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4449.5, -4449.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.948 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.984\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4450\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.0292, -0.9742, -0.7103,  0.1782]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.7008e-01, 5.9315e-05, 2.9860e-02]])\n",
      "Player 0 Prediction: tensor([[-0.3339, -0.7414, -1.3235, -0.1287]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 3.2698e-01, 2.0848e-04, 6.7281e-01]])\n",
      "Player 0 Prediction: tensor([[-2.4019, -2.5727, -1.8092, -1.7095]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51402\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6394/10000 (63.9%)\n",
      "    Average reward: +0.511\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3606/10000 (36.1%)\n",
      "    Average reward: -0.511\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10109 (39.9%)\n",
      "    Action 1: 8858 (34.9%)\n",
      "    Action 2: 1269 (5.0%)\n",
      "    Action 3: 5115 (20.2%)\n",
      "  Player 1:\n",
      "    Action 0: 7080 (27.2%)\n",
      "    Action 1: 11521 (44.2%)\n",
      "    Action 2: 2268 (8.7%)\n",
      "    Action 3: 5182 (19.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5111.5, -5111.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.045\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5111\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7897, 0.0018, 0.2085]])\n",
      "Player 1 Prediction: tensor([[0.0170, 0.5916, 0.3914, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51057\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6127/10000 (61.3%)\n",
      "    Average reward: -0.491\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3873/10000 (38.7%)\n",
      "    Average reward: +0.491\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18373 (69.4%)\n",
      "    Action 1: 8089 (30.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4283 (17.4%)\n",
      "    Action 1: 13345 (54.3%)\n",
      "    Action 2: 2780 (11.3%)\n",
      "    Action 3: 4187 (17.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4908.0, 4908.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.888 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.918 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.903\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4908\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}, {'exploitability': 0.32542499999999996}, {'exploitability': 0.321175}, {'exploitability': 0.3722}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45003/50000 [40:19<03:51, 21.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 285805/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 288474/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45998/50000 [41:06<03:06, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 292224/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 294752/2000000\n",
      "P1 SL Buffer Size:  292224\n",
      "P1 SL buffer distribution [113856. 108543.  24551.  45274.]\n",
      "P1 actions distribution [0.38961892 0.37143766 0.08401432 0.1549291 ]\n",
      "P2 SL Buffer Size:  294752\n",
      "P2 SL buffer distribution [ 97651. 124872.  27595.  44634.]\n",
      "P2 actions distribution [0.33129885 0.42365107 0.09362108 0.151429  ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.3065, -0.8368, -1.0779,  0.3165]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5272, 0.0037, 0.4691]])\n",
      "Player 1 Prediction: tensor([[-1.1467, -1.8374, -0.8460, -0.5767]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0933, 0.0010, 0.9058]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45998/50000 [41:22<03:06, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50725\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5907/10000 (59.1%)\n",
      "    Average reward: -0.228\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4093/10000 (40.9%)\n",
      "    Average reward: +0.228\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8035 (31.8%)\n",
      "    Action 1: 10183 (40.3%)\n",
      "    Action 2: 2092 (8.3%)\n",
      "    Action 3: 4957 (19.6%)\n",
      "  Player 1:\n",
      "    Action 0: 9199 (36.1%)\n",
      "    Action 1: 9543 (37.5%)\n",
      "    Action 2: 2760 (10.8%)\n",
      "    Action 3: 3956 (15.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2279.0, 2279.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.058\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2279\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[4.8026e-01, 5.1973e-01, 6.7397e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.1671e-01, 3.7909e-05, 8.3256e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51836\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5292/10000 (52.9%)\n",
      "    Average reward: +0.486\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4708/10000 (47.1%)\n",
      "    Average reward: -0.486\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6781 (26.9%)\n",
      "    Action 1: 11651 (46.1%)\n",
      "    Action 2: 2826 (11.2%)\n",
      "    Action 3: 3992 (15.8%)\n",
      "  Player 1:\n",
      "    Action 0: 16640 (62.6%)\n",
      "    Action 1: 9946 (37.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4856.5, -4856.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.954 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.989\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4857\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.2203e-01, 8.7796e-01, 9.2886e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.9609,  1.4970, -1.5155,  0.5262]])\n",
      "Player 1 Prediction: tensor([[9.9993e-01, 0.0000e+00, 7.1348e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.3935, -0.5046, -1.9587, -0.4350]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.8829e-01, 1.2185e-04, 3.1159e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50044\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6248/10000 (62.5%)\n",
      "    Average reward: +0.273\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3752/10000 (37.5%)\n",
      "    Average reward: -0.273\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11302 (44.9%)\n",
      "    Action 1: 6996 (27.8%)\n",
      "    Action 2: 1727 (6.9%)\n",
      "    Action 3: 5155 (20.5%)\n",
      "  Player 1:\n",
      "    Action 0: 5935 (23.9%)\n",
      "    Action 1: 10788 (43.4%)\n",
      "    Action 2: 2261 (9.1%)\n",
      "    Action 3: 5880 (23.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2733.5, -2733.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2733\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7791, 0.0018, 0.2191]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8283, 0.0255, 0.1462]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51069\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6108/10000 (61.1%)\n",
      "    Average reward: -0.532\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3892/10000 (38.9%)\n",
      "    Average reward: +0.532\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18229 (68.8%)\n",
      "    Action 1: 8261 (31.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4407 (17.9%)\n",
      "    Action 1: 13171 (53.6%)\n",
      "    Action 2: 2802 (11.4%)\n",
      "    Action 3: 4199 (17.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5323.5, 5323.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.895 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.927 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.911\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5323\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}, {'exploitability': 0.32542499999999996}, {'exploitability': 0.321175}, {'exploitability': 0.3722}, {'exploitability': 0.250625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47003/50000 [42:26<02:33, 19.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 298467/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 300867/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [43:10<01:28, 22.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 305012/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 307409/2000000\n",
      "P1 SL Buffer Size:  305012\n",
      "P1 SL buffer distribution [119355. 112750.  25428.  47479.]\n",
      "P1 actions distribution [0.39131247 0.36965759 0.08336721 0.15566273]\n",
      "P2 SL Buffer Size:  307409\n",
      "P2 SL buffer distribution [101980. 129554.  28875.  47000.]\n",
      "P2 actions distribution [0.33174045 0.42143854 0.09393024 0.15289077]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.1714, -0.7160, -1.1162,  0.4215]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.2120e-01, 7.4000e-05, 7.8722e-02]])\n",
      "Player 1 Prediction: tensor([[ 0.0461, -0.2541, -1.3720,  0.3866]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4799, 0.0006, 0.5195]])\n",
      "Player 1 Prediction: tensor([[ 4.0380,  5.2459, -1.7208,  2.8642]])\n",
      "Player 0 Prediction: tensor([[0.7667, 0.0000, 0.2333, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [43:23<01:28, 22.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51123\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5709/10000 (57.1%)\n",
      "    Average reward: -0.299\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4291/10000 (42.9%)\n",
      "    Average reward: +0.299\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9475 (37.0%)\n",
      "    Action 1: 9961 (38.9%)\n",
      "    Action 2: 2278 (8.9%)\n",
      "    Action 3: 3890 (15.2%)\n",
      "  Player 1:\n",
      "    Action 0: 6744 (26.4%)\n",
      "    Action 1: 11575 (45.4%)\n",
      "    Action 2: 2594 (10.2%)\n",
      "    Action 3: 4606 (18.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2989.5, 2989.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.043\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2989\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[4.9497e-01, 5.0502e-01, 6.5032e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[3.3797e-02, 9.6619e-01, 1.1827e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 2.8015e-01, 9.7437e-05, 7.1975e-01]])\n",
      "Player 0 Prediction: tensor([[0.8577, 0.1403, 0.0021, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52013\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5169/10000 (51.7%)\n",
      "    Average reward: +0.431\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4831/10000 (48.3%)\n",
      "    Average reward: -0.431\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6772 (26.7%)\n",
      "    Action 1: 11603 (45.8%)\n",
      "    Action 2: 2867 (11.3%)\n",
      "    Action 3: 4091 (16.1%)\n",
      "  Player 1:\n",
      "    Action 0: 16638 (62.4%)\n",
      "    Action 1: 10042 (37.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4311.0, -4311.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.955 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.990\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4311\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.8431e-01, 1.5230e-02, 4.5647e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.2946, -0.8965, -1.0089,  0.0824]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0092, 0.0078, 0.9830]])\n",
      "Player 0 Prediction: tensor([[-0.9940, -1.0817, -1.1376, -0.3157]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52027\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6193/10000 (61.9%)\n",
      "    Average reward: +0.329\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3807/10000 (38.1%)\n",
      "    Average reward: -0.329\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10635 (41.5%)\n",
      "    Action 1: 8484 (33.1%)\n",
      "    Action 2: 1629 (6.4%)\n",
      "    Action 3: 4902 (19.1%)\n",
      "  Player 1:\n",
      "    Action 0: 7072 (26.8%)\n",
      "    Action 1: 11134 (42.2%)\n",
      "    Action 2: 2474 (9.4%)\n",
      "    Action 3: 5697 (21.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3286.0, -3286.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3286\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[4.0182e-01, 5.9771e-01, 4.6508e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7084, 0.0023, 0.2893]])\n",
      "Player 1 Prediction: tensor([[0.1245, 0.7796, 0.0959, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51082\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6089/10000 (60.9%)\n",
      "    Average reward: -0.545\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3911/10000 (39.1%)\n",
      "    Average reward: +0.545\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18123 (68.7%)\n",
      "    Action 1: 8272 (31.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4603 (18.6%)\n",
      "    Action 1: 13181 (53.4%)\n",
      "    Action 2: 2830 (11.5%)\n",
      "    Action 3: 4073 (16.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5454.0, 5454.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.897 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.935 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.916\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5454\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}, {'exploitability': 0.32542499999999996}, {'exploitability': 0.321175}, {'exploitability': 0.3722}, {'exploitability': 0.250625}, {'exploitability': 0.313775}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49005/50000 [44:25<00:43, 22.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 311551/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 314036/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [45:11<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.7641,  1.8622, -0.9892,  1.7276]])\n",
      "Player 0 Prediction: tensor([[0.8300, 0.1674, 0.0026, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.5820,  3.0082, -2.0954,  3.3029]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1286, 0.0405, 0.8310]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50192\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5833/10000 (58.3%)\n",
      "    Average reward: -0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4167/10000 (41.7%)\n",
      "    Average reward: +0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7681 (31.0%)\n",
      "    Action 1: 9954 (40.1%)\n",
      "    Action 2: 2123 (8.6%)\n",
      "    Action 3: 5050 (20.4%)\n",
      "  Player 1:\n",
      "    Action 0: 9769 (38.5%)\n",
      "    Action 1: 8898 (35.1%)\n",
      "    Action 2: 2187 (8.6%)\n",
      "    Action 3: 4530 (17.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2881.0, 2881.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.056\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2881\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[4.8781e-01, 5.1218e-01, 5.9749e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.9349e-01, 4.9516e-05, 1.0646e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52186\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5206/10000 (52.1%)\n",
      "    Average reward: +0.441\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4794/10000 (47.9%)\n",
      "    Average reward: -0.441\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6807 (26.8%)\n",
      "    Action 1: 11640 (45.9%)\n",
      "    Action 2: 2790 (11.0%)\n",
      "    Action 3: 4145 (16.3%)\n",
      "  Player 1:\n",
      "    Action 0: 16711 (62.3%)\n",
      "    Action 1: 10093 (37.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4406.5, -4406.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.956 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.990\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4406\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.2086e-01, 8.7913e-01, 1.0119e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.2362, -0.5563, -1.3139, -0.0237]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.4133e-01, 2.5425e-04, 1.5842e-01]])\n",
      "Player 0 Prediction: tensor([[ 3.8332,  4.4878, -2.0403,  2.3189]])\n",
      "Player 1 Prediction: tensor([[0.6962, 0.0000, 0.3038, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51993\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6152/10000 (61.5%)\n",
      "    Average reward: +0.268\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3848/10000 (38.5%)\n",
      "    Average reward: -0.268\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11062 (43.0%)\n",
      "    Action 1: 8506 (33.0%)\n",
      "    Action 2: 1638 (6.4%)\n",
      "    Action 3: 4544 (17.6%)\n",
      "  Player 1:\n",
      "    Action 0: 7055 (26.9%)\n",
      "    Action 1: 11331 (43.2%)\n",
      "    Action 2: 2323 (8.9%)\n",
      "    Action 3: 5534 (21.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2681.5, -2681.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.033 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2681\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7923, 0.0020, 0.2057]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8260, 0.0220, 0.1520]])\n",
      "Player 1 Prediction: tensor([[0.0074, 0.1253, 0.8673, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50988\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6119/10000 (61.2%)\n",
      "    Average reward: -0.517\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3881/10000 (38.8%)\n",
      "    Average reward: +0.517\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18002 (68.4%)\n",
      "    Action 1: 8321 (31.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4708 (19.1%)\n",
      "    Action 1: 13049 (52.9%)\n",
      "    Action 2: 2884 (11.7%)\n",
      "    Action 3: 4024 (16.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5168.5, 5168.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.900 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.942 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.921\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.5169\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.134825}, {'exploitability': 1.0445}, {'exploitability': 1.029}, {'exploitability': 0.8865000000000001}, {'exploitability': 0.765425}, {'exploitability': 0.5977250000000001}, {'exploitability': 0.669375}, {'exploitability': 0.56305}, {'exploitability': 0.571725}, {'exploitability': 0.501825}, {'exploitability': 0.47362499999999996}, {'exploitability': 0.433725}, {'exploitability': 0.498075}, {'exploitability': 0.39849999999999997}, {'exploitability': 0.303125}, {'exploitability': 0.32985}, {'exploitability': 0.3543}, {'exploitability': 0.29925}, {'exploitability': 0.25027499999999997}, {'exploitability': 0.32542499999999996}, {'exploitability': 0.321175}, {'exploitability': 0.3722}, {'exploitability': 0.250625}, {'exploitability': 0.313775}, {'exploitability': 0.278125}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aee5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x36c05d0f0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x36c05d0f0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 51\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x36c05d0f0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 51\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x36f9a7430>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x36f9a7430>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2731ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-Categorical\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659e3cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/50000 [00:00<17:32, 47.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 63/200000\n",
      "   Player 0 SL buffer: 8/2000000\n",
      "   Player 1 RL buffer: 65/200000\n",
      "   Player 1 SL buffer: 2/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1007/50000 [00:29<25:22, 32.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0019 â†’ 0.0019\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 62961/200000\n",
      "   Player 0 SL buffer: 7194/2000000\n",
      "   Player 1 RL buffer: 65166/200000\n",
      "   Player 1 SL buffer: 7573/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:05<30:31, 26.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0013 â†’ 0.0013\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 126444/200000\n",
      "   Player 0 SL buffer: 13800/2000000\n",
      "   Player 1 RL buffer: 129684/200000\n",
      "   Player 1 SL buffer: 14576/2000000\n",
      "P1 SL Buffer Size:  13800\n",
      "P1 SL buffer distribution [4937. 7710.   17. 1136.]\n",
      "P1 actions distribution [0.35775362 0.55869565 0.00123188 0.08231884]\n",
      "P2 SL Buffer Size:  14576\n",
      "P2 SL buffer distribution [5751. 7201.   96. 1528.]\n",
      "P2 actions distribution [0.39455269 0.49403128 0.00658617 0.10482986]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[1.0595e-03, 2.2003e-03, 2.6557e-03, 2.7594e-03, 3.7055e-03,\n",
      "          3.1820e-03, 2.8264e-03, 8.3665e-03, 1.1789e-02, 4.4661e-03,\n",
      "          2.9042e-02, 1.3249e-02, 6.9857e-03, 9.1324e-03, 3.9682e-03,\n",
      "          1.7467e-02, 7.2669e-03, 3.2454e-02, 4.2645e-02, 5.7101e-03,\n",
      "          2.2708e-02, 3.7740e-03, 4.1712e-03, 5.3284e-03, 2.6989e-03,\n",
      "          1.0835e-01, 2.4907e-03, 4.5428e-02, 4.4824e-02, 1.3454e-02,\n",
      "          1.3599e-01, 1.5584e-02, 1.1747e-01, 9.5535e-02, 1.3048e-02,\n",
      "          3.1608e-02, 5.1461e-03, 1.1711e-02, 9.4585e-03, 1.7582e-02,\n",
      "          3.9713e-02, 5.2468e-03, 1.1379e-02, 8.9044e-03, 2.3378e-03,\n",
      "          2.8813e-03, 2.9855e-03, 2.8688e-03, 2.8219e-03, 2.2307e-03,\n",
      "          1.3495e-03],\n",
      "         [1.2787e-03, 1.9563e-03, 2.5785e-03, 2.5674e-03, 3.8141e-03,\n",
      "          3.2389e-03, 2.9258e-03, 7.9455e-03, 9.6744e-03, 5.4983e-03,\n",
      "          3.0317e-02, 9.4270e-03, 8.5116e-03, 9.9259e-03, 4.3193e-03,\n",
      "          3.9890e-02, 7.4244e-03, 3.3811e-02, 4.4096e-02, 7.0108e-03,\n",
      "          1.5982e-02, 3.4409e-03, 4.5658e-03, 4.0596e-03, 2.9062e-03,\n",
      "          1.1463e-01, 2.5793e-03, 4.9813e-02, 4.9991e-02, 5.6101e-03,\n",
      "          2.2934e-02, 1.8056e-02, 1.6184e-01, 1.3278e-01, 1.0659e-02,\n",
      "          6.0692e-02, 4.6260e-03, 1.0624e-02, 8.3528e-03, 9.2969e-03,\n",
      "          2.8908e-02, 6.8019e-03, 9.0828e-03, 7.1423e-03, 2.6774e-03,\n",
      "          2.7366e-03, 2.9859e-03, 2.8295e-03, 2.9900e-03, 2.4135e-03,\n",
      "          1.7853e-03],\n",
      "         [1.0982e-05, 1.1444e-05, 9.2314e-06, 1.5397e-05, 1.0117e-05,\n",
      "          1.4688e-05, 1.0979e-05, 1.8047e-05, 1.4547e-05, 1.0222e-05,\n",
      "          5.8289e-06, 1.0419e-05, 8.1330e-04, 7.2345e-04, 1.2416e-05,\n",
      "          1.5502e-03, 1.1504e-05, 4.7380e-04, 5.2975e-04, 1.2455e-05,\n",
      "          5.9685e-04, 1.2258e-05, 9.6488e-03, 2.3780e-01, 7.4735e-01,\n",
      "          1.7855e-05, 1.4032e-05, 1.6763e-05, 1.1876e-05, 1.7829e-05,\n",
      "          1.1322e-05, 1.5453e-05, 1.1423e-05, 7.6740e-06, 1.1400e-05,\n",
      "          9.2660e-06, 1.4641e-05, 1.3155e-05, 1.0987e-05, 1.1816e-05,\n",
      "          1.1420e-05, 1.0789e-05, 1.3832e-05, 1.2342e-05, 9.6070e-06,\n",
      "          1.4061e-05, 1.2732e-05, 1.2030e-05, 9.0478e-06, 1.0739e-05,\n",
      "          1.1160e-05],\n",
      "         [2.4620e-03, 5.0583e-03, 5.1840e-03, 4.1447e-03, 6.8939e-03,\n",
      "          4.4634e-03, 4.7867e-03, 9.0343e-03, 9.7474e-03, 5.5979e-03,\n",
      "          1.9675e-02, 7.2092e-03, 1.1471e-02, 1.2801e-02, 5.2978e-03,\n",
      "          1.5946e-02, 7.0508e-03, 2.5449e-02, 4.1761e-02, 6.8212e-03,\n",
      "          1.6130e-02, 5.0338e-03, 2.6174e-02, 2.2237e-02, 4.4571e-03,\n",
      "          1.6627e-01, 4.8314e-03, 7.9786e-02, 7.8924e-02, 6.9655e-03,\n",
      "          4.1413e-02, 1.6790e-02, 6.9763e-02, 8.4024e-02, 7.3664e-03,\n",
      "          2.5368e-02, 5.4574e-03, 2.0413e-02, 1.4229e-02, 9.0136e-03,\n",
      "          1.9187e-02, 8.6253e-03, 1.0909e-02, 9.0291e-03, 4.5501e-03,\n",
      "          5.3198e-03, 4.7481e-03, 7.1530e-03, 5.2387e-03, 5.9916e-03,\n",
      "          3.7497e-03]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9315, 0.0139, 0.0546]])\n",
      "Player 1 Prediction: tensor([[[5.7887e-04, 1.4158e-03, 1.4461e-03, 1.3967e-03, 1.9277e-03,\n",
      "          1.3605e-03, 1.4804e-03, 5.7830e-03, 7.3124e-03, 2.1093e-03,\n",
      "          1.1397e-01, 1.9367e-02, 4.8094e-03, 5.4161e-03, 2.1266e-03,\n",
      "          2.7678e-02, 4.6909e-03, 1.5960e-02, 1.9535e-02, 2.7639e-03,\n",
      "          6.3599e-02, 2.3454e-03, 2.2959e-03, 2.8763e-03, 1.5557e-03,\n",
      "          1.5757e-01, 1.3052e-03, 3.0883e-03, 2.7930e-03, 4.8109e-03,\n",
      "          1.8427e-01, 6.3840e-03, 2.4251e-02, 2.0771e-02, 1.2046e-02,\n",
      "          5.9503e-02, 2.9656e-03, 6.8808e-03, 5.0840e-03, 2.6573e-02,\n",
      "          1.3877e-01, 3.4935e-03, 8.2672e-03, 6.4264e-03, 1.3130e-03,\n",
      "          1.4688e-03, 2.0599e-03, 1.7584e-03, 1.8211e-03, 1.5716e-03,\n",
      "          9.5409e-04],\n",
      "         [1.1758e-03, 2.0350e-03, 2.8683e-03, 3.1733e-03, 4.8044e-03,\n",
      "          3.5333e-03, 3.4074e-03, 1.0343e-02, 1.4058e-02, 4.9578e-03,\n",
      "          7.1271e-03, 6.1327e-03, 1.0935e-02, 1.3476e-02, 4.3478e-03,\n",
      "          1.3894e-02, 5.1833e-03, 5.3818e-02, 6.2829e-02, 6.6929e-03,\n",
      "          1.5955e-02, 3.4782e-03, 4.9401e-03, 6.5011e-03, 2.5568e-03,\n",
      "          1.0196e-01, 2.8581e-03, 1.0588e-02, 1.1229e-02, 5.1927e-03,\n",
      "          1.3569e-01, 1.6080e-02, 1.7131e-01, 1.4301e-01, 6.1516e-03,\n",
      "          1.6510e-02, 4.8356e-03, 2.2573e-02, 1.8833e-02, 7.8532e-03,\n",
      "          7.2220e-03, 7.1530e-03, 1.3082e-02, 1.1047e-02, 2.2508e-03,\n",
      "          2.7119e-03, 3.5679e-03, 3.0801e-03, 3.2928e-03, 2.3501e-03,\n",
      "          1.3507e-03],\n",
      "         [1.2953e-05, 7.9599e-06, 1.0479e-05, 1.3667e-05, 1.1182e-05,\n",
      "          1.7897e-05, 1.2215e-05, 1.1789e-05, 9.2423e-06, 1.2943e-05,\n",
      "          1.1192e-05, 7.9999e-06, 7.3995e-04, 7.7639e-04, 7.0051e-06,\n",
      "          2.5011e-03, 9.7365e-06, 4.0312e-04, 4.0463e-04, 8.7514e-06,\n",
      "          9.3133e-04, 8.8328e-06, 4.9574e-01, 4.9777e-01, 2.4331e-04,\n",
      "          2.4934e-05, 1.1868e-05, 1.4461e-05, 1.0882e-05, 1.3655e-05,\n",
      "          9.8528e-06, 1.3160e-05, 1.2394e-05, 5.7521e-06, 1.0968e-05,\n",
      "          5.8386e-06, 1.1473e-05, 9.1964e-06, 1.4636e-05, 1.6848e-05,\n",
      "          1.0963e-05, 9.0913e-06, 2.0567e-05, 1.3908e-05, 1.3985e-05,\n",
      "          1.2324e-05, 1.1344e-05, 9.0650e-06, 1.0058e-05, 1.1771e-05,\n",
      "          1.0279e-05],\n",
      "         [3.2516e-03, 3.9449e-03, 4.4113e-03, 4.1989e-03, 3.5470e-03,\n",
      "          4.0633e-03, 3.0407e-03, 6.6530e-03, 9.5730e-03, 5.0430e-03,\n",
      "          1.9278e-02, 6.3464e-03, 1.0837e-02, 1.3135e-02, 4.2530e-03,\n",
      "          1.4091e-02, 5.9080e-03, 2.3209e-02, 2.5862e-02, 4.7257e-03,\n",
      "          6.6972e-02, 4.5034e-03, 1.6488e-02, 2.0054e-02, 5.3112e-03,\n",
      "          1.6966e-01, 4.2687e-03, 5.2861e-02, 4.9674e-02, 5.2475e-03,\n",
      "          7.4121e-02, 9.2669e-03, 9.5587e-02, 7.8829e-02, 7.4697e-03,\n",
      "          3.0367e-02, 5.8546e-03, 1.5277e-02, 1.5632e-02, 7.1626e-03,\n",
      "          3.3513e-02, 3.9550e-03, 1.4660e-02, 9.8669e-03, 3.9720e-03,\n",
      "          3.9485e-03, 4.6768e-03, 4.5413e-03, 4.2281e-03, 3.8543e-03,\n",
      "          2.8061e-03]]])\n",
      "Player 0 Prediction: tensor([[0.9614, 0.0000, 0.0386, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[3.0143e-04, 6.4856e-04, 6.3850e-04, 7.0255e-04, 1.0556e-03,\n",
      "          6.4410e-04, 7.4075e-04, 1.5873e-02, 2.1684e-02, 9.9064e-04,\n",
      "          7.5032e-03, 1.7480e-03, 1.2113e-01, 1.5000e-01, 9.5919e-04,\n",
      "          4.0083e-04, 1.5058e-03, 1.6245e-02, 2.2847e-02, 1.3763e-03,\n",
      "          4.1641e-03, 1.3005e-03, 1.3158e-03, 1.3857e-03, 7.3771e-04,\n",
      "          2.5454e-01, 1.0656e-03, 1.3444e-03, 1.1233e-03, 1.2054e-03,\n",
      "          1.6224e-03, 3.5124e-03, 3.1281e-02, 2.8814e-02, 1.5754e-03,\n",
      "          2.5591e-04, 1.1458e-03, 1.2272e-01, 1.2665e-01, 1.8875e-03,\n",
      "          5.4062e-03, 1.5073e-03, 1.7215e-02, 1.6540e-02, 6.3065e-04,\n",
      "          5.3803e-04, 8.8366e-04, 6.5058e-04, 6.5089e-04, 8.4064e-04,\n",
      "          5.0014e-04],\n",
      "         [1.6758e-03, 2.4996e-03, 2.8358e-03, 3.1360e-03, 3.9009e-03,\n",
      "          3.3997e-03, 2.8633e-03, 5.2769e-02, 5.8080e-02, 5.6658e-03,\n",
      "          1.3045e-02, 7.3464e-03, 5.0868e-02, 6.0063e-02, 4.0949e-03,\n",
      "          1.0040e-02, 4.6896e-03, 1.2323e-02, 1.6869e-02, 5.6699e-03,\n",
      "          1.2543e-02, 3.4896e-03, 4.3602e-03, 5.1054e-03, 2.4900e-03,\n",
      "          1.6320e-01, 2.9373e-03, 9.0737e-03, 8.4738e-03, 3.8062e-03,\n",
      "          1.1258e-02, 7.5673e-03, 6.2278e-02, 4.7162e-02, 4.6278e-03,\n",
      "          1.4077e-02, 4.0256e-03, 7.5877e-02, 7.1037e-02, 6.9739e-03,\n",
      "          1.5341e-02, 1.0766e-02, 5.6466e-02, 5.2697e-02, 3.0872e-03,\n",
      "          2.6885e-03, 4.0439e-03, 3.4124e-03, 3.9712e-03, 3.0147e-03,\n",
      "          2.3124e-03],\n",
      "         [8.9861e-05, 6.6336e-05, 1.1045e-04, 1.0430e-04, 8.9246e-05,\n",
      "          9.5539e-05, 7.2927e-05, 8.8810e-05, 7.5558e-05, 9.5965e-05,\n",
      "          1.0942e-04, 6.7658e-05, 1.2438e-02, 1.2491e-02, 1.0859e-04,\n",
      "          1.0211e-02, 9.2625e-05, 4.6488e-01, 4.8495e-01, 7.5291e-05,\n",
      "          4.7202e-03, 9.5545e-05, 1.1628e-03, 3.2508e-03, 1.6551e-03,\n",
      "          1.0856e-04, 1.3309e-04, 9.0104e-05, 7.9548e-05, 2.0187e-04,\n",
      "          1.2138e-04, 1.1976e-04, 1.2837e-04, 8.5803e-05, 1.2856e-04,\n",
      "          1.4110e-04, 8.3636e-05, 8.0804e-05, 1.3883e-04, 1.3329e-04,\n",
      "          8.7219e-05, 8.5676e-05, 8.4016e-05, 9.3515e-05, 8.0407e-05,\n",
      "          1.1619e-04, 8.3354e-05, 1.2227e-04, 1.1273e-04, 8.3745e-05,\n",
      "          8.3641e-05],\n",
      "         [1.4228e-03, 1.9083e-03, 2.9935e-03, 2.8962e-03, 4.4294e-03,\n",
      "          3.4796e-03, 3.1410e-03, 4.4849e-03, 6.9483e-03, 3.1330e-03,\n",
      "          8.6760e-03, 3.7828e-03, 3.4264e-03, 4.7888e-03, 2.7955e-03,\n",
      "          5.0502e-03, 3.4106e-03, 9.0226e-02, 1.1014e-01, 2.6827e-03,\n",
      "          1.4885e-02, 2.6377e-03, 5.3535e-03, 6.8497e-03, 2.2491e-03,\n",
      "          1.4528e-01, 2.4135e-03, 7.0257e-03, 7.8128e-03, 3.9563e-03,\n",
      "          1.1295e-02, 4.0658e-03, 2.2702e-01, 2.1790e-01, 4.7964e-03,\n",
      "          8.6483e-03, 3.3629e-03, 6.9470e-03, 4.5877e-03, 5.0409e-03,\n",
      "          7.9421e-03, 3.8626e-03, 4.4041e-03, 5.1902e-03, 1.7453e-03,\n",
      "          2.1882e-03, 3.3278e-03, 2.5330e-03, 3.0980e-03, 2.0871e-03,\n",
      "          1.6758e-03]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7943, 0.0306, 0.1751]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:17<30:31, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 63807\n",
      "Average episode length: 6.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5621/10000 (56.2%)\n",
      "    Average reward: -0.310\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4379/10000 (43.8%)\n",
      "    Average reward: +0.310\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 12937 (39.6%)\n",
      "    Action 1: 17470 (53.4%)\n",
      "    Action 2: 759 (2.3%)\n",
      "    Action 3: 1530 (4.7%)\n",
      "  Player 1:\n",
      "    Action 0: 13057 (42.0%)\n",
      "    Action 1: 13514 (43.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 4540 (14.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3104.5, 3104.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.012 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.030\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3105\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.6404, 0.3447, 0.0150, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1147, 0.8747, 0.0106, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8027, 0.0277, 0.1696]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52617\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5685/10000 (56.9%)\n",
      "    Average reward: -0.104\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4315/10000 (43.1%)\n",
      "    Average reward: +0.104\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4305 (17.6%)\n",
      "    Action 1: 17900 (73.3%)\n",
      "    Action 2: 479 (2.0%)\n",
      "    Action 3: 1739 (7.1%)\n",
      "  Player 1:\n",
      "    Action 0: 22982 (81.5%)\n",
      "    Action 1: 5212 (18.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1042.0, 1042.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.770 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.691 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.730\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1042\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6576, 0.3150, 0.0273, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.1184e-03, 1.4927e-03, 1.5818e-03, 1.7350e-03, 2.4930e-03,\n",
      "          1.3433e-03, 1.7517e-03, 1.2440e-02, 1.4318e-02, 5.2825e-03,\n",
      "          9.2442e-02, 7.6944e-03, 1.2756e-02, 1.3521e-02, 2.9640e-03,\n",
      "          3.9666e-02, 6.2829e-03, 2.5698e-02, 1.8172e-02, 3.2609e-03,\n",
      "          4.5235e-02, 1.9462e-03, 2.8611e-03, 4.1622e-03, 1.8023e-03,\n",
      "          9.5331e-02, 1.6505e-03, 6.0423e-03, 6.2617e-03, 5.9039e-03,\n",
      "          2.8909e-01, 5.2150e-03, 2.2602e-02, 2.2358e-02, 8.1289e-03,\n",
      "          7.9014e-02, 4.7986e-03, 1.1088e-02, 8.9074e-03, 6.4999e-03,\n",
      "          6.3842e-02, 8.5550e-03, 1.0389e-02, 8.6503e-03, 2.2452e-03,\n",
      "          2.2666e-03, 2.6044e-03, 1.5659e-03, 2.3544e-03, 1.5189e-03,\n",
      "          1.0929e-03],\n",
      "         [1.2146e-03, 2.2642e-03, 1.8798e-03, 1.9915e-03, 2.3628e-03,\n",
      "          2.1016e-03, 1.9544e-03, 1.7029e-02, 2.4293e-02, 9.3634e-03,\n",
      "          6.4792e-03, 5.9902e-03, 2.9495e-02, 3.4998e-02, 6.0804e-03,\n",
      "          2.4986e-02, 4.1799e-03, 4.7162e-02, 5.0231e-02, 4.6825e-03,\n",
      "          1.9351e-02, 3.7460e-03, 4.2773e-03, 5.3514e-03, 2.4742e-03,\n",
      "          1.2406e-01, 2.5789e-03, 1.0933e-02, 9.3902e-03, 4.5787e-03,\n",
      "          2.0374e-01, 9.6154e-03, 8.3069e-02, 7.5607e-02, 5.6403e-03,\n",
      "          1.4076e-02, 8.9150e-03, 3.2408e-02, 2.7986e-02, 6.1743e-03,\n",
      "          5.2516e-03, 1.1617e-02, 2.1055e-02, 1.4546e-02, 2.4396e-03,\n",
      "          2.4355e-03, 2.3353e-03, 1.9730e-03, 2.1883e-03, 1.7836e-03,\n",
      "          1.6662e-03],\n",
      "         [3.3167e-05, 3.3219e-05, 3.4445e-05, 4.8486e-05, 3.7139e-05,\n",
      "          3.8147e-05, 4.0347e-05, 2.5427e-05, 3.3723e-05, 2.4704e-05,\n",
      "          2.3093e-05, 2.8255e-05, 1.6420e-03, 1.7178e-03, 1.7103e-05,\n",
      "          3.2537e-03, 4.2451e-05, 7.0293e-04, 8.7962e-04, 2.4857e-05,\n",
      "          1.3228e-03, 2.6789e-05, 4.8857e-01, 4.9988e-01, 6.8246e-04,\n",
      "          3.7991e-05, 2.7208e-05, 4.0792e-05, 3.3278e-05, 3.4062e-05,\n",
      "          3.3293e-05, 4.2801e-05, 4.0206e-05, 2.3047e-05, 2.9138e-05,\n",
      "          3.3632e-05, 3.0921e-05, 3.6518e-05, 3.3843e-05, 3.0800e-05,\n",
      "          4.4633e-05, 3.5545e-05, 2.2176e-05, 3.9057e-05, 2.4567e-05,\n",
      "          2.3021e-05, 2.2332e-05, 3.1398e-05, 2.5235e-05, 3.3145e-05,\n",
      "          3.2996e-05],\n",
      "         [3.1390e-03, 3.6535e-03, 4.0951e-03, 3.7030e-03, 4.2099e-03,\n",
      "          4.5300e-03, 4.5338e-03, 2.1331e-02, 2.1783e-02, 6.0440e-03,\n",
      "          2.8454e-02, 7.3007e-03, 1.4775e-02, 1.8360e-02, 5.4574e-03,\n",
      "          1.5668e-02, 5.1766e-03, 2.3042e-02, 2.0380e-02, 6.0696e-03,\n",
      "          3.7307e-02, 4.0457e-03, 2.2291e-02, 2.2814e-02, 3.4389e-03,\n",
      "          1.7880e-01, 4.7626e-03, 6.5302e-02, 5.3198e-02, 6.4904e-03,\n",
      "          9.2196e-02, 8.2313e-03, 3.1326e-02, 3.1936e-02, 8.6805e-03,\n",
      "          3.9128e-02, 7.2860e-03, 3.2178e-02, 1.7531e-02, 6.5755e-03,\n",
      "          3.0677e-02, 6.4884e-03, 2.0404e-02, 1.7405e-02, 5.0651e-03,\n",
      "          3.8138e-03, 4.8881e-03, 4.1012e-03, 4.9947e-03, 3.9503e-03,\n",
      "          2.9850e-03]]])\n",
      "Player 1 Prediction: tensor([[0.9435, 0.0000, 0.0565, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[6.0969e-04, 8.6242e-04, 9.5183e-04, 7.4312e-04, 9.0403e-04,\n",
      "          1.2579e-03, 8.5031e-04, 3.5655e-02, 3.8942e-02, 3.8862e-03,\n",
      "          1.2348e-02, 1.9481e-03, 1.0232e-01, 1.3839e-01, 2.0753e-03,\n",
      "          1.6578e-03, 1.7950e-03, 1.7369e-02, 1.4397e-02, 1.2505e-03,\n",
      "          4.1078e-03, 1.2092e-03, 1.8456e-03, 2.5394e-03, 1.4050e-03,\n",
      "          2.2207e-01, 7.9727e-04, 2.0735e-03, 2.0321e-03, 1.5519e-03,\n",
      "          2.7515e-03, 3.0566e-03, 2.4231e-02, 2.2356e-02, 2.5091e-03,\n",
      "          1.6260e-03, 2.3129e-03, 1.0137e-01, 1.1038e-01, 1.9136e-03,\n",
      "          1.2482e-02, 4.4467e-03, 4.5403e-02, 4.0809e-02, 1.1034e-03,\n",
      "          6.2451e-04, 9.3921e-04, 8.2008e-04, 1.0142e-03, 1.1560e-03,\n",
      "          8.4625e-04],\n",
      "         [1.4951e-03, 1.5459e-03, 1.5807e-03, 2.5038e-03, 1.8513e-03,\n",
      "          1.5682e-03, 1.7457e-03, 3.7137e-02, 4.3218e-02, 8.8678e-03,\n",
      "          1.0649e-02, 5.4186e-03, 1.0135e-01, 9.7376e-02, 4.6593e-03,\n",
      "          1.1838e-02, 3.2180e-03, 1.4823e-02, 1.5534e-02, 3.1345e-03,\n",
      "          1.1417e-02, 2.1993e-03, 3.6714e-03, 3.8106e-03, 1.8186e-03,\n",
      "          1.7097e-01, 2.2176e-03, 6.5842e-03, 5.5328e-03, 3.4620e-03,\n",
      "          1.2180e-02, 5.2421e-03, 6.4374e-02, 6.7062e-02, 4.7935e-03,\n",
      "          1.3933e-02, 4.3302e-03, 7.3278e-02, 7.1243e-02, 4.8147e-03,\n",
      "          8.4190e-03, 1.2395e-02, 3.6135e-02, 2.7714e-02, 1.6565e-03,\n",
      "          2.3429e-03, 2.0314e-03, 1.9033e-03, 1.7908e-03, 1.9397e-03,\n",
      "          1.2284e-03],\n",
      "         [3.4078e-04, 2.4176e-04, 2.5900e-04, 3.0675e-04, 3.2128e-04,\n",
      "          3.5532e-04, 2.8531e-04, 3.1603e-04, 2.9363e-04, 3.1572e-04,\n",
      "          3.2703e-04, 3.1317e-04, 4.0963e-02, 3.4157e-02, 1.5498e-04,\n",
      "          1.9054e-02, 4.4980e-04, 4.2485e-01, 4.3063e-01, 4.8181e-04,\n",
      "          1.6140e-02, 3.0202e-04, 5.9498e-03, 1.0554e-02, 4.1625e-03,\n",
      "          3.2268e-04, 2.5765e-04, 3.2619e-04, 4.0978e-04, 3.8786e-04,\n",
      "          2.6691e-04, 2.8860e-04, 5.2327e-04, 1.9225e-04, 2.1575e-04,\n",
      "          5.9663e-04, 2.6998e-04, 2.2572e-04, 4.5482e-04, 2.4722e-04,\n",
      "          3.4262e-04, 3.7910e-04, 2.0110e-04, 3.9037e-04, 2.7084e-04,\n",
      "          3.3578e-04, 2.2258e-04, 2.7224e-04, 3.3481e-04, 2.6578e-04,\n",
      "          4.7273e-04],\n",
      "         [2.2910e-03, 2.4248e-03, 2.3552e-03, 2.9329e-03, 2.4408e-03,\n",
      "          2.5762e-03, 2.1101e-03, 1.4673e-02, 2.1045e-02, 5.7649e-03,\n",
      "          1.5754e-02, 5.8082e-03, 8.1614e-03, 1.1537e-02, 4.9240e-03,\n",
      "          1.2033e-02, 4.1282e-03, 1.0670e-01, 1.0899e-01, 3.1417e-03,\n",
      "          1.2017e-02, 3.6598e-03, 5.7711e-03, 7.7518e-03, 3.2804e-03,\n",
      "          1.4999e-01, 3.2816e-03, 7.8011e-03, 7.6736e-03, 4.0982e-03,\n",
      "          9.6337e-03, 7.1461e-03, 1.3343e-01, 1.4825e-01, 4.2827e-03,\n",
      "          1.5325e-02, 3.7479e-03, 1.6160e-02, 1.4640e-02, 5.6598e-03,\n",
      "          1.7600e-02, 4.4708e-03, 3.2194e-02, 2.2642e-02, 3.3637e-03,\n",
      "          3.2509e-03, 3.4613e-03, 2.6949e-03, 2.4609e-03, 2.0513e-03,\n",
      "          2.4360e-03]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8135, 0.0385, 0.1480]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54101\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6384/10000 (63.8%)\n",
      "    Average reward: +0.445\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3616/10000 (36.2%)\n",
      "    Average reward: -0.445\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11106 (41.5%)\n",
      "    Action 1: 10410 (38.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 5238 (19.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10644 (38.9%)\n",
      "    Action 1: 13929 (50.9%)\n",
      "    Action 2: 1042 (3.8%)\n",
      "    Action 3: 1732 (6.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4451.5, -4451.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.041\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4451\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8107, 0.0373, 0.1520]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8072, 0.0421, 0.1507]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51884\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6345/10000 (63.4%)\n",
      "    Average reward: +0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3655/10000 (36.5%)\n",
      "    Average reward: -0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21205 (77.0%)\n",
      "    Action 1: 6332 (23.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5610 (23.0%)\n",
      "    Action 1: 16259 (66.8%)\n",
      "    Action 2: 695 (2.9%)\n",
      "    Action 3: 1783 (7.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1373.5, -1373.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.778 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.877 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.827\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1373\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3005/50000 [02:18<29:41, 26.38it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0011 â†’ 0.0011\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 190179/200000\n",
      "   Player 0 SL buffer: 19743/2000000\n",
      "   Player 1 RL buffer: 193948/200000\n",
      "   Player 1 SL buffer: 21075/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3998/50000 [02:56<29:15, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0009 â†’ 0.0009\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 25934/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 27666/2000000\n",
      "P1 SL Buffer Size:  25934\n",
      "P1 SL buffer distribution [ 9677. 12969.    24.  3264.]\n",
      "P1 actions distribution [0.37313951 0.50007712 0.00092543 0.12585795]\n",
      "P2 SL Buffer Size:  27666\n",
      "P2 SL buffer distribution [10482. 13017.   101.  4066.]\n",
      "P2 actions distribution [0.3788766  0.47050531 0.00365069 0.1469674 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[7.3234e-04, 1.4678e-03, 1.7172e-03, 1.9467e-03, 2.1477e-03,\n",
      "          2.2074e-03, 1.9232e-03, 8.3176e-03, 1.4013e-02, 9.1333e-03,\n",
      "          4.1602e-03, 4.3877e-03, 1.0823e-02, 1.6581e-02, 4.5644e-03,\n",
      "          6.1908e-03, 3.5408e-03, 4.9269e-02, 6.9916e-02, 1.0313e-02,\n",
      "          2.0159e-02, 4.4234e-03, 8.2399e-03, 1.1958e-02, 2.4005e-03,\n",
      "          1.5841e-01, 2.1933e-03, 1.8756e-02, 1.8234e-02, 7.0376e-03,\n",
      "          4.5001e-02, 1.9738e-02, 1.2340e-01, 8.4419e-02, 5.7020e-03,\n",
      "          8.7269e-03, 1.4300e-02, 4.0306e-02, 2.8054e-02, 6.6823e-03,\n",
      "          1.0025e-02, 3.6962e-02, 6.1196e-02, 2.8529e-02, 1.6960e-03,\n",
      "          2.0787e-03, 2.0896e-03, 1.9263e-03, 1.8855e-03, 1.3713e-03,\n",
      "          7.4846e-04],\n",
      "         [7.1941e-04, 1.1118e-03, 1.4324e-03, 1.4289e-03, 2.1745e-03,\n",
      "          2.1607e-03, 1.6736e-03, 1.2610e-02, 1.9262e-02, 8.9376e-03,\n",
      "          8.3558e-03, 3.9602e-03, 3.7969e-03, 5.2442e-03, 3.3740e-03,\n",
      "          2.0299e-02, 4.2123e-03, 5.6237e-02, 8.4690e-02, 1.1276e-02,\n",
      "          1.6273e-02, 2.9278e-03, 4.3434e-03, 5.1787e-03, 1.8036e-03,\n",
      "          1.4495e-01, 1.6072e-03, 1.3822e-02, 1.3296e-02, 3.7867e-03,\n",
      "          1.3677e-02, 2.1450e-02, 1.4888e-01, 1.3154e-01, 6.2311e-03,\n",
      "          2.5757e-02, 7.8730e-03, 1.1972e-02, 6.7310e-03, 5.6149e-03,\n",
      "          1.4518e-02, 3.9395e-02, 6.5356e-02, 3.0333e-02, 1.4142e-03,\n",
      "          1.5243e-03, 1.5762e-03, 1.5751e-03, 1.5155e-03, 1.2768e-03,\n",
      "          8.4933e-04],\n",
      "         [5.2929e-06, 5.4172e-06, 4.5107e-06, 7.3653e-06, 5.2646e-06,\n",
      "          7.2847e-06, 5.2705e-06, 9.7060e-06, 7.2319e-06, 4.9677e-06,\n",
      "          3.0864e-06, 4.9346e-06, 5.9861e-04, 5.0673e-04, 5.8951e-06,\n",
      "          5.7173e-04, 5.4180e-06, 5.5408e-04, 6.3019e-04, 6.4115e-06,\n",
      "          3.5844e-04, 6.5554e-06, 7.3168e-03, 2.4437e-01, 7.4484e-01,\n",
      "          8.7099e-06, 7.2113e-06, 9.1190e-06, 5.5225e-06, 9.6170e-06,\n",
      "          5.6243e-06, 7.4872e-06, 5.7231e-06, 3.9772e-06, 5.5265e-06,\n",
      "          5.1159e-06, 7.0203e-06, 6.4064e-06, 5.6903e-06, 5.6865e-06,\n",
      "          5.4717e-06, 5.5010e-06, 6.2826e-06, 5.6185e-06, 4.6406e-06,\n",
      "          7.0986e-06, 6.1248e-06, 5.9874e-06, 4.1862e-06, 5.0856e-06,\n",
      "          5.6006e-06],\n",
      "         [1.1057e-03, 2.4782e-03, 2.6636e-03, 2.1406e-03, 3.5305e-03,\n",
      "          2.3813e-03, 2.5834e-03, 8.5747e-03, 1.0742e-02, 6.2673e-03,\n",
      "          8.6829e-03, 4.4724e-03, 9.5065e-03, 1.2000e-02, 3.4689e-03,\n",
      "          6.1594e-03, 3.2579e-03, 4.8333e-02, 8.6712e-02, 9.0807e-03,\n",
      "          8.5180e-03, 2.8984e-03, 1.9344e-02, 1.9131e-02, 2.2927e-03,\n",
      "          2.0430e-01, 2.4940e-03, 3.1516e-02, 3.0860e-02, 3.7257e-03,\n",
      "          1.3419e-02, 2.5174e-02, 9.2680e-02, 8.3688e-02, 4.4040e-03,\n",
      "          1.1057e-02, 5.4751e-03, 3.8288e-02, 2.4169e-02, 6.0571e-03,\n",
      "          1.3663e-02, 3.8248e-02, 4.6704e-02, 2.0206e-02, 2.3496e-03,\n",
      "          2.7158e-03, 2.1326e-03, 3.4005e-03, 2.3801e-03, 2.6861e-03,\n",
      "          1.8815e-03]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8302, 0.0090, 0.1608]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3998/50000 [03:07<29:15, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 63159\n",
      "Average episode length: 6.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5817/10000 (58.2%)\n",
      "    Average reward: -0.321\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4183/10000 (41.8%)\n",
      "    Average reward: +0.321\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13629 (43.0%)\n",
      "    Action 1: 15403 (48.6%)\n",
      "    Action 2: 278 (0.9%)\n",
      "    Action 3: 2356 (7.4%)\n",
      "  Player 1:\n",
      "    Action 0: 11344 (36.0%)\n",
      "    Action 1: 15685 (49.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 4464 (14.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3213.0, 3213.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.030\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3213\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8014, 0.1902, 0.0084, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6620, 0.0202, 0.3178]])\n",
      "Player 0 Prediction: tensor([[0.4439, 0.5389, 0.0172, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54427\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5797/10000 (58.0%)\n",
      "    Average reward: -0.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4203/10000 (42.0%)\n",
      "    Average reward: +0.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5079 (20.0%)\n",
      "    Action 1: 17060 (67.0%)\n",
      "    Action 2: 270 (1.1%)\n",
      "    Action 3: 3044 (12.0%)\n",
      "  Player 1:\n",
      "    Action 0: 22090 (76.2%)\n",
      "    Action 1: 6884 (23.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1287.5, 1287.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.851 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.791 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.821\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1288\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8090, 0.1790, 0.0121, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[5.9430e-04, 8.0185e-04, 7.1732e-04, 9.2329e-04, 1.1332e-03,\n",
      "          7.8042e-04, 9.8198e-04, 8.4133e-03, 1.3101e-02, 8.5265e-03,\n",
      "          2.3878e-02, 6.3866e-03, 1.4832e-02, 2.1309e-02, 3.6952e-03,\n",
      "          9.6079e-03, 2.4913e-03, 1.4489e-02, 1.8825e-02, 7.4605e-03,\n",
      "          1.8642e-02, 3.5199e-03, 3.6509e-03, 5.1300e-03, 1.1659e-03,\n",
      "          2.1224e-01, 1.1316e-03, 6.9191e-03, 5.6733e-03, 3.3350e-03,\n",
      "          6.7660e-02, 5.8561e-03, 2.6220e-02, 2.1757e-02, 6.8367e-03,\n",
      "          6.2833e-02, 1.0269e-02, 6.4172e-02, 5.4092e-02, 1.3213e-02,\n",
      "          1.1584e-01, 2.5486e-02, 6.5496e-02, 3.3467e-02, 9.8139e-04,\n",
      "          1.1192e-03, 1.2129e-03, 7.2303e-04, 1.1174e-03, 6.3382e-04,\n",
      "          6.6463e-04],\n",
      "         [3.5122e-04, 5.7783e-04, 6.4768e-04, 6.1239e-04, 6.1827e-04,\n",
      "          6.3175e-04, 7.7985e-04, 2.5852e-02, 4.1454e-02, 1.5833e-02,\n",
      "          1.7319e-02, 4.3643e-03, 1.2072e-02, 1.8634e-02, 4.1762e-03,\n",
      "          1.2394e-02, 2.5341e-03, 1.0619e-02, 1.0734e-02, 4.5819e-03,\n",
      "          4.2530e-03, 2.3073e-03, 1.5738e-03, 2.7035e-03, 8.8426e-04,\n",
      "          1.4788e-01, 6.9236e-04, 7.6349e-03, 7.6063e-03, 2.0150e-03,\n",
      "          1.4674e-02, 5.3807e-03, 2.6850e-02, 2.5285e-02, 5.5647e-03,\n",
      "          6.4754e-02, 7.7775e-03, 6.0549e-02, 5.1963e-02, 9.0087e-03,\n",
      "          5.3307e-02, 4.9750e-02, 1.5241e-01, 1.0614e-01, 7.7777e-04,\n",
      "          7.7050e-04, 5.8579e-04, 5.4528e-04, 6.1614e-04, 5.9715e-04,\n",
      "          3.6425e-04],\n",
      "         [1.3217e-05, 1.0600e-05, 1.5132e-05, 1.3242e-05, 1.6600e-05,\n",
      "          1.5550e-05, 1.4065e-05, 6.4291e-06, 1.5216e-05, 1.2550e-05,\n",
      "          1.1566e-05, 1.3215e-05, 1.5857e-03, 2.0894e-03, 8.1257e-06,\n",
      "          2.0110e-03, 1.5830e-05, 1.2511e-03, 1.4779e-03, 9.7948e-06,\n",
      "          2.8935e-04, 8.2725e-06, 4.7761e-01, 5.0702e-01, 6.1263e-03,\n",
      "          1.3030e-05, 1.3022e-05, 1.8294e-05, 1.6519e-05, 1.4091e-05,\n",
      "          1.5379e-05, 1.2677e-05, 2.0776e-05, 7.3350e-06, 1.2804e-05,\n",
      "          1.6947e-05, 1.0954e-05, 1.1895e-05, 1.1957e-05, 1.3653e-05,\n",
      "          1.5820e-05, 1.6198e-05, 7.7733e-06, 1.3631e-05, 1.1598e-05,\n",
      "          8.0692e-06, 7.1964e-06, 1.3747e-05, 1.1491e-05, 1.1354e-05,\n",
      "          1.6678e-05],\n",
      "         [1.4357e-03, 1.4685e-03, 1.6659e-03, 1.4261e-03, 1.9611e-03,\n",
      "          1.4038e-03, 1.3992e-03, 1.5450e-02, 2.3950e-02, 5.4886e-03,\n",
      "          1.7648e-02, 5.6954e-03, 6.6034e-03, 7.9829e-03, 2.5283e-03,\n",
      "          4.6618e-03, 2.9063e-03, 5.2771e-02, 5.4518e-02, 4.4835e-03,\n",
      "          3.7106e-03, 2.2584e-03, 1.6735e-02, 2.0493e-02, 1.5366e-03,\n",
      "          2.8221e-01, 1.7096e-03, 3.6392e-02, 3.1405e-02, 3.3006e-03,\n",
      "          8.6964e-03, 5.2745e-03, 8.0293e-02, 6.3013e-02, 5.6361e-03,\n",
      "          1.1906e-02, 4.6643e-03, 1.6892e-02, 1.5259e-02, 7.6978e-03,\n",
      "          3.1307e-02, 1.8358e-02, 6.5452e-02, 3.9759e-02, 1.4866e-03,\n",
      "          1.5162e-03, 1.7697e-03, 1.7883e-03, 1.2491e-03, 1.6556e-03,\n",
      "          1.1301e-03]]])\n",
      "Player 1 Prediction: tensor([[0.1103, 0.8855, 0.0042, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.1660e-04, 4.7865e-04, 4.7811e-04, 4.9248e-04, 4.5026e-04,\n",
      "          5.0291e-04, 5.5679e-04, 3.3427e-02, 4.7558e-02, 1.2366e-02,\n",
      "          2.2258e-03, 1.7991e-03, 2.7422e-02, 4.3671e-02, 2.9186e-03,\n",
      "          2.4944e-03, 8.1936e-04, 1.6130e-02, 1.6146e-02, 3.4915e-03,\n",
      "          6.2555e-04, 1.1122e-03, 1.2739e-03, 2.2949e-03, 8.3075e-04,\n",
      "          1.3757e-01, 6.2393e-04, 1.2504e-03, 1.1633e-03, 7.6408e-04,\n",
      "          1.0866e-03, 4.0923e-03, 3.5886e-02, 3.5109e-02, 1.4863e-03,\n",
      "          7.2166e-03, 6.2036e-03, 1.3189e-01, 1.1514e-01, 2.8685e-03,\n",
      "          9.8891e-03, 3.3583e-02, 1.3880e-01, 1.1224e-01, 4.6689e-04,\n",
      "          5.0810e-04, 5.0667e-04, 4.7897e-04, 4.1091e-04, 4.3518e-04,\n",
      "          4.3955e-04],\n",
      "         [6.4827e-04, 6.8080e-04, 8.9292e-04, 9.1527e-04, 8.1595e-04,\n",
      "          7.1233e-04, 9.8832e-04, 3.5486e-03, 6.0478e-03, 1.3766e-02,\n",
      "          8.9292e-02, 6.3812e-03, 3.3809e-02, 3.3544e-02, 3.5460e-03,\n",
      "          5.8699e-02, 2.7034e-03, 2.9218e-03, 2.9793e-03, 2.3469e-03,\n",
      "          2.6124e-03, 2.3181e-03, 2.1954e-03, 3.2610e-03, 1.0095e-03,\n",
      "          5.2254e-02, 8.5447e-04, 2.5826e-03, 2.7012e-03, 1.8170e-03,\n",
      "          1.0416e-02, 2.6231e-03, 7.8178e-03, 7.1449e-03, 3.5481e-03,\n",
      "          2.9696e-01, 3.7705e-03, 2.5737e-02, 3.1534e-02, 1.5023e-02,\n",
      "          1.9729e-01, 2.7244e-02, 1.6248e-02, 1.1710e-02, 1.0426e-03,\n",
      "          1.1524e-03, 6.5703e-04, 8.2542e-04, 8.9553e-04, 1.0201e-03,\n",
      "          4.9686e-04],\n",
      "         [1.0263e-05, 1.0497e-05, 7.7593e-06, 9.4338e-06, 1.7758e-05,\n",
      "          1.0988e-05, 1.2998e-05, 8.6862e-06, 6.8573e-06, 1.1169e-05,\n",
      "          1.0477e-05, 8.7839e-06, 2.2290e-03, 1.5437e-03, 6.4625e-06,\n",
      "          5.7873e-04, 1.0324e-05, 4.3412e-03, 2.8660e-03, 1.0697e-05,\n",
      "          9.8764e-01, 8.4669e-06, 4.6814e-05, 2.2409e-04, 1.2439e-04,\n",
      "          9.9242e-06, 1.0078e-05, 9.6891e-06, 1.4253e-05, 8.5961e-06,\n",
      "          8.3861e-06, 7.3060e-06, 1.1093e-05, 8.9239e-06, 6.2811e-06,\n",
      "          1.4439e-05, 7.3466e-06, 6.3428e-06, 1.2116e-05, 5.2538e-06,\n",
      "          1.1324e-05, 1.0194e-05, 4.5575e-06, 1.3334e-05, 9.6688e-06,\n",
      "          8.5878e-06, 6.9712e-06, 9.6462e-06, 1.0294e-05, 1.1640e-05,\n",
      "          1.2259e-05],\n",
      "         [1.8912e-03, 1.5090e-03, 1.6163e-03, 1.9326e-03, 2.4230e-03,\n",
      "          1.7426e-03, 1.2312e-03, 1.3866e-02, 2.9897e-02, 5.1059e-03,\n",
      "          4.3482e-02, 7.9301e-03, 4.0828e-03, 5.5591e-03, 2.7891e-03,\n",
      "          9.5501e-03, 3.9598e-03, 4.7334e-02, 5.4395e-02, 3.8740e-03,\n",
      "          3.5485e-02, 4.3295e-03, 3.5323e-03, 3.2795e-03, 1.9266e-03,\n",
      "          2.5535e-01, 1.9313e-03, 2.6010e-03, 2.5259e-03, 3.8560e-03,\n",
      "          3.9986e-02, 5.7289e-03, 6.0697e-02, 6.4455e-02, 5.8634e-03,\n",
      "          2.9030e-02, 2.5458e-03, 9.5048e-03, 1.4197e-02, 1.2736e-02,\n",
      "          7.6581e-02, 1.5433e-02, 6.0029e-02, 3.1660e-02, 1.6030e-03,\n",
      "          2.4106e-03, 1.5707e-03, 2.0617e-03, 1.6741e-03, 1.1464e-03,\n",
      "          2.1023e-03]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4886, 0.0166, 0.4948]])\n",
      "Player 0 Prediction: tensor([[[5.4184e-06, 1.1412e-05, 1.2209e-05, 6.7676e-06, 1.0258e-05,\n",
      "          1.1064e-05, 1.1451e-05, 5.1486e-04, 7.0516e-04, 1.6046e-04,\n",
      "          3.7444e-04, 2.9493e-05, 1.5854e-01, 1.7265e-01, 3.1397e-05,\n",
      "          8.0187e-05, 1.6708e-05, 5.0204e-05, 6.9007e-05, 4.0705e-05,\n",
      "          1.5296e-05, 3.8694e-05, 2.5895e-05, 2.1396e-05, 1.2848e-05,\n",
      "          9.4321e-02, 9.6893e-06, 1.4326e-05, 1.0680e-05, 1.2604e-05,\n",
      "          2.0738e-05, 3.0913e-05, 6.4880e-05, 5.4343e-05, 3.5075e-05,\n",
      "          3.3977e-04, 7.9483e-05, 2.8131e-01, 2.8598e-01, 5.4721e-05,\n",
      "          1.3766e-03, 3.5079e-04, 1.1848e-03, 1.2477e-03, 1.1544e-05,\n",
      "          4.5154e-06, 7.2948e-06, 8.6628e-06, 9.6751e-06, 7.8163e-06,\n",
      "          7.8901e-06],\n",
      "         [1.8669e-05, 2.6488e-05, 2.5202e-05, 1.9189e-05, 2.9709e-05,\n",
      "          3.1871e-05, 3.1246e-05, 1.6404e-01, 1.6609e-01, 6.0021e-04,\n",
      "          5.5533e-04, 1.2236e-04, 3.7402e-04, 4.3869e-04, 1.1006e-04,\n",
      "          1.4819e-04, 7.8052e-05, 1.7073e-04, 2.7103e-04, 1.0526e-04,\n",
      "          2.3669e-05, 3.3104e-05, 9.5243e-05, 1.0778e-04, 2.7664e-05,\n",
      "          1.4330e-01, 1.9967e-05, 9.2765e-05, 7.4574e-05, 4.8667e-05,\n",
      "          6.7670e-05, 9.4903e-05, 2.4633e-04, 2.7104e-04, 1.7164e-04,\n",
      "          4.1377e-04, 2.2998e-04, 1.8309e-02, 1.8021e-02, 3.9592e-04,\n",
      "          1.3984e-03, 1.5489e-03, 2.4289e-01, 2.3864e-01, 3.5261e-05,\n",
      "          2.8843e-05, 2.8822e-05, 3.0585e-05, 2.5076e-05, 1.7523e-05,\n",
      "          1.6948e-05],\n",
      "         [7.4795e-06, 6.7260e-06, 7.5460e-06, 6.2297e-06, 5.1109e-06,\n",
      "          1.3446e-05, 5.1181e-06, 5.8999e-06, 1.1090e-05, 1.0509e-05,\n",
      "          1.1528e-05, 9.5889e-06, 1.6661e-03, 1.6713e-03, 4.2288e-06,\n",
      "          2.1292e-03, 1.1072e-05, 4.9840e-01, 4.9495e-01, 1.2079e-05,\n",
      "          8.5377e-05, 8.4508e-06, 2.3594e-04, 3.6875e-04, 8.7542e-05,\n",
      "          8.7304e-06, 7.1707e-06, 1.3586e-05, 1.8275e-05, 1.3863e-05,\n",
      "          6.4046e-06, 7.2636e-06, 1.6668e-05, 6.3249e-06, 6.9516e-06,\n",
      "          1.9945e-05, 6.7657e-06, 5.5062e-06, 1.3565e-05, 7.6859e-06,\n",
      "          1.0261e-05, 1.5112e-05, 7.3176e-06, 8.3870e-06, 9.7527e-06,\n",
      "          7.7377e-06, 7.6041e-06, 5.0031e-06, 1.3064e-05, 6.1061e-06,\n",
      "          2.1381e-05],\n",
      "         [5.8504e-04, 6.1550e-04, 6.5045e-04, 4.7798e-04, 5.9518e-04,\n",
      "          3.6098e-04, 5.5108e-04, 1.8599e-02, 1.9073e-02, 3.6829e-03,\n",
      "          7.4972e-03, 1.9768e-03, 2.7386e-03, 3.4869e-03, 1.0414e-03,\n",
      "          2.3741e-03, 1.1284e-03, 1.1064e-01, 7.5395e-02, 1.8941e-03,\n",
      "          9.8286e-04, 1.2764e-03, 1.8894e-03, 4.5214e-03, 9.7370e-04,\n",
      "          2.6046e-01, 6.5190e-04, 2.6837e-03, 2.7156e-03, 1.2663e-03,\n",
      "          1.7181e-03, 3.0494e-03, 1.0439e-01, 1.0220e-01, 1.5088e-03,\n",
      "          6.2776e-03, 1.3744e-03, 7.6218e-03, 8.7864e-03, 3.2048e-03,\n",
      "          1.4463e-02, 7.7927e-03, 9.9586e-02, 1.0335e-01, 5.8064e-04,\n",
      "          7.1104e-04, 6.2968e-04, 4.6791e-04, 3.8849e-04, 6.2415e-04,\n",
      "          4.8148e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9921, 0.0000, 0.0079, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59271\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6135/10000 (61.4%)\n",
      "    Average reward: +0.638\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3865/10000 (38.6%)\n",
      "    Average reward: -0.638\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11282 (37.9%)\n",
      "    Action 1: 14292 (48.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 4218 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 10967 (37.2%)\n",
      "    Action 1: 14373 (48.8%)\n",
      "    Action 2: 350 (1.2%)\n",
      "    Action 3: 3789 (12.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6380.0, -6380.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.036 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.037\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6380\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8592, 0.0124, 0.1284]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9025, 0.0106, 0.0869]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56450\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6097/10000 (61.0%)\n",
      "    Average reward: +0.047\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3903/10000 (39.0%)\n",
      "    Average reward: -0.047\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22051 (73.2%)\n",
      "    Action 1: 8078 (26.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6016 (22.9%)\n",
      "    Action 1: 17011 (64.6%)\n",
      "    Action 2: 286 (1.1%)\n",
      "    Action 3: 3008 (11.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [467.5, -467.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.839 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.894 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.866\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0467\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5005/50000 [04:11<28:19, 26.47it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 32134/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 34209/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5999/50000 [04:49<28:19, 25.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 38581/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 40899/2000000\n",
      "P1 SL Buffer Size:  38581\n",
      "P1 SL buffer distribution [15220. 18738.    52.  4571.]\n",
      "P1 actions distribution [0.3944947  0.48567948 0.00134781 0.11847801]\n",
      "P2 SL Buffer Size:  40899\n",
      "P2 SL buffer distribution [16075. 19384.   220.  5220.]\n",
      "P2 actions distribution [0.39304139 0.47394802 0.0053791  0.12763148]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[5.1450e-04, 9.3794e-04, 1.2124e-03, 1.4097e-03, 1.7152e-03,\n",
      "          1.9050e-03, 1.4555e-03, 7.0015e-03, 1.0845e-02, 5.2807e-03,\n",
      "          4.9401e-03, 2.9780e-03, 1.6192e-02, 2.3693e-02, 6.3646e-03,\n",
      "          8.7985e-03, 3.3530e-03, 6.7310e-02, 8.7203e-02, 1.8726e-02,\n",
      "          2.2613e-02, 3.4502e-03, 8.5785e-03, 1.0322e-02, 1.7991e-03,\n",
      "          2.5635e-01, 1.8168e-03, 1.6188e-02, 1.5032e-02, 4.1678e-03,\n",
      "          1.8672e-02, 2.5070e-02, 9.4809e-02, 5.9798e-02, 4.7083e-03,\n",
      "          9.7393e-03, 1.2673e-02, 2.1306e-02, 1.5735e-02, 8.1599e-03,\n",
      "          2.0970e-02, 3.4907e-02, 3.7251e-02, 1.5158e-02, 1.2864e-03,\n",
      "          1.4275e-03, 1.3769e-03, 1.5543e-03, 1.6322e-03, 1.1077e-03,\n",
      "          5.0982e-04],\n",
      "         [5.4542e-04, 8.2564e-04, 9.6066e-04, 9.9410e-04, 1.6268e-03,\n",
      "          1.9015e-03, 1.4263e-03, 9.8903e-03, 1.4509e-02, 4.7086e-03,\n",
      "          1.0851e-02, 3.3097e-03, 1.8149e-03, 2.9022e-03, 3.2858e-03,\n",
      "          1.9431e-02, 3.3585e-03, 7.7210e-02, 1.0003e-01, 2.2367e-02,\n",
      "          2.8216e-02, 2.1752e-03, 3.9238e-03, 5.3996e-03, 1.3465e-03,\n",
      "          1.5231e-01, 1.2879e-03, 1.2275e-02, 1.1302e-02, 2.5244e-03,\n",
      "          2.6041e-02, 3.9315e-02, 1.4713e-01, 1.1532e-01, 4.3995e-03,\n",
      "          1.3091e-02, 7.3711e-03, 6.8992e-03, 2.8516e-03, 5.4400e-03,\n",
      "          1.7094e-02, 4.5109e-02, 4.5111e-02, 1.5056e-02, 1.1611e-03,\n",
      "          1.1081e-03, 1.1786e-03, 1.0686e-03, 1.0271e-03, 9.2258e-04,\n",
      "          5.9735e-04],\n",
      "         [5.9652e-06, 6.2604e-06, 4.1102e-06, 7.9154e-06, 6.0372e-06,\n",
      "          6.7199e-06, 5.6699e-06, 1.1259e-05, 8.5480e-06, 4.9232e-06,\n",
      "          3.6076e-06, 5.0687e-06, 4.4592e-04, 4.4872e-04, 6.5623e-06,\n",
      "          7.9270e-04, 6.2890e-06, 6.0417e-04, 5.9689e-04, 8.0057e-06,\n",
      "          5.1602e-04, 7.6238e-06, 8.1044e-03, 2.5096e-01, 7.3725e-01,\n",
      "          8.6440e-06, 9.0048e-06, 9.3673e-06, 6.4266e-06, 9.0673e-06,\n",
      "          5.4646e-06, 7.4141e-06, 6.0620e-06, 4.1487e-06, 7.8364e-06,\n",
      "          5.1481e-06, 8.0862e-06, 7.4346e-06, 6.1334e-06, 6.4474e-06,\n",
      "          5.6899e-06, 6.4350e-06, 8.1446e-06, 7.3911e-06, 4.5624e-06,\n",
      "          9.2460e-06, 8.5277e-06, 6.4731e-06, 4.1639e-06, 5.3539e-06,\n",
      "          8.3174e-06],\n",
      "         [5.6416e-04, 1.1175e-03, 9.9354e-04, 1.1581e-03, 1.7573e-03,\n",
      "          1.2244e-03, 1.1628e-03, 5.9174e-03, 5.8100e-03, 3.6296e-03,\n",
      "          3.9141e-03, 2.1842e-03, 4.6568e-03, 5.3384e-03, 2.1753e-03,\n",
      "          2.9539e-03, 1.8245e-03, 1.0165e-01, 1.3812e-01, 1.5182e-02,\n",
      "          6.1974e-03, 1.6801e-03, 1.6664e-02, 1.4672e-02, 1.1982e-03,\n",
      "          2.7704e-01, 1.4094e-03, 1.9868e-02, 1.8030e-02, 1.9214e-03,\n",
      "          7.3536e-03, 3.0379e-02, 8.6545e-02, 7.1630e-02, 2.4204e-03,\n",
      "          5.2808e-03, 5.7977e-03, 1.5072e-02, 9.7426e-03, 4.0822e-03,\n",
      "          1.3351e-02, 3.5009e-02, 3.0627e-02, 1.3610e-02, 1.2494e-03,\n",
      "          1.5016e-03, 1.1058e-03, 1.5441e-03, 1.4386e-03, 1.4635e-03,\n",
      "          7.8237e-04]]])\n",
      "Player 0 Prediction: tensor([[0.2643, 0.7292, 0.0066, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[5.6296e-04, 7.7334e-04, 1.0551e-03, 1.2628e-03, 2.0812e-03,\n",
      "          1.7909e-03, 1.3788e-03, 1.2148e-02, 1.5837e-02, 3.7362e-03,\n",
      "          3.1989e-03, 1.8104e-03, 7.8148e-03, 1.1188e-02, 3.4586e-03,\n",
      "          4.1663e-03, 2.0565e-03, 9.7498e-02, 1.1824e-01, 1.7756e-02,\n",
      "          4.9830e-03, 1.8392e-03, 4.0506e-03, 5.2738e-03, 1.2733e-03,\n",
      "          1.4290e-01, 1.2246e-03, 4.7443e-03, 3.7043e-03, 2.2090e-03,\n",
      "          5.3040e-03, 2.6992e-02, 1.6103e-01, 1.3487e-01, 1.9234e-03,\n",
      "          3.0972e-03, 7.4178e-03, 1.6341e-02, 1.0144e-02, 3.6730e-03,\n",
      "          1.2332e-02, 3.7544e-02, 6.5666e-02, 2.6257e-02, 9.9361e-04,\n",
      "          1.1143e-03, 1.1882e-03, 1.1396e-03, 1.2974e-03, 1.0181e-03,\n",
      "          6.4141e-04],\n",
      "         [1.0304e-03, 2.4183e-03, 2.5549e-03, 1.8289e-03, 3.1090e-03,\n",
      "          2.8597e-03, 2.3028e-03, 7.4460e-04, 8.6552e-04, 6.4487e-03,\n",
      "          1.2724e-01, 7.5227e-03, 7.4179e-02, 7.8234e-02, 4.5935e-03,\n",
      "          9.9110e-02, 3.5353e-03, 1.8594e-02, 2.2302e-02, 1.4591e-02,\n",
      "          1.6906e-02, 2.5261e-03, 6.4067e-03, 6.9421e-03, 2.4682e-03,\n",
      "          1.1297e-01, 2.4560e-03, 7.3105e-03, 7.1178e-03, 3.2374e-03,\n",
      "          1.9832e-02, 1.8057e-02, 2.6418e-02, 1.5122e-02, 3.7448e-03,\n",
      "          7.3402e-02, 7.5364e-03, 3.3482e-02, 2.0726e-02, 9.7925e-03,\n",
      "          7.8370e-02, 2.5338e-02, 6.6046e-03, 2.2236e-03, 3.1332e-03,\n",
      "          1.9657e-03, 2.7687e-03, 2.5508e-03, 2.6689e-03, 2.0662e-03,\n",
      "          1.7932e-03],\n",
      "         [1.1167e-05, 7.7186e-06, 1.0342e-05, 1.2345e-05, 7.8331e-06,\n",
      "          1.0426e-05, 1.0416e-05, 8.6077e-06, 7.4276e-06, 7.8085e-06,\n",
      "          7.1712e-06, 8.1023e-06, 6.6518e-04, 6.8733e-04, 9.8880e-06,\n",
      "          6.1022e-04, 8.1072e-06, 1.3556e-03, 1.2368e-03, 7.1192e-06,\n",
      "          9.9404e-01, 1.1070e-05, 9.4238e-05, 7.0264e-04, 2.0887e-04,\n",
      "          1.1566e-05, 8.7728e-06, 1.1273e-05, 6.8823e-06, 9.9191e-06,\n",
      "          8.3849e-06, 1.1626e-05, 9.6808e-06, 5.8467e-06, 1.5952e-05,\n",
      "          9.8575e-06, 6.8746e-06, 7.3491e-06, 7.2112e-06, 1.1618e-05,\n",
      "          8.2123e-06, 9.0113e-06, 8.8248e-06, 1.4733e-05, 3.9383e-06,\n",
      "          1.2178e-05, 1.0362e-05, 1.1048e-05, 9.8908e-06, 9.9535e-06,\n",
      "          8.0359e-06],\n",
      "         [8.9598e-04, 1.0774e-03, 9.7091e-04, 1.2918e-03, 1.3208e-03,\n",
      "          1.2860e-03, 1.1404e-03, 7.6434e-03, 6.6622e-03, 4.7974e-03,\n",
      "          4.5146e-03, 2.4583e-03, 2.3501e-03, 3.2845e-03, 1.9991e-03,\n",
      "          2.2457e-03, 1.4930e-03, 5.8297e-02, 7.6815e-02, 1.2589e-02,\n",
      "          8.9534e-02, 1.7959e-03, 5.0766e-03, 4.0985e-03, 1.2038e-03,\n",
      "          3.8123e-01, 1.3495e-03, 3.1878e-03, 3.1237e-03, 1.9261e-03,\n",
      "          8.1902e-02, 1.1628e-02, 4.8165e-02, 4.4554e-02, 2.5908e-03,\n",
      "          6.1112e-03, 4.2114e-03, 7.8512e-03, 4.5483e-03, 4.1233e-03,\n",
      "          2.6414e-02, 2.1953e-02, 2.4824e-02, 1.5900e-02, 1.1014e-03,\n",
      "          1.4716e-03, 1.4367e-03, 1.3683e-03, 1.8806e-03, 1.5097e-03,\n",
      "          7.9460e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3493, 0.0096, 0.6411]])\n",
      "Player 1 Prediction: tensor([[[2.9242e-06, 5.3055e-06, 6.2026e-06, 5.4811e-06, 1.0539e-05,\n",
      "          5.4241e-06, 8.6119e-06, 4.4615e-04, 4.1284e-04, 3.3710e-05,\n",
      "          2.4679e-04, 1.7246e-05, 3.6845e-01, 3.4618e-01, 1.4743e-05,\n",
      "          4.1962e-05, 1.4661e-05, 1.9818e-04, 2.6936e-04, 3.4767e-05,\n",
      "          5.0495e-05, 1.6082e-05, 2.2443e-05, 2.2210e-05, 8.4412e-06,\n",
      "          1.7977e-01, 1.4635e-05, 7.9060e-06, 6.6579e-06, 1.3751e-05,\n",
      "          1.7273e-05, 4.4508e-05, 9.2715e-05, 1.0912e-04, 3.1044e-05,\n",
      "          1.1362e-05, 4.9126e-05, 5.0234e-02, 5.2528e-02, 3.8462e-05,\n",
      "          1.1354e-04, 1.4163e-04, 1.0317e-04, 9.6414e-05, 7.5129e-06,\n",
      "          5.1402e-06, 9.9138e-06, 6.1951e-06, 5.6967e-06, 6.7554e-06,\n",
      "          2.5209e-06],\n",
      "         [2.2828e-05, 3.4924e-05, 2.2251e-05, 3.3283e-05, 4.6892e-05,\n",
      "          5.2176e-05, 4.7213e-05, 3.4379e-01, 3.3194e-01, 1.0421e-04,\n",
      "          3.1778e-04, 8.7428e-05, 3.4738e-05, 3.2475e-05, 7.8396e-05,\n",
      "          3.5584e-05, 5.5660e-05, 6.9578e-04, 7.6349e-04, 2.3202e-04,\n",
      "          1.6989e-04, 5.0896e-05, 1.4355e-04, 1.9438e-04, 2.3973e-05,\n",
      "          1.9761e-01, 5.6502e-05, 1.6496e-04, 1.5256e-04, 6.5338e-05,\n",
      "          1.6900e-04, 2.3034e-04, 6.5318e-04, 4.5428e-04, 8.3611e-05,\n",
      "          6.2208e-05, 1.6518e-04, 7.2633e-04, 7.6869e-04, 1.6041e-04,\n",
      "          2.5022e-04, 7.8677e-04, 6.4812e-02, 5.3420e-02, 2.8980e-05,\n",
      "          2.5940e-05, 2.7198e-05, 4.6486e-05, 2.5786e-05, 3.0556e-05,\n",
      "          1.3959e-05],\n",
      "         [4.6592e-06, 3.6633e-06, 6.4528e-06, 6.1452e-06, 6.5221e-06,\n",
      "          6.0596e-06, 4.0634e-06, 1.0224e-05, 5.2805e-06, 6.3694e-06,\n",
      "          1.1967e-05, 2.7991e-06, 8.2276e-04, 6.6077e-04, 6.3717e-06,\n",
      "          1.2410e-03, 6.7803e-06, 4.8476e-01, 5.1118e-01, 7.1080e-06,\n",
      "          3.6749e-05, 5.5359e-06, 2.2511e-04, 6.0175e-04, 1.5170e-04,\n",
      "          6.2686e-06, 1.0257e-05, 5.3797e-06, 6.3243e-06, 1.7154e-05,\n",
      "          7.9075e-06, 6.9794e-06, 1.0207e-05, 6.9879e-06, 8.9343e-06,\n",
      "          1.3824e-05, 5.4353e-06, 7.7734e-06, 1.4679e-05, 1.0670e-05,\n",
      "          4.5368e-06, 8.7525e-06, 7.7214e-06, 7.0009e-06, 7.9113e-06,\n",
      "          7.0440e-06, 7.0336e-06, 6.9761e-06, 5.8086e-06, 7.3742e-06,\n",
      "          1.0911e-05],\n",
      "         [2.8241e-04, 4.2775e-04, 5.6696e-04, 8.0241e-04, 1.4216e-03,\n",
      "          8.7859e-04, 7.4597e-04, 7.9112e-03, 8.1191e-03, 1.2074e-03,\n",
      "          2.1389e-03, 9.2224e-04, 1.6836e-03, 3.0171e-03, 1.1546e-03,\n",
      "          1.1771e-03, 1.0463e-03, 1.2900e-01, 1.4113e-01, 9.5523e-03,\n",
      "          1.5186e-03, 5.2458e-04, 2.9972e-03, 3.8528e-03, 6.6567e-04,\n",
      "          1.6882e-01, 5.6531e-04, 3.9293e-03, 2.6040e-03, 7.0700e-04,\n",
      "          2.6867e-03, 1.2857e-02, 1.4388e-01, 1.3365e-01, 1.0649e-03,\n",
      "          1.6002e-03, 5.5957e-03, 1.5845e-02, 6.9610e-03, 1.5522e-03,\n",
      "          1.1182e-02, 3.6660e-02, 8.7181e-02, 3.6306e-02, 4.3180e-04,\n",
      "          6.5269e-04, 6.5660e-04, 4.8727e-04, 5.4821e-04, 5.2946e-04,\n",
      "          2.9756e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60623\n",
      "Average episode length: 6.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5899/10000 (59.0%)\n",
      "    Average reward: -0.507\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4101/10000 (41.0%)\n",
      "    Average reward: +0.507\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10424 (35.4%)\n",
      "    Action 1: 16403 (55.7%)\n",
      "    Action 2: 137 (0.5%)\n",
      "    Action 3: 2493 (8.5%)\n",
      "  Player 1:\n",
      "    Action 0: 11689 (37.5%)\n",
      "    Action 1: 16864 (54.1%)\n",
      "    Action 2: 1709 (5.5%)\n",
      "    Action 3: 904 (2.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5070.0, 5070.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.010 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5070\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3302, 0.6679, 0.0019, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0485, 0.9504, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9010, 0.0029, 0.0961]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5999/50000 [05:08<28:19, 25.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53497\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5932/10000 (59.3%)\n",
      "    Average reward: -0.101\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4068/10000 (40.7%)\n",
      "    Average reward: +0.101\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5187 (20.7%)\n",
      "    Action 1: 16767 (66.8%)\n",
      "    Action 2: 139 (0.6%)\n",
      "    Action 3: 2990 (11.9%)\n",
      "  Player 1:\n",
      "    Action 0: 21727 (76.5%)\n",
      "    Action 1: 6687 (23.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1005.5, 1005.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.859 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.787 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.823\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1006\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7453, 0.2459, 0.0088, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.5848e-04, 4.9293e-04, 4.2781e-04, 5.3067e-04, 7.5101e-04,\n",
      "          3.9339e-04, 6.2092e-04, 4.8096e-03, 7.1117e-03, 6.2477e-03,\n",
      "          5.7283e-02, 1.0865e-02, 5.9637e-03, 5.6442e-03, 2.8911e-03,\n",
      "          7.9238e-03, 2.1133e-03, 1.1954e-02, 1.1729e-02, 4.2099e-03,\n",
      "          3.3160e-02, 3.7002e-03, 1.4964e-03, 2.2022e-03, 6.8941e-04,\n",
      "          2.0392e-01, 4.9190e-04, 7.7851e-04, 7.3977e-04, 2.2476e-03,\n",
      "          5.4161e-02, 3.4720e-03, 1.7119e-02, 2.3811e-02, 5.3332e-03,\n",
      "          5.4861e-02, 6.5060e-03, 2.1677e-02, 2.4573e-02, 3.1441e-02,\n",
      "          2.8717e-01, 2.2278e-02, 3.2946e-02, 1.8954e-02, 6.7696e-04,\n",
      "          6.1975e-04, 7.0931e-04, 5.0084e-04, 6.2881e-04, 3.7256e-04,\n",
      "          4.3765e-04],\n",
      "         [2.3501e-04, 4.0085e-04, 3.9700e-04, 3.1229e-04, 4.0954e-04,\n",
      "          4.0938e-04, 4.7211e-04, 1.9695e-02, 3.0450e-02, 1.2862e-02,\n",
      "          4.6150e-03, 3.5837e-03, 3.1055e-02, 4.0818e-02, 4.2757e-03,\n",
      "          2.4261e-03, 9.4803e-04, 1.6255e-02, 1.6303e-02, 4.8589e-03,\n",
      "          2.5723e-04, 7.3465e-04, 9.9096e-04, 1.5761e-03, 5.1824e-04,\n",
      "          1.5532e-01, 4.3289e-04, 8.5633e-04, 6.5443e-04, 5.9395e-04,\n",
      "          1.5092e-02, 4.9438e-03, 3.8097e-02, 3.5533e-02, 1.7234e-03,\n",
      "          7.1995e-03, 1.6671e-02, 1.4458e-01, 1.2045e-01, 1.0958e-02,\n",
      "          2.4669e-02, 4.4219e-02, 1.0714e-01, 7.3289e-02, 4.7893e-04,\n",
      "          4.7549e-04, 4.1761e-04, 3.2876e-04, 3.7712e-04, 3.6750e-04,\n",
      "          2.7651e-04],\n",
      "         [2.6918e-05, 1.7630e-05, 2.6870e-05, 2.8408e-05, 2.6826e-05,\n",
      "          2.9151e-05, 2.3116e-05, 1.4095e-05, 2.5090e-05, 2.1676e-05,\n",
      "          1.9385e-05, 1.8719e-05, 2.3572e-03, 2.7341e-03, 1.3382e-05,\n",
      "          4.5767e-03, 2.6827e-05, 2.0248e-03, 2.8257e-03, 2.0243e-05,\n",
      "          1.6432e-03, 1.3992e-05, 5.4009e-01, 4.4254e-01, 2.2061e-04,\n",
      "          2.6226e-05, 2.4416e-05, 4.2473e-05, 2.8544e-05, 2.1023e-05,\n",
      "          2.5142e-05, 2.4683e-05, 3.3286e-05, 1.5776e-05, 1.7284e-05,\n",
      "          2.4668e-05, 2.0712e-05, 2.2230e-05, 2.4912e-05, 1.7460e-05,\n",
      "          3.6558e-05, 3.1433e-05, 1.4731e-05, 2.8452e-05, 1.9445e-05,\n",
      "          1.5846e-05, 1.4291e-05, 2.1446e-05, 2.0250e-05, 2.4884e-05,\n",
      "          3.4290e-05],\n",
      "         [1.3803e-03, 1.2107e-03, 1.7861e-03, 1.4612e-03, 1.6543e-03,\n",
      "          1.2814e-03, 1.4689e-03, 2.5811e-02, 2.9959e-02, 7.4541e-03,\n",
      "          3.4832e-02, 7.5959e-03, 7.6886e-03, 8.7357e-03, 2.0284e-03,\n",
      "          6.1967e-03, 2.4309e-03, 5.4088e-02, 6.7568e-02, 1.8726e-02,\n",
      "          5.9881e-03, 3.5698e-03, 1.1773e-02, 1.5290e-02, 1.0675e-03,\n",
      "          1.5227e-01, 1.5823e-03, 2.0226e-02, 1.4096e-02, 3.0121e-03,\n",
      "          2.1842e-02, 1.1677e-02, 1.0474e-01, 9.1075e-02, 6.5948e-03,\n",
      "          1.3477e-02, 5.1011e-03, 1.7692e-02, 1.4893e-02, 1.6518e-02,\n",
      "          5.9494e-02, 2.8236e-02, 4.6888e-02, 3.9790e-02, 1.4799e-03,\n",
      "          1.2632e-03, 1.7338e-03, 1.4245e-03, 1.4136e-03, 1.4474e-03,\n",
      "          9.8423e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9922, 0.0000, 0.0078, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.7665e-05, 5.7681e-05, 5.5326e-05, 4.6830e-05, 6.3210e-05,\n",
      "          7.4459e-05, 6.2195e-05, 5.8558e-03, 7.0271e-03, 1.6022e-03,\n",
      "          1.0502e-03, 3.0726e-04, 8.2689e-02, 1.1091e-01, 4.3908e-04,\n",
      "          3.0537e-05, 1.2870e-04, 7.7112e-03, 6.6711e-03, 4.3706e-04,\n",
      "          4.0508e-05, 1.4474e-04, 1.4400e-04, 2.6052e-04, 9.6015e-05,\n",
      "          1.8532e-01, 5.6660e-05, 9.1275e-05, 7.7888e-05, 7.5980e-05,\n",
      "          9.6416e-05, 6.2083e-04, 1.3237e-02, 1.3429e-02, 1.9254e-04,\n",
      "          1.2063e-04, 1.5653e-03, 2.4155e-01, 2.6234e-01, 9.6149e-04,\n",
      "          5.1570e-03, 4.3839e-03, 2.3431e-02, 2.0984e-02, 5.9802e-05,\n",
      "          3.6672e-05, 5.1316e-05, 4.8123e-05, 5.3414e-05, 5.1792e-05,\n",
      "          5.9577e-05],\n",
      "         [1.6618e-04, 2.7067e-04, 2.4765e-04, 2.4348e-04, 2.4551e-04,\n",
      "          2.2232e-04, 3.3534e-04, 3.9662e-02, 5.2763e-02, 9.7604e-03,\n",
      "          9.4963e-03, 2.6332e-03, 3.2925e-02, 3.8287e-02, 1.8010e-03,\n",
      "          8.8465e-04, 7.1186e-04, 3.1461e-03, 3.9326e-03, 1.7327e-03,\n",
      "          2.4545e-04, 3.9227e-04, 8.9334e-04, 1.3468e-03, 3.0190e-04,\n",
      "          1.8564e-01, 2.2905e-04, 8.7053e-04, 6.4137e-04, 4.6527e-04,\n",
      "          7.9417e-04, 2.3464e-03, 1.2881e-02, 1.2328e-02, 1.3060e-03,\n",
      "          7.8944e-03, 4.8938e-03, 1.3236e-01, 1.2281e-01, 8.6712e-03,\n",
      "          2.8756e-02, 3.2999e-02, 1.3491e-01, 1.0480e-01, 3.0155e-04,\n",
      "          3.0453e-04, 2.5083e-04, 2.7988e-04, 2.2969e-04, 2.4165e-04,\n",
      "          1.5066e-04],\n",
      "         [2.8072e-05, 1.5635e-05, 1.9273e-05, 1.8625e-05, 2.3361e-05,\n",
      "          2.8146e-05, 1.7769e-05, 1.9072e-05, 1.9323e-05, 2.6040e-05,\n",
      "          2.8055e-05, 1.9148e-05, 5.4848e-03, 5.0657e-03, 1.2194e-05,\n",
      "          2.2329e-03, 3.0966e-05, 4.9695e-01, 4.8473e-01, 3.4213e-05,\n",
      "          2.8845e-03, 1.6574e-05, 3.7942e-04, 9.6801e-04, 3.3734e-04,\n",
      "          2.1299e-05, 2.0348e-05, 2.9385e-05, 3.5495e-05, 2.2816e-05,\n",
      "          1.9333e-05, 1.5668e-05, 3.6081e-05, 1.2252e-05, 1.2985e-05,\n",
      "          4.0890e-05, 1.6932e-05, 1.3420e-05, 3.0007e-05, 1.4382e-05,\n",
      "          2.5146e-05, 3.0341e-05, 1.2310e-05, 2.8584e-05, 2.0930e-05,\n",
      "          2.4952e-05, 1.6209e-05, 1.8525e-05, 2.3172e-05, 2.0361e-05,\n",
      "          4.9924e-05],\n",
      "         [2.6551e-04, 2.0766e-04, 2.5507e-04, 3.0110e-04, 3.0287e-04,\n",
      "          2.2976e-04, 1.9826e-04, 1.7163e-02, 2.6259e-02, 2.9758e-03,\n",
      "          2.6990e-03, 1.2975e-03, 1.2174e-03, 1.5258e-03, 5.0278e-04,\n",
      "          1.1884e-03, 6.1757e-04, 7.8106e-02, 7.2721e-02, 2.8092e-03,\n",
      "          3.3647e-04, 7.7390e-04, 8.7537e-04, 1.4359e-03, 3.2609e-04,\n",
      "          1.4967e-01, 3.5910e-04, 9.3006e-04, 9.1131e-04, 5.5708e-04,\n",
      "          6.3223e-04, 2.7238e-03, 2.3534e-01, 2.4644e-01, 7.7696e-04,\n",
      "          1.3571e-03, 8.1613e-04, 3.3076e-03, 3.6103e-03, 2.8001e-03,\n",
      "          5.2526e-03, 9.7292e-03, 6.2611e-02, 5.5643e-02, 3.3627e-04,\n",
      "          3.2198e-04, 3.3898e-04, 2.7544e-04, 2.0668e-04, 1.8274e-04,\n",
      "          2.7819e-04]]])\n",
      "Player 1 Prediction: tensor([[0.2763, 0.7174, 0.0062, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[8.4886e-06, 9.0231e-06, 7.3529e-06, 1.6093e-05, 7.4274e-06,\n",
      "          8.7087e-06, 1.1863e-05, 1.0244e-01, 1.0496e-01, 1.9035e-04,\n",
      "          1.4380e-04, 3.6783e-05, 2.7434e-04, 3.0323e-04, 8.0717e-05,\n",
      "          4.6702e-05, 2.1865e-05, 1.6388e-04, 1.1870e-04, 8.3603e-05,\n",
      "          5.9648e-06, 3.0282e-05, 1.8830e-05, 2.5975e-05, 1.7471e-05,\n",
      "          1.7531e-01, 8.8833e-06, 1.0283e-05, 1.1546e-05, 1.1078e-05,\n",
      "          1.5427e-05, 5.0787e-05, 2.4561e-04, 2.5879e-04, 3.4588e-05,\n",
      "          1.7951e-04, 2.1444e-04, 1.0291e-03, 9.2764e-04, 1.8505e-04,\n",
      "          4.6118e-04, 4.3289e-04, 3.1154e-01, 2.9996e-01, 1.0523e-05,\n",
      "          8.5005e-06, 9.2716e-06, 9.8847e-06, 8.0894e-06, 8.5343e-06,\n",
      "          1.2513e-05],\n",
      "         [3.7044e-04, 4.2481e-04, 6.6163e-04, 4.8383e-04, 4.5815e-04,\n",
      "          6.3348e-04, 6.8050e-04, 1.5353e-02, 1.8661e-02, 1.5658e-02,\n",
      "          5.9768e-02, 7.9850e-03, 2.4370e-02, 4.0192e-02, 3.6581e-03,\n",
      "          4.4732e-03, 1.1381e-03, 8.5496e-03, 7.3819e-03, 2.3188e-03,\n",
      "          2.7254e-04, 7.9325e-04, 1.2781e-03, 2.5117e-03, 4.5788e-04,\n",
      "          1.4116e-01, 6.6647e-04, 1.5547e-03, 1.5823e-03, 7.4255e-04,\n",
      "          1.9107e-03, 4.1833e-03, 1.6145e-02, 2.1980e-02, 2.5879e-03,\n",
      "          3.6217e-02, 7.5337e-03, 1.3194e-01, 1.0011e-01, 2.3381e-02,\n",
      "          1.2227e-01, 4.0814e-02, 7.2780e-02, 4.9758e-02, 4.5817e-04,\n",
      "          8.1462e-04, 5.8987e-04, 5.6163e-04, 7.3672e-04, 7.1756e-04,\n",
      "          2.6968e-04],\n",
      "         [5.8186e-05, 6.2127e-05, 5.8294e-05, 7.2254e-05, 8.4385e-05,\n",
      "          1.0044e-04, 7.9215e-05, 5.5033e-05, 8.3087e-05, 1.1020e-04,\n",
      "          9.6546e-05, 8.6657e-05, 4.6090e-01, 4.8164e-01, 4.7990e-05,\n",
      "          4.9531e-03, 8.9620e-05, 1.7195e-02, 2.0090e-02, 9.2508e-05,\n",
      "          6.7848e-03, 4.0286e-05, 1.2709e-03, 3.0351e-03, 9.1950e-04,\n",
      "          5.8151e-05, 7.5028e-05, 8.5126e-05, 8.1530e-05, 5.4330e-05,\n",
      "          9.5516e-05, 5.3871e-05, 1.1524e-04, 5.3543e-05, 3.9385e-05,\n",
      "          1.5086e-04, 3.7960e-05, 4.8456e-05, 1.1261e-04, 5.4546e-05,\n",
      "          9.2633e-05, 1.2907e-04, 4.5082e-05, 9.1958e-05, 6.2142e-05,\n",
      "          7.4600e-05, 5.1875e-05, 7.5323e-05, 7.0966e-05, 5.9653e-05,\n",
      "          1.3271e-04],\n",
      "         [9.3129e-04, 1.6985e-03, 1.1425e-03, 1.9522e-03, 1.8678e-03,\n",
      "          1.3981e-03, 1.3060e-03, 2.8960e-02, 3.9228e-02, 9.5916e-03,\n",
      "          1.9632e-02, 6.6840e-03, 5.0420e-03, 5.5960e-03, 2.7772e-03,\n",
      "          5.4847e-03, 3.6643e-03, 4.6080e-02, 6.4399e-02, 1.1542e-02,\n",
      "          4.5880e-03, 3.4208e-03, 6.6242e-03, 7.5071e-03, 1.3422e-03,\n",
      "          1.0230e-01, 1.1390e-03, 8.2474e-03, 5.1251e-03, 3.4442e-03,\n",
      "          5.3079e-03, 1.2121e-02, 1.3598e-01, 1.2758e-01, 4.0930e-03,\n",
      "          1.0002e-02, 3.9935e-03, 1.0894e-02, 1.4729e-02, 1.2982e-02,\n",
      "          5.2600e-02, 3.1206e-02, 9.2550e-02, 7.2986e-02, 1.6718e-03,\n",
      "          1.8609e-03, 1.5268e-03, 1.6037e-03, 1.1469e-03, 1.0037e-03,\n",
      "          1.4425e-03]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60632\n",
      "Average episode length: 6.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6114/10000 (61.1%)\n",
      "    Average reward: +0.505\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3886/10000 (38.9%)\n",
      "    Average reward: -0.505\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13865 (44.6%)\n",
      "    Action 1: 14834 (47.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 2418 (7.8%)\n",
      "  Player 1:\n",
      "    Action 0: 11003 (37.3%)\n",
      "    Action 1: 15017 (50.9%)\n",
      "    Action 2: 203 (0.7%)\n",
      "    Action 3: 3292 (11.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5045.5, -5045.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5046\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6204, 0.0088, 0.3708]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9073, 0.0082, 0.0845]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55423\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6132/10000 (61.3%)\n",
      "    Average reward: +0.103\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3868/10000 (38.7%)\n",
      "    Average reward: -0.103\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21941 (74.2%)\n",
      "    Action 1: 7629 (25.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5789 (22.4%)\n",
      "    Action 1: 16953 (65.6%)\n",
      "    Action 2: 148 (0.6%)\n",
      "    Action 3: 2963 (11.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1032.5, -1032.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.824 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.883 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.853\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1032\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7003/50000 [06:07<27:59, 25.60it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 45130/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 47468/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 7999/50000 [06:46<32:46, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 51610/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 53870/2000000\n",
      "P1 SL Buffer Size:  51610\n",
      "P1 SL buffer distribution [20625. 24911.   556.  5518.]\n",
      "P1 actions distribution [0.39963185 0.48267778 0.01077311 0.10691726]\n",
      "P2 SL Buffer Size:  53870\n",
      "P2 SL buffer distribution [20811. 25914.  1042.  6103.]\n",
      "P2 actions distribution [0.38631892 0.48104696 0.01934286 0.11329126]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[2.0859e-04, 4.0735e-04, 5.4205e-04, 5.8684e-04, 8.0179e-04,\n",
      "          9.4503e-04, 7.2056e-04, 7.9345e-03, 1.3241e-02, 5.7412e-03,\n",
      "          3.4837e-03, 1.8124e-03, 8.9063e-03, 1.2539e-02, 2.7762e-03,\n",
      "          3.8481e-03, 1.3676e-03, 9.6302e-02, 1.2740e-01, 2.0558e-02,\n",
      "          7.3580e-03, 1.3374e-03, 3.5422e-03, 4.4496e-03, 7.5334e-04,\n",
      "          3.0653e-01, 8.1283e-04, 6.6666e-03, 6.5250e-03, 1.8185e-03,\n",
      "          9.6410e-03, 2.0086e-02, 6.5139e-02, 4.3819e-02, 1.7780e-03,\n",
      "          3.6878e-03, 5.9614e-03, 1.5361e-02, 1.1446e-02, 4.7084e-03,\n",
      "          1.3770e-02, 4.3411e-02, 7.3492e-02, 3.3949e-02, 5.8510e-04,\n",
      "          5.8385e-04, 6.1136e-04, 6.4936e-04, 6.9762e-04, 4.7999e-04,\n",
      "          2.1956e-04],\n",
      "         [3.0046e-04, 4.4003e-04, 5.3292e-04, 5.0784e-04, 8.0809e-04,\n",
      "          1.0126e-03, 7.5256e-04, 1.3763e-02, 2.0292e-02, 6.2245e-03,\n",
      "          7.8206e-03, 1.7855e-03, 1.8938e-03, 3.0775e-03, 1.6789e-03,\n",
      "          8.5296e-03, 1.5120e-03, 8.3330e-02, 1.1355e-01, 2.0087e-02,\n",
      "          1.8852e-02, 1.1523e-03, 2.4010e-03, 3.2653e-03, 7.1372e-04,\n",
      "          2.2867e-01, 6.4382e-04, 5.8216e-03, 5.2390e-03, 1.4567e-03,\n",
      "          1.9605e-02, 2.3603e-02, 9.1770e-02, 6.9741e-02, 2.1705e-03,\n",
      "          9.8016e-03, 3.7596e-03, 6.2169e-03, 3.1417e-03, 4.3994e-03,\n",
      "          1.6091e-02, 4.8202e-02, 9.1118e-02, 5.0747e-02, 5.9392e-04,\n",
      "          5.5747e-04, 5.6856e-04, 5.0372e-04, 5.1742e-04, 4.7550e-04,\n",
      "          3.0369e-04],\n",
      "         [5.9746e-06, 6.3288e-06, 4.1137e-06, 7.6041e-06, 6.4192e-06,\n",
      "          6.6602e-06, 5.8114e-06, 1.1309e-05, 8.5616e-06, 4.9302e-06,\n",
      "          3.9470e-06, 4.7916e-06, 6.1618e-04, 6.3732e-04, 6.5959e-06,\n",
      "          8.1385e-04, 6.3883e-06, 9.9611e-04, 9.5611e-04, 8.2953e-06,\n",
      "          6.9728e-04, 8.0653e-06, 9.2360e-03, 2.6984e-01, 7.1592e-01,\n",
      "          8.6647e-06, 9.4743e-06, 9.2980e-06, 6.3977e-06, 8.9212e-06,\n",
      "          5.7546e-06, 7.7097e-06, 6.1394e-06, 4.2482e-06, 8.3082e-06,\n",
      "          5.5238e-06, 7.9888e-06, 7.5169e-06, 6.3260e-06, 6.4843e-06,\n",
      "          5.1398e-06, 6.6076e-06, 8.1551e-06, 7.3187e-06, 4.4761e-06,\n",
      "          9.8197e-06, 8.8647e-06, 6.6876e-06, 4.0139e-06, 5.5134e-06,\n",
      "          8.6487e-06],\n",
      "         [2.7334e-04, 5.3612e-04, 4.6416e-04, 5.7011e-04, 8.2678e-04,\n",
      "          6.2320e-04, 5.7797e-04, 7.5095e-03, 7.8541e-03, 4.4726e-03,\n",
      "          2.1483e-03, 1.0506e-03, 2.3421e-03, 2.4874e-03, 1.0577e-03,\n",
      "          1.3599e-03, 9.1371e-04, 9.7106e-02, 1.3678e-01, 1.5862e-02,\n",
      "          2.9375e-03, 8.3836e-04, 1.2978e-02, 1.1258e-02, 5.8100e-04,\n",
      "          3.6762e-01, 7.0933e-04, 1.1689e-02, 1.0080e-02, 9.8952e-04,\n",
      "          3.1960e-03, 2.3424e-02, 6.2794e-02, 5.1005e-02, 1.2018e-03,\n",
      "          2.6518e-03, 3.2504e-03, 7.4804e-03, 5.3669e-03, 2.5653e-03,\n",
      "          9.5341e-03, 3.8963e-02, 5.0412e-02, 2.9249e-02, 5.9265e-04,\n",
      "          7.9284e-04, 5.0596e-04, 6.9578e-04, 7.1989e-04, 7.4370e-04,\n",
      "          3.5954e-04]]])\n",
      "Player 0 Prediction: tensor([[0.2460, 0.7478, 0.0062, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.1845e-04, 3.0827e-04, 4.2142e-04, 4.8137e-04, 7.8575e-04,\n",
      "          7.4828e-04, 5.5795e-04, 1.3242e-02, 1.7021e-02, 3.9818e-03,\n",
      "          1.7054e-03, 8.6689e-04, 4.5427e-03, 6.1760e-03, 1.2131e-03,\n",
      "          1.7086e-03, 7.2516e-04, 1.1499e-01, 1.5029e-01, 1.4323e-02,\n",
      "          1.9600e-03, 6.6581e-04, 1.9408e-03, 2.7686e-03, 4.9385e-04,\n",
      "          1.9021e-01, 4.8099e-04, 2.5012e-03, 2.0133e-03, 8.5137e-04,\n",
      "          2.5357e-03, 1.6056e-02, 8.9378e-02, 7.0896e-02, 5.8674e-04,\n",
      "          1.1947e-03, 2.9805e-03, 1.0790e-02, 7.6653e-03, 1.7191e-03,\n",
      "          6.7760e-03, 4.0191e-02, 1.2830e-01, 7.9883e-02, 4.2082e-04,\n",
      "          4.0899e-04, 4.6604e-04, 4.3618e-04, 4.8130e-04, 3.9667e-04,\n",
      "          2.4528e-04],\n",
      "         [5.2475e-04, 1.1447e-03, 1.2553e-03, 8.5581e-04, 1.3950e-03,\n",
      "          1.3963e-03, 1.0868e-03, 5.0714e-04, 6.0360e-04, 5.8275e-03,\n",
      "          1.2082e-01, 3.9128e-03, 1.2228e-01, 1.2764e-01, 1.7358e-03,\n",
      "          5.1379e-02, 1.3730e-03, 1.1782e-02, 1.3772e-02, 1.1618e-02,\n",
      "          1.1554e-02, 1.1963e-03, 4.2052e-03, 4.3027e-03, 1.1581e-03,\n",
      "          1.8868e-01, 1.0415e-03, 4.3267e-03, 4.5493e-03, 1.6854e-03,\n",
      "          2.3540e-02, 9.9142e-03, 9.9862e-03, 5.8922e-03, 1.1138e-03,\n",
      "          4.9505e-02, 2.2468e-03, 3.9342e-02, 2.6583e-02, 7.2689e-03,\n",
      "          9.1278e-02, 1.4933e-02, 4.7231e-03, 2.4014e-03, 1.4946e-03,\n",
      "          8.6088e-04, 1.2008e-03, 1.0879e-03, 1.2214e-03, 9.6717e-04,\n",
      "          8.3766e-04],\n",
      "         [6.1513e-06, 4.1941e-06, 5.5704e-06, 6.3437e-06, 4.2690e-06,\n",
      "          5.3618e-06, 5.8902e-06, 4.5945e-06, 3.9837e-06, 4.1084e-06,\n",
      "          4.1989e-06, 4.2349e-06, 5.2153e-04, 5.4081e-04, 5.3646e-06,\n",
      "          3.4337e-04, 4.4476e-06, 1.1800e-03, 1.0438e-03, 3.9933e-06,\n",
      "          9.9562e-01, 6.3210e-06, 4.3581e-05, 3.8094e-04, 1.1023e-04,\n",
      "          6.0587e-06, 4.9750e-06, 5.9918e-06, 3.5504e-06, 5.0805e-06,\n",
      "          4.5763e-06, 6.5336e-06, 5.2974e-06, 3.1791e-06, 9.5435e-06,\n",
      "          5.5291e-06, 3.5302e-06, 3.8964e-06, 3.9808e-06, 6.1444e-06,\n",
      "          3.8131e-06, 5.0115e-06, 4.7725e-06, 8.1426e-06, 2.0169e-06,\n",
      "          6.9589e-06, 5.6728e-06, 6.0231e-06, 5.2330e-06, 5.5305e-06,\n",
      "          4.5394e-06],\n",
      "         [3.9057e-04, 4.7550e-04, 3.9267e-04, 5.7142e-04, 5.6021e-04,\n",
      "          5.9457e-04, 5.0511e-04, 9.8244e-03, 8.4401e-03, 5.5562e-03,\n",
      "          2.3490e-03, 1.1013e-03, 1.0254e-03, 1.3422e-03, 8.6002e-04,\n",
      "          9.6823e-04, 6.9006e-04, 4.3283e-02, 5.9014e-02, 1.1293e-02,\n",
      "          4.5468e-02, 7.8389e-04, 2.9572e-03, 2.2928e-03, 4.9959e-04,\n",
      "          5.6394e-01, 6.0608e-04, 1.5877e-03, 1.4015e-03, 9.2576e-04,\n",
      "          3.7493e-02, 7.8347e-03, 3.0919e-02, 2.6407e-02, 1.1227e-03,\n",
      "          2.7624e-03, 2.0697e-03, 3.1720e-03, 1.8896e-03, 2.3021e-03,\n",
      "          1.9141e-02, 2.1871e-02, 3.5820e-02, 3.3293e-02, 4.7355e-04,\n",
      "          6.8172e-04, 5.9139e-04, 5.7118e-04, 8.3969e-04, 7.1396e-04,\n",
      "          3.3305e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3284, 0.0087, 0.6629]])\n",
      "Player 1 Prediction: tensor([[[2.4722e-05, 4.2015e-05, 4.4753e-05, 4.0081e-05, 8.2016e-05,\n",
      "          6.5285e-05, 6.2298e-05, 7.0920e-03, 7.7172e-03, 4.0286e-04,\n",
      "          1.0880e-03, 1.1121e-04, 1.8144e-01, 2.0388e-01, 1.6493e-04,\n",
      "          2.1274e-05, 9.6413e-05, 9.0383e-02, 1.2135e-01, 1.3186e-03,\n",
      "          2.7071e-04, 1.1924e-04, 2.5709e-04, 3.1557e-04, 6.9730e-05,\n",
      "          2.7551e-01, 1.0185e-04, 1.5519e-04, 1.2579e-04, 9.0214e-05,\n",
      "          1.0448e-04, 1.3280e-03, 1.8954e-02, 2.1744e-02, 7.5723e-05,\n",
      "          4.4622e-06, 3.0344e-04, 2.6857e-02, 2.8950e-02, 2.1365e-04,\n",
      "          6.0349e-04, 2.6914e-03, 3.0249e-03, 2.3773e-03, 5.5776e-05,\n",
      "          3.2241e-05, 5.9663e-05, 4.8917e-05, 3.9355e-05, 5.6040e-05,\n",
      "          2.4484e-05],\n",
      "         [4.5075e-04, 7.6362e-04, 6.1692e-04, 6.4585e-04, 1.2634e-03,\n",
      "          1.2680e-03, 8.7750e-04, 2.5976e-02, 3.3692e-02, 5.0524e-03,\n",
      "          9.8318e-03, 1.9672e-03, 6.3939e-02, 6.0092e-02, 8.8215e-04,\n",
      "          8.0041e-04, 1.0367e-03, 4.5625e-02, 6.0394e-02, 1.5175e-02,\n",
      "          1.3325e-02, 8.2973e-04, 5.6834e-03, 6.8638e-03, 5.7989e-04,\n",
      "          3.4709e-01, 8.5464e-04, 5.8365e-03, 5.1963e-03, 1.4895e-03,\n",
      "          8.9719e-03, 1.3083e-02, 4.5882e-02, 3.6982e-02, 7.9611e-04,\n",
      "          5.9679e-04, 1.7212e-03, 2.5586e-02, 2.8019e-02, 2.3033e-03,\n",
      "          5.7370e-03, 4.0585e-02, 4.1400e-02, 2.5707e-02, 7.3618e-04,\n",
      "          5.8784e-04, 7.6251e-04, 7.5314e-04, 7.1292e-04, 5.7339e-04,\n",
      "          4.0868e-04],\n",
      "         [1.4008e-05, 9.6638e-06, 1.2666e-05, 1.6066e-05, 1.2714e-05,\n",
      "          1.3598e-05, 1.0499e-05, 1.6100e-05, 1.0667e-05, 1.0368e-05,\n",
      "          1.7547e-05, 7.6506e-06, 1.4216e-03, 1.3239e-03, 1.5341e-05,\n",
      "          1.6973e-03, 1.3498e-05, 4.8040e-01, 5.1034e-01, 1.4548e-05,\n",
      "          1.4224e-03, 1.5986e-05, 3.5538e-04, 1.7238e-03, 6.7989e-04,\n",
      "          1.2494e-05, 2.4500e-05, 1.2247e-05, 1.0524e-05, 2.6886e-05,\n",
      "          1.6039e-05, 1.4909e-05, 1.6373e-05, 1.2406e-05, 2.4658e-05,\n",
      "          2.2464e-05, 1.1343e-05, 1.3848e-05, 2.2190e-05, 1.9077e-05,\n",
      "          9.6050e-06, 1.3863e-05, 1.2596e-05, 1.6982e-05, 1.0875e-05,\n",
      "          2.6446e-05, 1.7600e-05, 1.6962e-05, 1.2422e-05, 1.1134e-05,\n",
      "          1.8999e-05],\n",
      "         [7.3656e-05, 1.1584e-04, 1.3388e-04, 1.8020e-04, 2.4675e-04,\n",
      "          2.1575e-04, 1.7960e-04, 6.3747e-03, 6.9219e-03, 1.9989e-03,\n",
      "          4.1206e-04, 2.3998e-04, 3.2331e-04, 4.5836e-04, 3.2039e-04,\n",
      "          2.3658e-04, 2.4317e-04, 1.6330e-01, 1.8195e-01, 6.2511e-03,\n",
      "          5.2894e-04, 1.5832e-04, 1.2129e-03, 1.3324e-03, 1.6313e-04,\n",
      "          3.6959e-01, 1.8272e-04, 9.4445e-04, 6.0514e-04, 1.9456e-04,\n",
      "          4.0002e-04, 7.7195e-03, 7.7487e-02, 6.1428e-02, 2.7914e-04,\n",
      "          3.6364e-04, 1.2100e-03, 1.8977e-03, 1.0815e-03, 5.8280e-04,\n",
      "          3.4593e-03, 2.0206e-02, 4.5698e-02, 3.2124e-02, 1.2610e-04,\n",
      "          1.8133e-04, 1.5343e-04, 1.3307e-04, 1.7137e-04, 1.2921e-04,\n",
      "          8.2910e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 7999/50000 [06:58<32:46, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58167\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5948/10000 (59.5%)\n",
      "    Average reward: -0.736\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4052/10000 (40.5%)\n",
      "    Average reward: +0.736\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9908 (35.3%)\n",
      "    Action 1: 15390 (54.8%)\n",
      "    Action 2: 128 (0.5%)\n",
      "    Action 3: 2645 (9.4%)\n",
      "  Player 1:\n",
      "    Action 0: 11278 (37.5%)\n",
      "    Action 1: 14821 (49.2%)\n",
      "    Action 2: 1860 (6.2%)\n",
      "    Action 3: 2137 (7.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7363.5, 7363.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.020\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7363\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.6974, 0.2995, 0.0031, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2043, 0.7923, 0.0034, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.3282, 0.6655, 0.0063, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53306\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5956/10000 (59.6%)\n",
      "    Average reward: -0.049\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4044/10000 (40.4%)\n",
      "    Average reward: +0.049\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5286 (21.1%)\n",
      "    Action 1: 16696 (66.6%)\n",
      "    Action 2: 147 (0.6%)\n",
      "    Action 3: 2947 (11.8%)\n",
      "  Player 1:\n",
      "    Action 0: 21666 (76.7%)\n",
      "    Action 1: 6564 (23.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-494.5, 494.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.864 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.782 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.823\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0495\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7256, 0.2557, 0.0187, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.1854e-04, 3.2968e-04, 3.3489e-04, 3.5855e-04, 4.6652e-04,\n",
      "          3.0534e-04, 3.9922e-04, 2.6642e-03, 4.4713e-03, 4.7766e-03,\n",
      "          3.5728e-02, 1.3571e-02, 3.5322e-03, 4.2341e-03, 2.3673e-03,\n",
      "          1.3027e-02, 1.7280e-03, 9.0728e-02, 1.0369e-01, 1.2999e-02,\n",
      "          1.9548e-02, 5.1679e-03, 1.6816e-03, 2.2387e-03, 4.6438e-04,\n",
      "          2.1931e-01, 3.7721e-04, 1.9396e-03, 1.6443e-03, 3.2004e-03,\n",
      "          3.1487e-02, 6.6718e-03, 9.6216e-02, 7.3610e-02, 4.4243e-03,\n",
      "          3.8003e-02, 6.8452e-03, 1.3079e-02, 1.3958e-02, 3.0653e-02,\n",
      "          7.2257e-02, 1.8585e-02, 2.7664e-02, 1.2031e-02, 4.5496e-04,\n",
      "          6.1347e-04, 5.5757e-04, 3.6468e-04, 4.3112e-04, 3.0268e-04,\n",
      "          2.8719e-04],\n",
      "         [1.7221e-04, 2.8433e-04, 3.1087e-04, 3.4086e-04, 3.3296e-04,\n",
      "          3.2804e-04, 3.6587e-04, 1.2344e-02, 1.7511e-02, 9.4147e-03,\n",
      "          1.2165e-02, 3.8945e-03, 8.8392e-03, 9.8053e-03, 3.4146e-03,\n",
      "          4.5975e-03, 9.6861e-04, 7.9470e-02, 7.9063e-02, 1.2729e-02,\n",
      "          5.6103e-03, 1.3577e-03, 1.6668e-03, 2.7918e-03, 5.2897e-04,\n",
      "          1.8395e-01, 3.3733e-04, 3.4781e-03, 3.3692e-03, 9.7831e-04,\n",
      "          7.1417e-03, 1.4160e-02, 8.8481e-02, 7.9304e-02, 2.0490e-03,\n",
      "          1.6946e-02, 7.0877e-03, 3.2652e-02, 2.7092e-02, 8.7063e-03,\n",
      "          3.9562e-02, 4.0977e-02, 1.0106e-01, 7.2181e-02, 3.8632e-04,\n",
      "          3.4216e-04, 3.2355e-04, 3.1564e-04, 2.8804e-04, 3.0994e-04,\n",
      "          2.1511e-04],\n",
      "         [4.5928e-06, 3.6914e-06, 4.0544e-06, 5.3253e-06, 6.0294e-06,\n",
      "          6.1404e-06, 4.6420e-06, 2.5187e-06, 4.3894e-06, 4.7615e-06,\n",
      "          3.1102e-06, 4.5505e-06, 4.3446e-04, 6.4194e-04, 2.4180e-06,\n",
      "          7.1015e-04, 6.1477e-06, 3.5316e-04, 3.9518e-04, 2.8906e-06,\n",
      "          1.7925e-04, 2.4534e-06, 4.6487e-01, 5.2530e-01, 6.9397e-03,\n",
      "          3.7458e-06, 4.2617e-06, 4.4941e-06, 4.6649e-06, 4.2416e-06,\n",
      "          4.5389e-06, 4.1515e-06, 6.4366e-06, 2.3764e-06, 5.0366e-06,\n",
      "          4.6428e-06, 3.5649e-06, 3.9330e-06, 4.0480e-06, 5.6705e-06,\n",
      "          6.2701e-06, 4.3874e-06, 3.2881e-06, 4.9447e-06, 3.6836e-06,\n",
      "          2.7715e-06, 2.4135e-06, 4.5529e-06, 3.4461e-06, 3.4199e-06,\n",
      "          4.3508e-06],\n",
      "         [4.1053e-04, 5.2866e-04, 6.0759e-04, 5.0535e-04, 6.5498e-04,\n",
      "          5.1012e-04, 6.4638e-04, 7.4946e-03, 1.0138e-02, 4.9333e-03,\n",
      "          7.3543e-03, 4.0429e-03, 9.6912e-03, 1.1163e-02, 1.5418e-03,\n",
      "          2.0622e-03, 1.0796e-03, 7.7449e-02, 9.9012e-02, 1.5857e-02,\n",
      "          2.6803e-03, 1.3817e-03, 1.6172e-02, 2.0292e-02, 7.5569e-04,\n",
      "          2.0921e-01, 6.2484e-04, 2.6860e-02, 2.5587e-02, 1.1327e-03,\n",
      "          3.0758e-03, 9.5523e-03, 1.4832e-01, 1.2060e-01, 1.9193e-03,\n",
      "          2.9040e-03, 3.0362e-03, 1.8193e-02, 1.8867e-02, 5.2308e-03,\n",
      "          1.1319e-02, 2.1604e-02, 4.4552e-02, 2.6197e-02, 6.8815e-04,\n",
      "          6.3968e-04, 7.2404e-04, 5.5033e-04, 5.0062e-04, 6.4610e-04,\n",
      "          4.9781e-04]]])\n",
      "Player 1 Prediction: tensor([[0.1065, 0.8889, 0.0046, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.3337e-04, 1.8805e-04, 2.0286e-04, 1.8067e-04, 1.7589e-04,\n",
      "          1.8483e-04, 2.0361e-04, 1.7390e-02, 2.1293e-02, 6.7965e-03,\n",
      "          3.2116e-03, 1.5200e-03, 9.7429e-03, 1.0445e-02, 2.1986e-03,\n",
      "          1.4632e-03, 4.2056e-04, 9.3888e-02, 8.8005e-02, 8.2361e-03,\n",
      "          2.2847e-04, 5.3898e-04, 7.2254e-04, 1.7581e-03, 3.0943e-04,\n",
      "          1.6305e-01, 2.0281e-04, 6.9126e-04, 6.5515e-04, 4.2181e-04,\n",
      "          2.7332e-04, 9.7104e-03, 1.0164e-01, 9.6529e-02, 7.6540e-04,\n",
      "          2.1266e-03, 4.9726e-03, 3.2623e-02, 2.9205e-02, 2.8872e-03,\n",
      "          6.7668e-03, 3.0701e-02, 1.3567e-01, 1.1031e-01, 1.9579e-04,\n",
      "          2.6105e-04, 2.2669e-04, 1.7949e-04, 1.5656e-04, 2.0007e-04,\n",
      "          1.5357e-04],\n",
      "         [3.8545e-04, 4.7985e-04, 5.9647e-04, 7.5802e-04, 5.7687e-04,\n",
      "          5.0496e-04, 7.0215e-04, 5.7018e-04, 7.5108e-04, 6.7600e-03,\n",
      "          1.2510e-01, 1.4055e-02, 4.7911e-02, 4.3169e-02, 2.7539e-03,\n",
      "          5.5494e-02, 1.2939e-03, 8.4854e-03, 1.0650e-02, 6.7823e-03,\n",
      "          7.3640e-03, 1.7440e-03, 4.3231e-03, 5.6842e-03, 8.3606e-04,\n",
      "          6.1727e-02, 5.9716e-04, 2.7084e-03, 2.4534e-03, 1.2714e-03,\n",
      "          1.0173e-02, 7.8133e-03, 2.0126e-02, 1.6353e-02, 1.4735e-03,\n",
      "          1.6913e-01, 2.3521e-03, 2.5547e-02, 3.5374e-02, 2.7700e-02,\n",
      "          2.4709e-01, 1.2645e-02, 1.9083e-03, 1.3708e-03, 7.9487e-04,\n",
      "          7.9273e-04, 4.9110e-04, 6.0882e-04, 5.4878e-04, 7.8527e-04,\n",
      "          4.2859e-04],\n",
      "         [3.1733e-06, 2.7374e-06, 1.7585e-06, 3.1977e-06, 5.8173e-06,\n",
      "          3.7757e-06, 3.8161e-06, 2.9004e-06, 1.5175e-06, 3.4087e-06,\n",
      "          2.3368e-06, 2.5432e-06, 5.8136e-04, 4.6444e-04, 1.6102e-06,\n",
      "          1.6023e-04, 3.0115e-06, 1.0661e-03, 6.7637e-04, 3.0509e-06,\n",
      "          9.9674e-01, 2.1476e-06, 1.6015e-05, 1.0455e-04, 7.5012e-05,\n",
      "          2.6363e-06, 2.8892e-06, 2.1979e-06, 3.1123e-06, 2.0468e-06,\n",
      "          1.9978e-06, 1.9784e-06, 2.9023e-06, 2.4767e-06, 1.8958e-06,\n",
      "          3.3471e-06, 1.9829e-06, 1.8119e-06, 3.3055e-06, 1.6536e-06,\n",
      "          3.6039e-06, 2.4491e-06, 1.7184e-06, 3.8715e-06, 2.5692e-06,\n",
      "          2.2780e-06, 1.9938e-06, 2.7008e-06, 2.5927e-06, 3.1451e-06,\n",
      "          2.7658e-06],\n",
      "         [6.7433e-04, 6.6465e-04, 7.0789e-04, 8.6473e-04, 1.0147e-03,\n",
      "          7.6478e-04, 6.0646e-04, 1.1400e-02, 1.7752e-02, 5.5784e-03,\n",
      "          2.6384e-02, 7.2101e-03, 3.6432e-03, 4.0816e-03, 1.7052e-03,\n",
      "          5.9821e-03, 1.8170e-03, 5.1741e-02, 6.6497e-02, 8.4641e-03,\n",
      "          1.7472e-02, 3.3945e-03, 2.2768e-03, 2.2468e-03, 1.0610e-03,\n",
      "          3.3683e-01, 8.1624e-04, 1.5955e-03, 1.7739e-03, 1.6741e-03,\n",
      "          1.6292e-02, 7.1119e-03, 8.0624e-02, 8.9516e-02, 2.9130e-03,\n",
      "          9.0681e-03, 1.4784e-03, 5.5181e-03, 1.0019e-02, 1.4526e-02,\n",
      "          3.9593e-02, 2.5943e-02, 7.0210e-02, 3.4536e-02, 7.9086e-04,\n",
      "          1.2125e-03, 7.2676e-04, 8.1988e-04, 7.8390e-04, 5.2115e-04,\n",
      "          1.1054e-03]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2290, 0.0127, 0.7582]])\n",
      "Player 0 Prediction: tensor([[[2.4118e-06, 4.7176e-06, 5.4641e-06, 2.6293e-06, 4.0357e-06,\n",
      "          4.6528e-06, 4.3028e-06, 3.4291e-04, 4.2918e-04, 7.1231e-05,\n",
      "          5.7427e-04, 2.8308e-05, 2.7359e-01, 2.7188e-01, 1.9761e-05,\n",
      "          2.3581e-04, 9.5017e-06, 3.0105e-04, 3.2774e-04, 4.0418e-05,\n",
      "          5.4849e-06, 2.3996e-05, 1.1316e-05, 8.4888e-06, 4.8531e-06,\n",
      "          2.9115e-01, 3.2640e-06, 5.2408e-06, 3.7646e-06, 7.0389e-06,\n",
      "          4.3192e-06, 3.2308e-05, 2.0541e-04, 1.4959e-04, 1.7168e-05,\n",
      "          8.8003e-05, 7.3337e-05, 7.9374e-02, 8.0408e-02, 6.0161e-05,\n",
      "          1.4367e-04, 1.7244e-04, 7.0003e-05, 7.7314e-05, 5.0697e-06,\n",
      "          2.0481e-06, 3.0660e-06, 3.6261e-06, 4.2616e-06, 3.9219e-06,\n",
      "          3.0183e-06],\n",
      "         [5.6093e-06, 9.0727e-06, 8.4833e-06, 6.8295e-06, 9.6065e-06,\n",
      "          1.1754e-05, 9.1774e-06, 3.2543e-01, 3.0993e-01, 1.7686e-04,\n",
      "          3.0139e-04, 6.5808e-05, 7.1644e-05, 5.6727e-05, 4.4690e-05,\n",
      "          3.9234e-05, 1.8044e-05, 2.0530e-03, 3.0761e-03, 1.8338e-04,\n",
      "          2.2335e-05, 1.1151e-05, 8.1317e-05, 9.6197e-05, 1.1246e-05,\n",
      "          2.6905e-01, 5.8495e-06, 6.0549e-05, 3.7639e-05, 1.6783e-05,\n",
      "          1.4139e-05, 1.9332e-04, 1.0559e-03, 1.0838e-03, 3.8622e-05,\n",
      "          2.2076e-05, 8.0375e-05, 6.8087e-04, 7.3962e-04, 1.1757e-04,\n",
      "          1.1624e-04, 4.0011e-04, 4.2845e-02, 4.1648e-02, 1.2996e-05,\n",
      "          7.0938e-06, 8.5696e-06, 1.3069e-05, 8.4891e-06, 6.3469e-06,\n",
      "          6.8698e-06],\n",
      "         [4.9733e-06, 4.7869e-06, 4.9528e-06, 5.2488e-06, 3.8789e-06,\n",
      "          1.2152e-05, 3.9904e-06, 4.7821e-06, 8.2916e-06, 8.5138e-06,\n",
      "          7.8739e-06, 7.7386e-06, 8.5878e-04, 8.5325e-04, 2.7372e-06,\n",
      "          1.3149e-03, 1.0743e-05, 5.3129e-01, 4.6423e-01, 8.0494e-06,\n",
      "          4.2787e-05, 7.3434e-06, 2.9958e-04, 5.5808e-04, 2.3873e-04,\n",
      "          6.4829e-06, 5.3803e-06, 8.4290e-06, 1.3135e-05, 1.1402e-05,\n",
      "          4.2285e-06, 6.4978e-06, 1.3404e-05, 5.8535e-06, 5.8984e-06,\n",
      "          1.3429e-05, 5.5177e-06, 4.0681e-06, 9.9011e-06, 8.4985e-06,\n",
      "          8.4713e-06, 9.8575e-06, 8.4948e-06, 7.1797e-06, 6.5378e-06,\n",
      "          6.2366e-06, 6.7041e-06, 3.5889e-06, 1.0605e-05, 4.0796e-06,\n",
      "          1.4012e-05],\n",
      "         [1.7449e-04, 2.2896e-04, 1.8475e-04, 1.4428e-04, 1.6669e-04,\n",
      "          1.0826e-04, 2.4706e-04, 1.2816e-02, 7.5299e-03, 3.3427e-03,\n",
      "          1.5480e-03, 9.8236e-04, 2.1904e-03, 2.2683e-03, 4.2324e-04,\n",
      "          7.0512e-04, 3.5012e-04, 1.8327e-01, 1.2403e-01, 4.0146e-03,\n",
      "          3.5151e-04, 5.6903e-04, 1.7945e-03, 5.2607e-03, 4.6463e-04,\n",
      "          2.8778e-01, 1.9481e-04, 2.3037e-03, 2.4241e-03, 3.4727e-04,\n",
      "          3.6446e-04, 3.3529e-03, 7.0360e-02, 7.1483e-02, 4.3080e-04,\n",
      "          1.2592e-03, 6.1875e-04, 4.4700e-03, 4.9606e-03, 1.3615e-03,\n",
      "          3.5438e-03, 1.0460e-02, 8.3121e-02, 9.6601e-02, 2.2115e-04,\n",
      "          2.5736e-04, 2.2079e-04, 1.2979e-04, 1.3426e-04, 2.5411e-04,\n",
      "          1.8072e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57946\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5920/10000 (59.2%)\n",
      "    Average reward: +0.801\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4080/10000 (40.8%)\n",
      "    Average reward: -0.801\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 12127 (40.7%)\n",
      "    Action 1: 14349 (48.2%)\n",
      "    Action 2: 1234 (4.1%)\n",
      "    Action 3: 2059 (6.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8583 (30.5%)\n",
      "    Action 1: 15844 (56.2%)\n",
      "    Action 2: 251 (0.9%)\n",
      "    Action 3: 3499 (12.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8012.5, -8012.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.989 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.012\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8013\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7256, 0.2557, 0.0187, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.1065, 0.8889, 0.0046, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.2270, 0.7607, 0.0123, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53968\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6059/10000 (60.6%)\n",
      "    Average reward: +0.060\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3941/10000 (39.4%)\n",
      "    Average reward: -0.060\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22185 (76.9%)\n",
      "    Action 1: 6659 (23.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5145 (20.5%)\n",
      "    Action 1: 17188 (68.4%)\n",
      "    Action 2: 215 (0.9%)\n",
      "    Action 3: 2576 (10.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [602.5, -602.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.780 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.843 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.811\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0602\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9003/50000 [08:01<26:48, 25.49it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 58065/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 60303/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9999/50000 [08:40<26:28, 25.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 64330/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 66973/2000000\n",
      "P1 SL Buffer Size:  64330\n",
      "P1 SL buffer distribution [25263. 30909.  1576.  6582.]\n",
      "P1 actions distribution [0.39270947 0.48047567 0.02449868 0.10231618]\n",
      "P2 SL Buffer Size:  66973\n",
      "P2 SL buffer distribution [25073. 32399.  2180.  7321.]\n",
      "P2 actions distribution [0.37437475 0.48376211 0.03255043 0.10931271]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5848, 0.4099, 0.0053, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[8.4465e-05, 1.9256e-04, 1.7405e-04, 2.2242e-04, 2.9289e-04,\n",
      "          3.5555e-04, 2.3783e-04, 1.9045e-03, 2.8010e-03, 1.6182e-03,\n",
      "          4.8912e-03, 2.4862e-03, 1.8087e-03, 2.2100e-03, 1.1926e-03,\n",
      "          9.6713e-03, 1.1613e-03, 9.1634e-02, 1.0836e-01, 1.9583e-02,\n",
      "          2.8371e-02, 1.4471e-03, 1.7970e-03, 1.8305e-03, 3.0811e-04,\n",
      "          4.8110e-01, 2.5146e-04, 2.0817e-03, 2.0833e-03, 3.4716e-03,\n",
      "          3.2976e-02, 7.5962e-03, 3.9712e-02, 2.9136e-02, 2.0829e-03,\n",
      "          1.0176e-02, 2.3975e-03, 3.0000e-03, 1.9919e-03, 8.1237e-03,\n",
      "          2.7682e-02, 1.6541e-02, 2.5829e-02, 1.7461e-02, 2.4697e-04,\n",
      "          2.6669e-04, 2.7620e-04, 2.6508e-04, 2.9776e-04, 2.2426e-04,\n",
      "          9.7489e-05],\n",
      "         [1.1988e-04, 1.8902e-04, 2.5510e-04, 2.4032e-04, 3.1884e-04,\n",
      "          3.7170e-04, 3.1689e-04, 4.4278e-03, 6.8129e-03, 2.7259e-03,\n",
      "          3.8347e-03, 8.4642e-04, 6.3738e-03, 7.7885e-03, 1.2099e-03,\n",
      "          4.1638e-03, 6.2634e-04, 9.8526e-02, 1.3674e-01, 2.1993e-02,\n",
      "          9.3520e-03, 4.6147e-04, 1.3574e-03, 1.7480e-03, 2.4099e-04,\n",
      "          2.9189e-01, 2.6510e-04, 3.5872e-03, 2.8818e-03, 6.2895e-04,\n",
      "          6.8332e-03, 1.8213e-02, 9.4931e-02, 5.5347e-02, 1.0945e-03,\n",
      "          4.7868e-03, 3.1696e-03, 9.3578e-03, 6.9031e-03, 2.1543e-03,\n",
      "          1.1311e-02, 3.7999e-02, 8.5249e-02, 5.0953e-02, 2.2567e-04,\n",
      "          2.0069e-04, 2.5154e-04, 1.9682e-04, 2.2043e-04, 2.0603e-04,\n",
      "          1.0322e-04],\n",
      "         [1.0391e-06, 9.3875e-07, 8.1492e-07, 1.3794e-06, 1.2380e-06,\n",
      "          1.3487e-06, 1.2320e-06, 1.9977e-06, 1.2845e-06, 1.0846e-06,\n",
      "          1.0208e-06, 7.0769e-07, 8.9292e-05, 1.1845e-04, 9.1803e-07,\n",
      "          2.2042e-04, 9.5142e-07, 8.9223e-05, 7.9606e-05, 1.3781e-06,\n",
      "          1.0802e-04, 1.1646e-06, 4.4867e-01, 5.4655e-01, 4.0283e-03,\n",
      "          2.1871e-06, 1.7065e-06, 1.5277e-06, 1.1559e-06, 1.2452e-06,\n",
      "          1.0256e-06, 1.3463e-06, 1.0366e-06, 4.7983e-07, 1.2481e-06,\n",
      "          5.5787e-07, 1.3090e-06, 9.9573e-07, 1.2841e-06, 1.2520e-06,\n",
      "          7.3849e-07, 9.7832e-07, 1.5858e-06, 1.4162e-06, 1.0463e-06,\n",
      "          1.6416e-06, 1.7872e-06, 9.6250e-07, 5.8665e-07, 1.1016e-06,\n",
      "          1.5206e-06],\n",
      "         [1.7201e-04, 2.6498e-04, 1.8896e-04, 3.3013e-04, 2.5313e-04,\n",
      "          3.4155e-04, 1.9938e-04, 2.4982e-03, 3.3021e-03, 2.5063e-03,\n",
      "          1.5464e-03, 6.3740e-04, 2.3818e-03, 2.8715e-03, 6.7515e-04,\n",
      "          6.9495e-04, 4.7894e-04, 7.6368e-02, 8.4184e-02, 1.4579e-02,\n",
      "          2.8265e-03, 5.2925e-04, 1.4112e-02, 1.3377e-02, 3.4164e-04,\n",
      "          5.0311e-01, 3.1692e-04, 1.1914e-02, 1.0318e-02, 4.8979e-04,\n",
      "          1.7857e-03, 1.4626e-02, 8.9097e-02, 4.8328e-02, 6.6134e-04,\n",
      "          1.4821e-03, 2.1693e-03, 9.3226e-03, 8.4415e-03, 2.0005e-03,\n",
      "          8.5508e-03, 1.6080e-02, 2.5732e-02, 1.7766e-02, 3.0196e-04,\n",
      "          4.0585e-04, 3.2798e-04, 2.9860e-04, 3.3600e-04, 3.1329e-04,\n",
      "          1.6237e-04]]])\n",
      "Player 0 Prediction: tensor([[0.2122, 0.7819, 0.0058, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.0609e-04, 1.4850e-04, 2.0585e-04, 2.2981e-04, 3.7496e-04,\n",
      "          3.6070e-04, 2.6536e-04, 6.7837e-03, 8.2573e-03, 1.8423e-03,\n",
      "          1.1216e-03, 4.2394e-04, 4.2368e-03, 5.4808e-03, 8.1710e-04,\n",
      "          9.6048e-04, 3.6406e-04, 1.0783e-01, 1.5496e-01, 1.9197e-02,\n",
      "          1.2384e-03, 3.4432e-04, 1.3186e-03, 1.7660e-03, 2.4733e-04,\n",
      "          2.5619e-01, 2.3340e-04, 1.9281e-03, 1.5596e-03, 4.3485e-04,\n",
      "          1.3201e-03, 1.3233e-02, 1.0460e-01, 6.2204e-02, 3.0720e-04,\n",
      "          5.4069e-04, 2.1198e-03, 1.0120e-02, 6.9979e-03, 1.0508e-03,\n",
      "          6.3138e-03, 3.3835e-02, 1.0179e-01, 7.4958e-02, 2.0803e-04,\n",
      "          1.9369e-04, 2.2551e-04, 2.0505e-04, 2.3293e-04, 2.0106e-04,\n",
      "          1.2154e-04],\n",
      "         [2.6097e-04, 5.5828e-04, 6.3381e-04, 4.1282e-04, 6.6971e-04,\n",
      "          6.9545e-04, 5.3484e-04, 1.5346e-04, 1.7046e-04, 2.7130e-03,\n",
      "          7.5879e-02, 2.7560e-03, 1.4853e-01, 1.4976e-01, 9.0759e-04,\n",
      "          4.3229e-02, 6.9226e-04, 5.3863e-03, 6.9356e-03, 1.0659e-02,\n",
      "          8.3546e-03, 6.2804e-04, 2.7269e-03, 2.9027e-03, 5.8221e-04,\n",
      "          2.5831e-01, 5.0401e-04, 2.9151e-03, 3.4351e-03, 9.2001e-04,\n",
      "          3.1988e-02, 5.8635e-03, 6.1771e-03, 2.9479e-03, 5.1461e-04,\n",
      "          3.7083e-02, 1.1281e-03, 3.1495e-02, 2.3583e-02, 6.0918e-03,\n",
      "          1.0442e-01, 9.0234e-03, 1.9266e-03, 1.2469e-03, 7.0056e-04,\n",
      "          4.1840e-04, 5.8671e-04, 5.0116e-04, 5.9573e-04, 4.9382e-04,\n",
      "          4.0831e-04],\n",
      "         [3.8422e-06, 2.6241e-06, 3.4453e-06, 4.0336e-06, 2.6661e-06,\n",
      "          3.2807e-06, 3.7372e-06, 2.8720e-06, 2.4983e-06, 2.5492e-06,\n",
      "          2.6045e-06, 2.6261e-06, 3.4165e-04, 3.7112e-04, 3.3214e-06,\n",
      "          2.4104e-04, 2.6585e-06, 9.3577e-04, 8.6831e-04, 2.5325e-06,\n",
      "          9.9666e-01, 4.1701e-06, 2.9587e-05, 3.2417e-04, 9.8749e-05,\n",
      "          3.6535e-06, 3.2394e-06, 3.8225e-06, 2.1938e-06, 3.1021e-06,\n",
      "          2.8833e-06, 4.0731e-06, 3.3981e-06, 1.9397e-06, 6.1117e-06,\n",
      "          3.4437e-06, 2.1619e-06, 2.3641e-06, 2.4893e-06, 3.8555e-06,\n",
      "          2.2480e-06, 3.1912e-06, 2.9764e-06, 5.2538e-06, 1.1779e-06,\n",
      "          4.4864e-06, 3.6173e-06, 3.7487e-06, 3.2888e-06, 3.4630e-06,\n",
      "          2.8473e-06],\n",
      "         [2.1978e-04, 2.7595e-04, 2.1220e-04, 3.1716e-04, 3.0621e-04,\n",
      "          3.5239e-04, 2.8852e-04, 5.2426e-03, 4.3922e-03, 3.2801e-03,\n",
      "          1.6404e-03, 6.4759e-04, 7.5905e-04, 9.7077e-04, 5.0950e-04,\n",
      "          5.6193e-04, 4.1926e-04, 3.3078e-02, 4.7307e-02, 1.4661e-02,\n",
      "          3.2398e-02, 4.8441e-04, 1.9002e-03, 1.4809e-03, 2.9092e-04,\n",
      "          6.5587e-01, 3.4475e-04, 9.7976e-04, 8.4445e-04, 5.4493e-04,\n",
      "          2.7183e-02, 7.1057e-03, 3.4220e-02, 2.5297e-02, 6.4460e-04,\n",
      "          1.6480e-03, 1.3990e-03, 2.9198e-03, 1.7909e-03, 1.7652e-03,\n",
      "          1.8062e-02, 1.6573e-02, 2.3046e-02, 2.5340e-02, 2.7432e-04,\n",
      "          4.1143e-04, 3.2629e-04, 3.2740e-04, 4.9528e-04, 4.1149e-04,\n",
      "          1.8518e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6446, 0.0169, 0.3385]])\n",
      "Player 1 Prediction: tensor([[[1.0598e-05, 1.6833e-05, 1.8420e-05, 1.5033e-05, 3.0062e-05,\n",
      "          2.7857e-05, 2.5098e-05, 4.6871e-03, 5.2327e-03, 1.3360e-04,\n",
      "          6.6076e-04, 3.4586e-05, 2.6185e-01, 2.7781e-01, 8.0567e-05,\n",
      "          6.9947e-06, 3.1826e-05, 6.8652e-02, 9.5365e-02, 1.1207e-03,\n",
      "          9.1111e-05, 4.1122e-05, 1.2605e-04, 1.3467e-04, 2.4632e-05,\n",
      "          2.2938e-01, 3.7891e-05, 8.3192e-05, 7.4997e-05, 2.9759e-05,\n",
      "          3.0082e-05, 7.1626e-04, 1.4086e-02, 1.3478e-02, 2.5077e-05,\n",
      "          5.9613e-07, 1.3827e-04, 1.0444e-02, 1.2312e-02, 7.7056e-05,\n",
      "          2.1818e-04, 1.3656e-03, 6.5168e-04, 4.9707e-04, 2.2941e-05,\n",
      "          1.1380e-05, 1.8819e-05, 1.8204e-05, 1.7186e-05, 2.0915e-05,\n",
      "          1.1251e-05],\n",
      "         [2.9909e-04, 4.9412e-04, 4.5111e-04, 4.4916e-04, 7.3364e-04,\n",
      "          9.7553e-04, 5.4410e-04, 1.7717e-02, 2.3118e-02, 2.6556e-03,\n",
      "          7.1866e-03, 1.2259e-03, 1.2483e-01, 1.1817e-01, 6.1417e-04,\n",
      "          7.6860e-04, 6.8610e-04, 3.0983e-02, 5.0425e-02, 1.6585e-02,\n",
      "          1.0639e-02, 6.5677e-04, 5.0440e-03, 6.8001e-03, 4.2283e-04,\n",
      "          4.0246e-01, 4.4146e-04, 5.2476e-03, 4.4397e-03, 9.8452e-04,\n",
      "          7.6260e-03, 1.0997e-02, 3.4700e-02, 2.1119e-02, 3.7015e-04,\n",
      "          2.0383e-04, 1.0645e-03, 1.2721e-02, 1.2826e-02, 1.3151e-03,\n",
      "          3.1784e-03, 3.2922e-02, 1.3487e-02, 8.6794e-03, 4.7619e-04,\n",
      "          3.6814e-04, 4.7912e-04, 3.5851e-04, 4.8850e-04, 3.4899e-04,\n",
      "          2.2055e-04],\n",
      "         [1.3676e-05, 1.0403e-05, 9.6230e-06, 1.4481e-05, 1.1728e-05,\n",
      "          1.1703e-05, 9.5277e-06, 1.4131e-05, 1.0212e-05, 8.5699e-06,\n",
      "          1.7458e-05, 6.5141e-06, 9.8718e-04, 1.1281e-03, 1.3582e-05,\n",
      "          9.4469e-04, 1.0446e-05, 5.0100e-01, 4.9213e-01, 1.2092e-05,\n",
      "          1.3506e-03, 1.4188e-05, 1.4087e-04, 1.0674e-03, 6.7632e-04,\n",
      "          9.9615e-06, 2.7249e-05, 1.3725e-05, 8.5106e-06, 2.3597e-05,\n",
      "          1.6005e-05, 1.1678e-05, 1.5384e-05, 1.0605e-05, 2.2879e-05,\n",
      "          1.7335e-05, 1.0194e-05, 1.1148e-05, 1.8574e-05, 1.7873e-05,\n",
      "          7.6435e-06, 1.6621e-05, 1.1049e-05, 1.3968e-05, 8.5553e-06,\n",
      "          2.4521e-05, 1.4768e-05, 1.4796e-05, 1.3972e-05, 1.1821e-05,\n",
      "          1.7049e-05],\n",
      "         [5.1833e-05, 7.3911e-05, 7.9829e-05, 1.2082e-04, 1.8342e-04,\n",
      "          1.6439e-04, 1.1438e-04, 3.4945e-03, 4.0524e-03, 1.1532e-03,\n",
      "          2.6045e-04, 1.7532e-04, 3.0774e-04, 3.1260e-04, 1.9459e-04,\n",
      "          2.0225e-04, 1.7167e-04, 1.3719e-01, 1.7592e-01, 1.1362e-02,\n",
      "          3.4824e-04, 1.1273e-04, 6.4479e-04, 8.2275e-04, 1.0030e-04,\n",
      "          4.5667e-01, 1.1892e-04, 5.3485e-04, 3.3233e-04, 1.3679e-04,\n",
      "          2.8364e-04, 9.8658e-03, 7.7065e-02, 4.3073e-02, 1.8568e-04,\n",
      "          1.9427e-04, 8.9648e-04, 2.2594e-03, 9.7273e-04, 5.4626e-04,\n",
      "          3.3351e-03, 1.7482e-02, 2.8027e-02, 1.9765e-02, 9.2384e-05,\n",
      "          1.0329e-04, 9.7626e-05, 9.4820e-05, 1.1237e-04, 8.2140e-05,\n",
      "          6.0547e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55463\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5805/10000 (58.1%)\n",
      "    Average reward: -0.804\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4195/10000 (41.9%)\n",
      "    Average reward: +0.804\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8659 (32.2%)\n",
      "    Action 1: 14730 (54.8%)\n",
      "    Action 2: 314 (1.2%)\n",
      "    Action 3: 3193 (11.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8505 (29.8%)\n",
      "    Action 1: 13812 (48.3%)\n",
      "    Action 2: 2504 (8.8%)\n",
      "    Action 3: 3746 (13.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-8036.0, 8036.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.002 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.8036\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.5848, 0.4099, 0.0053, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2122, 0.7819, 0.0058, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.3492, 0.6340, 0.0168, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9999/50000 [08:58<26:28, 25.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52634\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5854/10000 (58.5%)\n",
      "    Average reward: -0.069\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4146/10000 (41.5%)\n",
      "    Average reward: +0.069\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4705 (18.9%)\n",
      "    Action 1: 16535 (66.4%)\n",
      "    Action 2: 355 (1.4%)\n",
      "    Action 3: 3312 (13.3%)\n",
      "  Player 1:\n",
      "    Action 0: 21535 (77.7%)\n",
      "    Action 1: 6192 (22.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-687.5, 687.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.847 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.766 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.806\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0688\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.5639e-04, 2.6923e-04, 2.2491e-04, 2.6226e-04, 2.9421e-04,\n",
      "          3.1865e-04, 2.6895e-04, 5.7153e-03, 8.5491e-03, 5.4201e-03,\n",
      "          5.1141e-03, 4.6506e-03, 1.3760e-02, 1.7503e-02, 4.1907e-03,\n",
      "          1.1673e-03, 1.1764e-03, 2.7189e-02, 3.5191e-02, 6.7716e-03,\n",
      "          1.0026e-03, 1.1334e-03, 9.2293e-04, 1.6195e-03, 4.0265e-04,\n",
      "          1.1760e-01, 3.2043e-04, 4.5047e-03, 4.1754e-03, 9.3910e-04,\n",
      "          8.0586e-03, 1.3192e-02, 1.2716e-01, 1.0525e-01, 2.2423e-03,\n",
      "          5.5226e-03, 2.5205e-02, 1.0548e-01, 8.4107e-02, 1.7445e-02,\n",
      "          3.4170e-02, 4.3950e-02, 9.5953e-02, 5.9764e-02, 3.0231e-04,\n",
      "          2.7018e-04, 2.5305e-04, 2.0900e-04, 2.6558e-04, 1.8868e-04,\n",
      "          1.9068e-04],\n",
      "         [1.4129e-04, 2.1086e-04, 2.5125e-04, 2.4457e-04, 2.1422e-04,\n",
      "          2.4318e-04, 2.9283e-04, 5.7754e-03, 8.9723e-03, 6.0522e-03,\n",
      "          5.7419e-03, 3.0219e-03, 8.6639e-03, 1.3063e-02, 2.8762e-03,\n",
      "          2.8959e-03, 1.0201e-03, 2.8821e-02, 3.2209e-02, 6.7536e-03,\n",
      "          8.4970e-04, 6.7509e-04, 9.5597e-04, 1.8657e-03, 3.6058e-04,\n",
      "          9.2624e-02, 2.8589e-04, 4.5095e-03, 3.8822e-03, 7.7689e-04,\n",
      "          5.8508e-03, 1.7567e-02, 1.7730e-01, 1.5119e-01, 2.1336e-03,\n",
      "          1.1390e-02, 1.3707e-02, 4.9246e-02, 3.8817e-02, 1.1773e-02,\n",
      "          3.8639e-02, 5.0865e-02, 1.1485e-01, 8.0610e-02, 2.7295e-04,\n",
      "          3.2441e-04, 2.6858e-04, 2.6276e-04, 2.3599e-04, 2.8684e-04,\n",
      "          1.5576e-04],\n",
      "         [1.3587e-05, 1.4380e-05, 1.6771e-05, 1.4326e-05, 1.6473e-05,\n",
      "          2.1460e-05, 2.4331e-05, 1.1933e-05, 1.9574e-05, 1.8920e-05,\n",
      "          1.8380e-05, 1.9099e-05, 2.2101e-03, 2.4920e-03, 1.3857e-05,\n",
      "          1.5919e-03, 2.5174e-05, 4.5819e-03, 6.4166e-03, 1.3423e-05,\n",
      "          8.8700e-04, 1.0586e-05, 9.9482e-03, 2.1478e-01, 7.5640e-01,\n",
      "          1.3444e-05, 1.8336e-05, 2.0725e-05, 1.7207e-05, 1.4928e-05,\n",
      "          1.4186e-05, 1.1805e-05, 2.6658e-05, 1.0203e-05, 1.8612e-05,\n",
      "          1.7647e-05, 1.2777e-05, 1.2213e-05, 1.5619e-05, 1.9397e-05,\n",
      "          1.7861e-05, 1.5338e-05, 8.5739e-06, 2.2811e-05, 1.5523e-05,\n",
      "          1.1921e-05, 1.3439e-05, 1.7634e-05, 1.5565e-05, 1.4439e-05,\n",
      "          2.3617e-05],\n",
      "         [4.9724e-04, 5.2716e-04, 5.2002e-04, 4.9051e-04, 6.7559e-04,\n",
      "          5.2776e-04, 4.3338e-04, 3.9402e-03, 7.2687e-03, 5.0805e-03,\n",
      "          5.6298e-03, 3.1997e-03, 9.7067e-03, 1.1935e-02, 1.2131e-03,\n",
      "          2.1952e-03, 1.2911e-03, 4.9001e-02, 5.7769e-02, 1.1579e-02,\n",
      "          7.3090e-04, 1.5181e-03, 6.7667e-03, 6.9843e-03, 7.4612e-04,\n",
      "          1.3367e-01, 6.5685e-04, 1.3235e-02, 1.1427e-02, 9.5901e-04,\n",
      "          3.1959e-03, 7.4687e-03, 1.7956e-01, 1.7674e-01, 1.8745e-03,\n",
      "          4.5797e-03, 5.5030e-03, 6.3807e-02, 6.7211e-02, 6.1144e-03,\n",
      "          1.9654e-02, 2.6198e-02, 5.0439e-02, 3.3148e-02, 5.7223e-04,\n",
      "          7.0855e-04, 6.0682e-04, 8.2110e-04, 5.0956e-04, 5.6560e-04,\n",
      "          5.5016e-04]]])\n",
      "Player 1 Prediction: tensor([[0.1967, 0.8008, 0.0025, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[8.8055e-05, 1.3672e-04, 1.1762e-04, 1.2894e-04, 1.1749e-04,\n",
      "          1.3181e-04, 1.3735e-04, 7.7476e-03, 1.0426e-02, 4.4961e-03,\n",
      "          8.9543e-04, 8.5295e-04, 1.2094e-02, 1.5578e-02, 2.1417e-03,\n",
      "          2.6321e-04, 3.1639e-04, 3.7168e-02, 3.4839e-02, 4.8684e-03,\n",
      "          5.1464e-05, 1.7099e-04, 5.2863e-04, 1.7533e-03, 2.7462e-04,\n",
      "          9.2444e-02, 1.6225e-04, 1.3481e-03, 9.8607e-04, 2.1981e-04,\n",
      "          3.2810e-04, 1.1577e-02, 1.8412e-01, 1.7533e-01, 4.7497e-04,\n",
      "          1.0468e-03, 1.0107e-02, 6.8291e-02, 5.9320e-02, 2.4671e-03,\n",
      "          5.6054e-03, 3.5988e-02, 1.1795e-01, 9.6032e-02, 1.1517e-04,\n",
      "          1.4375e-04, 1.4691e-04, 1.1899e-04, 1.1167e-04, 1.2668e-04,\n",
      "          1.1514e-04],\n",
      "         [3.2141e-04, 3.8432e-04, 4.5148e-04, 5.2861e-04, 4.0463e-04,\n",
      "          3.7163e-04, 5.1390e-04, 3.1103e-04, 4.6123e-04, 5.8587e-03,\n",
      "          6.3980e-02, 7.7695e-03, 7.0935e-02, 6.8024e-02, 2.4952e-03,\n",
      "          2.7479e-02, 1.3810e-03, 2.6381e-03, 3.5868e-03, 4.2563e-03,\n",
      "          2.0637e-03, 1.1896e-03, 2.4242e-03, 4.1240e-03, 6.8998e-04,\n",
      "          4.6535e-02, 4.3175e-04, 4.1226e-03, 3.6650e-03, 1.1005e-03,\n",
      "          2.6456e-02, 8.3793e-03, 2.9097e-02, 2.4324e-02, 1.4379e-03,\n",
      "          1.1318e-01, 3.7783e-03, 4.4157e-02, 5.1776e-02, 3.7057e-02,\n",
      "          3.0837e-01, 1.6522e-02, 2.1432e-03, 1.4580e-03, 6.1257e-04,\n",
      "          6.6702e-04, 3.3731e-04, 4.4856e-04, 4.6433e-04, 5.6283e-04,\n",
      "          2.7804e-04],\n",
      "         [5.5894e-06, 4.9779e-06, 3.7748e-06, 4.6119e-06, 1.0086e-05,\n",
      "          5.8168e-06, 8.1272e-06, 4.4556e-06, 3.0586e-06, 5.3981e-06,\n",
      "          4.8033e-06, 4.4405e-06, 9.7156e-04, 6.2138e-04, 3.1832e-06,\n",
      "          2.4213e-04, 5.5618e-06, 2.8125e-03, 1.9326e-03, 5.3028e-06,\n",
      "          9.9290e-01, 3.9690e-06, 2.5965e-05, 1.6162e-04, 1.2357e-04,\n",
      "          4.9705e-06, 5.7592e-06, 4.9115e-06, 7.0888e-06, 4.2062e-06,\n",
      "          4.1070e-06, 3.9807e-06, 5.4348e-06, 4.2126e-06, 3.2050e-06,\n",
      "          6.6946e-06, 3.9474e-06, 3.2815e-06, 6.3727e-06, 2.8221e-06,\n",
      "          5.4375e-06, 4.4868e-06, 2.1691e-06, 6.7768e-06, 4.7194e-06,\n",
      "          4.6525e-06, 3.5057e-06, 5.2075e-06, 4.7616e-06, 6.6909e-06,\n",
      "          6.2642e-06],\n",
      "         [8.4786e-04, 6.0560e-04, 6.0544e-04, 8.1006e-04, 8.6357e-04,\n",
      "          6.7763e-04, 5.0545e-04, 6.9931e-03, 1.4718e-02, 4.4097e-03,\n",
      "          2.1672e-02, 5.7910e-03, 4.2672e-03, 4.4519e-03, 1.5766e-03,\n",
      "          5.9309e-03, 1.9004e-03, 4.2945e-02, 5.8237e-02, 7.6910e-03,\n",
      "          3.8738e-03, 2.6592e-03, 1.5405e-03, 1.4312e-03, 7.9838e-04,\n",
      "          1.9954e-01, 8.0432e-04, 1.5476e-03, 1.4867e-03, 1.7193e-03,\n",
      "          1.5493e-02, 7.3319e-03, 1.3775e-01, 1.3967e-01, 3.1186e-03,\n",
      "          9.7000e-03, 2.5647e-03, 1.4625e-02, 2.0323e-02, 2.2045e-02,\n",
      "          6.9501e-02, 3.2680e-02, 8.0848e-02, 3.7900e-02, 7.5092e-04,\n",
      "          1.0250e-03, 6.5314e-04, 8.8459e-04, 7.4877e-04, 4.7193e-04,\n",
      "          1.0125e-03]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8828, 0.0162, 0.1011]])\n",
      "Player 0 Prediction: tensor([[[1.3454e-06, 2.1432e-06, 2.6449e-06, 1.4342e-06, 2.2190e-06,\n",
      "          2.2956e-06, 2.4536e-06, 1.6661e-04, 2.3760e-04, 4.3944e-05,\n",
      "          9.5849e-05, 1.3098e-05, 1.4146e-01, 1.4806e-01, 2.3126e-05,\n",
      "          3.8886e-05, 5.3573e-06, 8.9375e-05, 1.0238e-04, 1.8287e-05,\n",
      "          5.5892e-07, 6.3617e-06, 4.4064e-06, 4.9445e-06, 2.8964e-06,\n",
      "          2.4408e-01, 2.0626e-06, 5.6157e-06, 4.5555e-06, 3.0111e-06,\n",
      "          3.0321e-06, 4.6521e-05, 1.9569e-04, 1.5282e-04, 1.0320e-05,\n",
      "          8.2020e-05, 1.0766e-04, 2.3123e-01, 2.3241e-01, 6.2864e-05,\n",
      "          4.9980e-04, 1.4950e-04, 2.3624e-04, 3.1369e-04, 2.4272e-06,\n",
      "          1.0105e-06, 1.2603e-06, 1.9422e-06, 1.9947e-06, 1.5007e-06,\n",
      "          1.7827e-06],\n",
      "         [3.4981e-06, 8.2283e-06, 5.3578e-06, 4.7879e-06, 7.1858e-06,\n",
      "          5.4302e-06, 6.8060e-06, 1.1319e-01, 1.0824e-01, 1.6591e-04,\n",
      "          4.5101e-04, 6.2921e-05, 3.0957e-05, 2.2977e-05, 4.2654e-05,\n",
      "          1.0310e-05, 2.0608e-05, 2.4641e-04, 3.1480e-04, 9.1371e-05,\n",
      "          3.3447e-06, 6.0841e-06, 3.6873e-05, 5.3537e-05, 7.7264e-06,\n",
      "          4.1392e-01, 4.7124e-06, 7.3455e-05, 5.9264e-05, 1.2044e-05,\n",
      "          2.7399e-05, 2.1524e-04, 4.1390e-03, 4.6501e-03, 3.4888e-05,\n",
      "          6.1087e-05, 1.5955e-04, 1.4761e-03, 1.4314e-03, 3.4493e-04,\n",
      "          9.7915e-04, 7.3787e-04, 1.7604e-01, 1.7255e-01, 7.9306e-06,\n",
      "          5.2348e-06, 6.6500e-06, 7.9098e-06, 5.2080e-06, 4.3083e-06,\n",
      "          4.2031e-06],\n",
      "         [3.7779e-06, 2.6413e-06, 2.7712e-06, 3.2124e-06, 2.3731e-06,\n",
      "          6.8958e-06, 2.0491e-06, 3.3621e-06, 4.6273e-06, 4.3888e-06,\n",
      "          5.9398e-06, 4.2120e-06, 6.1039e-04, 6.5087e-04, 2.0155e-06,\n",
      "          9.5097e-04, 6.2278e-06, 4.8211e-01, 5.1513e-01, 5.3609e-06,\n",
      "          3.6123e-05, 2.9729e-06, 8.7189e-05, 1.4834e-04, 1.0431e-04,\n",
      "          3.9363e-06, 2.6066e-06, 5.3416e-06, 8.2743e-06, 4.2048e-06,\n",
      "          2.8577e-06, 2.9016e-06, 6.4277e-06, 2.3060e-06, 2.3562e-06,\n",
      "          7.8903e-06, 2.5339e-06, 1.9553e-06, 6.0174e-06, 2.9245e-06,\n",
      "          5.1033e-06, 6.8523e-06, 3.0472e-06, 3.9558e-06, 4.2193e-06,\n",
      "          3.6867e-06, 3.8657e-06, 2.5097e-06, 5.2442e-06, 3.2426e-06,\n",
      "          9.1676e-06],\n",
      "         [1.0654e-04, 1.1121e-04, 1.3817e-04, 1.1350e-04, 1.3053e-04,\n",
      "          7.8824e-05, 1.2185e-04, 7.1685e-03, 7.0215e-03, 1.8614e-03,\n",
      "          1.4096e-03, 6.2226e-04, 1.7912e-03, 2.0127e-03, 2.9716e-04,\n",
      "          5.9048e-04, 3.0725e-04, 4.2708e-02, 3.2918e-02, 2.6845e-03,\n",
      "          3.8353e-05, 3.3196e-04, 6.6193e-04, 2.2146e-03, 2.2605e-04,\n",
      "          9.6397e-02, 1.7433e-04, 1.5101e-03, 1.6070e-03, 2.4641e-04,\n",
      "          2.6618e-04, 2.2946e-03, 2.5715e-01, 3.0679e-01, 3.1109e-04,\n",
      "          7.6503e-04, 9.8663e-04, 8.0901e-03, 8.9795e-03, 1.8628e-03,\n",
      "          5.2720e-03, 1.4532e-02, 7.7794e-02, 1.0848e-01, 1.4183e-04,\n",
      "          1.3973e-04, 1.5849e-04, 8.6722e-05, 7.8870e-05, 1.0945e-04,\n",
      "          1.0996e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9983, 0.0000, 0.0017, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55791\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5765/10000 (57.6%)\n",
      "    Average reward: +0.796\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4235/10000 (42.4%)\n",
      "    Average reward: -0.796\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9209 (32.3%)\n",
      "    Action 1: 13292 (46.6%)\n",
      "    Action 2: 2473 (8.7%)\n",
      "    Action 3: 3540 (12.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7962 (29.2%)\n",
      "    Action 1: 15159 (55.6%)\n",
      "    Action 2: 523 (1.9%)\n",
      "    Action 3: 3633 (13.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7960.0, -7960.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.040 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.990 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7960\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5437, 0.4165, 0.0398, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8260, 0.0322, 0.1419]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52017\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6117/10000 (61.2%)\n",
      "    Average reward: +0.020\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3883/10000 (38.8%)\n",
      "    Average reward: -0.020\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22289 (80.0%)\n",
      "    Action 1: 5556 (20.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4186 (17.3%)\n",
      "    Action 1: 17237 (71.3%)\n",
      "    Action 2: 514 (2.1%)\n",
      "    Action 3: 2235 (9.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [204.5, -204.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.721 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.786 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.753\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0204\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11004/50000 [09:54<26:06, 24.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 70506/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 73240/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11998/50000 [10:34<24:58, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 76799/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 79455/2000000\n",
      "P1 SL Buffer Size:  76799\n",
      "P1 SL buffer distribution [30799. 35584.  2898.  7518.]\n",
      "P1 actions distribution [0.40103387 0.46333937 0.03773487 0.0978919 ]\n",
      "P2 SL Buffer Size:  79455\n",
      "P2 SL buffer distribution [28752. 38767.  3259.  8677.]\n",
      "P2 actions distribution [0.36186521 0.4879114  0.04101693 0.10920647]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[1.3431e-04, 2.9656e-04, 2.8183e-04, 3.2056e-04, 3.8505e-04,\n",
      "          4.1719e-04, 4.2869e-04, 2.5726e-03, 3.5419e-03, 2.2243e-03,\n",
      "          2.8510e-03, 1.5612e-03, 8.3940e-03, 1.0774e-02, 3.2471e-03,\n",
      "          3.2113e-03, 8.1881e-04, 5.0279e-02, 6.6403e-02, 1.3961e-02,\n",
      "          1.8475e-02, 8.5945e-04, 1.9310e-03, 2.7044e-03, 4.2060e-04,\n",
      "          2.2985e-01, 4.0822e-04, 6.2605e-03, 6.1023e-03, 2.3212e-03,\n",
      "          3.1530e-02, 2.3098e-02, 1.2021e-01, 9.4127e-02, 1.7804e-03,\n",
      "          6.5106e-03, 1.7074e-02, 6.4332e-02, 4.8678e-02, 1.0120e-02,\n",
      "          2.7383e-02, 2.4704e-02, 5.5712e-02, 3.1353e-02, 3.1684e-04,\n",
      "          3.6154e-04, 3.2448e-04, 3.0846e-04, 2.9963e-04, 2.1470e-04,\n",
      "          1.2685e-04],\n",
      "         [1.6393e-04, 2.5594e-04, 2.7298e-04, 2.7546e-04, 4.5585e-04,\n",
      "          4.3909e-04, 3.7782e-04, 3.1168e-03, 4.1600e-03, 2.4162e-03,\n",
      "          6.6639e-03, 2.0996e-03, 2.5889e-03, 4.2954e-03, 1.7503e-03,\n",
      "          9.4750e-03, 1.5857e-03, 4.4707e-02, 6.7046e-02, 1.5930e-02,\n",
      "          4.7813e-03, 6.2068e-04, 1.2732e-03, 1.4961e-03, 3.6131e-04,\n",
      "          1.8257e-01, 4.0025e-04, 6.7764e-03, 6.7220e-03, 1.8527e-03,\n",
      "          4.0325e-02, 2.4899e-02, 1.4515e-01, 1.1092e-01, 4.8710e-03,\n",
      "          4.4325e-02, 8.8831e-03, 3.1534e-02, 2.0959e-02, 1.5425e-02,\n",
      "          4.1446e-02, 2.9041e-02, 6.4852e-02, 4.0556e-02, 2.6396e-04,\n",
      "          2.8201e-04, 3.0182e-04, 2.9748e-04, 2.8561e-04, 2.8349e-04,\n",
      "          1.6084e-04],\n",
      "         [3.0487e-06, 3.1059e-06, 3.3684e-06, 4.7368e-06, 3.3161e-06,\n",
      "          5.4876e-06, 3.0824e-06, 5.7372e-06, 4.8273e-06, 2.8197e-06,\n",
      "          2.0056e-06, 2.7925e-06, 5.2732e-04, 3.9486e-04, 3.9360e-06,\n",
      "          5.1138e-04, 3.0529e-06, 7.1831e-04, 7.6334e-04, 4.1248e-06,\n",
      "          3.8563e-04, 4.1184e-06, 6.7955e-03, 2.1767e-01, 7.7208e-01,\n",
      "          4.8104e-06, 4.5182e-06, 4.9757e-06, 2.6469e-06, 5.9066e-06,\n",
      "          3.9670e-06, 4.8687e-06, 3.8229e-06, 2.5992e-06, 3.0152e-06,\n",
      "          3.3882e-06, 4.1019e-06, 3.4923e-06, 3.1349e-06, 4.1124e-06,\n",
      "          3.2802e-06, 3.1603e-06, 3.4398e-06, 3.3089e-06, 2.8053e-06,\n",
      "          4.7987e-06, 3.4268e-06, 3.8056e-06, 2.8386e-06, 2.9477e-06,\n",
      "          3.2092e-06],\n",
      "         [2.3948e-04, 4.7264e-04, 6.0718e-04, 4.3384e-04, 6.4355e-04,\n",
      "          4.2488e-04, 4.7683e-04, 2.9950e-03, 3.6769e-03, 2.4779e-03,\n",
      "          2.4646e-03, 9.8075e-04, 5.1111e-03, 7.3250e-03, 1.0054e-03,\n",
      "          1.0807e-03, 5.8828e-04, 6.2990e-02, 1.0074e-01, 1.3829e-02,\n",
      "          2.8460e-03, 6.1669e-04, 6.5350e-03, 6.5361e-03, 4.5253e-04,\n",
      "          2.2388e-01, 4.7622e-04, 1.5946e-02, 1.5139e-02, 7.9928e-04,\n",
      "          4.7416e-03, 2.8359e-02, 1.4476e-01, 1.3127e-01, 8.2901e-04,\n",
      "          2.0979e-03, 5.9290e-03, 7.0174e-02, 5.2288e-02, 3.0320e-03,\n",
      "          7.4781e-03, 1.9261e-02, 2.5334e-02, 1.9594e-02, 4.9421e-04,\n",
      "          4.6236e-04, 3.5822e-04, 5.7048e-04, 3.4518e-04, 4.9608e-04,\n",
      "          3.3397e-04]]])\n",
      "Player 0 Prediction: tensor([[0.4694, 0.5020, 0.0286, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.8151e-05, 2.0331e-05, 2.9081e-05, 3.1032e-05, 4.2867e-05,\n",
      "          3.1249e-05, 3.7836e-05, 9.7033e-04, 9.2377e-04, 3.5157e-04,\n",
      "          5.5671e-03, 4.2313e-04, 4.0183e-04, 5.5761e-04, 1.6269e-04,\n",
      "          1.9080e-01, 9.2700e-05, 3.0681e-04, 6.5097e-04, 1.3034e-03,\n",
      "          5.4782e-03, 1.3021e-04, 1.4848e-04, 2.6612e-04, 3.1491e-05,\n",
      "          4.8936e-01, 1.8285e-05, 2.1419e-04, 1.7803e-04, 7.5330e-04,\n",
      "          1.1997e-02, 1.3290e-03, 5.7836e-04, 3.6363e-04, 5.3465e-04,\n",
      "          1.8036e-01, 1.3946e-03, 2.0795e-03, 1.6865e-03, 3.9979e-03,\n",
      "          4.3926e-02, 3.7542e-03, 2.8216e-02, 2.0307e-02, 2.3089e-05,\n",
      "          3.2662e-05, 3.4984e-05, 3.0022e-05, 3.0019e-05, 2.0041e-05,\n",
      "          9.7208e-06],\n",
      "         [9.4129e-05, 1.6282e-04, 1.2181e-04, 1.3026e-04, 2.4213e-04,\n",
      "          1.8624e-04, 2.1737e-04, 1.5404e-03, 1.3274e-03, 8.5574e-04,\n",
      "          3.8627e-02, 3.5593e-03, 2.3157e-03, 3.1185e-03, 9.4860e-04,\n",
      "          2.7378e-02, 3.4802e-04, 6.3500e-03, 7.6058e-03, 4.7667e-03,\n",
      "          9.2409e-04, 2.0481e-04, 1.0111e-03, 9.6310e-04, 1.3916e-04,\n",
      "          1.6017e-01, 2.9669e-04, 2.1854e-03, 2.7374e-03, 5.0340e-04,\n",
      "          2.1810e-01, 5.1140e-03, 5.9114e-02, 3.2699e-02, 5.6481e-04,\n",
      "          1.8808e-01, 3.7987e-03, 1.2909e-02, 1.1063e-02, 3.6108e-02,\n",
      "          1.5173e-01, 3.4464e-03, 3.2501e-03, 3.8153e-03, 1.5474e-04,\n",
      "          1.3957e-04, 2.0553e-04, 2.1730e-04, 1.7127e-04, 1.8087e-04,\n",
      "          1.1447e-04],\n",
      "         [1.8633e-06, 1.2045e-06, 2.7001e-06, 2.5568e-06, 1.7172e-06,\n",
      "          3.2835e-06, 2.4410e-06, 2.0850e-06, 1.3935e-06, 1.9172e-06,\n",
      "          1.5395e-06, 1.5067e-06, 9.0578e-04, 6.5621e-04, 1.4406e-06,\n",
      "          2.6040e-03, 1.3356e-06, 1.7076e-04, 1.5862e-04, 1.2734e-06,\n",
      "          9.8249e-01, 1.8003e-06, 1.1762e-03, 1.1691e-02, 6.4314e-05,\n",
      "          3.4073e-06, 1.1155e-06, 1.7820e-06, 1.1647e-06, 1.6306e-06,\n",
      "          1.8167e-06, 3.0207e-06, 2.7821e-06, 1.1966e-06, 2.5390e-06,\n",
      "          1.6483e-06, 1.3455e-06, 1.1042e-06, 1.8537e-06, 2.6754e-06,\n",
      "          2.3044e-06, 2.0334e-06, 2.5893e-06, 3.4585e-06, 1.7459e-06,\n",
      "          1.9032e-06, 1.0854e-06, 1.8075e-06, 2.0947e-06, 2.4200e-06,\n",
      "          1.5795e-06],\n",
      "         [1.6138e-04, 1.7882e-04, 2.0373e-04, 1.6187e-04, 1.5828e-04,\n",
      "          1.3499e-04, 1.8693e-04, 9.1457e-04, 8.5292e-04, 1.0262e-03,\n",
      "          3.0652e-03, 4.3628e-04, 1.3725e-03, 2.4894e-03, 2.6644e-04,\n",
      "          3.4697e-04, 2.2946e-04, 4.6088e-03, 5.0910e-03, 4.5303e-03,\n",
      "          2.8364e-01, 2.1489e-04, 1.2777e-03, 1.5797e-03, 2.2002e-04,\n",
      "          2.7854e-01, 1.5186e-04, 3.0460e-03, 3.0055e-03, 3.8362e-04,\n",
      "          3.0295e-01, 2.9042e-03, 1.8996e-02, 1.5580e-02, 3.8546e-04,\n",
      "          2.1706e-03, 1.7509e-03, 8.6654e-03, 9.4398e-03, 1.1064e-03,\n",
      "          2.6677e-02, 1.9781e-03, 3.7876e-03, 4.1792e-03, 1.3633e-04,\n",
      "          1.1335e-04, 1.3861e-04, 1.4506e-04, 1.1667e-04, 2.2553e-04,\n",
      "          8.3881e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3746, 0.4556, 0.1698, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[3.2272e-06, 4.2222e-06, 2.2796e-06, 3.9127e-06, 3.7139e-06,\n",
      "          3.1901e-06, 3.6414e-06, 1.8090e-04, 1.1136e-04, 1.9334e-05,\n",
      "          2.0100e-01, 3.7751e-05, 1.1778e-04, 1.2708e-04, 1.8430e-05,\n",
      "          5.5116e-04, 1.5364e-05, 1.1853e-04, 1.5148e-04, 3.1925e-05,\n",
      "          1.4408e-04, 1.4383e-05, 1.2055e-05, 1.4661e-05, 4.1727e-06,\n",
      "          2.2985e-01, 3.6550e-06, 1.1460e-05, 1.0346e-05, 4.9578e-05,\n",
      "          1.5353e-04, 3.9237e-05, 9.6413e-05, 1.0320e-04, 4.2054e-05,\n",
      "          3.0111e-04, 8.1766e-05, 2.6928e-04, 1.9129e-04, 1.8687e-04,\n",
      "          5.6496e-01, 7.7302e-05, 4.1842e-04, 4.3360e-04, 3.6612e-06,\n",
      "          2.4268e-06, 6.6186e-06, 3.3405e-06, 3.0508e-06, 3.0075e-06,\n",
      "          1.4811e-06],\n",
      "         [1.5054e-04, 2.0654e-04, 1.1907e-04, 1.8575e-04, 4.2802e-04,\n",
      "          2.2043e-04, 5.3920e-04, 5.3149e-03, 5.0454e-03, 1.1021e-03,\n",
      "          1.4285e-02, 2.1439e-03, 6.7725e-03, 6.4366e-03, 1.1239e-03,\n",
      "          3.0302e-04, 5.2074e-04, 9.0947e-03, 8.7890e-03, 4.2287e-03,\n",
      "          6.4648e-04, 2.3075e-04, 1.0342e-03, 5.5252e-04, 1.5619e-04,\n",
      "          2.8244e-01, 4.3833e-04, 3.3204e-03, 4.0275e-03, 7.0717e-04,\n",
      "          8.0842e-03, 4.0205e-03, 2.8985e-01, 1.6379e-01, 6.0839e-04,\n",
      "          2.9380e-03, 5.2057e-03, 4.8187e-02, 4.5767e-02, 7.5727e-03,\n",
      "          3.8552e-02, 5.5166e-03, 7.4294e-03, 1.0306e-02, 2.0735e-04,\n",
      "          1.3414e-04, 2.3924e-04, 3.3670e-04, 2.8196e-04, 2.5408e-04,\n",
      "          1.4957e-04],\n",
      "         [2.4117e-05, 1.2592e-05, 2.2294e-05, 4.7427e-05, 1.7284e-05,\n",
      "          3.1327e-05, 2.1343e-05, 2.0455e-05, 1.1485e-05, 1.4778e-05,\n",
      "          1.9519e-05, 1.4853e-05, 3.7867e-03, 3.3579e-03, 2.4371e-05,\n",
      "          9.5889e-01, 1.7790e-05, 2.9527e-03, 3.9137e-03, 1.8012e-05,\n",
      "          1.8119e-03, 1.7615e-05, 8.6660e-03, 1.4850e-02, 7.5234e-04,\n",
      "          1.7270e-05, 2.1205e-05, 1.3807e-05, 1.8227e-05, 2.7023e-05,\n",
      "          2.5956e-05, 4.6535e-05, 3.5103e-05, 1.5767e-05, 2.6417e-05,\n",
      "          2.6629e-05, 3.0221e-05, 2.3463e-05, 2.8752e-05, 4.7494e-05,\n",
      "          2.3504e-05, 2.2406e-05, 2.8688e-05, 4.2867e-05, 2.9790e-05,\n",
      "          3.2307e-05, 1.7846e-05, 2.1582e-05, 2.0969e-05, 2.1186e-05,\n",
      "          1.8788e-05],\n",
      "         [6.4094e-04, 5.9554e-04, 8.0589e-04, 7.6969e-04, 9.0897e-04,\n",
      "          9.8460e-04, 9.8532e-04, 8.5193e-03, 4.2313e-03, 3.6953e-03,\n",
      "          5.3848e-03, 1.3380e-03, 6.6031e-03, 1.3738e-02, 2.2548e-03,\n",
      "          1.1618e-03, 1.7099e-03, 2.0822e-02, 2.2953e-02, 1.3831e-02,\n",
      "          1.3674e-02, 8.4155e-04, 5.2368e-03, 6.0189e-03, 9.4223e-04,\n",
      "          1.4848e-01, 7.0333e-04, 1.7920e-02, 2.4539e-02, 1.1899e-03,\n",
      "          1.6016e-02, 1.9117e-02, 2.8382e-01, 1.9609e-01, 1.3301e-03,\n",
      "          3.1519e-03, 6.5922e-03, 2.3951e-02, 3.1713e-02, 3.1430e-03,\n",
      "          2.2055e-02, 8.8865e-03, 1.8192e-02, 2.9811e-02, 7.7994e-04,\n",
      "          3.6495e-04, 9.6148e-04, 6.5033e-04, 6.9619e-04, 8.0892e-04,\n",
      "          3.8551e-04]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11998/50000 [10:48<24:58, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53485\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5654/10000 (56.5%)\n",
      "    Average reward: -0.753\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4346/10000 (43.5%)\n",
      "    Average reward: +0.753\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8853 (33.8%)\n",
      "    Action 1: 13900 (53.1%)\n",
      "    Action 2: 839 (3.2%)\n",
      "    Action 3: 2569 (9.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9170 (33.6%)\n",
      "    Action 1: 14248 (52.1%)\n",
      "    Action 2: 2062 (7.5%)\n",
      "    Action 3: 1844 (6.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7528.0, 7528.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.014 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.018 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.016\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7528\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9888, 0.0026, 0.0085]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9437, 0.0187, 0.0376]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52403\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5560/10000 (55.6%)\n",
      "    Average reward: -0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4440/10000 (44.4%)\n",
      "    Average reward: +0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5004 (19.9%)\n",
      "    Action 1: 15577 (62.0%)\n",
      "    Action 2: 991 (3.9%)\n",
      "    Action 3: 3558 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20548 (75.3%)\n",
      "    Action 1: 6725 (24.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1078.5, 1078.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.891 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.806 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.849\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1079\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6688, 0.3242, 0.0070, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[4.9020e-05, 6.7228e-05, 8.7309e-05, 1.0048e-04, 1.2323e-04,\n",
      "          6.8832e-05, 8.4570e-05, 2.2547e-04, 3.2068e-04, 2.9911e-04,\n",
      "          2.3038e-02, 1.0097e-02, 5.3510e-04, 4.5861e-04, 3.3384e-04,\n",
      "          5.2110e-03, 8.7824e-04, 3.1450e-02, 4.5021e-02, 5.7763e-03,\n",
      "          1.7922e-02, 1.7407e-03, 7.2363e-04, 1.2083e-03, 1.1990e-04,\n",
      "          5.3414e-01, 1.0798e-04, 1.8158e-03, 1.6685e-03, 2.1200e-03,\n",
      "          6.1725e-02, 1.0561e-03, 3.5124e-03, 2.7329e-03, 2.0745e-03,\n",
      "          2.2853e-02, 5.0366e-04, 1.3669e-03, 2.8907e-03, 6.4645e-02,\n",
      "          1.4544e-01, 2.2866e-03, 1.7095e-03, 7.2635e-04, 1.1982e-04,\n",
      "          1.1670e-04, 1.1274e-04, 7.1267e-05, 1.3687e-04, 7.4783e-05,\n",
      "          5.3292e-05],\n",
      "         [4.7968e-05, 1.2248e-04, 9.1322e-05, 1.1556e-04, 1.0769e-04,\n",
      "          9.7069e-05, 7.5976e-05, 1.8001e-03, 2.4217e-03, 1.4103e-03,\n",
      "          1.9971e-03, 7.5523e-04, 4.5076e-03, 4.6755e-03, 1.1652e-03,\n",
      "          1.0145e-03, 3.8027e-04, 1.1246e-01, 1.0255e-01, 1.5162e-02,\n",
      "          3.3040e-03, 4.5223e-04, 7.0802e-04, 1.4812e-03, 2.0960e-04,\n",
      "          5.1368e-01, 1.3242e-04, 2.7538e-03, 2.9085e-03, 5.4017e-04,\n",
      "          4.9691e-03, 4.5739e-03, 1.8438e-02, 1.6613e-02, 6.8822e-04,\n",
      "          6.5694e-03, 2.7530e-03, 1.2014e-02, 1.0171e-02, 3.0577e-03,\n",
      "          9.1282e-03, 1.9523e-02, 6.8121e-02, 4.5541e-02, 1.0522e-04,\n",
      "          1.2206e-04, 1.2328e-04, 1.0827e-04, 9.4158e-05, 7.9313e-05,\n",
      "          8.1808e-05],\n",
      "         [1.0294e-06, 1.2497e-06, 1.0182e-06, 1.5468e-06, 1.3933e-06,\n",
      "          1.4639e-06, 1.8120e-06, 8.3215e-07, 1.4515e-06, 8.6389e-07,\n",
      "          8.1507e-07, 1.8228e-06, 6.6205e-05, 8.1438e-05, 6.4991e-07,\n",
      "          2.4157e-04, 1.9390e-06, 4.8674e-05, 6.2751e-05, 7.6886e-07,\n",
      "          2.4944e-05, 7.7546e-07, 4.8958e-01, 5.0034e-01, 9.5052e-03,\n",
      "          1.2517e-06, 1.0492e-06, 1.2679e-06, 1.4623e-06, 1.1950e-06,\n",
      "          1.6562e-06, 1.9300e-06, 1.7810e-06, 4.2931e-07, 1.5952e-06,\n",
      "          1.4804e-06, 1.1736e-06, 1.2623e-06, 1.2454e-06, 1.7786e-06,\n",
      "          1.4850e-06, 9.4241e-07, 6.8966e-07, 1.4207e-06, 8.5251e-07,\n",
      "          7.3441e-07, 8.4888e-07, 1.3207e-06, 1.1897e-06, 1.2326e-06,\n",
      "          1.0042e-06],\n",
      "         [1.7867e-04, 2.3695e-04, 2.4541e-04, 2.3522e-04, 2.8886e-04,\n",
      "          2.8032e-04, 2.7813e-04, 1.5644e-03, 2.3521e-03, 1.5112e-03,\n",
      "          2.2276e-03, 1.1726e-03, 7.3165e-03, 7.9980e-03, 1.1674e-03,\n",
      "          9.5591e-04, 5.6908e-04, 1.2060e-01, 1.3110e-01, 1.5833e-02,\n",
      "          2.3680e-03, 5.1092e-04, 2.2526e-02, 2.1950e-02, 3.1432e-04,\n",
      "          4.1485e-01, 3.3983e-04, 1.0104e-02, 1.0211e-02, 5.3333e-04,\n",
      "          2.4630e-03, 4.5275e-03, 2.3538e-02, 2.0435e-02, 5.7948e-04,\n",
      "          1.1350e-03, 6.6164e-03, 6.3111e-02, 5.0070e-02, 4.0588e-03,\n",
      "          7.3625e-03, 7.9831e-03, 1.5145e-02, 1.1301e-02, 3.0614e-04,\n",
      "          2.5481e-04, 3.0958e-04, 2.9270e-04, 2.4695e-04, 2.4681e-04,\n",
      "          1.9771e-04]]])\n",
      "Player 1 Prediction: tensor([[0.1749, 0.8223, 0.0028, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.9134e-05, 4.5390e-05, 5.9701e-05, 6.2664e-05, 5.3447e-05,\n",
      "          2.4432e-05, 4.9032e-05, 3.1456e-04, 4.1453e-04, 4.1081e-04,\n",
      "          8.9817e-03, 2.0125e-03, 3.1926e-04, 2.5528e-04, 2.6198e-04,\n",
      "          2.6291e-02, 2.3818e-04, 4.0319e-04, 4.7434e-04, 4.1808e-03,\n",
      "          1.7906e-03, 3.6704e-04, 3.3063e-04, 5.6116e-04, 7.1429e-05,\n",
      "          2.4674e-01, 6.5466e-05, 4.1731e-04, 4.1910e-04, 6.4532e-04,\n",
      "          2.8188e-03, 7.2947e-04, 5.3204e-04, 4.7349e-04, 4.0856e-04,\n",
      "          4.7261e-01, 4.0563e-04, 1.0539e-02, 1.3905e-02, 5.1005e-03,\n",
      "          1.2470e-01, 3.7166e-03, 4.4868e-02, 2.2530e-02, 6.4878e-05,\n",
      "          8.0024e-05, 4.8341e-05, 5.1017e-05, 6.5761e-05, 4.3080e-05,\n",
      "          2.1554e-05],\n",
      "         [1.9387e-04, 3.2183e-04, 3.3389e-04, 2.4221e-04, 2.8488e-04,\n",
      "          2.8905e-04, 2.1459e-04, 1.0179e-03, 1.2415e-03, 1.6556e-03,\n",
      "          4.1423e-03, 4.9855e-03, 1.0467e-03, 9.4481e-04, 1.4678e-03,\n",
      "          6.7816e-02, 9.8433e-04, 8.1763e-02, 6.8959e-02, 1.3217e-02,\n",
      "          4.5996e-03, 7.3398e-04, 3.6626e-03, 5.2838e-03, 4.7241e-04,\n",
      "          1.1103e-01, 3.3308e-04, 2.3422e-03, 2.4253e-03, 8.1474e-04,\n",
      "          5.4178e-02, 4.1295e-03, 1.2244e-02, 1.4816e-02, 7.4042e-04,\n",
      "          8.2304e-02, 1.9409e-03, 1.5605e-02, 2.3003e-02, 1.1060e-02,\n",
      "          1.9963e-01, 1.2286e-02, 1.0296e-01, 8.0073e-02, 3.4707e-04,\n",
      "          3.7771e-04, 3.7257e-04, 2.8888e-04, 3.4224e-04, 2.1432e-04,\n",
      "          2.7262e-04],\n",
      "         [5.0506e-06, 1.3460e-05, 4.9951e-06, 9.8916e-06, 1.1290e-05,\n",
      "          1.0048e-05, 1.3070e-05, 6.8324e-06, 5.6680e-06, 5.6697e-06,\n",
      "          5.2277e-06, 7.8092e-06, 1.2255e-03, 7.8771e-04, 4.6499e-06,\n",
      "          2.5533e-03, 8.6470e-06, 3.2664e-04, 2.7183e-04, 4.9602e-06,\n",
      "          9.7933e-01, 6.5795e-06, 1.9090e-03, 1.2500e-02, 7.8590e-04,\n",
      "          6.8445e-06, 4.2601e-06, 7.5844e-06, 8.3120e-06, 9.3154e-06,\n",
      "          4.2394e-06, 1.3090e-05, 5.9678e-06, 9.1664e-06, 7.1013e-06,\n",
      "          7.2424e-06, 6.7561e-06, 6.5884e-06, 9.3727e-06, 5.9965e-06,\n",
      "          1.1714e-05, 5.6263e-06, 3.9856e-06, 9.0310e-06, 6.1455e-06,\n",
      "          4.2147e-06, 7.0964e-06, 6.1478e-06, 8.2165e-06, 7.6484e-06,\n",
      "          5.8716e-06],\n",
      "         [3.0031e-04, 5.7993e-04, 6.3704e-04, 5.0412e-04, 5.0650e-04,\n",
      "          6.0017e-04, 5.3494e-04, 2.4028e-03, 2.6316e-03, 2.1015e-03,\n",
      "          3.8169e-02, 6.4153e-03, 3.6938e-03, 4.1916e-03, 1.4485e-03,\n",
      "          5.5376e-03, 1.2899e-03, 8.5448e-03, 9.2305e-03, 1.0785e-02,\n",
      "          7.2250e-02, 2.3797e-03, 7.4247e-03, 7.6990e-03, 7.5982e-04,\n",
      "          1.1330e-01, 5.4623e-04, 3.5346e-03, 2.9688e-03, 1.2883e-03,\n",
      "          2.3187e-01, 4.4512e-03, 1.0513e-02, 1.0021e-02, 1.6065e-03,\n",
      "          1.7720e-02, 2.5403e-03, 1.1550e-02, 1.4258e-02, 3.5685e-02,\n",
      "          3.0363e-01, 9.0533e-03, 2.0783e-02, 1.0522e-02, 5.3445e-04,\n",
      "          5.9423e-04, 3.6439e-04, 4.5937e-04, 6.0435e-04, 4.8790e-04,\n",
      "          4.9900e-04]]])\n",
      "Player 1 Prediction: tensor([[0.1970, 0.7882, 0.0148, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.9607e-06, 4.4233e-06, 6.0542e-06, 6.9357e-06, 6.0997e-06,\n",
      "          4.2405e-06, 5.8159e-06, 2.0588e-05, 1.8190e-05, 1.7191e-05,\n",
      "          2.2655e-01, 1.4300e-04, 1.3065e-04, 1.1422e-04, 2.3037e-05,\n",
      "          1.9204e-04, 2.2512e-05, 1.1766e-04, 1.2339e-04, 1.2070e-04,\n",
      "          7.9711e-05, 2.6823e-05, 2.6720e-05, 3.9138e-05, 6.7921e-06,\n",
      "          5.0996e-02, 5.2349e-06, 3.3287e-05, 2.8952e-05, 5.5187e-05,\n",
      "          1.0405e-04, 4.1925e-05, 1.3982e-04, 1.1496e-04, 3.8828e-05,\n",
      "          1.2870e-03, 5.1977e-05, 4.9237e-04, 7.0500e-04, 6.3743e-04,\n",
      "          7.1671e-01, 1.0200e-04, 3.2613e-04, 2.7167e-04, 1.2925e-05,\n",
      "          7.1772e-06, 5.9060e-06, 6.0270e-06, 5.8621e-06, 5.5003e-06,\n",
      "          3.9556e-06],\n",
      "         [3.1266e-04, 5.0580e-04, 4.0503e-04, 3.4643e-04, 3.4591e-04,\n",
      "          3.3929e-04, 3.2449e-04, 1.6579e-02, 2.3713e-02, 2.3990e-03,\n",
      "          2.4544e-03, 2.1859e-03, 6.2312e-03, 3.6398e-03, 1.6887e-03,\n",
      "          4.3170e-03, 8.9187e-04, 8.6468e-02, 6.2540e-02, 9.9400e-03,\n",
      "          3.2753e-03, 6.1398e-04, 3.2704e-03, 5.2051e-03, 6.1692e-04,\n",
      "          6.4014e-02, 3.7370e-04, 6.1535e-03, 3.8921e-03, 7.6316e-04,\n",
      "          4.0865e-03, 8.0421e-03, 3.3234e-02, 6.0896e-02, 9.8993e-04,\n",
      "          7.3261e-03, 2.3841e-03, 2.7820e-02, 6.6187e-02, 3.2586e-03,\n",
      "          3.6458e-02, 1.4104e-02, 1.9341e-01, 2.2469e-01, 5.1565e-04,\n",
      "          4.3797e-04, 7.2141e-04, 3.4560e-04, 5.9712e-04, 2.5280e-04,\n",
      "          4.3903e-04],\n",
      "         [9.6377e-05, 1.1880e-04, 6.0696e-05, 1.0020e-04, 9.1845e-05,\n",
      "          1.2806e-04, 1.2423e-04, 7.5937e-05, 9.2904e-05, 8.5858e-05,\n",
      "          8.3232e-05, 1.2569e-04, 5.2376e-03, 6.5522e-03, 5.1007e-05,\n",
      "          8.7316e-01, 1.4967e-04, 9.3750e-03, 1.0553e-02, 8.4227e-05,\n",
      "          6.5377e-03, 6.7549e-05, 2.3728e-02, 5.0786e-02, 9.8732e-03,\n",
      "          8.2057e-05, 5.2875e-05, 1.6933e-04, 1.1378e-04, 1.4074e-04,\n",
      "          8.0678e-05, 1.8246e-04, 1.3509e-04, 5.6190e-05, 9.4037e-05,\n",
      "          1.1226e-04, 1.2180e-04, 5.9195e-05, 1.4435e-04, 9.9723e-05,\n",
      "          1.4750e-04, 6.7166e-05, 8.4159e-05, 9.8793e-05, 6.1081e-05,\n",
      "          7.7561e-05, 1.0124e-04, 7.4702e-05, 9.2112e-05, 1.0708e-04,\n",
      "          1.0503e-04],\n",
      "         [8.5897e-04, 1.5531e-03, 1.4133e-03, 1.0448e-03, 8.3854e-04,\n",
      "          8.8602e-04, 1.5328e-03, 8.7116e-03, 5.3087e-03, 5.5525e-03,\n",
      "          2.0417e-02, 4.8793e-03, 1.0312e-02, 9.6921e-03, 2.6901e-03,\n",
      "          7.3105e-03, 2.2240e-03, 7.3990e-02, 6.5903e-02, 2.3199e-02,\n",
      "          1.5440e-02, 2.7508e-03, 3.0438e-02, 3.3813e-02, 1.4185e-03,\n",
      "          1.7873e-01, 1.1750e-03, 1.6735e-02, 1.8435e-02, 2.0999e-03,\n",
      "          2.1378e-02, 9.9670e-03, 7.5886e-02, 6.2233e-02, 2.2218e-03,\n",
      "          8.5622e-03, 6.7889e-03, 3.0949e-02, 3.4395e-02, 1.8997e-02,\n",
      "          6.2007e-02, 1.9265e-02, 5.0780e-02, 3.9123e-02, 1.6621e-03,\n",
      "          1.1770e-03, 9.3029e-04, 1.0111e-03, 1.1227e-03, 1.3163e-03,\n",
      "          8.7793e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55613\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5675/10000 (56.8%)\n",
      "    Average reward: +0.755\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4325/10000 (43.2%)\n",
      "    Average reward: -0.755\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13111 (46.9%)\n",
      "    Action 1: 8864 (31.7%)\n",
      "    Action 2: 2950 (10.6%)\n",
      "    Action 3: 3029 (10.8%)\n",
      "  Player 1:\n",
      "    Action 0: 8085 (29.2%)\n",
      "    Action 1: 16033 (58.0%)\n",
      "    Action 2: 673 (2.4%)\n",
      "    Action 3: 2868 (10.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7550.5, -7550.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.975 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.006\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7550\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4084, 0.5725, 0.0191, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9479, 0.0148, 0.0373]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51113\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6140/10000 (61.4%)\n",
      "    Average reward: +0.053\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3860/10000 (38.6%)\n",
      "    Average reward: -0.053\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22366 (81.9%)\n",
      "    Action 1: 4948 (18.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3566 (15.0%)\n",
      "    Action 1: 17412 (73.2%)\n",
      "    Action 2: 653 (2.7%)\n",
      "    Action 3: 2168 (9.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [525.5, -525.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.683 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.740 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.711\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0525\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13003/50000 [11:49<24:18, 25.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 82961/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 85830/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 14000/50000 [12:31<24:25, 24.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 89274/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 92051/2000000\n",
      "P1 SL Buffer Size:  89274\n",
      "P1 SL buffer distribution [36456. 40261.  4283.  8274.]\n",
      "P1 actions distribution [0.40836078 0.45098237 0.04797589 0.09268096]\n",
      "P2 SL Buffer Size:  92051\n",
      "P2 SL buffer distribution [33164. 44848.  4360.  9679.]\n",
      "P2 actions distribution [0.36027854 0.48720818 0.04736505 0.10514823]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[2.1512e-04, 3.1835e-04, 3.5827e-04, 3.4739e-04, 3.3873e-04,\n",
      "          4.1172e-04, 3.8842e-04, 2.0222e-03, 3.7835e-03, 2.2394e-03,\n",
      "          1.8658e-03, 1.9764e-03, 1.6934e-02, 2.2698e-02, 4.4241e-03,\n",
      "          3.2165e-03, 1.3961e-03, 1.8419e-02, 2.2988e-02, 6.5088e-03,\n",
      "          5.4874e-03, 1.9163e-03, 1.3338e-03, 1.7228e-03, 3.7199e-04,\n",
      "          7.6546e-02, 5.3489e-04, 1.2704e-02, 1.2261e-02, 4.5060e-03,\n",
      "          3.1751e-02, 3.2550e-02, 1.9705e-01, 1.4875e-01, 2.8596e-03,\n",
      "          1.0145e-02, 2.1743e-02, 8.2574e-02, 6.6692e-02, 9.7409e-03,\n",
      "          1.8579e-02, 3.1148e-02, 7.2966e-02, 4.3169e-02, 3.1257e-04,\n",
      "          3.2637e-04, 3.3240e-04, 3.6666e-04, 3.2645e-04, 2.6299e-04,\n",
      "          1.2915e-04],\n",
      "         [1.9482e-04, 3.0524e-04, 3.0867e-04, 2.8907e-04, 3.8210e-04,\n",
      "          4.2022e-04, 3.8980e-04, 4.0174e-03, 6.0256e-03, 2.7874e-03,\n",
      "          3.6443e-03, 2.0491e-03, 8.3223e-03, 1.3385e-02, 2.4157e-03,\n",
      "          2.0975e-02, 3.3938e-03, 1.8338e-02, 2.4626e-02, 6.1929e-03,\n",
      "          2.7160e-03, 8.0309e-04, 9.1425e-04, 1.1738e-03, 4.5164e-04,\n",
      "          8.8046e-02, 3.7983e-04, 7.9875e-03, 8.3204e-03, 4.4249e-03,\n",
      "          7.5239e-02, 2.7767e-02, 1.8673e-01, 1.3927e-01, 8.4771e-03,\n",
      "          5.2814e-02, 9.9279e-03, 3.4015e-02, 2.4539e-02, 1.3047e-02,\n",
      "          2.6551e-02, 2.9681e-02, 8.3815e-02, 5.2401e-02, 3.1223e-04,\n",
      "          3.1695e-04, 2.9377e-04, 2.9107e-04, 3.3656e-04, 2.7720e-04,\n",
      "          2.1752e-04],\n",
      "         [9.4278e-06, 1.1166e-05, 1.3583e-05, 1.1163e-05, 9.9464e-06,\n",
      "          1.2463e-05, 8.4046e-06, 1.7232e-05, 1.4869e-05, 9.1271e-06,\n",
      "          6.6549e-06, 9.8428e-06, 9.1057e-04, 7.3179e-04, 1.4489e-05,\n",
      "          1.2558e-03, 1.1541e-05, 1.6815e-03, 2.0124e-03, 1.1176e-05,\n",
      "          4.1003e-04, 1.0922e-05, 7.7512e-03, 2.1842e-01, 7.6633e-01,\n",
      "          1.2874e-05, 1.4641e-05, 1.5481e-05, 1.1354e-05, 2.1656e-05,\n",
      "          1.1405e-05, 1.3104e-05, 1.1358e-05, 9.3659e-06, 1.0342e-05,\n",
      "          1.1800e-05, 1.5948e-05, 1.2478e-05, 1.5220e-05, 1.0848e-05,\n",
      "          9.3916e-06, 7.7085e-06, 1.2696e-05, 9.6332e-06, 1.1825e-05,\n",
      "          9.9186e-06, 1.1581e-05, 1.1838e-05, 8.7874e-06, 9.1481e-06,\n",
      "          1.1222e-05],\n",
      "         [2.9959e-04, 5.6972e-04, 6.0247e-04, 4.4069e-04, 8.8492e-04,\n",
      "          5.8519e-04, 6.7302e-04, 3.3843e-03, 4.0351e-03, 2.1136e-03,\n",
      "          2.3304e-03, 1.0019e-03, 8.5730e-03, 8.5316e-03, 1.1152e-03,\n",
      "          1.6711e-03, 1.0075e-03, 6.1915e-02, 1.0027e-01, 1.2288e-02,\n",
      "          2.0151e-03, 9.2902e-04, 7.4019e-03, 6.6707e-03, 5.3433e-04,\n",
      "          6.1820e-02, 6.6025e-04, 1.7157e-02, 1.5083e-02, 1.5522e-03,\n",
      "          1.0637e-02, 2.6670e-02, 2.1351e-01, 2.0880e-01, 1.0027e-03,\n",
      "          4.2584e-03, 1.0595e-02, 7.1964e-02, 5.3628e-02, 4.9726e-03,\n",
      "          9.6485e-03, 1.7742e-02, 1.9870e-02, 1.6012e-02, 5.9249e-04,\n",
      "          7.5133e-04, 5.1748e-04, 6.1890e-04, 7.5551e-04, 8.3465e-04,\n",
      "          4.9800e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9430, 0.0056, 0.0514]])\n",
      "Player 1 Prediction: tensor([[[1.7606e-04, 3.0330e-04, 2.6026e-04, 1.9994e-04, 2.4788e-04,\n",
      "          2.4412e-04, 2.5556e-04, 8.5613e-04, 1.0811e-03, 9.7538e-04,\n",
      "          2.4071e-02, 7.7541e-03, 2.9244e-03, 4.0822e-03, 1.3782e-03,\n",
      "          2.5942e-02, 3.0349e-03, 6.3611e-03, 6.0105e-03, 2.8786e-03,\n",
      "          2.6915e-02, 5.7767e-03, 8.2605e-04, 7.5104e-04, 2.7001e-04,\n",
      "          2.2563e-01, 3.4102e-04, 1.8081e-03, 1.6199e-03, 1.6173e-02,\n",
      "          2.2350e-01, 4.9310e-03, 3.6288e-02, 2.4481e-02, 1.2125e-02,\n",
      "          7.0073e-02, 4.0143e-03, 1.1864e-02, 1.0453e-02, 3.6886e-02,\n",
      "          1.5960e-01, 5.3978e-03, 1.7183e-02, 1.2247e-02, 2.4817e-04,\n",
      "          2.4348e-04, 2.5856e-04, 3.1349e-04, 2.9367e-04, 3.0894e-04,\n",
      "          1.4645e-04],\n",
      "         [1.1821e-04, 2.1883e-04, 1.9857e-04, 2.0976e-04, 2.5127e-04,\n",
      "          3.2756e-04, 2.5732e-04, 3.8647e-03, 6.7464e-03, 1.8110e-03,\n",
      "          3.4313e-04, 7.8942e-04, 2.0327e-02, 2.3006e-02, 2.5937e-03,\n",
      "          8.3875e-04, 8.5488e-04, 2.3657e-02, 2.6232e-02, 6.1965e-03,\n",
      "          1.0409e-03, 3.4626e-04, 8.9426e-04, 1.3095e-03, 2.1960e-04,\n",
      "          7.5296e-02, 2.8263e-04, 3.1399e-03, 3.0529e-03, 1.3381e-03,\n",
      "          1.0929e-02, 2.0544e-02, 1.9938e-01, 1.7201e-01, 1.1583e-03,\n",
      "          1.5084e-03, 1.3847e-02, 9.2154e-02, 7.9614e-02, 2.4263e-03,\n",
      "          5.1929e-03, 3.0593e-02, 9.7324e-02, 6.6268e-02, 2.0011e-04,\n",
      "          2.1061e-04, 1.9364e-04, 1.8549e-04, 2.3573e-04, 1.5869e-04,\n",
      "          1.0063e-04],\n",
      "         [5.3191e-06, 3.6994e-06, 6.2048e-06, 4.5857e-06, 5.4464e-06,\n",
      "          7.6843e-06, 4.2948e-06, 4.8779e-06, 4.7275e-06, 5.4145e-06,\n",
      "          6.7077e-06, 3.8436e-06, 2.8527e-04, 2.8173e-04, 3.3416e-06,\n",
      "          1.3250e-03, 4.6022e-06, 4.1821e-04, 4.8818e-04, 3.6520e-06,\n",
      "          1.4366e-04, 3.1716e-06, 5.6483e-01, 4.3196e-01, 4.8519e-05,\n",
      "          8.6190e-06, 6.2292e-06, 6.2660e-06, 4.6257e-06, 7.1048e-06,\n",
      "          4.6768e-06, 6.0508e-06, 5.2253e-06, 2.8451e-06, 4.8825e-06,\n",
      "          3.2463e-06, 5.9142e-06, 4.4844e-06, 9.5822e-06, 7.7815e-06,\n",
      "          4.4632e-06, 3.4526e-06, 1.0233e-05, 5.8607e-06, 8.7808e-06,\n",
      "          4.1042e-06, 4.7963e-06, 4.0607e-06, 4.5295e-06, 4.2555e-06,\n",
      "          4.6811e-06],\n",
      "         [6.8356e-04, 6.1345e-04, 6.4159e-04, 5.2793e-04, 4.9564e-04,\n",
      "          8.6099e-04, 5.4773e-04, 3.0416e-03, 5.5540e-03, 2.7647e-03,\n",
      "          4.0328e-03, 1.2605e-03, 1.3185e-02, 1.2597e-02, 1.3039e-03,\n",
      "          2.1448e-03, 1.3819e-03, 5.3261e-02, 4.0813e-02, 8.1822e-03,\n",
      "          1.2944e-02, 1.3526e-03, 9.8472e-03, 1.3025e-02, 9.3461e-04,\n",
      "          5.8005e-02, 8.7246e-04, 2.3451e-02, 2.0918e-02, 1.5586e-03,\n",
      "          2.7117e-02, 1.2974e-02, 2.4568e-01, 1.5323e-01, 1.3712e-03,\n",
      "          7.2024e-03, 1.5219e-02, 6.4665e-02, 8.1363e-02, 5.8381e-03,\n",
      "          3.2019e-02, 8.4497e-03, 2.4502e-02, 1.8860e-02, 7.4793e-04,\n",
      "          6.9724e-04, 7.3194e-04, 5.2498e-04, 7.4064e-04, 7.3694e-04,\n",
      "          5.3081e-04]]])\n",
      "Player 0 Prediction: tensor([[0.9954, 0.0000, 0.0046, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[8.1708e-06, 1.9540e-05, 1.8395e-05, 1.7460e-05, 2.2713e-05,\n",
      "          1.7024e-05, 2.0559e-05, 4.0277e-04, 4.6164e-04, 1.0245e-04,\n",
      "          5.6667e-04, 7.1204e-05, 9.6824e-02, 1.1028e-01, 1.2177e-04,\n",
      "          3.0006e-06, 8.2718e-05, 1.3039e-02, 1.5448e-02, 4.9484e-04,\n",
      "          5.3693e-05, 8.5042e-05, 6.1342e-05, 6.0344e-05, 1.8353e-05,\n",
      "          1.2861e-01, 5.1053e-05, 1.6851e-04, 1.1641e-04, 7.7011e-05,\n",
      "          6.8174e-05, 1.4240e-03, 8.7152e-02, 1.0127e-01, 7.1873e-05,\n",
      "          3.7271e-06, 6.1585e-04, 2.2370e-01, 2.0874e-01, 1.2402e-04,\n",
      "          3.9789e-04, 1.1057e-03, 4.5308e-03, 3.3535e-03, 1.6851e-05,\n",
      "          1.1822e-05, 1.5409e-05, 1.8257e-05, 1.3340e-05, 2.0692e-05,\n",
      "          1.1054e-05],\n",
      "         [1.7670e-04, 3.0838e-04, 2.8917e-04, 3.0833e-04, 4.0268e-04,\n",
      "          4.6434e-04, 3.6955e-04, 1.6157e-02, 2.0677e-02, 3.3926e-03,\n",
      "          6.0688e-04, 8.1763e-04, 4.0821e-02, 3.9266e-02, 8.2236e-04,\n",
      "          6.8690e-04, 8.0511e-04, 1.3146e-02, 1.5447e-02, 4.5153e-03,\n",
      "          4.1701e-03, 4.9335e-04, 2.0602e-03, 3.0852e-03, 2.5612e-04,\n",
      "          2.3478e-01, 4.5280e-04, 3.4347e-03, 3.8324e-03, 1.4968e-03,\n",
      "          2.7439e-03, 8.5998e-03, 1.0445e-01, 9.4531e-02, 7.5599e-04,\n",
      "          6.5591e-04, 3.9049e-03, 8.5058e-02, 8.5952e-02, 9.5707e-04,\n",
      "          3.8309e-03, 2.2055e-02, 9.7209e-02, 7.3740e-02, 3.3219e-04,\n",
      "          2.6498e-04, 3.0340e-04, 3.1989e-04, 2.9736e-04, 3.1338e-04,\n",
      "          1.8222e-04],\n",
      "         [7.1352e-06, 4.5748e-06, 1.0766e-05, 5.5909e-06, 8.1758e-06,\n",
      "          6.2307e-06, 4.8851e-06, 6.0719e-06, 6.7324e-06, 6.5572e-06,\n",
      "          9.6484e-06, 4.8287e-06, 5.1640e-04, 6.8432e-04, 1.1410e-05,\n",
      "          7.6821e-04, 8.0989e-06, 5.0620e-01, 4.9022e-01, 6.2450e-06,\n",
      "          1.6905e-04, 6.9882e-06, 1.1269e-04, 7.1329e-04, 2.7204e-04,\n",
      "          6.6608e-06, 1.4265e-05, 5.6745e-06, 4.9821e-06, 1.7176e-05,\n",
      "          1.0057e-05, 9.8359e-06, 8.9062e-06, 9.2057e-06, 9.0412e-06,\n",
      "          1.0351e-05, 8.4101e-06, 5.7746e-06, 1.1847e-05, 1.0272e-05,\n",
      "          6.0910e-06, 5.3227e-06, 7.1369e-06, 6.6737e-06, 6.5117e-06,\n",
      "          8.9133e-06, 5.1372e-06, 1.2518e-05, 8.0872e-06, 6.1783e-06,\n",
      "          6.5554e-06],\n",
      "         [5.4385e-05, 7.3118e-05, 8.7695e-05, 8.0448e-05, 1.5136e-04,\n",
      "          1.5338e-04, 1.0430e-04, 2.0030e-03, 2.3122e-03, 5.5098e-04,\n",
      "          3.4492e-04, 1.3474e-04, 3.5120e-04, 5.9924e-04, 1.6819e-04,\n",
      "          1.4656e-04, 1.6007e-04, 9.1685e-02, 8.9150e-02, 5.0195e-03,\n",
      "          3.4504e-04, 1.5730e-04, 4.1551e-04, 6.8557e-04, 7.1645e-05,\n",
      "          4.1193e-02, 1.1597e-04, 6.8541e-04, 6.0003e-04, 2.0637e-04,\n",
      "          6.3272e-04, 7.5790e-03, 3.5617e-01, 3.3441e-01, 1.1456e-04,\n",
      "          3.0747e-04, 2.8920e-03, 6.5641e-03, 2.5635e-03, 6.5367e-04,\n",
      "          3.0307e-03, 1.0616e-02, 1.9505e-02, 1.6566e-02, 6.5244e-05,\n",
      "          1.1093e-04, 9.4435e-05, 7.1301e-05, 8.1207e-05, 9.4436e-05,\n",
      "          6.8489e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3631, 0.4795, 0.1575, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.9281e-06, 5.8506e-06, 5.6713e-06, 6.3957e-06, 3.4020e-06,\n",
      "          5.0944e-06, 4.5719e-06, 8.4012e-02, 8.2659e-02, 2.2306e-05,\n",
      "          2.7498e-04, 2.1282e-05, 9.8443e-05, 1.0651e-04, 1.5926e-05,\n",
      "          3.6842e-05, 2.1399e-05, 7.2654e-05, 6.2333e-05, 2.9703e-05,\n",
      "          1.1650e-05, 2.2547e-05, 1.2049e-05, 1.1076e-05, 4.6263e-06,\n",
      "          3.5707e-01, 6.2551e-06, 2.8559e-05, 2.4025e-05, 2.1630e-05,\n",
      "          2.2515e-05, 9.9540e-05, 3.4164e-04, 2.1894e-04, 2.4020e-05,\n",
      "          2.1692e-05, 1.0824e-04, 2.1942e-04, 2.2463e-04, 3.9109e-05,\n",
      "          1.8971e-04, 1.1849e-04, 2.3667e-01, 2.3699e-01, 5.6356e-06,\n",
      "          3.2936e-06, 4.1083e-06, 7.5160e-06, 4.9334e-06, 5.1500e-06,\n",
      "          3.2426e-06],\n",
      "         [5.7083e-04, 6.7219e-04, 4.5831e-04, 4.5387e-04, 9.3643e-04,\n",
      "          6.9737e-04, 5.4042e-04, 1.8083e-02, 1.1580e-02, 3.2731e-03,\n",
      "          3.2563e-03, 2.2979e-03, 2.2288e-02, 3.1679e-02, 2.2098e-03,\n",
      "          8.6312e-03, 1.2796e-03, 2.5695e-02, 2.2720e-02, 5.6847e-03,\n",
      "          3.5282e-03, 9.9828e-04, 1.1833e-03, 3.4169e-03, 3.6595e-04,\n",
      "          4.1602e-01, 1.3871e-03, 7.0068e-03, 7.3024e-03, 1.8509e-03,\n",
      "          1.2169e-02, 7.7862e-03, 1.0346e-01, 1.1170e-01, 1.9542e-03,\n",
      "          8.2482e-03, 6.1048e-03, 1.8778e-02, 2.1949e-02, 6.1524e-03,\n",
      "          1.1451e-02, 7.6096e-03, 2.5807e-02, 4.6167e-02, 9.6280e-04,\n",
      "          6.1344e-04, 4.0316e-04, 8.9258e-04, 5.3932e-04, 6.2797e-04,\n",
      "          5.4997e-04],\n",
      "         [1.2416e-04, 8.8188e-05, 1.6537e-04, 8.6683e-05, 2.8014e-04,\n",
      "          1.7963e-04, 1.4142e-04, 1.0374e-04, 1.2143e-04, 8.2524e-05,\n",
      "          1.8823e-04, 2.0068e-04, 3.8678e-01, 5.5004e-01, 2.3717e-04,\n",
      "          1.4978e-02, 1.8459e-04, 1.1577e-02, 1.3585e-02, 1.1364e-04,\n",
      "          2.5301e-03, 1.1774e-04, 3.3484e-03, 6.9708e-03, 3.4914e-03,\n",
      "          1.7571e-04, 1.0846e-04, 1.4260e-04, 1.0424e-04, 1.4484e-04,\n",
      "          1.2979e-04, 2.9494e-04, 1.6631e-04, 1.9335e-04, 2.4722e-04,\n",
      "          2.2203e-04, 2.2363e-04, 1.6698e-04, 2.1545e-04, 1.5741e-04,\n",
      "          1.1838e-04, 1.0791e-04, 1.9289e-04, 2.2061e-04, 1.2816e-04,\n",
      "          1.5530e-04, 8.4966e-05, 1.9599e-04, 1.0876e-04, 1.1220e-04,\n",
      "          1.6360e-04],\n",
      "         [1.0231e-03, 1.7487e-03, 1.1027e-03, 1.0881e-03, 1.8825e-03,\n",
      "          2.2422e-03, 1.4842e-03, 8.2210e-03, 1.0322e-02, 4.3475e-03,\n",
      "          8.2465e-03, 2.7551e-03, 4.6947e-03, 8.9742e-03, 1.2842e-03,\n",
      "          2.1782e-03, 2.2301e-03, 6.9861e-02, 7.7905e-02, 1.5913e-02,\n",
      "          1.5015e-02, 1.6150e-03, 1.1712e-02, 1.5619e-02, 7.9901e-04,\n",
      "          7.4513e-02, 1.5964e-03, 1.3086e-02, 1.7053e-02, 3.1865e-03,\n",
      "          1.7555e-02, 2.6963e-02, 1.7189e-01, 1.3506e-01, 1.4020e-03,\n",
      "          5.3930e-03, 1.7844e-02, 3.8256e-02, 2.9804e-02, 5.4130e-03,\n",
      "          3.1745e-02, 4.2499e-02, 4.7501e-02, 3.8253e-02, 1.1400e-03,\n",
      "          1.1965e-03, 1.5069e-03, 8.3993e-04, 1.4606e-03, 1.5634e-03,\n",
      "          1.0101e-03]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 14000/50000 [12:49<24:25, 24.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55616\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5404/10000 (54.0%)\n",
      "    Average reward: -0.665\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4596/10000 (46.0%)\n",
      "    Average reward: +0.665\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9975 (36.1%)\n",
      "    Action 1: 13897 (50.3%)\n",
      "    Action 2: 1340 (4.9%)\n",
      "    Action 3: 2389 (8.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10165 (36.3%)\n",
      "    Action 1: 13117 (46.8%)\n",
      "    Action 2: 2787 (9.9%)\n",
      "    Action 3: 1946 (6.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6655.0, 6655.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.036\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6655\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.5823, 0.3935, 0.0242, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2695, 0.1305, 0.5999]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52802\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5207/10000 (52.1%)\n",
      "    Average reward: -0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4793/10000 (47.9%)\n",
      "    Average reward: +0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5056 (19.9%)\n",
      "    Action 1: 15219 (59.8%)\n",
      "    Action 2: 1661 (6.5%)\n",
      "    Action 3: 3525 (13.8%)\n",
      "  Player 1:\n",
      "    Action 0: 20158 (73.7%)\n",
      "    Action 1: 7183 (26.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1901.0, 1901.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.907 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.831 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.869\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1901\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4235, 0.5400, 0.0365, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.5687e-05, 4.8896e-05, 6.3908e-05, 7.6910e-05, 9.0742e-05,\n",
      "          4.8650e-05, 6.1570e-05, 1.1658e-04, 1.7527e-04, 1.8295e-04,\n",
      "          9.6395e-03, 4.8122e-03, 3.5159e-04, 3.6054e-04, 2.7108e-04,\n",
      "          4.0332e-03, 5.8589e-04, 1.1435e-02, 1.8032e-02, 2.9855e-03,\n",
      "          4.6496e-02, 1.8908e-03, 8.5150e-04, 1.7985e-03, 9.4122e-05,\n",
      "          5.8311e-01, 8.2620e-05, 1.8175e-03, 1.6526e-03, 3.1820e-03,\n",
      "          6.7152e-02, 6.5720e-04, 2.4768e-03, 1.9351e-03, 3.0558e-03,\n",
      "          4.6916e-02, 3.8957e-04, 6.2203e-04, 1.2491e-03, 5.3424e-02,\n",
      "          1.2419e-01, 1.3981e-03, 1.1498e-03, 4.9001e-04, 8.5564e-05,\n",
      "          8.7725e-05, 8.4769e-05, 5.4670e-05, 1.0472e-04, 5.5735e-05,\n",
      "          3.9260e-05],\n",
      "         [4.8391e-05, 1.1208e-04, 8.8731e-05, 1.0691e-04, 1.0514e-04,\n",
      "          9.2500e-05, 7.3214e-05, 1.1426e-03, 1.5906e-03, 1.0752e-03,\n",
      "          1.8024e-03, 6.6225e-04, 5.8968e-03, 6.5910e-03, 1.5090e-03,\n",
      "          1.2140e-03, 3.4382e-04, 1.1930e-01, 1.1237e-01, 1.5277e-02,\n",
      "          8.8155e-03, 4.6640e-04, 1.0581e-03, 3.1614e-03, 2.1452e-04,\n",
      "          5.0367e-01, 1.3028e-04, 4.2103e-03, 4.5996e-03, 9.1158e-04,\n",
      "          9.3976e-03, 3.9212e-03, 1.7986e-02, 1.5519e-02, 1.1583e-03,\n",
      "          1.5582e-02, 2.9219e-03, 1.1489e-02, 9.5774e-03, 2.0256e-03,\n",
      "          7.7230e-03, 1.8948e-02, 5.1423e-02, 3.5012e-02, 9.3919e-05,\n",
      "          1.1428e-04, 1.1413e-04, 1.0395e-04, 9.4868e-05, 7.4028e-05,\n",
      "          7.4615e-05],\n",
      "         [7.9335e-07, 1.0234e-06, 7.9194e-07, 1.2467e-06, 1.1065e-06,\n",
      "          1.0970e-06, 1.5006e-06, 6.6234e-07, 1.1401e-06, 6.7777e-07,\n",
      "          6.2352e-07, 1.4212e-06, 4.3050e-05, 5.1017e-05, 5.2826e-07,\n",
      "          1.3899e-04, 1.5512e-06, 3.1799e-05, 3.9019e-05, 6.0499e-07,\n",
      "          2.2952e-05, 6.1060e-07, 4.9012e-01, 5.0072e-01, 8.7909e-03,\n",
      "          9.6626e-07, 8.1411e-07, 9.3900e-07, 1.1283e-06, 9.5205e-07,\n",
      "          1.3202e-06, 1.5844e-06, 1.3973e-06, 3.4425e-07, 1.3179e-06,\n",
      "          1.1650e-06, 9.4263e-07, 1.0019e-06, 9.4066e-07, 1.4527e-06,\n",
      "          1.1796e-06, 7.2033e-07, 5.4412e-07, 1.0704e-06, 6.6025e-07,\n",
      "          5.7462e-07, 6.8510e-07, 1.0939e-06, 9.4847e-07, 9.7849e-07,\n",
      "          7.5777e-07],\n",
      "         [2.0398e-04, 2.5566e-04, 2.7722e-04, 2.5027e-04, 3.2742e-04,\n",
      "          3.1915e-04, 2.9567e-04, 1.2445e-03, 1.7390e-03, 1.3502e-03,\n",
      "          2.4283e-03, 1.2588e-03, 7.8625e-03, 8.8421e-03, 1.3706e-03,\n",
      "          1.0152e-03, 6.0383e-04, 6.7103e-02, 7.2410e-02, 1.1789e-02,\n",
      "          4.8237e-03, 5.4853e-04, 1.9535e-01, 1.8178e-01, 5.5982e-04,\n",
      "          1.5819e-01, 3.7357e-04, 1.2770e-02, 1.2835e-02, 6.3460e-04,\n",
      "          5.0931e-03, 4.3497e-03, 2.5186e-02, 2.1946e-02, 6.0976e-04,\n",
      "          1.3705e-03, 8.0560e-03, 7.6588e-02, 6.5020e-02, 4.0734e-03,\n",
      "          7.1986e-03, 7.2048e-03, 1.3173e-02, 9.2824e-03, 3.3652e-04,\n",
      "          2.8598e-04, 3.3620e-04, 3.1895e-04, 2.7114e-04, 2.7397e-04,\n",
      "          2.1928e-04]]])\n",
      "Player 1 Prediction: tensor([[0.1249, 0.8606, 0.0144, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.0536e-05, 1.8125e-05, 1.8315e-05, 1.7749e-05, 1.6292e-05,\n",
      "          9.7548e-06, 1.8706e-05, 1.7981e-03, 2.4271e-03, 1.2889e-04,\n",
      "          2.2316e-02, 5.4205e-04, 1.7989e-03, 1.4351e-03, 9.7068e-05,\n",
      "          8.2970e-02, 8.6946e-05, 5.2173e-04, 6.8762e-04, 1.1167e-03,\n",
      "          1.9101e-03, 1.8765e-04, 1.7458e-04, 4.3470e-04, 2.5376e-05,\n",
      "          8.2883e-01, 2.3501e-05, 1.0401e-04, 1.0401e-04, 2.0022e-04,\n",
      "          1.3542e-03, 2.1939e-04, 7.4458e-06, 6.9687e-06, 2.5803e-04,\n",
      "          1.8633e-02, 9.8736e-05, 8.0185e-05, 1.7897e-04, 2.2166e-03,\n",
      "          2.6246e-02, 8.2836e-04, 1.1514e-03, 5.7386e-04, 1.9257e-05,\n",
      "          2.0498e-05, 1.3463e-05, 1.8108e-05, 1.8490e-05, 1.6543e-05,\n",
      "          8.5773e-06],\n",
      "         [1.4068e-04, 2.2102e-04, 2.2262e-04, 1.9434e-04, 2.1382e-04,\n",
      "          1.8049e-04, 1.9863e-04, 5.6356e-03, 7.6407e-03, 1.0205e-03,\n",
      "          4.2316e-02, 4.2843e-03, 5.5857e-03, 5.1509e-03, 1.2331e-03,\n",
      "          1.1779e-01, 4.2452e-04, 6.9138e-02, 6.5804e-02, 9.6428e-03,\n",
      "          4.4309e-03, 5.3131e-04, 4.0592e-03, 7.2292e-03, 3.7646e-04,\n",
      "          2.6877e-01, 3.3114e-04, 1.9874e-03, 2.0978e-03, 7.7074e-04,\n",
      "          2.2772e-01, 3.1537e-03, 1.3909e-02, 1.7707e-02, 8.1460e-04,\n",
      "          7.8672e-02, 1.5687e-03, 1.8977e-03, 2.3089e-03, 9.7758e-03,\n",
      "          7.0312e-03, 3.3635e-03, 1.3239e-03, 1.4028e-03, 2.8195e-04,\n",
      "          3.1141e-04, 1.9983e-04, 2.7105e-04, 2.6414e-04, 1.9000e-04,\n",
      "          2.1798e-04],\n",
      "         [4.2345e-06, 1.3003e-05, 3.9385e-06, 1.0218e-05, 8.0861e-06,\n",
      "          9.7411e-06, 9.4312e-06, 7.7834e-06, 5.6185e-06, 3.9230e-06,\n",
      "          4.9755e-06, 7.1220e-06, 1.1454e-03, 7.7947e-04, 4.0916e-06,\n",
      "          2.3507e-03, 1.0386e-05, 3.1081e-04, 2.5958e-04, 4.4638e-06,\n",
      "          9.8692e-01, 5.6375e-06, 8.2723e-04, 6.7706e-03, 3.6887e-04,\n",
      "          7.3523e-06, 3.6989e-06, 4.8131e-06, 7.9078e-06, 4.1154e-06,\n",
      "          4.2260e-06, 1.3289e-05, 4.8973e-06, 6.7317e-06, 5.0700e-06,\n",
      "          6.2464e-06, 3.4937e-06, 5.9746e-06, 6.7293e-06, 4.6004e-06,\n",
      "          1.1684e-05, 5.5751e-06, 3.3443e-06, 8.6481e-06, 4.9773e-06,\n",
      "          3.9411e-06, 6.3855e-06, 6.9133e-06, 5.2971e-06, 8.1072e-06,\n",
      "          5.5298e-06],\n",
      "         [6.6680e-05, 9.1210e-05, 9.5030e-05, 7.9210e-05, 1.0681e-04,\n",
      "          1.0943e-04, 1.0347e-04, 2.9774e-04, 3.7419e-04, 3.2345e-04,\n",
      "          7.0850e-03, 1.2992e-03, 5.4186e-04, 6.4697e-04, 2.4881e-04,\n",
      "          1.2892e-03, 2.1953e-04, 8.8342e-03, 1.0548e-02, 2.7730e-03,\n",
      "          7.2066e-02, 4.5881e-04, 5.2060e-03, 6.4962e-03, 2.0157e-04,\n",
      "          7.4075e-01, 1.1648e-04, 3.6515e-04, 3.6474e-04, 4.1626e-04,\n",
      "          7.6053e-02, 9.6140e-04, 3.0582e-04, 2.7470e-04, 3.3553e-04,\n",
      "          4.2428e-03, 5.6783e-04, 1.4813e-03, 1.7074e-03, 1.0525e-02,\n",
      "          3.9161e-02, 9.9181e-04, 5.7489e-04, 5.1139e-04, 1.2061e-04,\n",
      "          1.0092e-04, 8.5248e-05, 7.7709e-05, 1.3745e-04, 6.4287e-05,\n",
      "          1.4497e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7377, 0.0709, 0.1914]])\n",
      "Player 0 Prediction: tensor([[[1.0866e-06, 1.2567e-06, 1.3202e-06, 1.4107e-06, 1.1596e-06,\n",
      "          7.1173e-07, 1.6030e-06, 2.7122e-04, 3.1038e-04, 1.1918e-05,\n",
      "          6.4602e-04, 2.0411e-05, 3.9400e-04, 3.7682e-04, 8.1870e-06,\n",
      "          7.6557e-01, 4.1790e-06, 6.3711e-05, 7.0640e-05, 1.4262e-04,\n",
      "          4.1579e-05, 1.0103e-05, 1.1205e-05, 3.2881e-05, 1.4039e-06,\n",
      "          1.9599e-01, 2.0336e-06, 1.2258e-05, 7.7339e-06, 8.6887e-06,\n",
      "          2.2597e-05, 2.3197e-05, 8.9115e-07, 7.2110e-07, 1.0558e-05,\n",
      "          3.5345e-02, 1.3442e-05, 6.4736e-06, 1.2438e-05, 6.9050e-05,\n",
      "          3.1215e-04, 7.1198e-05, 5.5091e-05, 3.2673e-05, 1.8964e-06,\n",
      "          1.5751e-06, 9.8729e-07, 1.6790e-06, 8.7288e-07, 1.2278e-06,\n",
      "          6.9908e-07],\n",
      "         [4.3754e-06, 6.7282e-06, 8.6592e-06, 6.8478e-06, 1.0308e-05,\n",
      "          6.7505e-06, 5.1517e-06, 3.4287e-04, 3.6088e-04, 2.8209e-05,\n",
      "          7.5606e-01, 2.1054e-04, 8.8749e-05, 6.4442e-05, 3.7285e-05,\n",
      "          2.3423e-03, 1.9742e-05, 5.8436e-04, 3.9296e-04, 2.3191e-04,\n",
      "          3.2208e-04, 2.7874e-05, 1.2645e-04, 2.4056e-04, 1.4696e-05,\n",
      "          1.7920e-01, 1.1175e-05, 1.0741e-04, 1.4852e-04, 3.2149e-05,\n",
      "          3.4660e-03, 8.0713e-05, 1.2751e-04, 2.3321e-04, 3.6741e-05,\n",
      "          3.2682e-02, 2.4173e-05, 1.3717e-05, 1.3120e-05, 4.7749e-04,\n",
      "          2.1667e-02, 4.7634e-05, 9.9594e-06, 1.1574e-05, 1.3481e-05,\n",
      "          9.7231e-06, 7.5381e-06, 9.2987e-06, 1.1208e-05, 7.5292e-06,\n",
      "          9.0148e-06],\n",
      "         [6.2779e-07, 2.0989e-06, 4.5186e-07, 1.5893e-06, 1.7672e-06,\n",
      "          1.7309e-06, 1.9810e-06, 1.1845e-06, 1.1990e-06, 1.0468e-06,\n",
      "          8.4581e-07, 1.5904e-06, 2.6606e-04, 2.0010e-04, 6.6650e-07,\n",
      "          3.6634e-04, 1.8486e-06, 5.8453e-05, 4.2568e-05, 7.1356e-07,\n",
      "          9.9873e-01, 1.0795e-06, 4.6096e-05, 1.7993e-04, 6.4924e-05,\n",
      "          1.3144e-06, 8.2791e-07, 7.4144e-07, 1.6045e-06, 6.9836e-07,\n",
      "          9.6095e-07, 1.7669e-06, 9.0871e-07, 9.8032e-07, 6.8021e-07,\n",
      "          1.4799e-06, 5.4967e-07, 1.1508e-06, 1.7974e-06, 8.0675e-07,\n",
      "          1.4093e-06, 1.1611e-06, 7.1804e-07, 1.5142e-06, 1.1074e-06,\n",
      "          8.6416e-07, 1.2608e-06, 1.5162e-06, 8.7308e-07, 1.2962e-06,\n",
      "          9.1316e-07],\n",
      "         [4.3609e-05, 6.5648e-05, 4.4727e-05, 5.1150e-05, 7.4998e-05,\n",
      "          5.3805e-05, 5.8795e-05, 1.3275e-04, 2.1800e-04, 1.7551e-04,\n",
      "          3.5648e-03, 8.4326e-04, 3.4375e-04, 3.1430e-04, 1.8172e-04,\n",
      "          6.6422e-04, 2.0941e-04, 1.1132e-02, 1.3876e-02, 1.4163e-03,\n",
      "          3.8204e-02, 2.7406e-04, 2.7506e-03, 4.2210e-03, 1.3372e-04,\n",
      "          8.6215e-01, 5.0526e-05, 1.8835e-04, 2.1775e-04, 2.7565e-04,\n",
      "          2.6758e-02, 5.3172e-04, 1.4005e-04, 1.0138e-04, 1.7654e-04,\n",
      "          2.1703e-03, 2.7948e-04, 7.1009e-04, 1.0734e-03, 6.6486e-03,\n",
      "          1.7830e-02, 5.9574e-04, 3.8300e-04, 2.2215e-04, 6.6396e-05,\n",
      "          7.8766e-05, 4.2169e-05, 3.9904e-05, 7.4864e-05, 4.1089e-05,\n",
      "          1.0620e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54666\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5587/10000 (55.9%)\n",
      "    Average reward: +0.907\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4413/10000 (44.1%)\n",
      "    Average reward: -0.907\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 12919 (47.0%)\n",
      "    Action 1: 10245 (37.3%)\n",
      "    Action 2: 3254 (11.8%)\n",
      "    Action 3: 1048 (3.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7104 (26.1%)\n",
      "    Action 1: 16199 (59.6%)\n",
      "    Action 2: 1217 (4.5%)\n",
      "    Action 3: 2680 (9.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9072.5, -9072.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.997\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9073\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3546, 0.6315, 0.0138, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8002, 0.0245, 0.1753]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51029\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6348/10000 (63.5%)\n",
      "    Average reward: +0.169\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3652/10000 (36.5%)\n",
      "    Average reward: -0.169\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22108 (81.4%)\n",
      "    Action 1: 5047 (18.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3550 (14.9%)\n",
      "    Action 1: 17034 (71.3%)\n",
      "    Action 2: 853 (3.6%)\n",
      "    Action 3: 2437 (10.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1693.0, -1693.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.693 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.756 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.725\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1693\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15005/50000 [13:50<23:20, 24.99it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 95485/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 98686/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15998/50000 [14:30<23:17, 24.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 102285/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 105367/2000000\n",
      "P1 SL Buffer Size:  102285\n",
      "P1 SL buffer distribution [42097. 45763.  5620.  8805.]\n",
      "P1 actions distribution [0.41156572 0.44740676 0.05494452 0.086083  ]\n",
      "P2 SL Buffer Size:  105367\n",
      "P2 SL buffer distribution [37579. 51407.  5582. 10799.]\n",
      "P2 actions distribution [0.35664867 0.4878852  0.05297674 0.10248939]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5327, 0.4635, 0.0038, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.7201e-05, 3.4906e-05, 3.8118e-05, 4.1507e-05, 5.7156e-05,\n",
      "          5.0613e-05, 4.7593e-05, 2.9412e-04, 3.3531e-04, 2.0540e-04,\n",
      "          5.2216e-03, 1.2001e-03, 3.0543e-04, 3.0736e-04, 1.8953e-04,\n",
      "          1.6805e-02, 8.2637e-04, 2.7390e-03, 3.5277e-03, 1.8065e-03,\n",
      "          8.6220e-02, 4.7269e-03, 2.6077e-04, 2.5264e-04, 6.4753e-05,\n",
      "          4.0581e-01, 5.1816e-05, 3.8807e-04, 3.5318e-04, 1.5273e-02,\n",
      "          2.3369e-01, 5.5965e-04, 1.0890e-03, 6.3858e-04, 2.3357e-03,\n",
      "          4.2092e-02, 2.2161e-04, 1.0120e-03, 9.9558e-04, 3.2390e-02,\n",
      "          1.3194e-01, 5.9180e-04, 2.8732e-03, 1.7939e-03, 4.6558e-05,\n",
      "          4.6655e-05, 5.1008e-05, 5.1865e-05, 5.3754e-05, 5.9460e-05,\n",
      "          2.5061e-05],\n",
      "         [5.9386e-05, 9.6164e-05, 1.2673e-04, 1.0309e-04, 1.5686e-04,\n",
      "          1.8961e-04, 1.7210e-04, 2.2621e-03, 2.9041e-03, 1.2217e-03,\n",
      "          5.7451e-04, 8.5239e-04, 1.3693e-02, 1.5782e-02, 2.8364e-03,\n",
      "          1.7162e-03, 6.7530e-04, 1.0399e-01, 1.5964e-01, 2.2704e-02,\n",
      "          7.9835e-03, 5.7702e-04, 1.1302e-03, 1.3171e-03, 1.2386e-04,\n",
      "          3.6353e-01, 1.6230e-04, 1.8516e-03, 2.0137e-03, 1.8663e-03,\n",
      "          1.2697e-02, 8.2357e-03, 4.5353e-02, 2.7152e-02, 7.4204e-04,\n",
      "          1.5205e-03, 5.2699e-03, 5.3438e-02, 4.2902e-02, 5.0274e-03,\n",
      "          1.5138e-02, 1.0308e-02, 3.6123e-02, 2.5009e-02, 1.1413e-04,\n",
      "          9.9064e-05, 1.4486e-04, 1.1459e-04, 1.1991e-04, 1.1448e-04,\n",
      "          6.1027e-05],\n",
      "         [1.4542e-06, 8.2225e-07, 9.3422e-07, 1.4006e-06, 1.3375e-06,\n",
      "          1.5742e-06, 1.4962e-06, 1.3964e-06, 1.0259e-06, 1.2638e-06,\n",
      "          1.3672e-06, 8.2007e-07, 6.0892e-05, 9.7517e-05, 7.2309e-07,\n",
      "          8.0875e-04, 9.0492e-07, 8.1585e-05, 9.1971e-05, 1.2517e-06,\n",
      "          3.9121e-04, 1.0650e-06, 5.1886e-01, 4.7943e-01, 1.2263e-04,\n",
      "          2.9624e-06, 1.5689e-06, 1.5092e-06, 1.1554e-06, 1.3453e-06,\n",
      "          8.5597e-07, 1.4285e-06, 1.2576e-06, 4.9381e-07, 1.4816e-06,\n",
      "          6.5320e-07, 1.1515e-06, 9.1770e-07, 1.4298e-06, 1.8441e-06,\n",
      "          9.4131e-07, 1.0534e-06, 2.6145e-06, 1.9871e-06, 1.1639e-06,\n",
      "          1.4218e-06, 1.6447e-06, 8.3405e-07, 7.8941e-07, 1.2269e-06,\n",
      "          1.4256e-06],\n",
      "         [2.8514e-04, 3.1762e-04, 2.8217e-04, 3.8048e-04, 2.1106e-04,\n",
      "          5.0410e-04, 2.3403e-04, 1.5002e-03, 2.4888e-03, 1.9294e-03,\n",
      "          4.0389e-03, 8.1297e-04, 4.2209e-03, 4.1271e-03, 9.4687e-04,\n",
      "          2.0718e-03, 7.5237e-04, 6.1034e-02, 5.8305e-02, 1.8203e-02,\n",
      "          3.3746e-02, 1.3087e-03, 1.3845e-02, 1.5371e-02, 5.9308e-04,\n",
      "          3.2766e-01, 5.1096e-04, 2.2348e-02, 1.5396e-02, 1.2490e-03,\n",
      "          5.1423e-02, 9.8673e-03, 1.1411e-01, 6.2236e-02, 8.7336e-04,\n",
      "          4.3735e-03, 6.1880e-03, 1.4697e-02, 1.7722e-02, 7.8128e-03,\n",
      "          6.9149e-02, 7.8283e-03, 2.3178e-02, 1.3370e-02, 3.6329e-04,\n",
      "          4.3002e-04, 4.0488e-04, 2.9706e-04, 4.8559e-04, 3.4964e-04,\n",
      "          1.6867e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2313, 0.0407, 0.7281]])\n",
      "Player 1 Prediction: tensor([[[6.6909e-06, 9.8151e-06, 2.0624e-05, 2.2040e-05, 3.2361e-05,\n",
      "          1.9180e-05, 2.3742e-05, 2.5312e-05, 2.7455e-05, 1.7880e-04,\n",
      "          1.4324e-03, 1.8969e-04, 8.7113e-06, 9.6489e-06, 8.1869e-05,\n",
      "          1.7813e-02, 1.6195e-04, 3.0041e-05, 8.3846e-05, 1.2492e-03,\n",
      "          5.9687e-03, 4.8715e-04, 9.5246e-05, 1.2509e-04, 1.9206e-05,\n",
      "          5.9916e-02, 1.6098e-05, 2.2753e-04, 1.2730e-04, 1.2646e-03,\n",
      "          1.2587e-02, 5.2639e-04, 2.8300e-04, 1.6888e-04, 4.5170e-04,\n",
      "          3.7746e-01, 1.7162e-04, 4.6089e-03, 4.1608e-03, 3.8476e-03,\n",
      "          4.2121e-01, 9.0680e-04, 5.3590e-02, 3.0234e-02, 1.3270e-05,\n",
      "          1.7924e-05, 1.4521e-05, 2.0441e-05, 2.0007e-05, 1.8011e-05,\n",
      "          8.9297e-06],\n",
      "         [4.2905e-05, 6.4930e-05, 1.0675e-04, 7.2603e-05, 1.1987e-04,\n",
      "          1.5260e-04, 1.6250e-04, 1.8082e-04, 1.8733e-04, 6.9085e-04,\n",
      "          8.4426e-04, 1.1853e-03, 6.8074e-04, 7.2609e-04, 1.2199e-03,\n",
      "          2.8006e-02, 3.8831e-04, 7.2625e-03, 7.4754e-03, 3.6159e-03,\n",
      "          5.9832e-03, 3.5119e-04, 8.5873e-04, 1.0009e-03, 1.0319e-04,\n",
      "          4.5362e-03, 1.4089e-04, 1.0419e-03, 1.6027e-03, 8.3022e-04,\n",
      "          1.9669e-01, 2.1936e-03, 1.2012e-02, 5.3724e-03, 4.0613e-04,\n",
      "          2.1186e-01, 1.6664e-03, 1.0248e-02, 9.7692e-03, 1.9686e-02,\n",
      "          4.2753e-01, 2.3693e-03, 1.5104e-02, 1.4752e-02, 1.1307e-04,\n",
      "          8.6124e-05, 1.3493e-04, 1.0165e-04, 8.9353e-05, 1.1370e-04,\n",
      "          6.8089e-05],\n",
      "         [2.5635e-06, 1.7466e-06, 2.8270e-06, 1.9612e-06, 2.9966e-06,\n",
      "          2.5200e-06, 2.9913e-06, 2.0233e-06, 1.9258e-06, 2.6665e-06,\n",
      "          1.7129e-06, 1.9595e-06, 2.4507e-04, 3.8538e-04, 1.8907e-06,\n",
      "          4.1769e-03, 2.2053e-06, 1.1301e-04, 9.9497e-05, 1.6130e-06,\n",
      "          9.8156e-01, 2.1927e-06, 1.4767e-03, 1.1754e-02, 9.2305e-05,\n",
      "          5.2197e-06, 1.9932e-06, 2.4174e-06, 1.8242e-06, 2.0515e-06,\n",
      "          1.5262e-06, 3.9957e-06, 3.2651e-06, 1.0492e-06, 3.4977e-06,\n",
      "          1.4164e-06, 2.1184e-06, 1.2761e-06, 1.2464e-06, 2.6313e-06,\n",
      "          2.4041e-06, 1.8605e-06, 4.8573e-06, 4.5508e-06, 1.3519e-06,\n",
      "          2.4013e-06, 1.5486e-06, 2.3214e-06, 1.8506e-06, 2.6691e-06,\n",
      "          2.2383e-06],\n",
      "         [1.1681e-04, 8.9582e-05, 9.3557e-05, 1.0812e-04, 1.0412e-04,\n",
      "          1.0726e-04, 9.2530e-05, 7.9443e-04, 8.1397e-04, 5.6938e-04,\n",
      "          2.9379e-03, 3.0286e-04, 4.5150e-04, 7.0988e-04, 2.1190e-04,\n",
      "          5.0158e-04, 1.7196e-04, 1.0754e-03, 1.2036e-03, 2.5563e-03,\n",
      "          1.7167e-01, 4.3099e-04, 1.3027e-03, 1.4437e-03, 1.3013e-04,\n",
      "          3.6087e-02, 1.2403e-04, 1.6366e-03, 1.7736e-03, 6.1434e-04,\n",
      "          5.7286e-01, 8.8838e-04, 8.4445e-03, 9.5547e-03, 2.8650e-04,\n",
      "          2.0969e-03, 1.0170e-03, 1.5695e-03, 1.6859e-03, 2.6871e-03,\n",
      "          1.3700e-01, 3.0813e-03, 1.5888e-02, 1.3844e-02, 9.0691e-05,\n",
      "          1.1348e-04, 1.4877e-04, 1.0271e-04, 1.8824e-04, 1.6538e-04,\n",
      "          6.4730e-05]]])\n",
      "Player 0 Prediction: tensor([[0.2245, 0.5304, 0.2451, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54282\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5261/10000 (52.6%)\n",
      "    Average reward: -0.628\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4739/10000 (47.4%)\n",
      "    Average reward: +0.628\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10126 (37.9%)\n",
      "    Action 1: 12904 (48.3%)\n",
      "    Action 2: 1617 (6.1%)\n",
      "    Action 3: 2052 (7.7%)\n",
      "  Player 1:\n",
      "    Action 0: 8396 (30.4%)\n",
      "    Action 1: 14422 (52.3%)\n",
      "    Action 2: 2668 (9.7%)\n",
      "    Action 3: 2097 (7.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6276.0, 6276.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6276\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.6314, 0.3507, 0.0179, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1887, 0.1578, 0.6535]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15998/50000 [14:49<23:17, 24.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53067\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5035/10000 (50.3%)\n",
      "    Average reward: -0.179\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4965/10000 (49.6%)\n",
      "    Average reward: +0.179\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5073 (19.8%)\n",
      "    Action 1: 14791 (57.6%)\n",
      "    Action 2: 2129 (8.3%)\n",
      "    Action 3: 3672 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 19879 (72.5%)\n",
      "    Action 1: 7523 (27.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1789.5, 1789.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.848 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.884\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1789\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3704, 0.6193, 0.0103, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.9698e-04, 2.3959e-04, 2.0165e-04, 2.7357e-04, 2.9618e-04,\n",
      "          2.2835e-04, 2.3616e-04, 1.0789e-03, 1.8335e-03, 1.9800e-03,\n",
      "          1.5962e-02, 7.7348e-03, 5.8858e-03, 6.6306e-03, 5.8129e-03,\n",
      "          2.5406e-03, 1.1103e-03, 1.8642e-02, 2.5681e-02, 4.1972e-03,\n",
      "          2.5900e-02, 3.6687e-03, 1.6530e-03, 3.0755e-03, 4.5745e-04,\n",
      "          1.6643e-01, 3.0551e-04, 5.4937e-03, 5.0251e-03, 5.4671e-03,\n",
      "          1.9172e-01, 1.5083e-02, 1.0800e-01, 9.8255e-02, 6.2683e-03,\n",
      "          3.4626e-02, 1.2720e-02, 1.7391e-02, 2.2313e-02, 5.0608e-02,\n",
      "          8.4975e-02, 1.4708e-02, 1.5101e-02, 8.1411e-03, 2.6911e-04,\n",
      "          3.0930e-04, 3.4773e-04, 1.9019e-04, 3.3910e-04, 1.9982e-04,\n",
      "          2.0052e-04],\n",
      "         [9.7414e-05, 1.3356e-04, 1.5439e-04, 1.8624e-04, 1.5173e-04,\n",
      "          1.6091e-04, 1.7948e-04, 5.3376e-03, 8.8734e-03, 3.8017e-03,\n",
      "          5.4371e-03, 1.8922e-03, 1.4747e-02, 1.9103e-02, 7.9230e-03,\n",
      "          1.7438e-03, 4.4940e-04, 1.6949e-02, 1.8357e-02, 3.7186e-03,\n",
      "          6.1435e-03, 5.9903e-04, 8.8251e-04, 2.6894e-03, 3.4356e-04,\n",
      "          1.2329e-01, 1.9949e-04, 7.5943e-03, 7.6397e-03, 2.1201e-03,\n",
      "          4.6504e-02, 2.4739e-02, 1.6732e-01, 1.5793e-01, 2.1978e-03,\n",
      "          1.4197e-02, 1.4342e-02, 4.7365e-02, 3.5233e-02, 7.1450e-03,\n",
      "          2.0907e-02, 3.4310e-02, 9.9560e-02, 6.6248e-02, 1.6806e-04,\n",
      "          1.9528e-04, 1.5239e-04, 1.4252e-04, 1.6114e-04, 1.7821e-04,\n",
      "          9.8654e-05],\n",
      "         [7.2682e-06, 5.3814e-06, 8.0426e-06, 6.9821e-06, 9.9777e-06,\n",
      "          8.8234e-06, 9.7483e-06, 4.1324e-06, 9.4597e-06, 7.0269e-06,\n",
      "          6.4158e-06, 8.4755e-06, 4.1597e-04, 4.9107e-04, 5.1021e-06,\n",
      "          6.9974e-04, 1.0141e-05, 7.4532e-04, 1.0087e-03, 5.8477e-06,\n",
      "          2.0911e-04, 4.1723e-06, 4.9492e-01, 4.9290e-01, 8.2959e-03,\n",
      "          7.8392e-06, 9.1977e-06, 1.0227e-05, 9.6192e-06, 7.4395e-06,\n",
      "          7.8809e-06, 7.9859e-06, 1.2784e-05, 4.1015e-06, 7.7793e-06,\n",
      "          9.2367e-06, 7.5342e-06, 6.6481e-06, 7.4258e-06, 9.3417e-06,\n",
      "          9.0365e-06, 7.3737e-06, 5.2848e-06, 7.8566e-06, 6.0849e-06,\n",
      "          4.3252e-06, 4.5266e-06, 8.9314e-06, 7.0849e-06, 7.2813e-06,\n",
      "          9.6442e-06],\n",
      "         [5.1470e-04, 4.4682e-04, 4.8510e-04, 4.7079e-04, 4.9804e-04,\n",
      "          3.7155e-04, 3.9989e-04, 1.8775e-03, 2.8123e-03, 1.6800e-03,\n",
      "          4.9073e-03, 2.1935e-03, 9.4827e-03, 1.0879e-02, 1.3766e-03,\n",
      "          1.5517e-03, 8.9237e-04, 5.0626e-02, 5.9191e-02, 8.6086e-03,\n",
      "          2.2417e-03, 1.0160e-03, 3.9332e-02, 4.5618e-02, 1.0274e-03,\n",
      "          6.2950e-02, 5.0033e-04, 3.3144e-02, 2.9025e-02, 1.0414e-03,\n",
      "          8.7580e-03, 8.8101e-03, 2.0662e-01, 2.0500e-01, 1.8951e-03,\n",
      "          3.0285e-03, 1.0012e-02, 5.0264e-02, 4.3805e-02, 7.2461e-03,\n",
      "          1.3293e-02, 1.5159e-02, 2.9300e-02, 1.8468e-02, 4.2640e-04,\n",
      "          4.3108e-04, 5.3159e-04, 5.2598e-04, 3.7415e-04, 5.2395e-04,\n",
      "          3.7149e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0568, 0.9405, 0.0027, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[6.7935e-05, 9.5742e-05, 8.4570e-05, 9.0439e-05, 8.2657e-05,\n",
      "          8.5716e-05, 9.4668e-05, 5.8186e-03, 8.7392e-03, 2.4773e-03,\n",
      "          9.7604e-04, 6.1469e-04, 2.0681e-02, 2.3473e-02, 4.5317e-03,\n",
      "          2.4666e-04, 1.6672e-04, 2.1486e-02, 2.0891e-02, 3.2862e-03,\n",
      "          2.8232e-04, 1.8547e-04, 8.0181e-04, 2.8398e-03, 2.3142e-04,\n",
      "          1.0527e-01, 1.2665e-04, 1.2464e-03, 1.0068e-03, 3.4470e-04,\n",
      "          1.4796e-03, 1.6839e-02, 1.9171e-01, 1.9864e-01, 2.9186e-04,\n",
      "          8.0490e-04, 9.9913e-03, 7.0265e-02, 6.0198e-02, 8.3613e-04,\n",
      "          1.4492e-03, 2.7003e-02, 1.0796e-01, 8.5601e-02, 7.9863e-05,\n",
      "          9.1962e-05, 1.0008e-04, 8.5076e-05, 8.0102e-05, 9.0760e-05,\n",
      "          7.7113e-05],\n",
      "         [1.6607e-04, 1.8298e-04, 2.1953e-04, 2.6037e-04, 1.9643e-04,\n",
      "          1.9507e-04, 2.4324e-04, 2.2533e-04, 4.0494e-04, 3.0730e-03,\n",
      "          7.1817e-02, 6.2707e-03, 3.7506e-02, 3.3320e-02, 2.2554e-03,\n",
      "          1.3142e-02, 5.4412e-04, 6.3003e-04, 9.0873e-04, 1.2073e-03,\n",
      "          4.8419e-03, 7.4683e-04, 1.8111e-03, 2.8080e-03, 3.9186e-04,\n",
      "          8.1920e-02, 2.2632e-04, 1.5203e-03, 1.8669e-03, 1.3279e-03,\n",
      "          1.8098e-01, 6.7117e-03, 3.3382e-02, 3.9114e-02, 8.2880e-04,\n",
      "          2.0503e-01, 2.0497e-03, 2.2617e-02, 2.6495e-02, 2.5663e-02,\n",
      "          1.7529e-01, 8.1965e-03, 1.0405e-03, 7.5241e-04, 2.9572e-04,\n",
      "          3.1688e-04, 1.5068e-04, 2.1593e-04, 2.3805e-04, 2.7586e-04,\n",
      "          1.2952e-04],\n",
      "         [3.4248e-06, 3.1108e-06, 2.3275e-06, 2.8413e-06, 5.9352e-06,\n",
      "          3.7214e-06, 5.1595e-06, 2.7315e-06, 1.9496e-06, 3.2972e-06,\n",
      "          2.9884e-06, 2.8498e-06, 4.3873e-04, 2.5233e-04, 2.0596e-06,\n",
      "          1.0745e-04, 3.0874e-06, 2.5516e-03, 1.5108e-03, 3.1220e-06,\n",
      "          9.9483e-01, 2.6098e-06, 1.3164e-05, 1.0670e-04, 6.1996e-05,\n",
      "          3.2774e-06, 3.5100e-06, 2.9191e-06, 4.6918e-06, 2.6237e-06,\n",
      "          2.4842e-06, 2.5634e-06, 3.3513e-06, 2.7436e-06, 1.8921e-06,\n",
      "          3.9236e-06, 2.4201e-06, 2.0499e-06, 3.7619e-06, 1.8317e-06,\n",
      "          3.4207e-06, 2.6584e-06, 1.3728e-06, 3.9312e-06, 2.7507e-06,\n",
      "          2.9850e-06, 2.2721e-06, 3.3000e-06, 2.9244e-06, 4.1393e-06,\n",
      "          3.8145e-06],\n",
      "         [6.4648e-04, 4.3046e-04, 4.1603e-04, 5.4367e-04, 6.2337e-04,\n",
      "          4.6997e-04, 3.5721e-04, 3.0691e-03, 6.3737e-03, 2.1975e-03,\n",
      "          1.5752e-02, 4.7764e-03, 1.9514e-03, 1.8724e-03, 1.1403e-03,\n",
      "          3.9639e-03, 1.3471e-03, 5.1253e-02, 7.4397e-02, 7.5115e-03,\n",
      "          2.2537e-02, 2.1680e-03, 1.3787e-03, 1.5567e-03, 8.3749e-04,\n",
      "          2.7065e-01, 5.3812e-04, 1.2631e-03, 1.2471e-03, 1.5181e-03,\n",
      "          8.1723e-02, 9.0121e-03, 1.0815e-01, 1.1215e-01, 2.6413e-03,\n",
      "          1.0863e-02, 1.8184e-03, 3.1871e-03, 4.8944e-03, 3.0472e-02,\n",
      "          5.2945e-02, 2.0190e-02, 5.1283e-02, 2.3917e-02, 5.1578e-04,\n",
      "          7.4996e-04, 4.4265e-04, 6.1930e-04, 5.7482e-04, 3.3088e-04,\n",
      "          7.3667e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2978, 0.0134, 0.6888]])\n",
      "Player 0 Prediction: tensor([[[9.2625e-06, 1.3936e-05, 1.3317e-05, 1.1560e-05, 1.6772e-05,\n",
      "          1.9317e-05, 1.3623e-05, 8.3151e-04, 1.0772e-03, 2.1409e-04,\n",
      "          2.2916e-04, 9.7566e-05, 9.4196e-02, 1.0172e-01, 3.7825e-04,\n",
      "          2.5094e-06, 3.6597e-05, 1.2726e-02, 1.2950e-02, 2.1569e-04,\n",
      "          5.4267e-05, 4.4740e-05, 1.1623e-04, 2.4212e-04, 3.4837e-05,\n",
      "          1.6345e-01, 1.4935e-05, 1.0961e-04, 8.4514e-05, 6.1524e-05,\n",
      "          1.6982e-04, 1.1378e-03, 1.5678e-01, 1.2433e-01, 6.2364e-05,\n",
      "          7.2865e-06, 1.3657e-03, 1.8118e-01, 1.3607e-01, 1.5310e-04,\n",
      "          1.1375e-04, 1.5866e-03, 4.1584e-03, 3.8002e-03, 1.5568e-05,\n",
      "          9.5656e-06, 1.4863e-05, 1.0333e-05, 1.6534e-05, 1.6819e-05,\n",
      "          1.2394e-05],\n",
      "         [5.7849e-05, 9.0587e-05, 9.1714e-05, 9.3906e-05, 9.1233e-05,\n",
      "          1.1091e-04, 7.7709e-05, 1.6433e-02, 2.2563e-02, 3.2930e-03,\n",
      "          2.1785e-03, 7.8258e-04, 3.6579e-02, 3.4831e-02, 1.1110e-03,\n",
      "          2.6787e-04, 2.0364e-04, 6.0683e-03, 7.6942e-03, 1.0073e-03,\n",
      "          1.2386e-03, 1.8332e-04, 8.8635e-04, 1.4023e-03, 1.3456e-04,\n",
      "          1.3749e-01, 7.8026e-05, 1.0270e-03, 8.8282e-04, 6.5376e-04,\n",
      "          7.0609e-04, 3.9599e-03, 1.2892e-01, 1.2871e-01, 4.3781e-04,\n",
      "          1.0435e-03, 1.5716e-03, 1.2506e-01, 1.2427e-01, 9.5077e-04,\n",
      "          1.1096e-03, 1.6939e-02, 1.0588e-01, 8.2243e-02, 1.0793e-04,\n",
      "          9.5147e-05, 7.1682e-05, 9.8040e-05, 9.4584e-05, 7.1495e-05,\n",
      "          5.4767e-05],\n",
      "         [7.6287e-06, 4.5598e-06, 5.8247e-06, 5.5072e-06, 7.3674e-06,\n",
      "          8.0392e-06, 7.1618e-06, 4.8812e-06, 6.3570e-06, 8.6965e-06,\n",
      "          7.7741e-06, 6.1250e-06, 7.3119e-04, 5.8804e-04, 3.5356e-06,\n",
      "          2.0198e-04, 1.0477e-05, 4.9940e-01, 4.9644e-01, 8.8083e-06,\n",
      "          3.0571e-04, 5.8548e-06, 4.0234e-04, 1.2852e-03, 3.1789e-04,\n",
      "          5.9074e-06, 8.8209e-06, 1.1520e-05, 1.0631e-05, 1.2924e-05,\n",
      "          5.5249e-06, 5.8513e-06, 1.6264e-05, 4.2452e-06, 5.4135e-06,\n",
      "          1.5516e-05, 7.9731e-06, 3.8429e-06, 9.6525e-06, 6.6428e-06,\n",
      "          6.7903e-06, 7.9323e-06, 5.3044e-06, 8.1602e-06, 6.2547e-06,\n",
      "          7.7514e-06, 5.2267e-06, 4.9066e-06, 1.0246e-05, 5.6793e-06,\n",
      "          1.8145e-05],\n",
      "         [7.9213e-05, 5.4116e-05, 4.7121e-05, 6.0237e-05, 4.6344e-05,\n",
      "          4.5365e-05, 5.0389e-05, 1.2909e-03, 1.3583e-03, 5.9132e-04,\n",
      "          3.0023e-04, 2.4294e-04, 4.6123e-04, 4.7983e-04, 1.4590e-04,\n",
      "          1.6446e-04, 1.0963e-04, 1.1971e-01, 1.1996e-01, 2.8649e-03,\n",
      "          1.7764e-04, 1.6784e-04, 7.4672e-04, 1.6967e-03, 1.3660e-04,\n",
      "          7.0630e-02, 5.9895e-05, 1.0386e-03, 8.1566e-04, 1.2212e-04,\n",
      "          3.2455e-04, 3.1040e-03, 3.0213e-01, 3.0371e-01, 1.3667e-04,\n",
      "          2.6860e-04, 4.3577e-04, 1.1389e-03, 1.0008e-03, 6.7260e-04,\n",
      "          7.2088e-04, 9.9836e-03, 2.6539e-02, 2.5737e-02, 7.0608e-05,\n",
      "          7.0497e-05, 7.3859e-05, 6.6420e-05, 4.5317e-05, 5.4755e-05,\n",
      "          4.8854e-05]]])\n",
      "Player 1 Prediction: tensor([[0.3920, 0.3800, 0.2280, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57275\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5902/10000 (59.0%)\n",
      "    Average reward: +0.707\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4098/10000 (41.0%)\n",
      "    Average reward: -0.707\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11457 (39.9%)\n",
      "    Action 1: 13862 (48.3%)\n",
      "    Action 2: 2713 (9.5%)\n",
      "    Action 3: 666 (2.3%)\n",
      "  Player 1:\n",
      "    Action 0: 10385 (36.3%)\n",
      "    Action 1: 14546 (50.9%)\n",
      "    Action 2: 1841 (6.4%)\n",
      "    Action 3: 1805 (6.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7068.0, -7068.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.036 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.031\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7068\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7287, 0.0189, 0.2524]])\n",
      "Player 1 Prediction: tensor([[0.5329, 0.4102, 0.0569, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51417\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6231/10000 (62.3%)\n",
      "    Average reward: +0.038\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3769/10000 (37.7%)\n",
      "    Average reward: -0.038\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21948 (80.6%)\n",
      "    Action 1: 5295 (19.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3739 (15.5%)\n",
      "    Action 1: 16958 (70.1%)\n",
      "    Action 2: 1039 (4.3%)\n",
      "    Action 3: 2438 (10.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [376.0, -376.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.711 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.775 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.743\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0376\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17003/50000 [15:47<24:06, 22.81it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 108760/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 111950/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17998/50000 [16:30<21:56, 24.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 115290/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 118096/2000000\n",
      "P1 SL Buffer Size:  115290\n",
      "P1 SL buffer distribution [47323. 51817.  6905.  9245.]\n",
      "P1 actions distribution [0.41046925 0.44944922 0.05989245 0.08018909]\n",
      "P2 SL Buffer Size:  118096\n",
      "P2 SL buffer distribution [41459. 58122.  6780. 11735.]\n",
      "P2 actions distribution [0.35106185 0.49215892 0.05741092 0.09936831]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5367, 0.4608, 0.0025, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[4.8083e-05, 9.7348e-05, 9.2443e-05, 1.2677e-04, 1.5379e-04,\n",
      "          2.1279e-04, 1.2491e-04, 6.5451e-04, 1.0012e-03, 7.6147e-04,\n",
      "          2.0106e-03, 1.6322e-03, 4.0931e-04, 4.8985e-04, 6.8877e-04,\n",
      "          2.3079e-02, 2.1538e-03, 2.6303e-02, 2.9145e-02, 1.2150e-02,\n",
      "          5.7040e-02, 5.1136e-03, 2.4483e-03, 2.4386e-03, 1.6853e-04,\n",
      "          3.8573e-01, 1.2613e-04, 4.6901e-03, 4.0364e-03, 1.6708e-02,\n",
      "          1.8395e-01, 4.3215e-03, 3.2619e-02, 1.8357e-02, 3.3300e-03,\n",
      "          5.5355e-02, 1.6320e-03, 2.3768e-03, 2.3057e-03, 3.5140e-02,\n",
      "          4.7149e-02, 4.9226e-03, 1.8194e-02, 9.5489e-03, 1.4747e-04,\n",
      "          1.5293e-04, 1.4580e-04, 1.3590e-04, 1.7571e-04, 1.3991e-04,\n",
      "          6.4995e-05],\n",
      "         [6.8700e-05, 9.8307e-05, 1.4863e-04, 1.2087e-04, 1.5108e-04,\n",
      "          1.6075e-04, 1.5510e-04, 5.4746e-04, 7.6715e-04, 8.6829e-04,\n",
      "          2.2950e-02, 3.5017e-03, 3.3554e-02, 3.7143e-02, 4.4083e-03,\n",
      "          4.1749e-02, 3.3089e-03, 1.2413e-02, 1.8106e-02, 9.6887e-03,\n",
      "          1.9758e-02, 9.4367e-04, 1.0369e-03, 1.2012e-03, 1.4089e-04,\n",
      "          1.5409e-01, 1.5541e-04, 3.7263e-03, 4.0514e-03, 1.1620e-02,\n",
      "          3.2115e-01, 4.0937e-03, 1.2588e-02, 8.4002e-03, 5.4719e-03,\n",
      "          5.7233e-02, 3.0531e-03, 8.5447e-03, 7.3221e-03, 3.3659e-02,\n",
      "          1.3414e-01, 5.2272e-03, 7.0935e-03, 4.6048e-03, 1.2532e-04,\n",
      "          1.0900e-04, 1.5001e-04, 9.9903e-05, 1.1968e-04, 1.2273e-04,\n",
      "          5.8996e-05],\n",
      "         [1.3205e-06, 1.4336e-06, 1.0935e-06, 1.9253e-06, 1.7696e-06,\n",
      "          1.7501e-06, 1.7310e-06, 2.5314e-06, 1.9162e-06, 1.6051e-06,\n",
      "          1.2214e-06, 1.0207e-06, 7.3723e-05, 1.1326e-04, 1.4203e-06,\n",
      "          3.4575e-04, 1.3255e-06, 5.4157e-05, 4.7578e-05, 1.7397e-06,\n",
      "          8.4752e-04, 1.6092e-06, 4.6850e-01, 5.2008e-01, 9.8732e-03,\n",
      "          2.9825e-06, 2.3290e-06, 2.1706e-06, 1.5934e-06, 1.4638e-06,\n",
      "          1.3522e-06, 1.9224e-06, 1.3450e-06, 5.4733e-07, 1.7151e-06,\n",
      "          6.2231e-07, 1.8722e-06, 1.1173e-06, 1.4312e-06, 1.5212e-06,\n",
      "          1.0130e-06, 1.3440e-06, 2.3315e-06, 2.2634e-06, 1.2232e-06,\n",
      "          2.1018e-06, 2.4747e-06, 1.3168e-06, 8.1802e-07, 1.7912e-06,\n",
      "          1.9285e-06],\n",
      "         [1.5262e-04, 1.8446e-04, 1.2463e-04, 2.1415e-04, 1.6720e-04,\n",
      "          2.5832e-04, 1.3330e-04, 1.0525e-03, 1.1869e-03, 9.5441e-04,\n",
      "          1.5892e-03, 5.5801e-04, 2.2313e-03, 2.3261e-03, 6.0976e-04,\n",
      "          1.8420e-03, 4.3315e-04, 8.2311e-02, 9.4132e-02, 1.6145e-02,\n",
      "          5.9631e-03, 1.0600e-03, 7.5302e-03, 7.0902e-03, 2.3851e-04,\n",
      "          5.1716e-01, 2.1233e-04, 1.0829e-02, 9.9015e-03, 1.3124e-03,\n",
      "          1.3051e-02, 9.6195e-03, 8.7509e-02, 5.0582e-02, 5.5566e-04,\n",
      "          2.3432e-03, 2.7280e-03, 7.0099e-03, 5.5654e-03, 7.1934e-03,\n",
      "          1.3593e-02, 6.5412e-03, 1.5329e-02, 8.8264e-03, 2.4239e-04,\n",
      "          3.1796e-04, 2.3939e-04, 2.3306e-04, 2.4527e-04, 2.5764e-04,\n",
      "          1.1788e-04]]])\n",
      "Player 0 Prediction: tensor([[0.3009, 0.6960, 0.0031, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[4.9933e-05, 6.9081e-05, 1.0920e-04, 1.1164e-04, 1.7048e-04,\n",
      "          1.6738e-04, 1.1429e-04, 2.5955e-03, 3.1651e-03, 9.9240e-04,\n",
      "          7.4344e-04, 2.2047e-04, 2.3998e-03, 2.5979e-03, 6.8390e-04,\n",
      "          6.3507e-04, 3.1233e-04, 9.0290e-02, 1.7026e-01, 2.4084e-02,\n",
      "          1.9098e-03, 4.7174e-04, 1.3257e-03, 1.7626e-03, 1.3165e-04,\n",
      "          3.6563e-01, 1.0820e-04, 1.4126e-03, 9.6069e-04, 5.4004e-04,\n",
      "          1.7449e-03, 9.3055e-03, 7.0163e-02, 4.2271e-02, 2.4372e-04,\n",
      "          7.8602e-04, 2.1822e-03, 1.9011e-02, 1.4117e-02, 1.6075e-03,\n",
      "          5.5157e-03, 1.5188e-02, 8.2268e-02, 6.0906e-02, 1.0705e-04,\n",
      "          8.9792e-05, 1.0761e-04, 9.9466e-05, 9.7456e-05, 1.0233e-04,\n",
      "          6.3807e-05],\n",
      "         [6.6541e-05, 1.2852e-04, 1.7384e-04, 9.7481e-05, 1.9215e-04,\n",
      "          1.6553e-04, 1.4859e-04, 1.0941e-05, 1.1150e-05, 5.4253e-04,\n",
      "          4.6428e-02, 2.4020e-03, 6.8892e-02, 6.9071e-02, 5.3939e-04,\n",
      "          3.6780e-02, 6.0791e-04, 2.9990e-04, 4.5205e-04, 2.5426e-03,\n",
      "          1.0487e-02, 5.0669e-04, 1.2513e-03, 1.3066e-03, 1.6601e-04,\n",
      "          4.9053e-02, 1.4829e-04, 1.0196e-03, 1.6684e-03, 1.5052e-03,\n",
      "          3.8800e-01, 1.0200e-03, 6.3551e-03, 3.2123e-03, 3.3952e-04,\n",
      "          7.0330e-02, 3.9523e-04, 9.2697e-03, 7.0914e-03, 1.9561e-02,\n",
      "          1.9595e-01, 6.8026e-04, 9.4947e-05, 6.8522e-05, 1.7074e-04,\n",
      "          1.1109e-04, 1.6134e-04, 1.2052e-04, 1.7009e-04, 1.3119e-04,\n",
      "          1.0522e-04],\n",
      "         [1.1881e-06, 9.3734e-07, 1.1696e-06, 1.4431e-06, 9.3242e-07,\n",
      "          1.1224e-06, 1.3194e-06, 8.8538e-07, 8.5395e-07, 8.8301e-07,\n",
      "          8.1570e-07, 8.6968e-07, 8.5287e-05, 9.8901e-05, 1.1487e-06,\n",
      "          1.3170e-04, 9.9028e-07, 3.3516e-04, 2.9126e-04, 8.4944e-07,\n",
      "          9.9887e-01, 1.4309e-06, 6.5977e-06, 8.9867e-05, 4.7702e-05,\n",
      "          1.2348e-06, 1.0394e-06, 1.3175e-06, 7.4019e-07, 1.0131e-06,\n",
      "          8.6290e-07, 1.5049e-06, 1.1587e-06, 6.2599e-07, 1.9157e-06,\n",
      "          1.1381e-06, 6.9556e-07, 7.1579e-07, 7.6056e-07, 1.2044e-06,\n",
      "          7.4348e-07, 1.0075e-06, 1.0639e-06, 2.0129e-06, 3.5716e-07,\n",
      "          1.4684e-06, 1.1903e-06, 1.2529e-06, 1.1787e-06, 1.2771e-06,\n",
      "          9.3951e-07],\n",
      "         [1.2711e-04, 1.3481e-04, 1.0337e-04, 1.5316e-04, 1.5364e-04,\n",
      "          2.0005e-04, 1.5544e-04, 2.0439e-03, 1.4688e-03, 1.1322e-03,\n",
      "          1.3149e-03, 3.9450e-04, 3.7901e-04, 4.9910e-04, 3.3238e-04,\n",
      "          5.9837e-04, 2.4036e-04, 1.5930e-02, 2.3694e-02, 1.8160e-02,\n",
      "          5.8973e-02, 6.1301e-04, 6.5910e-04, 5.5804e-04, 1.6216e-04,\n",
      "          6.5317e-01, 1.6714e-04, 5.2776e-04, 4.2090e-04, 8.3266e-04,\n",
      "          8.9932e-02, 4.4298e-03, 2.4454e-02, 2.1332e-02, 3.4070e-04,\n",
      "          1.3940e-03, 1.1225e-03, 1.8374e-03, 7.4573e-04, 4.9377e-03,\n",
      "          3.9015e-02, 8.3936e-03, 8.1634e-03, 9.3057e-03, 1.5288e-04,\n",
      "          2.1073e-04, 1.6584e-04, 1.7663e-04, 2.8814e-04, 2.0362e-04,\n",
      "          9.6398e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7843, 0.0136, 0.2021]])\n",
      "Player 1 Prediction: tensor([[[1.0478e-07, 1.5877e-07, 2.1612e-07, 1.4663e-07, 3.1430e-07,\n",
      "          2.1243e-07, 3.2607e-07, 1.4885e-04, 1.4010e-04, 1.3829e-06,\n",
      "          1.8955e-04, 7.4205e-07, 4.7105e-01, 4.7246e-01, 5.8221e-07,\n",
      "          1.1466e-05, 8.4500e-07, 3.5084e-05, 5.2753e-05, 3.7752e-06,\n",
      "          7.3405e-06, 1.6057e-06, 1.9076e-06, 1.7065e-06, 2.8700e-07,\n",
      "          5.2926e-02, 3.9280e-07, 7.9326e-07, 6.4411e-07, 1.8257e-06,\n",
      "          3.4156e-06, 2.4196e-06, 1.5658e-06, 1.2190e-06, 1.4648e-06,\n",
      "          2.5235e-07, 1.6985e-06, 1.3623e-03, 1.5715e-03, 5.8192e-06,\n",
      "          1.2802e-06, 5.3083e-06, 5.2130e-07, 4.0281e-07, 3.1647e-07,\n",
      "          1.3279e-07, 2.2868e-07, 1.9561e-07, 1.8670e-07, 2.1999e-07,\n",
      "          1.0455e-07],\n",
      "         [1.3693e-06, 1.8129e-06, 1.4959e-06, 1.7848e-06, 2.5270e-06,\n",
      "          3.8520e-06, 2.7383e-06, 4.6429e-01, 4.6063e-01, 9.6958e-06,\n",
      "          5.7610e-05, 7.2003e-06, 2.4648e-05, 1.7713e-05, 7.8520e-06,\n",
      "          3.3322e-06, 5.7595e-06, 3.9133e-04, 8.7545e-04, 1.4936e-04,\n",
      "          7.3401e-05, 1.0256e-05, 3.0313e-05, 3.8184e-05, 1.7911e-06,\n",
      "          7.1385e-02, 2.3504e-06, 2.8229e-05, 2.6194e-05, 1.6473e-05,\n",
      "          3.4655e-05, 5.0415e-05, 5.3309e-05, 4.7634e-05, 2.7162e-06,\n",
      "          1.8755e-07, 1.8755e-05, 4.7366e-05, 4.3392e-05, 8.3108e-06,\n",
      "          2.0252e-06, 3.2404e-05, 8.0820e-04, 7.3845e-04, 1.5048e-06,\n",
      "          1.2637e-06, 1.2174e-06, 1.4059e-06, 1.3387e-06, 1.4764e-06,\n",
      "          6.4058e-07],\n",
      "         [1.8902e-06, 1.6886e-06, 1.7661e-06, 2.1422e-06, 2.6409e-06,\n",
      "          2.0985e-06, 1.5246e-06, 3.9574e-06, 2.3348e-06, 1.8775e-06,\n",
      "          5.2068e-06, 9.2514e-07, 1.1818e-04, 1.2190e-04, 1.9585e-06,\n",
      "          3.7817e-04, 2.1993e-06, 5.0049e-01, 4.9853e-01, 2.5450e-06,\n",
      "          1.4369e-05, 1.9715e-06, 3.2727e-05, 1.2519e-04, 7.7662e-05,\n",
      "          1.9639e-06, 4.7526e-06, 2.2859e-06, 1.9064e-06, 5.0438e-06,\n",
      "          3.1833e-06, 2.1442e-06, 3.6929e-06, 2.0192e-06, 3.6723e-06,\n",
      "          4.3996e-06, 1.9091e-06, 2.5531e-06, 4.3746e-06, 3.8976e-06,\n",
      "          1.1339e-06, 4.6588e-06, 3.0798e-06, 2.6915e-06, 2.5207e-06,\n",
      "          2.6959e-06, 2.6687e-06, 2.5854e-06, 2.5199e-06, 3.5129e-06,\n",
      "          4.7903e-06],\n",
      "         [3.3600e-05, 4.0461e-05, 4.5611e-05, 7.7103e-05, 1.7364e-04,\n",
      "          1.2917e-04, 7.4355e-05, 1.0607e-03, 9.4080e-04, 1.9267e-04,\n",
      "          2.8948e-04, 1.2093e-04, 3.1394e-04, 3.6747e-04, 1.3895e-04,\n",
      "          3.4789e-04, 1.7416e-04, 1.0413e-01, 2.1203e-01, 2.4326e-02,\n",
      "          1.1476e-03, 1.2100e-04, 4.4047e-04, 6.5213e-04, 6.9541e-05,\n",
      "          5.3909e-01, 6.6389e-05, 6.0094e-04, 2.4756e-04, 1.7979e-04,\n",
      "          8.7694e-04, 7.9029e-03, 5.1690e-02, 2.9162e-02, 1.2939e-04,\n",
      "          2.7176e-04, 1.5827e-03, 4.6783e-03, 9.9433e-04, 1.0616e-03,\n",
      "          3.2963e-03, 6.0412e-03, 2.5445e-03, 1.7540e-03, 5.6889e-05,\n",
      "          6.9770e-05, 6.2159e-05, 5.5304e-05, 5.6101e-05, 6.1746e-05,\n",
      "          3.5711e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52963\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5049/10000 (50.5%)\n",
      "    Average reward: -0.601\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4951/10000 (49.5%)\n",
      "    Average reward: +0.601\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9544 (36.5%)\n",
      "    Action 1: 12648 (48.4%)\n",
      "    Action 2: 2182 (8.3%)\n",
      "    Action 3: 1763 (6.7%)\n",
      "  Player 1:\n",
      "    Action 0: 7544 (28.1%)\n",
      "    Action 1: 15521 (57.9%)\n",
      "    Action 2: 2734 (10.2%)\n",
      "    Action 3: 1027 (3.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6005.5, 6005.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.004\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6006\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9616, 0.0021, 0.0363]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2008, 0.0324, 0.7668]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17998/50000 [16:49<21:56, 24.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53471\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5014/10000 (50.1%)\n",
      "    Average reward: -0.139\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4986/10000 (49.9%)\n",
      "    Average reward: +0.139\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5128 (19.8%)\n",
      "    Action 1: 14829 (57.2%)\n",
      "    Action 2: 2270 (8.8%)\n",
      "    Action 3: 3709 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 19798 (71.9%)\n",
      "    Action 1: 7737 (28.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1391.5, 1391.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.857 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.890\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1391\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.2855e-04, 2.0022e-04, 1.6738e-04, 1.9074e-04, 2.0414e-04,\n",
      "          2.1400e-04, 1.8752e-04, 2.8903e-03, 5.5497e-03, 3.0627e-03,\n",
      "          7.0759e-03, 4.7515e-03, 1.4105e-02, 1.7646e-02, 6.7802e-03,\n",
      "          1.5575e-03, 7.5963e-04, 6.7641e-03, 8.8999e-03, 2.2743e-03,\n",
      "          1.0090e-02, 2.6348e-03, 9.7646e-04, 1.4546e-03, 3.4808e-04,\n",
      "          1.0168e-01, 2.2877e-04, 6.4525e-03, 6.1097e-03, 3.6777e-03,\n",
      "          3.1088e-02, 2.0443e-02, 1.7147e-01, 1.4967e-01, 5.3471e-03,\n",
      "          1.3572e-02, 2.1263e-02, 1.1777e-01, 8.9748e-02, 2.0588e-02,\n",
      "          2.6692e-02, 2.1357e-02, 5.7537e-02, 3.5193e-02, 2.0723e-04,\n",
      "          1.8638e-04, 1.7841e-04, 1.5502e-04, 2.0584e-04, 1.4361e-04,\n",
      "          1.3670e-04],\n",
      "         [1.0670e-04, 1.2937e-04, 1.7581e-04, 1.6001e-04, 1.4442e-04,\n",
      "          1.6535e-04, 1.8657e-04, 3.3629e-03, 5.4341e-03, 3.0413e-03,\n",
      "          1.1477e-02, 6.0793e-03, 1.5983e-02, 1.9706e-02, 4.1894e-03,\n",
      "          5.3893e-03, 8.1701e-04, 1.1152e-02, 1.2297e-02, 2.6662e-03,\n",
      "          7.5021e-03, 8.5645e-04, 8.7031e-04, 2.1314e-03, 3.0591e-04,\n",
      "          8.7779e-02, 2.0889e-04, 3.6269e-03, 3.5921e-03, 4.4589e-03,\n",
      "          8.5605e-02, 2.2489e-02, 1.8712e-01, 1.8011e-01, 8.6046e-03,\n",
      "          2.3413e-02, 1.0804e-02, 4.6369e-02, 3.7638e-02, 1.5746e-02,\n",
      "          3.0237e-02, 2.6732e-02, 6.4942e-02, 4.5027e-02, 1.6403e-04,\n",
      "          2.0264e-04, 1.6865e-04, 1.6760e-04, 1.6869e-04, 2.0580e-04,\n",
      "          9.6627e-05],\n",
      "         [1.0956e-05, 1.2064e-05, 1.4719e-05, 1.1104e-05, 1.3904e-05,\n",
      "          1.6683e-05, 1.9184e-05, 9.3245e-06, 1.6366e-05, 1.5813e-05,\n",
      "          1.6003e-05, 1.6106e-05, 1.0939e-03, 1.1353e-03, 1.2260e-05,\n",
      "          7.7872e-04, 2.1285e-05, 4.0979e-03, 4.8785e-03, 1.0695e-05,\n",
      "          1.0220e-03, 8.4168e-06, 5.5413e-03, 1.8583e-01, 7.9506e-01,\n",
      "          1.1887e-05, 1.4873e-05, 1.6350e-05, 1.3594e-05, 1.2596e-05,\n",
      "          1.0119e-05, 9.9510e-06, 2.2473e-05, 8.8518e-06, 1.5790e-05,\n",
      "          1.4082e-05, 1.0904e-05, 8.8359e-06, 1.2347e-05, 1.7126e-05,\n",
      "          1.5876e-05, 1.1653e-05, 7.4716e-06, 1.7960e-05, 1.1978e-05,\n",
      "          9.8348e-06, 1.1144e-05, 1.4590e-05, 1.3735e-05, 1.2202e-05,\n",
      "          1.8030e-05],\n",
      "         [3.1775e-04, 3.1546e-04, 3.3165e-04, 3.0227e-04, 4.1314e-04,\n",
      "          3.1464e-04, 2.5768e-04, 1.2533e-03, 2.0830e-03, 1.7204e-03,\n",
      "          3.2885e-03, 1.9798e-03, 3.2454e-03, 3.6621e-03, 7.6605e-04,\n",
      "          1.5145e-03, 7.6261e-04, 2.7307e-02, 3.0225e-02, 6.4962e-03,\n",
      "          2.1698e-03, 1.0439e-03, 1.7148e-02, 1.7685e-02, 1.3226e-03,\n",
      "          5.3139e-02, 3.7687e-04, 1.7846e-02, 1.4750e-02, 7.1147e-04,\n",
      "          8.4559e-03, 9.5052e-03, 3.2448e-01, 3.4228e-01, 1.3743e-03,\n",
      "          6.7879e-03, 3.3583e-03, 7.4336e-03, 9.5203e-03, 4.6030e-03,\n",
      "          8.5960e-03, 1.2685e-02, 2.9624e-02, 1.5905e-02, 3.3493e-04,\n",
      "          4.1783e-04, 3.6183e-04, 5.1934e-04, 3.3040e-04, 3.4354e-04,\n",
      "          3.3330e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9214, 0.0041, 0.0746]])\n",
      "Player 0 Prediction: tensor([[[1.3404e-04, 1.5119e-04, 1.3857e-04, 1.7566e-04, 2.2482e-04,\n",
      "          1.2943e-04, 1.6584e-04, 7.5834e-04, 9.5864e-04, 8.5332e-04,\n",
      "          6.0695e-02, 1.2601e-02, 1.6888e-03, 1.2637e-03, 1.3139e-03,\n",
      "          5.5254e-03, 1.1161e-03, 6.2815e-03, 8.5540e-03, 1.1449e-03,\n",
      "          4.1231e-02, 4.2647e-03, 1.4353e-03, 2.0805e-03, 3.1962e-04,\n",
      "          2.4054e-01, 1.4442e-04, 6.1838e-04, 7.1515e-04, 6.6551e-03,\n",
      "          1.9324e-01, 2.7897e-03, 6.7071e-03, 8.7061e-03, 1.3022e-02,\n",
      "          7.6157e-02, 1.6577e-03, 3.4036e-03, 7.4345e-03, 5.8897e-02,\n",
      "          2.1641e-01, 2.9997e-03, 3.2944e-03, 2.1654e-03, 2.1010e-04,\n",
      "          1.8470e-04, 2.1675e-04, 1.4378e-04, 1.9293e-04, 1.4072e-04,\n",
      "          1.4331e-04],\n",
      "         [4.9775e-05, 7.1813e-05, 7.4445e-05, 7.2111e-05, 7.5047e-05,\n",
      "          8.1003e-05, 8.1823e-05, 5.6224e-03, 8.7772e-03, 2.2774e-03,\n",
      "          1.6730e-03, 1.4848e-03, 2.5049e-02, 2.8158e-02, 3.6335e-03,\n",
      "          3.1566e-04, 1.6780e-04, 6.0840e-03, 6.5821e-03, 9.5229e-04,\n",
      "          3.8258e-04, 1.7128e-04, 3.9922e-04, 6.8208e-04, 1.3144e-04,\n",
      "          1.0046e-01, 9.3296e-05, 6.1779e-04, 4.4598e-04, 6.1320e-04,\n",
      "          7.5163e-03, 1.5174e-02, 2.3846e-01, 2.2138e-01, 7.6427e-04,\n",
      "          3.8515e-04, 1.2563e-02, 1.0322e-01, 8.7627e-02, 1.8696e-03,\n",
      "          2.3755e-03, 1.6930e-02, 5.7300e-02, 3.8628e-02, 9.4719e-05,\n",
      "          8.4397e-05, 7.4523e-05, 6.1443e-05, 8.4665e-05, 7.6511e-05,\n",
      "          5.5673e-05],\n",
      "         [1.9193e-05, 1.0910e-05, 1.9518e-05, 1.8933e-05, 1.8278e-05,\n",
      "          2.0156e-05, 1.7048e-05, 9.0424e-06, 1.9660e-05, 1.5868e-05,\n",
      "          1.2836e-05, 1.4595e-05, 8.0582e-04, 9.8505e-04, 9.9187e-06,\n",
      "          2.2056e-03, 1.8617e-05, 1.7961e-03, 2.9661e-03, 1.4902e-05,\n",
      "          6.0066e-04, 9.3176e-06, 5.4667e-01, 4.4313e-01, 1.2947e-04,\n",
      "          2.0172e-05, 1.9049e-05, 3.5607e-05, 2.0251e-05, 1.6452e-05,\n",
      "          1.7194e-05, 1.8349e-05, 2.4918e-05, 1.0689e-05, 1.1319e-05,\n",
      "          1.6642e-05, 1.6972e-05, 1.5890e-05, 1.9622e-05, 1.3582e-05,\n",
      "          2.4552e-05, 2.0057e-05, 1.3522e-05, 1.7548e-05, 1.3520e-05,\n",
      "          1.1331e-05, 1.0272e-05, 1.6951e-05, 1.5607e-05, 1.6520e-05,\n",
      "          2.5785e-05],\n",
      "         [8.3284e-04, 6.8970e-04, 9.1374e-04, 8.7190e-04, 7.1359e-04,\n",
      "          5.9605e-04, 7.8816e-04, 4.2769e-03, 3.8755e-03, 2.4786e-03,\n",
      "          1.7645e-02, 4.8226e-03, 7.9913e-03, 7.5356e-03, 1.4779e-03,\n",
      "          3.8937e-03, 1.2030e-03, 4.3823e-02, 5.2412e-02, 1.1798e-02,\n",
      "          6.9726e-03, 2.3724e-03, 2.5230e-02, 4.1779e-02, 1.4078e-03,\n",
      "          8.3142e-02, 7.3586e-04, 3.4606e-02, 2.3098e-02, 1.7959e-03,\n",
      "          4.1787e-02, 1.9463e-02, 1.9939e-01, 1.8303e-01, 4.4426e-03,\n",
      "          1.5897e-02, 7.9030e-03, 1.7883e-02, 1.2788e-02, 1.9246e-02,\n",
      "          3.0085e-02, 1.7882e-02, 1.8092e-02, 1.7233e-02, 6.9562e-04,\n",
      "          6.2139e-04, 8.7031e-04, 7.5861e-04, 7.7767e-04, 8.5621e-04,\n",
      "          5.1363e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9953, 0.0000, 0.0047, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[8.3926e-06, 1.2555e-05, 1.2254e-05, 1.0220e-05, 1.4915e-05,\n",
      "          1.6662e-05, 1.1819e-05, 6.7939e-04, 8.7425e-04, 1.8475e-04,\n",
      "          4.5744e-04, 1.4356e-04, 9.4815e-02, 1.0244e-01, 2.3305e-04,\n",
      "          3.7866e-06, 4.1222e-05, 1.4675e-02, 1.4120e-02, 1.4705e-04,\n",
      "          8.0876e-05, 5.6379e-05, 1.0338e-04, 1.9551e-04, 3.2818e-05,\n",
      "          1.5364e-01, 1.3154e-05, 7.9279e-05, 6.7472e-05, 8.2625e-05,\n",
      "          1.5452e-04, 9.6313e-04, 1.5980e-01, 1.1654e-01, 1.1427e-04,\n",
      "          8.6409e-06, 9.9675e-04, 1.9257e-01, 1.3926e-01, 2.1095e-04,\n",
      "          1.4342e-04, 1.1776e-03, 2.4431e-03, 2.2798e-03, 1.3734e-05,\n",
      "          8.4015e-06, 1.3277e-05, 9.0902e-06, 1.4607e-05, 1.5082e-05,\n",
      "          1.1164e-05],\n",
      "         [4.2938e-05, 6.7244e-05, 6.7834e-05, 6.9018e-05, 6.4850e-05,\n",
      "          7.9835e-05, 5.6520e-05, 2.0400e-02, 2.8112e-02, 3.2254e-03,\n",
      "          2.4361e-03, 1.2055e-03, 4.4094e-02, 4.3257e-02, 6.3489e-04,\n",
      "          3.4203e-04, 1.8743e-04, 5.1404e-03, 6.3774e-03, 6.5317e-04,\n",
      "          1.0934e-03, 1.6183e-04, 5.5220e-04, 8.2131e-04, 9.6851e-05,\n",
      "          1.4381e-01, 5.6153e-05, 5.7728e-04, 5.1196e-04, 6.9707e-04,\n",
      "          5.6088e-04, 3.7259e-03, 1.6614e-01, 1.6927e-01, 6.7809e-04,\n",
      "          5.8010e-04, 1.1099e-03, 1.0329e-01, 1.0405e-01, 8.2440e-04,\n",
      "          4.4685e-04, 1.4209e-02, 7.2222e-02, 5.7565e-02, 7.7611e-05,\n",
      "          6.5789e-05, 5.1543e-05, 7.1775e-05, 6.9262e-05, 4.9592e-05,\n",
      "          4.0074e-05],\n",
      "         [5.6192e-06, 3.3042e-06, 4.3540e-06, 3.8793e-06, 5.2525e-06,\n",
      "          5.9468e-06, 4.9037e-06, 3.5372e-06, 4.7561e-06, 6.3420e-06,\n",
      "          5.7753e-06, 4.5466e-06, 4.7491e-04, 3.9997e-04, 2.5038e-06,\n",
      "          1.5190e-04, 7.7357e-06, 4.9882e-01, 4.9823e-01, 6.4979e-06,\n",
      "          1.6397e-04, 4.1731e-06, 3.1955e-04, 1.0081e-03, 1.9490e-04,\n",
      "          4.3089e-06, 6.5299e-06, 8.6867e-06, 7.7597e-06, 9.7696e-06,\n",
      "          3.9976e-06, 4.4073e-06, 1.2116e-05, 3.1696e-06, 4.0720e-06,\n",
      "          1.1319e-05, 5.9003e-06, 2.7924e-06, 7.0506e-06, 4.9919e-06,\n",
      "          4.9661e-06, 5.8168e-06, 4.1144e-06, 5.7854e-06, 4.6302e-06,\n",
      "          5.7423e-06, 3.8474e-06, 3.6842e-06, 7.7964e-06, 4.0699e-06,\n",
      "          1.3871e-05],\n",
      "         [6.6718e-05, 4.3152e-05, 3.8563e-05, 4.9047e-05, 3.6878e-05,\n",
      "          3.5766e-05, 4.2208e-05, 1.0343e-03, 1.1120e-03, 4.6399e-04,\n",
      "          2.3882e-04, 1.9465e-04, 2.8620e-04, 3.0194e-04, 1.1104e-04,\n",
      "          1.6318e-04, 8.9966e-05, 1.1497e-01, 1.1249e-01, 2.9360e-03,\n",
      "          1.5613e-04, 1.5279e-04, 7.9741e-04, 2.0461e-03, 1.4011e-04,\n",
      "          6.1816e-02, 4.7207e-05, 1.0664e-03, 8.4149e-04, 1.0747e-04,\n",
      "          2.1576e-04, 3.9365e-03, 3.2192e-01, 3.2267e-01, 1.3198e-04,\n",
      "          3.9925e-04, 2.8712e-04, 5.7155e-04, 5.0645e-04, 4.8783e-04,\n",
      "          3.9627e-04, 1.0114e-02, 1.7213e-02, 1.8923e-02, 5.7128e-05,\n",
      "          5.6071e-05, 6.0424e-05, 5.2370e-05, 3.6043e-05, 4.6421e-05,\n",
      "          3.8397e-05]]])\n",
      "Player 1 Prediction: tensor([[0.3853, 0.3680, 0.2468, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.8120e-06, 3.7953e-06, 3.2154e-06, 7.3807e-06, 3.1867e-06,\n",
      "          3.9850e-06, 4.8723e-06, 1.1057e-01, 1.1505e-01, 4.2558e-05,\n",
      "          9.8925e-05, 2.9181e-05, 1.8399e-04, 1.8316e-04, 7.1675e-05,\n",
      "          4.9962e-05, 1.2489e-05, 1.0158e-04, 8.9124e-05, 4.2408e-05,\n",
      "          1.7269e-05, 2.2159e-05, 2.0330e-05, 2.4772e-05, 1.0541e-05,\n",
      "          2.0774e-01, 4.0401e-06, 1.3915e-05, 1.6218e-05, 2.1118e-05,\n",
      "          4.7347e-05, 9.0024e-05, 5.8866e-04, 5.1955e-04, 4.4106e-05,\n",
      "          1.2304e-04, 2.0161e-04, 3.8480e-04, 2.9097e-04, 1.0454e-04,\n",
      "          3.1801e-05, 1.6904e-04, 2.8109e-01, 2.8183e-01, 4.9491e-06,\n",
      "          4.4888e-06, 4.5477e-06, 4.0477e-06, 4.1095e-06, 4.9616e-06,\n",
      "          4.7298e-06],\n",
      "         [1.4442e-04, 1.2646e-04, 2.2580e-04, 1.8796e-04, 1.4941e-04,\n",
      "          3.0025e-04, 1.6039e-04, 6.0328e-03, 6.7787e-03, 4.2067e-03,\n",
      "          1.7368e-02, 3.8486e-03, 2.2113e-02, 2.4784e-02, 1.7722e-03,\n",
      "          2.1022e-03, 3.6212e-04, 1.9949e-02, 1.5032e-02, 9.5060e-04,\n",
      "          1.1723e-03, 3.7697e-04, 1.0964e-03, 1.9596e-03, 1.8519e-04,\n",
      "          1.5263e-01, 2.3659e-04, 2.0073e-03, 2.1150e-03, 1.1785e-03,\n",
      "          3.0015e-03, 7.6679e-03, 2.0021e-01, 3.7508e-01, 1.4223e-03,\n",
      "          4.3062e-03, 2.6762e-03, 3.3579e-02, 3.4522e-02, 4.3195e-03,\n",
      "          5.5019e-03, 8.7324e-03, 1.3679e-02, 1.4324e-02, 1.5736e-04,\n",
      "          2.6412e-04, 1.6888e-04, 2.0295e-04, 3.1197e-04, 2.2524e-04,\n",
      "          9.4845e-05],\n",
      "         [8.0481e-05, 1.0747e-04, 9.0494e-05, 9.4402e-05, 1.3956e-04,\n",
      "          1.4457e-04, 1.5579e-04, 6.9667e-05, 1.4535e-04, 1.9439e-04,\n",
      "          1.2597e-04, 1.4543e-04, 4.6203e-01, 4.3628e-01, 7.2336e-05,\n",
      "          2.4946e-03, 1.6380e-04, 3.2173e-02, 3.9321e-02, 1.2226e-04,\n",
      "          4.9895e-03, 6.6795e-05, 3.9163e-03, 1.1131e-02, 2.2453e-03,\n",
      "          7.6904e-05, 1.5914e-04, 1.7849e-04, 1.1615e-04, 1.3335e-04,\n",
      "          1.2670e-04, 1.0577e-04, 2.4683e-04, 1.0300e-04, 8.4211e-05,\n",
      "          3.0660e-04, 7.7693e-05, 7.3581e-05, 2.1016e-04, 1.1944e-04,\n",
      "          1.1556e-04, 1.7217e-04, 1.0767e-04, 1.1750e-04, 8.9257e-05,\n",
      "          1.1199e-04, 7.7957e-05, 1.1479e-04, 1.6127e-04, 8.2390e-05,\n",
      "          2.3500e-04],\n",
      "         [8.3332e-04, 1.4514e-03, 7.3019e-04, 1.1161e-03, 8.7347e-04,\n",
      "          8.8063e-04, 9.8099e-04, 5.4379e-03, 4.9409e-03, 4.5252e-03,\n",
      "          1.0017e-02, 5.8398e-03, 4.3873e-03, 4.3376e-03, 2.6014e-03,\n",
      "          3.1280e-03, 2.2882e-03, 6.3210e-02, 9.8242e-02, 1.5526e-02,\n",
      "          1.1732e-02, 2.6442e-03, 1.8131e-02, 2.2796e-02, 2.0670e-03,\n",
      "          7.3473e-02, 5.1956e-04, 2.9310e-02, 1.6115e-02, 2.7477e-03,\n",
      "          1.1893e-02, 2.9834e-02, 1.9133e-01, 1.7578e-01, 2.8083e-03,\n",
      "          1.5474e-02, 4.3647e-03, 7.7137e-03, 7.7570e-03, 1.4609e-02,\n",
      "          2.9694e-02, 3.6765e-02, 3.0026e-02, 2.4431e-02, 1.0447e-03,\n",
      "          1.1546e-03, 9.3294e-04, 1.1083e-03, 6.5547e-04, 9.9808e-04,\n",
      "          7.4352e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54327\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5702/10000 (57.0%)\n",
      "    Average reward: +0.721\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4298/10000 (43.0%)\n",
      "    Average reward: -0.721\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11863 (42.9%)\n",
      "    Action 1: 11471 (41.5%)\n",
      "    Action 2: 3082 (11.1%)\n",
      "    Action 3: 1228 (4.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7767 (29.1%)\n",
      "    Action 1: 15094 (56.6%)\n",
      "    Action 2: 1553 (5.8%)\n",
      "    Action 3: 2269 (8.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7212.0, -7212.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.983 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.017\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7212\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3578, 0.6340, 0.0082, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7767, 0.0210, 0.2023]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51229\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6186/10000 (61.9%)\n",
      "    Average reward: +0.034\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3814/10000 (38.1%)\n",
      "    Average reward: -0.034\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22055 (81.3%)\n",
      "    Action 1: 5080 (18.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3528 (14.6%)\n",
      "    Action 1: 17088 (70.9%)\n",
      "    Action 2: 1103 (4.6%)\n",
      "    Action 3: 2375 (9.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [340.5, -340.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.696 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.757 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.727\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0340\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19003/50000 [17:46<20:59, 24.62it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 121616/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 124109/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [18:28<20:42, 24.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 128227/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 130651/2000000\n",
      "P1 SL Buffer Size:  128227\n",
      "P1 SL buffer distribution [52439. 58000.  8284.  9504.]\n",
      "P1 actions distribution [0.40895443 0.45232283 0.06460418 0.07411856]\n",
      "P2 SL Buffer Size:  130651\n",
      "P2 SL buffer distribution [45326. 64876.  8079. 12370.]\n",
      "P2 actions distribution [0.34692425 0.49655954 0.0618365  0.09467972]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[1.0688e-04, 1.7130e-04, 1.7376e-04, 1.7199e-04, 1.5106e-04,\n",
      "          2.1228e-04, 1.8495e-04, 2.8075e-03, 6.1698e-03, 2.5260e-03,\n",
      "          3.8008e-04, 5.9018e-04, 1.0874e-02, 1.4457e-02, 3.3493e-03,\n",
      "          6.0938e-04, 6.8480e-04, 1.3323e-02, 1.7553e-02, 5.4624e-03,\n",
      "          1.9330e-03, 5.3019e-04, 1.7162e-03, 2.4049e-03, 1.9402e-04,\n",
      "          1.0117e-01, 2.8398e-04, 7.2614e-03, 6.9447e-03, 1.9608e-03,\n",
      "          1.1654e-02, 4.2041e-02, 2.4629e-01, 2.0474e-01, 1.2215e-03,\n",
      "          3.1806e-03, 1.5302e-02, 6.2429e-02, 4.6836e-02, 3.3756e-03,\n",
      "          1.1203e-02, 3.6455e-02, 7.0291e-02, 3.9606e-02, 1.6226e-04,\n",
      "          1.6816e-04, 1.6543e-04, 1.7175e-04, 1.5990e-04, 1.2614e-04,\n",
      "          6.6348e-05],\n",
      "         [1.0286e-04, 1.8801e-04, 1.7710e-04, 1.6254e-04, 2.0366e-04,\n",
      "          2.3303e-04, 2.1591e-04, 2.4367e-03, 3.5615e-03, 1.7861e-03,\n",
      "          1.8426e-03, 1.4825e-03, 2.7248e-03, 4.0325e-03, 1.5728e-03,\n",
      "          1.4329e-02, 2.8519e-03, 1.5623e-02, 2.1906e-02, 5.8104e-03,\n",
      "          1.6136e-03, 6.4923e-04, 1.1027e-03, 1.4621e-03, 2.5343e-04,\n",
      "          1.2596e-01, 2.4105e-04, 6.5459e-03, 7.3340e-03, 1.0809e-02,\n",
      "          1.2284e-01, 2.7832e-02, 1.5788e-01, 1.2778e-01, 6.9038e-03,\n",
      "          6.0189e-02, 5.4365e-03, 1.9809e-02, 1.5874e-02, 8.9064e-03,\n",
      "          2.8040e-02, 3.3351e-02, 9.4727e-02, 5.2071e-02, 1.8935e-04,\n",
      "          1.4713e-04, 1.7766e-04, 1.6693e-04, 1.9338e-04, 1.5742e-04,\n",
      "          1.1889e-04],\n",
      "         [1.1620e-05, 1.2435e-05, 1.7245e-05, 1.3453e-05, 1.2794e-05,\n",
      "          1.4362e-05, 9.7668e-06, 2.0505e-05, 1.8789e-05, 1.0769e-05,\n",
      "          7.6157e-06, 1.1985e-05, 1.3201e-03, 1.0881e-03, 1.7769e-05,\n",
      "          1.2493e-03, 1.3755e-05, 2.3062e-03, 2.5981e-03, 1.4541e-05,\n",
      "          4.8765e-04, 1.2194e-05, 7.3114e-03, 2.4300e-01, 7.4006e-01,\n",
      "          1.4847e-05, 1.8439e-05, 1.8635e-05, 1.3121e-05, 2.5436e-05,\n",
      "          1.3757e-05, 1.5571e-05, 1.3330e-05, 1.1622e-05, 1.3395e-05,\n",
      "          1.4283e-05, 1.8967e-05, 1.3356e-05, 1.8107e-05, 1.3810e-05,\n",
      "          1.0337e-05, 9.4264e-06, 1.3830e-05, 1.1663e-05, 1.1878e-05,\n",
      "          1.1678e-05, 1.3853e-05, 1.4292e-05, 9.8768e-06, 1.2286e-05,\n",
      "          1.2523e-05],\n",
      "         [1.9851e-04, 4.3498e-04, 3.9261e-04, 3.1161e-04, 6.4986e-04,\n",
      "          4.1942e-04, 4.3608e-04, 2.0750e-03, 2.7191e-03, 1.2908e-03,\n",
      "          1.9345e-03, 8.0818e-04, 3.2562e-03, 3.4939e-03, 7.1582e-04,\n",
      "          2.1001e-03, 7.0937e-04, 4.4512e-02, 8.2485e-02, 9.0159e-03,\n",
      "          8.4902e-04, 9.8913e-04, 6.8841e-03, 6.2460e-03, 3.5277e-04,\n",
      "          7.3871e-02, 4.7591e-04, 1.9140e-02, 1.5455e-02, 2.5441e-03,\n",
      "          1.3579e-02, 4.4548e-02, 2.5121e-01, 2.5588e-01, 7.3066e-04,\n",
      "          4.2943e-03, 4.5936e-03, 1.3439e-02, 1.0371e-02, 6.6948e-03,\n",
      "          7.4873e-03, 2.3141e-02, 5.0830e-02, 2.5273e-02, 3.9988e-04,\n",
      "          5.1868e-04, 3.5859e-04, 3.8782e-04, 5.1661e-04, 6.2722e-04,\n",
      "          3.5365e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7513, 0.0130, 0.2357]])\n",
      "Player 1 Prediction: tensor([[[1.4304e-04, 2.5305e-04, 1.9661e-04, 1.4897e-04, 1.8399e-04,\n",
      "          1.8992e-04, 1.8416e-04, 1.6293e-03, 2.2075e-03, 1.2657e-03,\n",
      "          9.1894e-03, 3.2563e-03, 1.8128e-03, 2.3952e-03, 1.1758e-03,\n",
      "          1.8062e-02, 2.8481e-03, 4.9229e-03, 5.7840e-03, 2.5542e-03,\n",
      "          2.2842e-02, 2.1851e-03, 1.1918e-03, 1.2485e-03, 2.1847e-04,\n",
      "          1.5563e-01, 2.8587e-04, 1.6745e-03, 1.7531e-03, 1.2542e-02,\n",
      "          3.7977e-01, 7.1890e-03, 2.3167e-02, 1.5153e-02, 8.2829e-03,\n",
      "          7.9486e-02, 2.8048e-03, 3.7602e-03, 3.9843e-03, 2.3763e-02,\n",
      "          1.2366e-01, 8.7309e-03, 4.2453e-02, 1.8401e-02, 2.0000e-04,\n",
      "          1.9359e-04, 2.0395e-04, 2.3079e-04, 2.3889e-04, 2.3392e-04,\n",
      "          1.2361e-04],\n",
      "         [8.0060e-05, 1.5007e-04, 1.2985e-04, 1.4907e-04, 1.8523e-04,\n",
      "          2.5369e-04, 1.7220e-04, 5.0276e-03, 8.0967e-03, 2.3433e-03,\n",
      "          1.8048e-04, 3.6127e-04, 1.0495e-02, 1.1351e-02, 1.6902e-03,\n",
      "          2.5871e-04, 4.7095e-04, 1.6244e-02, 2.0375e-02, 5.0782e-03,\n",
      "          1.3273e-03, 3.2407e-04, 1.2434e-03, 1.9279e-03, 1.6239e-04,\n",
      "          1.1007e-01, 2.2768e-04, 2.1452e-03, 2.1296e-03, 1.2229e-03,\n",
      "          3.9604e-03, 2.6895e-02, 2.4818e-01, 2.1874e-01, 5.8684e-04,\n",
      "          6.6223e-04, 1.0145e-02, 7.6560e-02, 6.3787e-02, 1.5755e-03,\n",
      "          5.7846e-03, 2.8424e-02, 6.5627e-02, 4.4305e-02, 1.4088e-04,\n",
      "          1.3736e-04, 1.3759e-04, 1.3466e-04, 1.6503e-04, 1.1467e-04,\n",
      "          6.8430e-05],\n",
      "         [6.0165e-06, 3.9011e-06, 7.0819e-06, 4.8520e-06, 6.5881e-06,\n",
      "          9.0852e-06, 4.7679e-06, 5.4702e-06, 5.3171e-06, 5.6264e-06,\n",
      "          8.0016e-06, 4.3712e-06, 3.5367e-04, 3.5757e-04, 3.5736e-06,\n",
      "          1.3510e-03, 5.7910e-06, 3.7388e-04, 4.5421e-04, 4.1842e-06,\n",
      "          1.2949e-04, 3.5806e-06, 6.1154e-01, 3.8517e-01, 2.1798e-05,\n",
      "          9.6307e-06, 6.5346e-06, 6.9490e-06, 4.7228e-06, 7.7093e-06,\n",
      "          5.4905e-06, 7.0351e-06, 6.0623e-06, 3.4074e-06, 5.8482e-06,\n",
      "          3.8170e-06, 6.6362e-06, 5.0397e-06, 1.1375e-05, 8.3038e-06,\n",
      "          4.7937e-06, 4.2830e-06, 1.2041e-05, 6.8687e-06, 1.0440e-05,\n",
      "          4.6447e-06, 4.9901e-06, 4.7224e-06, 4.8455e-06, 5.1104e-06,\n",
      "          5.2316e-06],\n",
      "         [7.4673e-04, 6.3504e-04, 6.4467e-04, 5.2482e-04, 4.7591e-04,\n",
      "          9.2141e-04, 5.4468e-04, 2.4175e-03, 4.7172e-03, 2.3300e-03,\n",
      "          5.5597e-03, 1.4277e-03, 7.8564e-03, 7.4514e-03, 1.2639e-03,\n",
      "          3.6397e-03, 1.5788e-03, 3.8601e-02, 2.7892e-02, 6.3988e-03,\n",
      "          8.7743e-03, 2.1562e-03, 1.4037e-02, 1.7892e-02, 9.5985e-04,\n",
      "          7.0974e-02, 9.8123e-04, 4.3255e-02, 3.9077e-02, 4.0654e-03,\n",
      "          5.0468e-02, 2.0928e-02, 2.5313e-01, 1.3262e-01, 1.4510e-03,\n",
      "          1.0172e-02, 9.1019e-03, 1.4588e-02, 2.1320e-02, 1.4789e-02,\n",
      "          4.8682e-02, 1.1461e-02, 5.9223e-02, 2.9609e-02, 7.5013e-04,\n",
      "          6.5244e-04, 7.2026e-04, 5.1685e-04, 7.2607e-04, 7.6714e-04,\n",
      "          5.2872e-04]]])\n",
      "Player 0 Prediction: tensor([[0.9926, 0.0000, 0.0074, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[3.3432e-06, 5.9078e-06, 4.6222e-06, 3.7437e-06, 5.7062e-06,\n",
      "          6.1154e-06, 5.2379e-06, 8.1914e-05, 9.7235e-05, 8.7234e-05,\n",
      "          2.1760e-05, 1.2794e-05, 8.7709e-03, 1.0564e-02, 8.5475e-05,\n",
      "          1.9736e-07, 2.1272e-05, 6.5496e-03, 6.9700e-03, 2.4518e-04,\n",
      "          2.4359e-05, 1.1294e-05, 3.1703e-05, 5.1777e-05, 5.3168e-06,\n",
      "          6.5874e-03, 1.2366e-05, 4.2689e-05, 4.0406e-05, 2.1196e-05,\n",
      "          4.3844e-05, 1.1735e-03, 5.1972e-02, 6.8809e-02, 1.4956e-05,\n",
      "          1.9471e-06, 3.5711e-04, 4.0771e-01, 4.0955e-01, 4.7248e-05,\n",
      "          1.1041e-03, 9.4096e-04, 9.8501e-03, 8.0219e-03, 5.7223e-06,\n",
      "          3.3154e-06, 6.4219e-06, 4.2633e-06, 4.1896e-06, 6.2110e-06,\n",
      "          3.4942e-06],\n",
      "         [3.6757e-05, 9.1156e-05, 7.3574e-05, 7.1911e-05, 1.1135e-04,\n",
      "          1.3607e-04, 9.1222e-05, 9.6677e-04, 1.3339e-03, 1.0863e-03,\n",
      "          8.1024e-05, 2.0497e-04, 3.4408e-03, 3.7161e-03, 1.4802e-04,\n",
      "          6.3084e-05, 1.8390e-04, 3.5317e-04, 4.5369e-04, 8.1853e-04,\n",
      "          5.7950e-04, 1.3847e-04, 7.3276e-04, 8.7605e-04, 6.2346e-05,\n",
      "          6.6974e-03, 1.2157e-04, 1.5329e-03, 1.2165e-03, 5.3724e-04,\n",
      "          6.4891e-03, 4.0365e-03, 1.1527e-01, 1.1745e-01, 1.6307e-04,\n",
      "          1.7741e-03, 1.0030e-03, 2.1811e-01, 2.1845e-01, 2.8332e-03,\n",
      "          1.4482e-02, 2.4800e-02, 1.4016e-01, 1.0852e-01, 6.9583e-05,\n",
      "          5.6352e-05, 9.4737e-05, 8.2964e-05, 9.2724e-05, 7.6039e-05,\n",
      "          4.3868e-05],\n",
      "         [6.8799e-06, 5.3408e-06, 9.7599e-06, 7.0596e-06, 6.0760e-06,\n",
      "          6.2229e-06, 4.5651e-06, 7.8704e-06, 5.7624e-06, 5.8172e-06,\n",
      "          8.7280e-06, 4.9039e-06, 5.7738e-04, 5.0835e-04, 9.9571e-06,\n",
      "          7.9508e-04, 9.6520e-06, 5.0257e-01, 4.9401e-01, 5.8861e-06,\n",
      "          1.8587e-04, 6.4188e-06, 1.2804e-04, 7.3180e-04, 1.7006e-04,\n",
      "          5.6762e-06, 1.0664e-05, 5.6856e-06, 5.9156e-06, 1.7568e-05,\n",
      "          9.1796e-06, 9.3844e-06, 8.9728e-06, 7.2819e-06, 1.0802e-05,\n",
      "          1.1479e-05, 8.5238e-06, 7.8043e-06, 1.5259e-05, 8.0268e-06,\n",
      "          6.1963e-06, 5.0209e-06, 4.9167e-06, 6.9820e-06, 8.1105e-06,\n",
      "          7.4571e-06, 5.2569e-06, 8.8961e-06, 6.1414e-06, 7.0448e-06,\n",
      "          5.6426e-06],\n",
      "         [2.9400e-05, 5.8288e-05, 5.3874e-05, 4.2532e-05, 8.4310e-05,\n",
      "          8.5803e-05, 8.0701e-05, 1.5381e-03, 1.9341e-03, 3.0101e-04,\n",
      "          2.3554e-04, 7.5397e-05, 2.4457e-04, 3.8886e-04, 1.1245e-04,\n",
      "          1.1591e-04, 9.6005e-05, 9.0377e-03, 8.6606e-03, 1.9169e-03,\n",
      "          6.8143e-05, 1.1571e-04, 2.2568e-04, 2.9579e-04, 5.4534e-05,\n",
      "          8.6567e-03, 7.0485e-05, 1.1706e-03, 7.6804e-04, 2.9418e-04,\n",
      "          1.6077e-03, 9.6523e-03, 3.9135e-01, 4.0366e-01, 9.1331e-05,\n",
      "          3.6490e-04, 1.0995e-03, 2.1769e-03, 9.6768e-04, 9.9751e-04,\n",
      "          2.1535e-03, 1.4419e-02, 7.4138e-02, 6.0163e-02, 4.3209e-05,\n",
      "          6.5686e-05, 5.0536e-05, 4.6748e-05, 5.1431e-05, 4.9488e-05,\n",
      "          4.3611e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3818, 0.1477, 0.4704, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [18:39<20:42, 24.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56099\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5199/10000 (52.0%)\n",
      "    Average reward: -0.540\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4801/10000 (48.0%)\n",
      "    Average reward: +0.540\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10148 (36.3%)\n",
      "    Action 1: 13756 (49.2%)\n",
      "    Action 2: 1974 (7.1%)\n",
      "    Action 3: 2103 (7.5%)\n",
      "  Player 1:\n",
      "    Action 0: 9931 (35.3%)\n",
      "    Action 1: 13632 (48.5%)\n",
      "    Action 2: 2892 (10.3%)\n",
      "    Action 3: 1663 (5.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5402.5, 5402.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.035\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5403\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9529e-01, 3.6491e-04, 4.3474e-03]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7034, 0.0141, 0.2825]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53490\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5073/10000 (50.7%)\n",
      "    Average reward: +0.001\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4927/10000 (49.3%)\n",
      "    Average reward: -0.001\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5025 (19.3%)\n",
      "    Action 1: 14788 (56.9%)\n",
      "    Action 2: 2465 (9.5%)\n",
      "    Action 3: 3693 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 19755 (71.8%)\n",
      "    Action 1: 7764 (28.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8.0, -8.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.858 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.890\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0008\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[9.8194e-05, 1.4712e-04, 1.1965e-04, 1.4092e-04, 1.4498e-04,\n",
      "          1.5740e-04, 1.3859e-04, 3.5635e-03, 7.3271e-03, 2.7374e-03,\n",
      "          4.1432e-03, 3.4835e-03, 1.9317e-02, 2.3514e-02, 7.9151e-03,\n",
      "          1.4148e-03, 6.6941e-04, 6.7970e-03, 8.9378e-03, 1.7698e-03,\n",
      "          3.4574e-03, 1.4929e-03, 7.6094e-04, 1.1596e-03, 2.7353e-04,\n",
      "          1.5506e-01, 1.6732e-04, 4.8907e-03, 4.6811e-03, 2.4317e-03,\n",
      "          1.4582e-02, 3.3904e-02, 1.8747e-01, 1.7128e-01, 3.5598e-03,\n",
      "          1.2589e-02, 2.3621e-02, 8.7340e-02, 6.8822e-02, 9.1000e-03,\n",
      "          1.3921e-02, 2.2075e-02, 5.2605e-02, 3.1349e-02, 1.4420e-04,\n",
      "          1.3100e-04, 1.2263e-04, 1.1225e-04, 1.5424e-04, 1.0540e-04,\n",
      "          9.7850e-05],\n",
      "         [9.5771e-05, 1.1314e-04, 1.5742e-04, 1.3216e-04, 1.2577e-04,\n",
      "          1.4665e-04, 1.5811e-04, 3.8035e-03, 6.1345e-03, 2.9063e-03,\n",
      "          9.5075e-03, 5.4757e-03, 1.3521e-02, 1.7586e-02, 4.1240e-03,\n",
      "          5.3316e-03, 1.1908e-03, 1.9303e-02, 1.9876e-02, 2.6770e-03,\n",
      "          8.1775e-03, 9.0400e-04, 8.0417e-04, 2.2290e-03, 2.7757e-04,\n",
      "          1.1155e-01, 1.8216e-04, 4.4709e-03, 4.4489e-03, 5.2954e-03,\n",
      "          6.9376e-02, 2.6617e-02, 1.9614e-01, 1.8698e-01, 4.7912e-03,\n",
      "          2.1650e-02, 1.0372e-02, 4.2874e-02, 3.6233e-02, 1.1511e-02,\n",
      "          2.9292e-02, 2.5658e-02, 5.0801e-02, 3.5967e-02, 1.3731e-04,\n",
      "          1.7280e-04, 1.4707e-04, 1.4791e-04, 1.4425e-04, 1.8966e-04,\n",
      "          8.4156e-05],\n",
      "         [1.1631e-05, 1.2861e-05, 1.5680e-05, 1.2417e-05, 1.5144e-05,\n",
      "          1.9056e-05, 2.0924e-05, 1.0447e-05, 1.7372e-05, 1.8102e-05,\n",
      "          1.8036e-05, 1.7800e-05, 1.3743e-03, 1.4135e-03, 1.3099e-05,\n",
      "          8.2676e-04, 2.4348e-05, 5.3706e-03, 6.0840e-03, 1.1897e-05,\n",
      "          1.2080e-03, 9.4236e-06, 4.6387e-03, 1.7254e-01, 8.0591e-01,\n",
      "          1.3983e-05, 1.6723e-05, 1.8169e-05, 1.4732e-05, 1.3850e-05,\n",
      "          1.0790e-05, 1.0701e-05, 2.5126e-05, 9.9977e-06, 1.7143e-05,\n",
      "          1.5807e-05, 1.1744e-05, 9.1544e-06, 1.3416e-05, 1.9362e-05,\n",
      "          1.8124e-05, 1.3016e-05, 8.8674e-06, 1.9838e-05, 1.3287e-05,\n",
      "          1.0735e-05, 1.2535e-05, 1.5501e-05, 1.6079e-05, 1.3442e-05,\n",
      "          2.0786e-05],\n",
      "         [3.2068e-04, 2.9994e-04, 3.2788e-04, 3.0021e-04, 3.9899e-04,\n",
      "          2.9672e-04, 2.5580e-04, 1.1929e-03, 1.8985e-03, 1.6070e-03,\n",
      "          3.8688e-03, 2.1130e-03, 2.8633e-03, 3.0982e-03, 7.3273e-04,\n",
      "          1.6405e-03, 8.0540e-04, 2.6796e-02, 2.8321e-02, 6.6809e-03,\n",
      "          1.8628e-03, 1.2745e-03, 2.1044e-02, 2.0028e-02, 1.7485e-03,\n",
      "          6.7900e-02, 3.6280e-04, 1.8038e-02, 1.4906e-02, 7.5285e-04,\n",
      "          7.4971e-03, 1.1130e-02, 3.1446e-01, 3.3814e-01, 1.7528e-03,\n",
      "          9.4013e-03, 2.7121e-03, 5.7906e-03, 7.7852e-03, 4.6736e-03,\n",
      "          9.3190e-03, 1.1728e-02, 2.7003e-02, 1.4281e-02, 3.2163e-04,\n",
      "          4.2301e-04, 3.4803e-04, 5.1265e-04, 3.3045e-04, 3.3866e-04,\n",
      "          3.1977e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9302, 0.0031, 0.0666]])\n",
      "Player 0 Prediction: tensor([[[9.6554e-05, 1.0028e-04, 9.2847e-05, 1.2111e-04, 1.5691e-04,\n",
      "          8.5617e-05, 1.1269e-04, 5.8620e-04, 7.3449e-04, 5.9137e-04,\n",
      "          4.4725e-02, 8.6851e-03, 1.0195e-03, 7.2757e-04, 1.0078e-03,\n",
      "          9.4430e-03, 9.9673e-04, 4.4102e-03, 5.4739e-03, 8.4807e-04,\n",
      "          2.3621e-02, 2.6563e-03, 1.1376e-03, 1.5980e-03, 2.3057e-04,\n",
      "          2.0491e-01, 9.7178e-05, 4.1843e-04, 4.9647e-04, 4.6411e-03,\n",
      "          1.5743e-01, 2.5152e-03, 4.6910e-03, 6.7239e-03, 9.2879e-03,\n",
      "          1.6630e-01, 1.0881e-03, 1.4322e-03, 3.7918e-03, 4.9735e-02,\n",
      "          2.7115e-01, 1.9389e-03, 1.9739e-03, 1.2904e-03, 1.4828e-04,\n",
      "          1.2076e-04, 1.4311e-04, 9.8568e-05, 1.2500e-04, 9.5242e-05,\n",
      "          9.8164e-05],\n",
      "         [3.8669e-05, 5.6844e-05, 5.8300e-05, 5.3961e-05, 5.9078e-05,\n",
      "          6.1761e-05, 6.2694e-05, 7.1133e-03, 1.0996e-02, 2.4893e-03,\n",
      "          1.3724e-03, 8.6477e-04, 2.5121e-02, 2.8004e-02, 4.7025e-03,\n",
      "          1.9026e-04, 1.7904e-04, 5.0991e-03, 5.4293e-03, 9.1924e-04,\n",
      "          3.3004e-04, 1.5342e-04, 3.2886e-04, 6.1015e-04, 1.0389e-04,\n",
      "          1.2942e-01, 7.1990e-05, 6.2062e-04, 4.5423e-04, 4.5922e-04,\n",
      "          5.0936e-03, 2.0758e-02, 2.0503e-01, 2.0293e-01, 2.4012e-04,\n",
      "          2.9534e-04, 1.3513e-02, 1.0602e-01, 9.1159e-02, 8.3667e-04,\n",
      "          2.2521e-03, 1.9582e-02, 6.4235e-02, 4.2208e-02, 7.5285e-05,\n",
      "          6.3726e-05, 5.9275e-05, 4.7898e-05, 6.6773e-05, 6.0469e-05,\n",
      "          4.4778e-05],\n",
      "         [1.8214e-05, 1.0492e-05, 1.8709e-05, 1.8414e-05, 1.7062e-05,\n",
      "          2.0339e-05, 1.6555e-05, 8.6238e-06, 1.9634e-05, 1.5561e-05,\n",
      "          1.2127e-05, 1.4677e-05, 8.1533e-04, 1.0134e-03, 9.1618e-06,\n",
      "          2.6346e-03, 1.7982e-05, 1.5638e-03, 2.5521e-03, 1.3577e-05,\n",
      "          6.3099e-04, 9.1378e-06, 5.4695e-01, 4.4305e-01, 1.0022e-04,\n",
      "          1.9883e-05, 1.8698e-05, 3.6089e-05, 2.0236e-05, 1.6126e-05,\n",
      "          1.7455e-05, 1.8251e-05, 2.3414e-05, 1.0334e-05, 1.0520e-05,\n",
      "          1.5920e-05, 1.6336e-05, 1.5903e-05, 1.8936e-05, 1.2831e-05,\n",
      "          2.4378e-05, 1.9952e-05, 1.3424e-05, 1.7338e-05, 1.3419e-05,\n",
      "          1.1154e-05, 1.0178e-05, 1.6665e-05, 1.5200e-05, 1.5833e-05,\n",
      "          2.4810e-05],\n",
      "         [8.1890e-04, 7.0205e-04, 9.2662e-04, 8.7008e-04, 7.1238e-04,\n",
      "          5.9431e-04, 8.0145e-04, 3.9326e-03, 3.5468e-03, 2.4132e-03,\n",
      "          2.4741e-02, 5.5622e-03, 7.2951e-03, 6.8354e-03, 1.4769e-03,\n",
      "          4.5693e-03, 1.3088e-03, 3.7329e-02, 4.2884e-02, 1.2377e-02,\n",
      "          7.0071e-03, 3.2104e-03, 3.0992e-02, 4.9816e-02, 1.8142e-03,\n",
      "          1.1109e-01, 7.2641e-04, 3.5592e-02, 2.3504e-02, 1.9560e-03,\n",
      "          4.2839e-02, 2.1745e-02, 1.7751e-01, 1.5329e-01, 6.2533e-03,\n",
      "          2.4210e-02, 6.5858e-03, 1.4868e-02, 1.0616e-02, 2.2775e-02,\n",
      "          4.1577e-02, 1.5829e-02, 1.6141e-02, 1.5184e-02, 7.1166e-04,\n",
      "          6.3764e-04, 8.7419e-04, 7.4712e-04, 8.1236e-04, 8.8155e-04,\n",
      "          5.1706e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9954, 0.0000, 0.0046, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.2301e-05, 1.4744e-05, 1.5812e-05, 1.2413e-05, 1.6184e-05,\n",
      "          1.9507e-05, 1.3479e-05, 4.2653e-03, 5.5341e-03, 2.2873e-04,\n",
      "          3.7561e-04, 1.5005e-04, 1.3237e-01, 1.5956e-01, 5.7289e-04,\n",
      "          9.2324e-06, 5.1971e-05, 4.6332e-02, 3.9760e-02, 1.7837e-04,\n",
      "          3.4031e-05, 5.0736e-05, 1.0557e-04, 3.8852e-04, 3.9429e-05,\n",
      "          3.3218e-01, 1.5729e-05, 8.9244e-05, 8.2343e-05, 8.1436e-05,\n",
      "          1.2685e-04, 2.0961e-03, 5.8708e-02, 5.6152e-02, 9.8446e-05,\n",
      "          1.8681e-05, 1.0198e-03, 7.3889e-02, 7.4830e-02, 2.0349e-04,\n",
      "          4.3288e-04, 1.6156e-03, 4.2716e-03, 3.8810e-03, 1.3600e-05,\n",
      "          8.8995e-06, 1.2781e-05, 1.0421e-05, 1.4953e-05, 1.4609e-05,\n",
      "          1.5877e-05],\n",
      "         [1.8774e-05, 3.6109e-05, 3.0337e-05, 3.5473e-05, 2.9602e-05,\n",
      "          2.4630e-05, 3.5902e-05, 3.7429e-03, 4.3619e-03, 9.6359e-04,\n",
      "          4.3268e-03, 1.0703e-03, 4.9123e-02, 5.1180e-02, 6.0213e-04,\n",
      "          1.1407e-04, 1.3424e-04, 7.1015e-04, 8.9077e-04, 2.7624e-04,\n",
      "          2.9477e-04, 8.9974e-05, 2.1072e-04, 4.2050e-04, 5.1183e-05,\n",
      "          1.2049e-01, 3.0375e-05, 3.7235e-04, 4.0588e-04, 2.6870e-04,\n",
      "          1.7097e-03, 5.5006e-03, 2.6389e-01, 3.0845e-01, 1.5381e-04,\n",
      "          1.1903e-03, 8.5110e-04, 8.2548e-02, 8.0740e-02, 1.0092e-03,\n",
      "          5.2880e-04, 5.4228e-03, 4.0361e-03, 3.4176e-03, 3.4042e-05,\n",
      "          3.2235e-05, 2.8196e-05, 3.5380e-05, 2.7528e-05, 3.1200e-05,\n",
      "          1.8074e-05],\n",
      "         [1.0201e-05, 4.5967e-06, 5.4273e-06, 6.6899e-06, 7.5461e-06,\n",
      "          9.3350e-06, 5.8541e-06, 6.3912e-06, 5.8894e-06, 8.3254e-06,\n",
      "          9.6707e-06, 6.1864e-06, 7.7393e-04, 7.7858e-04, 3.9960e-06,\n",
      "          2.3157e-04, 1.1193e-05, 5.2542e-01, 4.7088e-01, 1.0476e-05,\n",
      "          7.2621e-04, 4.4434e-06, 1.5271e-04, 5.4553e-04, 1.7056e-04,\n",
      "          7.2398e-06, 6.9005e-06, 9.0575e-06, 1.2131e-05, 6.9454e-06,\n",
      "          6.9760e-06, 5.4448e-06, 1.1522e-05, 3.2789e-06, 3.8993e-06,\n",
      "          1.2812e-05, 5.1895e-06, 4.1288e-06, 9.4650e-06, 5.2839e-06,\n",
      "          8.4707e-06, 8.7211e-06, 3.6733e-06, 9.2646e-06, 6.4597e-06,\n",
      "          9.2332e-06, 6.8239e-06, 7.4570e-06, 6.9609e-06, 6.9889e-06,\n",
      "          1.7595e-05],\n",
      "         [5.1248e-05, 3.3903e-05, 4.3940e-05, 7.2488e-05, 5.2405e-05,\n",
      "          4.3497e-05, 3.8023e-05, 1.2803e-03, 1.8213e-03, 4.3596e-04,\n",
      "          3.7172e-04, 2.1965e-04, 2.4481e-04, 2.4490e-04, 1.1919e-04,\n",
      "          2.3718e-04, 1.3617e-04, 8.2724e-02, 8.5947e-02, 3.4821e-03,\n",
      "          1.1569e-04, 2.0235e-04, 7.8880e-04, 2.7456e-03, 1.7224e-04,\n",
      "          9.2523e-02, 7.6525e-05, 5.2456e-04, 4.4565e-04, 1.3143e-04,\n",
      "          4.1305e-04, 4.0809e-03, 3.2905e-01, 3.3193e-01, 2.1582e-04,\n",
      "          5.4399e-04, 3.2073e-04, 4.4748e-04, 4.3830e-04, 6.7523e-04,\n",
      "          5.8078e-04, 9.4595e-03, 2.2376e-02, 2.3730e-02, 7.9272e-05,\n",
      "          6.2487e-05, 7.8220e-05, 5.3078e-05, 4.8867e-05, 2.9781e-05,\n",
      "          5.4076e-05]]])\n",
      "Player 1 Prediction: tensor([[0.3076, 0.4505, 0.2419, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.1107e-06, 1.7222e-06, 1.5389e-06, 3.4727e-06, 1.3399e-06,\n",
      "          1.6074e-06, 1.9566e-06, 2.4098e-01, 2.3762e-01, 1.8975e-05,\n",
      "          3.9317e-05, 1.2724e-05, 1.0478e-04, 1.0149e-04, 5.0286e-05,\n",
      "          4.6407e-05, 6.1900e-06, 8.3877e-05, 7.3472e-05, 1.7916e-05,\n",
      "          4.0845e-06, 1.0098e-05, 7.9286e-06, 1.9982e-05, 4.9371e-06,\n",
      "          2.3126e-01, 1.9264e-06, 5.4523e-06, 6.7688e-06, 8.7127e-06,\n",
      "          1.7689e-05, 5.9209e-05, 8.1360e-05, 7.6985e-05, 1.6399e-05,\n",
      "          1.6761e-04, 6.0306e-05, 6.4501e-05, 7.1044e-05, 5.3329e-05,\n",
      "          7.4490e-05, 7.7464e-05, 1.4512e-01, 1.4354e-01, 1.8784e-06,\n",
      "          1.6233e-06, 1.6205e-06, 1.7525e-06, 1.5296e-06, 1.6185e-06,\n",
      "          2.5594e-06],\n",
      "         [4.3728e-05, 5.3164e-05, 7.8729e-05, 6.7992e-05, 5.1335e-05,\n",
      "          6.8879e-05, 7.7243e-05, 1.1583e-03, 1.4219e-03, 1.0201e-03,\n",
      "          2.0793e-02, 2.7995e-03, 1.2950e-02, 1.5562e-02, 1.2624e-03,\n",
      "          5.3143e-04, 2.0355e-04, 3.0401e-03, 2.0176e-03, 3.2193e-04,\n",
      "          2.2513e-04, 1.6493e-04, 3.1228e-04, 8.4242e-04, 7.2116e-05,\n",
      "          1.2727e-01, 8.9902e-05, 1.0554e-03, 1.5418e-03, 3.9916e-04,\n",
      "          5.8304e-03, 7.0912e-03, 2.4030e-01, 4.9000e-01, 2.9857e-04,\n",
      "          6.3447e-03, 1.3705e-03, 1.8410e-02, 1.9706e-02, 4.0870e-03,\n",
      "          4.6168e-03, 2.7104e-03, 1.7003e-03, 1.5433e-03, 5.1568e-05,\n",
      "          7.9996e-05, 7.2364e-05, 7.6644e-05, 9.6492e-05, 8.9913e-05,\n",
      "          3.1559e-05],\n",
      "         [6.5274e-05, 6.4480e-05, 5.5658e-05, 8.0126e-05, 9.3169e-05,\n",
      "          1.2435e-04, 9.5093e-05, 6.4163e-05, 9.2260e-05, 1.2050e-04,\n",
      "          1.0933e-04, 9.5607e-05, 4.4636e-01, 4.8418e-01, 5.5521e-05,\n",
      "          2.7872e-03, 1.1482e-04, 2.3016e-02, 2.3636e-02, 9.8612e-05,\n",
      "          9.7186e-03, 3.9609e-05, 1.4163e-03, 3.9728e-03, 1.2969e-03,\n",
      "          6.8747e-05, 8.7322e-05, 9.1279e-05, 9.4315e-05, 5.5397e-05,\n",
      "          1.2262e-04, 6.0489e-05, 1.2016e-04, 5.5869e-05, 4.1079e-05,\n",
      "          1.5637e-04, 3.9800e-05, 5.1858e-05, 1.2833e-04, 6.4837e-05,\n",
      "          1.0722e-04, 1.3750e-04, 5.2529e-05, 9.0626e-05, 6.7305e-05,\n",
      "          8.8162e-05, 7.1515e-05, 1.0513e-04, 8.1382e-05, 6.9483e-05,\n",
      "          1.4351e-04],\n",
      "         [5.2565e-04, 9.8894e-04, 5.8447e-04, 1.3872e-03, 1.0567e-03,\n",
      "          8.8231e-04, 7.9558e-04, 5.8106e-03, 6.8561e-03, 3.9028e-03,\n",
      "          1.4173e-02, 5.5552e-03, 3.5160e-03, 3.0621e-03, 2.1880e-03,\n",
      "          4.3401e-03, 2.7040e-03, 4.0677e-02, 6.6893e-02, 1.5610e-02,\n",
      "          8.6874e-03, 3.0990e-03, 2.0411e-02, 3.1503e-02, 2.1086e-03,\n",
      "          1.1451e-01, 7.2512e-04, 1.3475e-02, 7.2959e-03, 2.8800e-03,\n",
      "          1.7998e-02, 2.7177e-02, 1.9156e-01, 1.7062e-01, 4.1378e-03,\n",
      "          1.9428e-02, 4.4389e-03, 5.0739e-03, 6.7374e-03, 1.9734e-02,\n",
      "          3.5280e-02, 3.0566e-02, 3.6364e-02, 3.8295e-02, 1.0742e-03,\n",
      "          1.1969e-03, 1.0298e-03, 9.2861e-04, 7.8039e-04, 5.2482e-04,\n",
      "          8.4874e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54893\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5777/10000 (57.8%)\n",
      "    Average reward: +0.803\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4223/10000 (42.2%)\n",
      "    Average reward: -0.803\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11841 (42.8%)\n",
      "    Action 1: 12109 (43.8%)\n",
      "    Action 2: 3054 (11.0%)\n",
      "    Action 3: 663 (2.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7838 (28.8%)\n",
      "    Action 1: 15287 (56.1%)\n",
      "    Action 2: 1937 (7.1%)\n",
      "    Action 3: 2164 (7.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8030.5, -8030.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.985 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8031\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3677, 0.6139, 0.0184, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7280, 0.0838, 0.1883]])\n",
      "Player 1 Prediction: tensor([[0.1496, 0.2883, 0.5620, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50783\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6097/10000 (61.0%)\n",
      "    Average reward: -0.038\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3903/10000 (39.0%)\n",
      "    Average reward: +0.038\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22315 (82.6%)\n",
      "    Action 1: 4691 (17.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3215 (13.5%)\n",
      "    Action 1: 17372 (73.1%)\n",
      "    Action 2: 1097 (4.6%)\n",
      "    Action 3: 2093 (8.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-384.0, 384.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.666 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.721 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.694\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0384\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21005/50000 [19:45<19:40, 24.56it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 134713/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 136826/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21998/50000 [20:27<19:23, 24.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 141063/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 143317/2000000\n",
      "P1 SL Buffer Size:  141063\n",
      "P1 SL buffer distribution [57125. 64711.  9545.  9682.]\n",
      "P1 actions distribution [0.4049609  0.45873829 0.0676648  0.068636  ]\n",
      "P2 SL Buffer Size:  143317\n",
      "P2 SL buffer distribution [49090. 71824.  9333. 13070.]\n",
      "P2 actions distribution [0.3425274  0.50115478 0.06512137 0.09119644]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[6.1736e-05, 1.1351e-04, 1.4031e-04, 1.6974e-04, 2.0451e-04,\n",
      "          3.1551e-04, 1.9388e-04, 1.5120e-03, 2.1968e-03, 1.4434e-03,\n",
      "          2.6495e-03, 9.3106e-04, 1.8278e-03, 2.4870e-03, 1.4105e-03,\n",
      "          5.1835e-03, 1.5545e-03, 2.5808e-02, 2.7979e-02, 1.4907e-02,\n",
      "          2.0501e-02, 2.9650e-03, 6.7293e-03, 7.8982e-03, 2.6039e-04,\n",
      "          4.4497e-01, 2.7561e-04, 1.3847e-02, 9.1690e-03, 6.4511e-03,\n",
      "          5.0554e-02, 1.2555e-02, 1.0614e-01, 4.9239e-02, 2.5456e-03,\n",
      "          1.0374e-02, 3.8360e-03, 8.8862e-03, 1.0075e-02, 1.8901e-02,\n",
      "          5.1142e-02, 1.5620e-02, 3.6399e-02, 1.8431e-02, 1.7835e-04,\n",
      "          1.6356e-04, 1.7987e-04, 1.5553e-04, 2.1552e-04, 1.6773e-04,\n",
      "          7.8364e-05],\n",
      "         [4.3208e-05, 5.6020e-05, 7.9681e-05, 6.2584e-05, 9.0581e-05,\n",
      "          1.1438e-04, 9.1871e-05, 2.5617e-04, 3.4830e-04, 3.4609e-04,\n",
      "          1.5391e-02, 3.6130e-03, 1.0411e-02, 1.2873e-02, 1.6211e-03,\n",
      "          3.2589e-02, 4.0225e-03, 3.6079e-03, 5.8712e-03, 4.1165e-03,\n",
      "          3.6399e-03, 6.1832e-04, 8.6141e-04, 1.0220e-03, 1.2425e-04,\n",
      "          1.4652e-01, 1.0328e-04, 2.1491e-03, 2.4125e-03, 2.8746e-02,\n",
      "          5.5040e-01, 1.8634e-03, 5.1907e-03, 3.7874e-03, 4.8918e-03,\n",
      "          4.1485e-02, 8.9397e-04, 2.0757e-03, 2.4397e-03, 2.3160e-02,\n",
      "          7.1644e-02, 2.5899e-03, 4.7414e-03, 2.5496e-03, 8.2756e-05,\n",
      "          6.6606e-05, 8.7802e-05, 5.7390e-05, 7.1268e-05, 7.5777e-05,\n",
      "          3.7852e-05],\n",
      "         [3.8796e-06, 4.4865e-06, 2.4189e-06, 4.8611e-06, 4.3975e-06,\n",
      "          3.8550e-06, 3.8240e-06, 6.9839e-06, 5.7292e-06, 3.2152e-06,\n",
      "          2.3234e-06, 3.2690e-06, 2.2425e-04, 2.8799e-04, 4.7058e-06,\n",
      "          9.4405e-04, 3.8809e-06, 3.7397e-04, 3.5328e-04, 5.2201e-06,\n",
      "          2.1215e-03, 5.2410e-06, 1.0445e-02, 2.4353e-01, 7.4153e-01,\n",
      "          5.0222e-06, 7.1028e-06, 6.2177e-06, 4.2616e-06, 4.3387e-06,\n",
      "          3.4880e-06, 4.8849e-06, 3.6130e-06, 1.9656e-06, 5.8679e-06,\n",
      "          2.5539e-06, 5.7862e-06, 3.6743e-06, 3.2183e-06, 3.8276e-06,\n",
      "          2.8892e-06, 4.0454e-06, 5.8265e-06, 5.6382e-06, 2.2491e-06,\n",
      "          5.9933e-06, 6.1786e-06, 4.0537e-06, 2.5943e-06, 4.3231e-06,\n",
      "          5.5626e-06],\n",
      "         [1.1356e-04, 1.8521e-04, 1.3091e-04, 1.7508e-04, 2.4517e-04,\n",
      "          2.6235e-04, 1.7709e-04, 1.4350e-03, 1.1646e-03, 8.3466e-04,\n",
      "          1.3398e-03, 5.1769e-04, 1.2592e-03, 1.1143e-03, 4.8388e-04,\n",
      "          1.9092e-03, 4.6632e-04, 8.4473e-02, 1.3248e-01, 2.1722e-02,\n",
      "          3.3114e-03, 1.1774e-03, 5.6303e-03, 5.1179e-03, 2.0953e-04,\n",
      "          4.6002e-01, 2.4489e-04, 6.4558e-03, 5.9003e-03, 1.8460e-03,\n",
      "          8.8428e-03, 1.4478e-02, 8.7765e-02, 5.7410e-02, 5.3628e-04,\n",
      "          2.4920e-03, 2.2394e-03, 5.7852e-03, 3.5693e-03, 5.8819e-03,\n",
      "          8.8619e-03, 1.4571e-02, 2.7361e-02, 1.8058e-02, 2.3072e-04,\n",
      "          3.5359e-04, 1.8189e-04, 2.6049e-04, 2.8301e-04, 3.1554e-04,\n",
      "          1.2512e-04]]])\n",
      "Player 0 Prediction: tensor([[0.7486, 0.2450, 0.0063, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.8794e-06, 2.8077e-06, 7.0267e-06, 5.4695e-06, 9.2197e-06,\n",
      "          8.0378e-06, 7.8480e-06, 1.5399e-03, 1.5319e-03, 5.3057e-05,\n",
      "          6.5741e-03, 4.6014e-05, 6.5780e-04, 7.3398e-04, 1.9989e-05,\n",
      "          9.5533e-02, 3.2022e-05, 8.5770e-05, 2.7026e-04, 5.6683e-04,\n",
      "          2.1498e-03, 8.0682e-05, 1.3566e-04, 1.6454e-04, 8.1587e-06,\n",
      "          8.7287e-01, 4.7073e-06, 1.0507e-04, 6.4143e-05, 2.0111e-04,\n",
      "          3.9158e-03, 1.8628e-04, 1.3498e-05, 6.1285e-06, 1.0687e-04,\n",
      "          8.2383e-03, 5.6569e-05, 1.2371e-05, 1.4289e-05, 1.1035e-03,\n",
      "          1.0826e-03, 7.6251e-04, 6.5766e-04, 3.3106e-04, 5.5350e-06,\n",
      "          5.2942e-06, 4.7880e-06, 8.3392e-06, 6.6376e-06, 4.3649e-06,\n",
      "          2.5955e-06],\n",
      "         [2.3492e-05, 3.6532e-05, 3.7229e-05, 3.7020e-05, 6.6069e-05,\n",
      "          7.5181e-05, 5.7934e-05, 5.6127e-04, 4.2796e-04, 9.3208e-05,\n",
      "          6.1508e-03, 9.2204e-04, 9.2628e-04, 1.1248e-03, 3.9900e-04,\n",
      "          5.4884e-02, 3.9215e-04, 9.3359e-03, 1.9056e-02, 6.2239e-03,\n",
      "          4.1642e-03, 5.2013e-04, 1.1194e-03, 1.3140e-03, 6.0987e-05,\n",
      "          4.2972e-01, 6.4038e-05, 1.8213e-03, 2.4098e-03, 1.6545e-03,\n",
      "          4.3664e-01, 2.1527e-03, 4.1649e-03, 2.0297e-03, 1.3276e-04,\n",
      "          2.0609e-03, 3.1540e-04, 5.4640e-05, 6.2517e-05, 4.9394e-03,\n",
      "          3.2130e-03, 1.8151e-04, 3.0249e-05, 2.0605e-05, 5.1231e-05,\n",
      "          3.7787e-05, 7.1454e-05, 3.7838e-05, 5.8580e-05, 3.9566e-05,\n",
      "          2.6212e-05],\n",
      "         [9.2851e-07, 7.8809e-07, 8.4160e-07, 1.2126e-06, 8.5297e-07,\n",
      "          1.2460e-06, 1.4593e-06, 9.6864e-07, 7.1594e-07, 6.7976e-07,\n",
      "          8.0799e-07, 6.6883e-07, 1.4955e-04, 1.5246e-04, 5.6675e-07,\n",
      "          1.3676e-03, 6.8908e-07, 5.1853e-05, 4.5334e-05, 7.1911e-07,\n",
      "          9.9657e-01, 8.9646e-07, 1.0369e-04, 1.4646e-03, 5.0189e-05,\n",
      "          1.6928e-06, 6.1570e-07, 1.1417e-06, 7.6490e-07, 7.2757e-07,\n",
      "          5.8254e-07, 9.5914e-07, 1.1411e-06, 4.6712e-07, 1.8415e-06,\n",
      "          6.5131e-07, 5.7526e-07, 5.7225e-07, 6.9067e-07, 1.1746e-06,\n",
      "          6.5172e-07, 1.5645e-06, 1.6989e-06, 2.0031e-06, 5.5006e-07,\n",
      "          1.0819e-06, 8.2711e-07, 7.8587e-07, 1.1659e-06, 1.3228e-06,\n",
      "          1.2927e-06],\n",
      "         [1.4404e-05, 1.4728e-05, 1.0810e-05, 1.6792e-05, 1.7509e-05,\n",
      "          1.7852e-05, 1.2663e-05, 5.3905e-05, 5.0349e-05, 5.7264e-05,\n",
      "          4.5503e-04, 5.6204e-05, 5.5909e-05, 5.1239e-05, 2.4916e-05,\n",
      "          1.1568e-04, 3.1013e-05, 1.6527e-03, 2.1364e-03, 1.8456e-03,\n",
      "          1.7322e-01, 1.1348e-04, 2.4947e-04, 2.6071e-04, 2.0102e-05,\n",
      "          7.3175e-01, 1.8331e-05, 1.6071e-04, 1.1451e-04, 2.8366e-04,\n",
      "          7.0802e-02, 3.5101e-04, 2.5433e-04, 1.4866e-04, 4.5339e-05,\n",
      "          2.9371e-04, 1.2716e-04, 1.9859e-04, 1.0597e-04, 1.6302e-03,\n",
      "          1.2815e-02, 1.3587e-04, 5.9552e-05, 4.4464e-05, 1.4039e-05,\n",
      "          1.0799e-05, 1.3210e-05, 1.5886e-05, 2.3696e-05, 2.1378e-05,\n",
      "          6.4706e-06]]])\n",
      "Player 0 Prediction: tensor([[0.0502, 0.1260, 0.8238, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21998/50000 [20:39<19:23, 24.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54754\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4993/10000 (49.9%)\n",
      "    Average reward: -0.594\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5007/10000 (50.1%)\n",
      "    Average reward: +0.594\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10078 (37.3%)\n",
      "    Action 1: 13233 (49.0%)\n",
      "    Action 2: 2120 (7.8%)\n",
      "    Action 3: 1599 (5.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8427 (30.4%)\n",
      "    Action 1: 14747 (53.2%)\n",
      "    Action 2: 2789 (10.1%)\n",
      "    Action 3: 1761 (6.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5938.5, 5938.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5938\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9550e-01, 2.4507e-04, 4.2586e-03]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9207, 0.0071, 0.0722]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53656\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5107/10000 (51.1%)\n",
      "    Average reward: +0.046\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4893/10000 (48.9%)\n",
      "    Average reward: -0.046\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4850 (18.7%)\n",
      "    Action 1: 14976 (57.6%)\n",
      "    Action 2: 2475 (9.5%)\n",
      "    Action 3: 3680 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20002 (72.3%)\n",
      "    Action 1: 7673 (27.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [459.0, -459.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.910 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.852 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.881\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0459\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6345, 0.3640, 0.0015, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.8112e-05, 3.9634e-05, 4.5086e-05, 4.9423e-05, 5.6613e-05,\n",
      "          3.8411e-05, 5.2968e-05, 1.9907e-04, 3.2618e-04, 3.3388e-04,\n",
      "          9.9850e-03, 5.6529e-03, 1.0454e-03, 1.2277e-03, 1.3796e-03,\n",
      "          3.0847e-02, 7.9677e-04, 8.6557e-03, 1.1941e-02, 1.3724e-03,\n",
      "          2.5025e-02, 2.2925e-03, 5.6973e-04, 5.9227e-04, 8.9120e-05,\n",
      "          2.0681e-01, 5.3954e-05, 1.2818e-03, 1.2549e-03, 4.3817e-03,\n",
      "          9.9471e-02, 3.5682e-03, 3.1296e-02, 2.6602e-02, 1.9111e-02,\n",
      "          3.8756e-01, 2.0212e-03, 1.8264e-03, 3.0655e-03, 3.4843e-02,\n",
      "          7.1900e-02, 6.6309e-04, 8.2679e-04, 4.2715e-04, 5.5068e-05,\n",
      "          9.5330e-05, 6.7141e-05, 4.8108e-05, 5.0719e-05, 4.1520e-05,\n",
      "          4.1052e-05],\n",
      "         [4.1921e-05, 5.6980e-05, 6.1674e-05, 6.3744e-05, 7.0792e-05,\n",
      "          6.7292e-05, 8.1042e-05, 1.2742e-03, 1.6943e-03, 9.0910e-04,\n",
      "          1.2134e-02, 5.5929e-03, 1.6700e-02, 1.8993e-02, 4.5009e-03,\n",
      "          8.0206e-03, 1.1388e-03, 3.3956e-02, 3.4754e-02, 2.9704e-03,\n",
      "          1.2090e-02, 8.1794e-04, 5.7630e-04, 1.1206e-03, 1.7057e-04,\n",
      "          2.5396e-01, 8.8989e-05, 3.0853e-03, 3.1281e-03, 5.6355e-03,\n",
      "          4.9724e-02, 1.5548e-02, 1.4322e-01, 1.4197e-01, 1.7855e-02,\n",
      "          7.0874e-02, 7.2768e-03, 2.7855e-02, 2.2872e-02, 9.3279e-03,\n",
      "          2.0035e-02, 6.3258e-03, 2.5394e-02, 1.7528e-02, 7.7915e-05,\n",
      "          5.8252e-05, 6.4983e-05, 6.0958e-05, 5.8825e-05, 7.4024e-05,\n",
      "          4.0483e-05],\n",
      "         [1.4616e-06, 1.2718e-06, 1.1277e-06, 1.9532e-06, 1.9369e-06,\n",
      "          2.2912e-06, 1.4271e-06, 8.6796e-07, 1.3315e-06, 1.7737e-06,\n",
      "          8.8460e-07, 1.5371e-06, 1.0394e-04, 1.4364e-04, 8.0585e-07,\n",
      "          1.6951e-04, 2.1578e-06, 9.1202e-05, 1.1432e-04, 9.2588e-07,\n",
      "          1.7756e-04, 6.0490e-07, 4.3025e-01, 5.6536e-01, 3.5363e-03,\n",
      "          1.1333e-06, 1.3880e-06, 1.1734e-06, 1.3208e-06, 1.2866e-06,\n",
      "          1.4598e-06, 1.3061e-06, 2.0028e-06, 6.8428e-07, 1.7609e-06,\n",
      "          1.4454e-06, 1.0908e-06, 1.1931e-06, 1.1349e-06, 1.8627e-06,\n",
      "          2.3713e-06, 1.3466e-06, 1.5638e-06, 1.4574e-06, 1.0963e-06,\n",
      "          7.0506e-07, 7.7398e-07, 1.5640e-06, 1.1918e-06, 9.9735e-07,\n",
      "          1.2195e-06],\n",
      "         [1.2987e-04, 1.8365e-04, 2.5694e-04, 1.5479e-04, 2.5378e-04,\n",
      "          1.8757e-04, 2.2656e-04, 8.0546e-04, 8.3230e-04, 7.5527e-04,\n",
      "          5.0981e-03, 2.2719e-03, 4.6293e-03, 5.0098e-03, 8.3946e-04,\n",
      "          2.1777e-03, 4.6981e-04, 1.8475e-02, 2.2336e-02, 6.2957e-03,\n",
      "          2.3112e-03, 1.0071e-03, 1.3987e-01, 1.3572e-01, 2.5698e-03,\n",
      "          9.1391e-02, 2.4306e-04, 3.6967e-02, 3.8407e-02, 5.7718e-04,\n",
      "          5.3858e-03, 1.8907e-02, 2.1047e-01, 1.7597e-01, 2.4452e-03,\n",
      "          1.0580e-02, 2.1906e-03, 6.0887e-03, 6.0666e-03, 4.4224e-03,\n",
      "          6.9885e-03, 5.9578e-03, 1.3966e-02, 8.6266e-03, 2.1594e-04,\n",
      "          2.4635e-04, 2.4374e-04, 1.7112e-04, 1.6901e-04, 2.4995e-04,\n",
      "          1.9098e-04]]])\n",
      "Player 1 Prediction: tensor([[1.6998e-01, 8.2957e-01, 4.4507e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[3.7877e-05, 4.6691e-05, 4.8094e-05, 4.5912e-05, 4.6285e-05,\n",
      "          4.2996e-05, 5.3009e-05, 2.4216e-03, 2.9073e-03, 1.0103e-03,\n",
      "          2.6436e-03, 9.3404e-04, 1.8968e-02, 2.2165e-02, 4.0800e-03,\n",
      "          7.3931e-04, 1.7674e-04, 5.0177e-02, 4.8362e-02, 4.5858e-03,\n",
      "          2.9590e-04, 1.2555e-04, 6.6102e-04, 2.4545e-03, 1.4873e-04,\n",
      "          2.3838e-01, 5.8794e-05, 5.6692e-04, 5.9005e-04, 2.6369e-04,\n",
      "          5.3702e-04, 1.3126e-02, 1.7993e-01, 1.8336e-01, 5.4976e-04,\n",
      "          2.2593e-03, 5.6540e-03, 4.9711e-02, 4.1395e-02, 4.6369e-04,\n",
      "          1.2833e-03, 9.6277e-03, 6.0725e-02, 4.8003e-02, 4.8470e-05,\n",
      "          6.9563e-05, 5.2561e-05, 4.5260e-05, 3.5974e-05, 5.3357e-05,\n",
      "          3.7937e-05],\n",
      "         [6.0957e-05, 7.2294e-05, 8.2860e-05, 1.1330e-04, 8.8049e-05,\n",
      "          7.0868e-05, 1.0730e-04, 3.4006e-05, 7.0130e-05, 8.3107e-04,\n",
      "          5.4353e-02, 6.4523e-03, 4.2436e-02, 3.9163e-02, 9.8846e-04,\n",
      "          1.9231e-02, 5.0073e-04, 5.3022e-04, 5.9892e-04, 8.1631e-04,\n",
      "          8.9262e-03, 6.3657e-04, 1.0659e-03, 1.1180e-03, 1.7808e-04,\n",
      "          1.5100e-01, 1.0557e-04, 5.6246e-04, 6.3796e-04, 1.0593e-03,\n",
      "          1.4372e-01, 4.9355e-03, 4.0208e-02, 5.7043e-02, 6.7853e-04,\n",
      "          2.9575e-01, 6.9026e-04, 1.0576e-02, 1.6166e-02, 1.6993e-02,\n",
      "          7.9274e-02, 1.2871e-03, 8.7506e-05, 7.0476e-05, 1.1784e-04,\n",
      "          9.7148e-05, 6.4858e-05, 8.6315e-05, 8.0876e-05, 1.2095e-04,\n",
      "          6.3995e-05],\n",
      "         [4.9073e-07, 4.1162e-07, 2.2968e-07, 5.0900e-07, 8.4892e-07,\n",
      "          6.2426e-07, 5.1037e-07, 4.5963e-07, 1.7675e-07, 5.3131e-07,\n",
      "          3.2228e-07, 3.5761e-07, 5.8274e-05, 4.0125e-05, 2.3548e-07,\n",
      "          1.4402e-05, 4.3118e-07, 2.8909e-04, 1.6303e-04, 4.1219e-07,\n",
      "          9.9939e-01, 2.7948e-07, 1.4452e-06, 1.7114e-05, 7.6850e-06,\n",
      "          3.7766e-07, 3.8780e-07, 2.8458e-07, 4.2082e-07, 2.6485e-07,\n",
      "          2.6510e-07, 2.8495e-07, 3.7362e-07, 3.3139e-07, 2.6059e-07,\n",
      "          4.1811e-07, 2.5306e-07, 2.5110e-07, 3.7837e-07, 2.1447e-07,\n",
      "          5.5702e-07, 3.1964e-07, 2.7571e-07, 5.5009e-07, 3.5213e-07,\n",
      "          2.9762e-07, 3.1317e-07, 3.8626e-07, 3.8122e-07, 4.5562e-07,\n",
      "          3.5534e-07],\n",
      "         [1.7553e-04, 1.6284e-04, 2.0371e-04, 2.2763e-04, 2.9579e-04,\n",
      "          2.1345e-04, 1.5581e-04, 1.3390e-03, 2.0281e-03, 8.1678e-04,\n",
      "          1.6055e-02, 4.5871e-03, 6.2082e-04, 5.9715e-04, 4.8149e-04,\n",
      "          5.2061e-03, 5.7676e-04, 1.7951e-02, 2.5314e-02, 5.7295e-03,\n",
      "          1.7628e-02, 2.1970e-03, 8.4647e-04, 1.0543e-03, 8.7890e-04,\n",
      "          6.6297e-01, 2.5543e-04, 5.6231e-04, 6.5163e-04, 7.2315e-04,\n",
      "          4.2115e-02, 7.6718e-03, 2.9422e-02, 3.1688e-02, 3.6241e-03,\n",
      "          3.9899e-02, 4.2670e-04, 5.4868e-04, 1.1237e-03, 1.8259e-02,\n",
      "          2.8759e-02, 6.6822e-03, 1.0994e-02, 6.6272e-03, 1.9561e-04,\n",
      "          3.4375e-04, 1.9556e-04, 2.1430e-04, 2.2488e-04, 1.4586e-04,\n",
      "          3.3102e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8316, 0.0026, 0.1658]])\n",
      "Player 0 Prediction: tensor([[[1.6682e-07, 3.0909e-07, 3.9413e-07, 1.8432e-07, 2.7900e-07,\n",
      "          3.1284e-07, 2.9744e-07, 3.9584e-05, 4.6922e-05, 2.9950e-06,\n",
      "          2.2204e-04, 3.7175e-06, 4.0343e-01, 4.0213e-01, 3.9592e-06,\n",
      "          3.6374e-04, 1.5275e-06, 5.2824e-05, 5.2915e-05, 2.2845e-06,\n",
      "          2.1304e-06, 2.1471e-06, 1.8168e-06, 1.2647e-06, 4.5702e-07,\n",
      "          1.8340e-01, 2.1967e-07, 1.0146e-06, 8.0713e-07, 2.3067e-06,\n",
      "          2.6221e-06, 1.0914e-05, 1.4505e-05, 1.2925e-05, 7.2070e-06,\n",
      "          1.3840e-05, 1.0123e-05, 4.9233e-03, 5.2263e-03, 6.2731e-06,\n",
      "          5.7104e-06, 4.7471e-06, 5.4667e-07, 7.0740e-07, 3.4027e-07,\n",
      "          1.4385e-07, 1.8160e-07, 2.4914e-07, 2.7717e-07, 3.0225e-07,\n",
      "          2.0236e-07],\n",
      "         [7.8153e-07, 1.2696e-06, 1.0182e-06, 8.3091e-07, 1.2983e-06,\n",
      "          1.3727e-06, 1.0585e-06, 4.0893e-01, 4.0487e-01, 3.6938e-05,\n",
      "          2.6079e-04, 1.3372e-05, 9.0438e-05, 7.6642e-05, 1.9116e-05,\n",
      "          6.9449e-06, 5.4900e-06, 1.8458e-03, 1.4531e-03, 4.2574e-05,\n",
      "          2.3365e-05, 2.6921e-06, 1.5077e-05, 1.7107e-05, 1.7814e-06,\n",
      "          1.6258e-01, 7.8324e-07, 1.5375e-05, 1.3222e-05, 8.0687e-06,\n",
      "          1.0598e-06, 1.4095e-04, 1.0252e-03, 1.4638e-03, 6.9555e-06,\n",
      "          4.8736e-06, 3.0248e-05, 3.9233e-03, 4.1070e-03, 3.7049e-06,\n",
      "          7.2171e-07, 3.8589e-05, 4.6758e-03, 4.2315e-03, 1.5962e-06,\n",
      "          7.0110e-07, 9.0082e-07, 1.6338e-06, 1.0639e-06, 7.8901e-07,\n",
      "          9.0159e-07],\n",
      "         [7.8087e-07, 8.7920e-07, 8.7188e-07, 9.1331e-07, 5.9654e-07,\n",
      "          2.1932e-06, 6.0552e-07, 7.1804e-07, 1.4508e-06, 1.4812e-06,\n",
      "          1.4156e-06, 1.2710e-06, 6.2651e-05, 6.4867e-05, 4.5929e-07,\n",
      "          9.9441e-05, 2.0013e-06, 5.0033e-01, 4.9916e-01, 1.3751e-06,\n",
      "          6.4928e-06, 1.1018e-06, 6.3019e-05, 1.3446e-04, 3.1684e-05,\n",
      "          1.0534e-06, 7.8016e-07, 1.3987e-06, 2.2756e-06, 2.0873e-06,\n",
      "          7.7930e-07, 1.1859e-06, 2.3303e-06, 1.0182e-06, 9.9899e-07,\n",
      "          2.3393e-06, 8.7529e-07, 6.8370e-07, 1.4601e-06, 1.3525e-06,\n",
      "          1.3562e-06, 1.5903e-06, 1.7936e-06, 1.2272e-06, 1.0115e-06,\n",
      "          9.2955e-07, 1.2367e-06, 5.8113e-07, 1.9097e-06, 5.5978e-07,\n",
      "          2.4237e-06],\n",
      "         [2.3538e-05, 2.8515e-05, 2.5284e-05, 1.6342e-05, 2.1917e-05,\n",
      "          1.3374e-05, 3.5303e-05, 6.1464e-04, 2.9019e-04, 2.4581e-04,\n",
      "          2.6196e-04, 1.7622e-04, 2.0968e-04, 2.1136e-04, 5.3737e-05,\n",
      "          1.5172e-04, 4.2989e-05, 1.2885e-01, 1.0009e-01, 3.3654e-03,\n",
      "          4.1556e-04, 1.2115e-04, 1.5778e-03, 6.3336e-03, 2.3062e-04,\n",
      "          7.1007e-01, 2.5583e-05, 9.5750e-04, 9.8119e-04, 6.3700e-05,\n",
      "          7.1008e-05, 2.9182e-03, 1.7256e-02, 1.6292e-02, 1.5687e-04,\n",
      "          9.5194e-04, 1.0819e-04, 2.8551e-04, 2.9592e-04, 2.7183e-04,\n",
      "          1.8996e-04, 1.0701e-03, 1.6986e-03, 2.7565e-03, 2.5721e-05,\n",
      "          3.1849e-05, 2.4929e-05, 1.4586e-05, 1.6070e-05, 3.8771e-05,\n",
      "          2.1992e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54853\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5983/10000 (59.8%)\n",
      "    Average reward: +0.603\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4017/10000 (40.2%)\n",
      "    Average reward: -0.603\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9679 (35.4%)\n",
      "    Action 1: 14553 (53.2%)\n",
      "    Action 2: 2353 (8.6%)\n",
      "    Action 3: 773 (2.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7361 (26.8%)\n",
      "    Action 1: 15001 (54.6%)\n",
      "    Action 2: 2172 (7.9%)\n",
      "    Action 3: 2961 (10.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6032.5, -6032.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.015 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.986 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.000\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6032\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8305e-01, 9.1400e-04, 1.6033e-02]])\n",
      "Player 1 Prediction: tensor([[0.5095, 0.4876, 0.0029, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50535\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6228/10000 (62.3%)\n",
      "    Average reward: +0.003\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3772/10000 (37.7%)\n",
      "    Average reward: -0.003\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22266 (82.9%)\n",
      "    Action 1: 4589 (17.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3218 (13.6%)\n",
      "    Action 1: 17251 (72.9%)\n",
      "    Action 2: 1129 (4.8%)\n",
      "    Action 3: 2082 (8.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [25.0, -25.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.660 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.724 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.692\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0025\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23003/50000 [21:43<18:29, 24.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 147474/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 149476/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [22:24<19:07, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 153929/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 155605/2000000\n",
      "P1 SL Buffer Size:  153929\n",
      "P1 SL buffer distribution [61436. 71756. 10650. 10087.]\n",
      "P1 actions distribution [0.39911907 0.46616297 0.06918774 0.06553021]\n",
      "P2 SL Buffer Size:  155605\n",
      "P2 SL buffer distribution [52690. 78541. 10635. 13739.]\n",
      "P2 actions distribution [0.3386138  0.50474599 0.06834613 0.08829408]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.4821, 0.5167, 0.0012, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.0919e-05, 2.2923e-05, 2.3476e-05, 2.4901e-05, 3.6069e-05,\n",
      "          3.3422e-05, 3.0870e-05, 1.5271e-04, 1.4896e-04, 1.0439e-04,\n",
      "          2.7868e-03, 5.9554e-04, 1.0953e-04, 1.0151e-04, 8.1348e-05,\n",
      "          1.3563e-02, 7.1465e-04, 1.6225e-03, 2.4633e-03, 1.3854e-03,\n",
      "          3.9837e-02, 1.9254e-03, 5.1011e-04, 4.8032e-04, 4.8900e-05,\n",
      "          5.9503e-01, 4.1982e-05, 5.0647e-04, 4.3652e-04, 8.6582e-03,\n",
      "          1.4026e-01, 5.6742e-04, 8.2207e-04, 5.6523e-04, 5.2944e-03,\n",
      "          5.5477e-02, 1.2340e-04, 3.3733e-04, 4.3497e-04, 1.8426e-02,\n",
      "          9.3636e-02, 1.3390e-03, 7.9304e-03, 3.0928e-03, 3.0744e-05,\n",
      "          2.9770e-05, 3.1340e-05, 3.3054e-05, 3.2304e-05, 3.6291e-05,\n",
      "          1.6830e-05],\n",
      "         [3.4770e-05, 5.8128e-05, 7.9796e-05, 5.8645e-05, 1.0672e-04,\n",
      "          1.3376e-04, 1.1285e-04, 6.2853e-04, 7.3953e-04, 5.6020e-04,\n",
      "          5.0644e-04, 8.0752e-04, 3.9364e-03, 4.7811e-03, 1.3644e-03,\n",
      "          1.1813e-03, 6.6653e-04, 9.1637e-02, 1.5036e-01, 2.3322e-02,\n",
      "          3.6951e-03, 7.8084e-04, 1.6577e-03, 1.7511e-03, 1.0585e-04,\n",
      "          4.3784e-01, 1.3524e-04, 1.2403e-03, 1.4630e-03, 3.1307e-03,\n",
      "          8.8564e-03, 8.8592e-03, 5.4836e-02, 3.9285e-02, 6.6742e-04,\n",
      "          6.5346e-04, 2.7145e-03, 2.5784e-02, 2.2428e-02, 3.8582e-03,\n",
      "          7.0141e-03, 1.2612e-02, 4.8173e-02, 3.0903e-02, 7.0754e-05,\n",
      "          6.0017e-05, 9.9337e-05, 6.8040e-05, 7.6922e-05, 7.0343e-05,\n",
      "          3.2556e-05],\n",
      "         [1.2727e-06, 7.5765e-07, 8.8130e-07, 1.3009e-06, 1.2589e-06,\n",
      "          1.6097e-06, 1.3941e-06, 1.2815e-06, 9.7934e-07, 1.1360e-06,\n",
      "          1.2717e-06, 7.6645e-07, 7.3560e-05, 9.5241e-05, 6.3543e-07,\n",
      "          9.7941e-04, 8.5983e-07, 7.5884e-05, 9.0253e-05, 1.1457e-06,\n",
      "          3.1945e-04, 9.2760e-07, 5.0706e-01, 4.9117e-01, 8.3074e-05,\n",
      "          2.8164e-06, 1.5023e-06, 1.3669e-06, 1.0615e-06, 1.2328e-06,\n",
      "          7.4914e-07, 1.3762e-06, 1.1281e-06, 4.5491e-07, 1.3781e-06,\n",
      "          5.9991e-07, 1.0298e-06, 8.3514e-07, 1.3286e-06, 1.7406e-06,\n",
      "          8.4648e-07, 9.8844e-07, 2.4205e-06, 1.8990e-06, 1.0038e-06,\n",
      "          1.2985e-06, 1.5444e-06, 7.7539e-07, 7.4025e-07, 1.1222e-06,\n",
      "          1.2538e-06],\n",
      "         [2.0842e-04, 2.3478e-04, 1.9624e-04, 2.6919e-04, 1.4608e-04,\n",
      "          3.7105e-04, 1.5756e-04, 8.2271e-04, 1.4601e-03, 1.1133e-03,\n",
      "          3.8336e-03, 6.6731e-04, 1.4277e-03, 1.5150e-03, 6.0331e-04,\n",
      "          1.9805e-03, 5.7175e-04, 4.9993e-02, 4.5734e-02, 1.6884e-02,\n",
      "          2.1305e-02, 1.8750e-03, 1.2358e-02, 1.3845e-02, 4.2043e-04,\n",
      "          3.7911e-01, 3.7173e-04, 2.4952e-02, 1.5043e-02, 2.6076e-03,\n",
      "          3.8484e-02, 1.0407e-02, 1.3514e-01, 6.2129e-02, 6.7805e-04,\n",
      "          4.6812e-03, 2.9360e-03, 5.1887e-03, 6.4947e-03, 1.2121e-02,\n",
      "          5.5456e-02, 1.0208e-02, 3.6658e-02, 1.7629e-02, 2.5152e-04,\n",
      "          2.8628e-04, 2.7605e-04, 1.9691e-04, 3.4883e-04, 2.3534e-04,\n",
      "          1.1680e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7693, 0.0219, 0.2088]])\n",
      "Player 1 Prediction: tensor([[[1.0158e-07, 7.0964e-08, 1.3305e-07, 1.8654e-07, 3.3537e-07,\n",
      "          3.2592e-07, 1.6918e-07, 8.4288e-05, 5.4143e-05, 1.7368e-06,\n",
      "          1.8774e-04, 1.0276e-06, 2.2119e-05, 2.0312e-05, 5.2516e-07,\n",
      "          8.6046e-01, 8.8892e-07, 4.1191e-06, 1.2196e-05, 2.0169e-05,\n",
      "          1.7226e-05, 1.3346e-06, 3.0433e-06, 4.3080e-06, 2.2534e-07,\n",
      "          7.9407e-02, 9.9464e-08, 2.2420e-06, 1.8531e-06, 4.4986e-06,\n",
      "          4.0737e-05, 7.7024e-06, 2.8309e-07, 1.1827e-07, 5.1290e-06,\n",
      "          5.9532e-02, 5.5979e-06, 5.9100e-07, 4.8258e-07, 2.7364e-05,\n",
      "          1.0350e-05, 2.4646e-05, 1.7045e-05, 1.4581e-05, 1.3516e-07,\n",
      "          2.1082e-07, 2.1269e-07, 2.2849e-07, 1.4674e-07, 1.0414e-07,\n",
      "          6.1372e-08],\n",
      "         [5.5535e-07, 4.1809e-07, 6.6284e-07, 5.1400e-07, 1.2721e-06,\n",
      "          6.1621e-07, 8.4494e-07, 1.7155e-05, 1.3387e-05, 8.3546e-07,\n",
      "          8.6618e-01, 2.6688e-05, 3.6873e-05, 3.1741e-05, 2.1583e-06,\n",
      "          2.3947e-03, 6.0544e-06, 6.7352e-06, 9.3084e-06, 2.2911e-05,\n",
      "          3.5353e-05, 5.5744e-06, 9.8154e-06, 7.0984e-06, 1.0920e-06,\n",
      "          1.0350e-01, 1.9689e-06, 3.0901e-05, 5.0317e-05, 5.3351e-05,\n",
      "          1.3514e-02, 1.2972e-05, 1.2127e-05, 7.3531e-06, 6.1297e-06,\n",
      "          1.1558e-03, 1.2675e-06, 1.0221e-06, 1.4946e-06, 1.6349e-04,\n",
      "          1.2672e-02, 3.3773e-07, 2.3832e-08, 2.9073e-08, 6.2225e-07,\n",
      "          7.0135e-07, 8.7499e-07, 8.8360e-07, 6.9443e-07, 7.6960e-07,\n",
      "          5.5312e-07],\n",
      "         [1.8671e-08, 2.0500e-08, 1.9601e-08, 3.5655e-08, 1.9529e-08,\n",
      "          2.9205e-08, 4.1390e-08, 4.4196e-08, 1.7919e-08, 3.0106e-08,\n",
      "          2.1001e-08, 2.3001e-08, 6.6923e-06, 5.2823e-06, 2.6386e-08,\n",
      "          7.2709e-05, 2.1756e-08, 1.3045e-06, 9.9257e-07, 2.6724e-08,\n",
      "          9.9991e-01, 2.8931e-08, 4.1373e-07, 3.0724e-06, 2.3356e-06,\n",
      "          3.8615e-08, 1.6029e-08, 2.6219e-08, 2.1140e-08, 1.7844e-08,\n",
      "          2.4229e-08, 4.8397e-08, 3.1475e-08, 1.4930e-08, 5.9964e-08,\n",
      "          2.4956e-08, 1.6270e-08, 1.7082e-08, 2.2857e-08, 2.8300e-08,\n",
      "          2.2689e-08, 3.5124e-08, 4.3987e-08, 9.6862e-08, 1.5397e-08,\n",
      "          2.2778e-08, 2.5519e-08, 2.4126e-08, 2.2595e-08, 4.7749e-08,\n",
      "          4.0114e-08],\n",
      "         [9.3230e-06, 6.1192e-06, 3.2643e-06, 4.6061e-06, 6.2806e-06,\n",
      "          6.7559e-06, 5.9474e-06, 2.2758e-05, 8.2562e-06, 2.9606e-05,\n",
      "          1.7587e-04, 2.0452e-05, 1.9692e-05, 3.2575e-05, 1.2862e-05,\n",
      "          2.9018e-05, 1.6832e-05, 7.4298e-04, 1.0957e-03, 9.7429e-04,\n",
      "          5.7172e-02, 3.4480e-05, 7.1635e-05, 5.6846e-05, 9.3225e-06,\n",
      "          8.4559e-01, 4.7166e-06, 7.2454e-05, 4.7068e-05, 1.2416e-04,\n",
      "          8.9049e-02, 1.5642e-04, 8.9625e-05, 5.1916e-05, 1.2385e-05,\n",
      "          1.1418e-04, 4.1105e-05, 6.9824e-05, 3.6742e-05, 5.8232e-04,\n",
      "          3.3001e-03, 3.1530e-05, 4.8707e-06, 8.4897e-06, 3.9801e-06,\n",
      "          6.0878e-06, 5.4933e-06, 6.1436e-06, 6.7240e-06, 1.0819e-05,\n",
      "          1.5613e-06]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [22:40<19:07, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54831\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5117/10000 (51.2%)\n",
      "    Average reward: -0.478\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4883/10000 (48.8%)\n",
      "    Average reward: +0.478\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10192 (37.6%)\n",
      "    Action 1: 13264 (48.9%)\n",
      "    Action 2: 2162 (8.0%)\n",
      "    Action 3: 1510 (5.6%)\n",
      "  Player 1:\n",
      "    Action 0: 8218 (29.7%)\n",
      "    Action 1: 14930 (53.9%)\n",
      "    Action 2: 2881 (10.4%)\n",
      "    Action 3: 1674 (6.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4779.0, 4779.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.018\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4779\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.4821, 0.5167, 0.0012, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6954, 0.0243, 0.2803]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53096\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5116/10000 (51.2%)\n",
      "    Average reward: +0.049\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4884/10000 (48.8%)\n",
      "    Average reward: -0.049\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4831 (18.8%)\n",
      "    Action 1: 14927 (58.1%)\n",
      "    Action 2: 2436 (9.5%)\n",
      "    Action 3: 3513 (13.7%)\n",
      "  Player 1:\n",
      "    Action 0: 19892 (72.6%)\n",
      "    Action 1: 7497 (27.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [491.0, -491.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.909 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.847 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.878\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0491\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[7.4251e-05, 9.2182e-05, 9.0715e-05, 1.0184e-04, 9.7477e-05,\n",
      "          1.1110e-04, 1.0006e-04, 2.0819e-03, 3.5436e-03, 2.0755e-03,\n",
      "          7.2540e-03, 4.5130e-03, 2.3221e-02, 3.1162e-02, 9.4866e-03,\n",
      "          2.2935e-03, 7.5918e-04, 5.0678e-03, 5.0351e-03, 1.4436e-03,\n",
      "          3.2035e-03, 1.2179e-03, 7.1056e-04, 9.8402e-04, 2.3956e-04,\n",
      "          1.1827e-01, 1.5672e-04, 4.7953e-03, 4.9423e-03, 3.1365e-03,\n",
      "          1.7651e-02, 2.8942e-02, 2.1329e-01, 2.0922e-01, 6.9928e-03,\n",
      "          2.1910e-02, 1.6583e-02, 7.7189e-02, 6.1592e-02, 1.1138e-02,\n",
      "          1.9803e-02, 1.7984e-02, 3.9261e-02, 2.1551e-02, 1.1948e-04,\n",
      "          9.5511e-05, 8.7068e-05, 8.5102e-05, 9.3473e-05, 7.5624e-05,\n",
      "          7.8151e-05],\n",
      "         [2.6745e-05, 3.7433e-05, 4.4063e-05, 4.0363e-05, 3.6046e-05,\n",
      "          4.0750e-05, 4.6583e-05, 4.8106e-04, 8.0139e-04, 6.1310e-04,\n",
      "          3.4971e-02, 6.1455e-03, 2.6026e-03, 2.7950e-03, 1.0186e-03,\n",
      "          3.3064e-03, 7.7340e-04, 1.8899e-03, 1.8217e-03, 6.9379e-04,\n",
      "          4.4956e-03, 4.1521e-04, 3.0330e-04, 6.0514e-04, 9.9448e-05,\n",
      "          4.3857e-02, 6.5198e-05, 1.5258e-03, 1.5172e-03, 6.8240e-03,\n",
      "          1.8676e-01, 3.2782e-03, 1.5736e-02, 1.5268e-02, 4.4205e-02,\n",
      "          4.6407e-01, 1.4540e-03, 4.3378e-03, 4.8360e-03, 2.8697e-02,\n",
      "          1.0106e-01, 3.2334e-03, 5.0475e-03, 3.8051e-03, 4.4552e-05,\n",
      "          5.1936e-05, 4.3893e-05, 4.5303e-05, 4.6645e-05, 6.2993e-05,\n",
      "          2.4302e-05],\n",
      "         [6.1506e-06, 9.2256e-06, 7.9914e-06, 6.8811e-06, 9.1445e-06,\n",
      "          1.1424e-05, 1.5205e-05, 6.2878e-06, 1.1446e-05, 1.0621e-05,\n",
      "          1.0387e-05, 1.2047e-05, 7.7953e-04, 7.5965e-04, 8.3929e-06,\n",
      "          7.4827e-04, 1.2669e-05, 1.4911e-03, 1.7275e-03, 5.5896e-06,\n",
      "          2.5838e-03, 5.3495e-06, 2.5295e-03, 1.1887e-01, 8.7014e-01,\n",
      "          7.7173e-06, 9.3751e-06, 9.7653e-06, 9.9096e-06, 7.8880e-06,\n",
      "          7.4845e-06, 6.6166e-06, 1.2364e-05, 5.0683e-06, 9.6330e-06,\n",
      "          9.2078e-06, 6.2550e-06, 5.8270e-06, 8.4692e-06, 1.0728e-05,\n",
      "          9.5798e-06, 6.7319e-06, 4.9381e-06, 1.1768e-05, 8.0924e-06,\n",
      "          6.5720e-06, 7.8979e-06, 1.0893e-05, 9.1926e-06, 7.9845e-06,\n",
      "          9.9841e-06],\n",
      "         [3.4074e-04, 3.7679e-04, 3.6746e-04, 3.1367e-04, 5.2757e-04,\n",
      "          3.5235e-04, 2.8947e-04, 1.0135e-03, 1.8879e-03, 1.6753e-03,\n",
      "          7.6564e-03, 3.0702e-03, 3.1075e-03, 3.6723e-03, 9.6649e-04,\n",
      "          3.0537e-03, 1.2689e-03, 3.7480e-02, 4.3843e-02, 6.6490e-03,\n",
      "          2.6824e-03, 2.0052e-03, 3.2895e-02, 3.1959e-02, 3.6455e-03,\n",
      "          1.5205e-01, 5.0371e-04, 1.5057e-02, 1.5616e-02, 9.7186e-04,\n",
      "          1.1915e-02, 1.3593e-02, 2.3795e-01, 2.4533e-01, 5.2828e-03,\n",
      "          2.1653e-02, 2.5651e-03, 7.7001e-03, 9.4689e-03, 6.4213e-03,\n",
      "          1.3513e-02, 1.0226e-02, 2.4712e-02, 1.1268e-02, 3.8686e-04,\n",
      "          5.3851e-04, 3.9150e-04, 5.6257e-04, 3.8202e-04, 4.0876e-04,\n",
      "          4.3288e-04]]])\n",
      "Player 1 Prediction: tensor([[1.5427e-01, 8.4547e-01, 2.6461e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[4.1753e-05, 5.2676e-05, 4.9133e-05, 5.0109e-05, 4.7083e-05,\n",
      "          4.7954e-05, 5.7111e-05, 2.9904e-03, 4.5658e-03, 1.7073e-03,\n",
      "          2.8628e-03, 1.2202e-03, 1.2954e-02, 1.5189e-02, 3.9914e-03,\n",
      "          3.4221e-04, 1.4594e-04, 2.0760e-02, 1.8350e-02, 3.1210e-03,\n",
      "          2.0874e-04, 9.2843e-05, 5.9615e-04, 2.8717e-03, 2.1057e-04,\n",
      "          1.0494e-01, 8.4170e-05, 7.8757e-04, 7.3734e-04, 2.6123e-04,\n",
      "          6.5891e-04, 1.8975e-02, 2.6260e-01, 2.6554e-01, 4.5386e-04,\n",
      "          1.2720e-03, 6.5184e-03, 5.3801e-02, 4.5005e-02, 5.4306e-04,\n",
      "          9.2067e-04, 1.9595e-02, 6.9087e-02, 5.5340e-02, 4.7154e-05,\n",
      "          5.2895e-05, 6.0111e-05, 5.2831e-05, 4.1375e-05, 5.0895e-05,\n",
      "          4.9144e-05],\n",
      "         [2.0422e-05, 2.1263e-05, 2.6430e-05, 3.3098e-05, 2.4002e-05,\n",
      "          2.3144e-05, 2.8410e-05, 1.7152e-05, 4.1498e-05, 4.7191e-04,\n",
      "          6.5252e-02, 2.6749e-03, 3.2441e-03, 2.6727e-03, 2.0938e-04,\n",
      "          2.6363e-03, 1.3111e-04, 4.7157e-05, 4.2693e-05, 1.4794e-04,\n",
      "          1.8428e-03, 1.5508e-04, 2.4974e-04, 3.3198e-04, 6.0279e-05,\n",
      "          2.0086e-02, 2.7831e-05, 1.8993e-04, 2.7132e-04, 4.4605e-04,\n",
      "          2.3423e-01, 7.2508e-04, 3.2507e-03, 4.3567e-03, 8.0939e-04,\n",
      "          5.0126e-01, 1.3568e-04, 1.3438e-03, 2.0542e-03, 1.4788e-02,\n",
      "          1.3474e-01, 6.0432e-04, 3.8008e-05, 3.2421e-05, 3.7455e-05,\n",
      "          3.5883e-05, 1.7768e-05, 2.4593e-05, 3.3628e-05, 3.6391e-05,\n",
      "          1.6625e-05],\n",
      "         [1.4123e-06, 1.3729e-06, 8.8984e-07, 1.1320e-06, 2.4443e-06,\n",
      "          1.5746e-06, 2.1588e-06, 1.1016e-06, 7.9145e-07, 1.3347e-06,\n",
      "          1.1742e-06, 1.2174e-06, 1.5727e-04, 9.9178e-05, 8.0523e-07,\n",
      "          4.9979e-05, 1.2288e-06, 5.9981e-04, 3.0883e-04, 1.0956e-06,\n",
      "          9.9867e-01, 1.1117e-06, 4.7371e-06, 3.7608e-05, 1.8628e-05,\n",
      "          1.4095e-06, 1.3839e-06, 1.1223e-06, 2.0203e-06, 1.0576e-06,\n",
      "          1.0680e-06, 1.0799e-06, 1.3079e-06, 1.0925e-06, 7.4408e-07,\n",
      "          1.5825e-06, 9.1543e-07, 8.0985e-07, 1.5088e-06, 7.2511e-07,\n",
      "          1.4355e-06, 1.0481e-06, 5.7980e-07, 1.6033e-06, 1.2302e-06,\n",
      "          1.2604e-06, 9.5362e-07, 1.4783e-06, 1.1527e-06, 1.6766e-06,\n",
      "          1.3597e-06],\n",
      "         [5.8779e-04, 3.7305e-04, 3.9267e-04, 5.0944e-04, 5.8399e-04,\n",
      "          4.3373e-04, 3.4522e-04, 2.1029e-03, 4.4621e-03, 1.5779e-03,\n",
      "          2.8434e-02, 7.2694e-03, 1.1233e-03, 1.1315e-03, 1.0253e-03,\n",
      "          7.4324e-03, 1.6186e-03, 3.9215e-02, 5.8246e-02, 5.7263e-03,\n",
      "          1.6040e-02, 4.0939e-03, 1.2034e-03, 1.3110e-03, 1.3214e-03,\n",
      "          3.8154e-01, 4.9746e-04, 8.9501e-04, 9.8792e-04, 1.9011e-03,\n",
      "          6.2099e-02, 9.9649e-03, 7.0152e-02, 6.9491e-02, 1.0955e-02,\n",
      "          4.9745e-02, 9.4967e-04, 2.1215e-03, 4.1877e-03, 3.3435e-02,\n",
      "          5.9738e-02, 1.2840e-02, 2.7235e-02, 1.0903e-02, 4.7977e-04,\n",
      "          7.2481e-04, 4.0732e-04, 5.9095e-04, 5.8067e-04, 3.0667e-04,\n",
      "          7.1201e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8472, 0.0019, 0.1509]])\n",
      "Player 0 Prediction: tensor([[[7.2595e-06, 9.8645e-06, 1.0739e-05, 8.8500e-06, 1.2341e-05,\n",
      "          1.4126e-05, 1.0609e-05, 8.9554e-04, 1.1166e-03, 1.8093e-04,\n",
      "          6.4405e-04, 1.5062e-04, 7.4767e-02, 8.1583e-02, 3.5248e-04,\n",
      "          6.8272e-06, 5.1320e-05, 2.8027e-02, 2.5206e-02, 1.8225e-04,\n",
      "          3.7170e-05, 2.9535e-05, 1.0745e-04, 2.3080e-04, 3.6340e-05,\n",
      "          3.4311e-01, 1.2299e-05, 7.0939e-05, 6.3644e-05, 5.8995e-05,\n",
      "          8.3354e-05, 1.7291e-03, 1.6389e-01, 1.2116e-01, 1.2200e-04,\n",
      "          8.2513e-06, 9.9635e-04, 8.2333e-02, 6.6261e-02, 1.1443e-04,\n",
      "          8.9152e-05, 1.3000e-03, 2.5519e-03, 2.2924e-03, 1.1697e-05,\n",
      "          7.2965e-06, 1.1796e-05, 7.8654e-06, 1.1107e-05, 1.3308e-05,\n",
      "          1.0718e-05],\n",
      "         [2.7304e-05, 4.0557e-05, 4.1530e-05, 4.2630e-05, 3.9474e-05,\n",
      "          5.1356e-05, 3.3289e-05, 6.6663e-03, 8.6771e-03, 2.8010e-03,\n",
      "          5.9539e-03, 9.7967e-04, 3.2654e-02, 3.1603e-02, 7.5631e-04,\n",
      "          1.2801e-04, 1.6528e-04, 5.4632e-03, 2.8542e-03, 6.4412e-04,\n",
      "          9.6386e-04, 1.2789e-04, 3.5833e-04, 5.0100e-04, 6.9606e-05,\n",
      "          1.2386e-01, 3.5711e-05, 4.4347e-04, 4.6599e-04, 4.1723e-04,\n",
      "          2.5069e-04, 4.2292e-03, 2.4669e-01, 3.0659e-01, 5.0348e-04,\n",
      "          7.9564e-04, 6.5472e-04, 8.0743e-02, 8.0560e-02, 3.5002e-04,\n",
      "          1.9811e-04, 6.6519e-03, 2.4283e-02, 2.0372e-02, 4.7746e-05,\n",
      "          3.9153e-05, 3.1954e-05, 4.2543e-05, 4.7732e-05, 3.4731e-05,\n",
      "          2.5045e-05],\n",
      "         [1.1762e-05, 7.1904e-06, 7.9318e-06, 7.5838e-06, 1.0538e-05,\n",
      "          1.1505e-05, 9.9152e-06, 7.0764e-06, 9.4775e-06, 1.2881e-05,\n",
      "          1.1045e-05, 9.1752e-06, 9.4474e-04, 8.4167e-04, 4.8906e-06,\n",
      "          2.6944e-04, 1.5495e-05, 5.0790e-01, 4.8660e-01, 1.2285e-05,\n",
      "          3.9624e-04, 8.3242e-06, 6.0310e-04, 1.7261e-03, 2.4187e-04,\n",
      "          9.2009e-06, 1.3089e-05, 1.6050e-05, 1.5235e-05, 1.8147e-05,\n",
      "          8.6876e-06, 8.9404e-06, 2.3769e-05, 6.1335e-06, 7.9180e-06,\n",
      "          2.2791e-05, 1.0987e-05, 5.3207e-06, 1.4604e-05, 1.0001e-05,\n",
      "          9.8024e-06, 1.1175e-05, 9.0177e-06, 1.1293e-05, 1.0347e-05,\n",
      "          1.1487e-05, 8.2096e-06, 8.2586e-06, 1.6273e-05, 7.4923e-06,\n",
      "          2.5437e-05],\n",
      "         [1.0522e-04, 6.6164e-05, 6.0580e-05, 7.8655e-05, 5.8513e-05,\n",
      "          5.4717e-05, 7.1710e-05, 1.2397e-03, 1.2873e-03, 6.0760e-04,\n",
      "          6.8245e-04, 3.9513e-04, 3.4372e-04, 3.8982e-04, 1.6674e-04,\n",
      "          3.5055e-04, 1.6105e-04, 1.3829e-01, 1.3688e-01, 4.2813e-03,\n",
      "          2.1428e-04, 3.3146e-04, 1.2782e-03, 2.9350e-03, 3.3106e-04,\n",
      "          1.1258e-01, 7.1171e-05, 1.5228e-03, 1.3663e-03, 1.8784e-04,\n",
      "          1.8736e-04, 6.6254e-03, 2.7730e-01, 2.6492e-01, 5.1486e-04,\n",
      "          9.6399e-04, 3.3222e-04, 1.3064e-03, 1.1319e-03, 6.6995e-04,\n",
      "          5.3214e-04, 9.6318e-03, 1.4957e-02, 1.3974e-02, 8.6715e-05,\n",
      "          9.7387e-05, 9.1413e-05, 8.8738e-05, 6.1428e-05, 7.4901e-05,\n",
      "          6.0274e-05]]])\n",
      "Player 1 Prediction: tensor([[0.4253, 0.5222, 0.0525, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57063\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5999/10000 (60.0%)\n",
      "    Average reward: +0.535\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4001/10000 (40.0%)\n",
      "    Average reward: -0.535\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8855 (31.4%)\n",
      "    Action 1: 16549 (58.6%)\n",
      "    Action 2: 2553 (9.0%)\n",
      "    Action 3: 281 (1.0%)\n",
      "  Player 1:\n",
      "    Action 0: 9611 (33.3%)\n",
      "    Action 1: 14397 (49.9%)\n",
      "    Action 2: 2447 (8.5%)\n",
      "    Action 3: 2370 (8.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5346.0, -5346.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.976 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.003\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5346\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2832, 0.7123, 0.0045, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9496, 0.0073, 0.0431]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50206\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6115/10000 (61.2%)\n",
      "    Average reward: -0.044\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3885/10000 (38.9%)\n",
      "    Average reward: +0.044\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22554 (84.1%)\n",
      "    Action 1: 4263 (15.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3048 (13.0%)\n",
      "    Action 1: 17528 (74.9%)\n",
      "    Action 2: 1004 (4.3%)\n",
      "    Action 3: 1809 (7.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-439.0, 439.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.632 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.695 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.663\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0439\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25003/50000 [23:44<17:39, 23.60it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 160201/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 162008/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [24:28<17:06, 23.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 166520/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 168390/2000000\n",
      "P1 SL Buffer Size:  166520\n",
      "P1 SL buffer distribution [65781. 78584. 11881. 10274.]\n",
      "P1 actions distribution [0.39503363 0.47191929 0.07134879 0.06169829]\n",
      "P2 SL Buffer Size:  168390\n",
      "P2 SL buffer distribution [56167. 85849. 11982. 14392.]\n",
      "P2 actions distribution [0.33355306 0.50982244 0.07115624 0.08546826]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5682, 0.4243, 0.0075, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[5.2634e-05, 1.3288e-04, 8.7420e-05, 7.9048e-05, 1.2672e-04,\n",
      "          1.0475e-04, 1.0615e-04, 8.3027e-04, 8.0442e-04, 4.5136e-04,\n",
      "          1.7004e-02, 1.5725e-03, 1.3820e-03, 1.5769e-03, 6.2582e-04,\n",
      "          1.8388e-02, 1.6065e-03, 4.2160e-03, 6.1015e-03, 1.8476e-03,\n",
      "          1.1116e-01, 2.3786e-03, 1.7929e-03, 2.3478e-03, 1.6493e-04,\n",
      "          2.3902e-01, 1.3383e-04, 1.9252e-03, 1.9134e-03, 1.6194e-02,\n",
      "          3.3908e-01, 4.7922e-03, 8.4014e-03, 4.9906e-03, 5.6785e-03,\n",
      "          3.4907e-02, 6.0371e-04, 3.7099e-04, 3.6933e-04, 1.9742e-02,\n",
      "          1.1998e-01, 3.9445e-03, 1.5247e-02, 7.0828e-03, 1.0159e-04,\n",
      "          9.3385e-05, 1.0399e-04, 1.0104e-04, 1.1401e-04, 1.0705e-04,\n",
      "          6.1855e-05],\n",
      "         [7.9013e-05, 1.3421e-04, 1.5020e-04, 1.8790e-04, 2.8406e-04,\n",
      "          2.4831e-04, 2.3649e-04, 2.5182e-03, 3.2333e-03, 1.3644e-03,\n",
      "          9.2625e-04, 5.0918e-04, 2.1581e-02, 2.6104e-02, 3.0392e-03,\n",
      "          3.9803e-04, 4.1481e-04, 1.8871e-02, 2.5279e-02, 7.6946e-03,\n",
      "          1.7644e-02, 1.4418e-03, 7.4507e-03, 1.0317e-02, 2.6497e-04,\n",
      "          1.8808e-01, 4.3750e-04, 5.0570e-03, 4.7443e-03, 1.6020e-03,\n",
      "          7.9079e-03, 3.7188e-02, 2.4299e-01, 2.0277e-01, 3.4844e-04,\n",
      "          4.3844e-04, 4.7728e-03, 1.5560e-02, 1.2423e-02, 2.2992e-03,\n",
      "          7.4909e-03, 2.2293e-02, 5.5255e-02, 3.6848e-02, 1.7514e-04,\n",
      "          1.5189e-04, 2.0600e-04, 1.9074e-04, 1.9368e-04, 1.5143e-04,\n",
      "          6.0321e-05],\n",
      "         [2.5086e-06, 1.6445e-06, 3.0541e-06, 3.5409e-06, 2.7018e-06,\n",
      "          6.5309e-06, 2.5554e-06, 2.5118e-06, 2.5027e-06, 2.3892e-06,\n",
      "          3.2769e-06, 1.9159e-06, 2.8691e-04, 3.2249e-04, 1.6660e-06,\n",
      "          1.5245e-03, 2.2691e-06, 1.5365e-04, 1.5223e-04, 2.3237e-06,\n",
      "          3.1108e-04, 1.8492e-06, 5.7528e-01, 4.2182e-01, 3.1723e-05,\n",
      "          6.1896e-06, 3.0851e-06, 3.4705e-06, 1.8059e-06, 2.6797e-06,\n",
      "          2.8920e-06, 3.3903e-06, 3.2635e-06, 1.4461e-06, 2.3288e-06,\n",
      "          1.3982e-06, 2.6684e-06, 2.0832e-06, 3.7282e-06, 4.5945e-06,\n",
      "          2.6717e-06, 2.7363e-06, 4.6278e-06, 3.5426e-06, 3.3052e-06,\n",
      "          3.6973e-06, 2.3989e-06, 2.1684e-06, 2.4486e-06, 2.5389e-06,\n",
      "          2.5016e-06],\n",
      "         [5.3724e-04, 4.6345e-04, 6.5362e-04, 5.2786e-04, 3.5365e-04,\n",
      "          4.8906e-04, 3.4269e-04, 1.1819e-03, 2.1338e-03, 1.5736e-03,\n",
      "          6.6282e-03, 1.4963e-03, 3.0443e-03, 3.8609e-03, 9.3111e-04,\n",
      "          2.8761e-03, 8.1163e-04, 3.2419e-02, 1.8728e-02, 6.7417e-03,\n",
      "          3.5447e-02, 2.8687e-03, 2.0506e-02, 2.5691e-02, 6.6508e-04,\n",
      "          2.4844e-01, 5.8301e-04, 4.0745e-02, 3.1711e-02, 4.7126e-03,\n",
      "          3.4013e-02, 2.7230e-02, 1.8639e-01, 5.9309e-02, 1.0563e-03,\n",
      "          6.2235e-03, 6.8037e-03, 9.8517e-03, 1.3685e-02, 1.5298e-02,\n",
      "          7.4558e-02, 9.7088e-03, 3.5978e-02, 1.9864e-02, 6.0576e-04,\n",
      "          3.1690e-04, 5.3482e-04, 4.1819e-04, 2.8984e-04, 3.9352e-04,\n",
      "          3.1535e-04]]])\n",
      "Player 0 Prediction: tensor([[0.9938, 0.0000, 0.0062, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[4.2673e-06, 1.3712e-05, 6.8105e-06, 1.2504e-05, 1.3792e-05,\n",
      "          1.0272e-05, 1.4221e-05, 1.0841e-03, 1.1138e-03, 5.3394e-05,\n",
      "          1.6082e-03, 2.1599e-05, 2.1637e-01, 2.2726e-01, 8.5407e-05,\n",
      "          3.5861e-06, 2.8334e-05, 4.0185e-02, 5.5830e-02, 3.0410e-04,\n",
      "          2.7324e-04, 3.5676e-05, 2.2495e-04, 2.3645e-04, 1.9920e-05,\n",
      "          3.8505e-01, 3.0609e-05, 1.6075e-04, 1.0994e-04, 4.7045e-05,\n",
      "          8.0677e-05, 1.1449e-03, 2.8535e-02, 3.3311e-02, 2.0965e-05,\n",
      "          4.3193e-07, 1.4677e-04, 3.0084e-03, 2.5530e-03, 6.1973e-05,\n",
      "          2.6095e-05, 4.1990e-04, 2.1301e-04, 2.0530e-04, 9.9505e-06,\n",
      "          7.6563e-06, 8.1324e-06, 8.0119e-06, 7.0667e-06, 1.3241e-05,\n",
      "          6.2455e-06],\n",
      "         [2.0994e-04, 3.7383e-04, 3.9600e-04, 4.9563e-04, 6.4089e-04,\n",
      "          6.4352e-04, 4.9818e-04, 4.2147e-03, 5.0079e-03, 1.4800e-03,\n",
      "          6.8841e-03, 1.1312e-03, 6.9865e-02, 7.1644e-02, 6.2897e-04,\n",
      "          1.2497e-03, 6.0744e-04, 3.0096e-02, 3.7489e-02, 7.8391e-03,\n",
      "          5.0861e-02, 2.8409e-03, 2.3142e-02, 2.9919e-02, 4.6590e-04,\n",
      "          3.0003e-01, 7.3847e-04, 1.2267e-02, 1.2888e-02, 3.5343e-03,\n",
      "          8.5114e-03, 2.2456e-02, 1.3042e-01, 1.2585e-01, 3.7947e-04,\n",
      "          1.8788e-04, 3.2882e-03, 6.0640e-03, 5.1088e-03, 1.5439e-03,\n",
      "          2.6523e-03, 9.2691e-03, 2.1326e-03, 1.5438e-03, 3.5764e-04,\n",
      "          3.8143e-04, 4.3679e-04, 3.2822e-04, 3.5169e-04, 4.6497e-04,\n",
      "          1.9266e-04],\n",
      "         [1.1347e-05, 7.6188e-06, 1.6994e-05, 1.3717e-05, 9.8727e-06,\n",
      "          1.3661e-05, 8.4808e-06, 8.8924e-06, 1.2725e-05, 9.0346e-06,\n",
      "          1.3474e-05, 9.5170e-06, 8.9393e-04, 1.0867e-03, 1.5951e-05,\n",
      "          7.4869e-04, 9.7155e-06, 4.9462e-01, 4.9994e-01, 1.1154e-05,\n",
      "          7.2476e-04, 1.1536e-05, 1.6677e-04, 8.8228e-04, 3.8942e-04,\n",
      "          1.3989e-05, 2.0456e-05, 7.6558e-06, 6.5128e-06, 2.0614e-05,\n",
      "          1.9926e-05, 1.7282e-05, 1.4280e-05, 1.2512e-05, 1.6735e-05,\n",
      "          1.5693e-05, 1.1177e-05, 9.0775e-06, 1.1521e-05, 1.7971e-05,\n",
      "          1.0182e-05, 1.3081e-05, 8.0392e-06, 1.4433e-05, 7.8456e-06,\n",
      "          2.0551e-05, 9.9328e-06, 1.8949e-05, 1.7128e-05, 9.6258e-06,\n",
      "          1.2660e-05],\n",
      "         [8.9898e-05, 8.3578e-05, 1.6008e-04, 1.1656e-04, 1.6360e-04,\n",
      "          1.3833e-04, 1.2069e-04, 9.5470e-04, 1.1015e-03, 5.3514e-04,\n",
      "          6.8893e-04, 3.2142e-04, 1.9571e-04, 3.8363e-04, 1.6767e-04,\n",
      "          2.5575e-04, 1.6969e-04, 1.4392e-01, 1.7432e-01, 8.7436e-03,\n",
      "          5.4859e-03, 4.1635e-04, 2.1470e-03, 2.8764e-03, 9.1333e-05,\n",
      "          4.1980e-01, 1.1489e-04, 1.0622e-03, 7.6239e-04, 4.9552e-04,\n",
      "          1.5428e-03, 1.8994e-02, 9.5083e-02, 8.3645e-02, 1.3056e-04,\n",
      "          4.8889e-04, 4.2992e-03, 1.2553e-02, 3.4165e-03, 1.1535e-03,\n",
      "          2.1108e-03, 5.6700e-03, 2.5501e-03, 1.9231e-03, 8.9165e-05,\n",
      "          7.6578e-05, 1.1633e-04, 7.7725e-05, 5.5422e-05, 8.1992e-05,\n",
      "          6.1776e-05]]])\n",
      "Player 0 Prediction: tensor([[0.1802, 0.4463, 0.3735, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.7269e-07, 6.5347e-07, 3.4855e-07, 8.7511e-07, 3.4513e-07,\n",
      "          5.4028e-07, 6.1339e-07, 3.9932e-01, 4.0084e-01, 2.4720e-06,\n",
      "          3.0140e-04, 1.5225e-06, 6.4207e-05, 6.7524e-05, 2.0608e-06,\n",
      "          1.9833e-05, 1.5780e-06, 2.1066e-05, 2.1977e-05, 2.7944e-06,\n",
      "          8.1163e-06, 1.8921e-06, 6.8369e-06, 6.7796e-06, 7.6555e-07,\n",
      "          1.9164e-01, 7.4878e-07, 3.8411e-06, 3.6780e-06, 2.9350e-06,\n",
      "          3.3095e-06, 1.3806e-05, 1.3790e-05, 9.1565e-06, 1.6894e-06,\n",
      "          6.8448e-07, 5.0277e-06, 2.3958e-07, 2.6560e-07, 3.3544e-06,\n",
      "          3.0243e-06, 7.8182e-06, 3.7882e-03, 3.7966e-03, 6.9039e-07,\n",
      "          4.6989e-07, 4.3721e-07, 7.1606e-07, 5.0980e-07, 5.3016e-07,\n",
      "          3.7315e-07],\n",
      "         [4.0108e-04, 4.0559e-04, 2.7807e-04, 3.9380e-04, 7.8359e-04,\n",
      "          4.8117e-04, 3.5488e-04, 1.3921e-02, 6.3698e-03, 1.1271e-03,\n",
      "          1.2198e-02, 9.5525e-04, 1.8880e-02, 2.5229e-02, 8.4652e-04,\n",
      "          5.1148e-03, 6.4934e-04, 7.8895e-02, 7.8786e-02, 7.0332e-03,\n",
      "          3.3715e-02, 2.8782e-03, 7.5035e-03, 1.6550e-02, 3.3632e-04,\n",
      "          5.1961e-01, 1.3650e-03, 2.2664e-02, 2.2637e-02, 1.4434e-03,\n",
      "          2.9912e-03, 1.1062e-02, 4.7717e-02, 4.1394e-02, 3.5637e-04,\n",
      "          4.1916e-04, 2.9812e-03, 1.3794e-03, 1.3496e-03, 1.4889e-03,\n",
      "          1.6520e-03, 1.3957e-03, 3.3467e-04, 6.2239e-04, 5.8834e-04,\n",
      "          3.6136e-04, 3.8798e-04, 5.5986e-04, 3.1848e-04, 5.1302e-04,\n",
      "          3.2210e-04],\n",
      "         [3.7830e-05, 2.5005e-05, 4.7051e-05, 3.8265e-05, 6.8982e-05,\n",
      "          7.3834e-05, 3.8940e-05, 3.3039e-05, 4.6048e-05, 2.2397e-05,\n",
      "          6.0872e-05, 7.4942e-05, 4.9792e-01, 4.8955e-01, 6.4338e-05,\n",
      "          4.5393e-03, 3.9930e-05, 1.1845e-03, 1.2297e-03, 3.5280e-05,\n",
      "          1.0385e-03, 3.9236e-05, 9.4246e-04, 1.0023e-03, 5.2811e-04,\n",
      "          5.4277e-05, 2.6672e-05, 3.7714e-05, 2.4037e-05, 3.2155e-05,\n",
      "          5.1663e-05, 9.3026e-05, 4.9999e-05, 5.4813e-05, 7.9405e-05,\n",
      "          6.4910e-05, 6.3854e-05, 4.6915e-05, 4.4999e-05, 5.3549e-05,\n",
      "          4.0338e-05, 4.8141e-05, 5.0031e-05, 8.2154e-05, 3.1084e-05,\n",
      "          7.1261e-05, 3.0969e-05, 5.4580e-05, 4.2515e-05, 3.1554e-05,\n",
      "          5.1620e-05],\n",
      "         [7.9480e-04, 9.6681e-04, 9.9728e-04, 7.6840e-04, 1.1436e-03,\n",
      "          1.0229e-03, 8.2812e-04, 1.8792e-03, 2.0099e-03, 1.7912e-03,\n",
      "          9.8793e-03, 3.3115e-03, 1.2482e-03, 2.7581e-03, 6.0574e-04,\n",
      "          1.5690e-03, 1.1573e-03, 5.9152e-02, 7.1259e-02, 1.0275e-02,\n",
      "          1.0089e-01, 2.3231e-03, 3.6381e-02, 4.3126e-02, 4.6152e-04,\n",
      "          4.4868e-01, 6.9201e-04, 1.4559e-02, 1.4813e-02, 5.3745e-03,\n",
      "          1.4442e-02, 3.1697e-02, 2.7243e-02, 1.8618e-02, 7.0234e-04,\n",
      "          3.4022e-03, 8.7351e-03, 1.0443e-02, 7.7221e-03, 6.5303e-03,\n",
      "          1.3419e-02, 8.9437e-03, 1.7842e-03, 1.5280e-03, 7.5016e-04,\n",
      "          3.7801e-04, 8.9028e-04, 4.9656e-04, 4.8890e-04, 6.4706e-04,\n",
      "          4.2451e-04]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [24:40<17:06, 23.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54550\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5031/10000 (50.3%)\n",
      "    Average reward: -0.557\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4969/10000 (49.7%)\n",
      "    Average reward: +0.557\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9843 (36.5%)\n",
      "    Action 1: 13452 (49.9%)\n",
      "    Action 2: 2211 (8.2%)\n",
      "    Action 3: 1440 (5.3%)\n",
      "  Player 1:\n",
      "    Action 0: 8292 (30.0%)\n",
      "    Action 1: 14745 (53.4%)\n",
      "    Action 2: 2892 (10.5%)\n",
      "    Action 3: 1675 (6.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5574.5, 5574.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.004 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.018\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5575\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.4781, 0.5208, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2830, 0.7159, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9087, 0.0036, 0.0877]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53010\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5109/10000 (51.1%)\n",
      "    Average reward: +0.052\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4891/10000 (48.9%)\n",
      "    Average reward: -0.052\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4420 (17.3%)\n",
      "    Action 1: 15209 (59.5%)\n",
      "    Action 2: 2401 (9.4%)\n",
      "    Action 3: 3542 (13.9%)\n",
      "  Player 1:\n",
      "    Action 0: 20244 (73.8%)\n",
      "    Action 1: 7194 (26.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [524.5, -524.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.884 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.830 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.857\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0524\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[5.2284e-05, 9.0278e-05, 6.9918e-05, 7.6520e-05, 8.5890e-05,\n",
      "          1.0278e-04, 7.5069e-05, 1.2580e-03, 2.3871e-03, 1.5341e-03,\n",
      "          1.1135e-02, 5.8797e-03, 1.0582e-02, 1.4080e-02, 3.6538e-03,\n",
      "          9.9661e-04, 7.4051e-04, 4.0262e-03, 4.2560e-03, 8.2543e-04,\n",
      "          2.1749e-03, 8.5316e-04, 5.1897e-04, 6.0754e-04, 2.2680e-04,\n",
      "          8.6371e-02, 1.0406e-04, 2.6001e-03, 2.5437e-03, 2.2673e-03,\n",
      "          1.1805e-02, 3.2962e-02, 2.6396e-01, 2.5016e-01, 8.4627e-03,\n",
      "          1.4736e-02, 1.3154e-02, 7.9155e-02, 6.2186e-02, 9.3475e-03,\n",
      "          1.6942e-02, 1.2709e-02, 4.0485e-02, 2.3248e-02, 8.8259e-05,\n",
      "          7.8787e-05, 7.5929e-05, 6.5853e-05, 8.4287e-05, 5.8725e-05,\n",
      "          5.7245e-05],\n",
      "         [5.0387e-05, 6.4274e-05, 8.6507e-05, 8.3085e-05, 7.1303e-05,\n",
      "          7.5031e-05, 8.5303e-05, 1.1020e-03, 2.2255e-03, 1.2568e-03,\n",
      "          1.0370e-02, 7.8995e-03, 8.8552e-03, 1.0404e-02, 2.2124e-03,\n",
      "          4.3385e-03, 1.2995e-03, 8.9970e-03, 8.2546e-03, 1.2343e-03,\n",
      "          3.2513e-03, 5.0510e-04, 4.9497e-04, 1.1198e-03, 1.6975e-04,\n",
      "          1.3153e-01, 1.0274e-04, 2.3578e-03, 2.3632e-03, 9.9644e-03,\n",
      "          7.8064e-02, 2.7130e-02, 2.2076e-01, 2.1348e-01, 1.3868e-02,\n",
      "          3.4902e-02, 5.5599e-03, 3.6577e-02, 2.9725e-02, 1.0727e-02,\n",
      "          1.3323e-02, 1.4415e-02, 4.9936e-02, 3.0120e-02, 8.2752e-05,\n",
      "          9.7291e-05, 7.9017e-05, 8.4444e-05, 9.1039e-05, 1.1608e-04,\n",
      "          4.6567e-05],\n",
      "         [1.0132e-05, 1.2057e-05, 1.3415e-05, 1.0999e-05, 1.3306e-05,\n",
      "          1.6360e-05, 1.8572e-05, 8.3007e-06, 1.6241e-05, 1.5057e-05,\n",
      "          1.5844e-05, 1.4601e-05, 1.1756e-03, 1.2319e-03, 1.1961e-05,\n",
      "          8.9360e-04, 1.9410e-05, 3.1180e-03, 3.8225e-03, 9.1443e-06,\n",
      "          1.4236e-03, 7.9910e-06, 4.6977e-03, 1.9656e-01, 7.8655e-01,\n",
      "          1.1332e-05, 1.3817e-05, 1.5923e-05, 1.1787e-05, 1.1235e-05,\n",
      "          8.8515e-06, 9.3331e-06, 1.8628e-05, 7.9818e-06, 1.3785e-05,\n",
      "          1.3977e-05, 1.0389e-05, 7.3288e-06, 1.2099e-05, 1.4658e-05,\n",
      "          1.4035e-05, 1.0268e-05, 7.6011e-06, 1.4891e-05, 1.1047e-05,\n",
      "          8.9935e-06, 1.0697e-05, 1.3906e-05, 1.3397e-05, 1.1351e-05,\n",
      "          1.5908e-05],\n",
      "         [2.7635e-04, 2.7108e-04, 3.1194e-04, 2.6202e-04, 3.6202e-04,\n",
      "          2.8371e-04, 2.1917e-04, 9.7617e-04, 1.4673e-03, 1.1831e-03,\n",
      "          5.9857e-03, 2.7127e-03, 2.4308e-03, 2.8663e-03, 6.7151e-04,\n",
      "          2.4929e-03, 7.6381e-04, 1.8373e-02, 1.7073e-02, 4.5875e-03,\n",
      "          8.1697e-04, 1.4876e-03, 1.8835e-02, 1.9039e-02, 3.0148e-03,\n",
      "          7.1285e-02, 3.5596e-04, 1.2480e-02, 1.1491e-02, 7.7328e-04,\n",
      "          6.1309e-03, 1.0876e-02, 3.2277e-01, 3.4859e-01, 4.7927e-03,\n",
      "          1.6794e-02, 2.1812e-03, 8.7376e-03, 1.0317e-02, 3.6774e-03,\n",
      "          9.7648e-03, 9.2123e-03, 2.6260e-02, 1.4474e-02, 2.7815e-04,\n",
      "          3.5586e-04, 3.0541e-04, 4.7617e-04, 2.8878e-04, 2.8278e-04,\n",
      "          2.8454e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9372, 0.0016, 0.0612]])\n",
      "Player 0 Prediction: tensor([[[6.9664e-05, 8.0370e-05, 7.2025e-05, 9.0198e-05, 1.2124e-04,\n",
      "          7.4530e-05, 8.8900e-05, 5.0365e-04, 6.7498e-04, 4.5442e-04,\n",
      "          1.3499e-01, 2.1440e-02, 6.8948e-04, 4.8854e-04, 4.8671e-04,\n",
      "          6.8875e-03, 1.5515e-03, 4.7362e-03, 6.3468e-03, 6.0691e-04,\n",
      "          1.8249e-02, 1.7896e-03, 1.0801e-03, 1.3452e-03, 2.6800e-04,\n",
      "          2.2010e-01, 7.9526e-05, 2.9207e-04, 3.3596e-04, 4.6631e-03,\n",
      "          9.0084e-02, 2.4149e-03, 5.5128e-03, 9.3651e-03, 2.7815e-02,\n",
      "          1.6241e-01, 6.2920e-04, 1.4203e-03, 4.8991e-03, 4.7049e-02,\n",
      "          2.1463e-01, 1.2185e-03, 1.9679e-03, 1.2662e-03, 1.1508e-04,\n",
      "          1.0025e-04, 1.1766e-04, 7.6394e-05, 1.0173e-04, 7.6546e-05,\n",
      "          7.7319e-05],\n",
      "         [1.3519e-05, 1.8798e-05, 2.0356e-05, 2.2729e-05, 2.1380e-05,\n",
      "          2.1934e-05, 2.0886e-05, 1.3063e-03, 2.0078e-03, 9.2374e-04,\n",
      "          9.2255e-04, 6.0910e-04, 2.1575e-02, 2.6129e-02, 2.4021e-03,\n",
      "          6.0647e-05, 6.7074e-05, 2.5340e-03, 1.5387e-03, 2.9323e-04,\n",
      "          9.7768e-05, 4.5536e-05, 1.1821e-04, 1.7371e-04, 4.0465e-05,\n",
      "          7.5663e-02, 2.4331e-05, 1.7780e-04, 1.4085e-04, 1.9086e-04,\n",
      "          1.7984e-03, 2.2319e-02, 2.8241e-01, 3.1139e-01, 1.1853e-04,\n",
      "          7.5567e-05, 8.9115e-03, 9.2390e-02, 7.9956e-02, 2.0064e-04,\n",
      "          1.8870e-04, 9.5713e-03, 3.2037e-02, 2.1304e-02, 2.7784e-05,\n",
      "          2.2588e-05, 2.1227e-05, 1.6911e-05, 2.4050e-05, 2.3080e-05,\n",
      "          1.5155e-05],\n",
      "         [1.8380e-05, 9.7845e-06, 1.7755e-05, 1.7018e-05, 1.5672e-05,\n",
      "          1.9382e-05, 1.4692e-05, 7.7459e-06, 1.8421e-05, 1.4653e-05,\n",
      "          1.1088e-05, 1.2380e-05, 8.8945e-04, 1.1401e-03, 8.4176e-06,\n",
      "          2.5650e-03, 1.5373e-05, 1.3533e-03, 2.3600e-03, 1.2597e-05,\n",
      "          6.0708e-04, 8.9763e-06, 5.3867e-01, 4.5171e-01, 6.1195e-05,\n",
      "          1.9783e-05, 1.8125e-05, 3.7600e-05, 1.8722e-05, 1.4354e-05,\n",
      "          1.5992e-05, 1.7698e-05, 2.1876e-05, 9.6476e-06, 9.2355e-06,\n",
      "          1.6485e-05, 1.6162e-05, 1.3350e-05, 1.8712e-05, 1.2075e-05,\n",
      "          2.2788e-05, 1.8322e-05, 1.2599e-05, 1.5093e-05, 1.2644e-05,\n",
      "          1.0617e-05, 9.6793e-06, 1.5235e-05, 1.4052e-05, 1.4803e-05,\n",
      "          2.3381e-05],\n",
      "         [8.1783e-04, 6.3676e-04, 9.2273e-04, 8.1466e-04, 6.3614e-04,\n",
      "          5.7513e-04, 7.5821e-04, 3.9014e-03, 3.1016e-03, 1.9377e-03,\n",
      "          3.9103e-02, 8.3553e-03, 6.1407e-03, 5.8083e-03, 1.3178e-03,\n",
      "          7.2524e-03, 1.2718e-03, 3.2723e-02, 3.3886e-02, 8.9893e-03,\n",
      "          2.8641e-03, 3.8656e-03, 2.4671e-02, 3.7654e-02, 2.4844e-03,\n",
      "          1.1757e-01, 6.4464e-04, 2.6145e-02, 1.7925e-02, 2.0257e-03,\n",
      "          2.9549e-02, 2.4400e-02, 1.7995e-01, 1.6588e-01, 1.9831e-02,\n",
      "          4.0529e-02, 5.5892e-03, 1.8991e-02, 1.4853e-02, 1.7995e-02,\n",
      "          3.6414e-02, 1.3359e-02, 1.7310e-02, 1.5719e-02, 6.9272e-04,\n",
      "          6.0261e-04, 7.5902e-04, 7.8821e-04, 7.7851e-04, 7.5762e-04,\n",
      "          4.5331e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9971, 0.0000, 0.0029, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[5.5246e-06, 8.4010e-06, 8.2553e-06, 6.9645e-06, 9.9708e-06,\n",
      "          1.1989e-05, 8.0221e-06, 8.4868e-04, 9.8635e-04, 1.2944e-04,\n",
      "          9.4977e-04, 2.1259e-04, 7.4733e-02, 8.1596e-02, 1.9690e-04,\n",
      "          4.2408e-06, 5.3377e-05, 3.9229e-02, 3.3879e-02, 1.2314e-04,\n",
      "          2.8966e-05, 2.3582e-05, 9.7177e-05, 1.9509e-04, 3.6723e-05,\n",
      "          2.4672e-01, 9.4771e-06, 5.1537e-05, 4.9195e-05, 5.0090e-05,\n",
      "          6.0751e-05, 2.2449e-03, 2.0350e-01, 1.5261e-01, 1.3140e-04,\n",
      "          7.9270e-06, 9.0273e-04, 8.6228e-02, 6.8325e-02, 9.1490e-05,\n",
      "          7.5894e-05, 1.1107e-03, 2.3411e-03, 2.0514e-03, 9.3454e-06,\n",
      "          5.8876e-06, 9.8006e-06, 6.0626e-06, 9.5406e-06, 1.0562e-05,\n",
      "          8.0539e-06],\n",
      "         [9.5358e-06, 1.4630e-05, 1.5126e-05, 1.6746e-05, 1.4374e-05,\n",
      "          1.7652e-05, 1.1819e-05, 1.8258e-03, 2.4367e-03, 8.6536e-04,\n",
      "          1.2093e-03, 4.7582e-04, 2.7747e-02, 2.8250e-02, 3.7671e-04,\n",
      "          6.6038e-05, 5.8124e-05, 5.7946e-03, 2.5375e-03, 2.5155e-04,\n",
      "          1.6041e-04, 4.0200e-05, 1.2580e-04, 1.9286e-04, 2.4001e-05,\n",
      "          9.6186e-02, 1.2761e-05, 1.4976e-04, 1.5678e-04, 1.2323e-04,\n",
      "          7.1245e-05, 7.1375e-03, 2.7180e-01, 3.6531e-01, 5.1371e-05,\n",
      "          6.9076e-05, 4.4308e-04, 8.0415e-02, 8.0467e-02, 5.5501e-05,\n",
      "          2.3837e-05, 4.0562e-03, 1.1416e-02, 9.4181e-03, 1.7175e-05,\n",
      "          1.5002e-05, 1.1936e-05, 1.6637e-05, 1.6996e-05, 1.2677e-05,\n",
      "          9.1558e-06],\n",
      "         [7.9854e-06, 4.8373e-06, 5.6965e-06, 5.6920e-06, 7.0469e-06,\n",
      "          8.1786e-06, 6.7123e-06, 4.9520e-06, 6.3443e-06, 9.0602e-06,\n",
      "          7.6857e-06, 5.8674e-06, 8.5913e-04, 7.1187e-04, 3.3560e-06,\n",
      "          1.8184e-04, 1.0595e-05, 4.9764e-01, 4.9768e-01, 8.6153e-06,\n",
      "          2.4225e-04, 5.4983e-06, 5.0520e-04, 1.6690e-03, 1.8283e-04,\n",
      "          6.1895e-06, 9.2285e-06, 1.1657e-05, 9.6555e-06, 1.1825e-05,\n",
      "          5.7431e-06, 6.1918e-06, 1.6211e-05, 4.2640e-06, 5.4773e-06,\n",
      "          1.6354e-05, 8.2617e-06, 3.6732e-06, 1.0242e-05, 6.6855e-06,\n",
      "          6.5406e-06, 7.6750e-06, 5.7480e-06, 7.2320e-06, 6.9165e-06,\n",
      "          7.5199e-06, 5.6323e-06, 5.2720e-06, 1.1162e-05, 5.2374e-06,\n",
      "          1.9102e-05],\n",
      "         [8.1091e-05, 4.5795e-05, 4.2519e-05, 5.5708e-05, 3.8747e-05,\n",
      "          4.0974e-05, 4.8810e-05, 9.3733e-04, 9.3128e-04, 4.3335e-04,\n",
      "          5.8950e-04, 3.3496e-04, 2.5295e-04, 2.9138e-04, 1.1298e-04,\n",
      "          2.7359e-04, 1.1015e-04, 1.0648e-01, 1.0227e-01, 3.3986e-03,\n",
      "          1.1142e-04, 2.4871e-04, 9.3402e-04, 2.3907e-03, 2.6088e-04,\n",
      "          1.2245e-01, 5.0791e-05, 1.0507e-03, 8.7828e-04, 1.3939e-04,\n",
      "          1.3498e-04, 5.1764e-03, 3.1637e-01, 2.9673e-01, 5.2371e-04,\n",
      "          7.0694e-04, 2.6121e-04, 1.5330e-03, 1.2317e-03, 4.3067e-04,\n",
      "          3.8336e-04, 7.3765e-03, 1.1879e-02, 1.1589e-02, 6.4475e-05,\n",
      "          6.4609e-05, 6.5048e-05, 6.4035e-05, 4.2036e-05, 5.0384e-05,\n",
      "          3.9827e-05]]])\n",
      "Player 1 Prediction: tensor([[0.2800, 0.3241, 0.3959, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56655\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6083/10000 (60.8%)\n",
      "    Average reward: +0.462\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3917/10000 (39.2%)\n",
      "    Average reward: -0.462\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9117 (33.0%)\n",
      "    Action 1: 15888 (57.6%)\n",
      "    Action 2: 2206 (8.0%)\n",
      "    Action 3: 379 (1.4%)\n",
      "  Player 1:\n",
      "    Action 0: 9235 (31.8%)\n",
      "    Action 1: 14864 (51.1%)\n",
      "    Action 2: 2693 (9.3%)\n",
      "    Action 3: 2273 (7.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4617.0, -4617.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.986 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.003\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4617\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2779, 0.7132, 0.0089, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.4374, 0.5590, 0.0036, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2181, 0.0227, 0.7592]])\n",
      "Player 1 Prediction: tensor([[0.2277, 0.1662, 0.6062, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49960\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6118/10000 (61.2%)\n",
      "    Average reward: -0.059\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3882/10000 (38.8%)\n",
      "    Average reward: +0.059\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22680 (84.9%)\n",
      "    Action 1: 4036 (15.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2881 (12.4%)\n",
      "    Action 1: 17668 (76.0%)\n",
      "    Action 2: 979 (4.2%)\n",
      "    Action 3: 1716 (7.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-595.0, 595.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.613 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.674 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.643\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0595\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27003/50000 [25:51<19:29, 19.66it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 172798/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 174791/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27998/50000 [26:34<15:44, 23.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 179188/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 181043/2000000\n",
      "P1 SL Buffer Size:  179188\n",
      "P1 SL buffer distribution [69875. 85800. 13121. 10392.]\n",
      "P1 actions distribution [0.38995357 0.47882671 0.07322477 0.05799496]\n",
      "P2 SL Buffer Size:  181043\n",
      "P2 SL buffer distribution [59721. 92761. 13400. 15161.]\n",
      "P2 actions distribution [0.32987191 0.51236999 0.07401557 0.08374254]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[6.5435e-05, 1.6684e-04, 1.3133e-04, 1.8115e-04, 1.8488e-04,\n",
      "          2.1820e-04, 2.2966e-04, 2.2591e-03, 2.6597e-03, 1.2782e-03,\n",
      "          9.8293e-04, 6.0618e-04, 1.0682e-02, 1.4436e-02, 3.0814e-03,\n",
      "          1.1904e-03, 6.0820e-04, 1.8254e-02, 2.5245e-02, 7.2471e-03,\n",
      "          1.6464e-02, 1.7550e-03, 7.0899e-03, 1.0346e-02, 3.5419e-04,\n",
      "          1.4918e-01, 3.3717e-04, 9.0055e-03, 6.9673e-03, 3.6836e-03,\n",
      "          1.6085e-02, 5.7057e-02, 2.5835e-01, 1.9543e-01, 1.0602e-03,\n",
      "          1.8809e-03, 7.1100e-03, 1.2019e-02, 8.0795e-03, 3.4876e-03,\n",
      "          8.1822e-03, 2.8226e-02, 7.1928e-02, 3.5228e-02, 1.6014e-04,\n",
      "          1.7631e-04, 1.6632e-04, 1.4710e-04, 1.4340e-04, 1.1544e-04,\n",
      "          6.7506e-05],\n",
      "         [8.0494e-05, 1.4424e-04, 1.6987e-04, 1.6086e-04, 2.3223e-04,\n",
      "          2.2514e-04, 2.2592e-04, 1.2590e-03, 1.5314e-03, 7.3535e-04,\n",
      "          1.8088e-03, 1.1229e-03, 3.1128e-03, 4.6434e-03, 1.4311e-03,\n",
      "          1.3043e-02, 2.0037e-03, 2.3747e-02, 3.9444e-02, 9.0175e-03,\n",
      "          1.4084e-02, 1.8847e-03, 4.8561e-03, 6.3136e-03, 3.8510e-04,\n",
      "          1.0804e-01, 4.0447e-04, 6.6718e-03, 6.2068e-03, 1.2198e-02,\n",
      "          1.8713e-01, 3.6777e-02, 1.6909e-01, 1.2682e-01, 4.1011e-03,\n",
      "          2.7406e-02, 5.0980e-03, 8.5268e-03, 4.6122e-03, 8.7814e-03,\n",
      "          2.8426e-02, 2.3519e-02, 6.5516e-02, 3.7919e-02, 1.6102e-04,\n",
      "          1.5346e-04, 1.9682e-04, 1.6848e-04, 1.6588e-04, 1.6167e-04,\n",
      "          8.8153e-05],\n",
      "         [3.5587e-06, 3.6893e-06, 4.8524e-06, 6.5121e-06, 4.3330e-06,\n",
      "          8.2783e-06, 4.0663e-06, 7.2441e-06, 7.3722e-06, 3.5639e-06,\n",
      "          2.2171e-06, 4.0815e-06, 8.1615e-04, 6.7442e-04, 5.0571e-06,\n",
      "          5.6684e-04, 3.5799e-06, 6.3510e-04, 5.8911e-04, 6.2068e-06,\n",
      "          3.9399e-04, 5.1166e-06, 5.0684e-03, 2.3088e-01, 7.6017e-01,\n",
      "          6.4927e-06, 5.9797e-06, 6.5294e-06, 3.4492e-06, 6.9253e-06,\n",
      "          5.5894e-06, 5.4796e-06, 5.0370e-06, 3.3958e-06, 4.2912e-06,\n",
      "          4.6084e-06, 5.9144e-06, 3.7104e-06, 3.7305e-06, 5.7451e-06,\n",
      "          4.1978e-06, 4.7054e-06, 3.6930e-06, 4.3691e-06, 3.0362e-06,\n",
      "          7.0162e-06, 4.5873e-06, 4.8758e-06, 3.6505e-06, 3.9506e-06,\n",
      "          4.0460e-06],\n",
      "         [1.5415e-04, 3.5981e-04, 4.4288e-04, 2.8251e-04, 4.6709e-04,\n",
      "          2.6436e-04, 2.9512e-04, 9.0024e-04, 1.1502e-03, 8.8552e-04,\n",
      "          2.2033e-03, 9.1785e-04, 1.2779e-03, 1.8547e-03, 6.1542e-04,\n",
      "          1.7493e-03, 4.1332e-04, 3.4902e-02, 5.8083e-02, 8.0529e-03,\n",
      "          1.8754e-03, 1.1702e-03, 1.0892e-02, 1.1089e-02, 2.7579e-04,\n",
      "          2.4022e-01, 3.3412e-04, 1.7930e-02, 1.4735e-02, 2.6537e-03,\n",
      "          1.3866e-02, 6.4588e-02, 2.2197e-01, 1.3407e-01, 5.9571e-04,\n",
      "          3.8949e-03, 5.3076e-03, 1.0677e-02, 1.1067e-02, 5.0737e-03,\n",
      "          9.1376e-03, 2.8495e-02, 4.6900e-02, 2.5926e-02, 3.5833e-04,\n",
      "          2.6239e-04, 2.6335e-04, 3.3795e-04, 2.1045e-04, 3.3021e-04,\n",
      "          2.2079e-04]]])\n",
      "Player 0 Prediction: tensor([[0.7840, 0.2117, 0.0043, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.6481e-05, 2.2660e-05, 2.4668e-05, 2.9014e-05, 3.5108e-05,\n",
      "          3.0200e-05, 3.1431e-05, 1.7402e-03, 1.4733e-03, 2.3082e-04,\n",
      "          4.1038e-03, 2.9881e-04, 4.8885e-04, 6.7489e-04, 1.5296e-04,\n",
      "          1.1629e-01, 1.4679e-04, 8.5514e-05, 3.2217e-04, 1.4172e-03,\n",
      "          9.6299e-03, 3.2709e-04, 5.8047e-04, 1.4045e-03, 3.9887e-05,\n",
      "          5.6939e-01, 2.3547e-05, 5.7638e-04, 4.4903e-04, 1.6954e-03,\n",
      "          2.9598e-02, 3.0693e-03, 4.3307e-04, 2.3120e-04, 5.8478e-04,\n",
      "          1.3879e-01, 1.2163e-03, 6.5629e-04, 5.0055e-04, 2.6843e-03,\n",
      "          3.1784e-02, 7.1421e-03, 4.5501e-02, 2.5919e-02, 2.4488e-05,\n",
      "          3.4094e-05, 3.1414e-05, 2.6595e-05, 2.6541e-05, 1.6204e-05,\n",
      "          1.0022e-05],\n",
      "         [6.4139e-05, 9.5508e-05, 7.6055e-05, 9.0966e-05, 1.7361e-04,\n",
      "          1.3652e-04, 1.6381e-04, 5.5223e-04, 3.9236e-04, 2.2907e-04,\n",
      "          1.9650e-03, 5.7251e-04, 1.1208e-03, 1.5987e-03, 3.8933e-04,\n",
      "          1.2020e-02, 2.1578e-04, 9.8568e-04, 1.0152e-03, 1.6546e-03,\n",
      "          4.7460e-03, 8.7995e-04, 3.0468e-03, 2.8713e-03, 1.6672e-04,\n",
      "          4.2114e-02, 3.3809e-04, 7.8390e-03, 7.1517e-03, 1.2071e-03,\n",
      "          7.2591e-01, 1.5173e-02, 6.0945e-02, 2.9274e-02, 1.2187e-04,\n",
      "          4.0781e-02, 1.1564e-03, 2.8102e-03, 2.5321e-03, 8.2788e-03,\n",
      "          1.5426e-02, 1.3343e-03, 7.6865e-04, 8.3732e-04, 1.0508e-04,\n",
      "          6.9899e-05, 1.6532e-04, 1.3707e-04, 1.1510e-04, 1.1724e-04,\n",
      "          6.5295e-05],\n",
      "         [1.5271e-06, 1.1645e-06, 2.6897e-06, 2.4727e-06, 1.5298e-06,\n",
      "          3.5862e-06, 2.2883e-06, 1.8371e-06, 1.4989e-06, 1.7434e-06,\n",
      "          1.5850e-06, 1.4504e-06, 7.5025e-04, 5.9114e-04, 1.2304e-06,\n",
      "          3.0986e-03, 1.4006e-06, 9.1300e-05, 7.0224e-05, 1.1640e-06,\n",
      "          9.9238e-01, 1.6904e-06, 5.4865e-04, 2.3841e-03, 1.0060e-05,\n",
      "          3.0558e-06, 8.9876e-07, 1.4477e-06, 9.6589e-07, 1.1222e-06,\n",
      "          1.7273e-06, 2.5898e-06, 2.8316e-06, 1.2352e-06, 2.9212e-06,\n",
      "          1.7168e-06, 1.3210e-06, 8.7507e-07, 1.7399e-06, 2.3716e-06,\n",
      "          2.0393e-06, 2.2224e-06, 2.4105e-06, 3.6534e-06, 1.9315e-06,\n",
      "          2.0857e-06, 9.1781e-07, 1.6105e-06, 1.9545e-06, 2.2335e-06,\n",
      "          1.5659e-06],\n",
      "         [3.0109e-05, 3.5779e-05, 4.4920e-05, 3.2668e-05, 2.7787e-05,\n",
      "          2.5477e-05, 3.5392e-05, 7.9515e-05, 6.2913e-05, 9.5798e-05,\n",
      "          1.2104e-03, 1.2439e-04, 1.1926e-04, 1.6793e-04, 4.6943e-05,\n",
      "          1.1306e-04, 4.8094e-05, 1.1984e-04, 8.5255e-05, 2.9692e-04,\n",
      "          6.0818e-02, 2.5647e-04, 3.2167e-04, 3.7249e-04, 3.7627e-05,\n",
      "          9.6622e-02, 2.9871e-05, 7.9959e-04, 5.3998e-04, 1.3454e-03,\n",
      "          7.9935e-01, 1.1741e-03, 1.8715e-03, 1.1360e-03, 9.6606e-05,\n",
      "          9.0597e-04, 3.5815e-04, 3.0518e-04, 4.2161e-04, 3.4037e-03,\n",
      "          2.5541e-02, 3.7440e-04, 4.8952e-04, 4.4872e-04, 2.7219e-05,\n",
      "          1.5934e-05, 2.5928e-05, 2.6034e-05, 2.1214e-05, 4.3614e-05,\n",
      "          1.6801e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0511, 0.1356, 0.8133, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[9.3806e-07, 1.2491e-06, 5.2962e-07, 1.1277e-06, 9.8775e-07,\n",
      "          8.6688e-07, 9.0317e-07, 1.3700e-04, 6.9940e-05, 4.7728e-06,\n",
      "          2.5116e-01, 7.3228e-06, 4.3318e-05, 4.4185e-05, 5.9307e-06,\n",
      "          8.3311e-05, 6.6193e-06, 1.7891e-05, 2.5107e-05, 8.1675e-06,\n",
      "          6.3794e-05, 8.5927e-06, 1.2653e-05, 2.0575e-05, 1.5973e-06,\n",
      "          3.3640e-01, 1.3854e-06, 8.9355e-06, 7.5801e-06, 3.0150e-05,\n",
      "          8.2777e-05, 2.6815e-05, 4.5243e-05, 3.7808e-05, 1.2420e-05,\n",
      "          8.1518e-05, 1.7575e-05, 2.6405e-05, 2.1196e-05, 3.4750e-05,\n",
      "          4.1119e-01, 3.8507e-05, 1.0438e-04, 9.3052e-05, 1.1236e-06,\n",
      "          7.2316e-07, 1.9621e-06, 9.2354e-07, 7.3685e-07, 8.5715e-07,\n",
      "          4.3457e-07],\n",
      "         [9.3476e-05, 1.2061e-04, 7.2762e-05, 1.1524e-04, 2.8062e-04,\n",
      "          1.3440e-04, 3.5650e-04, 3.4305e-03, 2.8360e-03, 4.1577e-04,\n",
      "          3.2075e-03, 5.8983e-04, 1.0947e-02, 1.0193e-02, 6.4663e-04,\n",
      "          1.2985e-04, 2.2507e-04, 3.3369e-03, 2.2156e-03, 1.2391e-03,\n",
      "          1.4995e-03, 6.2729e-04, 2.3510e-03, 1.2578e-03, 1.6664e-04,\n",
      "          3.2791e-01, 4.5696e-04, 1.3295e-02, 1.1398e-02, 1.0081e-03,\n",
      "          1.8437e-03, 1.3108e-02, 3.1809e-01, 1.8498e-01, 1.2612e-04,\n",
      "          1.1045e-03, 2.2827e-03, 2.9751e-02, 3.3468e-02, 1.4925e-03,\n",
      "          4.3838e-03, 2.3373e-03, 2.4000e-03, 3.1077e-03, 1.2527e-04,\n",
      "          6.8248e-05, 1.5534e-04, 1.9785e-04, 1.7000e-04, 1.6484e-04,\n",
      "          8.4811e-05],\n",
      "         [2.2137e-06, 1.0854e-06, 2.0446e-06, 5.1811e-06, 1.5130e-06,\n",
      "          3.0379e-06, 2.0617e-06, 1.8993e-06, 1.1429e-06, 1.2932e-06,\n",
      "          1.8727e-06, 1.5718e-06, 3.7711e-04, 3.9819e-04, 2.2132e-06,\n",
      "          9.9784e-01, 1.6605e-06, 2.0656e-04, 2.5578e-04, 1.6844e-06,\n",
      "          1.1972e-04, 1.6728e-06, 3.7201e-04, 3.1276e-04, 2.2395e-05,\n",
      "          1.4710e-06, 1.6648e-06, 1.0633e-06, 1.5986e-06, 2.0381e-06,\n",
      "          2.5524e-06, 4.4470e-06, 3.6022e-06, 1.5812e-06, 2.9973e-06,\n",
      "          2.8674e-06, 3.1455e-06, 2.0557e-06, 2.5059e-06, 4.9929e-06,\n",
      "          2.2027e-06, 2.4228e-06, 2.5106e-06, 4.4884e-06, 3.0841e-06,\n",
      "          3.4461e-06, 1.5051e-06, 1.9638e-06, 1.8673e-06, 2.0940e-06,\n",
      "          1.8072e-06],\n",
      "         [4.0465e-04, 4.0950e-04, 6.2038e-04, 5.1367e-04, 6.2165e-04,\n",
      "          6.9672e-04, 6.9518e-04, 3.0273e-03, 1.3295e-03, 1.4749e-03,\n",
      "          7.1794e-03, 1.1709e-03, 2.4529e-03, 4.7978e-03, 1.6637e-03,\n",
      "          1.2395e-03, 1.3383e-03, 2.2817e-03, 2.1013e-03, 3.8332e-03,\n",
      "          6.8880e-03, 1.8558e-03, 8.9641e-03, 8.4473e-03, 6.1489e-04,\n",
      "          1.0007e-01, 5.0521e-04, 3.6125e-02, 3.5182e-02, 6.4287e-03,\n",
      "          1.8701e-01, 5.1428e-02, 2.7626e-01, 1.3415e-01, 1.0130e-03,\n",
      "          4.9003e-03, 6.8326e-03, 6.8605e-03, 1.1574e-02, 1.1214e-02,\n",
      "          3.3775e-02, 7.1891e-03, 9.0161e-03, 1.2783e-02, 5.6695e-04,\n",
      "          2.0242e-04, 6.6925e-04, 4.0831e-04, 4.1654e-04, 5.6171e-04,\n",
      "          2.4527e-04]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27998/50000 [26:50<15:44, 23.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52969\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5192/10000 (51.9%)\n",
      "    Average reward: -0.415\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4808/10000 (48.1%)\n",
      "    Average reward: +0.415\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9272 (35.4%)\n",
      "    Action 1: 13328 (50.9%)\n",
      "    Action 2: 2325 (8.9%)\n",
      "    Action 3: 1257 (4.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7394 (27.6%)\n",
      "    Action 1: 15134 (56.5%)\n",
      "    Action 2: 2995 (11.2%)\n",
      "    Action 3: 1264 (4.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4150.0, 4150.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.978 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4150\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[5.3689e-01, 4.6296e-01, 1.5165e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[2.2301e-01, 7.7685e-01, 1.4410e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.1658, 0.8029, 0.0313, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53163\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5164/10000 (51.6%)\n",
      "    Average reward: +0.068\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4836/10000 (48.4%)\n",
      "    Average reward: -0.068\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4502 (17.6%)\n",
      "    Action 1: 15294 (59.7%)\n",
      "    Action 2: 2344 (9.1%)\n",
      "    Action 3: 3494 (13.6%)\n",
      "  Player 1:\n",
      "    Action 0: 20301 (73.7%)\n",
      "    Action 1: 7228 (26.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [681.0, -681.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.831 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.858\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0681\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2627, 0.7299, 0.0073, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[5.1386e-05, 5.5290e-05, 5.1921e-05, 6.6117e-05, 8.6305e-05,\n",
      "          5.2057e-05, 6.5725e-05, 3.0966e-04, 4.1844e-04, 3.0562e-04,\n",
      "          8.0711e-02, 1.5760e-02, 5.9030e-04, 4.1202e-04, 4.5645e-04,\n",
      "          7.4926e-03, 1.2180e-03, 3.4939e-03, 4.0974e-03, 5.0300e-04,\n",
      "          1.0440e-02, 1.0652e-03, 8.3644e-04, 1.0987e-03, 2.0312e-04,\n",
      "          2.1868e-01, 5.8085e-05, 2.1224e-04, 2.5866e-04, 3.3661e-03,\n",
      "          7.0683e-02, 2.2841e-03, 5.7820e-03, 1.0222e-02, 2.5719e-02,\n",
      "          2.4336e-01, 6.0652e-04, 1.1221e-03, 3.5195e-03, 3.6032e-02,\n",
      "          2.4457e-01, 1.0546e-03, 1.2789e-03, 8.7213e-04, 8.5153e-05,\n",
      "          7.2166e-05, 8.3002e-05, 5.5809e-05, 7.0788e-05, 5.4716e-05,\n",
      "          5.5307e-05],\n",
      "         [1.1797e-05, 1.6834e-05, 1.7827e-05, 1.9380e-05, 1.8939e-05,\n",
      "          1.8784e-05, 1.8244e-05, 1.1436e-03, 1.6582e-03, 6.5083e-04,\n",
      "          1.6906e-03, 5.5872e-04, 2.2369e-02, 2.6337e-02, 2.8869e-03,\n",
      "          4.3297e-05, 5.4635e-05, 3.2666e-03, 1.9779e-03, 3.0891e-04,\n",
      "          5.9141e-05, 3.4363e-05, 1.0596e-04, 1.6921e-04, 3.5462e-05,\n",
      "          9.6824e-02, 2.1193e-05, 1.2527e-04, 9.7320e-05, 1.4375e-04,\n",
      "          2.3551e-03, 2.6861e-02, 2.5479e-01, 2.7911e-01, 6.1592e-05,\n",
      "          1.0756e-04, 1.1194e-02, 1.0168e-01, 8.7240e-02, 1.6565e-04,\n",
      "          5.6502e-04, 1.1866e-02, 3.7495e-02, 2.5697e-02, 2.4820e-05,\n",
      "          1.9609e-05, 1.8821e-05, 1.4597e-05, 2.0976e-05, 1.9871e-05,\n",
      "          1.3479e-05],\n",
      "         [1.5057e-05, 8.2771e-06, 1.5113e-05, 1.4455e-05, 1.3072e-05,\n",
      "          1.6632e-05, 1.2560e-05, 6.4957e-06, 1.5829e-05, 1.2458e-05,\n",
      "          9.2041e-06, 1.0562e-05, 8.5710e-04, 1.1290e-03, 7.0854e-06,\n",
      "          2.7819e-03, 1.2630e-05, 1.1393e-03, 1.9955e-03, 1.0358e-05,\n",
      "          6.1522e-04, 7.6137e-06, 5.3805e-01, 4.5283e-01, 4.6002e-05,\n",
      "          1.6869e-05, 1.5202e-05, 3.2655e-05, 1.5852e-05, 1.2161e-05,\n",
      "          1.3876e-05, 1.4823e-05, 1.8127e-05, 8.0210e-06, 7.6784e-06,\n",
      "          1.3563e-05, 1.3024e-05, 1.1711e-05, 1.5781e-05, 9.6511e-06,\n",
      "          1.9617e-05, 1.5581e-05, 1.0813e-05, 1.2951e-05, 1.0730e-05,\n",
      "          8.9946e-06, 8.2514e-06, 1.3509e-05, 1.1956e-05, 1.2884e-05,\n",
      "          1.9783e-05],\n",
      "         [7.8297e-04, 6.0874e-04, 9.0150e-04, 7.7348e-04, 6.1674e-04,\n",
      "          5.4956e-04, 6.9717e-04, 3.5909e-03, 2.8328e-03, 1.8435e-03,\n",
      "          4.7758e-02, 1.0168e-02, 6.3366e-03, 6.2566e-03, 1.2966e-03,\n",
      "          7.2082e-03, 1.2979e-03, 2.6425e-02, 2.8646e-02, 8.3474e-03,\n",
      "          3.2942e-03, 3.8370e-03, 2.2636e-02, 3.4016e-02, 2.5277e-03,\n",
      "          1.0300e-01, 6.0515e-04, 2.9148e-02, 1.9839e-02, 1.9741e-03,\n",
      "          3.6336e-02, 2.6073e-02, 1.6936e-01, 1.5483e-01, 2.6572e-02,\n",
      "          5.3335e-02, 5.2059e-03, 2.0519e-02, 1.5635e-02, 1.8598e-02,\n",
      "          4.5882e-02, 1.2752e-02, 1.7025e-02, 1.5499e-02, 6.3690e-04,\n",
      "          5.9047e-04, 6.9261e-04, 7.3343e-04, 7.5868e-04, 7.1734e-04,\n",
      "          4.3021e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9943, 0.0000, 0.0057, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.6649e-06, 5.5180e-06, 5.5577e-06, 4.6926e-06, 6.5489e-06,\n",
      "          8.1054e-06, 5.5193e-06, 5.4761e-04, 6.1934e-04, 7.7666e-05,\n",
      "          5.6673e-04, 1.3175e-04, 5.5134e-02, 5.9555e-02, 1.9099e-04,\n",
      "          3.9102e-06, 3.5172e-05, 3.3434e-02, 2.7871e-02, 1.0196e-04,\n",
      "          9.7931e-06, 1.2213e-05, 6.9118e-05, 1.5812e-04, 2.7044e-05,\n",
      "          4.0819e-01, 6.5065e-06, 3.6240e-05, 3.5667e-05, 2.8770e-05,\n",
      "          3.4030e-05, 2.4013e-03, 1.5982e-01, 1.2192e-01, 6.4704e-05,\n",
      "          5.0904e-06, 8.9769e-04, 6.7735e-02, 5.6236e-02, 3.7748e-05,\n",
      "          4.6800e-05, 1.0261e-03, 1.4890e-03, 1.3656e-03, 6.2704e-06,\n",
      "          3.9216e-06, 6.5088e-06, 4.0750e-06, 6.3743e-06, 6.9689e-06,\n",
      "          5.3127e-06],\n",
      "         [7.4887e-06, 1.1385e-05, 1.1766e-05, 1.2902e-05, 1.1299e-05,\n",
      "          1.3866e-05, 9.0697e-06, 1.5301e-03, 2.0371e-03, 5.3016e-04,\n",
      "          1.3981e-03, 4.2977e-04, 2.3566e-02, 2.3948e-02, 3.9250e-04,\n",
      "          4.6996e-05, 4.0144e-05, 6.5442e-03, 3.2759e-03, 2.5548e-04,\n",
      "          7.6875e-05, 2.6674e-05, 9.3599e-05, 1.5872e-04, 1.8282e-05,\n",
      "          9.5198e-02, 1.0051e-05, 9.4518e-05, 9.9167e-05, 7.7248e-05,\n",
      "          7.4683e-05, 7.8766e-03, 2.9098e-01, 3.7688e-01, 2.2247e-05,\n",
      "          6.3804e-05, 5.0736e-04, 7.3767e-02, 7.3891e-02, 3.1779e-05,\n",
      "          3.3646e-05, 3.3202e-03, 6.8320e-03, 5.7183e-03, 1.3597e-05,\n",
      "          1.1744e-05, 9.1299e-06, 1.2826e-05, 1.3429e-05, 1.0227e-05,\n",
      "          7.1933e-06],\n",
      "         [7.9678e-06, 4.9560e-06, 5.6643e-06, 6.0564e-06, 6.9366e-06,\n",
      "          8.2859e-06, 7.0376e-06, 5.0327e-06, 6.4403e-06, 9.2937e-06,\n",
      "          7.6615e-06, 5.8672e-06, 9.5601e-04, 8.2994e-04, 3.4074e-06,\n",
      "          1.9983e-04, 1.0453e-05, 5.0410e-01, 4.9096e-01, 8.7050e-06,\n",
      "          2.5375e-04, 5.8000e-06, 4.8661e-04, 1.7190e-03, 1.5864e-04,\n",
      "          6.3725e-06, 9.1542e-06, 1.1871e-05, 9.5552e-06, 1.1919e-05,\n",
      "          6.0365e-06, 6.0757e-06, 1.6141e-05, 4.4039e-06, 5.5124e-06,\n",
      "          1.6308e-05, 7.8867e-06, 3.7477e-06, 1.0164e-05, 6.5050e-06,\n",
      "          6.5177e-06, 7.6922e-06, 5.9726e-06, 7.4247e-06, 6.9306e-06,\n",
      "          7.5057e-06, 5.9394e-06, 5.3767e-06, 1.1677e-05, 5.3163e-06,\n",
      "          1.9519e-05],\n",
      "         [9.3226e-05, 5.1885e-05, 4.8293e-05, 6.3438e-05, 4.4124e-05,\n",
      "          4.5998e-05, 5.4272e-05, 9.9564e-04, 9.7148e-04, 4.7070e-04,\n",
      "          8.0441e-04, 4.4904e-04, 3.0015e-04, 3.5812e-04, 1.2788e-04,\n",
      "          2.9656e-04, 1.2849e-04, 9.3560e-02, 8.9402e-02, 3.7182e-03,\n",
      "          1.3978e-04, 2.8223e-04, 9.4740e-04, 2.3352e-03, 3.0044e-04,\n",
      "          1.0863e-01, 5.8200e-05, 1.3328e-03, 1.1476e-03, 1.5531e-04,\n",
      "          1.5805e-04, 6.5902e-03, 3.2639e-01, 3.1673e-01, 7.4189e-04,\n",
      "          1.0397e-03, 2.9155e-04, 2.1937e-03, 1.7190e-03, 4.6358e-04,\n",
      "          4.8385e-04, 8.0060e-03, 1.4372e-02, 1.3063e-02, 6.9972e-05,\n",
      "          7.4543e-05, 7.1873e-05, 7.2866e-05, 4.8901e-05, 5.8510e-05,\n",
      "          4.4446e-05]]])\n",
      "Player 1 Prediction: tensor([[0.2114, 0.4075, 0.3811, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.6465e-06, 1.6093e-06, 1.3872e-06, 3.2585e-06, 1.3598e-06,\n",
      "          1.8916e-06, 2.2344e-06, 1.2496e-01, 1.2655e-01, 1.7496e-05,\n",
      "          1.0924e-04, 2.9147e-05, 1.1860e-04, 1.2363e-04, 4.2087e-05,\n",
      "          5.9618e-05, 9.9314e-06, 1.0842e-04, 8.4786e-05, 2.3362e-05,\n",
      "          2.6726e-06, 5.4775e-06, 1.1976e-05, 1.5983e-05, 7.3612e-06,\n",
      "          3.0964e-01, 1.8330e-06, 5.8591e-06, 7.3545e-06, 8.3584e-06,\n",
      "          1.2446e-05, 1.2966e-04, 4.1479e-04, 4.0920e-04, 3.7810e-05,\n",
      "          9.2242e-05, 1.3584e-04, 1.3905e-04, 1.2234e-04, 2.7901e-05,\n",
      "          1.1044e-05, 1.0941e-04, 2.1656e-01, 2.1983e-01, 2.1805e-06,\n",
      "          1.9418e-06, 2.0802e-06, 1.7045e-06, 1.6876e-06, 2.3837e-06,\n",
      "          2.0181e-06],\n",
      "         [2.1341e-05, 1.8646e-05, 3.2692e-05, 2.8696e-05, 2.0885e-05,\n",
      "          4.5054e-05, 2.1961e-05, 7.8724e-04, 9.3756e-04, 6.9635e-04,\n",
      "          5.2514e-03, 6.1764e-04, 6.0964e-03, 6.9158e-03, 4.8477e-04,\n",
      "          2.2200e-04, 6.3082e-05, 2.0314e-02, 9.4653e-03, 2.5608e-04,\n",
      "          1.0769e-04, 5.4173e-05, 1.7930e-04, 3.7943e-04, 2.8242e-05,\n",
      "          1.0689e-01, 3.4079e-05, 3.4465e-04, 4.0166e-04, 1.5508e-04,\n",
      "          2.5623e-04, 7.6729e-03, 2.5483e-01, 5.4705e-01, 6.5801e-05,\n",
      "          4.6437e-04, 8.2201e-04, 1.0204e-02, 1.2378e-02, 1.7695e-04,\n",
      "          3.6855e-04, 1.3528e-03, 1.6112e-03, 1.6567e-03, 2.3629e-05,\n",
      "          3.8827e-05, 2.5012e-05, 3.2197e-05, 4.7672e-05, 3.4668e-05,\n",
      "          1.4613e-05],\n",
      "         [3.1162e-05, 4.4827e-05, 3.4965e-05, 3.8025e-05, 5.2009e-05,\n",
      "          5.6793e-05, 6.3274e-05, 2.6175e-05, 5.8111e-05, 8.2189e-05,\n",
      "          4.7864e-05, 5.6083e-05, 4.9196e-01, 4.7607e-01, 2.7809e-05,\n",
      "          9.6667e-04, 6.4123e-05, 9.8986e-03, 1.1457e-02, 4.6785e-05,\n",
      "          1.6616e-03, 2.5041e-05, 1.3509e-03, 3.9229e-03, 5.5541e-04,\n",
      "          3.0432e-05, 6.4933e-05, 7.4424e-05, 4.2483e-05, 5.0040e-05,\n",
      "          5.2082e-05, 3.9989e-05, 9.9704e-05, 3.9447e-05, 3.3553e-05,\n",
      "          1.3104e-04, 3.0008e-05, 2.7192e-05, 8.9001e-05, 4.5616e-05,\n",
      "          4.2604e-05, 6.7614e-05, 4.6151e-05, 4.4411e-05, 3.6155e-05,\n",
      "          4.0689e-05, 3.2059e-05, 4.3895e-05, 6.8558e-05, 2.9113e-05,\n",
      "          9.5129e-05],\n",
      "         [8.9968e-04, 1.5084e-03, 7.5192e-04, 1.1638e-03, 7.9999e-04,\n",
      "          9.0246e-04, 1.0026e-03, 4.8888e-03, 3.9380e-03, 4.1567e-03,\n",
      "          3.2492e-02, 1.2467e-02, 3.8932e-03, 4.1611e-03, 2.5307e-03,\n",
      "          5.0820e-03, 2.6628e-03, 4.3932e-02, 7.2078e-02, 1.6115e-02,\n",
      "          8.4337e-03, 4.3327e-03, 1.9057e-02, 2.1282e-02, 3.4425e-03,\n",
      "          1.2469e-01, 5.1384e-04, 3.6345e-02, 1.9799e-02, 3.4117e-03,\n",
      "          6.3280e-03, 4.0367e-02, 1.5988e-01, 1.3804e-01, 1.5320e-02,\n",
      "          3.8802e-02, 3.7294e-03, 1.5036e-02, 1.4664e-02, 1.2914e-02,\n",
      "          3.0245e-02, 2.4495e-02, 2.0288e-02, 1.6328e-02, 9.8572e-04,\n",
      "          1.2330e-03, 9.3413e-04, 1.2236e-03, 6.9841e-04, 1.0052e-03,\n",
      "          7.4482e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56052\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5875/10000 (58.8%)\n",
      "    Average reward: +0.483\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4125/10000 (41.2%)\n",
      "    Average reward: -0.483\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8870 (31.8%)\n",
      "    Action 1: 15963 (57.2%)\n",
      "    Action 2: 2796 (10.0%)\n",
      "    Action 3: 286 (1.0%)\n",
      "  Player 1:\n",
      "    Action 0: 9035 (32.1%)\n",
      "    Action 1: 14373 (51.1%)\n",
      "    Action 2: 2608 (9.3%)\n",
      "    Action 3: 2121 (7.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4826.5, -4826.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.987 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.004\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4827\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9438, 0.0012, 0.0550]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9412, 0.0036, 0.0552]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49906\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6144/10000 (61.4%)\n",
      "    Average reward: -0.022\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3856/10000 (38.6%)\n",
      "    Average reward: +0.022\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22804 (85.3%)\n",
      "    Action 1: 3918 (14.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2794 (12.1%)\n",
      "    Action 1: 17836 (76.9%)\n",
      "    Action 2: 916 (4.0%)\n",
      "    Action 3: 1638 (7.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-218.5, 218.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.601 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.659 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.630\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0219\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29003/50000 [27:55<15:02, 23.26it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 185700/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 187262/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30000/50000 [28:39<14:21, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 191971/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 193829/2000000\n",
      "P1 SL Buffer Size:  191971\n",
      "P1 SL buffer distribution [73971. 93104. 14366. 10530.]\n",
      "P1 actions distribution [0.38532382 0.48498992 0.07483422 0.05485203]\n",
      "P2 SL Buffer Size:  193829\n",
      "P2 SL buffer distribution [63101. 99494. 14750. 16484.]\n",
      "P2 actions distribution [0.32554984 0.51330812 0.076098   0.08504403]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30000/50000 [28:51<14:21, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[1.2009e-04, 1.9049e-04, 1.9877e-04, 2.0073e-04, 1.9153e-04,\n",
      "          2.7263e-04, 2.2496e-04, 1.6667e-03, 2.7017e-03, 1.4851e-03,\n",
      "          4.5645e-04, 5.5612e-04, 3.5785e-03, 4.5903e-03, 1.6951e-03,\n",
      "          4.3023e-04, 5.4931e-04, 1.7156e-02, 2.3452e-02, 4.6753e-03,\n",
      "          2.2599e-03, 9.6974e-04, 5.8232e-03, 7.3060e-03, 3.2839e-04,\n",
      "          4.7455e-02, 5.1880e-04, 1.1571e-02, 8.3643e-03, 3.1290e-03,\n",
      "          1.3540e-02, 6.2086e-02, 3.0264e-01, 2.4677e-01, 1.1031e-03,\n",
      "          2.5576e-03, 1.6882e-02, 3.5388e-02, 2.3246e-02, 3.5220e-03,\n",
      "          1.0017e-02, 3.3421e-02, 6.0830e-02, 3.4713e-02, 1.8192e-04,\n",
      "          1.9072e-04, 1.9160e-04, 1.9241e-04, 1.8900e-04, 1.4157e-04,\n",
      "          7.2223e-05],\n",
      "         [1.1186e-04, 2.2022e-04, 2.0489e-04, 1.8626e-04, 2.6275e-04,\n",
      "          2.7803e-04, 2.6672e-04, 1.1893e-03, 1.4961e-03, 8.4428e-04,\n",
      "          1.3353e-03, 9.6719e-04, 1.0998e-03, 1.8909e-03, 7.4260e-04,\n",
      "          1.1093e-02, 2.1486e-03, 9.9372e-03, 1.3346e-02, 4.4523e-03,\n",
      "          2.6502e-03, 8.1313e-04, 3.3103e-03, 4.1897e-03, 4.4208e-04,\n",
      "          4.0886e-02, 4.4818e-04, 1.0030e-02, 8.4837e-03, 1.2535e-02,\n",
      "          2.4371e-01, 3.5901e-02, 1.5554e-01, 1.2865e-01, 8.7785e-03,\n",
      "          6.3419e-02, 1.5546e-02, 2.7195e-02, 1.6717e-02, 9.5781e-03,\n",
      "          2.6143e-02, 2.5371e-02, 6.7961e-02, 3.8284e-02, 2.1615e-04,\n",
      "          1.7490e-04, 2.2848e-04, 1.8495e-04, 2.2693e-04, 1.7870e-04,\n",
      "          1.3226e-04],\n",
      "         [1.9010e-05, 2.1011e-05, 2.8004e-05, 2.1972e-05, 1.9828e-05,\n",
      "          2.2759e-05, 1.5307e-05, 3.2542e-05, 3.2186e-05, 1.6359e-05,\n",
      "          1.1973e-05, 1.8937e-05, 2.0810e-03, 1.4836e-03, 2.7792e-05,\n",
      "          1.6914e-03, 2.2725e-05, 2.0916e-03, 2.3932e-03, 2.3279e-05,\n",
      "          5.5409e-04, 1.7683e-05, 8.2566e-03, 2.2780e-01, 7.5270e-01,\n",
      "          2.3639e-05, 3.0574e-05, 2.8408e-05, 2.1814e-05, 3.9159e-05,\n",
      "          2.3439e-05, 2.4437e-05, 2.1605e-05, 1.8215e-05, 2.2048e-05,\n",
      "          2.4028e-05, 3.0754e-05, 2.2052e-05, 2.7333e-05, 2.1382e-05,\n",
      "          1.6646e-05, 1.5614e-05, 2.2009e-05, 1.9586e-05, 1.9256e-05,\n",
      "          1.9238e-05, 2.3076e-05, 2.2339e-05, 1.6953e-05, 2.0816e-05,\n",
      "          2.0171e-05],\n",
      "         [2.6777e-04, 5.5604e-04, 5.1363e-04, 4.1041e-04, 8.6793e-04,\n",
      "          5.2439e-04, 5.7112e-04, 1.5835e-03, 1.8220e-03, 1.1917e-03,\n",
      "          1.9215e-03, 1.0282e-03, 2.9097e-03, 3.1421e-03, 7.9768e-04,\n",
      "          2.5512e-03, 8.8235e-04, 3.4159e-02, 5.4058e-02, 6.5401e-03,\n",
      "          8.0856e-04, 1.3941e-03, 1.1218e-02, 9.7840e-03, 4.8548e-04,\n",
      "          6.9391e-02, 6.8195e-04, 4.8368e-02, 3.3418e-02, 4.5958e-03,\n",
      "          2.3787e-02, 6.7175e-02, 2.1228e-01, 1.6574e-01, 1.1496e-03,\n",
      "          9.1076e-03, 9.5159e-03, 2.4226e-02, 2.2557e-02, 5.7185e-03,\n",
      "          9.0645e-03, 3.4904e-02, 7.2346e-02, 4.1912e-02, 5.3433e-04,\n",
      "          6.0939e-04, 4.8775e-04, 4.7456e-04, 7.0635e-04, 7.9614e-04,\n",
      "          4.6801e-04]]])\n",
      "Player 0 Prediction: tensor([[0.2764, 0.7228, 0.0008, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[7.4687e-05, 7.8260e-05, 1.1322e-04, 8.3786e-05, 7.8000e-05,\n",
      "          1.2898e-04, 1.2944e-04, 2.2393e-03, 2.2764e-03, 4.8944e-04,\n",
      "          1.1692e-02, 6.5755e-04, 7.9466e-04, 9.5002e-04, 2.1126e-04,\n",
      "          6.9162e-02, 3.3701e-04, 1.3970e-04, 3.5308e-04, 1.1900e-03,\n",
      "          4.4505e-03, 4.2251e-04, 9.4147e-04, 1.6788e-03, 1.0105e-04,\n",
      "          4.9627e-01, 1.1530e-04, 2.0728e-03, 1.3954e-03, 3.6171e-03,\n",
      "          8.6089e-02, 4.3732e-03, 8.0939e-04, 4.4263e-04, 1.3398e-03,\n",
      "          1.9235e-01, 4.4142e-03, 1.8412e-03, 1.5156e-03, 6.8250e-03,\n",
      "          4.8413e-02, 7.7432e-03, 2.6002e-02, 1.5040e-02, 7.3599e-05,\n",
      "          9.2703e-05, 7.9685e-05, 1.0137e-04, 1.0266e-04, 6.1610e-05,\n",
      "          4.6409e-05],\n",
      "         [1.5404e-04, 2.4479e-04, 2.0667e-04, 1.9797e-04, 3.0872e-04,\n",
      "          4.4974e-04, 3.2253e-04, 1.2249e-03, 8.4484e-04, 4.4298e-04,\n",
      "          2.3600e-03, 8.1154e-04, 4.5734e-04, 6.5284e-04, 4.9689e-04,\n",
      "          1.0324e-02, 5.4299e-04, 2.4357e-03, 2.7659e-03, 2.0693e-03,\n",
      "          2.2145e-03, 9.1423e-04, 2.5342e-03, 2.7676e-03, 3.7776e-04,\n",
      "          2.0758e-02, 5.2726e-04, 1.7820e-02, 1.3978e-02, 2.7398e-03,\n",
      "          7.3689e-01, 1.8153e-02, 4.5067e-02, 2.8463e-02, 4.2770e-04,\n",
      "          2.8806e-02, 5.6327e-03, 3.7470e-03, 4.0654e-03, 1.1169e-02,\n",
      "          1.9445e-02, 2.2441e-03, 1.2045e-03, 1.1011e-03, 2.3353e-04,\n",
      "          2.0896e-04, 2.9723e-04, 1.8907e-04, 3.6038e-04, 2.1915e-04,\n",
      "          1.3357e-04],\n",
      "         [3.4007e-05, 2.8464e-05, 4.4931e-05, 2.8604e-05, 2.4893e-05,\n",
      "          4.4659e-05, 4.0133e-05, 2.8512e-05, 3.1396e-05, 3.0722e-05,\n",
      "          3.8353e-05, 2.6012e-05, 5.4248e-03, 4.6746e-03, 2.9086e-05,\n",
      "          2.0801e-02, 3.4889e-05, 1.1257e-03, 9.4793e-04, 1.9413e-05,\n",
      "          9.5570e-01, 2.5498e-05, 1.5874e-03, 8.1774e-03, 7.6207e-05,\n",
      "          5.4870e-05, 2.3353e-05, 3.8813e-05, 2.4932e-05, 3.5309e-05,\n",
      "          3.0988e-05, 3.4016e-05, 4.5062e-05, 2.3329e-05, 5.5190e-05,\n",
      "          3.0198e-05, 3.2675e-05, 2.5785e-05, 4.6009e-05, 4.2926e-05,\n",
      "          3.1858e-05, 3.5868e-05, 7.2147e-05, 3.9484e-05, 4.5646e-05,\n",
      "          2.4433e-05, 1.8506e-05, 3.2971e-05, 4.9470e-05, 4.5962e-05,\n",
      "          3.1305e-05],\n",
      "         [1.3735e-04, 1.3441e-04, 1.0460e-04, 1.0157e-04, 1.3693e-04,\n",
      "          1.2124e-04, 1.3834e-04, 3.2593e-04, 3.6727e-04, 2.1511e-04,\n",
      "          1.8133e-03, 3.3786e-04, 5.3497e-04, 4.9396e-04, 1.2174e-04,\n",
      "          4.7268e-04, 2.3826e-04, 5.0113e-04, 4.4160e-04, 5.3616e-04,\n",
      "          3.6190e-02, 5.7995e-04, 6.2953e-04, 7.7588e-04, 1.1300e-04,\n",
      "          4.9079e-02, 1.5319e-04, 3.4310e-03, 2.2912e-03, 3.9133e-03,\n",
      "          8.2037e-01, 2.4092e-03, 2.7166e-03, 2.2591e-03, 3.2518e-04,\n",
      "          3.2413e-03, 1.8441e-03, 1.8366e-03, 1.9919e-03, 7.4865e-03,\n",
      "          4.7558e-02, 9.8120e-04, 8.7636e-04, 8.3896e-04, 1.2891e-04,\n",
      "          6.5363e-05, 1.0440e-04, 1.1840e-04, 1.4588e-04, 1.8210e-04,\n",
      "          8.8608e-05]]])\n",
      "Player 0 Prediction: tensor([[0.1093, 0.6893, 0.2014, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52724\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5301/10000 (53.0%)\n",
      "    Average reward: -0.436\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4699/10000 (47.0%)\n",
      "    Average reward: +0.436\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9155 (35.2%)\n",
      "    Action 1: 13405 (51.5%)\n",
      "    Action 2: 2104 (8.1%)\n",
      "    Action 3: 1359 (5.2%)\n",
      "  Player 1:\n",
      "    Action 0: 7144 (26.8%)\n",
      "    Action 1: 14330 (53.7%)\n",
      "    Action 2: 2976 (11.1%)\n",
      "    Action 3: 2251 (8.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4358.5, 4358.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.991 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.007\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4359\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9353e-01, 8.6061e-05, 6.3862e-03]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6528, 0.0039, 0.3433]])\n",
      "Player 0 Prediction: tensor([[0.1548, 0.8009, 0.0443, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52682\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5238/10000 (52.4%)\n",
      "    Average reward: +0.098\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4762/10000 (47.6%)\n",
      "    Average reward: -0.098\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4176 (16.5%)\n",
      "    Action 1: 15517 (61.3%)\n",
      "    Action 2: 2261 (8.9%)\n",
      "    Action 3: 3373 (13.3%)\n",
      "  Player 1:\n",
      "    Action 0: 20538 (75.1%)\n",
      "    Action 1: 6817 (24.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [979.5, -979.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.862 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.810 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.836\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0979\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[2.3771e-05, 4.2004e-05, 4.6991e-05, 4.5757e-05, 5.1245e-05,\n",
      "          4.4606e-05, 5.5898e-05, 6.6460e-04, 9.6300e-04, 4.2451e-04,\n",
      "          7.7946e-03, 4.2593e-03, 2.9755e-02, 3.8768e-02, 8.9397e-03,\n",
      "          4.2794e-03, 7.7608e-04, 1.3721e-02, 1.1101e-02, 1.5697e-03,\n",
      "          2.2935e-03, 6.9942e-04, 9.0084e-04, 9.1695e-04, 1.3095e-04,\n",
      "          1.9413e-01, 5.9376e-05, 2.5668e-03, 2.5115e-03, 1.8172e-03,\n",
      "          1.0721e-02, 3.1783e-02, 2.0516e-01, 2.2129e-01, 8.8970e-03,\n",
      "          3.0994e-02, 1.2768e-02, 5.6491e-02, 4.3510e-02, 6.6280e-03,\n",
      "          1.7664e-02, 4.9792e-03, 1.2142e-02, 7.2830e-03, 5.4198e-05,\n",
      "          7.7454e-05, 3.9116e-05, 5.3011e-05, 3.7421e-05, 4.1931e-05,\n",
      "          2.9320e-05],\n",
      "         [1.0469e-05, 1.3932e-05, 1.5292e-05, 1.4917e-05, 1.5561e-05,\n",
      "          1.3611e-05, 1.8648e-05, 8.9439e-05, 1.8390e-04, 8.9709e-05,\n",
      "          3.3212e-02, 7.7126e-03, 3.7985e-03, 2.9922e-03, 5.8516e-04,\n",
      "          8.4002e-03, 7.5516e-04, 1.6547e-03, 1.5174e-03, 5.3607e-04,\n",
      "          3.0348e-03, 1.9235e-04, 2.0623e-04, 2.5459e-04, 3.4763e-05,\n",
      "          1.0023e-01, 2.2582e-05, 6.3260e-04, 6.4119e-04, 1.8815e-02,\n",
      "          3.2252e-01, 1.5957e-03, 8.7983e-03, 1.5778e-02, 4.4118e-02,\n",
      "          3.6200e-01, 3.5190e-04, 7.6243e-04, 1.4587e-03, 1.4034e-02,\n",
      "          4.1322e-02, 4.7326e-04, 5.6788e-04, 4.3260e-04, 1.7735e-05,\n",
      "          1.4727e-05, 1.3260e-05, 1.6204e-05, 1.3820e-05, 2.0832e-05,\n",
      "          8.6690e-06],\n",
      "         [3.0927e-06, 4.2914e-06, 2.8334e-06, 4.3474e-06, 4.2547e-06,\n",
      "          5.9947e-06, 4.2844e-06, 3.0511e-06, 3.4423e-06, 5.8626e-06,\n",
      "          3.3404e-06, 4.7863e-06, 5.8728e-04, 5.8027e-04, 3.0926e-06,\n",
      "          4.4056e-04, 5.2265e-06, 5.1710e-04, 5.7572e-04, 2.2970e-06,\n",
      "          5.0773e-03, 1.9581e-06, 3.1626e-03, 1.8083e-01, 8.0809e-01,\n",
      "          2.3269e-06, 2.9929e-06, 2.4973e-06, 2.7073e-06, 2.7000e-06,\n",
      "          2.9116e-06, 2.1946e-06, 4.3625e-06, 1.9958e-06, 4.9769e-06,\n",
      "          3.1144e-06, 2.1666e-06, 2.4335e-06, 2.6847e-06, 4.6924e-06,\n",
      "          5.1118e-06, 2.4148e-06, 3.5222e-06, 4.7279e-06, 2.9383e-06,\n",
      "          2.0496e-06, 2.7791e-06, 3.8734e-06, 3.4081e-06, 2.5175e-06,\n",
      "          2.6543e-06],\n",
      "         [7.6388e-05, 1.3266e-04, 1.7317e-04, 9.6696e-05, 2.4587e-04,\n",
      "          1.5639e-04, 1.3215e-04, 3.9156e-04, 5.4662e-04, 6.6645e-04,\n",
      "          4.7313e-03, 3.6598e-03, 2.0209e-03, 2.5337e-03, 4.6444e-04,\n",
      "          2.9226e-03, 4.5909e-04, 2.0820e-02, 2.8623e-02, 4.4687e-03,\n",
      "          1.8829e-03, 1.1381e-03, 1.5620e-02, 1.4362e-02, 3.6523e-03,\n",
      "          5.3209e-01, 1.8179e-04, 5.5103e-03, 6.3961e-03, 3.6658e-04,\n",
      "          3.7569e-03, 1.5188e-02, 1.3497e-01, 1.2535e-01, 6.2137e-03,\n",
      "          2.2757e-02, 8.6187e-04, 4.4522e-03, 6.5375e-03, 3.5833e-03,\n",
      "          6.4266e-03, 3.0872e-03, 7.0699e-03, 4.0734e-03, 1.3877e-04,\n",
      "          2.5075e-04, 1.4002e-04, 1.6445e-04, 1.2612e-04, 1.5090e-04,\n",
      "          1.8650e-04]]])\n",
      "Player 1 Prediction: tensor([[0.5031, 0.4949, 0.0020, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.5655e-06, 5.5065e-06, 5.6781e-06, 5.1278e-06, 5.0548e-06,\n",
      "          2.5017e-06, 5.5360e-06, 5.3718e-04, 6.3344e-04, 3.8504e-05,\n",
      "          3.8522e-02, 1.1583e-03, 8.2054e-04, 6.8524e-04, 2.6359e-04,\n",
      "          4.0625e-02, 5.8462e-05, 3.1326e-05, 3.9924e-05, 2.9467e-04,\n",
      "          1.2797e-04, 3.6710e-05, 6.9417e-05, 1.0139e-04, 2.0337e-05,\n",
      "          6.6075e-01, 6.7334e-06, 2.4402e-05, 2.4457e-05, 6.4585e-05,\n",
      "          4.2497e-04, 4.3439e-04, 9.1932e-05, 1.2548e-04, 7.0226e-04,\n",
      "          2.1469e-01, 1.7677e-04, 3.9686e-04, 8.4391e-04, 7.5257e-04,\n",
      "          3.2212e-02, 2.5826e-04, 2.5195e-03, 1.3778e-03, 5.3959e-06,\n",
      "          9.1255e-06, 4.6071e-06, 4.8249e-06, 3.8759e-06, 4.2958e-06,\n",
      "          3.5507e-06],\n",
      "         [1.3716e-05, 1.5883e-05, 2.0705e-05, 1.7727e-05, 2.1169e-05,\n",
      "          1.4345e-05, 3.1669e-05, 5.9871e-04, 1.3607e-03, 1.6837e-04,\n",
      "          4.2020e-03, 2.8066e-03, 8.9563e-04, 7.2432e-04, 5.9627e-04,\n",
      "          1.2755e-02, 9.5253e-05, 1.0099e-03, 5.3922e-04, 3.0047e-04,\n",
      "          5.2442e-04, 5.5701e-05, 3.5497e-04, 3.8215e-04, 3.1545e-05,\n",
      "          3.7992e-02, 2.8069e-05, 2.9223e-04, 3.3085e-04, 2.9444e-04,\n",
      "          7.0935e-01, 3.9048e-03, 5.7512e-02, 1.0963e-01, 3.1486e-04,\n",
      "          2.7059e-02, 4.1843e-04, 1.1755e-03, 2.3959e-03, 8.8491e-03,\n",
      "          1.1735e-02, 3.0648e-04, 3.7852e-04, 3.5079e-04, 2.8831e-05,\n",
      "          2.0092e-05, 1.6527e-05, 2.0418e-05, 1.9387e-05, 2.1573e-05,\n",
      "          1.4742e-05],\n",
      "         [6.2366e-07, 9.9288e-07, 4.7901e-07, 1.5351e-06, 8.4171e-07,\n",
      "          1.5167e-06, 5.7873e-07, 8.8317e-07, 5.4163e-07, 8.3996e-07,\n",
      "          6.4448e-07, 4.5907e-07, 3.1576e-04, 2.7470e-04, 4.9293e-07,\n",
      "          5.7106e-04, 1.0730e-06, 5.4779e-05, 4.2952e-05, 5.3528e-07,\n",
      "          9.9850e-01, 4.2857e-07, 2.1490e-05, 1.8070e-04, 4.7071e-06,\n",
      "          7.1211e-07, 4.1630e-07, 6.4292e-07, 6.8155e-07, 3.9224e-07,\n",
      "          3.8223e-07, 8.4235e-07, 4.8688e-07, 9.7207e-07, 5.7467e-07,\n",
      "          5.0791e-07, 2.7200e-07, 4.8365e-07, 4.3339e-07, 4.1656e-07,\n",
      "          1.9619e-06, 7.4985e-07, 7.6323e-07, 9.9929e-07, 4.9150e-07,\n",
      "          4.1768e-07, 6.3549e-07, 7.5438e-07, 5.5116e-07, 7.6977e-07,\n",
      "          5.8716e-07],\n",
      "         [4.2893e-05, 6.5868e-05, 9.9261e-05, 6.0870e-05, 1.2955e-04,\n",
      "          6.3233e-05, 6.5778e-05, 2.3182e-04, 2.0399e-04, 1.6118e-04,\n",
      "          5.4372e-02, 9.7862e-03, 3.4029e-04, 4.3099e-04, 1.4299e-04,\n",
      "          8.8291e-03, 2.1568e-04, 4.7855e-04, 6.4075e-04, 8.4849e-04,\n",
      "          1.7156e-02, 1.7057e-03, 4.7546e-04, 4.9118e-04, 4.6695e-04,\n",
      "          2.5773e-01, 9.0798e-05, 3.9007e-04, 5.4180e-04, 5.3096e-04,\n",
      "          1.8642e-01, 1.8548e-03, 1.8855e-03, 1.7758e-03, 2.9595e-02,\n",
      "          2.8992e-01, 1.7537e-04, 4.0252e-04, 7.0872e-04, 2.7240e-02,\n",
      "          1.0187e-01, 3.8536e-04, 2.1422e-04, 2.5389e-04, 6.6146e-05,\n",
      "          8.0794e-05, 4.9101e-05, 4.3048e-05, 9.3342e-05, 5.0333e-05,\n",
      "          1.3240e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0582, 0.1319, 0.8099, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55641\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5932/10000 (59.3%)\n",
      "    Average reward: +0.474\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4068/10000 (40.7%)\n",
      "    Average reward: -0.474\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8334 (30.2%)\n",
      "    Action 1: 16189 (58.6%)\n",
      "    Action 2: 2708 (9.8%)\n",
      "    Action 3: 377 (1.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7992 (28.5%)\n",
      "    Action 1: 14492 (51.7%)\n",
      "    Action 2: 2740 (9.8%)\n",
      "    Action 3: 2809 (10.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4737.5, -4737.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.973 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.008 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.991\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4738\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8491, 0.0052, 0.1456]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8601, 0.0307, 0.1092]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49839\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6107/10000 (61.1%)\n",
      "    Average reward: -0.039\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3893/10000 (38.9%)\n",
      "    Average reward: +0.039\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22846 (85.4%)\n",
      "    Action 1: 3891 (14.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2801 (12.1%)\n",
      "    Action 1: 17815 (77.1%)\n",
      "    Action 2: 864 (3.7%)\n",
      "    Action 3: 1622 (7.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-389.0, 389.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.599 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.658 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.628\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0389\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31004/50000 [30:01<16:33, 19.11it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 198231/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 200203/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31998/50000 [30:46<12:58, 23.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 204682/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 206737/2000000\n",
      "P1 SL Buffer Size:  204682\n",
      "P1 SL buffer distribution [ 77852. 100453.  15652.  10725.]\n",
      "P1 actions distribution [0.38035587 0.49077594 0.07646984 0.05239835]\n",
      "P2 SL Buffer Size:  206737\n",
      "P2 SL buffer distribution [ 66395. 106069.  16209.  18064.]\n",
      "P2 actions distribution [0.32115683 0.51306249 0.07840396 0.08737672]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[1.0501e-04, 1.6023e-04, 1.7179e-04, 1.7450e-04, 1.6636e-04,\n",
      "          2.3768e-04, 1.9845e-04, 1.4346e-03, 2.2697e-03, 1.1396e-03,\n",
      "          3.4513e-04, 4.5246e-04, 1.8551e-03, 2.3284e-03, 1.1273e-03,\n",
      "          3.3537e-04, 4.5349e-04, 2.1634e-02, 2.7917e-02, 4.6878e-03,\n",
      "          1.5740e-03, 7.4862e-04, 4.2447e-03, 5.1159e-03, 2.9654e-04,\n",
      "          3.2997e-02, 4.8451e-04, 1.0911e-02, 6.7711e-03, 2.8375e-03,\n",
      "          1.4487e-02, 6.5625e-02, 3.0589e-01, 2.5160e-01, 7.8787e-04,\n",
      "          2.3116e-03, 1.8639e-02, 3.3299e-02, 1.8708e-02, 2.9456e-03,\n",
      "          9.8942e-03, 3.4517e-02, 6.7876e-02, 3.9259e-02, 1.5593e-04,\n",
      "          1.6332e-04, 1.6507e-04, 1.6199e-04, 1.6308e-04, 1.2277e-04,\n",
      "          6.2000e-05],\n",
      "         [1.1026e-04, 2.0978e-04, 1.9585e-04, 1.7875e-04, 2.5805e-04,\n",
      "          2.7067e-04, 2.5921e-04, 1.1184e-03, 1.3670e-03, 7.5863e-04,\n",
      "          1.0357e-03, 8.5157e-04, 8.8285e-04, 1.5705e-03, 6.4860e-04,\n",
      "          9.7251e-03, 2.0545e-03, 9.2955e-03, 1.2085e-02, 3.7732e-03,\n",
      "          2.5383e-03, 7.8315e-04, 3.5661e-03, 4.5602e-03, 4.3279e-04,\n",
      "          3.8869e-02, 4.8767e-04, 1.3919e-02, 1.0291e-02, 1.3639e-02,\n",
      "          2.3964e-01, 4.0296e-02, 1.7059e-01, 1.4537e-01, 7.4219e-03,\n",
      "          5.6864e-02, 1.6188e-02, 2.2478e-02, 1.4345e-02, 9.4169e-03,\n",
      "          2.5510e-02, 2.3453e-02, 5.7456e-02, 3.3953e-02, 2.0848e-04,\n",
      "          1.7046e-04, 2.1990e-04, 1.7641e-04, 2.1906e-04, 1.7010e-04,\n",
      "          1.2856e-04],\n",
      "         [1.8345e-05, 2.0716e-05, 2.6842e-05, 2.1314e-05, 1.9120e-05,\n",
      "          2.2076e-05, 1.4731e-05, 3.1454e-05, 3.1509e-05, 1.5756e-05,\n",
      "          1.1592e-05, 1.8353e-05, 2.1415e-03, 1.5085e-03, 2.6990e-05,\n",
      "          1.5546e-03, 2.1827e-05, 1.8644e-03, 2.1413e-03, 2.2379e-05,\n",
      "          4.9382e-04, 1.7020e-05, 8.0637e-03, 2.2665e-01, 7.5466e-01,\n",
      "          2.3341e-05, 3.0139e-05, 2.7890e-05, 2.1386e-05, 3.7358e-05,\n",
      "          2.3045e-05, 2.3406e-05, 2.1008e-05, 1.7566e-05, 2.1382e-05,\n",
      "          2.3011e-05, 2.9910e-05, 2.1966e-05, 2.6764e-05, 2.0502e-05,\n",
      "          1.6306e-05, 1.5160e-05, 2.0928e-05, 1.8711e-05, 1.8905e-05,\n",
      "          1.8710e-05, 2.2635e-05, 2.1450e-05, 1.6411e-05, 2.0211e-05,\n",
      "          1.9590e-05],\n",
      "         [2.5209e-04, 5.2533e-04, 4.9076e-04, 3.8719e-04, 8.3912e-04,\n",
      "          4.8805e-04, 5.4995e-04, 1.4679e-03, 1.6937e-03, 1.0843e-03,\n",
      "          1.7762e-03, 9.6805e-04, 2.7006e-03, 2.9009e-03, 7.4526e-04,\n",
      "          2.3578e-03, 8.2584e-04, 3.3685e-02, 5.0554e-02, 5.7583e-03,\n",
      "          6.7393e-04, 1.3120e-03, 1.0751e-02, 9.3811e-03, 4.7025e-04,\n",
      "          5.7038e-02, 6.6412e-04, 4.9814e-02, 2.9595e-02, 4.9757e-03,\n",
      "          2.0988e-02, 7.2866e-02, 2.1433e-01, 1.6084e-01, 1.1440e-03,\n",
      "          1.0657e-02, 1.0207e-02, 2.6230e-02, 2.2754e-02, 5.1780e-03,\n",
      "          8.2085e-03, 3.7266e-02, 8.3281e-02, 4.7441e-02, 5.1220e-04,\n",
      "          5.7308e-04, 4.6930e-04, 4.4765e-04, 6.8057e-04, 7.4588e-04,\n",
      "          4.4786e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.4525e-01, 4.5585e-04, 5.4298e-02]])\n",
      "Player 1 Prediction: tensor([[[1.3254e-04, 2.3262e-04, 1.8485e-04, 1.4197e-04, 1.7770e-04,\n",
      "          2.0053e-04, 1.7969e-04, 9.5422e-04, 1.1022e-03, 6.2219e-04,\n",
      "          4.2766e-03, 1.7915e-03, 4.7577e-04, 6.0358e-04, 4.2801e-04,\n",
      "          1.3915e-02, 2.5138e-03, 4.3962e-03, 5.4872e-03, 1.6386e-03,\n",
      "          2.0180e-02, 2.2250e-03, 2.3211e-03, 2.5509e-03, 2.5087e-04,\n",
      "          7.8807e-02, 3.4585e-04, 7.2101e-03, 5.2672e-03, 2.4296e-02,\n",
      "          5.0208e-01, 9.6080e-03, 2.1947e-02, 1.1708e-02, 6.6456e-03,\n",
      "          4.8853e-02, 3.6279e-03, 2.0547e-03, 2.1332e-03, 2.8438e-02,\n",
      "          1.4118e-01, 6.6822e-03, 1.9870e-02, 1.0984e-02, 1.7370e-04,\n",
      "          1.7810e-04, 1.8315e-04, 2.1661e-04, 2.2328e-04, 2.0106e-04,\n",
      "          1.0925e-04],\n",
      "         [9.1748e-05, 1.6662e-04, 1.5855e-04, 1.7530e-04, 2.4647e-04,\n",
      "          2.8915e-04, 2.1472e-04, 1.2694e-03, 1.6790e-03, 8.5141e-04,\n",
      "          4.3827e-04, 3.6149e-04, 1.1922e-03, 1.4281e-03, 5.4721e-04,\n",
      "          2.9145e-04, 3.4115e-04, 2.1452e-02, 2.5420e-02, 3.6475e-03,\n",
      "          1.2342e-03, 5.0920e-04, 2.6865e-03, 3.4959e-03, 2.9149e-04,\n",
      "          2.3295e-02, 4.3394e-04, 7.1963e-03, 3.4494e-03, 1.9066e-03,\n",
      "          1.6841e-02, 4.8865e-02, 3.2335e-01, 2.9210e-01, 5.0790e-04,\n",
      "          1.2656e-03, 1.8215e-02, 3.0551e-02, 1.8441e-02, 1.8412e-03,\n",
      "          4.8896e-03, 2.7592e-02, 6.6256e-02, 4.3501e-02, 1.5499e-04,\n",
      "          1.6092e-04, 1.6470e-04, 1.3766e-04, 1.9058e-04, 1.3625e-04,\n",
      "          7.2396e-05],\n",
      "         [9.2806e-06, 7.4976e-06, 1.2020e-05, 8.5605e-06, 1.0777e-05,\n",
      "          1.5961e-05, 8.8826e-06, 9.5044e-06, 9.8785e-06, 9.9676e-06,\n",
      "          1.3696e-05, 7.4089e-06, 7.5605e-04, 6.6371e-04, 6.5468e-06,\n",
      "          2.5284e-03, 1.0782e-05, 3.7372e-04, 4.2946e-04, 7.0014e-06,\n",
      "          2.5786e-04, 6.2645e-06, 6.2289e-01, 3.7162e-01, 3.6943e-05,\n",
      "          1.8418e-05, 1.0890e-05, 1.2601e-05, 8.6655e-06, 1.2902e-05,\n",
      "          1.0058e-05, 1.1427e-05, 1.0748e-05, 5.8130e-06, 1.0583e-05,\n",
      "          6.2007e-06, 1.2175e-05, 8.9573e-06, 2.1198e-05, 1.3013e-05,\n",
      "          8.4598e-06, 8.1089e-06, 2.0591e-05, 1.1533e-05, 1.8841e-05,\n",
      "          7.9647e-06, 8.6163e-06, 8.2545e-06, 8.7471e-06, 9.6613e-06,\n",
      "          9.6425e-06],\n",
      "         [8.4841e-04, 7.0766e-04, 7.1424e-04, 6.0411e-04, 5.5798e-04,\n",
      "          8.9440e-04, 6.1603e-04, 1.5704e-03, 2.9212e-03, 1.7595e-03,\n",
      "          4.9445e-03, 1.5591e-03, 5.5224e-03, 5.3769e-03, 1.1566e-03,\n",
      "          3.5753e-03, 1.5926e-03, 2.5446e-02, 1.5769e-02, 4.2197e-03,\n",
      "          8.6476e-03, 3.1143e-03, 2.2511e-02, 2.7826e-02, 1.0759e-03,\n",
      "          7.4046e-02, 1.1145e-03, 1.0888e-01, 8.0442e-02, 1.0060e-02,\n",
      "          5.9125e-02, 2.7875e-02, 1.4945e-01, 6.2687e-02, 1.7591e-03,\n",
      "          2.0655e-02, 1.6141e-02, 2.0747e-02, 3.2740e-02, 1.6182e-02,\n",
      "          5.5261e-02, 1.5548e-02, 6.1951e-02, 3.6813e-02, 8.2559e-04,\n",
      "          6.2751e-04, 8.2659e-04, 5.3718e-04, 7.7522e-04, 8.2317e-04,\n",
      "          5.8433e-04]]])\n",
      "Player 0 Prediction: tensor([[0.9990, 0.0000, 0.0010, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.7691e-05, 4.0527e-05, 3.5159e-05, 3.6354e-05, 4.9682e-05,\n",
      "          4.4421e-05, 4.7267e-05, 7.9867e-04, 7.8840e-04, 1.6593e-04,\n",
      "          1.3585e-03, 9.4618e-05, 2.1506e-02, 2.5935e-02, 1.0577e-04,\n",
      "          8.0837e-06, 1.2553e-04, 4.4751e-02, 4.8968e-02, 6.1964e-04,\n",
      "          1.6032e-04, 1.2208e-04, 5.3182e-04, 4.7141e-04, 5.6222e-05,\n",
      "          2.7397e-01, 1.6134e-04, 1.0503e-03, 4.1667e-04, 3.1799e-04,\n",
      "          8.4476e-04, 5.3247e-03, 1.6240e-01, 2.1975e-01, 7.8859e-05,\n",
      "          1.4261e-05, 2.2434e-03, 1.0182e-01, 7.4383e-02, 2.9906e-04,\n",
      "          4.0120e-04, 1.9582e-03, 4.0617e-03, 3.4485e-03, 3.3005e-05,\n",
      "          2.6060e-05, 3.0606e-05, 3.5212e-05, 2.8349e-05, 4.1259e-05,\n",
      "          2.5875e-05],\n",
      "         [2.9334e-04, 5.9583e-04, 5.2850e-04, 4.6230e-04, 7.7463e-04,\n",
      "          8.2720e-04, 6.2891e-04, 2.0379e-03, 2.5619e-03, 1.5577e-03,\n",
      "          3.7858e-03, 1.6132e-03, 1.3661e-02, 1.4382e-02, 3.8528e-04,\n",
      "          7.9391e-04, 7.3146e-04, 8.7814e-03, 9.8938e-03, 2.7865e-03,\n",
      "          6.8350e-03, 1.7073e-03, 2.0815e-02, 2.5524e-02, 6.8049e-04,\n",
      "          7.3889e-02, 1.3384e-03, 2.7392e-02, 1.9527e-02, 6.8817e-03,\n",
      "          1.8430e-02, 2.2147e-02, 2.6319e-01, 2.5165e-01, 8.1619e-04,\n",
      "          3.8504e-03, 1.1240e-02, 6.3491e-02, 6.3788e-02, 4.4429e-03,\n",
      "          7.8927e-03, 1.4765e-02, 1.0768e-02, 8.5273e-03, 5.0882e-04,\n",
      "          4.8182e-04, 5.7388e-04, 4.2433e-04, 5.0609e-04, 5.4836e-04,\n",
      "          2.7851e-04],\n",
      "         [2.0208e-05, 1.3798e-05, 2.7163e-05, 1.5285e-05, 1.9076e-05,\n",
      "          1.9247e-05, 1.4590e-05, 1.6058e-05, 2.1382e-05, 1.4917e-05,\n",
      "          2.5583e-05, 1.4183e-05, 1.5343e-03, 1.6735e-03, 3.3447e-05,\n",
      "          1.4770e-03, 2.5983e-05, 5.1674e-01, 4.7551e-01, 1.8169e-05,\n",
      "          7.9283e-04, 1.5976e-05, 1.9661e-04, 8.6096e-04, 2.8742e-04,\n",
      "          2.0032e-05, 3.2549e-05, 1.4452e-05, 1.4755e-05, 3.9267e-05,\n",
      "          3.0160e-05, 3.1051e-05, 2.1370e-05, 2.5020e-05, 2.9680e-05,\n",
      "          2.9538e-05, 2.3606e-05, 1.8480e-05, 2.8280e-05, 2.5446e-05,\n",
      "          1.7280e-05, 1.5942e-05, 1.9302e-05, 2.1083e-05, 2.0353e-05,\n",
      "          2.2908e-05, 1.5203e-05, 3.7548e-05, 2.3517e-05, 2.0357e-05,\n",
      "          1.9664e-05],\n",
      "         [1.4485e-04, 1.6793e-04, 1.7347e-04, 1.6186e-04, 3.2320e-04,\n",
      "          3.0513e-04, 2.2366e-04, 1.5591e-03, 1.6467e-03, 6.7650e-04,\n",
      "          8.2339e-04, 3.4893e-04, 3.8621e-04, 7.5645e-04, 3.1255e-04,\n",
      "          3.4360e-04, 3.7721e-04, 4.5035e-02, 4.6847e-02, 2.6123e-03,\n",
      "          8.7313e-04, 6.2856e-04, 2.7159e-03, 4.0337e-03, 1.6181e-04,\n",
      "          3.5176e-02, 2.8872e-04, 6.8089e-03, 2.4182e-03, 1.6985e-03,\n",
      "          1.3238e-02, 2.1475e-02, 3.3979e-01, 3.3941e-01, 2.7647e-04,\n",
      "          2.8234e-03, 1.8808e-02, 3.5639e-02, 2.0247e-02, 1.6527e-03,\n",
      "          3.2809e-03, 1.3461e-02, 1.8088e-02, 1.2616e-02, 1.4736e-04,\n",
      "          1.9483e-04, 1.8163e-04, 1.5072e-04, 1.5325e-04, 1.9430e-04,\n",
      "          1.3675e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5054, 0.0061, 0.4885]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31998/50000 [31:01<12:58, 23.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59123\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5351/10000 (53.5%)\n",
      "    Average reward: -0.559\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4649/10000 (46.5%)\n",
      "    Average reward: +0.559\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10531 (35.9%)\n",
      "    Action 1: 15292 (52.1%)\n",
      "    Action 2: 1432 (4.9%)\n",
      "    Action 3: 2074 (7.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8692 (29.2%)\n",
      "    Action 1: 12754 (42.8%)\n",
      "    Action 2: 3034 (10.2%)\n",
      "    Action 3: 5314 (17.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5585.5, 5585.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.031\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5585\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3975, 0.6019, 0.0007, 0.0000]])\n",
      "Player 0 Prediction: tensor([[2.6696e-01, 7.3233e-01, 7.0558e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9299, 0.0019, 0.0683]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52514\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5257/10000 (52.6%)\n",
      "    Average reward: +0.168\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4743/10000 (47.4%)\n",
      "    Average reward: -0.168\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3829 (15.2%)\n",
      "    Action 1: 15705 (62.3%)\n",
      "    Action 2: 2261 (9.0%)\n",
      "    Action 3: 3397 (13.5%)\n",
      "  Player 1:\n",
      "    Action 0: 20752 (76.0%)\n",
      "    Action 1: 6570 (24.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1684.0, -1684.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.838 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.796 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.817\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1684\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.9118e-05, 3.3903e-05, 3.8233e-05, 3.6978e-05, 4.1596e-05,\n",
      "          3.6773e-05, 4.5913e-05, 5.7165e-04, 8.4067e-04, 3.4849e-04,\n",
      "          4.7126e-03, 3.0103e-03, 3.5787e-02, 4.7616e-02, 9.1865e-03,\n",
      "          2.8787e-03, 7.2876e-04, 1.6860e-02, 1.2790e-02, 1.4373e-03,\n",
      "          1.1504e-03, 4.4004e-04, 7.6033e-04, 8.0787e-04, 1.1294e-04,\n",
      "          1.3128e-01, 4.8144e-05, 2.1403e-03, 2.1690e-03, 1.2373e-03,\n",
      "          6.9936e-03, 3.6903e-02, 2.5804e-01, 2.5637e-01, 6.5025e-03,\n",
      "          1.2875e-02, 1.1708e-02, 5.3938e-02, 4.0302e-02, 3.2782e-03,\n",
      "          7.7927e-03, 6.4049e-03, 1.3456e-02, 8.0007e-03, 4.2999e-05,\n",
      "          6.0434e-05, 3.1278e-05, 4.3223e-05, 3.0413e-05, 3.4488e-05,\n",
      "          2.3830e-05],\n",
      "         [8.1506e-06, 1.0959e-05, 1.2024e-05, 1.1537e-05, 1.1960e-05,\n",
      "          1.0553e-05, 1.4530e-05, 7.6720e-05, 1.8774e-04, 7.2828e-05,\n",
      "          2.2344e-02, 6.1982e-03, 2.5520e-03, 2.0350e-03, 4.0958e-04,\n",
      "          8.2950e-03, 7.0330e-04, 1.2048e-03, 1.0872e-03, 4.2824e-04,\n",
      "          3.5924e-03, 1.6890e-04, 1.6006e-04, 1.7154e-04, 2.6429e-05,\n",
      "          7.5693e-02, 1.7327e-05, 3.4892e-04, 3.6412e-04, 1.6976e-02,\n",
      "          3.4600e-01, 1.3020e-03, 7.4043e-03, 1.2242e-02, 4.4296e-02,\n",
      "          4.0292e-01, 2.1978e-04, 4.6132e-04, 1.1884e-03, 9.5761e-03,\n",
      "          3.0207e-02, 3.3292e-04, 3.2895e-04, 2.5008e-04, 1.3408e-05,\n",
      "          1.1508e-05, 9.9666e-06, 1.2731e-05, 1.0764e-05, 1.6727e-05,\n",
      "          6.6711e-06],\n",
      "         [3.1913e-06, 4.6301e-06, 2.9293e-06, 4.3616e-06, 4.4270e-06,\n",
      "          6.1711e-06, 4.5010e-06, 3.1456e-06, 3.5468e-06, 6.0180e-06,\n",
      "          3.5413e-06, 5.0782e-06, 6.9580e-04, 6.7799e-04, 3.2833e-06,\n",
      "          3.8531e-04, 5.3215e-06, 5.4149e-04, 5.9972e-04, 2.3523e-06,\n",
      "          5.3350e-03, 2.0291e-06, 2.5327e-03, 1.5870e-01, 8.3038e-01,\n",
      "          2.3835e-06, 3.1035e-06, 2.5333e-06, 2.7682e-06, 2.7337e-06,\n",
      "          2.9716e-06, 2.1768e-06, 4.4949e-06, 2.0755e-06, 5.1902e-06,\n",
      "          3.1973e-06, 2.2222e-06, 2.4913e-06, 2.8286e-06, 4.7384e-06,\n",
      "          5.1524e-06, 2.4314e-06, 3.6161e-06, 4.9410e-06, 3.1096e-06,\n",
      "          2.1120e-06, 2.8332e-06, 3.9682e-06, 3.5727e-06, 2.5919e-06,\n",
      "          2.7371e-06],\n",
      "         [7.8217e-05, 1.3531e-04, 1.7649e-04, 9.9048e-05, 2.4250e-04,\n",
      "          1.5989e-04, 1.3447e-04, 3.8245e-04, 5.3692e-04, 6.7916e-04,\n",
      "          5.8300e-03, 3.9761e-03, 2.7448e-03, 3.6762e-03, 4.9140e-04,\n",
      "          4.3195e-03, 5.1090e-04, 2.3439e-02, 3.1597e-02, 4.3464e-03,\n",
      "          1.4837e-03, 1.0980e-03, 1.4072e-02, 1.2862e-02, 3.6093e-03,\n",
      "          5.4755e-01, 1.8389e-04, 6.7639e-03, 7.4283e-03, 3.7531e-04,\n",
      "          2.7690e-03, 1.5824e-02, 1.2259e-01, 1.1567e-01, 6.3359e-03,\n",
      "          2.2686e-02, 8.7888e-04, 5.2098e-03, 7.6286e-03, 2.9696e-03,\n",
      "          5.0154e-03, 2.7839e-03, 5.8617e-03, 3.6119e-03, 1.3990e-04,\n",
      "          2.5763e-04, 1.4543e-04, 1.7079e-04, 1.2638e-04, 1.5102e-04,\n",
      "          1.9007e-04]]])\n",
      "Player 1 Prediction: tensor([[1.1009e-01, 8.8987e-01, 3.9010e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[1.6211e-05, 1.9475e-05, 1.8546e-05, 1.9169e-05, 1.9447e-05,\n",
      "          1.6750e-05, 2.2478e-05, 8.0019e-04, 8.9738e-04, 2.3424e-04,\n",
      "          2.4927e-03, 9.2429e-04, 2.1043e-02, 2.4139e-02, 4.4211e-03,\n",
      "          2.7400e-04, 8.7795e-05, 4.2552e-02, 3.3921e-02, 3.5263e-03,\n",
      "          3.5051e-05, 2.8890e-05, 4.0460e-04, 1.3326e-03, 1.2727e-04,\n",
      "          3.1260e-01, 2.6585e-05, 3.2843e-04, 3.4341e-04, 5.6564e-05,\n",
      "          1.3178e-04, 1.8580e-02, 1.8419e-01, 1.9417e-01, 2.1305e-04,\n",
      "          7.8017e-04, 4.3407e-03, 3.3129e-02, 2.8158e-02, 8.1782e-05,\n",
      "          4.4198e-04, 1.0293e-02, 4.2173e-02, 3.2451e-02, 1.9590e-05,\n",
      "          2.8359e-05, 2.0047e-05, 1.8199e-05, 1.3963e-05, 2.1344e-05,\n",
      "          1.5473e-05],\n",
      "         [6.0251e-06, 6.8192e-06, 8.0074e-06, 1.2150e-05, 9.0453e-06,\n",
      "          7.3351e-06, 9.9034e-06, 1.9961e-06, 6.1954e-06, 5.7408e-05,\n",
      "          2.8973e-02, 1.8362e-03, 2.7674e-03, 2.0153e-03, 9.6454e-05,\n",
      "          7.2009e-03, 6.8784e-05, 2.5507e-05, 1.8111e-05, 8.8232e-05,\n",
      "          9.6339e-04, 5.9212e-05, 1.1292e-04, 1.1776e-04, 1.8348e-05,\n",
      "          2.5171e-02, 1.0219e-05, 5.2809e-05, 7.0661e-05, 3.0067e-04,\n",
      "          3.4070e-01, 5.9949e-04, 2.0972e-03, 3.8655e-03, 2.9581e-04,\n",
      "          5.3297e-01, 2.5387e-05, 2.3226e-04, 6.3567e-04, 4.4030e-03,\n",
      "          4.3942e-02, 7.5551e-05, 1.7891e-06, 1.8352e-06, 1.1438e-05,\n",
      "          9.5126e-06, 6.0814e-06, 9.0176e-06, 7.9419e-06, 1.2325e-05,\n",
      "          6.7043e-06],\n",
      "         [1.9346e-07, 1.8096e-07, 8.9660e-08, 2.2481e-07, 3.3916e-07,\n",
      "          2.6473e-07, 2.1625e-07, 2.0492e-07, 7.1741e-08, 2.2510e-07,\n",
      "          1.4405e-07, 1.4968e-07, 4.3160e-05, 2.5095e-05, 9.8322e-08,\n",
      "          8.7212e-06, 1.7316e-07, 6.8391e-05, 3.5921e-05, 1.6125e-07,\n",
      "          9.9980e-01, 1.1974e-07, 3.6758e-07, 4.6844e-06, 3.0041e-06,\n",
      "          1.5845e-07, 1.5963e-07, 1.0906e-07, 1.6770e-07, 9.7292e-08,\n",
      "          1.1224e-07, 1.2246e-07, 1.3794e-07, 1.3829e-07, 1.0862e-07,\n",
      "          1.6906e-07, 1.0458e-07, 1.0598e-07, 1.4632e-07, 8.9258e-08,\n",
      "          2.3357e-07, 1.2189e-07, 1.1682e-07, 2.2813e-07, 1.3422e-07,\n",
      "          1.2127e-07, 1.1826e-07, 1.6321e-07, 1.5180e-07, 1.9428e-07,\n",
      "          1.2743e-07],\n",
      "         [1.1122e-04, 9.9027e-05, 1.2777e-04, 1.3773e-04, 1.9661e-04,\n",
      "          1.3869e-04, 9.7823e-05, 6.2255e-04, 8.8535e-04, 4.0414e-04,\n",
      "          2.2914e-02, 6.3485e-03, 4.4673e-04, 4.7478e-04, 3.0068e-04,\n",
      "          7.6175e-03, 4.7016e-04, 1.4892e-02, 2.4219e-02, 2.8035e-03,\n",
      "          6.3222e-03, 1.5945e-03, 2.0836e-04, 2.6219e-04, 6.3517e-04,\n",
      "          7.3761e-01, 1.5897e-04, 1.8786e-04, 2.0618e-04, 5.0170e-04,\n",
      "          1.6849e-02, 4.1517e-03, 1.5935e-02, 1.5721e-02, 1.5325e-02,\n",
      "          5.6862e-02, 2.4138e-04, 8.4851e-04, 1.7421e-03, 1.1890e-02,\n",
      "          2.0910e-02, 2.1779e-03, 3.4069e-03, 1.9357e-03, 1.0478e-04,\n",
      "          2.2381e-04, 1.0990e-04, 1.3763e-04, 1.3196e-04, 8.4513e-05,\n",
      "          2.1683e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7635, 0.0009, 0.2356]])\n",
      "Player 0 Prediction: tensor([[[1.6421e-07, 3.4152e-07, 3.3995e-07, 1.4440e-07, 3.4209e-07,\n",
      "          2.2621e-07, 2.9156e-07, 1.6182e-05, 2.1620e-05, 2.4421e-06,\n",
      "          1.3412e-04, 9.1396e-06, 3.2162e-01, 3.2088e-01, 8.3831e-06,\n",
      "          6.5707e-05, 1.8548e-06, 2.7786e-05, 2.2917e-05, 3.4303e-06,\n",
      "          2.7996e-07, 1.1011e-06, 1.8030e-06, 1.2211e-06, 6.7506e-07,\n",
      "          2.6528e-01, 2.3920e-07, 1.0107e-06, 7.8914e-07, 1.3474e-06,\n",
      "          1.1590e-06, 2.9534e-05, 2.1061e-05, 1.7061e-05, 1.2073e-05,\n",
      "          1.8262e-04, 1.8950e-05, 4.5504e-02, 4.5874e-02, 5.5378e-06,\n",
      "          1.9739e-04, 9.4463e-06, 6.2113e-06, 9.1865e-06, 3.2818e-07,\n",
      "          1.4298e-07, 1.7486e-07, 2.8594e-07, 2.0203e-07, 2.1432e-07,\n",
      "          2.3112e-07],\n",
      "         [3.7964e-07, 7.2488e-07, 5.1924e-07, 4.8429e-07, 1.1225e-06,\n",
      "          5.6244e-07, 8.6934e-07, 3.0341e-01, 3.0916e-01, 1.0041e-05,\n",
      "          1.5759e-04, 3.8592e-05, 2.4260e-05, 1.9314e-05, 2.5808e-05,\n",
      "          2.7248e-06, 3.4230e-06, 7.3221e-05, 2.6204e-05, 1.4705e-05,\n",
      "          2.7677e-06, 9.8454e-07, 3.8391e-06, 5.0372e-06, 8.6696e-07,\n",
      "          2.5684e-01, 5.9663e-07, 1.1823e-05, 1.1747e-05, 5.4960e-06,\n",
      "          2.9388e-06, 2.6160e-04, 1.5904e-03, 1.8996e-03, 4.7960e-06,\n",
      "          1.3135e-05, 1.7930e-05, 3.6191e-02, 3.9486e-02, 4.9813e-06,\n",
      "          7.5595e-06, 8.4931e-05, 2.5417e-02, 2.5158e-02, 9.3308e-07,\n",
      "          4.5432e-07, 5.8722e-07, 8.7340e-07, 5.1554e-07, 4.2087e-07,\n",
      "          5.2009e-07],\n",
      "         [6.5149e-07, 6.1553e-07, 5.9409e-07, 9.5851e-07, 4.0643e-07,\n",
      "          1.9024e-06, 2.5858e-07, 7.3324e-07, 8.5452e-07, 1.1210e-06,\n",
      "          1.1751e-06, 6.6789e-07, 7.2729e-05, 9.7025e-05, 3.9326e-07,\n",
      "          1.6932e-04, 1.4788e-06, 4.9897e-01, 5.0061e-01, 8.4286e-07,\n",
      "          7.9882e-06, 5.5962e-07, 8.5002e-06, 1.8240e-05, 1.0913e-05,\n",
      "          6.5477e-07, 3.9533e-07, 8.9856e-07, 1.2084e-06, 8.7869e-07,\n",
      "          5.2798e-07, 5.7177e-07, 1.2470e-06, 5.6141e-07, 5.8854e-07,\n",
      "          1.2433e-06, 3.6672e-07, 3.2929e-07, 7.8228e-07, 6.2266e-07,\n",
      "          1.3101e-06, 1.1452e-06, 1.2055e-06, 7.8315e-07, 7.2673e-07,\n",
      "          6.6058e-07, 7.6986e-07, 3.8590e-07, 9.7377e-07, 5.8166e-07,\n",
      "          1.3798e-06],\n",
      "         [4.2573e-05, 5.6077e-05, 7.8751e-05, 3.6365e-05, 6.6294e-05,\n",
      "          3.2676e-05, 5.3350e-05, 1.6987e-03, 8.2807e-04, 4.1867e-04,\n",
      "          3.1248e-03, 1.2060e-03, 5.7019e-04, 6.4565e-04, 1.3875e-04,\n",
      "          1.6141e-03, 1.1853e-04, 3.4423e-02, 2.3637e-02, 4.3857e-03,\n",
      "          6.9914e-05, 3.0683e-04, 9.1482e-04, 3.2804e-03, 6.6047e-04,\n",
      "          2.8069e-01, 7.1819e-05, 1.8432e-03, 2.6359e-03, 1.6736e-04,\n",
      "          2.6764e-04, 9.9626e-03, 2.6467e-01, 3.2878e-01, 1.8875e-03,\n",
      "          6.7685e-03, 2.6813e-04, 1.5430e-03, 1.8684e-03, 7.2614e-04,\n",
      "          1.1509e-03, 2.0783e-03, 5.8192e-03, 1.0033e-02, 6.2922e-05,\n",
      "          5.8507e-05, 5.6602e-05, 2.4470e-05, 3.5076e-05, 5.4062e-05,\n",
      "          6.6425e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55457\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5888/10000 (58.9%)\n",
      "    Average reward: +0.431\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4112/10000 (41.1%)\n",
      "    Average reward: -0.431\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8203 (29.8%)\n",
      "    Action 1: 16252 (59.1%)\n",
      "    Action 2: 2731 (9.9%)\n",
      "    Action 3: 300 (1.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8082 (28.9%)\n",
      "    Action 1: 14292 (51.1%)\n",
      "    Action 2: 2814 (10.1%)\n",
      "    Action 3: 2783 (9.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4315.0, -4315.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.969 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.013 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.991\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4315\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2393, 0.7556, 0.0052, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9565, 0.0074, 0.0360]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49640\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6169/10000 (61.7%)\n",
      "    Average reward: +0.004\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3831/10000 (38.3%)\n",
      "    Average reward: -0.004\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22962 (86.2%)\n",
      "    Action 1: 3684 (13.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2675 (11.6%)\n",
      "    Action 1: 17961 (78.1%)\n",
      "    Action 2: 803 (3.5%)\n",
      "    Action 3: 1555 (6.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [36.0, -36.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.580 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.639 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.610\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0036\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33003/50000 [32:10<12:13, 23.16it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 211196/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 213520/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33999/50000 [32:54<11:48, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 217636/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 220220/2000000\n",
      "P1 SL Buffer Size:  217636\n",
      "P1 SL buffer distribution [ 81665. 108128.  16914.  10929.]\n",
      "P1 actions distribution [0.37523663 0.49682957 0.07771692 0.05021688]\n",
      "P2 SL Buffer Size:  220220\n",
      "P2 SL buffer distribution [ 70054. 112515.  17586.  20065.]\n",
      "P2 actions distribution [0.31810916 0.5109209  0.07985651 0.09111343]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33999/50000 [33:04<11:48, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.4143, 0.5814, 0.0043, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.0214e-04, 1.7816e-04, 1.3940e-04, 1.4523e-04, 1.4848e-04,\n",
      "          2.1858e-04, 1.5087e-04, 5.0162e-04, 8.7070e-04, 6.2018e-04,\n",
      "          2.3327e-04, 7.9959e-04, 3.5222e-04, 4.0518e-04, 5.5926e-04,\n",
      "          1.7144e-03, 1.0325e-03, 1.9011e-02, 1.9942e-02, 3.8995e-03,\n",
      "          4.6852e-03, 1.1705e-03, 4.2025e-03, 4.2234e-03, 2.4791e-04,\n",
      "          4.4923e-02, 3.8680e-04, 1.4186e-02, 9.4332e-03, 1.0886e-02,\n",
      "          1.0026e-01, 4.2578e-02, 2.9474e-01, 2.2636e-01, 2.0731e-03,\n",
      "          1.6584e-02, 1.5027e-02, 1.2238e-02, 7.3501e-03, 9.1427e-03,\n",
      "          1.9962e-02, 2.0833e-02, 5.5202e-02, 3.1226e-02, 1.6176e-04,\n",
      "          1.8433e-04, 1.5491e-04, 1.7260e-04, 1.8435e-04, 1.2595e-04,\n",
      "          7.3647e-05],\n",
      "         [8.3235e-05, 1.5090e-04, 1.4959e-04, 1.4769e-04, 1.8579e-04,\n",
      "          1.9612e-04, 1.9055e-04, 8.6820e-04, 1.0512e-03, 6.0628e-04,\n",
      "          1.5236e-03, 5.8498e-04, 6.3466e-04, 1.0590e-03, 6.3630e-04,\n",
      "          6.7894e-03, 1.3150e-03, 8.9949e-03, 1.1638e-02, 3.0290e-03,\n",
      "          1.5495e-03, 6.1623e-04, 2.8104e-03, 3.5471e-03, 2.7534e-04,\n",
      "          2.4881e-02, 4.4365e-04, 1.9887e-02, 9.9112e-03, 1.3494e-02,\n",
      "          2.2331e-01, 4.2702e-02, 1.8079e-01, 1.5725e-01, 6.6922e-03,\n",
      "          5.9507e-02, 1.8386e-02, 2.9611e-02, 1.6571e-02, 8.1371e-03,\n",
      "          2.3613e-02, 2.2267e-02, 5.7627e-02, 3.5392e-02, 1.3820e-04,\n",
      "          1.2702e-04, 1.6660e-04, 1.1843e-04, 1.5050e-04, 1.2130e-04,\n",
      "          6.9699e-05],\n",
      "         [8.2989e-06, 9.7537e-06, 1.3545e-05, 1.0144e-05, 1.0465e-05,\n",
      "          1.1259e-05, 8.6883e-06, 1.4332e-05, 1.5369e-05, 1.0576e-05,\n",
      "          7.4885e-06, 7.9922e-06, 1.2309e-03, 8.4568e-04, 9.4655e-06,\n",
      "          8.3716e-04, 1.0381e-05, 4.4768e-04, 4.7454e-04, 1.0627e-05,\n",
      "          2.1973e-04, 6.5927e-06, 4.1706e-01, 5.6311e-01, 1.5328e-02,\n",
      "          1.6191e-05, 1.3618e-05, 1.1603e-05, 9.9970e-06, 1.7341e-05,\n",
      "          1.2402e-05, 1.2804e-05, 9.9797e-06, 6.3621e-06, 9.0677e-06,\n",
      "          6.6118e-06, 1.2912e-05, 9.6656e-06, 1.4887e-05, 1.1413e-05,\n",
      "          7.5232e-06, 7.6311e-06, 1.2303e-05, 9.6750e-06, 1.2009e-05,\n",
      "          8.4265e-06, 1.0380e-05, 8.5974e-06, 7.4971e-06, 1.1086e-05,\n",
      "          8.8699e-06],\n",
      "         [2.5137e-04, 3.7621e-04, 3.1150e-04, 3.1280e-04, 4.0997e-04,\n",
      "          3.7352e-04, 3.1447e-04, 8.3258e-04, 1.3371e-03, 7.6331e-04,\n",
      "          1.5062e-03, 8.0039e-04, 2.5434e-03, 2.9990e-03, 6.2586e-04,\n",
      "          2.0157e-03, 6.2279e-04, 2.5261e-02, 2.6607e-02, 4.0370e-03,\n",
      "          7.6258e-04, 1.1190e-03, 1.7977e-02, 1.6492e-02, 4.0169e-04,\n",
      "          3.6062e-02, 5.2693e-04, 8.0933e-02, 5.2146e-02, 5.0138e-03,\n",
      "          1.3718e-02, 5.0312e-02, 2.6404e-01, 1.4571e-01, 9.1880e-04,\n",
      "          9.4796e-03, 8.5883e-03, 2.0525e-02, 1.9140e-02, 5.2311e-03,\n",
      "          1.1148e-02, 2.5979e-02, 8.9496e-02, 4.9320e-02, 4.0431e-04,\n",
      "          3.8339e-04, 3.9701e-04, 2.8657e-04, 3.7091e-04, 4.9144e-04,\n",
      "          3.2814e-04]]])\n",
      "Player 0 Prediction: tensor([[0.8230, 0.1741, 0.0029, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[7.2520e-05, 7.5218e-05, 1.1213e-04, 8.1558e-05, 7.5424e-05,\n",
      "          1.2675e-04, 1.2871e-04, 2.1056e-03, 2.1737e-03, 4.2676e-04,\n",
      "          1.0526e-02, 5.8070e-04, 4.8940e-04, 6.2807e-04, 1.5082e-04,\n",
      "          7.7369e-02, 3.1295e-04, 1.0843e-04, 2.7145e-04, 1.0027e-03,\n",
      "          3.7281e-03, 3.7440e-04, 9.4708e-04, 1.7447e-03, 1.0437e-04,\n",
      "          4.4887e-01, 1.2617e-04, 3.8986e-03, 1.6544e-03, 5.1185e-03,\n",
      "          9.3809e-02, 5.8187e-03, 1.1963e-03, 6.4382e-04, 1.1779e-03,\n",
      "          2.3356e-01, 7.1004e-03, 3.3188e-03, 2.6393e-03, 6.5972e-03,\n",
      "          3.6004e-02, 7.2339e-03, 2.3444e-02, 1.3536e-02, 7.0568e-05,\n",
      "          9.1484e-05, 7.8755e-05, 9.7336e-05, 9.8498e-05, 5.8598e-05,\n",
      "          4.4339e-05],\n",
      "         [1.2532e-04, 1.8185e-04, 1.6326e-04, 1.5451e-04, 2.3822e-04,\n",
      "          3.4969e-04, 2.4760e-04, 1.1165e-03, 7.3651e-04, 3.1269e-04,\n",
      "          1.3957e-03, 5.4513e-04, 2.6573e-04, 4.3757e-04, 3.5034e-04,\n",
      "          7.2715e-03, 4.1677e-04, 1.6408e-03, 1.7431e-03, 1.2739e-03,\n",
      "          1.5196e-03, 6.7681e-04, 1.9870e-03, 2.1706e-03, 2.9698e-04,\n",
      "          1.6558e-02, 4.8109e-04, 2.9839e-02, 1.5462e-02, 3.5651e-03,\n",
      "          7.6216e-01, 1.6589e-02, 3.2788e-02, 2.1368e-02, 3.7327e-04,\n",
      "          2.9942e-02, 5.6039e-03, 4.3491e-03, 4.6575e-03, 8.8353e-03,\n",
      "          1.7737e-02, 1.5847e-03, 6.4991e-04, 6.0313e-04, 1.7479e-04,\n",
      "          1.5426e-04, 2.2867e-04, 1.4072e-04, 2.6540e-04, 1.7016e-04,\n",
      "          1.0087e-04],\n",
      "         [3.1914e-05, 2.6959e-05, 4.1631e-05, 2.7224e-05, 2.2845e-05,\n",
      "          4.0897e-05, 3.7995e-05, 2.7008e-05, 3.0546e-05, 2.9529e-05,\n",
      "          3.5799e-05, 2.3780e-05, 5.4180e-03, 4.7430e-03, 2.7161e-05,\n",
      "          2.0938e-02, 3.2622e-05, 1.0162e-03, 8.3652e-04, 1.8327e-05,\n",
      "          9.5629e-01, 2.3648e-05, 1.5283e-03, 7.7610e-03, 7.3742e-05,\n",
      "          5.2687e-05, 2.2537e-05, 3.7454e-05, 2.3656e-05, 3.3816e-05,\n",
      "          2.9411e-05, 3.1510e-05, 4.2619e-05, 2.1692e-05, 5.2602e-05,\n",
      "          2.7174e-05, 3.0996e-05, 2.4139e-05, 4.3362e-05, 4.0253e-05,\n",
      "          2.9700e-05, 3.4156e-05, 6.8941e-05, 3.6302e-05, 4.3010e-05,\n",
      "          2.2732e-05, 1.7099e-05, 3.1169e-05, 4.5886e-05, 4.5117e-05,\n",
      "          2.9805e-05],\n",
      "         [1.6214e-04, 1.6274e-04, 1.2659e-04, 1.1935e-04, 1.6583e-04,\n",
      "          1.4290e-04, 1.6553e-04, 3.6097e-04, 3.9984e-04, 2.2889e-04,\n",
      "          2.1741e-03, 4.0493e-04, 6.2766e-04, 5.8874e-04, 1.4409e-04,\n",
      "          5.5943e-04, 2.8104e-04, 5.5543e-04, 4.8042e-04, 5.4701e-04,\n",
      "          4.8153e-02, 7.7777e-04, 8.8279e-04, 1.0548e-03, 1.3622e-04,\n",
      "          6.8218e-02, 1.9661e-04, 6.0009e-03, 2.8344e-03, 7.5626e-03,\n",
      "          7.4837e-01, 3.7045e-03, 3.4228e-03, 2.8930e-03, 4.3749e-04,\n",
      "          6.2118e-03, 2.9891e-03, 2.6616e-03, 2.5538e-03, 1.0975e-02,\n",
      "          6.7517e-02, 1.2509e-03, 9.2481e-04, 8.6472e-04, 1.6008e-04,\n",
      "          7.9236e-05, 1.2500e-04, 1.4067e-04, 1.7565e-04, 2.1732e-04,\n",
      "          1.0674e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0270, 0.0967, 0.8763, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59428\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5478/10000 (54.8%)\n",
      "    Average reward: -0.518\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4522/10000 (45.2%)\n",
      "    Average reward: +0.518\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10480 (35.5%)\n",
      "    Action 1: 15584 (52.8%)\n",
      "    Action 2: 1441 (4.9%)\n",
      "    Action 3: 2036 (6.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8650 (28.9%)\n",
      "    Action 1: 12747 (42.7%)\n",
      "    Action 2: 3099 (10.4%)\n",
      "    Action 3: 5391 (18.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5182.5, 5182.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.017 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.030\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5182\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9315e-01, 4.6616e-05, 6.8054e-03]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6453, 0.0024, 0.3524]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52027\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5412/10000 (54.1%)\n",
      "    Average reward: +0.246\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4588/10000 (45.9%)\n",
      "    Average reward: -0.246\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3489 (14.0%)\n",
      "    Action 1: 16025 (64.5%)\n",
      "    Action 2: 2140 (8.6%)\n",
      "    Action 3: 3209 (12.9%)\n",
      "  Player 1:\n",
      "    Action 0: 21061 (77.5%)\n",
      "    Action 1: 6103 (22.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2463.5, -2463.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.806 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.769 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.787\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2464\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.9687e-05, 3.5808e-05, 3.9758e-05, 3.8467e-05, 4.2015e-05,\n",
      "          3.7530e-05, 4.6699e-05, 7.1213e-04, 1.0817e-03, 3.5649e-04,\n",
      "          5.0722e-03, 2.4909e-03, 2.7869e-02, 3.6871e-02, 7.4532e-03,\n",
      "          1.7132e-03, 6.0503e-04, 2.4442e-02, 1.7494e-02, 1.6941e-03,\n",
      "          1.3206e-03, 4.4741e-04, 8.4659e-04, 9.0175e-04, 1.1974e-04,\n",
      "          1.4543e-01, 4.9750e-05, 2.6977e-03, 2.7329e-03, 1.2145e-03,\n",
      "          6.9568e-03, 4.1186e-02, 2.5007e-01, 2.5966e-01, 5.7458e-03,\n",
      "          1.1216e-02, 1.1533e-02, 4.7507e-02, 3.6476e-02, 3.3173e-03,\n",
      "          6.8181e-03, 7.2375e-03, 1.7792e-02, 1.0329e-02, 4.3493e-05,\n",
      "          6.1686e-05, 3.1671e-05, 4.4678e-05, 3.2239e-05, 3.6707e-05,\n",
      "          2.4957e-05],\n",
      "         [9.6632e-06, 1.2644e-05, 1.4368e-05, 1.3435e-05, 1.3354e-05,\n",
      "          1.2315e-05, 1.7400e-05, 1.0308e-04, 3.1568e-04, 9.4306e-05,\n",
      "          2.0863e-02, 5.5587e-03, 1.7774e-03, 1.4156e-03, 3.4308e-04,\n",
      "          1.0983e-02, 8.4766e-04, 3.7092e-03, 3.3551e-03, 6.6568e-04,\n",
      "          2.9217e-03, 2.0384e-04, 1.8498e-04, 1.8119e-04, 3.0701e-05,\n",
      "          3.7512e-02, 2.0398e-05, 4.5366e-04, 4.7197e-04, 1.8335e-02,\n",
      "          3.6451e-01, 1.8064e-03, 1.2538e-02, 2.1004e-02, 4.6473e-02,\n",
      "          3.9983e-01, 2.0885e-04, 4.5051e-04, 1.3259e-03, 1.0306e-02,\n",
      "          3.0059e-02, 3.2300e-04, 3.4954e-04, 2.7773e-04, 1.5688e-05,\n",
      "          1.3483e-05, 1.1683e-05, 1.4914e-05, 1.3184e-05, 1.9769e-05,\n",
      "          7.6318e-06],\n",
      "         [3.5356e-06, 5.3313e-06, 3.3605e-06, 4.9459e-06, 4.9936e-06,\n",
      "          6.9680e-06, 5.1138e-06, 3.5609e-06, 4.1188e-06, 6.6803e-06,\n",
      "          4.0076e-06, 5.7083e-06, 9.0365e-04, 8.7657e-04, 3.7550e-06,\n",
      "          4.2465e-04, 6.0326e-06, 5.3910e-04, 5.9450e-04, 2.6271e-06,\n",
      "          5.0655e-03, 2.3206e-06, 2.8466e-03, 1.7126e-01, 8.1732e-01,\n",
      "          2.7548e-06, 3.4709e-06, 2.7938e-06, 3.0053e-06, 3.1246e-06,\n",
      "          3.2880e-06, 2.4898e-06, 5.0026e-06, 2.4047e-06, 5.9079e-06,\n",
      "          3.6070e-06, 2.5546e-06, 2.8489e-06, 3.2007e-06, 5.3660e-06,\n",
      "          5.7581e-06, 2.7022e-06, 4.1455e-06, 5.5756e-06, 3.4878e-06,\n",
      "          2.4083e-06, 3.1899e-06, 4.5593e-06, 3.9526e-06, 2.8875e-06,\n",
      "          3.1314e-06],\n",
      "         [9.6461e-05, 1.7013e-04, 2.1454e-04, 1.2337e-04, 2.9148e-04,\n",
      "          1.9902e-04, 1.6621e-04, 4.3101e-04, 6.1242e-04, 8.0412e-04,\n",
      "          6.0394e-03, 5.0205e-03, 5.1402e-03, 6.9121e-03, 6.3934e-04,\n",
      "          4.2920e-03, 6.5065e-04, 2.6367e-02, 3.3582e-02, 5.0021e-03,\n",
      "          1.6807e-03, 1.2834e-03, 1.6340e-02, 1.4587e-02, 4.1283e-03,\n",
      "          4.6477e-01, 2.2848e-04, 9.7846e-03, 1.0694e-02, 4.6951e-04,\n",
      "          3.4765e-03, 1.8959e-02, 1.4167e-01, 1.3685e-01, 8.1394e-03,\n",
      "          2.3093e-02, 1.1309e-03, 9.2026e-03, 1.3307e-02, 3.4356e-03,\n",
      "          6.2815e-03, 2.9446e-03, 5.7030e-03, 3.6336e-03, 1.7392e-04,\n",
      "          3.0972e-04, 1.8158e-04, 2.0456e-04, 1.5523e-04, 1.8891e-04,\n",
      "          2.3832e-04]]])\n",
      "Player 1 Prediction: tensor([[0.5622, 0.4366, 0.0012, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.4865e-05, 1.8036e-05, 1.7033e-05, 1.7636e-05, 1.7889e-05,\n",
      "          1.5139e-05, 2.0762e-05, 8.1934e-04, 9.3649e-04, 2.1308e-04,\n",
      "          2.5522e-03, 7.8924e-04, 1.5268e-02, 1.7499e-02, 3.3690e-03,\n",
      "          1.9439e-04, 6.9406e-05, 4.4377e-02, 3.4865e-02, 3.5423e-03,\n",
      "          3.6554e-05, 2.6776e-05, 3.3229e-04, 1.0309e-03, 1.2033e-04,\n",
      "          3.2133e-01, 2.4835e-05, 3.9600e-04, 4.1754e-04, 4.9580e-05,\n",
      "          1.3294e-04, 1.8174e-02, 1.9354e-01, 2.0915e-01, 1.9475e-04,\n",
      "          5.9938e-04, 3.8980e-03, 2.9660e-02, 2.5706e-02, 8.0032e-05,\n",
      "          3.9790e-04, 9.2216e-03, 3.3833e-02, 2.6915e-02, 1.7774e-05,\n",
      "          2.5880e-05, 1.8478e-05, 1.6655e-05, 1.2930e-05, 1.9531e-05,\n",
      "          1.4342e-05],\n",
      "         [5.1309e-06, 5.6993e-06, 6.7459e-06, 1.0050e-05, 7.4562e-06,\n",
      "          6.1865e-06, 8.3436e-06, 1.7251e-06, 6.0445e-06, 5.3790e-05,\n",
      "          2.6372e-02, 1.2722e-03, 2.0400e-03, 1.4558e-03, 6.2903e-05,\n",
      "          6.5904e-03, 5.9698e-05, 3.1527e-05, 2.3580e-05, 9.4671e-05,\n",
      "          6.5118e-04, 5.0880e-05, 9.1362e-05, 8.9830e-05, 1.5285e-05,\n",
      "          1.8027e-02, 8.5083e-06, 4.5445e-05, 6.1368e-05, 2.2190e-04,\n",
      "          3.6290e-01, 5.7792e-04, 1.7896e-03, 3.3856e-03, 2.3771e-04,\n",
      "          5.3748e-01, 1.9122e-05, 1.5760e-04, 4.6832e-04, 3.8599e-03,\n",
      "          3.1639e-02, 5.2933e-05, 1.1745e-06, 1.2683e-06, 9.6168e-06,\n",
      "          7.9203e-06, 5.1195e-06, 7.5206e-06, 6.6977e-06, 1.0375e-05,\n",
      "          5.5840e-06],\n",
      "         [1.9871e-07, 1.8583e-07, 9.2282e-08, 2.2758e-07, 3.4373e-07,\n",
      "          2.6959e-07, 2.1889e-07, 2.1052e-07, 7.3315e-08, 2.3048e-07,\n",
      "          1.4588e-07, 1.5321e-07, 4.7455e-05, 2.7015e-05, 1.0074e-07,\n",
      "          8.8805e-06, 1.7436e-07, 6.2714e-05, 3.2334e-05, 1.6392e-07,\n",
      "          9.9981e-01, 1.2207e-07, 3.6944e-07, 4.5191e-06, 2.7499e-06,\n",
      "          1.6273e-07, 1.6468e-07, 1.1035e-07, 1.7128e-07, 9.9923e-08,\n",
      "          1.1621e-07, 1.2611e-07, 1.3938e-07, 1.4286e-07, 1.1100e-07,\n",
      "          1.6979e-07, 1.0816e-07, 1.0937e-07, 1.4862e-07, 9.2462e-08,\n",
      "          2.3856e-07, 1.2389e-07, 1.1941e-07, 2.3257e-07, 1.3577e-07,\n",
      "          1.2428e-07, 1.2008e-07, 1.6952e-07, 1.5351e-07, 1.9816e-07,\n",
      "          1.2999e-07],\n",
      "         [1.1897e-04, 1.0619e-04, 1.3674e-04, 1.4650e-04, 2.0858e-04,\n",
      "          1.4725e-04, 1.0453e-04, 6.1967e-04, 8.8373e-04, 4.1949e-04,\n",
      "          2.1754e-02, 7.3085e-03, 6.0736e-04, 6.4895e-04, 3.3384e-04,\n",
      "          6.5086e-03, 5.2490e-04, 1.5502e-02, 2.5485e-02, 2.8190e-03,\n",
      "          6.3941e-03, 1.6417e-03, 1.9456e-04, 2.3797e-04, 6.3979e-04,\n",
      "          7.2627e-01, 1.7012e-04, 2.0769e-04, 2.2607e-04, 5.3230e-04,\n",
      "          1.9127e-02, 4.1906e-03, 1.7736e-02, 1.7814e-02, 1.8847e-02,\n",
      "          5.3436e-02, 2.6368e-04, 1.1267e-03, 2.3127e-03, 1.2775e-02,\n",
      "          2.3773e-02, 1.9730e-03, 2.9580e-03, 1.6903e-03, 1.1141e-04,\n",
      "          2.3816e-04, 1.1775e-04, 1.4607e-04, 1.3940e-04, 9.0655e-05,\n",
      "          2.3268e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6427, 0.0061, 0.3512]])\n",
      "Player 0 Prediction: tensor([[[6.7248e-08, 1.2024e-07, 1.4874e-07, 7.2875e-08, 1.1332e-07,\n",
      "          1.1768e-07, 1.1063e-07, 1.9349e-05, 2.1831e-05, 6.8202e-07,\n",
      "          2.6770e-04, 1.7203e-06, 4.3337e-01, 4.3351e-01, 1.8047e-06,\n",
      "          5.6854e-05, 7.7551e-07, 6.1884e-05, 5.2674e-05, 1.2251e-06,\n",
      "          2.7572e-07, 4.5907e-07, 1.0052e-06, 7.2999e-07, 2.5311e-07,\n",
      "          1.2791e-01, 8.3100e-08, 4.2684e-07, 3.3689e-07, 5.3114e-07,\n",
      "          5.3250e-07, 8.7463e-06, 7.6757e-06, 7.4227e-06, 2.7491e-06,\n",
      "          2.3799e-06, 4.5797e-06, 2.3929e-03, 2.2826e-03, 1.2561e-06,\n",
      "          1.2113e-06, 3.5991e-06, 2.9403e-07, 3.8541e-07, 1.2473e-07,\n",
      "          5.3153e-08, 6.5547e-08, 9.0674e-08, 1.0907e-07, 1.1991e-07,\n",
      "          7.7753e-08],\n",
      "         [8.0516e-07, 1.2204e-06, 1.1020e-06, 8.7352e-07, 1.3244e-06,\n",
      "          1.3670e-06, 1.0382e-06, 4.2137e-01, 4.2466e-01, 2.1114e-05,\n",
      "          6.5158e-04, 3.8639e-05, 8.4732e-05, 6.9618e-05, 3.8720e-05,\n",
      "          1.0895e-05, 5.4694e-06, 4.1685e-03, 2.3437e-03, 1.0876e-04,\n",
      "          7.9290e-06, 2.1855e-06, 1.0982e-05, 1.2630e-05, 1.6266e-06,\n",
      "          9.0512e-02, 7.6184e-07, 1.5758e-05, 1.6054e-05, 6.5165e-06,\n",
      "          5.0510e-07, 6.5156e-04, 2.0856e-03, 3.0223e-03, 6.2724e-06,\n",
      "          6.8051e-06, 2.3027e-05, 2.0597e-02, 2.3364e-02, 1.7721e-06,\n",
      "          5.2969e-07, 2.7968e-05, 2.9999e-03, 3.0348e-03, 1.4934e-06,\n",
      "          7.3654e-07, 9.2412e-07, 1.6645e-06, 1.0607e-06, 8.3402e-07,\n",
      "          9.5118e-07],\n",
      "         [7.2459e-07, 9.0277e-07, 8.1079e-07, 9.3566e-07, 5.1359e-07,\n",
      "          1.8709e-06, 5.3253e-07, 7.0737e-07, 1.3336e-06, 1.3536e-06,\n",
      "          1.4151e-06, 1.1842e-06, 8.5275e-05, 8.8564e-05, 4.6723e-07,\n",
      "          9.8751e-05, 1.7809e-06, 5.0003e-01, 4.9952e-01, 1.3161e-06,\n",
      "          5.5522e-06, 1.1021e-06, 2.9258e-05, 6.9999e-05, 1.9606e-05,\n",
      "          1.0018e-06, 7.4282e-07, 1.2566e-06, 1.9513e-06, 1.9148e-06,\n",
      "          7.2701e-07, 1.0972e-06, 2.0922e-06, 9.7772e-07, 9.7189e-07,\n",
      "          2.2046e-06, 8.7719e-07, 6.4284e-07, 1.2597e-06, 1.2611e-06,\n",
      "          1.2900e-06, 1.3359e-06, 1.6465e-06, 1.1474e-06, 9.4100e-07,\n",
      "          8.7283e-07, 1.1806e-06, 5.4872e-07, 1.8561e-06, 5.0372e-07,\n",
      "          2.2519e-06],\n",
      "         [2.0703e-05, 2.4798e-05, 2.1925e-05, 1.4148e-05, 1.9223e-05,\n",
      "          1.1493e-05, 3.3044e-05, 3.8840e-04, 1.8526e-04, 1.7216e-04,\n",
      "          3.8376e-04, 2.8617e-04, 2.6869e-04, 3.3092e-04, 4.6468e-05,\n",
      "          1.6893e-04, 4.4494e-05, 8.4697e-02, 6.7163e-02, 3.2833e-03,\n",
      "          2.6229e-04, 1.1393e-04, 7.5236e-04, 2.6561e-03, 2.3257e-04,\n",
      "          7.8845e-01, 2.1573e-05, 8.9826e-04, 8.3222e-04, 5.8002e-05,\n",
      "          3.4279e-05, 2.7165e-03, 2.0720e-02, 1.8625e-02, 4.3934e-04,\n",
      "          6.2573e-04, 9.5581e-05, 1.0447e-03, 1.0522e-03, 1.3921e-04,\n",
      "          1.0168e-04, 4.7200e-04, 7.9379e-04, 1.1476e-03, 2.0976e-05,\n",
      "          2.8158e-05, 2.0342e-05, 1.3651e-05, 1.3794e-05, 3.2488e-05,\n",
      "          1.9490e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55905\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5861/10000 (58.6%)\n",
      "    Average reward: +0.367\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4139/10000 (41.4%)\n",
      "    Average reward: -0.367\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7945 (28.8%)\n",
      "    Action 1: 16653 (60.3%)\n",
      "    Action 2: 2797 (10.1%)\n",
      "    Action 3: 234 (0.8%)\n",
      "  Player 1:\n",
      "    Action 0: 8149 (28.8%)\n",
      "    Action 1: 14202 (50.2%)\n",
      "    Action 2: 2869 (10.1%)\n",
      "    Action 3: 3056 (10.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3674.5, -3674.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.957 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.987\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3674\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2085, 0.7872, 0.0043, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7826, 0.0219, 0.1955]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49854\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6005/10000 (60.1%)\n",
      "    Average reward: -0.140\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3995/10000 (40.0%)\n",
      "    Average reward: +0.140\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 23002 (86.0%)\n",
      "    Action 1: 3733 (14.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2611 (11.3%)\n",
      "    Action 1: 18024 (78.0%)\n",
      "    Action 2: 799 (3.5%)\n",
      "    Action 3: 1685 (7.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1404.5, 1404.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.583 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.635 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.609\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.1404\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35003/50000 [34:16<10:55, 22.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 223933/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 226781/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [35:00<10:25, 22.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 230606/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 233160/2000000\n",
      "P1 SL Buffer Size:  230606\n",
      "P1 SL buffer distribution [ 85462. 115868.  18168.  11108.]\n",
      "P1 actions distribution [0.37059747 0.50245007 0.07878373 0.04816874]\n",
      "P2 SL Buffer Size:  233160\n",
      "P2 SL buffer distribution [ 73406. 118684.  18986.  22084.]\n",
      "P2 actions distribution [0.31483102 0.50902385 0.08142906 0.09471607]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[9.5917e-05, 1.3955e-04, 1.5178e-04, 1.5260e-04, 1.4970e-04,\n",
      "          2.0940e-04, 1.8100e-04, 7.9569e-04, 1.2481e-03, 8.4457e-04,\n",
      "          2.5599e-04, 3.7480e-04, 9.0034e-04, 1.1546e-03, 7.2704e-04,\n",
      "          2.4455e-04, 3.6774e-04, 1.7115e-02, 2.2753e-02, 3.9772e-03,\n",
      "          1.3039e-03, 6.2928e-04, 4.0060e-03, 4.5235e-03, 2.8943e-04,\n",
      "          2.0356e-02, 5.5995e-04, 1.6745e-02, 6.3610e-03, 3.5646e-03,\n",
      "          2.2865e-02, 6.6435e-02, 3.0957e-01, 2.6813e-01, 8.8918e-04,\n",
      "          4.5067e-03, 2.3651e-02, 3.9681e-02, 1.9933e-02, 2.9626e-03,\n",
      "          1.0269e-02, 2.9827e-02, 5.6823e-02, 3.3393e-02, 1.3558e-04,\n",
      "          1.4727e-04, 1.4957e-04, 1.3908e-04, 1.4621e-04, 1.1103e-04,\n",
      "          5.5334e-05],\n",
      "         [1.0007e-04, 1.7954e-04, 1.7106e-04, 1.5886e-04, 2.2802e-04,\n",
      "          2.3927e-04, 2.3342e-04, 8.7266e-04, 1.0120e-03, 5.8170e-04,\n",
      "          7.3095e-04, 6.6924e-04, 6.7410e-04, 1.1556e-03, 6.7164e-04,\n",
      "          7.6005e-03, 1.5049e-03, 6.3745e-03, 8.3044e-03, 2.6272e-03,\n",
      "          2.1130e-03, 7.0763e-04, 4.0874e-03, 4.9110e-03, 4.0431e-04,\n",
      "          2.0546e-02, 6.2704e-04, 2.0865e-02, 1.0721e-02, 1.5586e-02,\n",
      "          2.6501e-01, 3.7832e-02, 1.5047e-01, 1.3500e-01, 8.1058e-03,\n",
      "          6.9722e-02, 1.7611e-02, 3.2122e-02, 1.9620e-02, 8.7115e-03,\n",
      "          2.4072e-02, 2.2763e-02, 5.8241e-02, 3.4910e-02, 1.8502e-04,\n",
      "          1.5461e-04, 1.9548e-04, 1.5633e-04, 1.9319e-04, 1.5115e-04,\n",
      "          1.1769e-04],\n",
      "         [1.8087e-05, 2.0977e-05, 2.6839e-05, 2.1498e-05, 1.8308e-05,\n",
      "          2.1420e-05, 1.4695e-05, 3.1983e-05, 3.2095e-05, 1.5827e-05,\n",
      "          1.1328e-05, 1.8357e-05, 2.0455e-03, 1.4132e-03, 2.6104e-05,\n",
      "          1.5879e-03, 2.1173e-05, 1.5875e-03, 1.8118e-03, 2.2134e-05,\n",
      "          4.6448e-04, 1.6606e-05, 9.1189e-03, 2.4000e-01, 7.4106e-01,\n",
      "          2.4507e-05, 3.0240e-05, 2.7438e-05, 2.2328e-05, 3.7003e-05,\n",
      "          2.2891e-05, 2.2782e-05, 2.1167e-05, 1.7133e-05, 2.1430e-05,\n",
      "          2.2335e-05, 3.0257e-05, 2.2976e-05, 2.6933e-05, 2.0270e-05,\n",
      "          1.5985e-05, 1.5203e-05, 2.0019e-05, 1.8356e-05, 1.9233e-05,\n",
      "          1.8283e-05, 2.2421e-05, 2.0802e-05, 1.6170e-05, 2.0558e-05,\n",
      "          1.9467e-05],\n",
      "         [2.3444e-04, 5.0344e-04, 4.7232e-04, 3.6457e-04, 8.1123e-04,\n",
      "          4.4460e-04, 5.3471e-04, 1.2497e-03, 1.4380e-03, 9.1213e-04,\n",
      "          1.6437e-03, 9.1015e-04, 2.5185e-03, 2.7712e-03, 6.9195e-04,\n",
      "          2.1660e-03, 7.6961e-04, 2.2114e-02, 3.1990e-02, 4.4434e-03,\n",
      "          5.5270e-04, 1.2297e-03, 1.3012e-02, 1.1107e-02, 4.7378e-04,\n",
      "          4.7731e-02, 7.1657e-04, 7.2380e-02, 3.4092e-02, 6.0740e-03,\n",
      "          1.8448e-02, 7.6542e-02, 2.0382e-01, 1.5285e-01, 1.2556e-03,\n",
      "          1.4052e-02, 1.2078e-02, 3.4409e-02, 2.5056e-02, 4.7383e-03,\n",
      "          8.5779e-03, 4.4245e-02, 8.5030e-02, 5.0851e-02, 4.7562e-04,\n",
      "          5.4602e-04, 4.5120e-04, 4.2042e-04, 6.5913e-04, 7.1481e-04,\n",
      "          4.3538e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.3781e-01, 3.4291e-04, 6.1846e-02]])\n",
      "Player 1 Prediction: tensor([[[1.3417e-04, 2.2782e-04, 1.7927e-04, 1.3925e-04, 1.7352e-04,\n",
      "          1.9852e-04, 1.8046e-04, 6.4881e-04, 7.6145e-04, 5.5377e-04,\n",
      "          3.2245e-03, 1.5286e-03, 3.4871e-04, 4.5965e-04, 3.4512e-04,\n",
      "          1.0239e-02, 2.3957e-03, 3.8678e-03, 4.8756e-03, 1.3618e-03,\n",
      "          2.1112e-02, 2.3097e-03, 2.6668e-03, 2.9624e-03, 2.6213e-04,\n",
      "          7.3518e-02, 4.0557e-04, 1.4628e-02, 6.9528e-03, 3.1842e-02,\n",
      "          4.7535e-01, 1.1254e-02, 2.6554e-02, 1.4843e-02, 8.1700e-03,\n",
      "          6.4271e-02, 5.0398e-03, 3.1677e-03, 3.0745e-03, 3.0826e-02,\n",
      "          1.3534e-01, 5.4836e-03, 1.7301e-02, 9.5545e-03, 1.6633e-04,\n",
      "          1.7839e-04, 1.8572e-04, 2.0895e-04, 2.2046e-04, 1.9309e-04,\n",
      "          1.0925e-04],\n",
      "         [8.4519e-05, 1.4425e-04, 1.4268e-04, 1.5836e-04, 2.2086e-04,\n",
      "          2.4826e-04, 1.9249e-04, 9.5780e-04, 1.2230e-03, 6.6015e-04,\n",
      "          3.3810e-04, 3.0818e-04, 6.0009e-04, 7.3548e-04, 4.1127e-04,\n",
      "          2.9079e-04, 2.7406e-04, 1.6830e-02, 2.0455e-02, 2.8522e-03,\n",
      "          7.3428e-04, 4.1154e-04, 2.0734e-03, 2.5758e-03, 2.5889e-04,\n",
      "          1.8861e-02, 5.1264e-04, 1.3868e-02, 2.8805e-03, 2.6762e-03,\n",
      "          2.2915e-02, 5.0275e-02, 2.8777e-01, 2.6894e-01, 6.8981e-04,\n",
      "          4.0714e-03, 2.3915e-02, 5.0082e-02, 2.7314e-02, 1.8732e-03,\n",
      "          6.3276e-03, 2.9776e-02, 8.1009e-02, 5.2170e-02, 1.3343e-04,\n",
      "          1.4194e-04, 1.4463e-04, 1.2303e-04, 1.6543e-04, 1.2422e-04,\n",
      "          6.6082e-05],\n",
      "         [8.0005e-06, 6.6849e-06, 1.0807e-05, 7.8457e-06, 9.3645e-06,\n",
      "          1.3943e-05, 7.9387e-06, 8.8478e-06, 9.2028e-06, 9.0590e-06,\n",
      "          1.2332e-05, 6.4201e-06, 6.7423e-04, 5.8513e-04, 5.7867e-06,\n",
      "          2.3537e-03, 9.6227e-06, 3.2836e-04, 3.7385e-04, 6.4098e-06,\n",
      "          2.2455e-04, 5.5331e-06, 6.4782e-01, 3.4720e-01, 3.5366e-05,\n",
      "          1.6889e-05, 9.8706e-06, 1.1424e-05, 8.0469e-06, 1.1991e-05,\n",
      "          9.0685e-06, 1.0249e-05, 9.8408e-06, 5.2079e-06, 9.4046e-06,\n",
      "          5.4257e-06, 1.1013e-05, 8.2059e-06, 1.9225e-05, 1.1572e-05,\n",
      "          7.3230e-06, 7.3692e-06, 1.7623e-05, 9.8848e-06, 1.6997e-05,\n",
      "          6.9946e-06, 7.6280e-06, 7.3048e-06, 7.7353e-06, 9.1532e-06,\n",
      "          8.5243e-06],\n",
      "         [7.2553e-04, 6.3125e-04, 6.2163e-04, 5.2241e-04, 4.8390e-04,\n",
      "          7.5627e-04, 5.4411e-04, 1.2554e-03, 2.3536e-03, 1.3832e-03,\n",
      "          4.1596e-03, 1.3515e-03, 4.6776e-03, 4.7358e-03, 9.9876e-04,\n",
      "          3.0485e-03, 1.3780e-03, 1.6200e-02, 9.9396e-03, 3.1620e-03,\n",
      "          6.6914e-03, 2.7772e-03, 2.5060e-02, 2.9230e-02, 9.7696e-04,\n",
      "          5.9415e-02, 1.0717e-03, 1.3902e-01, 8.7966e-02, 1.1631e-02,\n",
      "          4.4177e-02, 2.9349e-02, 1.5434e-01, 6.5434e-02, 1.7193e-03,\n",
      "          2.3960e-02, 1.7503e-02, 2.4511e-02, 3.2634e-02, 1.4877e-02,\n",
      "          5.4362e-02, 1.7142e-02, 5.7738e-02, 3.5145e-02, 7.1626e-04,\n",
      "          5.5184e-04, 7.0415e-04, 4.6259e-04, 6.7329e-04, 7.1521e-04,\n",
      "          5.1811e-04]]])\n",
      "Player 0 Prediction: tensor([[9.9917e-01, 0.0000e+00, 8.2597e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[1.8833e-05, 4.2056e-05, 3.6723e-05, 3.8343e-05, 5.2192e-05,\n",
      "          4.6865e-05, 5.0178e-05, 6.8085e-04, 6.7038e-04, 1.4603e-04,\n",
      "          1.8601e-03, 9.4790e-05, 1.9805e-02, 2.5566e-02, 8.6687e-05,\n",
      "          8.6374e-06, 1.2273e-04, 5.1486e-02, 5.6121e-02, 5.3715e-04,\n",
      "          1.5278e-04, 1.1944e-04, 5.7870e-04, 4.8324e-04, 6.1139e-05,\n",
      "          2.3915e-01, 2.0620e-04, 2.3084e-03, 4.3341e-04, 5.1345e-04,\n",
      "          1.2286e-03, 6.4410e-03, 1.6291e-01, 2.3378e-01, 1.0175e-04,\n",
      "          2.8112e-05, 3.1655e-03, 1.0648e-01, 7.5716e-02, 3.1275e-04,\n",
      "          3.0773e-04, 2.0253e-03, 3.1202e-03, 2.6772e-03, 3.2792e-05,\n",
      "          2.7642e-05, 3.1408e-05, 3.5847e-05, 3.0176e-05, 4.1599e-05,\n",
      "          2.6437e-05],\n",
      "         [3.0597e-04, 6.0390e-04, 5.2499e-04, 4.6225e-04, 7.6663e-04,\n",
      "          7.9899e-04, 6.3204e-04, 1.8760e-03, 2.3311e-03, 1.3295e-03,\n",
      "          3.3398e-03, 1.6373e-03, 1.3951e-02, 1.5062e-02, 4.2194e-04,\n",
      "          8.8085e-04, 6.9001e-04, 7.5077e-03, 8.6484e-03, 2.2566e-03,\n",
      "          6.5279e-03, 1.7420e-03, 2.5441e-02, 2.8735e-02, 7.7146e-04,\n",
      "          6.3681e-02, 1.7562e-03, 4.5180e-02, 2.2618e-02, 9.0670e-03,\n",
      "          1.7040e-02, 2.6556e-02, 2.3300e-01, 2.2127e-01, 1.1791e-03,\n",
      "          9.0722e-03, 1.2346e-02, 8.4041e-02, 8.3013e-02, 4.1469e-03,\n",
      "          8.2557e-03, 1.3686e-02, 7.6123e-03, 5.9164e-03, 5.1163e-04,\n",
      "          4.7537e-04, 5.6860e-04, 4.1098e-04, 5.0289e-04, 5.5532e-04,\n",
      "          2.8795e-04],\n",
      "         [2.2522e-05, 1.5576e-05, 3.0896e-05, 1.7615e-05, 2.0907e-05,\n",
      "          2.1608e-05, 1.6464e-05, 1.8084e-05, 2.4703e-05, 1.6915e-05,\n",
      "          2.7902e-05, 1.5955e-05, 1.7691e-03, 1.8783e-03, 3.7719e-05,\n",
      "          1.6700e-03, 2.8633e-05, 5.1026e-01, 4.8084e-01, 2.0544e-05,\n",
      "          1.0345e-03, 1.7540e-05, 2.3426e-04, 9.5577e-04, 3.1436e-04,\n",
      "          2.3214e-05, 3.6437e-05, 1.6127e-05, 1.6615e-05, 4.3553e-05,\n",
      "          3.5116e-05, 3.5149e-05, 2.4130e-05, 2.8004e-05, 3.3350e-05,\n",
      "          3.2652e-05, 2.6255e-05, 2.1360e-05, 3.0949e-05, 2.9079e-05,\n",
      "          1.8798e-05, 1.8169e-05, 2.1238e-05, 2.3852e-05, 2.2977e-05,\n",
      "          2.5642e-05, 1.7190e-05, 4.1518e-05, 2.7314e-05, 2.3134e-05,\n",
      "          2.2105e-05],\n",
      "         [1.2070e-04, 1.3941e-04, 1.4269e-04, 1.3159e-04, 2.6451e-04,\n",
      "          2.3769e-04, 1.8636e-04, 1.0367e-03, 1.0875e-03, 4.6971e-04,\n",
      "          6.9784e-04, 2.9982e-04, 3.1052e-04, 6.6273e-04, 2.5107e-04,\n",
      "          2.6809e-04, 3.0722e-04, 3.2168e-02, 3.4902e-02, 1.7012e-03,\n",
      "          6.7741e-04, 5.1361e-04, 2.9037e-03, 3.9932e-03, 1.4380e-04,\n",
      "          2.9520e-02, 2.7867e-04, 1.2543e-02, 2.1210e-03, 2.0752e-03,\n",
      "          1.3318e-02, 2.3330e-02, 3.5137e-01, 3.7046e-01, 2.7856e-04,\n",
      "          4.6227e-03, 2.1267e-02, 3.5597e-02, 1.6643e-02, 1.2449e-03,\n",
      "          2.6145e-03, 1.1201e-02, 9.9990e-03, 6.9546e-03, 1.1807e-04,\n",
      "          1.5848e-04, 1.5035e-04, 1.2028e-04, 1.2679e-04, 1.6221e-04,\n",
      "          1.1204e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5686, 0.0055, 0.4260]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [35:15<10:25, 22.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58213\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5328/10000 (53.3%)\n",
      "    Average reward: -0.473\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4672/10000 (46.7%)\n",
      "    Average reward: +0.473\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10845 (36.5%)\n",
      "    Action 1: 15080 (50.7%)\n",
      "    Action 2: 2075 (7.0%)\n",
      "    Action 3: 1744 (5.9%)\n",
      "  Player 1:\n",
      "    Action 0: 6210 (21.8%)\n",
      "    Action 1: 14980 (52.6%)\n",
      "    Action 2: 3103 (10.9%)\n",
      "    Action 3: 4176 (14.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4732.5, 4732.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.967 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.997\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4733\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.4031, 0.5931, 0.0038, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3971, 0.0575, 0.5454]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52042\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5345/10000 (53.4%)\n",
      "    Average reward: +0.201\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4655/10000 (46.6%)\n",
      "    Average reward: -0.201\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3318 (13.3%)\n",
      "    Action 1: 16018 (64.3%)\n",
      "    Action 2: 2183 (8.8%)\n",
      "    Action 3: 3399 (13.6%)\n",
      "  Player 1:\n",
      "    Action 0: 21038 (77.6%)\n",
      "    Action 1: 6086 (22.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2007.0, -2007.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.797 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.768 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.783\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2007\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.0105e-05, 1.1217e-05, 1.6348e-05, 2.0625e-05, 1.7976e-05,\n",
      "          1.5509e-05, 1.5844e-05, 1.1638e-04, 1.4701e-04, 4.6139e-05,\n",
      "          1.9565e-03, 8.7492e-04, 1.9065e-03, 1.7391e-03, 1.6335e-03,\n",
      "          2.0318e-03, 5.1392e-04, 3.0996e-03, 3.6352e-03, 4.8466e-04,\n",
      "          3.0882e-02, 2.6508e-03, 8.3397e-04, 9.3112e-04, 6.4880e-05,\n",
      "          7.1817e-01, 2.4875e-05, 6.8085e-04, 6.1654e-04, 3.0961e-03,\n",
      "          3.1665e-02, 1.0839e-03, 3.0761e-03, 3.7173e-03, 1.4192e-02,\n",
      "          6.4961e-02, 2.5338e-04, 6.2312e-04, 2.2269e-03, 2.6045e-02,\n",
      "          7.2017e-02, 7.3401e-04, 1.8142e-03, 1.2360e-03, 2.4974e-05,\n",
      "          1.6558e-05, 1.3284e-05, 1.6464e-05, 1.7731e-05, 1.6118e-05,\n",
      "          9.6101e-06],\n",
      "         [4.5753e-06, 1.0453e-05, 8.4801e-06, 1.0161e-05, 9.3175e-06,\n",
      "          7.9033e-06, 5.2866e-06, 3.2408e-05, 1.0345e-04, 3.9713e-05,\n",
      "          1.2370e-02, 3.2816e-03, 3.0096e-03, 2.3410e-03, 4.6464e-04,\n",
      "          1.5440e-02, 2.0303e-03, 2.2767e-03, 2.2874e-03, 4.1946e-04,\n",
      "          2.0509e-04, 1.0345e-04, 3.6220e-04, 5.0622e-04, 2.8203e-05,\n",
      "          4.2514e-02, 1.3699e-05, 2.8877e-04, 3.1317e-04, 1.9134e-02,\n",
      "          3.4312e-01, 3.4693e-04, 1.3860e-03, 1.8167e-03, 5.1495e-02,\n",
      "          4.7796e-01, 3.3268e-05, 3.0491e-05, 1.3690e-04, 4.5709e-03,\n",
      "          1.1147e-02, 9.7315e-05, 9.3181e-05, 7.7902e-05, 8.1942e-06,\n",
      "          9.7188e-06, 8.2409e-06, 1.0420e-05, 1.2588e-05, 6.8365e-06,\n",
      "          8.1534e-06],\n",
      "         [2.3151e-06, 5.1577e-06, 2.4304e-06, 3.4284e-06, 3.8543e-06,\n",
      "          3.7051e-06, 6.7100e-06, 3.0380e-06, 4.0605e-06, 3.0030e-06,\n",
      "          2.7395e-06, 6.7436e-06, 4.2074e-04, 3.2641e-04, 2.5885e-06,\n",
      "          7.3951e-04, 5.1666e-06, 1.7696e-04, 1.9468e-04, 1.6378e-06,\n",
      "          1.0660e-03, 1.9553e-06, 9.8809e-03, 2.3604e-01, 7.5101e-01,\n",
      "          2.5311e-06, 2.4876e-06, 2.8269e-06, 3.6967e-06, 3.0175e-06,\n",
      "          3.9170e-06, 5.1631e-06, 3.6483e-06, 9.5945e-07, 4.2686e-06,\n",
      "          3.4470e-06, 2.6258e-06, 2.3630e-06, 3.2114e-06, 4.8007e-06,\n",
      "          3.5457e-06, 1.6411e-06, 1.3783e-06, 4.0089e-06, 2.3034e-06,\n",
      "          2.4032e-06, 3.1932e-06, 4.5310e-06, 3.8324e-06, 3.5959e-06,\n",
      "          2.1611e-06],\n",
      "         [1.2371e-04, 1.9452e-04, 1.6959e-04, 1.7538e-04, 2.4939e-04,\n",
      "          2.6622e-04, 1.7299e-04, 3.7392e-04, 5.8903e-04, 8.4667e-04,\n",
      "          1.2084e-03, 1.2546e-03, 5.3346e-03, 9.0083e-03, 9.0197e-04,\n",
      "          2.5111e-03, 8.0667e-04, 4.1821e-02, 5.1419e-02, 3.6526e-03,\n",
      "          9.3449e-03, 1.2096e-03, 1.6922e-01, 1.5096e-01, 8.6055e-03,\n",
      "          3.5044e-01, 2.6359e-04, 4.4941e-03, 4.7277e-03, 6.6609e-04,\n",
      "          1.1596e-02, 5.3684e-03, 4.0774e-02, 4.3426e-02, 3.2989e-03,\n",
      "          8.0585e-03, 1.0890e-03, 1.0496e-02, 1.2825e-02, 3.5340e-03,\n",
      "          4.5196e-03, 6.2602e-03, 1.7366e-02, 8.8018e-03, 2.4900e-04,\n",
      "          2.3174e-04, 2.3600e-04, 2.9050e-04, 1.8810e-04, 1.8137e-04,\n",
      "          2.0601e-04]]])\n",
      "Player 1 Prediction: tensor([[9.2668e-02, 9.0731e-01, 1.8234e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[4.2992e-05, 4.3430e-05, 5.1824e-05, 5.8633e-05, 4.6749e-05,\n",
      "          5.1303e-05, 4.5058e-05, 7.0490e-04, 6.8654e-04, 2.4312e-04,\n",
      "          2.4109e-03, 8.7155e-04, 9.8252e-02, 1.1529e-01, 1.3329e-02,\n",
      "          3.6254e-04, 1.9013e-04, 8.5577e-02, 6.9673e-02, 6.2732e-03,\n",
      "          6.2044e-04, 2.4920e-04, 1.3674e-03, 3.2476e-03, 3.1458e-04,\n",
      "          1.5071e-01, 6.1793e-05, 6.0831e-04, 7.1826e-04, 3.0580e-04,\n",
      "          6.9656e-04, 1.0132e-02, 1.1855e-01, 1.1425e-01, 5.7635e-04,\n",
      "          8.5379e-04, 2.3139e-03, 2.0036e-02, 1.7129e-02, 3.4906e-04,\n",
      "          9.4703e-04, 1.5105e-02, 8.3490e-02, 6.2829e-02, 5.7873e-05,\n",
      "          4.7533e-05, 4.5201e-05, 4.7743e-05, 4.8257e-05, 5.5972e-05,\n",
      "          3.4101e-05],\n",
      "         [3.4202e-06, 5.5124e-06, 4.6112e-06, 9.5652e-06, 4.9144e-06,\n",
      "          4.0661e-06, 3.9383e-06, 7.9059e-07, 2.8714e-06, 2.4409e-05,\n",
      "          1.2005e-02, 9.5604e-04, 2.1799e-03, 1.4131e-03, 7.6369e-05,\n",
      "          9.2156e-03, 1.1009e-04, 2.7534e-05, 2.3611e-05, 1.0304e-04,\n",
      "          1.4117e-04, 3.5992e-05, 1.6235e-04, 1.5139e-04, 1.3857e-05,\n",
      "          2.9664e-03, 7.0970e-06, 2.8859e-05, 3.8308e-05, 2.6369e-04,\n",
      "          3.1469e-01, 1.5172e-04, 3.1628e-04, 4.5112e-04, 2.4481e-04,\n",
      "          6.4007e-01, 4.1647e-06, 2.0388e-05, 6.2618e-05, 2.0664e-03,\n",
      "          1.1867e-02, 2.2210e-05, 2.3006e-07, 2.0110e-07, 6.2909e-06,\n",
      "          7.7459e-06, 4.2241e-06, 6.0391e-06, 7.7220e-06, 7.2992e-06,\n",
      "          4.3804e-06],\n",
      "         [7.1816e-07, 1.1646e-06, 4.9627e-07, 8.9375e-07, 1.9787e-06,\n",
      "          9.8075e-07, 1.3683e-06, 7.3540e-07, 4.6662e-07, 6.5528e-07,\n",
      "          7.4365e-07, 1.0302e-06, 1.0006e-04, 6.0032e-05, 5.0511e-07,\n",
      "          2.4918e-05, 8.9174e-07, 1.2850e-04, 7.7128e-05, 7.5744e-07,\n",
      "          9.9953e-01, 6.4177e-07, 3.0627e-06, 2.3453e-05, 2.2317e-05,\n",
      "          7.2058e-07, 6.6057e-07, 4.9802e-07, 9.7206e-07, 4.9284e-07,\n",
      "          7.2311e-07, 9.4405e-07, 7.0514e-07, 4.1402e-07, 5.7129e-07,\n",
      "          8.5110e-07, 6.3111e-07, 6.1383e-07, 1.1272e-06, 4.3885e-07,\n",
      "          9.0437e-07, 4.6471e-07, 3.6808e-07, 1.0133e-06, 7.1970e-07,\n",
      "          6.7892e-07, 7.7424e-07, 1.0339e-06, 9.9710e-07, 1.1803e-06,\n",
      "          5.3681e-07],\n",
      "         [1.5855e-04, 1.4473e-04, 1.6252e-04, 2.3120e-04, 2.3918e-04,\n",
      "          2.3980e-04, 1.3990e-04, 6.9297e-04, 9.2394e-04, 5.1924e-04,\n",
      "          4.5814e-03, 3.0227e-03, 5.9955e-04, 8.4197e-04, 5.0585e-04,\n",
      "          3.6918e-03, 6.9740e-04, 5.3454e-02, 8.0627e-02, 3.5446e-03,\n",
      "          3.0455e-02, 1.6935e-03, 8.4796e-04, 1.1536e-03, 1.2345e-03,\n",
      "          7.0434e-01, 2.5693e-04, 1.1468e-04, 1.1135e-04, 6.5214e-04,\n",
      "          2.2438e-02, 1.6501e-03, 7.2205e-03, 5.8449e-03, 7.2259e-03,\n",
      "          1.6398e-02, 2.8463e-04, 1.3736e-03, 2.0736e-03, 8.6405e-03,\n",
      "          1.5317e-02, 3.9392e-03, 6.8902e-03, 3.4867e-03, 1.6116e-04,\n",
      "          2.4934e-04, 1.9360e-04, 1.9245e-04, 1.8333e-04, 1.2171e-04,\n",
      "          2.3652e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.5826e-01, 6.0060e-04, 3.4114e-01]])\n",
      "Player 0 Prediction: tensor([[[7.8128e-08, 1.1661e-07, 1.3368e-07, 7.0750e-08, 1.1451e-07,\n",
      "          1.0317e-07, 1.1200e-07, 1.3263e-05, 1.4461e-05, 4.4176e-07,\n",
      "          1.2645e-04, 2.0967e-06, 4.7941e-01, 4.7604e-01, 1.8835e-06,\n",
      "          6.0143e-05, 8.9666e-07, 2.3405e-05, 2.3697e-05, 1.1109e-06,\n",
      "          1.5140e-06, 2.2549e-06, 1.1451e-06, 9.1002e-07, 3.0122e-07,\n",
      "          4.3527e-02, 1.0182e-07, 2.5490e-07, 2.0640e-07, 9.2353e-07,\n",
      "          1.4585e-06, 2.0087e-06, 2.5583e-07, 2.4207e-07, 7.1047e-06,\n",
      "          4.9056e-06, 8.9402e-07, 2.4775e-04, 4.6032e-04, 6.2145e-06,\n",
      "          1.4130e-05, 1.4590e-06, 2.8467e-07, 4.3842e-07, 1.6168e-07,\n",
      "          3.6254e-08, 7.7944e-08, 1.2418e-07, 1.0743e-07, 1.2119e-07,\n",
      "          8.1692e-08],\n",
      "         [4.7097e-07, 9.7884e-07, 6.5180e-07, 1.0078e-06, 1.5188e-06,\n",
      "          8.6893e-07, 7.9278e-07, 4.6378e-01, 4.5885e-01, 8.8113e-06,\n",
      "          3.0323e-04, 2.0280e-05, 1.0930e-04, 8.9210e-05, 5.0797e-05,\n",
      "          1.0829e-05, 5.8762e-06, 1.2758e-03, 6.5461e-04, 5.5681e-05,\n",
      "          1.1240e-06, 1.3654e-06, 1.5455e-05, 2.2689e-05, 1.8322e-06,\n",
      "          6.1413e-02, 9.2170e-07, 1.3481e-05, 1.3302e-05, 4.8810e-06,\n",
      "          1.0916e-06, 1.9596e-04, 7.5353e-04, 7.5056e-04, 2.7895e-06,\n",
      "          4.2501e-06, 8.0428e-06, 4.4816e-03, 3.5277e-03, 1.0895e-06,\n",
      "          1.2349e-07, 3.5893e-05, 1.6752e-03, 1.8396e-03, 1.3436e-06,\n",
      "          8.8376e-07, 8.7996e-07, 1.3363e-06, 1.2272e-06, 5.0817e-07,\n",
      "          9.1550e-07],\n",
      "         [1.0912e-06, 2.0933e-06, 1.5976e-06, 1.7424e-06, 9.1424e-07,\n",
      "          2.8779e-06, 9.3163e-07, 1.7036e-06, 2.7202e-06, 1.5160e-06,\n",
      "          2.5186e-06, 2.1492e-06, 9.5956e-05, 1.1029e-04, 7.4203e-07,\n",
      "          2.3786e-04, 4.0477e-06, 5.0016e-01, 4.9919e-01, 1.8278e-06,\n",
      "          4.7110e-06, 2.0719e-06, 4.4302e-05, 5.1530e-05, 1.6767e-05,\n",
      "          1.7306e-06, 8.0929e-07, 2.0847e-06, 3.5679e-06, 2.5295e-06,\n",
      "          1.3629e-06, 2.7034e-06, 3.0633e-06, 9.6136e-07, 1.4841e-06,\n",
      "          3.8679e-06, 1.1303e-06, 1.2186e-06, 2.3380e-06, 1.8608e-06,\n",
      "          2.3264e-06, 2.3047e-06, 1.7799e-06, 1.9700e-06, 1.7054e-06,\n",
      "          2.1187e-06, 2.2204e-06, 1.1570e-06, 2.2973e-06, 2.0670e-06,\n",
      "          3.3875e-06],\n",
      "         [2.6741e-05, 2.8451e-05, 2.5378e-05, 2.2579e-05, 2.2080e-05,\n",
      "          2.1411e-05, 2.8795e-05, 5.1230e-04, 2.4609e-04, 1.7698e-04,\n",
      "          2.1285e-04, 2.0701e-04, 2.4387e-04, 3.4322e-04, 6.7550e-05,\n",
      "          3.0900e-04, 5.5557e-05, 2.2912e-01, 1.6760e-01, 2.6357e-03,\n",
      "          6.9332e-04, 1.2830e-04, 2.9280e-03, 8.7796e-03, 4.2982e-04,\n",
      "          5.5997e-01, 3.9136e-05, 3.3450e-04, 3.7303e-04, 9.6028e-05,\n",
      "          1.0109e-04, 1.3469e-03, 7.6988e-03, 8.1294e-03, 3.1892e-04,\n",
      "          4.5937e-04, 1.1898e-04, 6.7240e-04, 7.6833e-04, 1.9413e-04,\n",
      "          1.3391e-04, 7.7776e-04, 1.5482e-03, 1.8526e-03, 4.1131e-05,\n",
      "          2.2419e-05, 3.8063e-05, 1.5063e-05, 2.1243e-05, 2.7351e-05,\n",
      "          3.3008e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56352\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6007/10000 (60.1%)\n",
      "    Average reward: +0.280\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3993/10000 (39.9%)\n",
      "    Average reward: -0.280\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8074 (29.2%)\n",
      "    Action 1: 16992 (61.5%)\n",
      "    Action 2: 2315 (8.4%)\n",
      "    Action 3: 263 (1.0%)\n",
      "  Player 1:\n",
      "    Action 0: 8326 (29.0%)\n",
      "    Action 1: 14181 (49.4%)\n",
      "    Action 2: 3131 (10.9%)\n",
      "    Action 3: 3070 (10.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2803.0, -2803.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.950 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.985\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2803\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8627, 0.0034, 0.1339]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9666, 0.0042, 0.0293]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50165\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6125/10000 (61.3%)\n",
      "    Average reward: -0.026\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3875/10000 (38.8%)\n",
      "    Average reward: +0.026\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22898 (85.3%)\n",
      "    Action 1: 3943 (14.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2736 (11.7%)\n",
      "    Action 1: 17969 (77.0%)\n",
      "    Action 2: 839 (3.6%)\n",
      "    Action 3: 1780 (7.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-257.0, 257.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.602 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.653 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.627\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0257\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37004/50000 [36:27<09:57, 21.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 237024/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 239912/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38000/50000 [37:12<08:45, 22.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 243384/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 246253/2000000\n",
      "P1 SL Buffer Size:  243384\n",
      "P1 SL buffer distribution [ 89206. 123498.  19376.  11304.]\n",
      "P1 actions distribution [0.36652368 0.50742037 0.07961082 0.04644512]\n",
      "P2 SL Buffer Size:  246253\n",
      "P2 SL buffer distribution [ 76691. 124984.  20452.  24126.]\n",
      "P2 actions distribution [0.31143174 0.50754306 0.0830528  0.09797241]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.4011, 0.5958, 0.0031, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[9.5256e-06, 1.8736e-05, 2.0525e-05, 2.1437e-05, 3.4816e-05,\n",
      "          3.1058e-05, 2.8497e-05, 4.8579e-05, 4.2722e-05, 4.4170e-05,\n",
      "          9.3876e-04, 3.2903e-04, 4.8327e-05, 5.2114e-05, 2.9857e-05,\n",
      "          1.2133e-02, 9.1439e-04, 1.4695e-03, 2.6115e-03, 1.5010e-03,\n",
      "          5.0503e-02, 2.5337e-03, 7.7544e-04, 8.1756e-04, 6.1267e-05,\n",
      "          4.8591e-01, 8.2571e-05, 1.8697e-03, 9.6288e-04, 1.1253e-02,\n",
      "          2.1970e-01, 1.0919e-03, 1.3027e-03, 8.1614e-04, 7.7612e-03,\n",
      "          7.5884e-02, 5.7305e-04, 5.4542e-04, 7.3151e-04, 1.7582e-02,\n",
      "          9.1769e-02, 9.4880e-04, 4.1808e-03, 1.8461e-03, 2.2978e-05,\n",
      "          2.4615e-05, 2.6401e-05, 2.7444e-05, 2.7877e-05, 2.9329e-05,\n",
      "          1.3725e-05],\n",
      "         [2.8162e-05, 4.0262e-05, 6.4898e-05, 4.5908e-05, 8.1499e-05,\n",
      "          1.0134e-04, 8.5016e-05, 1.3960e-04, 1.4641e-04, 1.4107e-04,\n",
      "          2.5654e-04, 3.1457e-04, 7.9802e-04, 1.3675e-03, 7.1075e-04,\n",
      "          1.3223e-03, 5.1606e-04, 5.8243e-02, 1.1158e-01, 2.0918e-02,\n",
      "          3.8337e-03, 6.0459e-04, 8.3794e-04, 8.9097e-04, 1.1980e-04,\n",
      "          4.1074e-01, 2.2977e-04, 3.1758e-03, 5.6421e-04, 3.5554e-03,\n",
      "          1.9546e-02, 2.0636e-02, 8.8827e-02, 7.2677e-02, 1.1672e-03,\n",
      "          4.0397e-03, 1.8045e-02, 3.7835e-02, 2.1124e-02, 3.3507e-03,\n",
      "          7.0641e-03, 1.5600e-02, 4.1069e-02, 2.7223e-02, 4.8900e-05,\n",
      "          4.8164e-05, 6.8893e-05, 4.7776e-05, 5.1259e-05, 5.5513e-05,\n",
      "          2.0722e-05],\n",
      "         [1.3241e-06, 9.5648e-07, 1.0196e-06, 1.5215e-06, 1.4175e-06,\n",
      "          1.7399e-06, 1.5848e-06, 1.5797e-06, 1.2163e-06, 1.4136e-06,\n",
      "          1.3844e-06, 9.1671e-07, 9.8565e-05, 1.2478e-04, 7.1981e-07,\n",
      "          1.3414e-03, 1.0065e-06, 1.0611e-04, 1.1692e-04, 1.3425e-06,\n",
      "          4.7931e-04, 1.0418e-06, 4.7431e-01, 5.2324e-01, 1.2854e-04,\n",
      "          3.3744e-06, 1.8363e-06, 1.6148e-06, 1.3015e-06, 1.4629e-06,\n",
      "          8.5890e-07, 1.5047e-06, 1.3234e-06, 5.3866e-07, 1.7132e-06,\n",
      "          6.0818e-07, 1.1913e-06, 9.2182e-07, 1.5879e-06, 1.9259e-06,\n",
      "          9.6960e-07, 1.1804e-06, 2.7942e-06, 2.2103e-06, 1.1595e-06,\n",
      "          1.5565e-06, 1.6479e-06, 8.7644e-07, 8.8217e-07, 1.4055e-06,\n",
      "          1.6166e-06],\n",
      "         [1.8174e-04, 2.0892e-04, 1.6211e-04, 2.2046e-04, 1.2386e-04,\n",
      "          2.9452e-04, 1.3247e-04, 4.0458e-04, 6.8455e-04, 6.3601e-04,\n",
      "          2.7197e-03, 5.4410e-04, 8.0302e-04, 8.8107e-04, 4.2730e-04,\n",
      "          1.4064e-03, 4.7843e-04, 2.7113e-02, 2.5030e-02, 1.3464e-02,\n",
      "          3.0176e-02, 2.6294e-03, 1.7080e-02, 1.9828e-02, 4.1103e-04,\n",
      "          3.7321e-01, 3.9701e-04, 4.7253e-02, 1.7845e-02, 9.1393e-03,\n",
      "          6.5329e-02, 1.1548e-02, 9.9412e-02, 4.4812e-02, 9.1956e-04,\n",
      "          2.0123e-02, 9.1592e-03, 8.3882e-03, 1.0654e-02, 1.5344e-02,\n",
      "          7.4795e-02, 7.9322e-03, 2.3702e-02, 1.2646e-02, 2.0728e-04,\n",
      "          2.3429e-04, 2.0594e-04, 1.4789e-04, 2.7477e-04, 1.8455e-04,\n",
      "          9.1350e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5190, 0.0610, 0.4200]])\n",
      "Player 1 Prediction: tensor([[[1.3975e-06, 1.0957e-06, 3.0006e-06, 3.2525e-06, 6.4365e-06,\n",
      "          3.2205e-06, 3.7917e-06, 4.4690e-04, 3.4951e-04, 1.0063e-05,\n",
      "          4.3799e-03, 2.6959e-05, 1.4082e-04, 1.8989e-04, 3.1231e-06,\n",
      "          1.9050e-01, 3.1586e-05, 2.4901e-05, 1.0996e-04, 3.2273e-04,\n",
      "          2.4998e-03, 1.0543e-04, 6.4206e-05, 1.0777e-04, 7.0900e-06,\n",
      "          7.4491e-01, 5.1456e-06, 1.7823e-04, 4.0044e-05, 5.8935e-04,\n",
      "          2.0047e-02, 1.6703e-04, 3.0993e-06, 2.0002e-06, 3.2461e-04,\n",
      "          3.1267e-02, 3.2962e-04, 1.5617e-05, 1.6322e-05, 8.0833e-04,\n",
      "          6.1549e-04, 4.5075e-04, 5.6177e-04, 3.1533e-04, 2.0280e-06,\n",
      "          2.7788e-06, 3.5148e-06, 3.5788e-06, 2.5098e-06, 2.1786e-06,\n",
      "          8.8103e-07],\n",
      "         [3.2801e-05, 3.5298e-05, 4.3538e-05, 4.0175e-05, 9.1992e-05,\n",
      "          6.4631e-05, 7.1875e-05, 7.2042e-04, 4.5193e-04, 4.9613e-05,\n",
      "          1.6383e-03, 4.4257e-04, 8.7494e-04, 1.8223e-03, 2.5256e-04,\n",
      "          1.8234e-01, 3.2297e-04, 9.5457e-03, 1.4944e-02, 7.3233e-03,\n",
      "          2.0462e-02, 7.8349e-04, 1.2980e-03, 1.4270e-03, 1.1101e-04,\n",
      "          3.2989e-01, 2.7180e-04, 7.4224e-03, 2.6380e-03, 4.5475e-03,\n",
      "          3.7720e-01, 6.9637e-03, 6.0309e-03, 3.3511e-03, 4.7041e-04,\n",
      "          6.4313e-03, 2.0875e-03, 4.2957e-04, 4.3238e-04, 3.4884e-03,\n",
      "          2.5701e-03, 1.6771e-04, 1.7066e-05, 1.4942e-05, 4.9426e-05,\n",
      "          4.7311e-05, 8.3149e-05, 5.6478e-05, 5.3964e-05, 5.3231e-05,\n",
      "          3.1467e-05],\n",
      "         [3.2859e-07, 3.4692e-07, 4.8647e-07, 5.7880e-07, 3.8285e-07,\n",
      "          5.7547e-07, 7.1054e-07, 5.4126e-07, 3.2595e-07, 4.2059e-07,\n",
      "          3.4912e-07, 3.5142e-07, 1.1124e-04, 9.7526e-05, 2.3701e-07,\n",
      "          1.5239e-03, 3.5189e-07, 3.0300e-05, 2.5315e-05, 3.7870e-07,\n",
      "          9.9720e-01, 4.6205e-07, 9.4801e-05, 8.8307e-04, 1.6111e-05,\n",
      "          9.0529e-07, 2.4912e-07, 3.6971e-07, 4.7091e-07, 3.0159e-07,\n",
      "          3.0286e-07, 4.8205e-07, 5.7758e-07, 2.6819e-07, 9.9980e-07,\n",
      "          3.9748e-07, 2.4826e-07, 2.8221e-07, 3.9404e-07, 5.2162e-07,\n",
      "          3.5741e-07, 5.5295e-07, 8.1418e-07, 1.2630e-06, 4.1008e-07,\n",
      "          5.4375e-07, 4.0843e-07, 3.9439e-07, 4.1519e-07, 5.2401e-07,\n",
      "          8.0393e-07],\n",
      "         [3.8446e-06, 5.1287e-06, 3.6404e-06, 5.0595e-06, 4.0836e-06,\n",
      "          5.3323e-06, 4.7560e-06, 6.9377e-06, 4.5360e-06, 1.2808e-05,\n",
      "          1.4042e-04, 1.3162e-05, 1.2700e-05, 1.3312e-05, 7.7644e-06,\n",
      "          1.9079e-05, 1.0689e-05, 5.3820e-04, 5.6895e-04, 5.1610e-04,\n",
      "          2.0400e-01, 1.0795e-04, 1.1934e-04, 1.3727e-04, 9.6914e-06,\n",
      "          6.2088e-01, 7.3518e-06, 1.5289e-04, 5.8345e-05, 9.8810e-04,\n",
      "          1.6483e-01, 1.6543e-04, 4.9291e-05, 3.1049e-05, 2.2635e-05,\n",
      "          9.3005e-04, 1.2369e-04, 5.9171e-05, 4.3194e-05, 1.0488e-03,\n",
      "          4.2925e-03, 1.7649e-05, 5.3357e-06, 3.2953e-06, 3.6164e-06,\n",
      "          4.3698e-06, 3.6391e-06, 3.0203e-06, 7.5617e-06, 6.3038e-06,\n",
      "          1.3986e-06]]])\n",
      "Player 0 Prediction: tensor([[0.0385, 0.1068, 0.8548, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38000/50000 [37:25<08:45, 22.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57058\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5365/10000 (53.6%)\n",
      "    Average reward: -0.441\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4635/10000 (46.4%)\n",
      "    Average reward: +0.441\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10172 (35.4%)\n",
      "    Action 1: 14975 (52.1%)\n",
      "    Action 2: 2135 (7.4%)\n",
      "    Action 3: 1479 (5.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6903 (24.4%)\n",
      "    Action 1: 14361 (50.8%)\n",
      "    Action 2: 3141 (11.1%)\n",
      "    Action 3: 3892 (13.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4413.5, 4413.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.993 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.007\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4414\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.4015e-01, 2.7671e-04, 5.9576e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7963, 0.0048, 0.1990]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52035\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5338/10000 (53.4%)\n",
      "    Average reward: +0.207\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4662/10000 (46.6%)\n",
      "    Average reward: -0.207\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3315 (13.3%)\n",
      "    Action 1: 16048 (64.4%)\n",
      "    Action 2: 2182 (8.8%)\n",
      "    Action 3: 3377 (13.6%)\n",
      "  Player 1:\n",
      "    Action 0: 21047 (77.6%)\n",
      "    Action 1: 6066 (22.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2066.0, -2066.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.796 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.767 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.781\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2066\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[2.1038e-05, 3.9151e-05, 4.1936e-05, 4.2018e-05, 4.5062e-05,\n",
      "          3.7951e-05, 5.0856e-05, 7.1999e-04, 1.1476e-03, 3.7757e-04,\n",
      "          4.4234e-03, 2.2878e-03, 1.6907e-02, 2.0818e-02, 5.9703e-03,\n",
      "          1.0015e-02, 9.1558e-04, 1.4057e-02, 1.2109e-02, 2.3541e-03,\n",
      "          1.5902e-03, 6.1664e-04, 8.9005e-04, 9.4387e-04, 1.1847e-04,\n",
      "          4.2221e-01, 5.5326e-05, 1.9275e-03, 1.8953e-03, 1.5169e-03,\n",
      "          1.1184e-02, 2.6691e-02, 8.5300e-02, 8.3567e-02, 1.5192e-02,\n",
      "          1.2757e-01, 1.1263e-02, 2.9218e-02, 2.7883e-02, 4.6049e-03,\n",
      "          1.5889e-02, 7.7409e-03, 1.8212e-02, 1.1238e-02, 4.7358e-05,\n",
      "          6.9007e-05, 3.4653e-05, 4.9782e-05, 3.4154e-05, 3.9938e-05,\n",
      "          2.5923e-05],\n",
      "         [8.2813e-06, 1.0474e-05, 1.2509e-05, 1.0824e-05, 1.1717e-05,\n",
      "          1.0687e-05, 1.4375e-05, 1.4695e-04, 5.7592e-04, 1.0532e-04,\n",
      "          1.9020e-02, 4.4691e-03, 1.0030e-03, 7.8954e-04, 2.3340e-04,\n",
      "          1.3272e-02, 1.2637e-03, 6.2904e-03, 5.8462e-03, 7.6506e-04,\n",
      "          1.1268e-03, 1.4528e-04, 1.5126e-04, 1.3321e-04, 2.5797e-05,\n",
      "          4.3725e-02, 1.8060e-05, 3.0629e-04, 3.1259e-04, 1.4306e-02,\n",
      "          2.3053e-01, 1.9599e-03, 9.8892e-03, 1.5269e-02, 6.0297e-02,\n",
      "          5.2233e-01, 1.6909e-04, 3.0688e-04, 1.0010e-03, 8.7154e-03,\n",
      "          3.4666e-02, 1.9626e-04, 2.6214e-04, 2.1075e-04, 1.3455e-05,\n",
      "          1.1349e-05, 9.9096e-06, 1.2947e-05, 1.1665e-05, 1.6303e-05,\n",
      "          6.6940e-06],\n",
      "         [3.8123e-06, 6.2346e-06, 3.6734e-06, 5.9672e-06, 5.4231e-06,\n",
      "          8.0045e-06, 5.8182e-06, 3.9867e-06, 4.8635e-06, 7.4685e-06,\n",
      "          4.3270e-06, 6.4153e-06, 1.2389e-03, 1.1469e-03, 4.1838e-06,\n",
      "          6.5695e-04, 6.9019e-06, 3.9156e-04, 4.2804e-04, 2.7962e-06,\n",
      "          5.1418e-03, 2.5801e-06, 3.6898e-03, 2.0338e-01, 7.8374e-01,\n",
      "          3.1136e-06, 3.8129e-06, 3.0179e-06, 3.2867e-06, 3.4832e-06,\n",
      "          3.6471e-06, 2.9107e-06, 5.3694e-06, 2.6846e-06, 6.5405e-06,\n",
      "          4.0353e-06, 2.7328e-06, 3.2490e-06, 3.5401e-06, 6.1432e-06,\n",
      "          6.5276e-06, 3.0643e-06, 4.8866e-06, 6.1576e-06, 3.9344e-06,\n",
      "          2.7031e-06, 3.6342e-06, 5.0746e-06, 4.3625e-06, 3.1952e-06,\n",
      "          3.3631e-06],\n",
      "         [1.0400e-04, 1.8979e-04, 2.3265e-04, 1.3244e-04, 3.1459e-04,\n",
      "          2.1621e-04, 1.8562e-04, 3.8547e-04, 5.4495e-04, 7.9744e-04,\n",
      "          4.8035e-03, 5.1235e-03, 1.2412e-02, 1.5598e-02, 9.0150e-04,\n",
      "          9.7265e-03, 8.6963e-04, 2.7904e-02, 3.3624e-02, 4.4888e-03,\n",
      "          1.8009e-03, 1.3433e-03, 2.5732e-02, 2.1703e-02, 4.1724e-03,\n",
      "          4.6116e-01, 2.4305e-04, 7.7008e-03, 8.6428e-03, 5.8902e-04,\n",
      "          3.0701e-03, 1.7229e-02, 1.1862e-01, 1.1474e-01, 8.9850e-03,\n",
      "          2.8733e-02, 1.4290e-03, 1.4492e-02, 2.1221e-02, 3.1757e-03,\n",
      "          4.9418e-03, 2.5202e-03, 4.6072e-03, 2.9971e-03, 1.9566e-04,\n",
      "          3.3586e-04, 1.9860e-04, 2.1180e-04, 1.7456e-04, 2.1391e-04,\n",
      "          2.6882e-04]]])\n",
      "Player 1 Prediction: tensor([[8.6147e-02, 9.1384e-01, 1.2393e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[1.7436e-05, 2.1653e-05, 1.9910e-05, 2.0242e-05, 2.1001e-05,\n",
      "          1.7147e-05, 2.4627e-05, 7.1083e-04, 8.5213e-04, 2.5019e-04,\n",
      "          2.4771e-03, 8.6022e-04, 1.6894e-02, 1.9277e-02, 3.5801e-03,\n",
      "          6.1484e-04, 9.8399e-05, 3.2547e-02, 2.6852e-02, 4.0998e-03,\n",
      "          3.4421e-05, 3.1853e-05, 2.7957e-04, 7.9915e-04, 1.3011e-04,\n",
      "          3.8294e-01, 3.0172e-05, 3.6146e-04, 3.7702e-04, 5.5569e-05,\n",
      "          1.8493e-04, 1.6617e-02, 1.5225e-01, 1.5817e-01, 3.8758e-04,\n",
      "          3.0548e-03, 5.5451e-03, 4.6042e-02, 4.0123e-02, 9.4518e-05,\n",
      "          1.0202e-03, 1.0783e-02, 3.9959e-02, 3.1326e-02, 2.0988e-05,\n",
      "          3.0498e-05, 2.1618e-05, 2.0110e-05, 1.4967e-05, 2.2471e-05,\n",
      "          1.6688e-05],\n",
      "         [3.8223e-06, 4.1003e-06, 4.9537e-06, 7.2383e-06, 5.5966e-06,\n",
      "          4.5730e-06, 6.0287e-06, 1.5762e-06, 6.0595e-06, 4.8818e-05,\n",
      "          2.0547e-02, 8.8722e-04, 1.0500e-03, 7.4805e-04, 3.7155e-05,\n",
      "          6.2904e-03, 5.9661e-05, 2.0317e-05, 1.6338e-05, 8.9518e-05,\n",
      "          3.1688e-04, 3.2751e-05, 6.0511e-05, 5.5744e-05, 1.1094e-05,\n",
      "          1.0816e-02, 6.3116e-06, 2.6274e-05, 3.6369e-05, 1.3843e-04,\n",
      "          3.1134e-01, 4.8536e-04, 1.1155e-03, 2.0724e-03, 2.1317e-04,\n",
      "          5.9350e-01, 1.3253e-05, 9.5965e-05, 3.2253e-04, 3.5811e-03,\n",
      "          4.5846e-02, 3.5204e-05, 8.3245e-07, 9.2202e-07, 7.0512e-06,\n",
      "          5.7241e-06, 3.7957e-06, 5.5033e-06, 4.9635e-06, 7.5673e-06,\n",
      "          4.1096e-06],\n",
      "         [1.8411e-07, 1.7730e-07, 8.4381e-08, 2.1245e-07, 3.1916e-07,\n",
      "          2.5582e-07, 2.0272e-07, 1.9917e-07, 7.0109e-08, 2.1438e-07,\n",
      "          1.3714e-07, 1.4459e-07, 4.7637e-05, 2.6372e-05, 9.5325e-08,\n",
      "          1.0526e-05, 1.6068e-07, 4.7565e-05, 2.4194e-05, 1.5111e-07,\n",
      "          9.9983e-01, 1.1440e-07, 2.8098e-07, 3.3578e-06, 2.1628e-06,\n",
      "          1.5072e-07, 1.4947e-07, 1.0206e-07, 1.6366e-07, 9.3645e-08,\n",
      "          1.0740e-07, 1.1938e-07, 1.2455e-07, 1.3538e-07, 1.0343e-07,\n",
      "          1.5386e-07, 9.9204e-08, 1.0340e-07, 1.3968e-07, 8.5577e-08,\n",
      "          2.2699e-07, 1.1609e-07, 1.1307e-07, 2.1278e-07, 1.2820e-07,\n",
      "          1.1729e-07, 1.1276e-07, 1.5825e-07, 1.4175e-07, 1.8498e-07,\n",
      "          1.1751e-07],\n",
      "         [1.0673e-04, 9.6480e-05, 1.2623e-04, 1.3338e-04, 1.8953e-04,\n",
      "          1.3317e-04, 9.6423e-05, 5.1000e-04, 7.0916e-04, 3.5613e-04,\n",
      "          1.4939e-02, 6.5536e-03, 8.1489e-04, 8.3702e-04, 3.4781e-04,\n",
      "          1.2462e-02, 5.9526e-04, 1.6185e-02, 2.6820e-02, 2.1800e-03,\n",
      "          5.1706e-03, 1.4310e-03, 1.6335e-04, 1.9270e-04, 5.1739e-04,\n",
      "          7.4290e-01, 1.5407e-04, 1.3313e-04, 1.4420e-04, 4.9958e-04,\n",
      "          1.3416e-02, 3.0350e-03, 1.4657e-02, 1.4663e-02, 1.9296e-02,\n",
      "          6.3055e-02, 2.5368e-04, 1.1436e-03, 2.4484e-03, 1.0483e-02,\n",
      "          1.6135e-02, 1.4410e-03, 2.2352e-03, 1.2591e-03, 1.0083e-04,\n",
      "          2.1859e-04, 1.0813e-04, 1.2934e-04, 1.2793e-04, 8.3253e-05,\n",
      "          2.1261e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 7.4106e-01, 4.0484e-04, 2.5854e-01]])\n",
      "Player 0 Prediction: tensor([[[6.8467e-08, 1.2488e-07, 1.5031e-07, 7.4128e-08, 1.1682e-07,\n",
      "          1.1912e-07, 1.1315e-07, 1.8711e-05, 2.1446e-05, 6.9443e-07,\n",
      "          2.1790e-04, 1.5453e-06, 4.0388e-01, 4.0096e-01, 1.5487e-06,\n",
      "          1.6089e-04, 1.0661e-06, 4.6647e-05, 4.3893e-05, 1.3712e-06,\n",
      "          2.5357e-07, 5.2079e-07, 9.4645e-07, 6.9025e-07, 2.4573e-07,\n",
      "          1.8956e-01, 8.6811e-08, 3.4211e-07, 2.6626e-07, 5.7793e-07,\n",
      "          6.3727e-07, 6.6681e-06, 4.5621e-06, 4.3278e-06, 4.7870e-06,\n",
      "          8.3307e-06, 4.6010e-06, 2.5678e-03, 2.4690e-03, 1.3416e-06,\n",
      "          1.9401e-06, 3.6368e-06, 2.6354e-07, 3.5917e-07, 1.2444e-07,\n",
      "          5.5721e-08, 6.6424e-08, 9.2878e-08, 1.0931e-07, 1.2218e-07,\n",
      "          7.8205e-08],\n",
      "         [5.5639e-07, 8.4332e-07, 7.8889e-07, 6.0219e-07, 9.5130e-07,\n",
      "          9.5972e-07, 7.2304e-07, 4.2887e-01, 4.2735e-01, 1.7822e-05,\n",
      "          3.6943e-04, 2.4676e-05, 7.4662e-05, 6.2278e-05, 2.9323e-05,\n",
      "          8.1651e-06, 4.9595e-06, 2.7834e-03, 1.7532e-03, 1.2867e-04,\n",
      "          3.3910e-06, 1.3106e-06, 6.4863e-06, 7.2221e-06, 1.1289e-06,\n",
      "          6.5562e-02, 5.4736e-07, 9.5278e-06, 9.9120e-06, 3.1568e-06,\n",
      "          1.6313e-07, 6.2982e-04, 1.5501e-03, 2.1522e-03, 3.7922e-06,\n",
      "          3.9093e-06, 1.9669e-05, 2.9207e-02, 3.3220e-02, 1.0826e-06,\n",
      "          4.4764e-07, 2.3025e-05, 3.0249e-03, 3.0778e-03, 1.0419e-06,\n",
      "          5.1554e-07, 6.5803e-07, 1.1662e-06, 7.3806e-07, 5.6952e-07,\n",
      "          6.7616e-07],\n",
      "         [7.2110e-07, 9.1170e-07, 8.1368e-07, 9.5631e-07, 5.0272e-07,\n",
      "          1.9001e-06, 5.3426e-07, 6.9917e-07, 1.3504e-06, 1.3688e-06,\n",
      "          1.4284e-06, 1.1934e-06, 9.5861e-05, 9.6723e-05, 4.7609e-07,\n",
      "          1.3383e-04, 1.7813e-06, 4.9961e-01, 4.9990e-01, 1.2878e-06,\n",
      "          4.8088e-06, 1.1011e-06, 2.7663e-05, 6.5250e-05, 1.6758e-05,\n",
      "          9.9116e-07, 7.3298e-07, 1.2473e-06, 1.9370e-06, 1.9195e-06,\n",
      "          7.2991e-07, 1.0993e-06, 2.0415e-06, 9.7610e-07, 9.7439e-07,\n",
      "          2.2408e-06, 8.7536e-07, 6.6147e-07, 1.2696e-06, 1.2485e-06,\n",
      "          1.3233e-06, 1.3523e-06, 1.6840e-06, 1.1307e-06, 9.5366e-07,\n",
      "          8.7519e-07, 1.1964e-06, 5.3876e-07, 1.8125e-06, 5.0592e-07,\n",
      "          2.2655e-06],\n",
      "         [1.7976e-05, 2.1732e-05, 1.9583e-05, 1.2336e-05, 1.6866e-05,\n",
      "          1.0163e-05, 2.9497e-05, 3.1828e-04, 1.4455e-04, 1.4704e-04,\n",
      "          2.7443e-04, 2.4717e-04, 3.9673e-04, 4.7374e-04, 4.7932e-05,\n",
      "          2.4797e-04, 4.5451e-05, 9.5201e-02, 7.7518e-02, 2.8275e-03,\n",
      "          2.0895e-04, 9.8429e-05, 7.7002e-04, 2.4828e-03, 1.8713e-04,\n",
      "          7.6861e-01, 1.8464e-05, 7.1322e-04, 6.4547e-04, 5.3322e-05,\n",
      "          2.3634e-05, 2.1924e-03, 2.1586e-02, 1.9291e-02, 3.5899e-04,\n",
      "          5.0302e-04, 9.1554e-05, 1.0939e-03, 1.1669e-03, 1.0213e-04,\n",
      "          6.1705e-05, 3.2487e-04, 5.1968e-04, 7.4763e-04, 1.8423e-05,\n",
      "          2.4930e-05, 1.7918e-05, 1.1881e-05, 1.2347e-05, 2.8485e-05,\n",
      "          1.7007e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55556\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5862/10000 (58.6%)\n",
      "    Average reward: +0.266\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4138/10000 (41.4%)\n",
      "    Average reward: -0.266\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9696 (35.1%)\n",
      "    Action 1: 14948 (54.1%)\n",
      "    Action 2: 2412 (8.7%)\n",
      "    Action 3: 582 (2.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6995 (25.1%)\n",
      "    Action 1: 15018 (53.8%)\n",
      "    Action 2: 2602 (9.3%)\n",
      "    Action 3: 3303 (11.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2660.0, -2660.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.010 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.981 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.996\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2660\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2883, 0.7105, 0.0012, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8531, 0.0046, 0.1423]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49869\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6070/10000 (60.7%)\n",
      "    Average reward: -0.073\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3930/10000 (39.3%)\n",
      "    Average reward: +0.073\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22987 (85.9%)\n",
      "    Action 1: 3764 (14.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2669 (11.5%)\n",
      "    Action 1: 18015 (77.9%)\n",
      "    Action 2: 738 (3.2%)\n",
      "    Action 3: 1696 (7.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-730.0, 730.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.586 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.640 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.613\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0730\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39003/50000 [38:36<08:02, 22.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 250061/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 252887/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39999/50000 [39:20<07:29, 22.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 256432/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 259639/2000000\n",
      "P1 SL Buffer Size:  256432\n",
      "P1 SL buffer distribution [ 93166. 131118.  20570.  11578.]\n",
      "P1 actions distribution [0.36331659 0.51131684 0.0802162  0.04515037]\n",
      "P2 SL Buffer Size:  259639\n",
      "P2 SL buffer distribution [ 80313. 130912.  21924.  26490.]\n",
      "P2 actions distribution [0.30932564 0.50420777 0.08444032 0.10202627]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[4.3974e-05, 1.0423e-04, 8.4328e-05, 1.1244e-04, 1.3337e-04,\n",
      "          1.4384e-04, 1.5704e-04, 5.0400e-04, 5.2887e-04, 4.0581e-04,\n",
      "          4.0942e-04, 3.0995e-04, 9.2278e-04, 1.2503e-03, 6.1345e-04,\n",
      "          4.2625e-04, 3.1643e-04, 1.8133e-02, 2.7177e-02, 5.0746e-03,\n",
      "          1.9385e-03, 8.4062e-04, 5.3167e-03, 5.9919e-03, 2.9178e-04,\n",
      "          1.7863e-01, 6.6853e-04, 1.7769e-02, 3.5407e-03, 3.0638e-03,\n",
      "          1.2889e-02, 6.4827e-02, 2.4165e-01, 1.9783e-01, 7.7677e-04,\n",
      "          6.8076e-03, 2.2129e-02, 3.5093e-02, 1.5717e-02, 2.0936e-03,\n",
      "          8.4065e-03, 2.8498e-02, 5.5300e-02, 3.2458e-02, 1.0147e-04,\n",
      "          1.1502e-04, 1.0672e-04, 8.6650e-05, 9.0614e-05, 7.8046e-05,\n",
      "          4.3088e-05],\n",
      "         [7.0557e-05, 1.1368e-04, 1.3812e-04, 1.2479e-04, 1.7302e-04,\n",
      "          1.7347e-04, 1.7691e-04, 5.4712e-04, 6.1290e-04, 3.6710e-04,\n",
      "          1.4022e-03, 6.9569e-04, 8.4041e-04, 1.1571e-03, 7.5485e-04,\n",
      "          8.8061e-03, 1.3529e-03, 9.5616e-03, 1.6649e-02, 3.6627e-03,\n",
      "          5.6331e-03, 1.2126e-03, 8.3108e-03, 9.3600e-03, 4.0217e-04,\n",
      "          1.3524e-01, 9.8973e-04, 2.2390e-02, 5.9126e-03, 1.2990e-02,\n",
      "          1.8467e-01, 4.2848e-02, 1.3812e-01, 1.1862e-01, 7.0292e-03,\n",
      "          6.2498e-02, 1.8466e-02, 2.4586e-02, 1.1698e-02, 7.5908e-03,\n",
      "          2.6645e-02, 1.8024e-02, 5.4619e-02, 3.3927e-02, 1.2953e-04,\n",
      "          1.2047e-04, 1.4933e-04, 1.2351e-04, 1.2677e-04, 1.1670e-04,\n",
      "          7.2535e-05],\n",
      "         [4.9106e-06, 5.3281e-06, 6.9658e-06, 9.5793e-06, 6.0135e-06,\n",
      "          1.1542e-05, 5.6760e-06, 9.8814e-06, 1.0402e-05, 4.9710e-06,\n",
      "          3.0666e-06, 5.5491e-06, 1.2210e-03, 1.0437e-03, 7.1318e-06,\n",
      "          7.6002e-04, 4.6871e-06, 7.9764e-04, 7.2712e-04, 8.6893e-06,\n",
      "          5.0329e-04, 7.0799e-06, 6.0741e-03, 2.4612e-01, 7.4247e-01,\n",
      "          9.2386e-06, 8.6527e-06, 9.0982e-06, 4.8683e-06, 8.9674e-06,\n",
      "          7.7610e-06, 7.0715e-06, 7.0240e-06, 4.5062e-06, 6.0767e-06,\n",
      "          6.5174e-06, 8.4567e-06, 5.3573e-06, 5.3960e-06, 7.9813e-06,\n",
      "          5.9469e-06, 6.6267e-06, 4.6187e-06, 5.6635e-06, 4.0819e-06,\n",
      "          9.2988e-06, 6.2503e-06, 6.7242e-06, 4.8928e-06, 5.7244e-06,\n",
      "          5.2340e-06],\n",
      "         [1.2883e-04, 3.2190e-04, 3.8080e-04, 2.4897e-04, 4.1931e-04,\n",
      "          2.2101e-04, 2.6277e-04, 5.8460e-04, 7.2520e-04, 5.3787e-04,\n",
      "          1.9193e-03, 7.9217e-04, 9.0538e-04, 1.5642e-03, 5.4663e-04,\n",
      "          1.2843e-03, 3.4173e-04, 1.5833e-02, 2.5164e-02, 4.2297e-03,\n",
      "          4.9565e-04, 1.0787e-03, 1.7476e-02, 1.7167e-02, 2.9128e-04,\n",
      "          1.4929e-01, 5.8296e-04, 4.6275e-02, 1.5754e-02, 5.0559e-03,\n",
      "          7.8302e-03, 8.9956e-02, 2.3864e-01, 1.4987e-01, 9.7375e-04,\n",
      "          1.3164e-02, 1.2027e-02, 2.2884e-02, 1.8458e-02, 5.0288e-03,\n",
      "          1.0866e-02, 3.3991e-02, 5.2979e-02, 3.1712e-02, 3.0591e-04,\n",
      "          2.3176e-04, 2.3410e-04, 2.9247e-04, 1.8610e-04, 2.9653e-04,\n",
      "          1.9729e-04]]])\n",
      "Player 0 Prediction: tensor([[0.8481, 0.1497, 0.0022, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[9.0584e-06, 1.0619e-05, 1.2571e-05, 1.5627e-05, 1.9586e-05,\n",
      "          1.5104e-05, 1.6669e-05, 5.2691e-04, 4.1843e-04, 6.4244e-05,\n",
      "          1.8913e-03, 1.2241e-04, 9.1432e-05, 1.6073e-04, 3.2094e-05,\n",
      "          7.1268e-02, 8.5677e-05, 2.6402e-05, 1.0509e-04, 4.5574e-04,\n",
      "          1.8116e-03, 1.8196e-04, 3.1042e-04, 7.4176e-04, 2.5229e-05,\n",
      "          5.3568e-01, 2.9281e-05, 1.3199e-03, 2.1460e-04, 1.9652e-03,\n",
      "          2.7320e-02, 2.4688e-03, 2.0776e-04, 1.3886e-04, 4.6057e-04,\n",
      "          2.9002e-01, 3.0301e-03, 1.2113e-03, 8.9627e-04, 1.3672e-03,\n",
      "          2.6278e-02, 2.5497e-03, 1.6095e-02, 1.0251e-02, 1.2037e-05,\n",
      "          1.7614e-05, 1.6827e-05, 1.3038e-05, 1.2966e-05, 8.3702e-06,\n",
      "          4.8760e-06],\n",
      "         [4.4397e-05, 4.9576e-05, 4.7879e-05, 5.3263e-05, 9.5049e-05,\n",
      "          7.3422e-05, 9.1092e-05, 3.4404e-04, 2.3084e-04, 9.6502e-05,\n",
      "          1.3293e-03, 2.9036e-04, 1.4619e-04, 2.8839e-04, 1.0989e-04,\n",
      "          9.5124e-03, 1.1784e-04, 4.5086e-04, 4.4758e-04, 4.9640e-04,\n",
      "          1.0539e-03, 3.9519e-04, 2.7474e-03, 2.5468e-03, 1.1847e-04,\n",
      "          4.5391e-02, 5.2479e-04, 2.4406e-02, 6.8313e-03, 3.1674e-03,\n",
      "          7.7676e-01, 1.5135e-02, 3.3306e-02, 1.6222e-02, 2.3728e-04,\n",
      "          3.2697e-02, 3.0258e-03, 2.9168e-03, 2.2063e-03, 5.2388e-03,\n",
      "          8.9981e-03, 7.1228e-04, 2.9300e-04, 3.3005e-04, 5.6313e-05,\n",
      "          3.5326e-05, 9.6537e-05, 7.5732e-05, 5.8594e-05, 6.5292e-05,\n",
      "          3.5434e-05],\n",
      "         [1.5547e-06, 1.3327e-06, 2.8468e-06, 2.8939e-06, 1.6495e-06,\n",
      "          3.7756e-06, 2.4362e-06, 2.1078e-06, 1.6087e-06, 1.9552e-06,\n",
      "          1.7440e-06, 1.5234e-06, 9.0130e-04, 7.9924e-04, 1.2792e-06,\n",
      "          4.3258e-03, 1.6059e-06, 1.0113e-04, 6.9469e-05, 1.2913e-06,\n",
      "          9.9121e-01, 1.8148e-06, 5.1692e-04, 1.9858e-03, 7.7026e-06,\n",
      "          3.1404e-06, 1.0505e-06, 1.6026e-06, 1.0637e-06, 1.2031e-06,\n",
      "          1.8174e-06, 2.7484e-06, 3.1161e-06, 1.3504e-06, 3.3223e-06,\n",
      "          1.8427e-06, 1.4900e-06, 9.3007e-07, 1.9032e-06, 2.4891e-06,\n",
      "          2.2251e-06, 2.4371e-06, 2.7432e-06, 3.9357e-06, 2.1347e-06,\n",
      "          2.3129e-06, 9.9277e-07, 1.7310e-06, 2.0579e-06, 2.5416e-06,\n",
      "          1.6987e-06],\n",
      "         [4.0394e-05, 5.0907e-05, 6.0356e-05, 4.4747e-05, 3.9280e-05,\n",
      "          3.5596e-05, 4.8909e-05, 7.9392e-05, 5.8658e-05, 9.4813e-05,\n",
      "          1.7328e-03, 1.8097e-04, 1.3991e-04, 2.2677e-04, 6.9445e-05,\n",
      "          1.3346e-04, 6.7124e-05, 1.2090e-04, 8.5743e-05, 2.4022e-04,\n",
      "          4.1454e-02, 6.2834e-04, 6.9195e-04, 7.9844e-04, 6.4864e-05,\n",
      "          2.3612e-01, 7.2120e-05, 2.6145e-03, 8.2091e-04, 8.7011e-03,\n",
      "          6.3493e-01, 2.8544e-03, 2.3310e-03, 1.2969e-03, 2.6330e-04,\n",
      "          7.9435e-03, 1.0459e-03, 8.1150e-04, 8.1934e-04, 9.9696e-03,\n",
      "          4.0698e-02, 5.0521e-04, 4.0341e-04, 3.6392e-04, 3.9302e-05,\n",
      "          2.3128e-05, 3.5034e-05, 3.4981e-05, 3.2566e-05, 6.3837e-05,\n",
      "          2.3776e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0365, 0.1051, 0.8583, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[5.4251e-07, 6.4864e-07, 2.8940e-07, 6.4632e-07, 5.9902e-07,\n",
      "          4.7948e-07, 5.2689e-07, 5.7704e-05, 2.8205e-05, 1.5366e-06,\n",
      "          2.0256e-01, 3.2905e-06, 1.6088e-05, 1.7975e-05, 1.4799e-06,\n",
      "          4.6997e-05, 3.8958e-06, 9.5744e-06, 1.3696e-05, 2.9554e-06,\n",
      "          1.1506e-05, 4.6241e-06, 7.7554e-06, 1.1916e-05, 1.0896e-06,\n",
      "          3.2364e-01, 1.7168e-06, 1.6825e-05, 4.1458e-06, 2.5102e-05,\n",
      "          4.0957e-05, 2.0278e-05, 1.9163e-05, 1.8356e-05, 8.8639e-06,\n",
      "          1.2241e-04, 3.2074e-05, 3.5930e-05, 2.7896e-05, 1.8827e-05,\n",
      "          4.7306e-01, 1.6630e-05, 3.8374e-05, 3.8821e-05, 5.9891e-07,\n",
      "          4.0468e-07, 1.0693e-06, 4.8434e-07, 3.9313e-07, 4.6603e-07,\n",
      "          2.2868e-07],\n",
      "         [7.2456e-05, 7.7094e-05, 4.9319e-05, 7.4060e-05, 1.7757e-04,\n",
      "          8.2509e-05, 2.2939e-04, 3.1983e-03, 2.6579e-03, 1.8637e-04,\n",
      "          2.7261e-03, 3.6643e-04, 1.9810e-03, 1.9998e-03, 1.8456e-04,\n",
      "          9.9068e-05, 1.2583e-04, 2.8883e-03, 1.9358e-03, 4.9852e-04,\n",
      "          2.2114e-04, 2.8315e-04, 1.6775e-03, 8.3077e-04, 1.3453e-04,\n",
      "          5.2288e-01, 7.1726e-04, 4.2689e-02, 8.8927e-03, 2.1231e-03,\n",
      "          1.4712e-03, 1.4087e-02, 1.9316e-01, 1.0692e-01, 2.2479e-04,\n",
      "          2.5026e-03, 6.2676e-03, 3.1591e-02, 3.4070e-02, 6.5095e-04,\n",
      "          1.8959e-03, 1.6401e-03, 2.1545e-03, 2.7034e-03, 8.0704e-05,\n",
      "          4.0996e-05, 9.5641e-05, 1.2309e-04, 9.8973e-05, 1.0621e-04,\n",
      "          5.6637e-05],\n",
      "         [9.1739e-07, 4.4322e-07, 8.4296e-07, 2.2806e-06, 6.3773e-07,\n",
      "          1.3287e-06, 8.6033e-07, 7.9018e-07, 4.7359e-07, 5.3087e-07,\n",
      "          7.8045e-07, 6.5580e-07, 1.7550e-04, 2.0720e-04, 9.2111e-07,\n",
      "          9.9913e-01, 6.8745e-07, 9.2144e-05, 1.1167e-04, 7.2122e-07,\n",
      "          3.5426e-05, 6.9581e-07, 1.1727e-04, 8.5589e-05, 7.0810e-06,\n",
      "          5.8900e-07, 7.3412e-07, 4.3100e-07, 6.6458e-07, 8.1034e-07,\n",
      "          1.0968e-06, 1.8933e-06, 1.5848e-06, 6.6757e-07, 1.3083e-06,\n",
      "          1.2490e-06, 1.3008e-06, 8.6005e-07, 1.0280e-06, 2.1078e-06,\n",
      "          9.0488e-07, 9.9174e-07, 1.0560e-06, 1.8272e-06, 1.3225e-06,\n",
      "          1.4159e-06, 6.4079e-07, 8.1966e-07, 7.7268e-07, 8.5685e-07,\n",
      "          7.5149e-07],\n",
      "         [3.1956e-04, 3.2440e-04, 4.9897e-04, 4.2000e-04, 5.2789e-04,\n",
      "          5.3832e-04, 5.6803e-04, 1.6785e-03, 7.0005e-04, 8.4125e-04,\n",
      "          6.3055e-03, 1.0216e-03, 1.6651e-03, 4.0648e-03, 1.4003e-03,\n",
      "          8.2419e-04, 1.0538e-03, 1.2704e-03, 1.1492e-03, 1.6707e-03,\n",
      "          2.2512e-03, 1.9774e-03, 1.4852e-02, 1.3254e-02, 5.8928e-04,\n",
      "          9.9191e-02, 7.8572e-04, 1.3330e-01, 4.2141e-02, 1.8383e-02,\n",
      "          1.2389e-01, 8.4088e-02, 2.1969e-01, 9.6427e-02, 1.5793e-03,\n",
      "          2.0274e-02, 1.3951e-02, 1.3690e-02, 1.4970e-02, 1.2606e-02,\n",
      "          2.8340e-02, 5.8919e-03, 3.5812e-03, 4.9852e-03, 4.4538e-04,\n",
      "          1.6719e-04, 5.4128e-04, 3.1580e-04, 3.4357e-04, 4.7343e-04,\n",
      "          1.9142e-04]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39999/50000 [39:35<07:29, 22.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56535\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5457/10000 (54.6%)\n",
      "    Average reward: -0.430\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4543/10000 (45.4%)\n",
      "    Average reward: +0.430\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9358 (33.4%)\n",
      "    Action 1: 15295 (54.6%)\n",
      "    Action 2: 1754 (6.3%)\n",
      "    Action 3: 1607 (5.7%)\n",
      "  Player 1:\n",
      "    Action 0: 7817 (27.4%)\n",
      "    Action 1: 13205 (46.3%)\n",
      "    Action 2: 3238 (11.4%)\n",
      "    Action 3: 4261 (14.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4303.5, 4303.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.005 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.016\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4304\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[4.1352e-01, 5.8645e-01, 2.6614e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[1.5518e-01, 8.4479e-01, 3.2823e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.6338e-01, 1.5583e-04, 3.6460e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52017\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5384/10000 (53.8%)\n",
      "    Average reward: +0.260\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4616/10000 (46.2%)\n",
      "    Average reward: -0.260\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3068 (12.4%)\n",
      "    Action 1: 16193 (65.2%)\n",
      "    Action 2: 2158 (8.7%)\n",
      "    Action 3: 3405 (13.7%)\n",
      "  Player 1:\n",
      "    Action 0: 21290 (78.3%)\n",
      "    Action 1: 5903 (21.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2596.5, -2596.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.775 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.755 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.765\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2596\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3018, 0.6971, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[4.9547e-06, 4.8398e-06, 7.8839e-06, 1.0843e-05, 9.9410e-06,\n",
      "          5.6554e-06, 8.2308e-06, 7.7364e-06, 1.3509e-05, 1.3898e-05,\n",
      "          6.0365e-04, 7.1753e-04, 8.9596e-05, 7.0716e-05, 3.4506e-04,\n",
      "          3.7807e-03, 5.0035e-04, 4.8291e-03, 8.4550e-03, 5.0610e-04,\n",
      "          4.3025e-02, 3.4868e-03, 2.5661e-04, 3.7116e-04, 2.4997e-05,\n",
      "          3.8416e-01, 1.2912e-05, 1.9567e-04, 1.5977e-04, 2.7644e-03,\n",
      "          4.7332e-02, 2.3766e-04, 2.1900e-03, 2.2283e-03, 1.9789e-02,\n",
      "          3.8663e-01, 5.6167e-05, 8.8161e-05, 4.4290e-04, 2.4287e-02,\n",
      "          6.1686e-02, 1.5080e-04, 2.5032e-04, 1.3029e-04, 1.1109e-05,\n",
      "          1.0866e-05, 9.3514e-06, 7.7830e-06, 1.1098e-05, 8.1773e-06,\n",
      "          5.0810e-06],\n",
      "         [2.4668e-06, 5.3517e-06, 4.3426e-06, 5.6729e-06, 5.3347e-06,\n",
      "          4.7900e-06, 3.0683e-06, 9.7662e-06, 2.9679e-05, 2.8724e-05,\n",
      "          7.5014e-03, 2.0176e-03, 1.2932e-03, 9.4314e-04, 3.3013e-04,\n",
      "          6.6574e-03, 8.9433e-04, 1.9468e-03, 2.0927e-03, 3.4417e-04,\n",
      "          1.6903e-04, 5.4466e-05, 1.5086e-04, 2.0027e-04, 1.7320e-05,\n",
      "          2.0819e-02, 7.0512e-06, 2.0434e-04, 2.4223e-04, 2.0626e-02,\n",
      "          3.9729e-01, 1.4982e-04, 4.6924e-04, 5.6558e-04, 5.2013e-02,\n",
      "          4.7218e-01, 2.3040e-05, 1.8717e-05, 6.7394e-05, 2.7400e-03,\n",
      "          7.3534e-03, 1.2772e-04, 2.0459e-04, 1.5777e-04, 4.0817e-06,\n",
      "          5.2857e-06, 4.9155e-06, 5.0968e-06, 5.7126e-06, 3.5050e-06,\n",
      "          4.0981e-06],\n",
      "         [9.6224e-07, 1.7115e-06, 8.8558e-07, 1.6214e-06, 1.7718e-06,\n",
      "          1.2256e-06, 2.2657e-06, 1.0030e-06, 1.2950e-06, 9.7618e-07,\n",
      "          7.1745e-07, 1.9869e-06, 9.4555e-05, 8.8112e-05, 6.7397e-07,\n",
      "          2.8645e-04, 1.9095e-06, 1.8426e-05, 2.0447e-05, 7.1762e-07,\n",
      "          2.8288e-04, 6.8388e-07, 4.7809e-01, 5.1354e-01, 7.5204e-03,\n",
      "          1.3094e-06, 1.1230e-06, 1.0442e-06, 1.4747e-06, 1.1727e-06,\n",
      "          1.7935e-06, 2.2369e-06, 1.4410e-06, 3.5035e-07, 1.6338e-06,\n",
      "          1.5099e-06, 1.2447e-06, 1.0990e-06, 1.2032e-06, 1.9385e-06,\n",
      "          1.6296e-06, 7.9464e-07, 7.3750e-07, 1.4643e-06, 8.6445e-07,\n",
      "          7.2645e-07, 8.2501e-07, 1.7023e-06, 1.4141e-06, 1.3995e-06,\n",
      "          6.8531e-07],\n",
      "         [8.6784e-05, 1.1423e-04, 1.2161e-04, 1.2419e-04, 1.5954e-04,\n",
      "          1.5123e-04, 1.3033e-04, 2.4309e-04, 3.3626e-04, 3.2947e-04,\n",
      "          6.2164e-04, 6.3018e-04, 9.1332e-03, 1.2394e-02, 9.9961e-04,\n",
      "          1.6252e-03, 5.3737e-04, 1.3093e-02, 1.5022e-02, 1.9106e-03,\n",
      "          3.6496e-03, 5.5421e-04, 3.3872e-01, 3.1434e-01, 5.0927e-03,\n",
      "          1.0920e-01, 1.8264e-04, 6.5814e-03, 6.7741e-03, 6.4186e-04,\n",
      "          4.1357e-03, 3.6511e-03, 3.6030e-02, 3.3215e-02, 2.2913e-03,\n",
      "          4.3316e-03, 1.5042e-03, 1.8548e-02, 1.5865e-02, 1.7774e-03,\n",
      "          2.9208e-03, 5.7085e-03, 1.5792e-02, 9.8090e-03, 1.3978e-04,\n",
      "          1.3437e-04, 1.5621e-04, 1.4061e-04, 1.1586e-04, 1.2797e-04,\n",
      "          1.1019e-04]]])\n",
      "Player 1 Prediction: tensor([[1.4699e-02, 9.8525e-01, 4.6262e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[3.5764e-05, 3.6702e-05, 4.2773e-05, 4.8455e-05, 3.8892e-05,\n",
      "          4.1853e-05, 3.6913e-05, 5.8543e-04, 6.1419e-04, 1.9735e-04,\n",
      "          2.2268e-03, 8.0474e-04, 8.3650e-02, 9.5527e-02, 1.5106e-02,\n",
      "          3.1368e-04, 1.7833e-04, 1.2473e-01, 1.0916e-01, 6.7536e-03,\n",
      "          5.0119e-04, 2.4007e-04, 7.8922e-04, 1.9587e-03, 2.3430e-04,\n",
      "          1.7445e-01, 5.1055e-05, 4.1254e-04, 5.0060e-04, 2.6353e-04,\n",
      "          7.3880e-04, 8.6828e-03, 8.9071e-02, 8.5921e-02, 4.7779e-04,\n",
      "          6.7360e-04, 2.5261e-03, 1.6286e-02, 1.4348e-02, 2.7585e-04,\n",
      "          8.4849e-04, 1.5420e-02, 8.2018e-02, 6.2907e-02, 4.6895e-05,\n",
      "          3.9309e-05, 3.6823e-05, 3.9027e-05, 3.9232e-05, 4.6362e-05,\n",
      "          2.8728e-05],\n",
      "         [2.1640e-06, 3.4447e-06, 2.8528e-06, 5.9562e-06, 3.0112e-06,\n",
      "          2.5596e-06, 2.3971e-06, 4.8019e-07, 1.8411e-06, 1.8052e-05,\n",
      "          9.8147e-03, 7.0405e-04, 1.5426e-03, 9.8672e-04, 4.6000e-05,\n",
      "          6.0549e-03, 6.5669e-05, 1.6930e-05, 1.5149e-05, 8.2098e-05,\n",
      "          1.1934e-04, 2.1801e-05, 1.0247e-04, 9.3366e-05, 8.6068e-06,\n",
      "          1.7746e-03, 4.4749e-06, 1.6433e-05, 2.2441e-05, 2.2643e-04,\n",
      "          4.3335e-01, 9.1542e-05, 1.3205e-04, 1.9117e-04, 2.0668e-04,\n",
      "          5.3266e-01, 2.1011e-06, 6.8424e-06, 2.4031e-05, 1.7331e-03,\n",
      "          9.8084e-03, 1.0630e-05, 9.1266e-08, 8.1663e-08, 3.8197e-06,\n",
      "          4.8564e-06, 2.5616e-06, 3.8029e-06, 4.8585e-06, 4.5593e-06,\n",
      "          2.7491e-06],\n",
      "         [6.2084e-07, 1.0256e-06, 4.2899e-07, 7.7665e-07, 1.7476e-06,\n",
      "          8.6095e-07, 1.2127e-06, 6.4904e-07, 4.0556e-07, 5.6629e-07,\n",
      "          6.4782e-07, 9.2434e-07, 9.7150e-05, 5.7778e-05, 4.4460e-07,\n",
      "          2.5598e-05, 7.8605e-07, 1.0083e-04, 5.9780e-05, 6.5614e-07,\n",
      "          9.9959e-01, 5.4547e-07, 2.3021e-06, 1.7516e-05, 1.8219e-05,\n",
      "          6.2088e-07, 5.8395e-07, 4.2281e-07, 8.4566e-07, 4.1904e-07,\n",
      "          6.2968e-07, 8.2743e-07, 6.0768e-07, 3.5494e-07, 4.9509e-07,\n",
      "          7.3358e-07, 5.4974e-07, 5.3262e-07, 9.8663e-07, 3.7883e-07,\n",
      "          7.7686e-07, 3.8891e-07, 3.2125e-07, 8.8360e-07, 6.1249e-07,\n",
      "          5.8849e-07, 6.6809e-07, 9.0271e-07, 8.6539e-07, 1.0481e-06,\n",
      "          4.5714e-07],\n",
      "         [1.0917e-04, 9.9589e-05, 1.1079e-04, 1.6012e-04, 1.6227e-04,\n",
      "          1.6568e-04, 9.4850e-05, 4.2940e-04, 5.9059e-04, 3.3893e-04,\n",
      "          2.7400e-03, 1.9162e-03, 4.5159e-04, 5.6953e-04, 3.8688e-04,\n",
      "          3.2211e-03, 5.9675e-04, 4.5108e-02, 6.9854e-02, 2.3318e-03,\n",
      "          2.2664e-02, 1.1840e-03, 5.7854e-04, 7.7669e-04, 8.1329e-04,\n",
      "          7.7423e-01, 1.7565e-04, 6.3733e-05, 6.2788e-05, 4.6791e-04,\n",
      "          1.3274e-02, 9.6769e-04, 4.8791e-03, 3.8986e-03, 5.3699e-03,\n",
      "          1.3485e-02, 2.1771e-04, 1.0830e-03, 1.6425e-03, 5.7694e-03,\n",
      "          9.5185e-03, 2.4674e-03, 4.0290e-03, 2.0322e-03, 1.0845e-04,\n",
      "          1.6992e-04, 1.3387e-04, 1.3056e-04, 1.2388e-04, 8.2640e-05,\n",
      "          1.6235e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2779, 0.0054, 0.7167]])\n",
      "Player 0 Prediction: tensor([[[2.0635e-07, 3.1025e-07, 4.0904e-07, 2.6390e-07, 4.1871e-07,\n",
      "          2.8533e-07, 3.3264e-07, 3.8341e-07, 4.0247e-07, 1.2600e-06,\n",
      "          5.7741e-06, 5.0202e-06, 5.5089e-03, 5.9766e-03, 6.1479e-06,\n",
      "          2.1239e-06, 3.6939e-06, 1.2175e-05, 1.5305e-05, 4.6641e-06,\n",
      "          6.7769e-06, 7.6485e-06, 3.2615e-06, 2.7470e-06, 8.7822e-07,\n",
      "          4.1769e-03, 3.0152e-07, 1.0068e-06, 9.9580e-07, 6.3811e-06,\n",
      "          4.1111e-06, 7.4663e-06, 1.7803e-04, 1.6889e-04, 1.4718e-05,\n",
      "          8.0916e-04, 5.5545e-06, 4.8528e-01, 4.9712e-01, 1.3470e-05,\n",
      "          3.5530e-04, 6.3870e-06, 1.2860e-04, 1.4823e-04, 5.0038e-07,\n",
      "          2.0990e-07, 2.8599e-07, 3.6139e-07, 3.9671e-07, 3.4549e-07,\n",
      "          2.1951e-07],\n",
      "         [7.1416e-08, 1.5352e-07, 1.4348e-07, 1.4077e-07, 2.0556e-07,\n",
      "          1.4844e-07, 1.0241e-07, 1.5303e-03, 1.7390e-03, 2.7601e-06,\n",
      "          4.7900e-07, 1.9066e-06, 1.7438e-06, 1.3996e-06, 7.3630e-06,\n",
      "          3.7651e-07, 1.3184e-06, 1.3391e-05, 8.5462e-06, 8.6849e-06,\n",
      "          2.7429e-07, 2.1239e-07, 1.1286e-06, 1.2650e-06, 2.6893e-07,\n",
      "          1.8484e-05, 1.0208e-07, 8.9071e-07, 1.1213e-06, 1.0440e-06,\n",
      "          7.5965e-08, 2.0879e-05, 2.5483e-04, 2.4423e-04, 4.8192e-07,\n",
      "          1.6603e-06, 1.3889e-06, 5.5160e-02, 5.3116e-02, 5.3002e-07,\n",
      "          7.0258e-06, 3.0405e-04, 4.4301e-01, 4.4453e-01, 1.7036e-07,\n",
      "          1.4005e-07, 2.4946e-07, 1.6835e-07, 2.0391e-07, 6.0827e-08,\n",
      "          1.6179e-07],\n",
      "         [1.5510e-06, 2.3946e-06, 2.3726e-06, 2.0469e-06, 1.3920e-06,\n",
      "          3.6703e-06, 1.3769e-06, 1.5682e-06, 2.2429e-06, 2.1941e-06,\n",
      "          2.4970e-06, 2.8075e-06, 9.6069e-05, 1.1420e-04, 1.0681e-06,\n",
      "          1.8560e-04, 3.0508e-06, 4.9964e-01, 4.9944e-01, 2.6723e-06,\n",
      "          5.0913e-06, 2.1116e-06, 1.5291e-04, 2.2017e-04, 3.5453e-05,\n",
      "          1.9347e-06, 1.1188e-06, 3.5764e-06, 3.4998e-06, 4.9292e-06,\n",
      "          1.5881e-06, 3.0055e-06, 4.3259e-06, 1.3219e-06, 2.8527e-06,\n",
      "          4.1631e-06, 2.6311e-06, 1.4545e-06, 3.5235e-06, 2.2833e-06,\n",
      "          2.5959e-06, 2.6932e-06, 2.0606e-06, 2.0009e-06, 2.2977e-06,\n",
      "          2.2781e-06, 2.7417e-06, 9.9821e-07, 3.7169e-06, 1.9069e-06,\n",
      "          4.2746e-06],\n",
      "         [3.4671e-05, 3.7840e-05, 5.3031e-05, 4.1081e-05, 3.9758e-05,\n",
      "          3.7835e-05, 4.6083e-05, 8.4772e-04, 5.0067e-04, 2.7217e-04,\n",
      "          2.0780e-04, 1.9089e-04, 4.0732e-04, 5.7881e-04, 1.2620e-04,\n",
      "          2.4846e-04, 9.2364e-05, 1.7695e-03, 1.4433e-03, 1.6825e-03,\n",
      "          2.3102e-05, 1.3909e-04, 7.8235e-04, 2.2571e-03, 4.6628e-04,\n",
      "          3.3962e-03, 4.3595e-05, 3.6580e-03, 2.9249e-03, 9.6308e-05,\n",
      "          4.9281e-04, 1.7080e-03, 4.4789e-01, 3.9519e-01, 4.0816e-04,\n",
      "          1.2743e-03, 1.8343e-04, 2.7847e-03, 3.5204e-03, 4.0756e-04,\n",
      "          1.4543e-03, 5.9209e-03, 6.3780e-02, 5.2296e-02, 4.4328e-05,\n",
      "          3.3887e-05, 4.1825e-05, 2.3976e-05, 2.6118e-05, 4.3619e-05,\n",
      "          2.9260e-05]]])\n",
      "Player 1 Prediction: tensor([[0.3675, 0.0000, 0.6325, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55297\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5815/10000 (58.1%)\n",
      "    Average reward: +0.360\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4185/10000 (41.9%)\n",
      "    Average reward: -0.360\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8266 (29.8%)\n",
      "    Action 1: 15619 (56.3%)\n",
      "    Action 2: 2287 (8.2%)\n",
      "    Action 3: 1550 (5.6%)\n",
      "  Player 1:\n",
      "    Action 0: 7634 (27.7%)\n",
      "    Action 1: 13888 (50.4%)\n",
      "    Action 2: 2696 (9.8%)\n",
      "    Action 3: 3357 (12.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3600.0, -3600.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.987 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.999\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3600\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1847, 0.8122, 0.0032, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9005, 0.0148, 0.0848]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50190\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6122/10000 (61.2%)\n",
      "    Average reward: -0.092\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3878/10000 (38.8%)\n",
      "    Average reward: +0.092\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 23031 (85.6%)\n",
      "    Action 1: 3871 (14.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2728 (11.7%)\n",
      "    Action 1: 18039 (77.5%)\n",
      "    Action 2: 799 (3.4%)\n",
      "    Action 3: 1722 (7.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-919.0, 919.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.594 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.648 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.621\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0919\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}, {'exploitability': 0.395175}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41003/50000 [40:44<06:38, 22.59it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 262876/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 266633/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [41:28<05:52, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 269339/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 273388/2000000\n",
      "P1 SL Buffer Size:  269339\n",
      "P1 SL buffer distribution [ 97010. 138690.  21663.  11976.]\n",
      "P1 actions distribution [0.36017807 0.51492728 0.08043024 0.04446441]\n",
      "P2 SL Buffer Size:  273388\n",
      "P2 SL buffer distribution [ 84025. 136984.  23401.  28978.]\n",
      "P2 actions distribution [0.30734707 0.50106076 0.0855963  0.10599587]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[9.2641e-05, 1.3026e-04, 1.4417e-04, 1.4196e-04, 1.4224e-04,\n",
      "          1.9103e-04, 1.6919e-04, 6.3995e-04, 9.8527e-04, 6.3186e-04,\n",
      "          2.8395e-04, 3.2543e-04, 5.1108e-04, 6.8329e-04, 5.0622e-04,\n",
      "          2.5473e-04, 3.6247e-04, 1.1270e-02, 1.5343e-02, 3.0358e-03,\n",
      "          1.0233e-03, 5.5167e-04, 3.9815e-03, 4.1925e-03, 2.9274e-04,\n",
      "          2.4427e-02, 1.5103e-03, 4.5586e-02, 5.5746e-03, 3.5557e-03,\n",
      "          2.2394e-02, 6.3533e-02, 2.8841e-01, 2.5221e-01, 1.4289e-03,\n",
      "          1.1005e-02, 3.5977e-02, 4.7776e-02, 2.1393e-02, 2.6812e-03,\n",
      "          1.0307e-02, 3.0000e-02, 5.4241e-02, 3.1276e-02, 1.2259e-04,\n",
      "          1.3811e-04, 1.4036e-04, 1.2821e-04, 1.3797e-04, 1.0649e-04,\n",
      "          5.2356e-05],\n",
      "         [9.4153e-05, 1.5996e-04, 1.4919e-04, 1.4139e-04, 2.0487e-04,\n",
      "          2.1445e-04, 2.0876e-04, 7.5616e-04, 8.4353e-04, 4.2579e-04,\n",
      "          7.4687e-04, 5.6220e-04, 6.2890e-04, 9.9930e-04, 6.6640e-04,\n",
      "          6.4519e-03, 1.5540e-03, 5.2843e-03, 7.3544e-03, 2.0082e-03,\n",
      "          2.4668e-03, 6.9772e-04, 4.2814e-03, 4.6551e-03, 3.9836e-04,\n",
      "          2.2967e-02, 1.6742e-03, 5.5782e-02, 1.1436e-02, 1.3909e-02,\n",
      "          1.9065e-01, 3.6358e-02, 1.7472e-01, 1.5768e-01, 9.1858e-03,\n",
      "          7.9670e-02, 2.5992e-02, 3.2266e-02, 1.6528e-02, 7.4153e-03,\n",
      "          2.1053e-02, 1.9528e-02, 4.8877e-02, 3.1320e-02, 1.6853e-04,\n",
      "          1.3727e-04, 1.7284e-04, 1.3824e-04, 1.7267e-04, 1.3683e-04,\n",
      "          1.0955e-04],\n",
      "         [1.9420e-05, 2.2050e-05, 2.9035e-05, 2.3252e-05, 1.9098e-05,\n",
      "          2.2312e-05, 1.5945e-05, 3.4719e-05, 3.4710e-05, 1.6622e-05,\n",
      "          1.1697e-05, 1.9672e-05, 2.1420e-03, 1.5036e-03, 2.7758e-05,\n",
      "          1.9032e-03, 2.2642e-05, 1.6207e-03, 1.8546e-03, 2.3117e-05,\n",
      "          4.8522e-04, 1.7673e-05, 8.8402e-03, 2.4009e-01, 7.4058e-01,\n",
      "          2.5970e-05, 3.2354e-05, 2.9434e-05, 2.4129e-05, 4.0260e-05,\n",
      "          2.4809e-05, 2.3605e-05, 2.2789e-05, 1.8112e-05, 2.3028e-05,\n",
      "          2.3593e-05, 3.2487e-05, 2.5341e-05, 2.9758e-05, 2.1929e-05,\n",
      "          1.7207e-05, 1.6616e-05, 2.1118e-05, 1.9229e-05, 2.0864e-05,\n",
      "          1.9107e-05, 2.3526e-05, 2.2225e-05, 1.7251e-05, 2.2275e-05,\n",
      "          2.0522e-05],\n",
      "         [2.0728e-04, 4.5094e-04, 4.1964e-04, 3.2979e-04, 7.6282e-04,\n",
      "          3.9840e-04, 4.7869e-04, 9.9840e-04, 1.1937e-03, 7.1590e-04,\n",
      "          1.5434e-03, 8.2529e-04, 2.1139e-03, 2.5555e-03, 6.2809e-04,\n",
      "          1.9176e-03, 6.9320e-04, 1.5536e-02, 2.3460e-02, 3.5118e-03,\n",
      "          3.9882e-04, 1.0605e-03, 1.1770e-02, 9.4965e-03, 4.7099e-04,\n",
      "          4.9553e-02, 1.4186e-03, 9.4463e-02, 4.1283e-02, 5.6880e-03,\n",
      "          1.4138e-02, 6.5514e-02, 1.9528e-01, 1.4023e-01, 1.9119e-03,\n",
      "          1.8529e-02, 1.6648e-02, 3.9971e-02, 2.3347e-02, 4.7105e-03,\n",
      "          9.3791e-03, 5.2579e-02, 8.6985e-02, 5.3088e-02, 4.2633e-04,\n",
      "          4.9735e-04, 4.0516e-04, 3.7717e-04, 5.8947e-04, 6.5643e-04,\n",
      "          3.9073e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.3788e-01, 2.0323e-04, 6.1920e-02]])\n",
      "Player 1 Prediction: tensor([[[1.3340e-04, 2.1712e-04, 1.6961e-04, 1.3231e-04, 1.6311e-04,\n",
      "          1.8884e-04, 1.7079e-04, 5.4965e-04, 6.4264e-04, 4.3757e-04,\n",
      "          3.4524e-03, 1.2901e-03, 2.4597e-04, 3.4241e-04, 2.7978e-04,\n",
      "          1.5604e-02, 3.0874e-03, 2.6562e-03, 3.5685e-03, 1.0544e-03,\n",
      "          2.1413e-02, 2.7212e-03, 2.8532e-03, 3.0228e-03, 2.5801e-04,\n",
      "          9.1001e-02, 8.0115e-04, 3.3429e-02, 7.2939e-03, 2.9780e-02,\n",
      "          4.2363e-01, 1.0796e-02, 2.3947e-02, 1.4114e-02, 1.2194e-02,\n",
      "          9.9252e-02, 6.3036e-03, 2.8060e-03, 2.3546e-03, 2.7031e-02,\n",
      "          1.2469e-01, 4.3589e-03, 1.3261e-02, 7.1110e-03, 1.5008e-04,\n",
      "          1.6934e-04, 1.7669e-04, 1.9919e-04, 2.0732e-04, 1.7959e-04,\n",
      "          1.0725e-04],\n",
      "         [7.2069e-05, 1.1494e-04, 1.1711e-04, 1.3254e-04, 1.8670e-04,\n",
      "          1.9801e-04, 1.5886e-04, 7.0982e-04, 8.9123e-04, 4.3686e-04,\n",
      "          3.8604e-04, 2.6091e-04, 4.0228e-04, 5.1953e-04, 2.8111e-04,\n",
      "          2.3443e-04, 2.4826e-04, 1.2682e-02, 1.5980e-02, 2.0917e-03,\n",
      "          5.4898e-04, 3.2225e-04, 1.7755e-03, 2.0609e-03, 2.2545e-04,\n",
      "          2.4965e-02, 1.2485e-03, 4.4609e-02, 2.6047e-03, 2.6986e-03,\n",
      "          1.8453e-02, 4.5731e-02, 2.9161e-01, 2.7401e-01, 9.9771e-04,\n",
      "          9.1346e-03, 3.3987e-02, 4.4657e-02, 2.1383e-02, 1.4760e-03,\n",
      "          4.8140e-03, 2.6976e-02, 6.4883e-02, 4.3989e-02, 1.0788e-04,\n",
      "          1.1700e-04, 1.1590e-04, 1.0073e-04, 1.3329e-04, 1.0583e-04,\n",
      "          5.6979e-05],\n",
      "         [7.9010e-06, 6.5169e-06, 1.0844e-05, 7.9950e-06, 9.3031e-06,\n",
      "          1.4066e-05, 8.0396e-06, 9.0806e-06, 9.5247e-06, 9.1367e-06,\n",
      "          1.2261e-05, 6.2799e-06, 6.6561e-04, 5.7779e-04, 5.7124e-06,\n",
      "          2.7690e-03, 9.7297e-06, 2.9617e-04, 3.4048e-04, 6.5845e-06,\n",
      "          2.3884e-04, 5.5432e-06, 6.5020e-01, 3.4447e-01, 3.4655e-05,\n",
      "          1.7044e-05, 9.9151e-06, 1.1742e-05, 8.0929e-06, 1.2563e-05,\n",
      "          9.4515e-06, 1.0310e-05, 9.9605e-06, 5.2351e-06, 9.3698e-06,\n",
      "          5.3858e-06, 1.0990e-05, 8.2571e-06, 1.9774e-05, 1.1913e-05,\n",
      "          7.4210e-06, 7.6612e-06, 1.8344e-05, 9.8148e-06, 1.7096e-05,\n",
      "          6.9464e-06, 7.6244e-06, 7.5252e-06, 7.8847e-06, 9.5275e-06,\n",
      "          8.3226e-06],\n",
      "         [6.5264e-04, 5.7684e-04, 5.4171e-04, 4.6364e-04, 4.2612e-04,\n",
      "          6.7519e-04, 4.7418e-04, 9.7363e-04, 1.8475e-03, 1.0821e-03,\n",
      "          3.8733e-03, 1.2059e-03, 3.8453e-03, 4.2694e-03, 8.8635e-04,\n",
      "          2.7747e-03, 1.2742e-03, 1.2472e-02, 7.7988e-03, 2.6068e-03,\n",
      "          5.4393e-03, 2.5549e-03, 2.2480e-02, 2.4637e-02, 9.3646e-04,\n",
      "          5.7256e-02, 1.8875e-03, 1.6068e-01, 1.1015e-01, 1.1039e-02,\n",
      "          3.4274e-02, 2.6449e-02, 1.4984e-01, 6.1943e-02, 2.4930e-03,\n",
      "          4.0105e-02, 2.2002e-02, 2.5071e-02, 2.7492e-02, 1.6279e-02,\n",
      "          5.4232e-02, 1.7128e-02, 4.5312e-02, 2.7735e-02, 6.4959e-04,\n",
      "          4.8939e-04, 6.0611e-04, 4.1651e-04, 5.9564e-04, 6.4582e-04,\n",
      "          4.6248e-04]]])\n",
      "Player 0 Prediction: tensor([[9.9940e-01, 0.0000e+00, 6.0463e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[1.6705e-05, 3.6674e-05, 3.2643e-05, 3.3479e-05, 4.5614e-05,\n",
      "          3.9229e-05, 4.3015e-05, 5.5118e-04, 5.5867e-04, 1.0033e-04,\n",
      "          2.1079e-03, 8.0004e-05, 1.1410e-02, 1.6125e-02, 6.1197e-05,\n",
      "          7.6968e-06, 1.1832e-04, 4.1992e-02, 4.6906e-02, 3.8120e-04,\n",
      "          1.1424e-04, 1.0152e-04, 5.6741e-04, 4.3017e-04, 5.5497e-05,\n",
      "          3.3699e-01, 4.2557e-04, 5.7741e-03, 3.7223e-04, 5.2640e-04,\n",
      "          9.8153e-04, 5.5921e-03, 1.4584e-01, 2.0636e-01, 1.5569e-04,\n",
      "          4.2889e-05, 3.8826e-03, 9.9686e-02, 6.4954e-02, 2.5608e-04,\n",
      "          2.5370e-04, 1.7091e-03, 2.2134e-03, 1.8832e-03, 2.6374e-05,\n",
      "          2.3591e-05, 2.6813e-05, 3.0672e-05, 2.5958e-05, 3.5739e-05,\n",
      "          2.3636e-05],\n",
      "         [2.4815e-04, 4.6279e-04, 3.9476e-04, 3.4773e-04, 5.9050e-04,\n",
      "          5.9958e-04, 4.8671e-04, 1.3689e-03, 1.6831e-03, 8.2272e-04,\n",
      "          2.7515e-03, 1.3284e-03, 1.3246e-02, 1.4175e-02, 3.3638e-04,\n",
      "          5.0519e-04, 5.8568e-04, 5.2701e-03, 6.3554e-03, 1.5723e-03,\n",
      "          4.2353e-03, 1.3150e-03, 2.2964e-02, 2.3677e-02, 7.0880e-04,\n",
      "          5.4237e-02, 3.1740e-03, 7.3402e-02, 2.1191e-02, 6.5438e-03,\n",
      "          1.0502e-02, 2.6572e-02, 2.7538e-01, 2.6157e-01, 1.5667e-03,\n",
      "          1.2781e-02, 1.4093e-02, 5.2013e-02, 4.7761e-02, 3.2508e-03,\n",
      "          5.5502e-03, 1.1480e-02, 5.8122e-03, 4.5904e-03, 3.9470e-04,\n",
      "          3.5202e-04, 4.2701e-04, 2.9644e-04, 3.8099e-04, 4.2061e-04,\n",
      "          2.2984e-04],\n",
      "         [2.2466e-05, 1.5263e-05, 3.1492e-05, 1.7808e-05, 2.0660e-05,\n",
      "          2.1711e-05, 1.6505e-05, 1.8011e-05, 2.5100e-05, 1.6308e-05,\n",
      "          2.6860e-05, 1.5638e-05, 1.7666e-03, 1.8347e-03, 3.7316e-05,\n",
      "          1.9706e-03, 2.9210e-05, 5.0691e-01, 4.8395e-01, 2.0566e-05,\n",
      "          9.9217e-04, 1.7266e-05, 2.2384e-04, 9.8703e-04, 3.1316e-04,\n",
      "          2.2989e-05, 3.6838e-05, 1.6087e-05, 1.6739e-05, 4.5045e-05,\n",
      "          3.5724e-05, 3.5271e-05, 2.4056e-05, 2.7510e-05, 3.3001e-05,\n",
      "          3.2703e-05, 2.6251e-05, 2.2026e-05, 3.0744e-05, 2.9224e-05,\n",
      "          1.8596e-05, 1.8939e-05, 2.1089e-05, 2.3444e-05, 2.3208e-05,\n",
      "          2.5456e-05, 1.7019e-05, 4.2223e-05, 2.7871e-05, 2.3232e-05,\n",
      "          2.1630e-05],\n",
      "         [9.4878e-05, 1.0927e-04, 1.1276e-04, 1.0616e-04, 2.1734e-04,\n",
      "          1.9256e-04, 1.4753e-04, 6.8369e-04, 7.4068e-04, 3.1178e-04,\n",
      "          6.1324e-04, 2.5046e-04, 2.3660e-04, 5.9940e-04, 2.0690e-04,\n",
      "          2.1348e-04, 2.4846e-04, 1.8124e-02, 2.1549e-02, 1.3652e-03,\n",
      "          3.7973e-04, 3.9478e-04, 2.7949e-03, 3.4240e-03, 1.3347e-04,\n",
      "          2.5730e-02, 6.7954e-04, 2.9700e-02, 2.2729e-03, 1.8799e-03,\n",
      "          9.3866e-03, 2.4331e-02, 3.6130e-01, 3.7613e-01, 4.6498e-04,\n",
      "          6.7811e-03, 2.9835e-02, 3.9872e-02, 1.1021e-02, 1.0405e-03,\n",
      "          2.7211e-03, 1.1402e-02, 6.8162e-03, 4.6265e-03, 9.4417e-05,\n",
      "          1.2867e-04, 1.1893e-04, 9.3709e-05, 1.0249e-04, 1.3235e-04,\n",
      "          9.0521e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6539, 0.0048, 0.3413]])\n",
      "Player 1 Prediction: tensor([[[6.5427e-07, 2.2805e-06, 1.9737e-06, 1.5170e-06, 3.0619e-06,\n",
      "          1.7236e-06, 2.3025e-06, 3.1653e-05, 2.7215e-05, 7.7767e-06,\n",
      "          1.6075e-04, 5.1996e-06, 6.3565e-02, 6.4517e-02, 2.3315e-06,\n",
      "          1.4491e-05, 9.1040e-06, 7.1800e-05, 7.8257e-05, 6.1873e-06,\n",
      "          8.6751e-06, 6.7655e-06, 2.2605e-05, 1.5293e-05, 3.0633e-06,\n",
      "          3.9283e-01, 2.1983e-05, 1.1181e-04, 1.8178e-05, 2.9487e-05,\n",
      "          4.3821e-05, 9.6057e-05, 2.0626e-04, 2.0684e-04, 2.6828e-05,\n",
      "          3.0392e-05, 9.3900e-05, 2.4192e-01, 2.3551e-01, 2.9626e-05,\n",
      "          8.3498e-05, 5.6207e-05, 5.4755e-05, 5.0485e-05, 1.4930e-06,\n",
      "          1.5476e-06, 1.7033e-06, 1.7140e-06, 1.9280e-06, 1.9987e-06,\n",
      "          8.8125e-07],\n",
      "         [6.0656e-06, 9.9618e-06, 8.5409e-06, 6.5889e-06, 9.3852e-06,\n",
      "          1.5424e-05, 1.1519e-05, 5.1213e-02, 5.0540e-02, 2.3573e-05,\n",
      "          5.4270e-05, 1.6879e-05, 3.0813e-06, 2.8317e-06, 8.7113e-06,\n",
      "          1.7239e-06, 7.1659e-06, 2.1038e-04, 1.9693e-04, 7.2489e-05,\n",
      "          4.9328e-05, 3.1936e-05, 2.7232e-04, 2.9205e-04, 1.4173e-05,\n",
      "          3.8810e-01, 7.3166e-05, 8.3177e-04, 2.4418e-04, 6.2235e-05,\n",
      "          4.3594e-05, 3.7963e-04, 5.6201e-04, 6.8105e-04, 2.7637e-05,\n",
      "          1.4659e-04, 5.6852e-04, 1.4798e-01, 1.4480e-01, 3.5177e-05,\n",
      "          7.0574e-05, 2.4306e-04, 1.0500e-01, 1.0703e-01, 7.2565e-06,\n",
      "          7.3250e-06, 6.8614e-06, 6.8338e-06, 7.2330e-06, 1.0147e-05,\n",
      "          3.5364e-06],\n",
      "         [7.7835e-07, 5.8492e-07, 1.6924e-06, 6.5110e-07, 1.1437e-06,\n",
      "          9.6703e-07, 7.3922e-07, 1.1628e-06, 1.2751e-06, 1.0232e-06,\n",
      "          1.9068e-06, 5.8595e-07, 9.2441e-05, 1.2067e-04, 1.6038e-06,\n",
      "          1.1851e-04, 1.5783e-06, 5.0592e-01, 4.9364e-01, 1.0128e-06,\n",
      "          4.6424e-06, 9.0475e-07, 1.3926e-05, 3.4710e-05, 7.7627e-06,\n",
      "          1.3233e-06, 1.6667e-06, 5.5017e-07, 8.2294e-07, 2.7622e-06,\n",
      "          1.7775e-06, 1.5877e-06, 1.4565e-06, 1.5078e-06, 1.3867e-06,\n",
      "          2.6558e-06, 1.2726e-06, 1.5193e-06, 1.9892e-06, 1.7260e-06,\n",
      "          6.9745e-07, 1.1449e-06, 1.4304e-06, 9.7362e-07, 1.3100e-06,\n",
      "          7.2635e-07, 8.4476e-07, 1.7705e-06, 1.2975e-06, 1.6153e-06,\n",
      "          1.2313e-06],\n",
      "         [6.0993e-05, 9.8179e-05, 9.4308e-05, 1.0791e-04, 2.1764e-04,\n",
      "          1.3854e-04, 1.2358e-04, 7.5868e-04, 7.1335e-04, 1.5474e-04,\n",
      "          9.4864e-04, 1.7702e-04, 2.4910e-04, 8.5626e-04, 1.8295e-04,\n",
      "          2.3236e-04, 2.1423e-04, 1.0279e-02, 1.4665e-02, 9.7275e-04,\n",
      "          2.7690e-04, 3.1716e-04, 3.3071e-03, 3.3187e-03, 1.3088e-04,\n",
      "          2.1927e-02, 4.5876e-04, 3.6982e-02, 2.5565e-03, 1.4092e-03,\n",
      "          9.7314e-03, 1.8943e-02, 3.3025e-01, 3.8969e-01, 4.2513e-04,\n",
      "          5.8352e-03, 3.9114e-02, 5.4500e-02, 1.2791e-02, 1.2127e-03,\n",
      "          3.2938e-03, 1.6913e-02, 8.4937e-03, 6.2822e-03, 5.6455e-05,\n",
      "          1.1463e-04, 9.4886e-05, 6.7267e-05, 8.1843e-05, 1.1980e-04,\n",
      "          5.7075e-05]]])\n",
      "Player 0 Prediction: tensor([[0.4119, 0.0000, 0.5881, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [41:45<05:52, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60136\n",
      "Average episode length: 6.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5353/10000 (53.5%)\n",
      "    Average reward: -0.647\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4647/10000 (46.5%)\n",
      "    Average reward: +0.647\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10024 (33.7%)\n",
      "    Action 1: 16338 (54.9%)\n",
      "    Action 2: 1672 (5.6%)\n",
      "    Action 3: 1752 (5.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8902 (29.3%)\n",
      "    Action 1: 12725 (41.9%)\n",
      "    Action 2: 3165 (10.4%)\n",
      "    Action 3: 5558 (18.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6467.5, 6467.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.004 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6468\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8594, 0.0032, 0.1374]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4560, 0.0386, 0.5055]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51904\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5332/10000 (53.3%)\n",
      "    Average reward: +0.184\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4668/10000 (46.7%)\n",
      "    Average reward: -0.184\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2993 (12.1%)\n",
      "    Action 1: 16282 (65.7%)\n",
      "    Action 2: 2077 (8.4%)\n",
      "    Action 3: 3440 (13.9%)\n",
      "  Player 1:\n",
      "    Action 0: 21262 (78.4%)\n",
      "    Action 1: 5850 (21.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1842.5, -1842.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.767 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.752 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.759\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.1842\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.9103e-05, 3.5426e-05, 3.8019e-05, 3.7790e-05, 4.0610e-05,\n",
      "          3.4257e-05, 4.6345e-05, 6.3435e-04, 1.0309e-03, 3.5662e-04,\n",
      "          4.1888e-03, 1.9020e-03, 2.0888e-02, 2.5877e-02, 6.4481e-03,\n",
      "          1.1010e-02, 1.2505e-03, 1.0570e-02, 8.9916e-03, 2.4307e-03,\n",
      "          9.7967e-04, 4.8013e-04, 5.9053e-04, 6.8407e-04, 9.9347e-05,\n",
      "          2.9894e-01, 4.9536e-05, 1.7404e-03, 1.8150e-03, 1.3586e-03,\n",
      "          8.4417e-03, 3.3867e-02, 1.2984e-01, 1.3475e-01, 1.6936e-02,\n",
      "          1.4847e-01, 1.0363e-02, 3.0197e-02, 2.7083e-02, 4.2819e-03,\n",
      "          1.4414e-02, 7.7157e-03, 1.9231e-02, 1.1580e-02, 4.3020e-05,\n",
      "          6.3149e-05, 3.1077e-05, 4.5957e-05, 3.1148e-05, 3.5998e-05,\n",
      "          2.3872e-05],\n",
      "         [5.9091e-06, 7.4955e-06, 8.9762e-06, 7.7138e-06, 8.3099e-06,\n",
      "          7.5425e-06, 1.0401e-05, 1.4174e-04, 5.1249e-04, 9.0455e-05,\n",
      "          1.4359e-02, 3.6181e-03, 6.2830e-04, 4.5611e-04, 1.3954e-04,\n",
      "          1.2834e-02, 1.2748e-03, 2.2913e-03, 2.2691e-03, 4.7497e-04,\n",
      "          1.3092e-03, 1.2192e-04, 1.0240e-04, 8.6664e-05, 1.8836e-05,\n",
      "          3.1620e-02, 1.2933e-05, 1.7467e-04, 1.8357e-04, 1.1629e-02,\n",
      "          2.6026e-01, 1.5994e-03, 9.6312e-03, 1.6342e-02, 6.3317e-02,\n",
      "          5.3695e-01, 8.4391e-05, 1.8848e-04, 6.8286e-04, 7.0106e-03,\n",
      "          1.9042e-02, 1.2208e-04, 1.7752e-04, 1.3322e-04, 9.8034e-06,\n",
      "          7.9975e-06, 7.0076e-06, 9.4110e-06, 8.4579e-06, 1.1865e-05,\n",
      "          4.7930e-06],\n",
      "         [4.1266e-06, 6.8006e-06, 3.9270e-06, 6.3012e-06, 5.8869e-06,\n",
      "          8.6832e-06, 6.1437e-06, 4.1439e-06, 5.1496e-06, 7.9481e-06,\n",
      "          4.7247e-06, 6.8949e-06, 1.3855e-03, 1.2704e-03, 4.5715e-06,\n",
      "          8.5930e-04, 7.2829e-06, 3.8299e-04, 4.1443e-04, 2.9211e-06,\n",
      "          6.1645e-03, 2.7362e-06, 3.2336e-03, 1.8843e-01, 7.9766e-01,\n",
      "          3.3022e-06, 3.9587e-06, 3.1891e-06, 3.5131e-06, 3.6487e-06,\n",
      "          3.8566e-06, 3.0779e-06, 5.6548e-06, 2.8921e-06, 6.9589e-06,\n",
      "          4.2259e-06, 2.9242e-06, 3.4045e-06, 3.8274e-06, 6.4759e-06,\n",
      "          7.0330e-06, 3.2453e-06, 5.2039e-06, 6.6224e-06, 4.2615e-06,\n",
      "          2.8762e-06, 3.8794e-06, 5.2522e-06, 4.6306e-06, 3.4425e-06,\n",
      "          3.5514e-06],\n",
      "         [8.2781e-05, 1.5039e-04, 1.8526e-04, 1.0386e-04, 2.4713e-04,\n",
      "          1.7144e-04, 1.4963e-04, 2.8525e-04, 3.9623e-04, 5.9051e-04,\n",
      "          3.1797e-03, 3.5338e-03, 8.9262e-03, 1.0404e-02, 8.1672e-04,\n",
      "          1.1245e-02, 9.6576e-04, 2.1261e-02, 2.6220e-02, 3.2995e-03,\n",
      "          1.0168e-03, 9.8744e-04, 2.9975e-02, 2.7527e-02, 3.1535e-03,\n",
      "          5.7747e-01, 1.9117e-04, 6.0357e-03, 6.6008e-03, 4.8311e-04,\n",
      "          2.5993e-03, 1.2352e-02, 8.0889e-02, 8.0261e-02, 8.4995e-03,\n",
      "          2.7794e-02, 1.2994e-03, 1.0564e-02, 1.5077e-02, 2.3262e-03,\n",
      "          3.6660e-03, 1.9803e-03, 3.4465e-03, 2.3191e-03, 1.5637e-04,\n",
      "          2.6983e-04, 1.5772e-04, 1.6919e-04, 1.3919e-04, 1.6660e-04,\n",
      "          2.1203e-04]]])\n",
      "Player 1 Prediction: tensor([[6.4866e-01, 3.5082e-01, 5.1560e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[1.5572e-06, 2.4292e-06, 2.4512e-06, 2.2552e-06, 2.3376e-06,\n",
      "          1.0518e-06, 2.4947e-06, 2.8069e-04, 3.5807e-04, 1.7627e-05,\n",
      "          1.4203e-02, 4.2586e-04, 4.0309e-04, 3.2277e-04, 8.2699e-05,\n",
      "          5.2843e-02, 6.3258e-05, 1.1071e-05, 1.2278e-05, 1.8954e-04,\n",
      "          3.1313e-05, 1.3222e-05, 1.6300e-05, 2.4731e-05, 9.1038e-06,\n",
      "          4.0360e-01, 3.1051e-06, 1.0454e-05, 1.1548e-05, 2.2679e-05,\n",
      "          1.4874e-04, 2.1584e-04, 5.4124e-05, 8.1632e-05, 1.1811e-03,\n",
      "          4.9825e-01, 7.7173e-05, 1.6160e-04, 3.7037e-04, 3.0415e-04,\n",
      "          2.4602e-02, 1.3204e-04, 9.2128e-04, 5.2000e-04, 2.2894e-06,\n",
      "          3.7928e-06, 2.1165e-06, 2.0226e-06, 1.6670e-06, 1.8318e-06,\n",
      "          1.5380e-06],\n",
      "         [3.6807e-06, 4.1646e-06, 5.7019e-06, 4.3180e-06, 5.8555e-06,\n",
      "          3.8646e-06, 8.4404e-06, 2.8411e-04, 7.3522e-04, 8.3965e-05,\n",
      "          6.9276e-04, 4.5188e-04, 1.3598e-04, 1.0781e-04, 9.8619e-05,\n",
      "          7.2128e-03, 4.2887e-05, 2.0085e-04, 1.5284e-04, 1.5003e-04,\n",
      "          1.6696e-04, 1.4491e-05, 8.8457e-05, 7.5851e-05, 8.5595e-06,\n",
      "          9.9914e-03, 7.5526e-06, 5.1383e-05, 5.9878e-05, 2.6551e-05,\n",
      "          8.6661e-01, 2.1809e-03, 3.4617e-02, 5.2487e-02, 5.9378e-05,\n",
      "          1.5389e-02, 1.0090e-04, 2.0314e-04, 5.0458e-04, 2.8072e-03,\n",
      "          3.7353e-03, 1.1831e-04, 1.3853e-04, 1.3622e-04, 7.2360e-06,\n",
      "          5.6239e-06, 4.4174e-06, 5.6652e-06, 5.0946e-06, 6.0982e-06,\n",
      "          4.0744e-06],\n",
      "         [5.3433e-07, 9.3348e-07, 4.3914e-07, 1.3617e-06, 7.2006e-07,\n",
      "          1.3720e-06, 4.8445e-07, 7.8226e-07, 4.9788e-07, 7.1265e-07,\n",
      "          5.7717e-07, 4.0052e-07, 4.5007e-04, 3.5882e-04, 4.4473e-07,\n",
      "          8.3327e-04, 9.4783e-07, 3.1671e-05, 2.3886e-05, 4.6574e-07,\n",
      "          9.9817e-01, 3.5089e-07, 1.1662e-05, 9.5453e-05, 2.8361e-06,\n",
      "          6.0141e-07, 3.6149e-07, 5.3132e-07, 6.1427e-07, 3.2091e-07,\n",
      "          3.3024e-07, 7.7864e-07, 4.1669e-07, 8.6417e-07, 5.2327e-07,\n",
      "          4.1324e-07, 2.2269e-07, 4.1588e-07, 3.4522e-07, 3.5442e-07,\n",
      "          1.7273e-06, 6.7059e-07, 7.1372e-07, 8.9212e-07, 4.1966e-07,\n",
      "          3.7997e-07, 5.3692e-07, 7.1646e-07, 4.8228e-07, 6.7093e-07,\n",
      "          4.8419e-07],\n",
      "         [2.2770e-05, 3.3359e-05, 5.3625e-05, 2.9975e-05, 6.5438e-05,\n",
      "          3.1238e-05, 3.2726e-05, 9.7543e-05, 8.1173e-05, 7.0395e-05,\n",
      "          2.1959e-02, 5.9270e-03, 3.9346e-04, 4.8616e-04, 1.0063e-04,\n",
      "          5.3952e-02, 3.1453e-04, 2.5165e-04, 3.8140e-04, 2.8168e-04,\n",
      "          3.3843e-03, 7.7155e-04, 2.4607e-04, 2.3473e-04, 1.8040e-04,\n",
      "          1.2008e-01, 4.6533e-05, 1.8868e-04, 2.8839e-04, 2.9183e-04,\n",
      "          6.5256e-02, 5.2602e-04, 4.7363e-04, 4.7759e-04, 5.2447e-02,\n",
      "          6.2743e-01, 1.0518e-04, 2.7112e-04, 4.3964e-04, 1.0578e-02,\n",
      "          3.1194e-02, 1.1695e-04, 6.0170e-05, 8.4008e-05, 3.2888e-05,\n",
      "          4.1281e-05, 2.5810e-05, 2.0629e-05, 4.7095e-05, 2.4308e-05,\n",
      "          7.2152e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9054, 0.0133, 0.0813]])\n",
      "Player 0 Prediction: tensor([[[5.2210e-08, 5.7190e-08, 5.9359e-08, 6.4653e-08, 6.2242e-08,\n",
      "          2.3916e-08, 8.6890e-08, 1.0796e-05, 1.2137e-05, 7.7139e-07,\n",
      "          4.1935e-05, 6.5546e-06, 5.0684e-05, 5.3124e-05, 3.3022e-06,\n",
      "          8.0489e-02, 7.5859e-07, 5.8634e-07, 6.5751e-07, 1.2899e-05,\n",
      "          1.7232e-07, 2.4598e-07, 3.7076e-07, 8.1603e-07, 1.6090e-07,\n",
      "          7.4072e-02, 9.3991e-08, 5.7854e-07, 4.6663e-07, 3.8385e-07,\n",
      "          8.6192e-07, 1.3573e-05, 5.1906e-06, 5.0758e-06, 1.2126e-05,\n",
      "          8.4501e-01, 7.5768e-06, 2.4855e-05, 4.4799e-05, 2.6958e-06,\n",
      "          4.1977e-05, 8.2771e-06, 3.6710e-05, 2.6347e-05, 8.0784e-08,\n",
      "          1.1005e-07, 5.5735e-08, 8.0104e-08, 2.8276e-08, 4.2453e-08,\n",
      "          4.2612e-08],\n",
      "         [9.0892e-08, 1.1849e-07, 1.8884e-07, 1.1806e-07, 2.9882e-07,\n",
      "          1.2688e-07, 1.5739e-07, 1.0518e-05, 2.9475e-05, 1.9188e-06,\n",
      "          1.5533e-01, 3.3431e-05, 1.1586e-06, 6.6428e-07, 8.6438e-07,\n",
      "          1.2358e-04, 3.4160e-06, 3.9810e-07, 2.7134e-07, 2.2585e-06,\n",
      "          1.0716e-05, 7.8001e-07, 2.4396e-06, 2.0167e-06, 3.1045e-07,\n",
      "          1.4490e-01, 2.2145e-07, 2.6579e-06, 3.4975e-06, 4.5346e-06,\n",
      "          2.3994e-03, 1.3471e-05, 2.0240e-05, 4.9379e-05, 2.0280e-05,\n",
      "          6.3561e-01, 4.2166e-07, 5.7531e-06, 1.4632e-05, 1.9854e-04,\n",
      "          6.1197e-02, 2.9024e-07, 2.5220e-07, 3.0649e-07, 2.8329e-07,\n",
      "          1.5492e-07, 1.4393e-07, 1.7515e-07, 1.7407e-07, 1.9834e-07,\n",
      "          1.5113e-07],\n",
      "         [4.7531e-08, 8.1332e-08, 2.2819e-08, 1.2596e-07, 9.4614e-08,\n",
      "          1.4363e-07, 5.6522e-08, 6.3883e-08, 4.9644e-08, 1.1342e-07,\n",
      "          5.5969e-08, 5.3094e-08, 5.6543e-05, 5.1410e-05, 3.8615e-08,\n",
      "          3.6320e-05, 8.1797e-08, 5.3060e-06, 3.1283e-06, 4.4217e-08,\n",
      "          9.9984e-01, 4.1685e-08, 3.8526e-07, 1.5022e-06, 4.4536e-07,\n",
      "          6.0836e-08, 4.3943e-08, 4.4293e-08, 6.9071e-08, 2.7408e-08,\n",
      "          4.0993e-08, 5.2007e-08, 4.8483e-08, 6.7138e-08, 3.4433e-08,\n",
      "          5.9141e-08, 1.9698e-08, 4.0805e-08, 5.7738e-08, 3.3282e-08,\n",
      "          1.0476e-07, 7.2677e-08, 7.8926e-08, 7.9279e-08, 6.0056e-08,\n",
      "          4.6689e-08, 6.7352e-08, 7.0731e-08, 5.6972e-08, 6.1932e-08,\n",
      "          4.9157e-08],\n",
      "         [3.3156e-05, 4.3900e-05, 5.3065e-05, 3.7087e-05, 9.9723e-05,\n",
      "          2.9678e-05, 3.6131e-05, 1.0056e-04, 1.1048e-04, 7.4323e-05,\n",
      "          2.3552e-02, 7.2624e-03, 5.8898e-04, 4.4536e-04, 1.6953e-04,\n",
      "          3.4082e-02, 5.3476e-04, 5.8779e-04, 8.9003e-04, 2.8645e-04,\n",
      "          2.3491e-03, 7.2371e-04, 3.0451e-04, 3.4700e-04, 2.4126e-04,\n",
      "          3.9794e-01, 3.6426e-05, 1.3902e-04, 2.2843e-04, 3.5533e-04,\n",
      "          3.3001e-02, 7.8159e-04, 1.0434e-03, 1.0025e-03, 2.8877e-02,\n",
      "          4.2652e-01, 1.1857e-04, 3.0252e-04, 6.5216e-04, 9.4198e-03,\n",
      "          2.5805e-02, 2.0117e-04, 1.4898e-04, 1.2110e-04, 3.2434e-05,\n",
      "          7.0612e-05, 2.3178e-05, 2.3452e-05, 4.7177e-05, 3.0728e-05,\n",
      "          8.9402e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55881\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5732/10000 (57.3%)\n",
      "    Average reward: +0.282\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4268/10000 (42.7%)\n",
      "    Average reward: -0.282\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9814 (35.0%)\n",
      "    Action 1: 14606 (52.1%)\n",
      "    Action 2: 2776 (9.9%)\n",
      "    Action 3: 859 (3.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6795 (24.4%)\n",
      "    Action 1: 14995 (53.9%)\n",
      "    Action 2: 2363 (8.5%)\n",
      "    Action 3: 3673 (13.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2822.5, -2822.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.977 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.999\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2823\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[6.5221e-01, 3.4770e-01, 9.7619e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8293e-01, 6.9918e-05, 1.6999e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50415\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6032/10000 (60.3%)\n",
      "    Average reward: -0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3968/10000 (39.7%)\n",
      "    Average reward: +0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22922 (85.0%)\n",
      "    Action 1: 4055 (15.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2856 (12.2%)\n",
      "    Action 1: 17988 (76.7%)\n",
      "    Action 2: 777 (3.3%)\n",
      "    Action 3: 1817 (7.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1269.5, 1269.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.611 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.663 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.637\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.1270\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}, {'exploitability': 0.395175}, {'exploitability': 0.4645}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43003/50000 [42:56<05:19, 21.89it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 275926/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 280266/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43999/50000 [43:42<04:34, 21.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 282427/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 286964/2000000\n",
      "P1 SL Buffer Size:  282427\n",
      "P1 SL buffer distribution [100950. 146279.  22789.  12409.]\n",
      "P1 actions distribution [0.3574375  0.51793561 0.08068988 0.04393702]\n",
      "P2 SL Buffer Size:  286964\n",
      "P2 SL buffer distribution [ 87607. 143101.  24845.  31411.]\n",
      "P2 actions distribution [0.30528917 0.49867231 0.0865788  0.10945972]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[4.3818e-05, 7.3085e-05, 9.3529e-05, 1.1795e-04, 1.6660e-04,\n",
      "          2.2093e-04, 1.4436e-04, 2.4832e-04, 2.3439e-04, 2.2357e-04,\n",
      "          2.0850e-03, 5.7159e-04, 2.2556e-04, 3.7657e-04, 1.8262e-04,\n",
      "          1.4081e-02, 2.6731e-03, 1.5484e-02, 1.6922e-02, 9.4736e-03,\n",
      "          6.7807e-02, 5.7171e-03, 6.4249e-03, 7.8666e-03, 3.5276e-04,\n",
      "          3.5698e-01, 1.6878e-03, 2.6314e-02, 7.1176e-03, 1.7982e-02,\n",
      "          1.3415e-01, 7.0809e-03, 3.2007e-02, 1.4790e-02, 1.2136e-02,\n",
      "          6.9586e-02, 6.0281e-03, 4.9242e-03, 7.8337e-03, 2.8866e-02,\n",
      "          9.5260e-02, 6.5632e-03, 1.1871e-02, 6.2449e-03, 1.0613e-04,\n",
      "          1.1444e-04, 1.2257e-04, 9.7027e-05, 1.5787e-04, 1.1980e-04,\n",
      "          4.9375e-05],\n",
      "         [2.8083e-05, 3.2654e-05, 4.6990e-05, 4.0307e-05, 5.7258e-05,\n",
      "          7.0006e-05, 5.4250e-05, 6.9549e-05, 7.4953e-05, 5.5329e-05,\n",
      "          2.2426e-03, 7.5339e-04, 1.1170e-02, 2.3443e-02, 3.1895e-03,\n",
      "          4.8828e-02, 5.8151e-03, 2.0647e-03, 3.8217e-03, 1.7827e-03,\n",
      "          1.0632e-02, 8.4012e-04, 8.3726e-04, 9.3342e-04, 1.3724e-04,\n",
      "          8.2798e-02, 4.6280e-04, 3.8062e-03, 1.4810e-03, 3.0749e-02,\n",
      "          5.4756e-01, 9.8711e-04, 1.1562e-03, 8.9589e-04, 1.1368e-02,\n",
      "          1.0596e-01, 1.2085e-03, 1.5866e-03, 2.8781e-03, 2.2360e-02,\n",
      "          6.5265e-02, 5.2239e-04, 1.0928e-03, 5.4527e-04, 4.9301e-05,\n",
      "          4.7047e-05, 5.1935e-05, 3.6450e-05, 4.1222e-05, 4.7982e-05,\n",
      "          2.1008e-05],\n",
      "         [4.4951e-06, 5.6692e-06, 2.5227e-06, 5.6316e-06, 4.8643e-06,\n",
      "          4.3802e-06, 4.6171e-06, 8.2349e-06, 6.7295e-06, 3.7951e-06,\n",
      "          2.4585e-06, 3.8119e-06, 2.2820e-04, 2.8239e-04, 5.0039e-06,\n",
      "          1.2254e-03, 4.2136e-06, 3.1134e-04, 2.6500e-04, 6.3720e-06,\n",
      "          2.1749e-03, 6.1403e-06, 1.0168e-02, 2.4436e-01, 7.4077e-01,\n",
      "          6.1949e-06, 8.8737e-06, 6.7057e-06, 5.5307e-06, 4.5785e-06,\n",
      "          4.0353e-06, 4.9772e-06, 4.1719e-06, 2.2427e-06, 7.2160e-06,\n",
      "          2.5758e-06, 6.5036e-06, 4.4236e-06, 3.4355e-06, 4.6993e-06,\n",
      "          3.2143e-06, 4.7892e-06, 7.2285e-06, 6.7488e-06, 2.6218e-06,\n",
      "          7.4906e-06, 6.9158e-06, 4.3185e-06, 2.9966e-06, 5.1816e-06,\n",
      "          7.5066e-06],\n",
      "         [1.0474e-04, 1.9439e-04, 1.2843e-04, 1.7266e-04, 2.2653e-04,\n",
      "          2.4886e-04, 1.6237e-04, 5.5376e-04, 3.9680e-04, 3.8290e-04,\n",
      "          1.0248e-03, 4.7233e-04, 6.7259e-04, 6.9251e-04, 4.1036e-04,\n",
      "          1.7999e-03, 4.5456e-04, 7.9150e-02, 1.3307e-01, 2.2469e-02,\n",
      "          6.8683e-03, 1.8948e-03, 6.1476e-03, 6.0142e-03, 2.6183e-04,\n",
      "          4.7059e-01, 8.9835e-04, 1.7584e-02, 5.9937e-03, 6.7679e-03,\n",
      "          2.7592e-02, 1.5994e-02, 5.0994e-02, 3.2097e-02, 1.8152e-03,\n",
      "          1.8679e-02, 9.1329e-03, 8.8046e-03, 5.8409e-03, 6.2327e-03,\n",
      "          1.9714e-02, 1.1173e-02, 1.4347e-02, 1.0186e-02, 2.0338e-04,\n",
      "          3.5434e-04, 1.6303e-04, 2.1757e-04, 2.6374e-04, 2.8785e-04,\n",
      "          1.0105e-04]]])\n",
      "Player 0 Prediction: tensor([[2.0787e-01, 7.9179e-01, 3.3988e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[3.6148e-05, 4.8533e-05, 7.8691e-05, 8.7842e-05, 1.4394e-04,\n",
      "          1.2900e-04, 8.7550e-05, 4.0048e-04, 4.1907e-04, 1.3478e-04,\n",
      "          6.0575e-04, 1.9181e-04, 1.5935e-04, 2.8924e-04, 9.0292e-05,\n",
      "          7.5260e-04, 3.7784e-04, 3.5272e-02, 9.2248e-02, 1.5658e-02,\n",
      "          4.3002e-03, 7.0698e-04, 1.5175e-03, 1.8102e-03, 1.7613e-04,\n",
      "          4.1722e-01, 5.9007e-04, 9.4067e-03, 7.1651e-04, 3.9020e-03,\n",
      "          3.8383e-02, 1.0785e-02, 3.5441e-02, 3.0183e-02, 1.1468e-03,\n",
      "          8.6004e-03, 1.9572e-02, 6.2398e-02, 4.4866e-02, 1.8113e-03,\n",
      "          3.8175e-03, 1.7179e-02, 7.6590e-02, 6.1223e-02, 6.4078e-05,\n",
      "          6.9009e-05, 7.3785e-05, 6.6854e-05, 6.2691e-05, 7.2920e-05,\n",
      "          4.0906e-05],\n",
      "         [1.2641e-05, 2.1227e-05, 3.3519e-05, 1.6937e-05, 3.4554e-05,\n",
      "          3.0641e-05, 2.6463e-05, 2.0715e-07, 1.7661e-07, 1.7971e-05,\n",
      "          2.6609e-03, 3.4795e-04, 1.6757e-02, 3.2049e-02, 1.6879e-04,\n",
      "          1.6054e-02, 3.1265e-04, 9.1149e-06, 1.5271e-05, 2.0321e-04,\n",
      "          3.6555e-03, 2.9330e-04, 4.4856e-04, 4.6554e-04, 6.6473e-05,\n",
      "          7.7586e-03, 1.2214e-04, 5.7105e-04, 5.9079e-04, 2.4841e-03,\n",
      "          6.7766e-01, 1.9617e-04, 5.1567e-04, 3.0944e-04, 3.7680e-04,\n",
      "          1.3142e-01, 9.2372e-05, 4.2357e-04, 5.7077e-04, 1.4532e-02,\n",
      "          8.8431e-02, 7.9826e-05, 3.7659e-06, 2.6196e-06, 2.7884e-05,\n",
      "          1.8757e-05, 2.8397e-05, 1.8347e-05, 3.0258e-05, 2.3522e-05,\n",
      "          1.4591e-05],\n",
      "         [4.8221e-07, 4.3155e-07, 4.6252e-07, 6.1222e-07, 3.4712e-07,\n",
      "          4.5420e-07, 5.8533e-07, 3.6137e-07, 3.8111e-07, 3.7201e-07,\n",
      "          2.8841e-07, 3.7978e-07, 4.3724e-05, 4.7287e-05, 4.4139e-07,\n",
      "          5.6770e-05, 4.2522e-07, 9.1874e-05, 7.7478e-05, 3.7020e-07,\n",
      "          9.9962e-01, 5.5408e-07, 1.7333e-06, 2.9448e-05, 1.6549e-05,\n",
      "          5.8571e-07, 4.5734e-07, 5.4635e-07, 3.2596e-07, 4.0431e-07,\n",
      "          3.2534e-07, 6.2030e-07, 4.5444e-07, 2.5154e-07, 8.6177e-07,\n",
      "          4.2796e-07, 2.7511e-07, 2.9392e-07, 2.8629e-07, 4.8084e-07,\n",
      "          2.8445e-07, 3.9278e-07, 4.4553e-07, 9.4384e-07, 1.3260e-07,\n",
      "          5.6028e-07, 4.4734e-07, 4.9072e-07, 4.9653e-07, 5.7740e-07,\n",
      "          4.3003e-07],\n",
      "         [6.7516e-05, 7.8065e-05, 5.0360e-05, 8.0514e-05, 8.2340e-05,\n",
      "          1.1061e-04, 7.8257e-05, 4.1032e-04, 2.6943e-04, 2.8900e-04,\n",
      "          9.5878e-04, 2.3417e-04, 1.1517e-04, 2.0503e-04, 1.5263e-04,\n",
      "          3.7800e-04, 1.3449e-04, 3.6646e-03, 6.1672e-03, 5.6048e-03,\n",
      "          6.1050e-02, 9.2972e-04, 3.1556e-04, 2.5449e-04, 1.1255e-04,\n",
      "          3.7311e-01, 3.8959e-04, 1.0294e-03, 1.3194e-04, 6.2451e-03,\n",
      "          4.4387e-01, 2.0714e-03, 7.8920e-03, 6.8070e-03, 8.1458e-04,\n",
      "          1.5976e-02, 3.0948e-03, 5.2824e-03, 2.7955e-03, 7.4226e-03,\n",
      "          3.3135e-02, 3.3828e-03, 2.0579e-03, 2.0410e-03, 7.8233e-05,\n",
      "          1.1154e-04, 8.2181e-05, 7.4881e-05, 1.6538e-04, 1.0243e-04,\n",
      "          4.5819e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.4986e-01, 4.2417e-04, 4.9715e-02]])\n",
      "Player 1 Prediction: tensor([[[2.8329e-08, 3.9809e-08, 6.1383e-08, 4.6371e-08, 1.4094e-07,\n",
      "          5.8830e-08, 1.1146e-07, 3.2451e-05, 2.0635e-05, 1.8201e-07,\n",
      "          6.0091e-05, 2.6062e-07, 4.7747e-01, 4.7426e-01, 5.6469e-08,\n",
      "          1.7275e-05, 1.1024e-06, 2.1005e-05, 3.1584e-05, 1.7018e-06,\n",
      "          6.4353e-06, 1.5089e-06, 6.7816e-07, 9.0758e-07, 2.0716e-07,\n",
      "          4.1265e-02, 7.3367e-07, 7.6215e-07, 2.8037e-07, 2.4708e-06,\n",
      "          5.3520e-06, 9.1363e-07, 1.1984e-07, 1.0807e-07, 3.5745e-06,\n",
      "          4.6894e-07, 2.2728e-06, 2.3498e-03, 4.4306e-03, 3.6652e-06,\n",
      "          5.7157e-07, 2.6624e-06, 4.4473e-07, 3.7448e-07, 7.7233e-08,\n",
      "          4.5872e-08, 1.1018e-07, 5.1275e-08, 4.4664e-08, 7.0152e-08,\n",
      "          2.4749e-08],\n",
      "         [7.6207e-07, 8.9752e-07, 5.8565e-07, 6.3166e-07, 1.1815e-06,\n",
      "          1.2924e-06, 1.2203e-06, 4.4141e-01, 4.4181e-01, 1.1483e-06,\n",
      "          1.0186e-05, 2.5322e-06, 4.0017e-06, 2.8001e-06, 2.6677e-06,\n",
      "          4.1523e-06, 2.5388e-06, 1.3766e-04, 1.9673e-04, 7.4507e-05,\n",
      "          5.9753e-05, 8.6392e-06, 1.3515e-05, 1.3443e-05, 1.4533e-06,\n",
      "          1.6488e-02, 7.7393e-06, 4.3379e-05, 9.5337e-06, 2.3283e-05,\n",
      "          7.7105e-06, 2.5955e-05, 3.2891e-05, 3.1012e-05, 4.5749e-06,\n",
      "          1.1655e-06, 3.2013e-05, 4.2294e-02, 5.5638e-02, 7.1010e-07,\n",
      "          7.4145e-08, 7.1881e-06, 8.4019e-04, 7.5332e-04, 5.6795e-07,\n",
      "          5.6469e-07, 4.7805e-07, 7.6687e-07, 4.3274e-07, 8.1011e-07,\n",
      "          3.3024e-07],\n",
      "         [1.8463e-07, 1.4986e-07, 2.3941e-07, 2.4610e-07, 2.8044e-07,\n",
      "          2.2258e-07, 2.0247e-07, 4.7330e-07, 2.5261e-07, 2.7147e-07,\n",
      "          5.4761e-07, 1.1147e-07, 1.5712e-05, 1.3068e-05, 1.7544e-07,\n",
      "          6.1902e-05, 2.6913e-07, 4.9955e-01, 5.0032e-01, 3.4556e-07,\n",
      "          2.8404e-06, 2.4023e-07, 5.1519e-06, 1.2014e-05, 3.9937e-06,\n",
      "          3.0031e-07, 4.2680e-07, 1.5062e-07, 2.5574e-07, 5.4402e-07,\n",
      "          3.8290e-07, 2.5443e-07, 4.3186e-07, 3.1560e-07, 5.3190e-07,\n",
      "          6.4947e-07, 1.7352e-07, 3.3171e-07, 5.3632e-07, 4.9532e-07,\n",
      "          1.2790e-07, 3.8297e-07, 3.6266e-07, 4.0596e-07, 3.9327e-07,\n",
      "          3.3206e-07, 2.9544e-07, 3.1437e-07, 2.1281e-07, 3.6228e-07,\n",
      "          6.2910e-07],\n",
      "         [2.2796e-05, 4.5015e-05, 4.8098e-05, 6.3077e-05, 1.1599e-04,\n",
      "          1.1154e-04, 7.8811e-05, 2.9231e-04, 2.1593e-04, 9.7593e-05,\n",
      "          4.3622e-04, 9.6502e-05, 1.4272e-04, 3.6556e-04, 1.3339e-04,\n",
      "          2.1127e-04, 1.5903e-04, 6.0918e-02, 1.5023e-01, 1.1264e-02,\n",
      "          4.7335e-03, 2.6050e-04, 1.1778e-03, 1.6504e-03, 1.1696e-04,\n",
      "          5.2412e-01, 3.7154e-04, 7.7221e-03, 3.3491e-04, 3.1239e-03,\n",
      "          7.1206e-02, 1.2462e-02, 4.5127e-02, 3.7009e-02, 5.7752e-04,\n",
      "          6.6450e-03, 1.5180e-02, 2.4229e-02, 8.6574e-03, 1.1310e-03,\n",
      "          2.8831e-03, 4.3358e-03, 1.0658e-03, 4.8296e-04, 3.3108e-05,\n",
      "          9.8509e-05, 4.5506e-05, 3.2651e-05, 5.2785e-05, 6.3572e-05,\n",
      "          2.0879e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43999/50000 [43:55<04:34, 21.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60232\n",
      "Average episode length: 6.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5390/10000 (53.9%)\n",
      "    Average reward: -0.634\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4610/10000 (46.1%)\n",
      "    Average reward: +0.634\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9957 (33.4%)\n",
      "    Action 1: 16557 (55.5%)\n",
      "    Action 2: 1662 (5.6%)\n",
      "    Action 3: 1644 (5.5%)\n",
      "  Player 1:\n",
      "    Action 0: 8855 (29.1%)\n",
      "    Action 1: 12678 (41.7%)\n",
      "    Action 2: 3226 (10.6%)\n",
      "    Action 3: 5653 (18.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6337.5, 6337.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.000 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.022\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6338\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8737, 0.0029, 0.1234]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4980, 0.0346, 0.4674]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51691\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5475/10000 (54.8%)\n",
      "    Average reward: +0.271\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4525/10000 (45.2%)\n",
      "    Average reward: -0.271\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2779 (11.3%)\n",
      "    Action 1: 16526 (67.2%)\n",
      "    Action 2: 1883 (7.7%)\n",
      "    Action 3: 3394 (13.8%)\n",
      "  Player 1:\n",
      "    Action 0: 21576 (79.6%)\n",
      "    Action 1: 5533 (20.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2710.0, -2710.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.741 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.730 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.735\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2710\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3380, 0.6611, 0.0009, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[5.5030e-06, 5.1930e-06, 8.7322e-06, 1.2101e-05, 1.0839e-05,\n",
      "          6.1256e-06, 9.0331e-06, 8.0332e-06, 1.4208e-05, 1.5439e-05,\n",
      "          5.4387e-04, 6.8615e-04, 1.1172e-04, 8.8379e-05, 4.1049e-04,\n",
      "          5.4083e-03, 7.3620e-04, 4.4431e-03, 7.9588e-03, 7.0271e-04,\n",
      "          3.6482e-02, 3.0885e-03, 2.2541e-04, 3.2518e-04, 2.5462e-05,\n",
      "          3.2816e-01, 1.4093e-05, 1.8752e-04, 1.5323e-04, 3.8978e-03,\n",
      "          7.2033e-02, 3.3343e-04, 4.4971e-03, 4.8804e-03, 2.2876e-02,\n",
      "          3.9659e-01, 6.6217e-05, 1.3208e-04, 6.5711e-04, 2.8489e-02,\n",
      "          7.4950e-02, 1.9436e-04, 3.2267e-04, 1.6495e-04, 1.2230e-05,\n",
      "          1.1931e-05, 1.0137e-05, 8.5597e-06, 1.2193e-05, 9.0104e-06,\n",
      "          5.6779e-06],\n",
      "         [2.1868e-06, 4.7982e-06, 3.9558e-06, 5.0686e-06, 4.7808e-06,\n",
      "          4.3054e-06, 2.7405e-06, 1.0079e-05, 2.7801e-05, 2.7238e-05,\n",
      "          7.4097e-03, 1.8868e-03, 1.0320e-03, 7.2235e-04, 2.5365e-04,\n",
      "          7.0148e-03, 9.9424e-04, 1.8936e-03, 2.0764e-03, 3.4450e-04,\n",
      "          2.4555e-04, 5.4861e-05, 1.3511e-04, 1.7723e-04, 1.6133e-05,\n",
      "          1.4892e-02, 6.3811e-06, 1.4601e-04, 1.7689e-04, 2.3334e-02,\n",
      "          4.0523e-01, 1.3323e-04, 3.9273e-04, 4.9293e-04, 4.8038e-02,\n",
      "          4.7304e-01, 1.8961e-05, 1.8716e-05, 6.9911e-05, 2.3887e-03,\n",
      "          6.7824e-03, 1.1967e-04, 1.8986e-04, 1.5059e-04, 3.6653e-06,\n",
      "          4.6882e-06, 4.4805e-06, 4.5603e-06, 5.1034e-06, 3.1126e-06,\n",
      "          3.7551e-06],\n",
      "         [9.6536e-07, 1.7193e-06, 8.9733e-07, 1.5961e-06, 1.7941e-06,\n",
      "          1.2445e-06, 2.2727e-06, 1.0067e-06, 1.2972e-06, 9.7657e-07,\n",
      "          7.3280e-07, 2.0166e-06, 9.7835e-05, 9.0292e-05, 6.7103e-07,\n",
      "          3.4348e-04, 1.9075e-06, 1.8590e-05, 2.0989e-05, 7.1568e-07,\n",
      "          2.9144e-04, 6.8549e-07, 4.6942e-01, 5.2276e-01, 6.9043e-03,\n",
      "          1.2986e-06, 1.1421e-06, 1.0478e-06, 1.4945e-06, 1.1828e-06,\n",
      "          1.8111e-06, 2.2517e-06, 1.4351e-06, 3.5068e-07, 1.6413e-06,\n",
      "          1.4909e-06, 1.2744e-06, 1.0990e-06, 1.2227e-06, 1.9679e-06,\n",
      "          1.6337e-06, 7.8964e-07, 7.5006e-07, 1.5005e-06, 8.7723e-07,\n",
      "          7.3456e-07, 8.2535e-07, 1.7117e-06, 1.4300e-06, 1.4129e-06,\n",
      "          6.8776e-07],\n",
      "         [8.2531e-05, 1.0759e-04, 1.1278e-04, 1.1655e-04, 1.4788e-04,\n",
      "          1.4119e-04, 1.2356e-04, 2.2032e-04, 3.0100e-04, 2.9487e-04,\n",
      "          5.1011e-04, 5.1549e-04, 6.5352e-03, 8.1547e-03, 1.0311e-03,\n",
      "          1.9215e-03, 6.6055e-04, 1.3124e-02, 1.5756e-02, 1.7328e-03,\n",
      "          2.0567e-03, 4.9292e-04, 3.2474e-01, 3.0679e-01, 4.9719e-03,\n",
      "          1.2960e-01, 1.6983e-04, 6.0440e-03, 6.2988e-03, 7.0944e-04,\n",
      "          5.1689e-03, 3.1225e-03, 2.9863e-02, 2.6667e-02, 2.3059e-03,\n",
      "          4.0194e-03, 1.8811e-03, 2.6324e-02, 2.1770e-02, 1.4449e-03,\n",
      "          2.5857e-03, 6.9417e-03, 2.0636e-02, 1.2935e-02, 1.3095e-04,\n",
      "          1.2672e-04, 1.4645e-04, 1.3027e-04, 1.0748e-04, 1.2143e-04,\n",
      "          1.0304e-04]]])\n",
      "Player 1 Prediction: tensor([[1.2248e-02, 9.8772e-01, 2.9495e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[3.3085e-05, 3.3319e-05, 3.9406e-05, 4.4577e-05, 3.5637e-05,\n",
      "          3.8219e-05, 3.3234e-05, 5.3931e-04, 5.7562e-04, 1.7750e-04,\n",
      "          2.7431e-03, 7.9815e-04, 1.1959e-01, 1.4068e-01, 1.5966e-02,\n",
      "          3.7028e-04, 1.9233e-04, 1.0136e-01, 8.2275e-02, 7.3779e-03,\n",
      "          3.9681e-04, 1.8289e-04, 6.7113e-04, 1.7803e-03, 1.9633e-04,\n",
      "          1.5586e-01, 4.6439e-05, 3.2964e-04, 4.0068e-04, 2.5382e-04,\n",
      "          8.4782e-04, 9.8313e-03, 1.0482e-01, 9.9070e-02, 3.9416e-04,\n",
      "          5.1653e-04, 2.2586e-03, 1.5475e-02, 1.3512e-02, 2.4266e-04,\n",
      "          8.8323e-04, 1.4299e-02, 5.9134e-02, 4.5449e-02, 4.2989e-05,\n",
      "          3.5606e-05, 3.3535e-05, 3.5405e-05, 3.6234e-05, 4.1819e-05,\n",
      "          2.6800e-05],\n",
      "         [1.8602e-06, 3.0171e-06, 2.4720e-06, 5.0552e-06, 2.5645e-06,\n",
      "          2.2104e-06, 2.0496e-06, 4.8237e-07, 1.6951e-06, 1.6648e-05,\n",
      "          1.0142e-02, 6.4177e-04, 1.0390e-03, 6.5945e-04, 3.8485e-05,\n",
      "          7.1288e-03, 6.3224e-05, 1.3720e-05, 1.2229e-05, 7.9704e-05,\n",
      "          1.6296e-04, 1.9985e-05, 9.0212e-05, 8.3430e-05, 7.6148e-06,\n",
      "          1.1565e-03, 3.9261e-06, 1.1967e-05, 1.6646e-05, 1.9198e-04,\n",
      "          4.0974e-01, 8.2105e-05, 1.1277e-04, 1.7190e-04, 1.8457e-04,\n",
      "          5.5840e-01, 1.6313e-06, 5.1689e-06, 1.9521e-05, 1.5442e-03,\n",
      "          8.1130e-03, 7.9726e-06, 6.9634e-08, 6.3491e-08, 3.2692e-06,\n",
      "          4.2230e-06, 2.2358e-06, 3.2820e-06, 4.1829e-06, 3.9200e-06,\n",
      "          2.4044e-06],\n",
      "         [6.4337e-07, 1.0616e-06, 4.4703e-07, 8.0995e-07, 1.8133e-06,\n",
      "          9.0836e-07, 1.2628e-06, 6.8132e-07, 4.2600e-07, 5.8748e-07,\n",
      "          6.8976e-07, 9.7166e-07, 9.8811e-05, 5.9522e-05, 4.6959e-07,\n",
      "          3.1452e-05, 8.2656e-07, 1.0159e-04, 6.1028e-05, 6.7484e-07,\n",
      "          9.9958e-01, 5.6940e-07, 2.2602e-06, 1.7364e-05, 1.7001e-05,\n",
      "          6.4012e-07, 6.1005e-07, 4.4382e-07, 8.7767e-07, 4.3304e-07,\n",
      "          6.5765e-07, 8.8567e-07, 6.3147e-07, 3.7355e-07, 5.1919e-07,\n",
      "          7.4525e-07, 5.8992e-07, 5.5735e-07, 1.0495e-06, 4.0083e-07,\n",
      "          8.1674e-07, 4.0042e-07, 3.4224e-07, 9.3300e-07, 6.3928e-07,\n",
      "          6.1939e-07, 7.0814e-07, 9.4192e-07, 9.0910e-07, 1.1062e-06,\n",
      "          4.8143e-07],\n",
      "         [9.6494e-05, 8.8284e-05, 9.6111e-05, 1.3881e-04, 1.3952e-04,\n",
      "          1.4477e-04, 8.4365e-05, 3.5993e-04, 4.7870e-04, 2.8115e-04,\n",
      "          2.1124e-03, 1.5158e-03, 3.3067e-04, 3.8362e-04, 3.4783e-04,\n",
      "          4.1597e-03, 7.4402e-04, 5.3666e-02, 8.4112e-02, 2.0220e-03,\n",
      "          1.2642e-02, 1.0138e-03, 4.6668e-04, 6.3423e-04, 7.1019e-04,\n",
      "          7.6691e-01, 1.5485e-04, 4.7808e-05, 4.7369e-05, 4.3539e-04,\n",
      "          1.5770e-02, 7.4271e-04, 4.1021e-03, 3.1949e-03, 5.5026e-03,\n",
      "          1.3208e-02, 2.1808e-04, 1.0966e-03, 1.6226e-03, 4.4992e-03,\n",
      "          7.3225e-03, 2.1814e-03, 3.5571e-03, 1.8172e-03, 9.4716e-05,\n",
      "          1.4843e-04, 1.1901e-04, 1.1286e-04, 1.0719e-04, 7.3436e-05,\n",
      "          1.4387e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2305, 0.0050, 0.7646]])\n",
      "Player 0 Prediction: tensor([[[1.8572e-06, 2.5900e-06, 3.8165e-06, 2.9560e-06, 3.8510e-06,\n",
      "          2.8423e-06, 2.0479e-06, 3.6979e-06, 3.4556e-06, 8.0408e-06,\n",
      "          1.5121e-05, 2.5585e-05, 1.7495e-03, 2.1298e-03, 1.6548e-04,\n",
      "          1.3285e-07, 2.7013e-05, 7.9921e-03, 8.0753e-03, 1.2502e-04,\n",
      "          2.5360e-05, 2.5805e-05, 3.6066e-05, 6.9178e-05, 1.3483e-05,\n",
      "          6.2218e-03, 2.8612e-06, 1.2977e-05, 1.5323e-05, 3.5192e-05,\n",
      "          2.8394e-05, 2.5931e-04, 2.4005e-01, 2.0979e-01, 3.3098e-05,\n",
      "          1.9462e-05, 9.6505e-05, 2.4756e-01, 2.5325e-01, 2.0222e-05,\n",
      "          6.4502e-04, 1.6713e-04, 1.1591e-02, 9.6588e-03, 3.7389e-06,\n",
      "          2.3036e-06, 3.9823e-06, 2.3188e-06, 4.0741e-06, 3.6793e-06,\n",
      "          2.3783e-06],\n",
      "         [7.5577e-06, 1.5166e-05, 1.4159e-05, 1.6532e-05, 1.2205e-05,\n",
      "          1.3760e-05, 8.6163e-06, 3.2387e-05, 4.7903e-05, 1.6662e-04,\n",
      "          1.8813e-05, 3.9233e-04, 2.7565e-03, 2.5596e-03, 5.2229e-04,\n",
      "          1.3211e-04, 1.3451e-04, 1.3772e-03, 7.4242e-04, 7.6572e-04,\n",
      "          7.5121e-05, 3.3009e-05, 9.7559e-05, 9.7962e-05, 2.2831e-05,\n",
      "          1.2279e-04, 1.2572e-05, 6.8625e-05, 7.3429e-05, 1.2468e-04,\n",
      "          1.0590e-04, 1.9671e-03, 2.0910e-01, 2.1010e-01, 5.0242e-05,\n",
      "          1.6540e-04, 4.5066e-05, 8.1018e-02, 8.2708e-02, 9.5033e-05,\n",
      "          4.6111e-04, 3.1427e-02, 2.0474e-01, 1.6746e-01, 1.1800e-05,\n",
      "          1.9065e-05, 1.9197e-05, 1.2204e-05, 1.5464e-05, 8.2867e-06,\n",
      "          1.0114e-05],\n",
      "         [3.3236e-05, 2.8599e-05, 2.7786e-05, 3.0075e-05, 3.7282e-05,\n",
      "          2.8557e-05, 2.6846e-05, 2.9265e-05, 2.2163e-05, 2.8987e-05,\n",
      "          2.6793e-05, 3.1232e-05, 2.5138e-03, 1.8454e-03, 1.5321e-05,\n",
      "          9.7477e-04, 3.7603e-05, 4.8086e-01, 5.0514e-01, 5.0464e-05,\n",
      "          5.6813e-04, 2.3905e-05, 1.8590e-03, 4.3982e-03, 4.4798e-04,\n",
      "          2.6560e-05, 2.4800e-05, 3.9895e-05, 3.0393e-05, 5.8394e-05,\n",
      "          2.7016e-05, 3.9885e-05, 6.0620e-05, 1.7021e-05, 3.3313e-05,\n",
      "          5.3614e-05, 4.3293e-05, 2.0650e-05, 4.6312e-05, 3.1007e-05,\n",
      "          2.9077e-05, 2.3548e-05, 2.3350e-05, 2.7503e-05, 2.7237e-05,\n",
      "          3.6006e-05, 3.3697e-05, 2.7571e-05, 5.4679e-05, 2.6127e-05,\n",
      "          5.0706e-05],\n",
      "         [3.3418e-05, 3.0141e-05, 3.8377e-05, 4.6073e-05, 3.2430e-05,\n",
      "          3.6848e-05, 2.8321e-05, 4.7355e-04, 4.2515e-04, 1.8802e-04,\n",
      "          7.7652e-05, 1.1320e-04, 2.0052e-04, 3.2269e-04, 1.4660e-04,\n",
      "          1.8515e-04, 9.4167e-05, 3.5913e-03, 3.5780e-03, 1.4713e-03,\n",
      "          1.5986e-05, 9.0369e-05, 4.0789e-04, 9.1514e-04, 3.0188e-04,\n",
      "          1.7061e-03, 3.8153e-05, 1.3302e-03, 1.1836e-03, 8.6121e-05,\n",
      "          3.3763e-04, 1.3305e-03, 4.3794e-01, 4.4184e-01, 2.2322e-04,\n",
      "          3.2948e-04, 1.6502e-04, 2.4975e-03, 2.1066e-03, 1.9113e-04,\n",
      "          4.6346e-04, 9.6769e-03, 4.5400e-02, 4.0100e-02, 3.4813e-05,\n",
      "          3.0440e-05, 3.9821e-05, 3.1420e-05, 2.4851e-05, 3.0260e-05,\n",
      "          2.4447e-05]]])\n",
      "Player 1 Prediction: tensor([[0.1102, 0.1582, 0.7316, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57665\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5889/10000 (58.9%)\n",
      "    Average reward: +0.144\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4111/10000 (41.1%)\n",
      "    Average reward: -0.144\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8191 (28.8%)\n",
      "    Action 1: 17059 (59.9%)\n",
      "    Action 2: 2374 (8.3%)\n",
      "    Action 3: 837 (2.9%)\n",
      "  Player 1:\n",
      "    Action 0: 8248 (28.2%)\n",
      "    Action 1: 14263 (48.8%)\n",
      "    Action 2: 3006 (10.3%)\n",
      "    Action 3: 3687 (12.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1442.0, -1442.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.960 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.990\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1442\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3380, 0.6611, 0.0009, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8115, 0.0040, 0.1845]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50596\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6068/10000 (60.7%)\n",
      "    Average reward: -0.101\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3932/10000 (39.3%)\n",
      "    Average reward: +0.101\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22972 (84.9%)\n",
      "    Action 1: 4098 (15.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2806 (11.9%)\n",
      "    Action 1: 17997 (76.5%)\n",
      "    Action 2: 798 (3.4%)\n",
      "    Action 3: 1925 (8.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1012.5, 1012.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.613 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.662 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.637\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.1013\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}, {'exploitability': 0.395175}, {'exploitability': 0.4645}, {'exploitability': 0.388975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45003/50000 [45:08<03:55, 21.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 288994/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 293746/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [45:55<03:05, 21.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 295672/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 300473/2000000\n",
      "P1 SL Buffer Size:  295672\n",
      "P1 SL buffer distribution [104949. 153911.  23898.  12914.]\n",
      "P1 actions distribution [0.35495076 0.52054642 0.08082605 0.04367678]\n",
      "P2 SL Buffer Size:  300473\n",
      "P2 SL buffer distribution [ 91068. 149094.  26288.  34023.]\n",
      "P2 actions distribution [0.30308214 0.49619766 0.08748873 0.11323147]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [46:08<03:05, 21.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[8.0284e-05, 1.1041e-04, 1.2379e-04, 1.1916e-04, 1.1912e-04,\n",
      "          1.6062e-04, 1.4389e-04, 5.5092e-04, 8.4408e-04, 4.7311e-04,\n",
      "          1.9761e-04, 2.7799e-04, 4.6069e-04, 6.6420e-04, 3.8910e-04,\n",
      "          1.9620e-04, 2.8148e-04, 9.5151e-03, 1.2942e-02, 2.6161e-03,\n",
      "          6.1381e-04, 4.1945e-04, 3.5708e-03, 3.7511e-03, 2.6232e-04,\n",
      "          1.9990e-02, 2.5228e-03, 3.7980e-02, 7.2172e-03, 2.6378e-03,\n",
      "          1.3848e-02, 6.4948e-02, 2.9414e-01, 2.4754e-01, 1.8540e-03,\n",
      "          1.1845e-02, 4.0436e-02, 5.4783e-02, 2.5605e-02, 2.0126e-03,\n",
      "          8.0819e-03, 3.2316e-02, 5.9055e-02, 3.3600e-02, 1.0317e-04,\n",
      "          1.1621e-04, 1.2072e-04, 1.0683e-04, 1.1659e-04, 9.1574e-05,\n",
      "          4.4872e-05],\n",
      "         [8.7369e-05, 1.4617e-04, 1.3553e-04, 1.2747e-04, 1.8877e-04,\n",
      "          1.9513e-04, 1.9094e-04, 6.6191e-04, 7.1175e-04, 3.5648e-04,\n",
      "          8.9461e-04, 5.2276e-04, 5.0061e-04, 8.3308e-04, 5.8458e-04,\n",
      "          6.0799e-03, 1.2228e-03, 4.0008e-03, 5.7604e-03, 1.7281e-03,\n",
      "          1.8100e-03, 7.1807e-04, 4.3909e-03, 4.6797e-03, 3.8823e-04,\n",
      "          2.3038e-02, 2.6145e-03, 5.4173e-02, 1.4169e-02, 1.2917e-02,\n",
      "          2.0861e-01, 4.1613e-02, 1.7062e-01, 1.5131e-01, 1.0182e-02,\n",
      "          7.8338e-02, 3.1517e-02, 3.5678e-02, 1.7450e-02, 6.4399e-03,\n",
      "          1.8466e-02, 1.8400e-02, 4.0510e-02, 2.6103e-02, 1.5366e-04,\n",
      "          1.2386e-04, 1.5536e-04, 1.2364e-04, 1.5586e-04, 1.2411e-04,\n",
      "          1.0096e-04],\n",
      "         [2.0210e-05, 2.3083e-05, 3.1041e-05, 2.4523e-05, 1.9606e-05,\n",
      "          2.3133e-05, 1.6942e-05, 3.7006e-05, 3.7074e-05, 1.7212e-05,\n",
      "          1.2174e-05, 2.0744e-05, 2.3320e-03, 1.5793e-03, 2.8800e-05,\n",
      "          1.8687e-03, 2.3358e-05, 1.7633e-03, 1.9943e-03, 2.4281e-05,\n",
      "          5.0182e-04, 1.8380e-05, 7.9905e-03, 2.3152e-01, 7.4943e-01,\n",
      "          2.7482e-05, 3.4236e-05, 3.0575e-05, 2.5437e-05, 4.2336e-05,\n",
      "          2.6324e-05, 2.4080e-05, 2.4194e-05, 1.8916e-05, 2.4034e-05,\n",
      "          2.4927e-05, 3.4272e-05, 2.7160e-05, 3.1987e-05, 2.3116e-05,\n",
      "          1.7871e-05, 1.7728e-05, 2.1474e-05, 1.9918e-05, 2.1543e-05,\n",
      "          1.9698e-05, 2.4456e-05, 2.3287e-05, 1.8166e-05, 2.3316e-05,\n",
      "          2.1469e-05],\n",
      "         [1.7639e-04, 4.0282e-04, 3.6199e-04, 2.8991e-04, 6.8788e-04,\n",
      "          3.4549e-04, 4.2165e-04, 8.2903e-04, 1.0121e-03, 5.7158e-04,\n",
      "          1.4200e-03, 7.4690e-04, 1.7738e-03, 2.4224e-03, 5.5795e-04,\n",
      "          2.0000e-03, 6.1938e-04, 1.3077e-02, 1.9492e-02, 2.7938e-03,\n",
      "          3.2421e-04, 9.4078e-04, 9.1822e-03, 7.5250e-03, 4.4182e-04,\n",
      "          3.7881e-02, 2.1954e-03, 5.5159e-02, 3.5611e-02, 4.1328e-03,\n",
      "          1.0367e-02, 6.7946e-02, 2.4073e-01, 1.6852e-01, 2.8039e-03,\n",
      "          1.8523e-02, 1.9052e-02, 4.6575e-02, 2.6579e-02, 4.1305e-03,\n",
      "          9.5046e-03, 5.4961e-02, 7.7237e-02, 4.6731e-02, 3.6799e-04,\n",
      "          4.3807e-04, 3.5228e-04, 3.3140e-04, 5.2326e-04, 5.7818e-04,\n",
      "          3.4843e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8760, 0.0029, 0.1211]])\n",
      "Player 1 Prediction: tensor([[[5.8310e-05, 6.6283e-05, 5.1484e-05, 4.7783e-05, 4.8934e-05,\n",
      "          9.3294e-05, 5.2473e-05, 1.2585e-03, 1.8520e-03, 1.2622e-04,\n",
      "          1.1170e-03, 2.9314e-04, 4.9634e-04, 5.9672e-04, 1.2286e-04,\n",
      "          2.0847e-03, 5.9562e-04, 2.3451e-02, 2.7057e-02, 1.3640e-03,\n",
      "          3.0210e-03, 7.2467e-04, 2.3069e-03, 2.4944e-03, 9.6974e-05,\n",
      "          6.4926e-01, 8.9265e-04, 1.8991e-02, 6.1595e-03, 6.7532e-03,\n",
      "          5.0749e-02, 1.4812e-02, 7.3322e-02, 5.7097e-02, 2.2820e-03,\n",
      "          8.5925e-03, 1.4217e-02, 2.5057e-03, 1.3924e-03, 4.8191e-03,\n",
      "          4.6657e-03, 6.1092e-03, 4.8490e-03, 2.6651e-03, 5.7144e-05,\n",
      "          6.0016e-05, 5.0678e-05, 7.1540e-05, 6.7906e-05, 4.4497e-05,\n",
      "          3.1773e-05],\n",
      "         [1.7180e-05, 2.3125e-05, 2.2920e-05, 2.4350e-05, 3.5029e-05,\n",
      "          5.2996e-05, 3.1353e-05, 3.1146e-04, 3.7068e-04, 5.7865e-05,\n",
      "          2.5762e-04, 8.1939e-05, 3.2283e-04, 4.2720e-04, 1.1501e-04,\n",
      "          8.4127e-04, 2.0869e-04, 1.7944e-02, 1.9523e-02, 7.7870e-04,\n",
      "          3.9894e-04, 1.5048e-04, 7.0327e-04, 9.2312e-04, 5.2684e-05,\n",
      "          4.0779e-02, 7.9048e-04, 6.0347e-02, 1.4377e-02, 3.1775e-03,\n",
      "          9.4417e-03, 2.0404e-02, 3.8092e-01, 3.9203e-01, 1.8767e-03,\n",
      "          3.3418e-03, 1.2472e-02, 5.8696e-03, 2.9705e-03, 3.3745e-04,\n",
      "          4.4085e-04, 2.9206e-03, 2.0363e-03, 1.6460e-03, 2.3643e-05,\n",
      "          2.1066e-05, 2.6947e-05, 1.6635e-05, 2.7591e-05, 2.0000e-05,\n",
      "          9.5639e-06],\n",
      "         [6.7892e-06, 5.7774e-06, 6.7643e-06, 7.1305e-06, 6.4129e-06,\n",
      "          7.4880e-06, 5.8830e-06, 9.5698e-06, 9.1457e-06, 5.8246e-06,\n",
      "          5.0730e-06, 4.8163e-06, 1.1608e-03, 1.0897e-03, 5.6551e-06,\n",
      "          2.3248e-03, 6.4924e-06, 4.2693e-04, 4.4512e-04, 7.2655e-06,\n",
      "          3.7359e-05, 4.1080e-06, 4.6138e-01, 5.2245e-01, 1.0384e-02,\n",
      "          9.1760e-06, 1.0499e-05, 8.1849e-06, 5.4473e-06, 1.3782e-05,\n",
      "          7.7082e-06, 5.7416e-06, 6.8328e-06, 3.8889e-06, 6.7747e-06,\n",
      "          3.2390e-06, 7.9878e-06, 6.3526e-06, 1.0996e-05, 1.0118e-05,\n",
      "          5.2816e-06, 6.0629e-06, 1.1982e-05, 5.6756e-06, 7.9264e-06,\n",
      "          5.0833e-06, 6.6610e-06, 5.4377e-06, 5.9549e-06, 8.5596e-06,\n",
      "          5.9433e-06],\n",
      "         [6.5687e-05, 1.2025e-04, 7.6737e-05, 9.4384e-05, 1.8297e-04,\n",
      "          1.4439e-04, 1.1146e-04, 2.0446e-04, 3.1778e-04, 1.4284e-04,\n",
      "          5.3006e-04, 2.9825e-04, 1.0193e-03, 1.0500e-03, 2.2302e-04,\n",
      "          1.0526e-03, 2.2644e-04, 8.0623e-03, 7.1892e-03, 1.6424e-03,\n",
      "          2.7383e-04, 3.3070e-04, 1.4359e-02, 1.3559e-02, 1.4567e-04,\n",
      "          3.4245e-02, 1.3843e-03, 2.7795e-01, 1.4436e-01, 2.8503e-03,\n",
      "          6.3322e-03, 6.3760e-02, 2.2427e-01, 1.2774e-01, 1.2527e-03,\n",
      "          7.9774e-03, 1.2314e-02, 9.6234e-03, 4.5610e-03, 2.6356e-03,\n",
      "          9.4153e-03, 9.3501e-03, 4.8081e-03, 2.9306e-03, 1.6143e-04,\n",
      "          9.3026e-05, 1.0020e-04, 1.1343e-04, 1.0650e-04, 1.3726e-04,\n",
      "          1.1229e-04]]])\n",
      "Player 0 Prediction: tensor([[0.6922, 0.2541, 0.0537, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[4.7576e-07, 7.5160e-07, 6.1957e-07, 4.0798e-07, 8.0502e-07,\n",
      "          7.1464e-07, 8.6546e-07, 3.4912e-05, 2.9165e-05, 2.8153e-06,\n",
      "          5.6529e-05, 1.6712e-06, 1.5338e-01, 1.5379e-01, 7.6889e-07,\n",
      "          1.2428e-05, 2.5579e-06, 2.4419e-05, 2.4737e-05, 2.9733e-06,\n",
      "          4.3420e-06, 2.7237e-06, 9.5244e-06, 8.0854e-06, 7.8265e-07,\n",
      "          3.7210e-01, 9.7096e-06, 2.5296e-05, 9.1482e-06, 1.2903e-05,\n",
      "          1.8997e-05, 3.3599e-05, 3.1085e-05, 2.4812e-05, 9.9158e-06,\n",
      "          1.7918e-05, 5.6606e-05, 1.5978e-01, 1.6037e-01, 1.4343e-05,\n",
      "          4.4302e-05, 2.7432e-05, 9.8361e-06, 9.4776e-06, 6.3439e-07,\n",
      "          5.2166e-07, 7.1805e-07, 5.0353e-07, 7.1357e-07, 5.7726e-07,\n",
      "          3.4659e-07],\n",
      "         [1.8636e-06, 3.6016e-06, 2.4518e-06, 2.1544e-06, 2.8530e-06,\n",
      "          6.0920e-06, 3.3011e-06, 1.2794e-01, 1.2760e-01, 4.7498e-06,\n",
      "          3.1636e-05, 4.3250e-06, 1.5271e-06, 1.2915e-06, 3.1004e-06,\n",
      "          7.0704e-07, 2.5431e-06, 3.3259e-05, 3.2719e-05, 2.2627e-05,\n",
      "          7.5068e-06, 7.2752e-06, 2.4258e-05, 2.6113e-05, 3.9515e-06,\n",
      "          4.1763e-01, 3.9332e-05, 4.7021e-04, 1.1924e-04, 3.4089e-05,\n",
      "          2.3767e-05, 2.0831e-04, 2.5615e-04, 2.8739e-04, 1.2239e-05,\n",
      "          4.3610e-05, 2.4557e-04, 1.2659e-01, 1.2648e-01, 8.1626e-06,\n",
      "          1.8149e-05, 7.5190e-05, 3.6416e-02, 3.5276e-02, 1.9535e-06,\n",
      "          1.6357e-06, 1.9703e-06, 1.9713e-06, 2.2864e-06, 2.2907e-06,\n",
      "          7.9308e-07],\n",
      "         [7.7162e-07, 5.7144e-07, 1.0990e-06, 8.0670e-07, 6.7457e-07,\n",
      "          8.6701e-07, 5.6063e-07, 1.4636e-06, 1.0113e-06, 8.7822e-07,\n",
      "          1.6618e-06, 3.7637e-07, 7.2542e-05, 8.7584e-05, 1.1239e-06,\n",
      "          1.6852e-04, 1.3696e-06, 4.9315e-01, 5.0643e-01, 8.3180e-07,\n",
      "          2.1829e-06, 6.9044e-07, 1.2763e-05, 2.4868e-05, 7.6685e-06,\n",
      "          7.9969e-07, 1.6127e-06, 7.0865e-07, 7.6939e-07, 3.0360e-06,\n",
      "          1.3136e-06, 9.1041e-07, 1.3145e-06, 1.0249e-06, 1.2853e-06,\n",
      "          2.1407e-06, 9.5135e-07, 1.4535e-06, 2.6383e-06, 1.9348e-06,\n",
      "          5.6760e-07, 1.2557e-06, 1.4190e-06, 8.2201e-07, 1.3020e-06,\n",
      "          5.9792e-07, 8.3802e-07, 1.0594e-06, 1.0827e-06, 1.7018e-06,\n",
      "          1.0297e-06],\n",
      "         [4.0817e-05, 7.4734e-05, 6.3288e-05, 8.1088e-05, 2.1091e-04,\n",
      "          1.1302e-04, 9.8426e-05, 5.6720e-04, 4.9105e-04, 7.6025e-05,\n",
      "          6.8132e-04, 1.2967e-04, 3.4645e-04, 8.7343e-04, 1.8690e-04,\n",
      "          3.5695e-04, 1.9700e-04, 3.7324e-03, 4.9803e-03, 5.8381e-04,\n",
      "          1.2641e-04, 1.5367e-04, 1.2022e-03, 1.1452e-03, 1.0478e-04,\n",
      "          2.1664e-02, 1.0141e-03, 5.5266e-02, 3.5916e-03, 1.7629e-03,\n",
      "          1.8769e-02, 3.8460e-02, 3.2005e-01, 3.6195e-01, 9.7161e-04,\n",
      "          6.9858e-03, 4.6725e-02, 5.5973e-02, 1.5545e-02, 1.6471e-03,\n",
      "          6.2474e-03, 1.8255e-02, 4.5483e-03, 3.5052e-03, 6.1992e-05,\n",
      "          7.0700e-05, 5.7140e-05, 6.3518e-05, 6.0601e-05, 9.7444e-05,\n",
      "          4.1717e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60321\n",
      "Average episode length: 6.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5465/10000 (54.6%)\n",
      "    Average reward: -0.569\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4535/10000 (45.4%)\n",
      "    Average reward: +0.569\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9990 (33.5%)\n",
      "    Action 1: 16582 (55.5%)\n",
      "    Action 2: 1651 (5.5%)\n",
      "    Action 3: 1632 (5.5%)\n",
      "  Player 1:\n",
      "    Action 0: 8800 (28.9%)\n",
      "    Action 1: 12703 (41.7%)\n",
      "    Action 2: 3271 (10.7%)\n",
      "    Action 3: 5692 (18.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5687.5, 5687.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.000 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.022\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5687\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8760, 0.0029, 0.1211]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6001, 0.0337, 0.3662]])\n",
      "Player 0 Prediction: tensor([[0.0354, 0.1198, 0.8448, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52003\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5451/10000 (54.5%)\n",
      "    Average reward: +0.252\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4549/10000 (45.5%)\n",
      "    Average reward: -0.252\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2943 (11.9%)\n",
      "    Action 1: 16471 (66.5%)\n",
      "    Action 2: 1895 (7.7%)\n",
      "    Action 3: 3447 (13.9%)\n",
      "  Player 1:\n",
      "    Action 0: 21503 (78.9%)\n",
      "    Action 1: 5744 (21.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2522.5, -2522.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.756 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.743 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.750\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2522\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.5039e-05, 2.8074e-05, 2.9367e-05, 3.0258e-05, 3.2507e-05,\n",
      "          2.6924e-05, 3.5222e-05, 4.4193e-04, 7.0176e-04, 2.7760e-04,\n",
      "          2.6991e-03, 1.3643e-03, 1.5681e-02, 2.0131e-02, 4.7619e-03,\n",
      "          7.4594e-03, 1.3906e-03, 9.3118e-03, 8.7540e-03, 1.5889e-03,\n",
      "          8.0434e-04, 3.3736e-04, 4.0730e-04, 4.8881e-04, 7.5559e-05,\n",
      "          2.3675e-01, 3.8614e-05, 9.7028e-04, 1.0510e-03, 9.9674e-04,\n",
      "          7.4907e-03, 3.4354e-02, 2.1172e-01, 2.1770e-01, 2.5542e-02,\n",
      "          6.7589e-02, 8.6203e-03, 3.1783e-02, 2.7073e-02, 4.2031e-03,\n",
      "          1.4319e-02, 6.6097e-03, 1.6030e-02, 1.0072e-02, 3.4025e-05,\n",
      "          4.9642e-05, 2.4868e-05, 3.5745e-05, 2.4635e-05, 2.8647e-05,\n",
      "          1.9124e-05],\n",
      "         [5.4811e-06, 6.9304e-06, 8.3100e-06, 7.5969e-06, 7.7758e-06,\n",
      "          7.0972e-06, 9.8937e-06, 1.4400e-04, 4.8154e-04, 9.8392e-05,\n",
      "          8.1453e-03, 2.6712e-03, 5.6037e-04, 3.8556e-04, 1.2267e-04,\n",
      "          1.7410e-02, 1.8871e-03, 1.7244e-03, 1.5417e-03, 3.6061e-04,\n",
      "          1.1558e-03, 1.3454e-04, 9.5848e-05, 8.3770e-05, 1.8292e-05,\n",
      "          1.9985e-02, 1.2357e-05, 1.9388e-04, 2.0062e-04, 1.4522e-02,\n",
      "          2.7225e-01, 2.4860e-03, 1.5512e-02, 2.7652e-02, 7.0075e-02,\n",
      "          5.1517e-01, 8.5793e-05, 2.0985e-04, 7.9010e-04, 5.4343e-03,\n",
      "          1.7907e-02, 1.2301e-04, 1.4858e-04, 1.1312e-04, 9.1613e-06,\n",
      "          7.5587e-06, 6.6073e-06, 8.9717e-06, 8.0824e-06, 1.1376e-05,\n",
      "          4.4685e-06],\n",
      "         [4.1551e-06, 6.9121e-06, 4.1005e-06, 6.4969e-06, 6.0830e-06,\n",
      "          8.7584e-06, 6.1246e-06, 4.2394e-06, 5.2712e-06, 8.1030e-06,\n",
      "          4.8470e-06, 6.9324e-06, 1.5126e-03, 1.3940e-03, 4.6809e-06,\n",
      "          9.9276e-04, 7.4935e-06, 3.5619e-04, 3.8120e-04, 2.9427e-06,\n",
      "          5.7779e-03, 2.7129e-06, 3.6799e-03, 2.0730e-01, 7.7841e-01,\n",
      "          3.3613e-06, 3.9661e-06, 3.2386e-06, 3.4250e-06, 3.6213e-06,\n",
      "          3.8582e-06, 3.1139e-06, 5.6002e-06, 2.8731e-06, 7.0999e-06,\n",
      "          4.3110e-06, 2.9723e-06, 3.4702e-06, 3.8352e-06, 6.5363e-06,\n",
      "          7.1858e-06, 3.2920e-06, 5.2866e-06, 6.7771e-06, 4.2818e-06,\n",
      "          2.9149e-06, 3.8383e-06, 5.4065e-06, 4.5723e-06, 3.5288e-06,\n",
      "          3.5768e-06],\n",
      "         [7.9117e-05, 1.3937e-04, 1.7635e-04, 1.0034e-04, 2.2771e-04,\n",
      "          1.6328e-04, 1.3833e-04, 2.5523e-04, 3.6040e-04, 5.2889e-04,\n",
      "          2.4870e-03, 2.8310e-03, 6.2694e-03, 7.3109e-03, 7.9712e-04,\n",
      "          9.0391e-03, 1.4801e-03, 2.4532e-02, 3.0429e-02, 3.0276e-03,\n",
      "          1.0699e-03, 8.3222e-04, 4.3073e-02, 3.8009e-02, 2.9847e-03,\n",
      "          5.3398e-01, 1.8522e-04, 5.4575e-03, 6.1149e-03, 5.0382e-04,\n",
      "          2.9159e-03, 1.0569e-02, 9.6289e-02, 9.3050e-02, 8.5524e-03,\n",
      "          2.3018e-02, 1.5693e-03, 1.1738e-02, 1.5594e-02, 1.9399e-03,\n",
      "          2.9688e-03, 2.0965e-03, 3.5254e-03, 2.3894e-03, 1.4819e-04,\n",
      "          2.5131e-04, 1.5278e-04, 1.5920e-04, 1.2999e-04, 1.5567e-04,\n",
      "          2.0106e-04]]])\n",
      "Player 1 Prediction: tensor([[6.0458e-02, 9.3954e-01, 3.2704e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[2.3948e-07, 4.1673e-07, 4.2712e-07, 2.6757e-07, 3.4127e-07,\n",
      "          1.3387e-07, 3.4435e-07, 5.7993e-06, 7.3326e-06, 2.1914e-06,\n",
      "          2.0918e-04, 6.0383e-05, 9.5190e-06, 7.2327e-06, 7.1537e-06,\n",
      "          3.2183e-03, 1.1454e-05, 3.1559e-07, 3.7095e-07, 1.6969e-05,\n",
      "          1.4306e-05, 2.9172e-06, 2.2600e-06, 4.8177e-06, 1.3851e-06,\n",
      "          2.3981e-03, 3.8576e-07, 1.3214e-06, 1.7360e-06, 7.5499e-06,\n",
      "          4.9095e-05, 2.0848e-05, 1.5027e-05, 2.6614e-05, 2.1640e-04,\n",
      "          9.5213e-01, 3.8353e-06, 1.5880e-04, 3.5370e-04, 9.8474e-05,\n",
      "          4.0081e-02, 1.2533e-05, 5.2482e-04, 3.1419e-04, 3.1460e-07,\n",
      "          5.1366e-07, 2.5818e-07, 4.1676e-07, 2.5272e-07, 2.8131e-07,\n",
      "          2.3610e-07],\n",
      "         [1.7487e-06, 1.7624e-06, 2.7739e-06, 2.1584e-06, 2.1864e-06,\n",
      "          1.5452e-06, 2.3218e-06, 5.5419e-05, 1.6577e-04, 2.1274e-05,\n",
      "          7.0499e-05, 1.5063e-04, 3.2667e-06, 3.0844e-06, 2.8187e-05,\n",
      "          1.6749e-03, 2.0732e-05, 1.0883e-05, 8.2662e-06, 3.8587e-05,\n",
      "          7.6487e-05, 8.1214e-06, 3.7214e-05, 4.4409e-05, 3.6678e-06,\n",
      "          1.9148e-04, 2.7738e-06, 1.4169e-05, 2.3666e-05, 1.9949e-05,\n",
      "          7.8422e-01, 1.7731e-04, 3.0769e-03, 3.4828e-03, 3.2185e-05,\n",
      "          4.0485e-02, 2.9772e-05, 2.6102e-04, 7.8878e-04, 1.5519e-02,\n",
      "          1.4735e-01, 2.3467e-04, 8.4508e-04, 7.9820e-04, 3.1924e-06,\n",
      "          2.3610e-06, 2.5898e-06, 2.5360e-06, 2.3335e-06, 2.3944e-06,\n",
      "          1.6129e-06],\n",
      "         [3.1237e-07, 6.4929e-07, 1.8947e-07, 5.4816e-07, 3.9957e-07,\n",
      "          6.6162e-07, 3.4067e-07, 5.2099e-07, 2.4560e-07, 3.3400e-07,\n",
      "          3.3528e-07, 2.0335e-07, 1.6391e-04, 1.2900e-04, 2.5113e-07,\n",
      "          5.2817e-04, 3.8804e-07, 1.0444e-05, 7.8837e-06, 2.1752e-07,\n",
      "          9.9909e-01, 2.3646e-07, 7.6570e-06, 4.5107e-05, 1.5107e-06,\n",
      "          3.2116e-07, 1.6782e-07, 2.8147e-07, 3.8727e-07, 1.9638e-07,\n",
      "          1.6212e-07, 3.9480e-07, 1.7559e-07, 4.8297e-07, 2.8975e-07,\n",
      "          1.8860e-07, 1.4546e-07, 2.6727e-07, 2.9257e-07, 1.6215e-07,\n",
      "          1.0132e-06, 3.6918e-07, 2.9639e-07, 3.6232e-07, 3.5982e-07,\n",
      "          2.3704e-07, 3.6520e-07, 3.7025e-07, 2.4615e-07, 3.5440e-07,\n",
      "          2.1585e-07],\n",
      "         [1.2006e-05, 2.2574e-05, 3.6499e-05, 2.7957e-05, 3.1845e-05,\n",
      "          2.1177e-05, 2.8398e-05, 5.3390e-05, 7.2738e-05, 4.5177e-05,\n",
      "          8.0308e-03, 2.3733e-03, 1.4326e-04, 1.5524e-04, 4.9838e-05,\n",
      "          1.3225e-02, 4.0024e-04, 2.5828e-05, 5.0692e-05, 7.2675e-05,\n",
      "          1.6104e-03, 4.8758e-04, 7.1612e-05, 6.3548e-05, 7.9410e-05,\n",
      "          4.9769e-03, 3.1787e-05, 1.0331e-04, 8.9792e-05, 1.4013e-04,\n",
      "          1.6654e-01, 1.4895e-04, 3.1995e-04, 3.3462e-04, 4.0642e-02,\n",
      "          4.8087e-01, 7.4054e-05, 1.8199e-04, 3.7820e-04, 3.0138e-02,\n",
      "          2.4697e-01, 1.6754e-04, 2.6796e-04, 2.4138e-04, 2.7817e-05,\n",
      "          2.9555e-05, 2.7673e-05, 1.6754e-05, 4.1484e-05, 1.5788e-05,\n",
      "          3.7583e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8299e-01, 3.7120e-05, 1.6977e-02]])\n",
      "Player 0 Prediction: tensor([[[9.9288e-09, 1.1676e-08, 1.3082e-08, 8.9816e-09, 9.5619e-09,\n",
      "          3.7650e-09, 1.4329e-08, 2.3881e-07, 2.4076e-07, 1.0211e-07,\n",
      "          7.9572e-07, 1.2681e-06, 1.2410e-06, 1.1962e-06, 3.7381e-07,\n",
      "          4.3223e-03, 1.4871e-07, 2.2982e-08, 2.3427e-08, 1.2275e-06,\n",
      "          9.6071e-08, 6.0157e-08, 6.5226e-08, 1.6692e-07, 3.2418e-08,\n",
      "          4.0451e-04, 1.4901e-08, 8.8406e-08, 8.2483e-08, 1.4235e-07,\n",
      "          3.5166e-07, 1.9196e-06, 1.9889e-06, 2.1663e-06, 2.7365e-06,\n",
      "          9.9496e-01, 4.7208e-07, 3.3372e-05, 5.9984e-05, 1.1599e-06,\n",
      "          1.2704e-04, 9.7000e-07, 4.5405e-05, 3.2434e-05, 1.3926e-08,\n",
      "          1.8052e-08, 7.6687e-09, 1.8138e-08, 5.4641e-09, 8.1903e-09,\n",
      "          8.8566e-09],\n",
      "         [1.4613e-08, 1.5792e-08, 2.9172e-08, 1.8989e-08, 3.1228e-08,\n",
      "          1.3867e-08, 1.5025e-08, 6.4391e-07, 2.1079e-06, 1.4804e-07,\n",
      "          2.5659e-03, 3.3087e-06, 7.3838e-09, 4.6409e-09, 7.2987e-08,\n",
      "          8.8786e-06, 3.9913e-07, 7.0312e-09, 5.2689e-09, 1.6408e-07,\n",
      "          1.2650e-06, 1.2032e-07, 2.8439e-07, 3.0660e-07, 3.9454e-08,\n",
      "          1.4156e-04, 2.7180e-08, 1.9513e-07, 3.7816e-07, 9.0304e-07,\n",
      "          7.4352e-04, 4.2287e-07, 1.0450e-06, 1.9157e-06, 2.8178e-06,\n",
      "          5.2294e-01, 4.6103e-08, 2.5438e-06, 7.4758e-06, 2.5955e-04,\n",
      "          4.7331e-01, 2.5131e-07, 8.1199e-07, 9.8887e-07, 3.9267e-08,\n",
      "          2.0244e-08, 2.6917e-08, 2.2780e-08, 2.6922e-08, 2.5607e-08,\n",
      "          1.6581e-08],\n",
      "         [2.4636e-08, 5.2041e-08, 1.1525e-08, 5.3922e-08, 5.0474e-08,\n",
      "          6.8866e-08, 4.0295e-08, 4.6876e-08, 2.9795e-08, 5.1607e-08,\n",
      "          3.6567e-08, 2.7001e-08, 2.6170e-05, 2.2676e-05, 2.4838e-08,\n",
      "          2.7651e-05, 3.4713e-08, 1.7604e-06, 1.2134e-06, 2.1067e-08,\n",
      "          9.9992e-01, 2.9480e-08, 2.4610e-07, 7.9163e-07, 1.8486e-07,\n",
      "          3.5600e-08, 2.1513e-08, 2.5422e-08, 4.3175e-08, 2.0925e-08,\n",
      "          1.9788e-08, 2.7902e-08, 2.0871e-08, 3.9712e-08, 1.8793e-08,\n",
      "          2.7209e-08, 1.3819e-08, 2.4452e-08, 5.2459e-08, 1.3959e-08,\n",
      "          6.1796e-08, 4.1362e-08, 3.3545e-08, 3.5757e-08, 4.5849e-08,\n",
      "          3.2243e-08, 3.9215e-08, 4.0439e-08, 2.9720e-08, 3.7400e-08,\n",
      "          2.2045e-08],\n",
      "         [1.9164e-05, 3.6909e-05, 4.1792e-05, 4.6572e-05, 6.4010e-05,\n",
      "          2.8402e-05, 3.7880e-05, 7.0197e-05, 1.1850e-04, 6.0956e-05,\n",
      "          1.2844e-02, 3.7495e-03, 2.5082e-04, 1.8439e-04, 1.0649e-04,\n",
      "          1.2383e-02, 8.1106e-04, 8.1706e-05, 1.3830e-04, 1.0309e-04,\n",
      "          1.1402e-03, 5.5664e-04, 8.8084e-05, 1.0236e-04, 1.3263e-04,\n",
      "          1.7740e-02, 3.1983e-05, 9.8356e-05, 9.1568e-05, 2.1309e-04,\n",
      "          8.8960e-02, 2.6065e-04, 8.9957e-04, 7.4478e-04, 3.2903e-02,\n",
      "          5.1077e-01, 9.0250e-05, 2.7662e-04, 6.8203e-04, 3.2466e-02,\n",
      "          2.7861e-01, 3.6524e-04, 8.3077e-04, 5.0426e-04, 3.0379e-05,\n",
      "          5.6849e-05, 2.8027e-05, 2.1102e-05, 4.8915e-05, 2.6149e-05,\n",
      "          5.7627e-05]]])\n",
      "Player 1 Prediction: tensor([[0.9918, 0.0000, 0.0082, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56877\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5829/10000 (58.3%)\n",
      "    Average reward: +0.301\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4171/10000 (41.7%)\n",
      "    Average reward: -0.301\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8322 (29.3%)\n",
      "    Action 1: 16242 (57.2%)\n",
      "    Action 2: 2385 (8.4%)\n",
      "    Action 3: 1438 (5.1%)\n",
      "  Player 1:\n",
      "    Action 0: 7724 (27.1%)\n",
      "    Action 1: 14086 (49.4%)\n",
      "    Action 2: 2850 (10.0%)\n",
      "    Action 3: 3830 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3006.5, -3006.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.980 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.013 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.996\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3006\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8313e-01, 2.1053e-05, 1.6850e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9582e-01, 1.9716e-05, 4.1610e-03]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51015\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6102/10000 (61.0%)\n",
      "    Average reward: -0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3898/10000 (39.0%)\n",
      "    Average reward: +0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 23058 (84.4%)\n",
      "    Action 1: 4268 (15.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2934 (12.4%)\n",
      "    Action 1: 18001 (76.0%)\n",
      "    Action 2: 778 (3.3%)\n",
      "    Action 3: 1976 (8.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1076.5, 1076.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.625 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.674 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.650\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.1076\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}, {'exploitability': 0.395175}, {'exploitability': 0.4645}, {'exploitability': 0.388975}, {'exploitability': 0.4347}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47003/50000 [47:24<02:17, 21.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 301975/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 307324/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48000/50000 [48:12<01:32, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 308447/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 313949/2000000\n",
      "P1 SL Buffer Size:  308447\n",
      "P1 SL buffer distribution [108791. 161341.  24910.  13405.]\n",
      "P1 actions distribution [0.35270565 0.52307528 0.08075942 0.04345965]\n",
      "P2 SL Buffer Size:  313949\n",
      "P2 SL buffer distribution [ 94472. 155075.  27779.  36623.]\n",
      "P2 actions distribution [0.30091512 0.49394965 0.08848252 0.1166527 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.2981, 0.7004, 0.0015, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[5.0858e-05, 1.1204e-04, 8.2063e-05, 7.5512e-05, 1.2559e-04,\n",
      "          9.5202e-05, 1.0882e-04, 3.2073e-04, 2.8771e-04, 1.8283e-04,\n",
      "          5.8499e-03, 9.6968e-04, 2.7175e-04, 4.3458e-04, 2.3070e-04,\n",
      "          5.1645e-02, 3.9152e-03, 2.1691e-03, 3.7417e-03, 1.2696e-03,\n",
      "          3.5816e-02, 4.5656e-03, 3.1926e-03, 3.9827e-03, 2.1117e-04,\n",
      "          2.2677e-01, 7.1870e-04, 1.9085e-02, 3.8834e-03, 1.5277e-02,\n",
      "          2.2563e-01, 9.1977e-03, 7.6958e-03, 5.4294e-03, 1.4780e-02,\n",
      "          2.0922e-01, 3.5227e-03, 1.5504e-03, 9.7698e-04, 1.7018e-02,\n",
      "          1.0362e-01, 2.4328e-03, 8.1916e-03, 4.6862e-03, 8.5573e-05,\n",
      "          8.5303e-05, 9.2204e-05, 8.7171e-05, 1.0383e-04, 9.1196e-05,\n",
      "          5.6046e-05],\n",
      "         [4.6454e-05, 6.6511e-05, 8.7473e-05, 9.2816e-05, 1.2690e-04,\n",
      "          1.2117e-04, 1.1252e-04, 3.2445e-04, 3.7239e-04, 2.5283e-04,\n",
      "          6.2696e-04, 3.2797e-04, 4.4204e-04, 8.5217e-04, 3.1225e-04,\n",
      "          3.5057e-04, 2.3250e-04, 1.4606e-02, 2.2068e-02, 3.5336e-03,\n",
      "          6.2110e-04, 4.8535e-04, 3.0403e-03, 3.4927e-03, 2.1222e-04,\n",
      "          1.5001e-01, 1.4728e-03, 3.0017e-02, 1.2581e-03, 2.4384e-03,\n",
      "          1.3277e-02, 5.8103e-02, 2.1672e-01, 2.0234e-01, 2.6447e-03,\n",
      "          1.3685e-02, 3.8479e-02, 5.2066e-02, 2.0187e-02, 2.1713e-03,\n",
      "          7.3462e-03, 2.4985e-02, 6.5837e-02, 4.3635e-02, 7.7870e-05,\n",
      "          7.2077e-05, 9.5213e-05, 8.2462e-05, 7.8180e-05, 7.4683e-05,\n",
      "          2.9451e-05],\n",
      "         [2.5842e-06, 1.8178e-06, 3.2290e-06, 4.3579e-06, 2.8630e-06,\n",
      "          7.1369e-06, 3.0126e-06, 3.0554e-06, 2.8666e-06, 2.8002e-06,\n",
      "          3.5709e-06, 2.1046e-06, 3.7787e-04, 4.3054e-04, 1.7243e-06,\n",
      "          2.2496e-03, 2.4726e-06, 1.3788e-04, 1.3192e-04, 2.8796e-06,\n",
      "          3.7333e-04, 1.9527e-06, 5.9436e-01, 4.0177e-01, 3.4742e-05,\n",
      "          6.9000e-06, 3.4576e-06, 4.0830e-06, 2.0174e-06, 3.1281e-06,\n",
      "          2.8283e-06, 3.5366e-06, 3.6968e-06, 1.5689e-06, 2.7163e-06,\n",
      "          1.3330e-06, 3.0654e-06, 2.2836e-06, 4.0057e-06, 5.0778e-06,\n",
      "          3.0149e-06, 3.1867e-06, 4.6785e-06, 3.5538e-06, 3.2242e-06,\n",
      "          3.7185e-06, 2.3874e-06, 2.1842e-06, 2.5633e-06, 3.1461e-06,\n",
      "          2.7325e-06],\n",
      "         [3.5910e-04, 3.5324e-04, 4.4132e-04, 3.3372e-04, 2.3799e-04,\n",
      "          2.9995e-04, 2.2760e-04, 4.3790e-04, 8.1972e-04, 6.0384e-04,\n",
      "          4.9108e-03, 1.1766e-03, 1.3801e-03, 2.6794e-03, 6.7787e-04,\n",
      "          3.1610e-03, 5.5246e-04, 9.6009e-03, 5.9498e-03, 2.6809e-03,\n",
      "          5.0639e-03, 3.6429e-03, 2.2658e-02, 2.8848e-02, 6.5601e-04,\n",
      "          2.1974e-01, 1.6764e-03, 1.0725e-01, 4.2430e-02, 7.2668e-03,\n",
      "          1.1233e-02, 2.5975e-02, 1.5442e-01, 5.6926e-02, 4.4202e-03,\n",
      "          9.4823e-02, 1.9797e-02, 1.6751e-02, 1.4825e-02, 1.7072e-02,\n",
      "          5.2692e-02, 9.9795e-03, 2.6531e-02, 1.6516e-02, 4.0367e-04,\n",
      "          2.2455e-04, 3.1975e-04, 2.8402e-04, 1.9665e-04, 2.7914e-04,\n",
      "          2.1508e-04]]])\n",
      "Player 0 Prediction: tensor([[0.9977, 0.0000, 0.0023, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[6.0809e-06, 8.7614e-06, 5.0088e-06, 7.3181e-06, 9.9492e-06,\n",
      "          9.3866e-06, 9.9113e-06, 3.8458e-04, 3.0983e-04, 2.0528e-05,\n",
      "          3.4043e-04, 2.1692e-05, 1.5088e-02, 2.5173e-02, 2.3707e-05,\n",
      "          1.8595e-06, 2.7634e-05, 3.9735e-02, 5.6341e-02, 3.5528e-04,\n",
      "          4.0804e-05, 4.0529e-05, 2.2254e-04, 3.5684e-04, 2.0151e-05,\n",
      "          5.2056e-01, 1.5708e-04, 8.5543e-04, 8.4567e-05, 1.6895e-04,\n",
      "          2.0966e-04, 2.4999e-03, 3.5008e-02, 5.6465e-02, 1.0489e-04,\n",
      "          2.2194e-05, 3.1454e-03, 1.4169e-01, 9.5617e-02, 6.0397e-05,\n",
      "          2.6559e-04, 1.5184e-03, 1.4234e-03, 1.5330e-03, 7.9510e-06,\n",
      "          7.0106e-06, 1.2093e-05, 4.7962e-06, 4.8830e-06, 8.3214e-06,\n",
      "          4.1934e-06],\n",
      "         [9.9663e-05, 1.7893e-04, 1.2237e-04, 1.1012e-04, 1.8828e-04,\n",
      "          1.7233e-04, 1.6517e-04, 2.9313e-04, 3.2174e-04, 1.9086e-04,\n",
      "          1.3957e-03, 6.8826e-04, 3.8342e-02, 4.3079e-02, 1.1180e-04,\n",
      "          3.8971e-04, 2.1908e-04, 4.8059e-04, 6.0165e-04, 6.7014e-04,\n",
      "          5.6973e-04, 4.3484e-04, 5.2840e-03, 4.5548e-03, 2.4676e-04,\n",
      "          2.0461e-01, 1.5008e-03, 4.7578e-02, 1.0073e-02, 6.7739e-03,\n",
      "          1.9474e-02, 3.3341e-02, 2.5362e-01, 2.2486e-01, 2.1225e-03,\n",
      "          1.1465e-02, 5.8386e-03, 3.2720e-02, 3.3729e-02, 1.5932e-03,\n",
      "          1.9667e-03, 5.9902e-03, 1.7816e-03, 1.3923e-03, 9.3422e-05,\n",
      "          6.5445e-05, 1.0653e-04, 9.8813e-05, 9.5207e-05, 1.3323e-04,\n",
      "          6.6410e-05],\n",
      "         [8.6591e-06, 5.9922e-06, 1.1436e-05, 1.6709e-05, 6.0288e-06,\n",
      "          1.1165e-05, 7.2804e-06, 9.9256e-06, 8.6742e-06, 7.2063e-06,\n",
      "          8.3568e-06, 5.4208e-06, 9.3855e-04, 1.2229e-03, 1.0835e-05,\n",
      "          1.9655e-03, 8.6230e-06, 5.0725e-01, 4.8594e-01, 8.7236e-06,\n",
      "          7.7631e-04, 1.0725e-05, 2.7381e-04, 9.7690e-04, 2.3718e-04,\n",
      "          7.2732e-06, 1.3997e-05, 5.0801e-06, 4.6982e-06, 1.5086e-05,\n",
      "          1.5430e-05, 1.3242e-05, 1.4001e-05, 8.9342e-06, 1.6058e-05,\n",
      "          1.4679e-05, 7.3809e-06, 7.3361e-06, 1.3978e-05, 1.4750e-05,\n",
      "          7.6162e-06, 9.3897e-06, 5.3296e-06, 1.0681e-05, 9.7493e-06,\n",
      "          1.7292e-05, 7.3099e-06, 1.2599e-05, 8.5333e-06, 8.8426e-06,\n",
      "          7.8043e-06],\n",
      "         [3.2697e-05, 5.1966e-05, 7.6692e-05, 6.2964e-05, 7.0996e-05,\n",
      "          6.4804e-05, 8.2251e-05, 2.6656e-04, 2.5136e-04, 1.5516e-04,\n",
      "          5.6476e-04, 1.5949e-04, 1.2196e-04, 4.9909e-04, 1.6120e-04,\n",
      "          1.4710e-04, 1.0471e-04, 3.3204e-03, 3.8500e-03, 9.6326e-04,\n",
      "          3.8687e-05, 1.7797e-04, 1.0932e-03, 1.0041e-03, 1.0607e-04,\n",
      "          7.3845e-02, 7.2703e-04, 1.8800e-02, 9.9026e-04, 1.9890e-03,\n",
      "          1.4328e-02, 6.4105e-02, 3.7002e-01, 3.3679e-01, 1.5429e-03,\n",
      "          1.1446e-02, 2.4518e-02, 3.2716e-02, 9.7218e-03, 9.3391e-04,\n",
      "          2.9699e-03, 1.0878e-02, 5.9408e-03, 4.0098e-03, 5.3916e-05,\n",
      "          5.1902e-05, 3.9093e-05, 4.1623e-05, 2.8964e-05, 5.4434e-05,\n",
      "          3.1086e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7521, 0.0090, 0.2388]])\n",
      "Player 1 Prediction: tensor([[[1.4340e-07, 2.8506e-07, 1.7179e-07, 1.9368e-07, 3.7191e-07,\n",
      "          2.3002e-07, 3.8639e-07, 3.1365e-05, 1.9598e-05, 1.1884e-06,\n",
      "          2.6058e-05, 9.4856e-07, 2.0922e-01, 2.0910e-01, 5.0382e-07,\n",
      "          7.9175e-06, 1.5411e-06, 1.3046e-05, 1.7689e-05, 2.3636e-06,\n",
      "          1.6261e-06, 1.6691e-06, 3.8171e-06, 5.3259e-06, 6.2350e-07,\n",
      "          2.1841e-01, 3.9166e-06, 8.2710e-06, 1.9785e-06, 5.0714e-06,\n",
      "          5.8642e-06, 1.9030e-05, 8.8610e-06, 1.0721e-05, 8.1981e-06,\n",
      "          1.4085e-05, 4.0854e-05, 1.8182e-01, 1.8109e-01, 4.0042e-06,\n",
      "          3.0409e-05, 2.0783e-05, 1.7369e-05, 1.8192e-05, 3.0510e-07,\n",
      "          2.7935e-07, 5.7322e-07, 1.5652e-07, 1.7775e-07, 2.4406e-07,\n",
      "          9.5434e-08],\n",
      "         [1.4926e-06, 2.1239e-06, 1.2720e-06, 1.3544e-06, 1.7313e-06,\n",
      "          2.4345e-06, 2.4086e-06, 1.7521e-01, 1.7581e-01, 3.5490e-06,\n",
      "          2.0156e-05, 4.1140e-06, 1.1053e-06, 1.0238e-06, 1.1232e-06,\n",
      "          8.0825e-07, 1.0908e-06, 1.0920e-05, 1.1434e-05, 1.6334e-05,\n",
      "          3.0000e-06, 5.3169e-06, 3.8644e-05, 3.2654e-05, 2.5158e-06,\n",
      "          2.4146e-01, 2.5021e-05, 2.5699e-04, 6.2373e-05, 3.3107e-05,\n",
      "          2.5398e-05, 2.5091e-04, 1.8145e-04, 1.6795e-04, 1.6172e-05,\n",
      "          3.3546e-05, 1.3769e-04, 1.6985e-01, 1.6578e-01, 5.1017e-06,\n",
      "          5.5027e-06, 5.5489e-05, 3.5000e-02, 3.5450e-02, 9.9976e-07,\n",
      "          8.1800e-07, 9.0942e-07, 1.5257e-06, 8.8144e-07, 1.8349e-06,\n",
      "          6.0618e-07],\n",
      "         [2.6718e-07, 1.6255e-07, 5.1138e-07, 4.9180e-07, 2.9309e-07,\n",
      "          4.3133e-07, 2.7720e-07, 6.0379e-07, 3.9305e-07, 4.1072e-07,\n",
      "          6.7951e-07, 1.5323e-07, 5.3427e-05, 6.0965e-05, 3.5247e-07,\n",
      "          9.2403e-05, 3.6115e-07, 5.0019e-01, 4.9955e-01, 4.3634e-07,\n",
      "          1.4701e-06, 3.8036e-07, 1.3294e-05, 1.5598e-05, 3.7144e-06,\n",
      "          3.6766e-07, 5.2752e-07, 1.7994e-07, 2.4604e-07, 8.0333e-07,\n",
      "          6.7238e-07, 4.9698e-07, 6.9807e-07, 5.0648e-07, 6.1437e-07,\n",
      "          1.0757e-06, 3.1671e-07, 4.2066e-07, 8.8270e-07, 8.0996e-07,\n",
      "          2.6746e-07, 5.6950e-07, 3.0393e-07, 3.7490e-07, 6.6701e-07,\n",
      "          3.9439e-07, 2.7214e-07, 4.6666e-07, 3.1426e-07, 5.8479e-07,\n",
      "          4.0615e-07],\n",
      "         [2.5539e-05, 5.2287e-05, 8.0170e-05, 6.3589e-05, 1.1178e-04,\n",
      "          6.2620e-05, 7.2907e-05, 2.8849e-04, 2.4302e-04, 7.2812e-05,\n",
      "          8.6878e-04, 1.2163e-04, 1.4932e-04, 7.9359e-04, 1.4131e-04,\n",
      "          1.7221e-04, 1.1094e-04, 2.0033e-03, 2.5584e-03, 5.6500e-04,\n",
      "          3.5386e-05, 1.4033e-04, 1.2560e-03, 1.4400e-03, 1.0147e-04,\n",
      "          7.2201e-02, 4.8561e-04, 2.1873e-02, 1.5027e-03, 1.7800e-03,\n",
      "          1.5092e-02, 5.8427e-02, 3.5266e-01, 3.4752e-01, 1.2512e-03,\n",
      "          1.0054e-02, 2.5668e-02, 4.0585e-02, 1.1004e-02, 9.3408e-04,\n",
      "          3.1946e-03, 1.4027e-02, 6.1685e-03, 3.7883e-03, 3.4805e-05,\n",
      "          5.3160e-05, 3.1992e-05, 3.3469e-05, 2.3509e-05, 5.9676e-05,\n",
      "          2.2335e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48000/50000 [48:28<01:32, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 61846\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5359/10000 (53.6%)\n",
      "    Average reward: -0.673\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4641/10000 (46.4%)\n",
      "    Average reward: +0.673\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10548 (33.9%)\n",
      "    Action 1: 16850 (54.2%)\n",
      "    Action 2: 2016 (6.5%)\n",
      "    Action 3: 1681 (5.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7906 (25.7%)\n",
      "    Action 1: 13691 (44.5%)\n",
      "    Action 2: 3233 (10.5%)\n",
      "    Action 3: 5921 (19.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6727.5, 6727.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.008 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.016\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6727\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9071e-01, 1.1022e-05, 9.2781e-03]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.8140e-01, 2.0546e-04, 2.1839e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51802\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5514/10000 (55.1%)\n",
      "    Average reward: +0.280\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4486/10000 (44.9%)\n",
      "    Average reward: -0.280\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2751 (11.1%)\n",
      "    Action 1: 16570 (67.1%)\n",
      "    Action 2: 1843 (7.5%)\n",
      "    Action 3: 3531 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 21511 (79.4%)\n",
      "    Action 1: 5596 (20.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2799.5, -2799.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.739 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.735 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.737\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2799\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[6.8074e-01, 3.1920e-01, 5.7886e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[4.7516e-06, 5.2713e-06, 8.0297e-06, 8.4552e-06, 1.1192e-05,\n",
      "          4.9325e-06, 6.7394e-06, 2.0065e-05, 4.1620e-05, 7.8591e-06,\n",
      "          7.1381e-04, 5.2272e-04, 9.1961e-05, 4.8405e-05, 6.8116e-05,\n",
      "          3.3111e-03, 9.2392e-04, 3.0256e-03, 5.2443e-03, 3.6678e-04,\n",
      "          1.6512e-02, 1.2263e-03, 1.3713e-04, 1.9379e-04, 1.9620e-05,\n",
      "          7.1228e-01, 6.8725e-06, 1.6407e-05, 1.7925e-05, 1.6948e-03,\n",
      "          3.8787e-02, 4.3726e-05, 2.3079e-04, 3.1691e-04, 1.2367e-02,\n",
      "          9.2692e-02, 7.5583e-06, 1.7551e-05, 9.9612e-05, 1.3783e-02,\n",
      "          9.4902e-02, 4.5014e-05, 6.8877e-05, 4.3391e-05, 9.6008e-06,\n",
      "          7.6977e-06, 7.5207e-06, 6.2127e-06, 9.8879e-06, 5.9933e-06,\n",
      "          4.3546e-06],\n",
      "         [1.6422e-05, 3.6427e-05, 2.8997e-05, 2.7504e-05, 3.1786e-05,\n",
      "          2.5141e-05, 2.0210e-05, 1.4083e-04, 2.1134e-04, 1.9223e-04,\n",
      "          1.4008e-03, 1.2069e-03, 1.4844e-01, 1.7747e-01, 2.0085e-02,\n",
      "          6.1345e-04, 5.0719e-04, 1.0384e-01, 8.6975e-02, 5.6595e-03,\n",
      "          1.3931e-04, 7.8212e-05, 6.6140e-04, 6.5035e-04, 6.3146e-05,\n",
      "          1.3196e-01, 3.8045e-05, 2.7844e-04, 2.7860e-04, 1.0400e-03,\n",
      "          4.2487e-03, 1.1427e-02, 1.1417e-01, 1.1035e-01, 4.4515e-04,\n",
      "          3.0062e-04, 2.2443e-03, 1.0510e-02, 9.9401e-03, 2.1384e-04,\n",
      "          3.9407e-04, 1.0646e-02, 2.5211e-02, 1.7576e-02, 2.6303e-05,\n",
      "          3.3582e-05, 3.8329e-05, 2.6785e-05, 3.0417e-05, 2.1288e-05,\n",
      "          3.3081e-05],\n",
      "         [3.5676e-06, 3.7235e-06, 3.7265e-06, 6.7114e-06, 3.5643e-06,\n",
      "          4.1329e-06, 4.5416e-06, 2.8095e-06, 3.3834e-06, 1.9962e-06,\n",
      "          2.0854e-06, 3.8703e-06, 2.2016e-04, 2.4345e-04, 1.6335e-06,\n",
      "          1.0377e-03, 4.8465e-06, 9.6896e-05, 1.4633e-04, 3.0320e-06,\n",
      "          1.8803e-04, 2.3320e-06, 5.0297e-01, 4.9488e-01, 6.6639e-05,\n",
      "          4.2385e-06, 2.9785e-06, 4.8076e-06, 3.0580e-06, 3.1271e-06,\n",
      "          4.8085e-06, 5.6774e-06, 3.9181e-06, 1.5202e-06, 2.9308e-06,\n",
      "          3.2278e-06, 3.8157e-06, 3.9049e-06, 3.8503e-06, 3.0627e-06,\n",
      "          4.8176e-06, 2.5352e-06, 2.3659e-06, 5.0004e-06, 2.4187e-06,\n",
      "          2.4832e-06, 3.3496e-06, 3.6733e-06, 4.0871e-06, 4.2011e-06,\n",
      "          3.1347e-06],\n",
      "         [2.1743e-04, 2.6125e-04, 3.2008e-04, 3.6995e-04, 3.4424e-04,\n",
      "          4.4421e-04, 3.8003e-04, 9.9433e-04, 9.6667e-04, 7.2724e-04,\n",
      "          2.0009e-03, 2.0091e-03, 7.1139e-03, 7.1779e-03, 1.4126e-03,\n",
      "          6.5887e-03, 1.7989e-03, 3.6103e-02, 3.9685e-02, 5.6571e-03,\n",
      "          8.8574e-03, 1.5219e-03, 1.2721e-01, 1.5463e-01, 4.3081e-03,\n",
      "          3.6607e-01, 4.4827e-04, 1.0096e-02, 7.2320e-03, 1.9587e-03,\n",
      "          3.0993e-02, 5.2281e-03, 2.1512e-02, 2.1775e-02, 9.7657e-03,\n",
      "          1.6171e-02, 3.9325e-03, 2.6446e-02, 1.7350e-02, 4.6016e-03,\n",
      "          7.8865e-03, 8.9313e-03, 1.1468e-02, 1.4822e-02, 3.8713e-04,\n",
      "          2.5863e-04, 4.0473e-04, 3.0563e-04, 3.4104e-04, 2.9188e-04,\n",
      "          2.1831e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8690e-01, 4.2295e-05, 1.3059e-02]])\n",
      "Player 0 Prediction: tensor([[[4.1392e-08, 3.5045e-08, 5.2242e-08, 8.1160e-08, 4.7275e-08,\n",
      "          1.9653e-08, 6.0264e-08, 1.1975e-07, 1.1421e-07, 2.0004e-07,\n",
      "          3.6127e-06, 2.7758e-06, 1.7968e-06, 1.6812e-06, 1.9656e-06,\n",
      "          2.2212e-03, 7.3463e-07, 1.3353e-07, 2.0296e-07, 9.1669e-06,\n",
      "          3.7812e-06, 7.6641e-07, 5.9297e-07, 1.1262e-06, 1.2350e-07,\n",
      "          1.1304e-03, 7.4813e-08, 4.7997e-07, 3.5299e-07, 1.8606e-06,\n",
      "          3.9428e-06, 2.1912e-06, 9.4799e-06, 9.8367e-06, 7.7275e-06,\n",
      "          9.9590e-01, 9.8779e-07, 7.7688e-05, 1.6597e-04, 6.2380e-06,\n",
      "          2.0693e-04, 3.0005e-06, 1.3699e-04, 8.4012e-05, 8.1994e-08,\n",
      "          9.1390e-08, 4.0201e-08, 7.3453e-08, 3.5083e-08, 4.7680e-08,\n",
      "          2.7233e-08],\n",
      "         [4.6701e-08, 7.1948e-08, 9.6009e-08, 5.8619e-08, 8.8357e-08,\n",
      "          6.2137e-08, 3.9441e-08, 4.4056e-07, 1.1928e-06, 5.5371e-07,\n",
      "          1.6036e-03, 5.3184e-06, 4.5264e-08, 2.5586e-08, 3.2384e-07,\n",
      "          3.4563e-05, 2.4504e-06, 2.1915e-07, 1.6550e-07, 9.9155e-07,\n",
      "          4.9850e-06, 3.8812e-07, 2.7652e-06, 2.1754e-06, 1.8458e-07,\n",
      "          3.8121e-05, 8.8042e-08, 7.4753e-07, 9.2117e-07, 8.2650e-06,\n",
      "          4.1292e-04, 9.0137e-07, 1.8404e-06, 3.7239e-06, 1.2020e-05,\n",
      "          4.7862e-01, 4.7784e-08, 1.6349e-06, 6.7476e-06, 1.6347e-04,\n",
      "          5.1906e-01, 6.0145e-07, 1.0220e-06, 1.2426e-06, 1.0423e-07,\n",
      "          9.1972e-08, 1.0852e-07, 7.7605e-08, 1.4032e-07, 5.3997e-08,\n",
      "          1.0001e-07],\n",
      "         [1.8173e-07, 6.1083e-07, 1.6133e-07, 3.8474e-07, 6.1364e-07,\n",
      "          4.2198e-07, 5.9581e-07, 2.6803e-07, 2.5728e-07, 3.7758e-07,\n",
      "          2.4117e-07, 4.4901e-07, 9.4953e-05, 6.8286e-05, 2.3501e-07,\n",
      "          1.5712e-04, 3.6630e-07, 9.6354e-06, 7.8336e-06, 1.8645e-07,\n",
      "          9.9960e-01, 2.4659e-07, 9.7725e-06, 2.5300e-05, 1.0720e-05,\n",
      "          2.5908e-07, 2.0955e-07, 3.1310e-07, 3.4643e-07, 3.2721e-07,\n",
      "          2.2800e-07, 5.3452e-07, 2.6064e-07, 3.0369e-07, 2.7455e-07,\n",
      "          3.2153e-07, 2.4731e-07, 2.9243e-07, 6.0435e-07, 2.2421e-07,\n",
      "          3.9051e-07, 2.4210e-07, 2.2210e-07, 3.4866e-07, 3.5339e-07,\n",
      "          2.4027e-07, 3.9405e-07, 4.1750e-07, 3.9742e-07, 3.1473e-07,\n",
      "          2.3007e-07],\n",
      "         [1.0626e-04, 1.9481e-04, 1.6527e-04, 1.7623e-04, 2.2821e-04,\n",
      "          1.5090e-04, 1.4099e-04, 2.0888e-04, 2.9103e-04, 3.0002e-04,\n",
      "          8.7922e-03, 4.8049e-03, 9.6575e-04, 8.1123e-04, 5.8774e-04,\n",
      "          9.4625e-03, 2.4723e-03, 3.2433e-04, 4.6119e-04, 6.8218e-04,\n",
      "          1.1647e-02, 1.5624e-03, 1.1387e-03, 1.5465e-03, 1.2186e-03,\n",
      "          8.5676e-02, 1.2033e-04, 1.3242e-03, 1.2119e-03, 1.0258e-03,\n",
      "          4.0168e-01, 7.0991e-04, 3.6481e-03, 2.5593e-03, 2.1691e-02,\n",
      "          1.6604e-01, 3.8035e-04, 2.1759e-03, 3.8082e-03, 3.3494e-02,\n",
      "          2.1345e-01, 2.8404e-03, 5.5966e-03, 3.0969e-03, 1.2694e-04,\n",
      "          1.8879e-04, 9.2759e-05, 1.0887e-04, 1.5401e-04, 1.7670e-04,\n",
      "          1.8968e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9729, 0.0000, 0.0271, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57679\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5813/10000 (58.1%)\n",
      "    Average reward: +0.164\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4187/10000 (41.9%)\n",
      "    Average reward: -0.164\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8476 (29.4%)\n",
      "    Action 1: 16666 (57.9%)\n",
      "    Action 2: 2717 (9.4%)\n",
      "    Action 3: 932 (3.2%)\n",
      "  Player 1:\n",
      "    Action 0: 8014 (27.7%)\n",
      "    Action 1: 14263 (49.4%)\n",
      "    Action 2: 2718 (9.4%)\n",
      "    Action 3: 3893 (13.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1643.0, -1643.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.976 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.996\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1643\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3904, 0.6088, 0.0008, 0.0000]])\n",
      "Player 1 Prediction: tensor([[1.0633e-02, 9.8935e-01, 1.8225e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3463, 0.0007, 0.6530]])\n",
      "Player 1 Prediction: tensor([[0.1452, 0.8001, 0.0547, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51001\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6107/10000 (61.1%)\n",
      "    Average reward: -0.060\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3893/10000 (38.9%)\n",
      "    Average reward: +0.060\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 23033 (84.4%)\n",
      "    Action 1: 4266 (15.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2945 (12.4%)\n",
      "    Action 1: 18030 (76.1%)\n",
      "    Action 2: 739 (3.1%)\n",
      "    Action 3: 1988 (8.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-603.0, 603.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.625 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.674 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.650\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0603\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}, {'exploitability': 0.395175}, {'exploitability': 0.4645}, {'exploitability': 0.388975}, {'exploitability': 0.4347}, {'exploitability': 0.418525}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49002/50000 [49:40<00:45, 21.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 314560/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 320903/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [50:26<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[4.8900e-05, 7.9500e-05, 1.0374e-04, 1.2972e-04, 1.8575e-04,\n",
      "          2.4052e-04, 1.5042e-04, 2.1962e-04, 2.1355e-04, 2.1347e-04,\n",
      "          2.0205e-03, 6.3517e-04, 2.0658e-04, 4.4728e-04, 2.1706e-04,\n",
      "          5.2034e-03, 2.2492e-03, 2.8993e-02, 3.1035e-02, 1.5512e-02,\n",
      "          5.2504e-02, 6.1697e-03, 1.0126e-02, 1.3785e-02, 4.2037e-04,\n",
      "          3.9420e-01, 2.0761e-03, 3.0631e-02, 1.1344e-02, 1.3774e-02,\n",
      "          5.6555e-02, 1.3562e-02, 7.9703e-02, 3.2109e-02, 1.0790e-02,\n",
      "          3.8147e-02, 7.8516e-03, 8.5824e-03, 1.2320e-02, 2.5637e-02,\n",
      "          5.8011e-02, 8.1349e-03, 1.6014e-02, 8.5961e-03, 1.2562e-04,\n",
      "          1.2648e-04, 1.3124e-04, 1.0519e-04, 1.7668e-04, 1.3224e-04,\n",
      "          5.5705e-05],\n",
      "         [2.1157e-05, 2.4737e-05, 3.3954e-05, 2.8599e-05, 4.1048e-05,\n",
      "          5.1704e-05, 3.7411e-05, 3.1250e-05, 3.3452e-05, 3.5020e-05,\n",
      "          1.7656e-03, 6.2518e-04, 8.8159e-03, 1.6633e-02, 2.4259e-03,\n",
      "          4.4037e-02, 5.5247e-03, 1.8789e-03, 3.6727e-03, 1.5578e-03,\n",
      "          1.3253e-02, 1.2593e-03, 6.4182e-04, 7.3971e-04, 1.0768e-04,\n",
      "          9.6957e-02, 2.9137e-04, 2.3643e-03, 1.1769e-03, 3.2058e-02,\n",
      "          5.4561e-01, 6.5289e-04, 5.9312e-04, 4.5349e-04, 1.5455e-02,\n",
      "          1.1034e-01, 5.2087e-04, 5.9428e-04, 1.4372e-03, 2.2134e-02,\n",
      "          6.4977e-02, 2.1754e-04, 4.6385e-04, 2.1372e-04, 3.6122e-05,\n",
      "          3.3861e-05, 3.8887e-05, 2.6334e-05, 3.1279e-05, 3.3273e-05,\n",
      "          1.6299e-05],\n",
      "         [4.6502e-06, 6.2135e-06, 2.6618e-06, 6.2956e-06, 4.7787e-06,\n",
      "          4.6545e-06, 4.7562e-06, 8.2943e-06, 7.1414e-06, 3.8187e-06,\n",
      "          2.5476e-06, 4.0211e-06, 2.2326e-04, 2.5423e-04, 5.1984e-06,\n",
      "          9.9221e-04, 4.4893e-06, 3.3181e-04, 2.8108e-04, 6.3577e-06,\n",
      "          2.2820e-03, 6.5453e-06, 7.0817e-03, 2.2955e-01, 7.5879e-01,\n",
      "          6.2288e-06, 9.0240e-06, 6.8799e-06, 5.4363e-06, 4.6890e-06,\n",
      "          3.8012e-06, 5.0166e-06, 4.1393e-06, 2.4001e-06, 7.4651e-06,\n",
      "          2.6445e-06, 7.0205e-06, 4.6367e-06, 3.4314e-06, 4.7211e-06,\n",
      "          3.4527e-06, 5.2399e-06, 7.1521e-06, 7.0091e-06, 2.4454e-06,\n",
      "          7.7313e-06, 7.0353e-06, 4.3137e-06, 3.1790e-06, 5.0428e-06,\n",
      "          7.6103e-06],\n",
      "         [9.3457e-05, 1.9846e-04, 1.2748e-04, 1.6645e-04, 2.3167e-04,\n",
      "          2.3869e-04, 1.6149e-04, 5.0748e-04, 3.7142e-04, 3.3124e-04,\n",
      "          9.9369e-04, 5.1522e-04, 6.0275e-04, 7.2381e-04, 4.4612e-04,\n",
      "          2.2362e-03, 4.6846e-04, 7.7683e-02, 1.3334e-01, 2.3087e-02,\n",
      "          7.7509e-03, 2.3615e-03, 6.4667e-03, 6.5177e-03, 2.7490e-04,\n",
      "          4.6240e-01, 1.1235e-03, 1.1687e-02, 4.5731e-03, 5.7631e-03,\n",
      "          2.0526e-02, 1.7546e-02, 6.2172e-02, 3.9523e-02, 3.5641e-03,\n",
      "          1.6298e-02, 9.0979e-03, 8.5542e-03, 5.9683e-03, 6.1419e-03,\n",
      "          1.9070e-02, 1.1996e-02, 1.5439e-02, 1.1087e-02, 1.9747e-04,\n",
      "          3.4954e-04, 1.5193e-04, 2.1620e-04, 2.5545e-04, 3.0577e-04,\n",
      "          9.9328e-05]]])\n",
      "Player 0 Prediction: tensor([[0.8753, 0.1232, 0.0015, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.8420e-06, 1.4455e-06, 4.2758e-06, 3.0209e-06, 5.5485e-06,\n",
      "          4.3874e-06, 4.8720e-06, 4.5028e-04, 4.0426e-04, 6.4698e-06,\n",
      "          8.2009e-03, 2.1840e-05, 2.1966e-04, 4.5144e-04, 3.5172e-06,\n",
      "          1.1516e-01, 6.4898e-05, 3.7812e-05, 1.9251e-04, 3.4857e-04,\n",
      "          8.3047e-03, 1.5958e-04, 9.5254e-05, 1.2434e-04, 7.2455e-06,\n",
      "          8.4026e-01, 1.7037e-05, 2.1885e-04, 6.2055e-05, 5.3914e-04,\n",
      "          1.3156e-02, 1.4591e-04, 2.1238e-06, 1.2616e-06, 3.3390e-04,\n",
      "          9.2621e-03, 1.8153e-04, 1.1884e-05, 1.4931e-05, 6.6644e-04,\n",
      "          3.8418e-04, 2.3622e-04, 1.4048e-04, 7.0751e-05, 2.6474e-06,\n",
      "          2.7043e-06, 2.4295e-06, 4.5982e-06, 3.3645e-06, 2.3172e-06,\n",
      "          1.3984e-06],\n",
      "         [2.2763e-05, 2.3415e-05, 3.3072e-05, 3.6996e-05, 6.1395e-05,\n",
      "          7.6284e-05, 4.7464e-05, 3.1125e-04, 2.1229e-04, 2.1582e-05,\n",
      "          1.0855e-03, 3.2802e-04, 2.8475e-04, 6.4697e-04, 3.8274e-04,\n",
      "          8.7502e-02, 5.4212e-04, 8.4516e-03, 1.7125e-02, 6.7571e-03,\n",
      "          4.7423e-02, 2.4070e-03, 8.8495e-04, 1.4508e-03, 1.0190e-04,\n",
      "          3.7989e-01, 3.1141e-04, 1.0513e-02, 2.7153e-03, 2.9837e-03,\n",
      "          4.0613e-01, 3.1575e-03, 1.6959e-03, 1.0836e-03, 1.1525e-03,\n",
      "          5.2706e-03, 1.7075e-03, 4.1103e-04, 3.7862e-04, 3.0915e-03,\n",
      "          2.9746e-03, 5.6495e-05, 2.0229e-06, 1.6087e-06, 3.6874e-05,\n",
      "          2.9946e-05, 5.9317e-05, 2.9556e-05, 4.3156e-05, 3.2127e-05,\n",
      "          1.5669e-05],\n",
      "         [8.6467e-07, 7.4733e-07, 7.1147e-07, 1.0850e-06, 6.2662e-07,\n",
      "          9.0604e-07, 1.2129e-06, 8.2935e-07, 6.8625e-07, 6.3276e-07,\n",
      "          6.8637e-07, 6.1627e-07, 1.2145e-04, 1.1429e-04, 3.8200e-07,\n",
      "          2.0709e-03, 6.4319e-07, 4.0964e-05, 3.3882e-05, 6.6653e-07,\n",
      "          9.9664e-01, 6.7950e-07, 6.3850e-05, 8.4597e-04, 3.4725e-05,\n",
      "          1.5521e-06, 6.2518e-07, 1.0337e-06, 7.2480e-07, 7.0331e-07,\n",
      "          4.7602e-07, 7.4812e-07, 1.0173e-06, 3.8840e-07, 1.6999e-06,\n",
      "          4.4431e-07, 4.7925e-07, 4.6932e-07, 5.9562e-07, 1.1537e-06,\n",
      "          5.8331e-07, 1.2912e-06, 1.7129e-06, 2.0008e-06, 4.4414e-07,\n",
      "          9.9731e-07, 6.9729e-07, 6.9216e-07, 1.2162e-06, 1.1826e-06,\n",
      "          1.3740e-06],\n",
      "         [3.2932e-06, 3.7341e-06, 2.3552e-06, 3.4473e-06, 4.0766e-06,\n",
      "          4.1391e-06, 2.6587e-06, 4.0697e-06, 3.3850e-06, 5.4363e-06,\n",
      "          9.3474e-05, 1.1999e-05, 7.7287e-06, 9.5657e-06, 4.3219e-06,\n",
      "          5.2923e-05, 7.9559e-06, 3.6631e-04, 5.2923e-04, 4.2068e-04,\n",
      "          4.7854e-01, 1.8206e-04, 8.3354e-05, 9.8274e-05, 6.3018e-06,\n",
      "          4.5779e-01, 1.7228e-05, 1.6690e-04, 3.0221e-05, 8.5794e-04,\n",
      "          5.2212e-02, 9.3034e-05, 2.2805e-05, 1.7286e-05, 1.0433e-04,\n",
      "          3.5171e-03, 7.5986e-05, 1.8069e-05, 8.7495e-06, 1.2771e-03,\n",
      "          3.3147e-03, 7.9744e-06, 7.1952e-07, 5.3618e-07, 3.2330e-06,\n",
      "          2.2908e-06, 2.7652e-06, 2.9011e-06, 5.4796e-06, 4.0343e-06,\n",
      "          1.2733e-06]]])\n",
      "Player 0 Prediction: tensor([[0.0243, 0.1090, 0.8667, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56974\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5406/10000 (54.1%)\n",
      "    Average reward: -0.505\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4594/10000 (45.9%)\n",
      "    Average reward: +0.505\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9060 (32.1%)\n",
      "    Action 1: 15678 (55.5%)\n",
      "    Action 2: 1940 (6.9%)\n",
      "    Action 3: 1566 (5.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7784 (27.1%)\n",
      "    Action 1: 13319 (46.4%)\n",
      "    Action 2: 3214 (11.2%)\n",
      "    Action 3: 4413 (15.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5050.0, 5050.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.998 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.011\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5050\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.3943e-01, 1.2236e-04, 6.0451e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2360, 0.0097, 0.7544]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52223\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5452/10000 (54.5%)\n",
      "    Average reward: +0.267\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4548/10000 (45.5%)\n",
      "    Average reward: -0.267\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2670 (10.7%)\n",
      "    Action 1: 16477 (66.0%)\n",
      "    Action 2: 1958 (7.8%)\n",
      "    Action 3: 3849 (15.4%)\n",
      "  Player 1:\n",
      "    Action 0: 21465 (78.7%)\n",
      "    Action 1: 5804 (21.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2668.5, -2668.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.740 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.747 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.744\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2668\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1588, 0.8389, 0.0023, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.0020e-05, 1.2575e-05, 1.4787e-05, 1.4988e-05, 1.9743e-05,\n",
      "          9.9397e-06, 1.5217e-05, 1.0340e-04, 2.1654e-04, 3.8259e-05,\n",
      "          1.3280e-02, 2.8698e-03, 1.8467e-04, 1.1229e-04, 7.0499e-05,\n",
      "          2.0573e-02, 1.9996e-03, 1.7621e-03, 2.0513e-03, 2.4503e-04,\n",
      "          2.4424e-03, 2.7499e-04, 1.5013e-04, 1.8903e-04, 3.7133e-05,\n",
      "          3.5687e-01, 1.2073e-05, 3.7420e-05, 3.9305e-05, 7.7178e-04,\n",
      "          1.2592e-02, 5.7951e-04, 3.6305e-03, 5.1336e-03, 4.5095e-02,\n",
      "          4.5071e-01, 8.5419e-05, 1.6279e-04, 4.3091e-04, 1.1986e-02,\n",
      "          6.4701e-02, 1.0988e-04, 1.4463e-04, 9.9324e-05, 1.4459e-05,\n",
      "          2.1952e-05, 1.7747e-05, 1.3319e-05, 1.4449e-05, 1.1796e-05,\n",
      "          1.2143e-05],\n",
      "         [6.6532e-06, 8.7405e-06, 9.6678e-06, 8.0977e-06, 1.1415e-05,\n",
      "          9.0006e-06, 1.1289e-05, 2.8504e-04, 3.8177e-04, 1.5205e-04,\n",
      "          5.8940e-04, 4.0016e-04, 1.6678e-02, 1.9667e-02, 2.5634e-03,\n",
      "          1.6953e-04, 9.1363e-05, 5.7398e-03, 3.2626e-03, 5.7825e-04,\n",
      "          6.0040e-05, 2.5118e-05, 4.7333e-05, 5.0905e-05, 1.6360e-05,\n",
      "          1.7403e-01, 1.0571e-05, 8.0647e-05, 8.1031e-05, 9.4340e-05,\n",
      "          1.5400e-03, 3.0726e-02, 2.8119e-01, 3.1171e-01, 1.0301e-04,\n",
      "          1.0017e-04, 5.6413e-03, 5.6145e-02, 4.7959e-02, 1.2530e-04,\n",
      "          2.2034e-04, 6.1202e-03, 1.9697e-02, 1.3543e-02, 9.9486e-06,\n",
      "          7.6859e-06, 9.5739e-06, 7.1290e-06, 7.2720e-06, 8.8975e-06,\n",
      "          7.1001e-06],\n",
      "         [9.1358e-06, 5.5262e-06, 7.3968e-06, 1.2265e-05, 7.1547e-06,\n",
      "          1.0791e-05, 5.3730e-06, 4.5591e-06, 5.9047e-06, 6.4035e-06,\n",
      "          4.2726e-06, 5.3283e-06, 1.0107e-03, 1.1338e-03, 3.6605e-06,\n",
      "          3.3640e-03, 7.6447e-06, 2.6772e-04, 3.8455e-04, 6.3093e-06,\n",
      "          9.4642e-04, 3.4594e-06, 5.2062e-01, 4.7197e-01, 2.8269e-05,\n",
      "          6.9184e-06, 6.8421e-06, 8.6993e-06, 5.9487e-06, 6.3986e-06,\n",
      "          7.9486e-06, 6.3371e-06, 8.1107e-06, 4.2971e-06, 5.5721e-06,\n",
      "          5.6209e-06, 5.7929e-06, 7.2081e-06, 5.8605e-06, 6.1913e-06,\n",
      "          1.3266e-05, 7.5481e-06, 7.6671e-06, 8.7369e-06, 4.8365e-06,\n",
      "          4.1972e-06, 4.4762e-06, 6.7872e-06, 5.8654e-06, 5.8551e-06,\n",
      "          6.9972e-06],\n",
      "         [2.6874e-04, 3.4309e-04, 5.0732e-04, 3.4786e-04, 4.1876e-04,\n",
      "          3.8419e-04, 5.1321e-04, 1.3652e-03, 1.0374e-03, 8.8429e-04,\n",
      "          1.7147e-02, 8.7655e-03, 1.1107e-02, 1.0677e-02, 1.5442e-03,\n",
      "          2.1015e-02, 3.5322e-03, 2.3028e-02, 3.0436e-02, 6.5357e-03,\n",
      "          3.0299e-03, 1.8140e-03, 4.2982e-02, 5.4956e-02, 2.2328e-03,\n",
      "          3.2366e-01, 3.9773e-04, 2.5564e-02, 1.7118e-02, 1.8953e-03,\n",
      "          2.0217e-02, 2.0699e-02, 7.9351e-02, 7.6667e-02, 3.9161e-02,\n",
      "          6.0391e-02, 4.9093e-03, 2.7007e-02, 1.9621e-02, 9.0078e-03,\n",
      "          1.3445e-02, 3.4755e-03, 4.0986e-03, 5.7912e-03, 3.7665e-04,\n",
      "          4.2394e-04, 3.8584e-04, 3.2059e-04, 4.1562e-04, 4.3686e-04,\n",
      "          2.9240e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9988, 0.0000, 0.0012, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.4842e-06, 3.2633e-06, 2.7871e-06, 1.1492e-06, 2.8698e-06,\n",
      "          2.1851e-06, 1.8878e-06, 2.9486e-05, 3.2683e-05, 1.4037e-05,\n",
      "          1.0704e-05, 3.4135e-05, 5.1064e-03, 6.9118e-03, 9.9039e-05,\n",
      "          4.1935e-07, 2.0198e-05, 4.9867e-03, 3.6505e-03, 5.8085e-05,\n",
      "          2.1715e-06, 3.8149e-06, 1.4203e-05, 3.6838e-05, 9.3751e-06,\n",
      "          4.7308e-03, 2.1564e-06, 1.3414e-05, 1.1996e-05, 7.1297e-06,\n",
      "          1.4023e-05, 1.3315e-03, 1.1154e-01, 1.1634e-01, 2.7930e-05,\n",
      "          4.3896e-05, 2.5079e-04, 3.9124e-01, 3.2177e-01, 8.8365e-06,\n",
      "          7.8251e-04, 2.4407e-04, 1.6293e-02, 1.4297e-02, 1.7636e-06,\n",
      "          1.8443e-06, 2.1856e-06, 2.1278e-06, 1.8948e-06, 2.4625e-06,\n",
      "          2.7365e-06],\n",
      "         [2.2169e-06, 3.1117e-06, 4.2181e-06, 5.3228e-06, 3.8453e-06,\n",
      "          2.5632e-06, 4.0275e-06, 6.6993e-05, 1.0364e-04, 3.6126e-05,\n",
      "          2.1448e-05, 1.2366e-04, 2.5919e-04, 2.9041e-04, 1.0232e-04,\n",
      "          2.0657e-05, 1.9175e-05, 7.2567e-05, 2.7810e-05, 8.3407e-05,\n",
      "          2.0855e-05, 1.0278e-05, 9.7676e-06, 1.6387e-05, 5.0198e-06,\n",
      "          4.7665e-04, 3.5942e-06, 2.6105e-05, 4.5011e-05, 1.3707e-05,\n",
      "          1.1841e-04, 1.0263e-03, 1.7106e-01, 1.7194e-01, 9.5127e-06,\n",
      "          1.5782e-04, 5.0564e-05, 2.0214e-01, 2.0164e-01, 1.2827e-04,\n",
      "          2.2987e-04, 1.8334e-02, 1.2813e-01, 1.0313e-01, 4.3601e-06,\n",
      "          3.7572e-06, 3.9522e-06, 3.6427e-06, 3.0780e-06, 3.4999e-06,\n",
      "          2.2474e-06],\n",
      "         [1.8934e-05, 1.2126e-05, 9.8438e-06, 1.7654e-05, 1.4613e-05,\n",
      "          1.9037e-05, 8.7444e-06, 2.0745e-05, 9.5908e-06, 1.9708e-05,\n",
      "          1.8245e-05, 8.3504e-06, 3.0923e-03, 2.5674e-03, 8.7831e-06,\n",
      "          5.7136e-04, 1.9747e-05, 5.0898e-01, 4.8089e-01, 2.0180e-05,\n",
      "          2.3228e-03, 9.7085e-06, 1.5393e-04, 7.4541e-04, 9.9821e-05,\n",
      "          1.2797e-05, 1.0566e-05, 1.3678e-05, 1.5013e-05, 1.4250e-05,\n",
      "          9.3430e-06, 7.7364e-06, 1.7909e-05, 8.6345e-06, 9.3291e-06,\n",
      "          1.7579e-05, 9.8579e-06, 7.8286e-06, 1.4729e-05, 7.3014e-06,\n",
      "          2.0009e-05, 2.0939e-05, 1.1530e-05, 1.3594e-05, 1.3914e-05,\n",
      "          1.7449e-05, 1.1088e-05, 1.2891e-05, 1.1992e-05, 1.2146e-05,\n",
      "          2.0031e-05],\n",
      "         [1.9098e-05, 2.1151e-05, 2.9686e-05, 3.8515e-05, 3.1371e-05,\n",
      "          3.0987e-05, 2.2273e-05, 4.3063e-04, 5.9206e-04, 1.5733e-04,\n",
      "          3.2044e-04, 2.9073e-04, 1.4604e-04, 1.7268e-04, 7.8729e-05,\n",
      "          6.6109e-04, 1.7184e-04, 7.0315e-03, 7.3578e-03, 9.6773e-04,\n",
      "          5.8224e-06, 9.7389e-05, 7.5815e-05, 1.4978e-04, 1.2606e-04,\n",
      "          5.3654e-03, 4.0847e-05, 4.3532e-04, 3.3790e-04, 8.2753e-05,\n",
      "          2.9460e-04, 1.7588e-03, 4.5873e-01, 4.6523e-01, 1.0979e-03,\n",
      "          2.9788e-03, 1.8332e-04, 1.0960e-03, 1.1874e-03, 6.2552e-04,\n",
      "          1.6651e-03, 2.7476e-03, 2.0027e-02, 1.6865e-02, 4.1972e-05,\n",
      "          2.7869e-05, 5.3248e-05, 2.3168e-05, 3.0232e-05, 1.5785e-05,\n",
      "          3.0738e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0814, 0.0679, 0.8506, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58319\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5887/10000 (58.9%)\n",
      "    Average reward: +0.072\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4113/10000 (41.1%)\n",
      "    Average reward: -0.072\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8551 (29.6%)\n",
      "    Action 1: 17086 (59.1%)\n",
      "    Action 2: 2372 (8.2%)\n",
      "    Action 3: 883 (3.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8311 (28.2%)\n",
      "    Action 1: 14162 (48.1%)\n",
      "    Action 2: 2820 (9.6%)\n",
      "    Action 3: 4134 (14.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [716.0, -716.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.968 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.995\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0716\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1588, 0.8389, 0.0023, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8451, 0.0089, 0.1460]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51400\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5962/10000 (59.6%)\n",
      "    Average reward: -0.202\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4038/10000 (40.4%)\n",
      "    Average reward: +0.202\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22894 (83.5%)\n",
      "    Action 1: 4533 (16.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3099 (12.9%)\n",
      "    Action 1: 17936 (74.8%)\n",
      "    Action 2: 833 (3.5%)\n",
      "    Action 3: 2105 (8.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2015.0, 2015.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.647 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.695 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.671\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.2015\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.3778}, {'exploitability': 0.47965}, {'exploitability': 0.5057750000000001}, {'exploitability': 0.7687999999999999}, {'exploitability': 0.7998000000000001}, {'exploitability': 0.753925}, {'exploitability': 0.786375}, {'exploitability': 0.6672}, {'exploitability': 0.660875}, {'exploitability': 0.6716500000000001}, {'exploitability': 0.5985499999999999}, {'exploitability': 0.50625}, {'exploitability': 0.509575}, {'exploitability': 0.44882500000000003}, {'exploitability': 0.4548}, {'exploitability': 0.495025}, {'exploitability': 0.44284999999999997}, {'exploitability': 0.37677499999999997}, {'exploitability': 0.353675}, {'exploitability': 0.395175}, {'exploitability': 0.4645}, {'exploitability': 0.388975}, {'exploitability': 0.4347}, {'exploitability': 0.418525}, {'exploitability': 0.2883}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddfbb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1059a1f00>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1059a1f00>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.06\n",
      "Using         eg_epsilon                    : 0.0\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1059a1f00>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.06\n",
      "Using         eg_epsilon                    : 0.0\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1059a0310>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1059a0310>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6fc850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-Noisy\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e4597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/50000 [00:00<19:20, 43.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 69/200000\n",
      "   Player 0 SL buffer: 10/2000000\n",
      "   Player 1 RL buffer: 59/200000\n",
      "   Player 1 SL buffer: 4/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1004/50000 [00:30<23:23, 34.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 63806/200000\n",
      "   Player 0 SL buffer: 6890/2000000\n",
      "   Player 1 RL buffer: 64322/200000\n",
      "   Player 1 SL buffer: 7497/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1999/50000 [01:00<23:25, 34.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 127903/200000\n",
      "   Player 0 SL buffer: 13604/2000000\n",
      "   Player 1 RL buffer: 128223/200000\n",
      "   Player 1 SL buffer: 14402/2000000\n",
      "P1 SL Buffer Size:  13604\n",
      "P1 SL buffer distribution [4169. 7357.  669. 1409.]\n",
      "P1 actions distribution [0.30645398 0.54079682 0.04917671 0.10357248]\n",
      "P2 SL Buffer Size:  14402\n",
      "P2 SL buffer distribution [4225. 7959.  608. 1610.]\n",
      "P2 actions distribution [0.29336203 0.55263158 0.04221636 0.11179003]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.2800, 0.6745, 0.0455, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.9938,  1.4187, -1.0911,  1.1235]])\n",
      "Player 0 Prediction: tensor([[0.1954, 0.7628, 0.0418, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 4.3951,  5.3366, -2.0340,  3.3026]])\n",
      "Player 0 Prediction: tensor([[0.4007, 0.5268, 0.0725, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 5.2755,  6.5749, -4.0791,  4.2460]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55618\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5148/10000 (51.5%)\n",
      "    Average reward: -1.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4852/10000 (48.5%)\n",
      "    Average reward: +1.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9588 (34.5%)\n",
      "    Action 1: 14942 (53.7%)\n",
      "    Action 2: 1543 (5.5%)\n",
      "    Action 3: 1739 (6.3%)\n",
      "  Player 1:\n",
      "    Action 0: 6921 (24.9%)\n",
      "    Action 1: 14292 (51.4%)\n",
      "    Action 2: 2648 (9.5%)\n",
      "    Action 3: 3945 (14.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-11291.0, 11291.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.993 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.1291\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8347, 0.0402, 0.1251]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7284, 0.0824, 0.1893]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48281\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5316/10000 (53.2%)\n",
      "    Average reward: -0.201\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4684/10000 (46.8%)\n",
      "    Average reward: +0.201\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2616 (11.6%)\n",
      "    Action 1: 16807 (74.4%)\n",
      "    Action 2: 1039 (4.6%)\n",
      "    Action 3: 2141 (9.5%)\n",
      "  Player 1:\n",
      "    Action 0: 21844 (85.1%)\n",
      "    Action 1: 3834 (14.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2011.5, 2011.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.678 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.608 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.643\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.2011\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2439, 0.7294, 0.0267, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.2805,  0.5183, -0.9771,  0.2282]])\n",
      "Player 1 Prediction: tensor([[0.9532, 0.0000, 0.0468, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 5.8830,  6.7604, -2.8940,  4.2268]])\n",
      "Player 1 Prediction: tensor([[0.1371, 0.8132, 0.0497, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 7.4344,  8.5127, -4.9076,  5.1968]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1999/50000 [01:20<23:25, 34.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55794\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5742/10000 (57.4%)\n",
      "    Average reward: +0.848\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4258/10000 (42.6%)\n",
      "    Average reward: -0.848\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8315 (29.2%)\n",
      "    Action 1: 13766 (48.3%)\n",
      "    Action 2: 2850 (10.0%)\n",
      "    Action 3: 3549 (12.5%)\n",
      "  Player 1:\n",
      "    Action 0: 8712 (31.9%)\n",
      "    Action 1: 15700 (57.5%)\n",
      "    Action 2: 1326 (4.9%)\n",
      "    Action 3: 1576 (5.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8484.5, -8484.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.985 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8485\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2429, 0.7317, 0.0254, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.1755, 0.8007, 0.0238, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.5461, 0.4026, 0.0513, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47729\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6357/10000 (63.6%)\n",
      "    Average reward: +0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3643/10000 (36.4%)\n",
      "    Average reward: -0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21680 (85.7%)\n",
      "    Action 1: 3627 (14.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2625 (11.7%)\n",
      "    Action 1: 16721 (74.6%)\n",
      "    Action 2: 1009 (4.5%)\n",
      "    Action 3: 2067 (9.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1272.5, -1272.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.593 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.678 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.635\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1273\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:315: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:379: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:245: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:265: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "  6%|â–Œ         | 3003/50000 [02:01<25:34, 30.64it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 192070/200000\n",
      "   Player 0 SL buffer: 20199/2000000\n",
      "   Player 1 RL buffer: 192056/200000\n",
      "   Player 1 SL buffer: 20860/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3999/50000 [02:35<25:33, 30.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 26954/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 27176/2000000\n",
      "P1 SL Buffer Size:  26954\n",
      "P1 SL buffer distribution [ 8236. 14397.  1763.  2558.]\n",
      "P1 actions distribution [0.30555762 0.53413223 0.06540773 0.09490243]\n",
      "P2 SL Buffer Size:  27176\n",
      "P2 SL buffer distribution [ 8293. 14331.  1725.  2827.]\n",
      "P2 actions distribution [0.30515896 0.5273403  0.06347513 0.10402561]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1746, 0.8129, 0.0125, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.3230,  1.1085, -1.4471,  0.7875]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9096, 0.0312, 0.0592]])\n",
      "Player 1 Prediction: tensor([[ 4.8197,  5.7805, -2.3817,  3.8012]])\n",
      "Player 0 Prediction: tensor([[0.9430, 0.0000, 0.0570, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3999/50000 [02:50<25:33, 30.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49462\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5301/10000 (53.0%)\n",
      "    Average reward: -1.105\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4699/10000 (47.0%)\n",
      "    Average reward: +1.105\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4169 (17.5%)\n",
      "    Action 1: 16282 (68.5%)\n",
      "    Action 2: 1517 (6.4%)\n",
      "    Action 3: 1791 (7.5%)\n",
      "  Player 1:\n",
      "    Action 0: 14291 (55.6%)\n",
      "    Action 1: 7438 (28.9%)\n",
      "    Action 2: 3271 (12.7%)\n",
      "    Action 3: 703 (2.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-11050.0, 11050.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.814 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.989 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.901\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.1050\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8526, 0.0492, 0.0983]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6279, 0.1494, 0.2227]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47790\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5353/10000 (53.5%)\n",
      "    Average reward: -0.141\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4647/10000 (46.5%)\n",
      "    Average reward: +0.141\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2110 (9.4%)\n",
      "    Action 1: 17160 (76.5%)\n",
      "    Action 2: 1322 (5.9%)\n",
      "    Action 3: 1852 (8.3%)\n",
      "  Player 1:\n",
      "    Action 0: 22131 (87.3%)\n",
      "    Action 1: 3215 (12.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1414.5, 1414.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.617 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.549 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.583\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1414\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.0940, -0.3975, -0.6965,  0.1532]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9435, 0.0164, 0.0401]])\n",
      "Player 0 Prediction: tensor([[-0.2215, -0.2249, -1.2049, -0.0015]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8614, 0.0429, 0.0957]])\n",
      "Player 0 Prediction: tensor([[ 4.5434,  5.5380, -1.8873,  4.2275]])\n",
      "Player 1 Prediction: tensor([[0.9338, 0.0000, 0.0662, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53382\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5813/10000 (58.1%)\n",
      "    Average reward: +1.042\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4187/10000 (41.9%)\n",
      "    Average reward: -1.042\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11235 (41.1%)\n",
      "    Action 1: 10434 (38.1%)\n",
      "    Action 2: 2913 (10.6%)\n",
      "    Action 3: 2782 (10.2%)\n",
      "  Player 1:\n",
      "    Action 0: 6920 (26.6%)\n",
      "    Action 1: 15574 (59.9%)\n",
      "    Action 2: 1548 (5.9%)\n",
      "    Action 3: 1976 (7.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [10415.5, -10415.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.0415\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4533, 0.4986, 0.0481, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6134, 0.1459, 0.2407]])\n",
      "Player 1 Prediction: tensor([[0.5867, 0.2496, 0.1637, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48374\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6385/10000 (63.8%)\n",
      "    Average reward: +0.141\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3615/10000 (36.1%)\n",
      "    Average reward: -0.141\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21629 (84.8%)\n",
      "    Action 1: 3888 (15.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2751 (12.0%)\n",
      "    Action 1: 16638 (72.8%)\n",
      "    Action 2: 1353 (5.9%)\n",
      "    Action 3: 2115 (9.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1410.0, -1410.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.616 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.701 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.658\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1410\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5004/50000 [03:40<24:43, 30.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 33470/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 33852/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6000/50000 [04:22<25:06, 29.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 40060/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 40311/2000000\n",
      "P1 SL Buffer Size:  40060\n",
      "P1 SL buffer distribution [12488. 20950.  3000.  3622.]\n",
      "P1 actions distribution [0.3117324  0.52296555 0.07488767 0.09041438]\n",
      "P2 SL Buffer Size:  40311\n",
      "P2 SL buffer distribution [12583. 20809.  3037.  3882.]\n",
      "P2 actions distribution [0.31214805 0.51621146 0.07533924 0.09630126]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1926, 0.7929, 0.0145, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.0186, -0.1028, -1.2825,  0.0452]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7110, 0.0916, 0.1974]])\n",
      "Player 1 Prediction: tensor([[ 4.6432,  6.1727, -2.4139,  4.0849]])\n",
      "Player 0 Prediction: tensor([[0.3854, 0.4579, 0.1567, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49908\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5315/10000 (53.1%)\n",
      "    Average reward: -0.955\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4685/10000 (46.9%)\n",
      "    Average reward: +0.955\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4437 (18.2%)\n",
      "    Action 1: 16142 (66.3%)\n",
      "    Action 2: 1981 (8.1%)\n",
      "    Action 3: 1771 (7.3%)\n",
      "  Player 1:\n",
      "    Action 0: 13857 (54.2%)\n",
      "    Action 1: 7847 (30.7%)\n",
      "    Action 2: 3161 (12.4%)\n",
      "    Action 3: 712 (2.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-9554.5, 9554.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.840 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.002 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.921\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.9555\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1632, 0.8312, 0.0055, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9156, 0.0319, 0.0525]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6000/50000 [04:40<25:06, 29.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47935\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5220/10000 (52.2%)\n",
      "    Average reward: -0.187\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4780/10000 (47.8%)\n",
      "    Average reward: +0.187\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2127 (9.4%)\n",
      "    Action 1: 16974 (74.9%)\n",
      "    Action 2: 1660 (7.3%)\n",
      "    Action 3: 1904 (8.4%)\n",
      "  Player 1:\n",
      "    Action 0: 21875 (86.6%)\n",
      "    Action 1: 3395 (13.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1866.0, 1866.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.633 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.569 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.601\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1866\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.6282,  2.6673, -0.4142,  2.2319]])\n",
      "Player 1 Prediction: tensor([[0.2183, 0.7245, 0.0572, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52528\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5976/10000 (59.8%)\n",
      "    Average reward: +1.005\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4024/10000 (40.2%)\n",
      "    Average reward: -1.005\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10298 (38.9%)\n",
      "    Action 1: 11841 (44.7%)\n",
      "    Action 2: 2773 (10.5%)\n",
      "    Action 3: 1577 (6.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6534 (25.1%)\n",
      "    Action 1: 15474 (59.4%)\n",
      "    Action 2: 2097 (8.1%)\n",
      "    Action 3: 1934 (7.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [10047.5, -10047.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.947 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.998\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.0048\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5495, 0.4018, 0.0487, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5292, 0.2273, 0.2435]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47920\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6509/10000 (65.1%)\n",
      "    Average reward: +0.215\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3491/10000 (34.9%)\n",
      "    Average reward: -0.215\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21475 (85.2%)\n",
      "    Action 1: 3733 (14.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2550 (11.2%)\n",
      "    Action 1: 16506 (72.7%)\n",
      "    Action 2: 1773 (7.8%)\n",
      "    Action 3: 1883 (8.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2155.0, -2155.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.605 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.689 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.647\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.2155\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7003/50000 [05:24<23:22, 30.66it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 46772/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 46712/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [06:01<29:37, 23.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 52903/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 53343/2000000\n",
      "P1 SL Buffer Size:  52903\n",
      "P1 SL buffer distribution [16969. 27094.  4137.  4703.]\n",
      "P1 actions distribution [0.32075686 0.51214487 0.07819972 0.08889855]\n",
      "P2 SL Buffer Size:  53343\n",
      "P2 SL buffer distribution [16923. 27245.  4333.  4842.]\n",
      "P2 actions distribution [0.31724875 0.51075118 0.08122903 0.09077105]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.5723,  2.5163, -0.7040,  1.9326]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7661, 0.0667, 0.1672]])\n",
      "Player 1 Prediction: tensor([[ 0.4152,  0.8209, -1.4830,  0.7239]])\n",
      "Player 0 Prediction: tensor([[0.4438, 0.4594, 0.0968, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58315\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5126/10000 (51.3%)\n",
      "    Average reward: -0.716\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4874/10000 (48.7%)\n",
      "    Average reward: +0.716\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9892 (34.0%)\n",
      "    Action 1: 14846 (51.1%)\n",
      "    Action 2: 2442 (8.4%)\n",
      "    Action 3: 1891 (6.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7761 (26.5%)\n",
      "    Action 1: 14709 (50.3%)\n",
      "    Action 2: 2482 (8.5%)\n",
      "    Action 3: 4292 (14.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7157.0, 7157.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7157\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9469, 0.0108, 0.0423]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6601, 0.1059, 0.2340]])\n",
      "Player 0 Prediction: tensor([[0.2760, 0.4569, 0.2670, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [06:20<29:37, 23.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48396\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5154/10000 (51.5%)\n",
      "    Average reward: -0.139\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4846/10000 (48.5%)\n",
      "    Average reward: +0.139\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2287 (10.0%)\n",
      "    Action 1: 16638 (72.5%)\n",
      "    Action 2: 1929 (8.4%)\n",
      "    Action 3: 2091 (9.1%)\n",
      "  Player 1:\n",
      "    Action 0: 21647 (85.1%)\n",
      "    Action 1: 3804 (14.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1392.5, 1392.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.668 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.608 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.638\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1393\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5884, 0.3647, 0.0469, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.2211,  1.7363, -1.1253,  1.2482]])\n",
      "Player 1 Prediction: tensor([[0.2670, 0.6865, 0.0465, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.3049,  2.6983, -2.4336,  2.4411]])\n",
      "Player 1 Prediction: tensor([[0.2280, 0.2060, 0.5661, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56844\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6431/10000 (64.3%)\n",
      "    Average reward: +0.868\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3569/10000 (35.7%)\n",
      "    Average reward: -0.868\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8271 (28.9%)\n",
      "    Action 1: 15197 (53.1%)\n",
      "    Action 2: 2431 (8.5%)\n",
      "    Action 3: 2729 (9.5%)\n",
      "  Player 1:\n",
      "    Action 0: 8383 (29.7%)\n",
      "    Action 1: 14893 (52.8%)\n",
      "    Action 2: 2944 (10.4%)\n",
      "    Action 3: 1996 (7.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8679.0, -8679.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.003 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8679\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2667, 0.7218, 0.0114, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.1142, 0.8761, 0.0096, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7328, 0.0427, 0.2244]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47780\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6509/10000 (65.1%)\n",
      "    Average reward: +0.197\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3491/10000 (34.9%)\n",
      "    Average reward: -0.197\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21299 (84.8%)\n",
      "    Action 1: 3817 (15.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2568 (11.3%)\n",
      "    Action 1: 16269 (71.8%)\n",
      "    Action 2: 2088 (9.2%)\n",
      "    Action 3: 1739 (7.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1971.0, -1971.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.615 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.699 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.657\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1971\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9005/50000 [07:11<27:49, 24.56it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 59480/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 60026/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 10000/50000 [07:55<27:23, 24.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 65909/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 66512/2000000\n",
      "P1 SL Buffer Size:  65909\n",
      "P1 SL buffer distribution [21182. 33490.  5336.  5901.]\n",
      "P1 actions distribution [0.32138251 0.50812484 0.08096011 0.08953254]\n",
      "P2 SL Buffer Size:  66512\n",
      "P2 SL buffer distribution [21302. 34103.  5529.  5578.]\n",
      "P2 actions distribution [0.32027303 0.51273454 0.08312786 0.08386457]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1485, 0.8470, 0.0045, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.9417,  3.3663, -0.7288,  2.3240]])\n",
      "Player 0 Prediction: tensor([[0.9950, 0.0000, 0.0050, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.5509,  3.1407, -3.4160,  1.3410]])\n",
      "Player 0 Prediction: tensor([[0.0995, 0.3486, 0.5519, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 10000/50000 [08:10<27:23, 24.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55290\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4568/10000 (45.7%)\n",
      "    Average reward: -0.785\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5432/10000 (54.3%)\n",
      "    Average reward: +0.785\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8015 (28.7%)\n",
      "    Action 1: 14805 (53.1%)\n",
      "    Action 2: 3239 (11.6%)\n",
      "    Action 3: 1821 (6.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7060 (25.8%)\n",
      "    Action 1: 17223 (62.8%)\n",
      "    Action 2: 2556 (9.3%)\n",
      "    Action 3: 571 (2.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7846.0, 7846.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.002 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.925 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.964\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7846\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9882, 0.0016, 0.0102]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9682, 0.0097, 0.0221]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48594\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5144/10000 (51.4%)\n",
      "    Average reward: -0.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4856/10000 (48.6%)\n",
      "    Average reward: +0.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2387 (10.3%)\n",
      "    Action 1: 16401 (70.9%)\n",
      "    Action 2: 2075 (9.0%)\n",
      "    Action 3: 2274 (9.8%)\n",
      "  Player 1:\n",
      "    Action 0: 21309 (83.7%)\n",
      "    Action 1: 4148 (16.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1292.0, 1292.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.690 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.641 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.666\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1292\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2893, 0.7021, 0.0086, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.0384, -0.2683, -0.9702,  0.7092]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9343, 0.0152, 0.0504]])\n",
      "Player 0 Prediction: tensor([[-0.6749,  0.3412, -1.4189, -0.3770]])\n",
      "Player 1 Prediction: tensor([[0.2081, 0.0000, 0.7919, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53052\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6352/10000 (63.5%)\n",
      "    Average reward: +0.960\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3648/10000 (36.5%)\n",
      "    Average reward: -0.960\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10223 (38.6%)\n",
      "    Action 1: 12666 (47.8%)\n",
      "    Action 2: 2729 (10.3%)\n",
      "    Action 3: 897 (3.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6561 (24.7%)\n",
      "    Action 1: 15161 (57.1%)\n",
      "    Action 2: 3050 (11.5%)\n",
      "    Action 3: 1765 (6.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9601.0, -9601.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.960 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.000\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9601\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9408, 0.0141, 0.0451]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8981, 0.0390, 0.0629]])\n",
      "Player 1 Prediction: tensor([[0.2465, 0.4881, 0.2654, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48351\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6519/10000 (65.2%)\n",
      "    Average reward: +0.155\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3481/10000 (34.8%)\n",
      "    Average reward: -0.155\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21261 (83.7%)\n",
      "    Action 1: 4132 (16.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2684 (11.7%)\n",
      "    Action 1: 16265 (70.8%)\n",
      "    Action 2: 2158 (9.4%)\n",
      "    Action 3: 1851 (8.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1553.5, -1553.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.641 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.714 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.678\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1553\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11006/50000 [09:03<24:18, 26.73it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 72228/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 72821/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11999/50000 [09:43<38:53, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 78290/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 79201/2000000\n",
      "P1 SL Buffer Size:  78290\n",
      "P1 SL buffer distribution [25448. 39455.  6519.  6868.]\n",
      "P1 actions distribution [0.3250479  0.50395964 0.08326734 0.08772512]\n",
      "P2 SL Buffer Size:  79201\n",
      "P2 SL buffer distribution [25572. 40690.  6614.  6325.]\n",
      "P2 actions distribution [0.32287471 0.51375614 0.08350905 0.0798601 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.6293,  1.0014, -0.2749,  0.4436]])\n",
      "Player 0 Prediction: tensor([[0.0806, 0.9162, 0.0032, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.1302,  0.8135, -2.0038,  0.4742]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5352, 0.0257, 0.4392]])\n",
      "Player 1 Prediction: tensor([[-2.3985, -1.6215, -2.8956, -2.3110]])\n",
      "Player 0 Prediction: tensor([[0.1031, 0.2905, 0.6064, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11999/50000 [10:00<38:53, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53494\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4833/10000 (48.3%)\n",
      "    Average reward: -0.825\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5167/10000 (51.7%)\n",
      "    Average reward: +0.825\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7060 (26.1%)\n",
      "    Action 1: 14842 (54.9%)\n",
      "    Action 2: 3094 (11.4%)\n",
      "    Action 3: 2036 (7.5%)\n",
      "  Player 1:\n",
      "    Action 0: 9591 (36.2%)\n",
      "    Action 1: 13373 (50.5%)\n",
      "    Action 2: 2688 (10.2%)\n",
      "    Action 3: 810 (3.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-8254.0, 8254.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.981 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.8254\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9819, 0.0010, 0.0171]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9667, 0.0065, 0.0268]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48722\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5213/10000 (52.1%)\n",
      "    Average reward: -0.059\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4787/10000 (47.9%)\n",
      "    Average reward: +0.059\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2456 (10.6%)\n",
      "    Action 1: 16137 (69.7%)\n",
      "    Action 2: 2128 (9.2%)\n",
      "    Action 3: 2427 (10.5%)\n",
      "  Player 1:\n",
      "    Action 0: 21162 (82.7%)\n",
      "    Action 1: 4412 (17.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-588.0, 588.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.706 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.663 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.685\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0588\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6437, 0.3207, 0.0357, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.0353,  2.9618, -0.9867,  1.7007]])\n",
      "Player 1 Prediction: tensor([[0.2968, 0.6790, 0.0242, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.6405,  3.3639, -2.0339,  2.4993]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3287, 0.0817, 0.5897]])\n",
      "Player 0 Prediction: tensor([[ 0.0406,  0.9407, -2.9215,  0.7537]])\n",
      "Player 1 Prediction: tensor([[0.0680, 0.1529, 0.7791, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-1.3544, -1.2501, -4.4487, -0.7635]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51800\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6217/10000 (62.2%)\n",
      "    Average reward: +0.949\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3783/10000 (37.8%)\n",
      "    Average reward: -0.949\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11162 (43.1%)\n",
      "    Action 1: 10901 (42.1%)\n",
      "    Action 2: 2768 (10.7%)\n",
      "    Action 3: 1076 (4.2%)\n",
      "  Player 1:\n",
      "    Action 0: 5789 (22.4%)\n",
      "    Action 1: 15364 (59.3%)\n",
      "    Action 2: 2836 (11.0%)\n",
      "    Action 3: 1904 (7.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9486.5, -9486.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.930 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.989\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9486\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9354, 0.0113, 0.0533]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48746\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6450/10000 (64.5%)\n",
      "    Average reward: +0.135\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3550/10000 (35.5%)\n",
      "    Average reward: -0.135\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21336 (83.1%)\n",
      "    Action 1: 4350 (16.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2781 (12.1%)\n",
      "    Action 1: 16398 (71.1%)\n",
      "    Action 2: 1944 (8.4%)\n",
      "    Action 3: 1937 (8.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1351.5, -1351.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.656 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.718 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.687\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1351\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13004/50000 [11:06<24:51, 24.80it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 84499/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 85387/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13998/50000 [11:47<24:18, 24.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 90810/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 91727/2000000\n",
      "P1 SL Buffer Size:  90810\n",
      "P1 SL buffer distribution [29538. 45709.  7688.  7875.]\n",
      "P1 actions distribution [0.32527255 0.50334765 0.08466028 0.08671952]\n",
      "P2 SL Buffer Size:  91727\n",
      "P2 SL buffer distribution [29620. 47307.  7774.  7026.]\n",
      "P2 actions distribution [0.32291474 0.51573691 0.08475149 0.07659686]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 3.3602,  2.7599, -0.8758,  2.2154]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9327, 0.0028, 0.0645]])\n",
      "Player 1 Prediction: tensor([[ 2.6072,  2.4928, -1.2740,  2.3996]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8535, 0.0267, 0.1198]])\n",
      "Player 1 Prediction: tensor([[ 0.1232, -1.4484, -2.0317, -0.0205]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13998/50000 [12:00<24:18, 24.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51476\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5008/10000 (50.1%)\n",
      "    Average reward: -0.671\n",
      "    Reward range: -7.0 to +6.0\n",
      "  Player 1:\n",
      "    Wins: 4992/10000 (49.9%)\n",
      "    Average reward: +0.671\n",
      "    Reward range: -6.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6032 (23.4%)\n",
      "    Action 1: 14628 (56.7%)\n",
      "    Action 2: 2389 (9.3%)\n",
      "    Action 3: 2734 (10.6%)\n",
      "  Player 1:\n",
      "    Action 0: 11973 (46.6%)\n",
      "    Action 1: 7691 (29.9%)\n",
      "    Action 2: 2533 (9.9%)\n",
      "    Action 3: 3496 (13.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6708.5, 6708.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.954 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.994\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6708\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.7410e-01, 5.9853e-04, 2.5304e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9478, 0.0042, 0.0481]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48668\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5283/10000 (52.8%)\n",
      "    Average reward: -0.056\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4717/10000 (47.2%)\n",
      "    Average reward: +0.056\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2455 (10.7%)\n",
      "    Action 1: 16093 (69.9%)\n",
      "    Action 2: 1992 (8.7%)\n",
      "    Action 3: 2483 (10.8%)\n",
      "  Player 1:\n",
      "    Action 0: 21128 (82.4%)\n",
      "    Action 1: 4517 (17.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-557.0, 557.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.705 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.672 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.689\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0557\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.4640,  0.4563, -0.5044,  0.8880]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5357, 0.0863, 0.3780]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51555\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6099/10000 (61.0%)\n",
      "    Average reward: +0.727\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3901/10000 (39.0%)\n",
      "    Average reward: -0.727\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9504 (36.5%)\n",
      "    Action 1: 10047 (38.6%)\n",
      "    Action 2: 2629 (10.1%)\n",
      "    Action 3: 3861 (14.8%)\n",
      "  Player 1:\n",
      "    Action 0: 6559 (25.7%)\n",
      "    Action 1: 14683 (57.5%)\n",
      "    Action 2: 2241 (8.8%)\n",
      "    Action 3: 2031 (8.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7265.5, -7265.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.963 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.012\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7266\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9180, 0.0102, 0.0718]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9110, 0.0237, 0.0653]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48755\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6262/10000 (62.6%)\n",
      "    Average reward: -0.006\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3738/10000 (37.4%)\n",
      "    Average reward: +0.006\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21234 (82.4%)\n",
      "    Action 1: 4531 (17.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2833 (12.3%)\n",
      "    Action 1: 16185 (70.4%)\n",
      "    Action 2: 1903 (8.3%)\n",
      "    Action 3: 2069 (9.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-62.5, 62.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.671 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.729 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.700\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0063\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15005/50000 [12:57<24:43, 23.59it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 97164/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 97882/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16000/50000 [13:40<23:29, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 103641/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 104074/2000000\n",
      "P1 SL Buffer Size:  103641\n",
      "P1 SL buffer distribution [33610. 51805.  8948.  9278.]\n",
      "P1 actions distribution [0.32429251 0.49985045 0.08633649 0.08952056]\n",
      "P2 SL Buffer Size:  104074\n",
      "P2 SL buffer distribution [33456. 53440.  8941.  8237.]\n",
      "P2 actions distribution [0.32146357 0.51348079 0.08591003 0.07914561]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1013, 0.8976, 0.0011, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.2153, -0.1568, -1.0992, -0.0085]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8776, 0.0186, 0.1038]])\n",
      "Player 1 Prediction: tensor([[-3.9014, -4.8112, -1.8431, -2.5627]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16000/50000 [13:51<23:29, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53353\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4897/10000 (49.0%)\n",
      "    Average reward: -0.545\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5103/10000 (51.0%)\n",
      "    Average reward: +0.545\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7552 (28.2%)\n",
      "    Action 1: 13755 (51.3%)\n",
      "    Action 2: 3087 (11.5%)\n",
      "    Action 3: 2405 (9.0%)\n",
      "  Player 1:\n",
      "    Action 0: 9456 (35.6%)\n",
      "    Action 1: 12650 (47.6%)\n",
      "    Action 2: 2432 (9.2%)\n",
      "    Action 3: 2016 (7.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5450.0, 5450.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.009 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.040 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5450\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.4881e-01, 8.5099e-01, 1.9926e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9337, 0.0028, 0.0635]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48885\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5278/10000 (52.8%)\n",
      "    Average reward: -0.009\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4722/10000 (47.2%)\n",
      "    Average reward: +0.009\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2510 (10.9%)\n",
      "    Action 1: 15941 (69.0%)\n",
      "    Action 2: 1942 (8.4%)\n",
      "    Action 3: 2715 (11.7%)\n",
      "  Player 1:\n",
      "    Action 0: 20937 (81.2%)\n",
      "    Action 1: 4840 (18.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-91.5, 91.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.717 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.697 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.707\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0092\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6525, 0.3196, 0.0279, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.5217,  0.9880, -0.8665,  1.0225]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3288, 0.1037, 0.5675]])\n",
      "Player 0 Prediction: tensor([[ 0.1400,  1.1878, -1.7147,  0.7110]])\n",
      "Player 1 Prediction: tensor([[0.1295, 0.0000, 0.8705, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54634\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6201/10000 (62.0%)\n",
      "    Average reward: +0.494\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3799/10000 (38.0%)\n",
      "    Average reward: -0.494\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6396 (23.7%)\n",
      "    Action 1: 16794 (62.3%)\n",
      "    Action 2: 2618 (9.7%)\n",
      "    Action 3: 1151 (4.3%)\n",
      "  Player 1:\n",
      "    Action 0: 8806 (31.8%)\n",
      "    Action 1: 14159 (51.2%)\n",
      "    Action 2: 3153 (11.4%)\n",
      "    Action 3: 1557 (5.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4942.0, -4942.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.918 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.969\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4942\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6525, 0.3196, 0.0279, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.3975, 0.5896, 0.0128, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7420, 0.0206, 0.2374]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48754\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6123/10000 (61.2%)\n",
      "    Average reward: -0.095\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3877/10000 (38.8%)\n",
      "    Average reward: +0.095\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20889 (81.1%)\n",
      "    Action 1: 4867 (18.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2955 (12.8%)\n",
      "    Action 1: 15934 (69.3%)\n",
      "    Action 2: 1777 (7.7%)\n",
      "    Action 3: 2332 (10.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-946.0, 946.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.699 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.747 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.723\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0946\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17002/50000 [14:53<22:59, 23.91it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 109834/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 110324/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18000/50000 [15:41<23:51, 22.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 116082/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 116744/2000000\n",
      "P1 SL Buffer Size:  116082\n",
      "P1 SL buffer distribution [37774. 57601. 10252. 10455.]\n",
      "P1 actions distribution [0.3254079  0.49620958 0.08831688 0.09006564]\n",
      "P2 SL Buffer Size:  116744\n",
      "P2 SL buffer distribution [37810. 59330. 10153.  9451.]\n",
      "P2 actions distribution [0.32387103 0.50820599 0.08696807 0.08095491]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.6231,  2.6715, -0.8754,  1.9466]])\n",
      "Player 0 Prediction: tensor([[0.0771, 0.9219, 0.0010, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.8145,  2.5848, -2.4238,  1.6144]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5838, 0.0160, 0.4002]])\n",
      "Player 1 Prediction: tensor([[ 0.7793,  1.6894, -2.7210,  0.9225]])\n",
      "Player 0 Prediction: tensor([[0.5007, 0.0000, 0.4993, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53377\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4877/10000 (48.8%)\n",
      "    Average reward: -0.415\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5123/10000 (51.2%)\n",
      "    Average reward: +0.415\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8299 (30.9%)\n",
      "    Action 1: 13447 (50.1%)\n",
      "    Action 2: 3207 (11.9%)\n",
      "    Action 3: 1908 (7.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6328 (23.9%)\n",
      "    Action 1: 16500 (62.2%)\n",
      "    Action 2: 2284 (8.6%)\n",
      "    Action 3: 1404 (5.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4146.0, 4146.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.919 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.971\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4146\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.4341e-01, 3.1082e-04, 5.6279e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9093, 0.0057, 0.0850]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18000/50000 [16:01<23:51, 22.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49086\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5379/10000 (53.8%)\n",
      "    Average reward: +0.019\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4621/10000 (46.2%)\n",
      "    Average reward: -0.019\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2441 (10.6%)\n",
      "    Action 1: 15999 (69.3%)\n",
      "    Action 2: 1812 (7.8%)\n",
      "    Action 3: 2843 (12.3%)\n",
      "  Player 1:\n",
      "    Action 0: 21050 (81.0%)\n",
      "    Action 1: 4941 (19.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [194.0, -194.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.710 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.702 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.706\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.0194\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6764, 0.3009, 0.0227, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.3917,  0.6224, -0.8115,  0.9077]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2346, 0.0932, 0.6721]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52455\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5885/10000 (58.9%)\n",
      "    Average reward: +0.697\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4115/10000 (41.1%)\n",
      "    Average reward: -0.697\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9482 (35.7%)\n",
      "    Action 1: 11036 (41.6%)\n",
      "    Action 2: 3069 (11.6%)\n",
      "    Action 3: 2955 (11.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6867 (26.5%)\n",
      "    Action 1: 14662 (56.6%)\n",
      "    Action 2: 2085 (8.0%)\n",
      "    Action 3: 2299 (8.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6970.0, -6970.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.973 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6970\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3394, 0.0677, 0.5929]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6440, 0.1241, 0.2318]])\n",
      "Player 1 Prediction: tensor([[0.0532, 0.0912, 0.8556, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48679\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6249/10000 (62.5%)\n",
      "    Average reward: -0.008\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3751/10000 (37.5%)\n",
      "    Average reward: +0.008\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20636 (79.9%)\n",
      "    Action 1: 5181 (20.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3060 (13.4%)\n",
      "    Action 1: 15630 (68.4%)\n",
      "    Action 2: 1638 (7.2%)\n",
      "    Action 3: 2534 (11.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-82.0, 82.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.723 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.763 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.743\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0082\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19005/50000 [16:56<21:38, 23.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 122279/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 123252/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [17:40<23:26, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 128728/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 129475/2000000\n",
      "P1 SL Buffer Size:  128728\n",
      "P1 SL buffer distribution [42365. 62896. 11645. 11822.]\n",
      "P1 actions distribution [0.32910478 0.48859611 0.09046206 0.09183705]\n",
      "P2 SL Buffer Size:  129475\n",
      "P2 SL buffer distribution [42308. 65024. 11484. 10659.]\n",
      "P2 actions distribution [0.32676578 0.50221278 0.08869666 0.08232477]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7375, 0.2524, 0.0101, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.1272, -0.2523, -0.6671, -0.1129]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1770, 0.0406, 0.7824]])\n",
      "Player 1 Prediction: tensor([[-0.7762, -1.8120, -0.7157, -0.3137]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [17:52<23:26, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50055\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5325/10000 (53.2%)\n",
      "    Average reward: -0.644\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4675/10000 (46.8%)\n",
      "    Average reward: +0.644\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5151 (20.8%)\n",
      "    Action 1: 13968 (56.3%)\n",
      "    Action 2: 2489 (10.0%)\n",
      "    Action 3: 3197 (12.9%)\n",
      "  Player 1:\n",
      "    Action 0: 11570 (45.8%)\n",
      "    Action 1: 8370 (33.1%)\n",
      "    Action 2: 2898 (11.5%)\n",
      "    Action 3: 2412 (9.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6444.5, 6444.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.937 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.991\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6444\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1414, 0.8577, 0.0009, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8443, 0.0119, 0.1437]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49549\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5396/10000 (54.0%)\n",
      "    Average reward: +0.105\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4604/10000 (46.0%)\n",
      "    Average reward: -0.105\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2824 (12.1%)\n",
      "    Action 1: 15570 (66.5%)\n",
      "    Action 2: 1935 (8.3%)\n",
      "    Action 3: 3075 (13.1%)\n",
      "  Player 1:\n",
      "    Action 0: 20611 (78.8%)\n",
      "    Action 1: 5534 (21.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1052.5, -1052.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.759 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.745 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.752\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.1052\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3586, 0.6376, 0.0038, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.5040,  0.6201, -0.9231,  0.3482]])\n",
      "Player 1 Prediction: tensor([[0.1283, 0.8705, 0.0012, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.2870,  0.6398, -1.9966,  0.5485]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6124, 0.0086, 0.3790]])\n",
      "Player 0 Prediction: tensor([[-2.3927, -1.3579, -3.0997, -1.9432]])\n",
      "Player 1 Prediction: tensor([[0.0679, 0.2517, 0.6804, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54373\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5967/10000 (59.7%)\n",
      "    Average reward: +0.605\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4033/10000 (40.3%)\n",
      "    Average reward: -0.605\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11113 (40.5%)\n",
      "    Action 1: 12703 (46.3%)\n",
      "    Action 2: 2954 (10.8%)\n",
      "    Action 3: 686 (2.5%)\n",
      "  Player 1:\n",
      "    Action 0: 8277 (30.8%)\n",
      "    Action 1: 13446 (50.0%)\n",
      "    Action 2: 2457 (9.1%)\n",
      "    Action 3: 2737 (10.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6053.0, -6053.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6053\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6927, 0.2890, 0.0183, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.4807, 0.5130, 0.0064, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0317, 0.0515, 0.9169, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49062\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6172/10000 (61.7%)\n",
      "    Average reward: -0.047\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3828/10000 (38.3%)\n",
      "    Average reward: +0.047\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20250 (77.8%)\n",
      "    Action 1: 5770 (22.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3254 (14.1%)\n",
      "    Action 1: 15198 (66.0%)\n",
      "    Action 2: 1637 (7.1%)\n",
      "    Action 3: 2953 (12.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-469.5, 469.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.763 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.795 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.779\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0469\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21004/50000 [18:58<25:40, 18.82it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 135214/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 135868/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21999/50000 [19:45<21:35, 21.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 141557/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 142420/2000000\n",
      "P1 SL Buffer Size:  141557\n",
      "P1 SL buffer distribution [46912. 68440. 13003. 13202.]\n",
      "P1 actions distribution [0.33140007 0.48348015 0.09185699 0.09326278]\n",
      "P2 SL Buffer Size:  142420\n",
      "P2 SL buffer distribution [46376. 71561. 12579. 11904.]\n",
      "P2 actions distribution [0.32562842 0.50246454 0.08832327 0.08358377]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7638, 0.2272, 0.0090, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.0299,  2.3067, -1.7591,  1.6610]])\n",
      "Player 0 Prediction: tensor([[0.9947, 0.0000, 0.0053, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.1840,  0.5255, -2.4578,  0.1539]])\n",
      "Player 0 Prediction: tensor([[0.0588, 0.0698, 0.8714, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21999/50000 [20:02<21:35, 21.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53772\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5445/10000 (54.4%)\n",
      "    Average reward: -0.389\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4555/10000 (45.6%)\n",
      "    Average reward: +0.389\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8038 (29.6%)\n",
      "    Action 1: 13048 (48.1%)\n",
      "    Action 2: 2471 (9.1%)\n",
      "    Action 3: 3567 (13.2%)\n",
      "  Player 1:\n",
      "    Action 0: 8453 (31.7%)\n",
      "    Action 1: 11364 (42.6%)\n",
      "    Action 2: 2369 (8.9%)\n",
      "    Action 3: 4462 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3894.0, 3894.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3894\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.7530e-01, 8.2392e-01, 7.8359e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8309, 0.0096, 0.1595]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49755\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5460/10000 (54.6%)\n",
      "    Average reward: +0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4540/10000 (45.4%)\n",
      "    Average reward: -0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2925 (12.4%)\n",
      "    Action 1: 15405 (65.5%)\n",
      "    Action 2: 1970 (8.4%)\n",
      "    Action 3: 3217 (13.7%)\n",
      "  Player 1:\n",
      "    Action 0: 20428 (77.9%)\n",
      "    Action 1: 5810 (22.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1805.5, -1805.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.774 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.763 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.768\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.1805\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.5669,  0.8674, -0.5818,  0.5149]])\n",
      "Player 1 Prediction: tensor([[0.1320, 0.8671, 0.0010, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.1906,  0.4563, -1.9565,  0.3182]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7780, 0.0024, 0.2196]])\n",
      "Player 0 Prediction: tensor([[-0.3706, -0.2447, -2.8514, -0.2596]])\n",
      "Player 1 Prediction: tensor([[0.9216, 0.0000, 0.0784, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53263\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5940/10000 (59.4%)\n",
      "    Average reward: +0.683\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4060/10000 (40.6%)\n",
      "    Average reward: -0.683\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8716 (33.0%)\n",
      "    Action 1: 12798 (48.4%)\n",
      "    Action 2: 2871 (10.9%)\n",
      "    Action 3: 2053 (7.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7776 (29.0%)\n",
      "    Action 1: 14033 (52.3%)\n",
      "    Action 2: 2581 (9.6%)\n",
      "    Action 3: 2435 (9.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6826.0, -6826.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6826\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7035, 0.2812, 0.0152, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.4936, 0.5015, 0.0049, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9244, 0.0056, 0.0700]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49281\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6085/10000 (60.9%)\n",
      "    Average reward: -0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3915/10000 (39.1%)\n",
      "    Average reward: +0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20086 (76.9%)\n",
      "    Action 1: 6037 (23.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3584 (15.5%)\n",
      "    Action 1: 15128 (65.3%)\n",
      "    Action 2: 1579 (6.8%)\n",
      "    Action 3: 2867 (12.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1266.5, 1266.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.780 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.818 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.799\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.1267\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23005/50000 [21:05<19:30, 23.06it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 147790/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 149119/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [21:51<34:58, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 154424/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 155594/2000000\n",
      "P1 SL Buffer Size:  154424\n",
      "P1 SL buffer distribution [51164. 74329. 14218. 14713.]\n",
      "P1 actions distribution [0.33132156 0.48133062 0.09207118 0.09527664]\n",
      "P2 SL Buffer Size:  155594\n",
      "P2 SL buffer distribution [50545. 78115. 13683. 13251.]\n",
      "P2 actions distribution [0.32485186 0.50204378 0.08794041 0.08516395]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.6383,  0.2127, -0.5538,  0.1818]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8326, 0.0028, 0.1646]])\n",
      "Player 1 Prediction: tensor([[ 0.0661,  0.8331, -1.1781,  0.0415]])\n",
      "Player 0 Prediction: tensor([[9.9918e-01, 0.0000e+00, 8.1848e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.8543, -2.2886, -2.8445, -2.4386]])\n",
      "Player 0 Prediction: tensor([[0.3823, 0.5297, 0.0879, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-6.7814, -5.8828, -4.9343, -1.7035]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [22:02<34:58, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54732\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5079/10000 (50.8%)\n",
      "    Average reward: -0.118\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4921/10000 (49.2%)\n",
      "    Average reward: +0.118\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9902 (35.9%)\n",
      "    Action 1: 12675 (45.9%)\n",
      "    Action 2: 2853 (10.3%)\n",
      "    Action 3: 2158 (7.8%)\n",
      "  Player 1:\n",
      "    Action 0: 6699 (24.7%)\n",
      "    Action 1: 16297 (60.0%)\n",
      "    Action 2: 2109 (7.8%)\n",
      "    Action 3: 2039 (7.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1178.0, 1178.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.940 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.993\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1178\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3077, 0.0521, 0.6402]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3300, 0.1604, 0.5096]])\n",
      "Player 0 Prediction: tensor([[0.0107, 0.0505, 0.9388, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50166\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5400/10000 (54.0%)\n",
      "    Average reward: +0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4600/10000 (46.0%)\n",
      "    Average reward: -0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3148 (13.2%)\n",
      "    Action 1: 15246 (64.0%)\n",
      "    Action 2: 2054 (8.6%)\n",
      "    Action 3: 3371 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20177 (76.6%)\n",
      "    Action 1: 6170 (23.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1512.5, -1512.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.798 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.785 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.792\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.1512\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.2106, -0.0942, -0.9273,  0.2767]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5769, 0.0096, 0.4135]])\n",
      "Player 0 Prediction: tensor([[ 4.2729,  6.4300, -0.8694,  2.2419]])\n",
      "Player 1 Prediction: tensor([[0.0469, 0.9093, 0.0438, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 4.9823,  6.3476, -3.0481,  5.3858]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53600\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5834/10000 (58.3%)\n",
      "    Average reward: +0.602\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4166/10000 (41.7%)\n",
      "    Average reward: -0.602\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9021 (33.9%)\n",
      "    Action 1: 12681 (47.6%)\n",
      "    Action 2: 2986 (11.2%)\n",
      "    Action 3: 1961 (7.4%)\n",
      "  Player 1:\n",
      "    Action 0: 8290 (30.8%)\n",
      "    Action 1: 13260 (49.2%)\n",
      "    Action 2: 2631 (9.8%)\n",
      "    Action 3: 2770 (10.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6015.0, -6015.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6015\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3097, 0.0516, 0.6387]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6010, 0.1497, 0.2493]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49569\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6178/10000 (61.8%)\n",
      "    Average reward: -0.037\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3822/10000 (38.2%)\n",
      "    Average reward: +0.037\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20311 (77.2%)\n",
      "    Action 1: 6012 (22.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3552 (15.3%)\n",
      "    Action 1: 15279 (65.7%)\n",
      "    Action 2: 1538 (6.6%)\n",
      "    Action 3: 2877 (12.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-366.0, 366.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.775 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.812 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.794\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0366\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25005/50000 [23:17<17:37, 23.63it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 160704/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 161941/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25998/50000 [23:59<16:37, 24.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 167425/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 168180/2000000\n",
      "P1 SL Buffer Size:  167425\n",
      "P1 SL buffer distribution [55464. 80049. 15509. 16403.]\n",
      "P1 actions distribution [0.33127669 0.47811856 0.09263252 0.09797223]\n",
      "P2 SL Buffer Size:  168180\n",
      "P2 SL buffer distribution [54764. 83842. 14649. 14925.]\n",
      "P2 actions distribution [0.3256273  0.49852539 0.0871031  0.0887442 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[1.5136e-01, 8.4861e-01, 2.6657e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.5202,  2.3150, -1.5538,  1.6632]])\n",
      "Player 0 Prediction: tensor([[9.9996e-01, 0.0000e+00, 3.5407e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.8100,  2.3707, -3.3967,  1.6627]])\n",
      "Player 0 Prediction: tensor([[0.1056, 0.8718, 0.0227, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.5231,  1.8571, -4.8395,  1.5330]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25998/50000 [24:12<16:37, 24.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54355\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5100/10000 (51.0%)\n",
      "    Average reward: -0.270\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4900/10000 (49.0%)\n",
      "    Average reward: +0.270\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8867 (32.6%)\n",
      "    Action 1: 12760 (46.9%)\n",
      "    Action 2: 2745 (10.1%)\n",
      "    Action 3: 2838 (10.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6649 (24.5%)\n",
      "    Action 1: 14741 (54.3%)\n",
      "    Action 2: 2280 (8.4%)\n",
      "    Action 3: 3475 (12.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2697.0, 2697.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.975 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.007\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2697\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.7851, 0.2081, 0.0068, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.5959, 0.3980, 0.0061, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9237, 0.0106, 0.0657]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50294\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5387/10000 (53.9%)\n",
      "    Average reward: +0.218\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4613/10000 (46.1%)\n",
      "    Average reward: -0.218\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3050 (12.8%)\n",
      "    Action 1: 15057 (63.0%)\n",
      "    Action 2: 2205 (9.2%)\n",
      "    Action 3: 3589 (15.0%)\n",
      "  Player 1:\n",
      "    Action 0: 20103 (76.2%)\n",
      "    Action 1: 6290 (23.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2182.0, -2182.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.799 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.792 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.796\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: 0.2182\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.7236,  0.7025, -0.5947,  0.7027]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.5082e-01, 4.9367e-04, 1.4868e-01]])\n",
      "Player 0 Prediction: tensor([[ 0.6658,  0.7686, -1.1238,  0.5294]])\n",
      "Player 1 Prediction: tensor([[9.9993e-01, 0.0000e+00, 6.5131e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-1.8047, -0.5541, -2.9934, -1.7170]])\n",
      "Player 1 Prediction: tensor([[0.0962, 0.8826, 0.0213, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-6.4623, -5.5117, -4.8202, -2.2484]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53473\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5816/10000 (58.2%)\n",
      "    Average reward: +0.590\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4184/10000 (41.8%)\n",
      "    Average reward: -0.590\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9635 (35.6%)\n",
      "    Action 1: 10562 (39.0%)\n",
      "    Action 2: 2884 (10.7%)\n",
      "    Action 3: 3994 (14.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7629 (28.9%)\n",
      "    Action 1: 13345 (50.6%)\n",
      "    Action 2: 1879 (7.1%)\n",
      "    Action 3: 3545 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5898.5, -5898.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.015 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5898\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7345, 0.2548, 0.0108, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7211, 0.0735, 0.2054]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49649\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6063/10000 (60.6%)\n",
      "    Average reward: -0.164\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3937/10000 (39.4%)\n",
      "    Average reward: +0.164\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20147 (76.5%)\n",
      "    Action 1: 6192 (23.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3656 (15.7%)\n",
      "    Action 1: 15144 (65.0%)\n",
      "    Action 2: 1583 (6.8%)\n",
      "    Action 3: 2927 (12.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1641.0, 1641.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.787 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.823 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.805\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1641\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27004/50000 [25:22<16:25, 23.34it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 173860/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 174608/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27998/50000 [26:05<15:41, 23.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 180162/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 180887/2000000\n",
      "P1 SL Buffer Size:  180162\n",
      "P1 SL buffer distribution [59742. 85473. 16756. 18191.]\n",
      "P1 actions distribution [0.33160156 0.47442302 0.09300518 0.10097024]\n",
      "P2 SL Buffer Size:  180887\n",
      "P2 SL buffer distribution [58983. 89530. 15726. 16648.]\n",
      "P2 actions distribution [0.3260765  0.49494989 0.08693825 0.09203536]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[1.4246e-01, 8.5753e-01, 1.6467e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 2.1088,  2.8948, -1.7369,  1.7689]])\n",
      "Player 0 Prediction: tensor([[9.9997e-01, 0.0000e+00, 2.7427e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.4513,  0.7679, -2.6667,  0.6227]])\n",
      "Player 0 Prediction: tensor([[0.5302, 0.4639, 0.0059, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27998/50000 [26:22<15:41, 23.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54146\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5642/10000 (56.4%)\n",
      "    Average reward: -0.213\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4358/10000 (43.6%)\n",
      "    Average reward: +0.213\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8113 (30.0%)\n",
      "    Action 1: 13229 (49.0%)\n",
      "    Action 2: 2623 (9.7%)\n",
      "    Action 3: 3053 (11.3%)\n",
      "  Player 1:\n",
      "    Action 0: 9821 (36.2%)\n",
      "    Action 1: 12691 (46.8%)\n",
      "    Action 2: 2382 (8.8%)\n",
      "    Action 3: 2234 (8.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2134.5, 2134.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2135\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.4246e-01, 8.5753e-01, 1.6467e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.4294e-01, 3.2309e-04, 1.5674e-01]])\n",
      "Player 0 Prediction: tensor([[0.0904, 0.8920, 0.0176, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50662\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5436/10000 (54.4%)\n",
      "    Average reward: +0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4564/10000 (45.6%)\n",
      "    Average reward: -0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3089 (12.8%)\n",
      "    Action 1: 15044 (62.6%)\n",
      "    Action 2: 2190 (9.1%)\n",
      "    Action 3: 3723 (15.5%)\n",
      "  Player 1:\n",
      "    Action 0: 20147 (75.7%)\n",
      "    Action 1: 6469 (24.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2875.5, -2875.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.804 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.800 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.802\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2875\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.6510e-01, 7.3481e-01, 9.3600e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.2699, -0.2436, -1.1890,  0.3061]])\n",
      "Player 1 Prediction: tensor([[9.9995e-01, 0.0000e+00, 5.3872e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-3.4189, -3.8705, -2.8183, -2.4608]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.6016e-01, 1.6875e-04, 1.3967e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56085\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5885/10000 (58.9%)\n",
      "    Average reward: +0.516\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4115/10000 (41.1%)\n",
      "    Average reward: -0.516\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7392 (26.3%)\n",
      "    Action 1: 13959 (49.6%)\n",
      "    Action 2: 2624 (9.3%)\n",
      "    Action 3: 4151 (14.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9051 (32.4%)\n",
      "    Action 1: 13667 (48.9%)\n",
      "    Action 2: 2402 (8.6%)\n",
      "    Action 3: 2839 (10.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5165.0, -5165.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.008 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.020\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5165\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5100, 0.4883, 0.0017, 0.0000]])\n",
      "Player 1 Prediction: tensor([[2.3994e-01, 7.5931e-01, 7.5046e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0347, 0.1373, 0.8280, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49777\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6020/10000 (60.2%)\n",
      "    Average reward: -0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3980/10000 (39.8%)\n",
      "    Average reward: +0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19941 (75.6%)\n",
      "    Action 1: 6422 (24.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3727 (15.9%)\n",
      "    Action 1: 14901 (63.6%)\n",
      "    Action 2: 1637 (7.0%)\n",
      "    Action 3: 3149 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1568.0, 1568.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.801 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.837 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.819\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1568\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29003/50000 [27:20<16:39, 21.01it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 186742/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 187339/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [28:11<15:42, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 193110/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 193945/2000000\n",
      "P1 SL Buffer Size:  193110\n",
      "P1 SL buffer distribution [63988. 90981. 17969. 20172.]\n",
      "P1 actions distribution [0.33135519 0.47113562 0.09305059 0.1044586 ]\n",
      "P2 SL Buffer Size:  193945\n",
      "P2 SL buffer distribution [63197. 95410. 16824. 18514.]\n",
      "P2 actions distribution [0.32585011 0.49194359 0.08674624 0.09546005]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.8210,  0.6164, -0.7845,  0.9469]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8418, 0.0023, 0.1559]])\n",
      "Player 1 Prediction: tensor([[ 0.5337, -0.1196, -1.6446,  0.8818]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7623, 0.0040, 0.2337]])\n",
      "Player 1 Prediction: tensor([[-0.7170, -1.2212, -1.9392,  0.5572]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [28:22<15:42, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53463\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5579/10000 (55.8%)\n",
      "    Average reward: -0.282\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4421/10000 (44.2%)\n",
      "    Average reward: +0.282\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7606 (28.4%)\n",
      "    Action 1: 12662 (47.3%)\n",
      "    Action 2: 2300 (8.6%)\n",
      "    Action 3: 4197 (15.7%)\n",
      "  Player 1:\n",
      "    Action 0: 8801 (33.0%)\n",
      "    Action 1: 10600 (39.7%)\n",
      "    Action 2: 2387 (8.9%)\n",
      "    Action 3: 4910 (18.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2815.5, 2815.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2816\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8418, 0.0023, 0.1559]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4701, 0.0613, 0.4686]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50968\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5175/10000 (51.7%)\n",
      "    Average reward: +0.131\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4825/10000 (48.2%)\n",
      "    Average reward: -0.131\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3376 (13.8%)\n",
      "    Action 1: 14697 (60.3%)\n",
      "    Action 2: 2433 (10.0%)\n",
      "    Action 3: 3883 (15.9%)\n",
      "  Player 1:\n",
      "    Action 0: 19677 (74.0%)\n",
      "    Action 1: 6902 (26.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1308.5, -1308.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.835 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.826 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.831\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1308\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.5050,  0.5985, -0.6037,  0.4917]])\n",
      "Player 1 Prediction: tensor([[8.6362e-02, 9.1362e-01, 1.5978e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.3188,  0.4048, -1.9668,  0.6608]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.3076e-01, 7.5820e-05, 6.9163e-02]])\n",
      "Player 0 Prediction: tensor([[ 0.1279, -0.5495, -2.9616,  0.0132]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55706\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5777/10000 (57.8%)\n",
      "    Average reward: +0.435\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4223/10000 (42.2%)\n",
      "    Average reward: -0.435\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7805 (27.5%)\n",
      "    Action 1: 12649 (44.6%)\n",
      "    Action 2: 2609 (9.2%)\n",
      "    Action 3: 5268 (18.6%)\n",
      "  Player 1:\n",
      "    Action 0: 8678 (31.7%)\n",
      "    Action 1: 13828 (50.5%)\n",
      "    Action 2: 1798 (6.6%)\n",
      "    Action 3: 3071 (11.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4351.0, -4351.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.027\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4351\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.7521e-01, 7.2471e-01, 8.1697e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[8.6362e-02, 9.1362e-01, 1.5978e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.2938e-01, 1.5910e-04, 1.7046e-01]])\n",
      "Player 1 Prediction: tensor([[0.6216, 0.3693, 0.0092, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50123\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6128/10000 (61.3%)\n",
      "    Average reward: -0.116\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3872/10000 (38.7%)\n",
      "    Average reward: +0.116\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19868 (75.0%)\n",
      "    Action 1: 6630 (25.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3902 (16.5%)\n",
      "    Action 1: 14863 (62.9%)\n",
      "    Action 2: 1693 (7.2%)\n",
      "    Action 3: 3167 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1157.5, 1157.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.812 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.850 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.831\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1158\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31005/50000 [29:28<14:26, 21.92it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 199903/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 200545/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [30:14<17:17, 17.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 206705/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 207179/2000000\n",
      "P1 SL Buffer Size:  206705\n",
      "P1 SL buffer distribution [68373. 96760. 19194. 22378.]\n",
      "P1 actions distribution [0.33077574 0.46810672 0.09285697 0.10826056]\n",
      "P2 SL Buffer Size:  207179\n",
      "P2 SL buffer distribution [ 67582. 100966.  17972.  20659.]\n",
      "P2 actions distribution [0.32620101 0.48733704 0.08674624 0.0997157 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[3.2873e-01, 6.7097e-01, 2.9666e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.5712,  2.0326, -1.7051,  1.6029]])\n",
      "Player 0 Prediction: tensor([[9.9943e-01, 0.0000e+00, 5.7112e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.6712,  2.1692, -3.5326,  2.0472]])\n",
      "Player 0 Prediction: tensor([[0.0565, 0.1120, 0.8315, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52226\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5893/10000 (58.9%)\n",
      "    Average reward: -0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4107/10000 (41.1%)\n",
      "    Average reward: +0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7192 (27.9%)\n",
      "    Action 1: 12225 (47.4%)\n",
      "    Action 2: 1951 (7.6%)\n",
      "    Action 3: 4417 (17.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8845 (33.5%)\n",
      "    Action 1: 9499 (35.9%)\n",
      "    Action 2: 2426 (9.2%)\n",
      "    Action 3: 5671 (21.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2784.5, 2784.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2784\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8374, 0.1590, 0.0037, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2974, 0.0941, 0.6085]])\n",
      "Player 0 Prediction: tensor([[0.0047, 0.0289, 0.9664, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [30:33<17:17, 17.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51381\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5386/10000 (53.9%)\n",
      "    Average reward: +0.358\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4614/10000 (46.1%)\n",
      "    Average reward: -0.358\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3325 (13.5%)\n",
      "    Action 1: 14808 (60.3%)\n",
      "    Action 2: 2357 (9.6%)\n",
      "    Action 3: 4061 (16.5%)\n",
      "  Player 1:\n",
      "    Action 0: 19788 (73.8%)\n",
      "    Action 1: 7042 (26.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3580.0, -3580.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.831 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.830 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.831\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3580\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.7221e-01, 7.2772e-01, 6.4150e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.1620, -0.1186, -1.2700,  0.2368]])\n",
      "Player 1 Prediction: tensor([[9.9996e-01, 0.0000e+00, 4.0096e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 4.9680,  6.9582, -3.1262,  5.4560]])\n",
      "Player 1 Prediction: tensor([[0.0860, 0.8949, 0.0191, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 7.1914,  9.4695, -5.0032,  4.6693]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55213\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6130/10000 (61.3%)\n",
      "    Average reward: +0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3870/10000 (38.7%)\n",
      "    Average reward: -0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8426 (30.1%)\n",
      "    Action 1: 10647 (38.1%)\n",
      "    Action 2: 1973 (7.1%)\n",
      "    Action 3: 6929 (24.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9082 (33.3%)\n",
      "    Action 1: 11461 (42.1%)\n",
      "    Action 2: 1635 (6.0%)\n",
      "    Action 3: 5060 (18.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3588.0, -3588.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.053\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3588\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3309, 0.0383, 0.6308]])\n",
      "Player 1 Prediction: tensor([[0.0752, 0.6107, 0.3141, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50294\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6086/10000 (60.9%)\n",
      "    Average reward: -0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3914/10000 (39.1%)\n",
      "    Average reward: +0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19679 (74.3%)\n",
      "    Action 1: 6804 (25.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3989 (16.8%)\n",
      "    Action 1: 14703 (61.7%)\n",
      "    Action 2: 1838 (7.7%)\n",
      "    Action 3: 3281 (13.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1368.0, 1368.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.822 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.861 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.842\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1368\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33005/50000 [31:28<12:13, 23.17it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 213093/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 213567/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [32:12<11:57, 22.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 219604/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 219956/2000000\n",
      "P1 SL Buffer Size:  219604\n",
      "P1 SL buffer distribution [ 72347. 102185.  20288.  24784.]\n",
      "P1 actions distribution [0.329443   0.46531484 0.09238447 0.11285769]\n",
      "P2 SL Buffer Size:  219956\n",
      "P2 SL buffer distribution [ 71768. 106102.  19036.  23050.]\n",
      "P2 actions distribution [0.32628344 0.48237829 0.08654458 0.10479369]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [32:23<11:57, 22.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.7570,  0.5497, -0.4326,  1.0137]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.4541e-01, 4.9429e-05, 5.4538e-02]])\n",
      "Player 1 Prediction: tensor([[ 0.3891, -0.1232, -1.6774,  0.8046]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6672, 0.0012, 0.3316]])\n",
      "Player 1 Prediction: tensor([[-3.7592, -3.0438, -1.9867, -1.4030]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52568\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5735/10000 (57.4%)\n",
      "    Average reward: -0.356\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4265/10000 (42.6%)\n",
      "    Average reward: +0.356\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7310 (28.1%)\n",
      "    Action 1: 12288 (47.2%)\n",
      "    Action 2: 1947 (7.5%)\n",
      "    Action 3: 4502 (17.3%)\n",
      "  Player 1:\n",
      "    Action 0: 8855 (33.4%)\n",
      "    Action 1: 9528 (35.9%)\n",
      "    Action 2: 2189 (8.3%)\n",
      "    Action 3: 5949 (22.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3564.0, 3564.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3564\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.4541e-01, 4.9429e-05, 5.4538e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 6.9978e-01, 2.1558e-04, 3.0001e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51456\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5280/10000 (52.8%)\n",
      "    Average reward: +0.251\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4720/10000 (47.2%)\n",
      "    Average reward: -0.251\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3462 (14.1%)\n",
      "    Action 1: 14688 (59.8%)\n",
      "    Action 2: 2350 (9.6%)\n",
      "    Action 3: 4081 (16.6%)\n",
      "  Player 1:\n",
      "    Action 0: 19693 (73.3%)\n",
      "    Action 1: 7182 (26.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2513.5, -2513.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.842 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.837 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.840\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2514\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7926, 0.2005, 0.0069, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.9080,  0.6938, -0.7542,  0.6824]])\n",
      "Player 1 Prediction: tensor([[0.3558, 0.6417, 0.0025, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.5168,  2.1005, -2.0998,  3.8441]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5557, 0.0758, 0.3685]])\n",
      "Player 0 Prediction: tensor([[ 4.1895,  4.4625, -2.0924,  3.6691]])\n",
      "Player 1 Prediction: tensor([[0.1068, 0.0000, 0.8932, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54642\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6089/10000 (60.9%)\n",
      "    Average reward: +0.449\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3911/10000 (39.1%)\n",
      "    Average reward: -0.449\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10053 (36.7%)\n",
      "    Action 1: 9417 (34.3%)\n",
      "    Action 2: 2366 (8.6%)\n",
      "    Action 3: 5582 (20.4%)\n",
      "  Player 1:\n",
      "    Action 0: 7953 (29.2%)\n",
      "    Action 1: 12445 (45.7%)\n",
      "    Action 2: 1780 (6.5%)\n",
      "    Action 3: 5046 (18.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4488.0, -4488.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.048\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4488\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.6858e-01, 2.4340e-04, 1.3118e-01]])\n",
      "Player 1 Prediction: tensor([[0.1273, 0.8714, 0.0013, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50517\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6050/10000 (60.5%)\n",
      "    Average reward: -0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3950/10000 (39.5%)\n",
      "    Average reward: +0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19447 (73.3%)\n",
      "    Action 1: 7082 (26.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4220 (17.6%)\n",
      "    Action 1: 14429 (60.2%)\n",
      "    Action 2: 1945 (8.1%)\n",
      "    Action 3: 3394 (14.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1901.0, 1901.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.837 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.882 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.860\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1901\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35003/50000 [33:29<10:58, 22.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 226070/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 226395/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [34:15<10:06, 23.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 232734/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 233216/2000000\n",
      "P1 SL Buffer Size:  232734\n",
      "P1 SL buffer distribution [ 76765. 107532.  21452.  26985.]\n",
      "P1 actions distribution [0.32984007 0.46203821 0.0921739  0.11594782]\n",
      "P2 SL Buffer Size:  233216\n",
      "P2 SL buffer distribution [ 76164. 111881.  20190.  24981.]\n",
      "P2 actions distribution [0.32658137 0.47973124 0.0865721  0.10711529]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8492, 0.1484, 0.0024, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.6797,  1.8261, -1.1415,  1.4476]])\n",
      "Player 0 Prediction: tensor([[0.5878, 0.4092, 0.0030, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.1823,  0.2133, -1.7178,  0.6029]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8972, 0.0233, 0.0795]])\n",
      "Player 1 Prediction: tensor([[ 0.1669,  0.1043, -1.5379,  0.9971]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55690\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5616/10000 (56.2%)\n",
      "    Average reward: -0.193\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4384/10000 (43.8%)\n",
      "    Average reward: +0.193\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8033 (28.7%)\n",
      "    Action 1: 13350 (47.8%)\n",
      "    Action 2: 2402 (8.6%)\n",
      "    Action 3: 4172 (14.9%)\n",
      "  Player 1:\n",
      "    Action 0: 9508 (34.3%)\n",
      "    Action 1: 11792 (42.5%)\n",
      "    Action 2: 2512 (9.1%)\n",
      "    Action 3: 3921 (14.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1927.5, 1927.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.040\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1928\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.2651e-01, 8.7348e-01, 4.1456e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6625, 0.0010, 0.3366]])\n",
      "Player 0 Prediction: tensor([[0.0289, 0.9594, 0.0117, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [34:34<10:06, 23.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51756\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5381/10000 (53.8%)\n",
      "    Average reward: +0.358\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4619/10000 (46.2%)\n",
      "    Average reward: -0.358\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3459 (14.0%)\n",
      "    Action 1: 14688 (59.4%)\n",
      "    Action 2: 2329 (9.4%)\n",
      "    Action 3: 4256 (17.2%)\n",
      "  Player 1:\n",
      "    Action 0: 19700 (72.9%)\n",
      "    Action 1: 7324 (27.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3577.5, -3577.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.843 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.843 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.843\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3578\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.9144,  2.4183, -1.0337,  1.5236]])\n",
      "Player 1 Prediction: tensor([[3.8232e-01, 6.1706e-01, 6.1346e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.4173,  2.1788, -2.0031,  2.5444]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6666, 0.0081, 0.3253]])\n",
      "Player 0 Prediction: tensor([[ 0.4553,  0.1073, -2.3965,  0.4820]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53297\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5865/10000 (58.7%)\n",
      "    Average reward: +0.477\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4135/10000 (41.3%)\n",
      "    Average reward: -0.477\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9996 (36.9%)\n",
      "    Action 1: 9712 (35.8%)\n",
      "    Action 2: 2479 (9.1%)\n",
      "    Action 3: 4937 (18.2%)\n",
      "  Player 1:\n",
      "    Action 0: 7450 (28.5%)\n",
      "    Action 1: 12872 (49.2%)\n",
      "    Action 2: 1631 (6.2%)\n",
      "    Action 3: 4220 (16.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4774.5, -4774.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.040\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4774\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4889, 0.0057, 0.5055]])\n",
      "Player 1 Prediction: tensor([[0.1778, 0.8090, 0.0132, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50491\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6042/10000 (60.4%)\n",
      "    Average reward: -0.196\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3958/10000 (39.6%)\n",
      "    Average reward: +0.196\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19454 (73.5%)\n",
      "    Action 1: 7028 (26.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4127 (17.2%)\n",
      "    Action 1: 14418 (60.1%)\n",
      "    Action 2: 1982 (8.3%)\n",
      "    Action 3: 3482 (14.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1956.0, 1956.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.835 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.878 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.857\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1956\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37006/50000 [35:26<07:54, 27.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 239267/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 239864/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37999/50000 [36:06<08:18, 24.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 245587/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 246335/2000000\n",
      "P1 SL Buffer Size:  245587\n",
      "P1 SL buffer distribution [ 81456. 112407.  22637.  29087.]\n",
      "P1 actions distribution [0.33167879 0.45770745 0.09217507 0.11843868]\n",
      "P2 SL Buffer Size:  246335\n",
      "P2 SL buffer distribution [ 80248. 118208.  21221.  26658.]\n",
      "P2 actions distribution [0.32576776 0.47986685 0.08614691 0.10821848]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[4.2426e-01, 5.7555e-01, 1.8438e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.6156,  1.2380, -1.2690,  0.1581]])\n",
      "Player 0 Prediction: tensor([[9.9959e-01, 0.0000e+00, 4.0652e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.1330, -3.1056, -2.8688, -2.5070]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4792, 0.0012, 0.5196]])\n",
      "Player 1 Prediction: tensor([[-4.1085, -5.8555, -3.0087, -2.3234]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54887\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5712/10000 (57.1%)\n",
      "    Average reward: -0.178\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4288/10000 (42.9%)\n",
      "    Average reward: +0.178\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8139 (29.9%)\n",
      "    Action 1: 12318 (45.2%)\n",
      "    Action 2: 1888 (6.9%)\n",
      "    Action 3: 4910 (18.0%)\n",
      "  Player 1:\n",
      "    Action 0: 10568 (38.2%)\n",
      "    Action 1: 10079 (36.5%)\n",
      "    Action 2: 2129 (7.7%)\n",
      "    Action 3: 4856 (17.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1782.5, 1782.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1782\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.5252e-01, 3.0381e-05, 4.7453e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.0328e-01, 1.0467e-04, 2.9661e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37999/50000 [36:24<08:18, 24.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52192\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5348/10000 (53.5%)\n",
      "    Average reward: +0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4652/10000 (46.5%)\n",
      "    Average reward: -0.373\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3728 (14.9%)\n",
      "    Action 1: 14419 (57.7%)\n",
      "    Action 2: 2431 (9.7%)\n",
      "    Action 3: 4417 (17.7%)\n",
      "  Player 1:\n",
      "    Action 0: 19487 (71.7%)\n",
      "    Action 1: 7710 (28.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3726.5, -3726.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.867 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.860 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.864\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3726\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.0470,  2.1882, -0.9906,  1.5971]])\n",
      "Player 1 Prediction: tensor([[4.0461e-01, 5.9482e-01, 5.6551e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.8025,  1.0621, -1.9607,  1.6691]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5922, 0.0396, 0.3682]])\n",
      "Player 0 Prediction: tensor([[ 0.5160,  0.8835, -2.0021,  1.4498]])\n",
      "Player 1 Prediction: tensor([[0.1156, 0.0000, 0.8844, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51270\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6046/10000 (60.5%)\n",
      "    Average reward: +0.564\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3954/10000 (39.5%)\n",
      "    Average reward: -0.564\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 12154 (47.2%)\n",
      "    Action 1: 7664 (29.8%)\n",
      "    Action 2: 2018 (7.8%)\n",
      "    Action 3: 3891 (15.1%)\n",
      "  Player 1:\n",
      "    Action 0: 5780 (22.6%)\n",
      "    Action 1: 13738 (53.8%)\n",
      "    Action 2: 1888 (7.4%)\n",
      "    Action 3: 4137 (16.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5635.0, -5635.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.966 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.999\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5635\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5169, 0.0053, 0.4778]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8559, 0.0018, 0.1423]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50415\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5983/10000 (59.8%)\n",
      "    Average reward: -0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4017/10000 (40.2%)\n",
      "    Average reward: +0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19845 (74.8%)\n",
      "    Action 1: 6681 (25.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3782 (15.8%)\n",
      "    Action 1: 14769 (61.8%)\n",
      "    Action 2: 1952 (8.2%)\n",
      "    Action 3: 3386 (14.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2864.0, 2864.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.814 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.850 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.832\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2864\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39003/50000 [37:16<07:16, 25.19it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 251952/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 252854/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39999/50000 [37:59<07:13, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 258330/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 259570/2000000\n",
      "P1 SL Buffer Size:  258330\n",
      "P1 SL buffer distribution [ 86089. 117303.  23781.  31157.]\n",
      "P1 actions distribution [0.33325204 0.45408199 0.09205667 0.1206093 ]\n",
      "P2 SL Buffer Size:  259570\n",
      "P2 SL buffer distribution [ 84686. 124096.  22422.  28366.]\n",
      "P2 actions distribution [0.32625496 0.47808298 0.08638132 0.10928073]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.0065,  0.8147, -0.7819,  0.9083]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1864, 0.0269, 0.7867]])\n",
      "Player 1 Prediction: tensor([[ 0.7505,  0.6176, -1.4258,  0.9527]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0226, 0.0221, 0.9553]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39999/50000 [38:14<07:13, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52040\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5718/10000 (57.2%)\n",
      "    Average reward: -0.274\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4282/10000 (42.8%)\n",
      "    Average reward: +0.274\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5119 (19.9%)\n",
      "    Action 1: 13623 (53.1%)\n",
      "    Action 2: 2152 (8.4%)\n",
      "    Action 3: 4772 (18.6%)\n",
      "  Player 1:\n",
      "    Action 0: 11980 (45.4%)\n",
      "    Action 1: 8414 (31.9%)\n",
      "    Action 2: 2927 (11.1%)\n",
      "    Action 3: 3053 (11.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2743.0, 2743.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.949 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.996\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2743\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8695, 0.1287, 0.0018, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.6327, 0.3648, 0.0025, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0042, 0.0433, 0.9525, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52607\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5283/10000 (52.8%)\n",
      "    Average reward: +0.379\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4717/10000 (47.2%)\n",
      "    Average reward: -0.379\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3821 (15.1%)\n",
      "    Action 1: 14350 (56.8%)\n",
      "    Action 2: 2509 (9.9%)\n",
      "    Action 3: 4583 (18.1%)\n",
      "  Player 1:\n",
      "    Action 0: 19380 (70.9%)\n",
      "    Action 1: 7964 (29.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3791.5, -3791.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.876 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.870 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.873\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3791\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8186, 0.1769, 0.0045, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.9154,  2.2337, -1.2350,  0.9511]])\n",
      "Player 1 Prediction: tensor([[0.9852, 0.0000, 0.0148, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.7749,  1.9333, -3.6767,  1.6278]])\n",
      "Player 1 Prediction: tensor([[0.1124, 0.7836, 0.1040, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52674\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5788/10000 (57.9%)\n",
      "    Average reward: +0.472\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4212/10000 (42.1%)\n",
      "    Average reward: -0.472\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10487 (38.7%)\n",
      "    Action 1: 9762 (36.0%)\n",
      "    Action 2: 2675 (9.9%)\n",
      "    Action 3: 4192 (15.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7149 (28.0%)\n",
      "    Action 1: 13062 (51.1%)\n",
      "    Action 2: 1620 (6.3%)\n",
      "    Action 3: 3727 (14.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4718.5, -4718.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.009 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.035\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4718\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.9829e-01, 1.1564e-04, 1.0159e-01]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.0120e-01, 8.4830e-05, 9.8711e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50570\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5999/10000 (60.0%)\n",
      "    Average reward: -0.248\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4001/10000 (40.0%)\n",
      "    Average reward: +0.248\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19680 (74.2%)\n",
      "    Action 1: 6839 (25.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4005 (16.7%)\n",
      "    Action 1: 14660 (61.0%)\n",
      "    Action 2: 2011 (8.4%)\n",
      "    Action 3: 3375 (14.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2483.0, 2483.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.824 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.866 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.845\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2483\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}, {'exploitability': 0.373075}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41003/50000 [39:11<06:20, 23.66it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 264776/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 266030/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [39:53<05:38, 23.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 271364/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 272497/2000000\n",
      "P1 SL Buffer Size:  271364\n",
      "P1 SL buffer distribution [ 90712. 122302.  25004.  33346.]\n",
      "P1 actions distribution [0.33428163 0.45069353 0.09214192 0.12288292]\n",
      "P2 SL Buffer Size:  272497\n",
      "P2 SL buffer distribution [ 88679. 130017.  23574.  30227.]\n",
      "P2 actions distribution [0.32543111 0.47713186 0.08651104 0.11092599]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.8739,  1.0308, -0.5833,  1.1111]])\n",
      "Player 0 Prediction: tensor([[2.3193e-02, 9.7680e-01, 6.4893e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.0800,  0.5301, -2.1968,  0.8492]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 5.4190e-01, 4.6764e-05, 4.5806e-01]])\n",
      "Player 1 Prediction: tensor([[ 4.0430,  5.4667, -2.7535,  4.0284]])\n",
      "Player 0 Prediction: tensor([[0.7147, 0.2835, 0.0018, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [40:04<05:38, 23.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53144\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5891/10000 (58.9%)\n",
      "    Average reward: -0.172\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4109/10000 (41.1%)\n",
      "    Average reward: +0.172\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7593 (28.8%)\n",
      "    Action 1: 12804 (48.6%)\n",
      "    Action 2: 2166 (8.2%)\n",
      "    Action 3: 3768 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 8083 (30.1%)\n",
      "    Action 1: 11470 (42.8%)\n",
      "    Action 2: 2773 (10.3%)\n",
      "    Action 3: 4487 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1716.0, 1716.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1716\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[4.4388e-01, 5.5598e-01, 1.3608e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[6.7284e-02, 9.3239e-01, 3.3088e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8238, 0.0229, 0.1533]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52550\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5290/10000 (52.9%)\n",
      "    Average reward: +0.444\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4710/10000 (47.1%)\n",
      "    Average reward: -0.444\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3636 (14.4%)\n",
      "    Action 1: 14348 (57.0%)\n",
      "    Action 2: 2532 (10.1%)\n",
      "    Action 3: 4660 (18.5%)\n",
      "  Player 1:\n",
      "    Action 0: 19450 (71.1%)\n",
      "    Action 1: 7924 (28.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4440.5, -4440.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.865 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.868 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.867\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4441\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.3559,  2.6545, -0.9077,  1.5854]])\n",
      "Player 1 Prediction: tensor([[4.4075e-01, 5.5877e-01, 4.7808e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.4137,  2.0232, -2.3758,  2.4462]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.3282e-01, 1.9493e-04, 6.6981e-02]])\n",
      "Player 0 Prediction: tensor([[ 0.1208, -0.4868, -2.6425,  0.0303]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53508\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5814/10000 (58.1%)\n",
      "    Average reward: +0.390\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4186/10000 (41.9%)\n",
      "    Average reward: -0.390\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9980 (36.9%)\n",
      "    Action 1: 11683 (43.2%)\n",
      "    Action 2: 2804 (10.4%)\n",
      "    Action 3: 2598 (9.6%)\n",
      "  Player 1:\n",
      "    Action 0: 7662 (29.0%)\n",
      "    Action 1: 13011 (49.2%)\n",
      "    Action 2: 2436 (9.2%)\n",
      "    Action 3: 3334 (12.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3897.5, -3897.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3897\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5266, 0.4728, 0.0006, 0.0000]])\n",
      "Player 1 Prediction: tensor([[4.4075e-01, 5.5877e-01, 4.7808e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6907, 0.0075, 0.3018]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50489\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6054/10000 (60.5%)\n",
      "    Average reward: -0.208\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3946/10000 (39.5%)\n",
      "    Average reward: +0.208\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19724 (74.6%)\n",
      "    Action 1: 6731 (25.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3993 (16.6%)\n",
      "    Action 1: 14786 (61.5%)\n",
      "    Action 2: 2004 (8.3%)\n",
      "    Action 3: 3251 (13.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2085.0, 2085.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.818 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.861 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.840\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2085\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}, {'exploitability': 0.373075}, {'exploitability': 0.280675}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43004/50000 [41:06<04:47, 24.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 277956/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 278977/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43998/50000 [41:51<04:27, 22.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 284136/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 285554/2000000\n",
      "P1 SL Buffer Size:  284136\n",
      "P1 SL buffer distribution [ 95092. 127254.  26247.  35543.]\n",
      "P1 actions distribution [0.33467072 0.447863   0.09237478 0.12509151]\n",
      "P2 SL Buffer Size:  285554\n",
      "P2 SL buffer distribution [ 92762. 135992.  24721.  32079.]\n",
      "P2 actions distribution [0.32484924 0.47623917 0.08657207 0.11233952]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.8136,  2.2219, -0.9083,  1.5354]])\n",
      "Player 0 Prediction: tensor([[2.2399e-02, 9.7759e-01, 6.0416e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 2.3179,  2.7596, -2.4163,  2.3236]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 5.0868e-01, 4.0941e-05, 4.9128e-01]])\n",
      "Player 1 Prediction: tensor([[-1.3896, -3.3907, -2.6126,  0.4007]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43998/50000 [42:04<04:27, 22.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54367\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5702/10000 (57.0%)\n",
      "    Average reward: -0.226\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4298/10000 (43.0%)\n",
      "    Average reward: +0.226\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8370 (30.9%)\n",
      "    Action 1: 12802 (47.2%)\n",
      "    Action 2: 2615 (9.6%)\n",
      "    Action 3: 3343 (12.3%)\n",
      "  Player 1:\n",
      "    Action 0: 8189 (30.1%)\n",
      "    Action 1: 12975 (47.6%)\n",
      "    Action 2: 2286 (8.4%)\n",
      "    Action 3: 3787 (13.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2257.5, 2257.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2258\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.0822e-01, 8.9178e-01, 1.7622e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 6.3219e-01, 6.7607e-05, 3.6774e-01]])\n",
      "Player 0 Prediction: tensor([[0.0986, 0.8931, 0.0083, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52805\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5274/10000 (52.7%)\n",
      "    Average reward: +0.375\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4726/10000 (47.3%)\n",
      "    Average reward: -0.375\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3835 (15.1%)\n",
      "    Action 1: 14363 (56.6%)\n",
      "    Action 2: 2512 (9.9%)\n",
      "    Action 3: 4648 (18.3%)\n",
      "  Player 1:\n",
      "    Action 0: 19366 (70.6%)\n",
      "    Action 1: 8081 (29.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3749.5, -3749.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.877 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.874 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.876\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3750\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5389, 0.4606, 0.0006, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.7913,  2.0581, -0.9904,  1.1340]])\n",
      "Player 1 Prediction: tensor([[4.7933e-01, 5.2021e-01, 4.5829e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.5645,  0.9420, -1.7912,  1.7856]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5766, 0.0333, 0.3901]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51643\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5867/10000 (58.7%)\n",
      "    Average reward: +0.383\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4133/10000 (41.3%)\n",
      "    Average reward: -0.383\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9680 (36.4%)\n",
      "    Action 1: 8537 (32.1%)\n",
      "    Action 2: 2489 (9.3%)\n",
      "    Action 3: 5922 (22.2%)\n",
      "  Player 1:\n",
      "    Action 0: 6986 (27.9%)\n",
      "    Action 1: 12218 (48.8%)\n",
      "    Action 2: 1374 (5.5%)\n",
      "    Action 3: 4437 (17.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3827.0, -3827.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3827\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5389, 0.4606, 0.0006, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8534, 0.0011, 0.1455]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50864\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6060/10000 (60.6%)\n",
      "    Average reward: -0.233\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3940/10000 (39.4%)\n",
      "    Average reward: +0.233\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19563 (73.6%)\n",
      "    Action 1: 7002 (26.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4083 (16.8%)\n",
      "    Action 1: 14586 (60.0%)\n",
      "    Action 2: 2123 (8.7%)\n",
      "    Action 3: 3507 (14.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2334.0, 2334.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.832 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.874 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.853\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2334\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}, {'exploitability': 0.373075}, {'exploitability': 0.280675}, {'exploitability': 0.30422499999999997}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45003/50000 [43:05<03:41, 22.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 290792/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 291871/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [43:50<03:02, 21.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 297427/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 298021/2000000\n",
      "P1 SL Buffer Size:  297427\n",
      "P1 SL buffer distribution [ 99836. 132363.  27571.  37657.]\n",
      "P1 actions distribution [0.33566556 0.44502685 0.09269838 0.12660922]\n",
      "P2 SL Buffer Size:  298021\n",
      "P2 SL buffer distribution [ 96728. 141644.  25772.  33877.]\n",
      "P2 actions distribution [0.32456773 0.47528194 0.08647713 0.1136732 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8826, 0.1161, 0.0013, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.9712,  2.1587, -1.3055,  1.4806]])\n",
      "Player 0 Prediction: tensor([[0.6379, 0.3602, 0.0019, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.4835,  2.4098, -2.4032,  3.0783]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4500, 0.0537, 0.4962]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [44:05<03:02, 21.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53576\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5653/10000 (56.5%)\n",
      "    Average reward: -0.270\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4347/10000 (43.5%)\n",
      "    Average reward: +0.270\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6336 (23.9%)\n",
      "    Action 1: 13922 (52.5%)\n",
      "    Action 2: 2478 (9.3%)\n",
      "    Action 3: 3768 (14.2%)\n",
      "  Player 1:\n",
      "    Action 0: 10921 (40.3%)\n",
      "    Action 1: 11165 (41.2%)\n",
      "    Action 2: 2302 (8.5%)\n",
      "    Action 3: 2684 (9.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2702.0, 2702.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.981 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.018\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2702\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[1.0338e-01, 8.9662e-01, 1.5641e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 6.0855e-01, 6.0430e-05, 3.9139e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52919\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5227/10000 (52.3%)\n",
      "    Average reward: +0.415\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4773/10000 (47.7%)\n",
      "    Average reward: -0.415\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3904 (15.3%)\n",
      "    Action 1: 14179 (55.7%)\n",
      "    Action 2: 2613 (10.3%)\n",
      "    Action 3: 4781 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 19186 (69.9%)\n",
      "    Action 1: 8256 (30.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4146.5, -4146.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.882 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.884\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4147\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8317, 0.1645, 0.0038, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.9244,  2.2987, -0.8810,  1.2320]])\n",
      "Player 1 Prediction: tensor([[0.2582, 0.7406, 0.0011, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.6640,  1.0243, -1.9467,  1.0078]])\n",
      "Player 1 Prediction: tensor([[0.0106, 0.0302, 0.9592, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53551\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5819/10000 (58.2%)\n",
      "    Average reward: +0.309\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4181/10000 (41.8%)\n",
      "    Average reward: -0.309\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10061 (36.9%)\n",
      "    Action 1: 11211 (41.2%)\n",
      "    Action 2: 2620 (9.6%)\n",
      "    Action 3: 3344 (12.3%)\n",
      "  Player 1:\n",
      "    Action 0: 7646 (29.1%)\n",
      "    Action 1: 13101 (49.8%)\n",
      "    Action 2: 2120 (8.1%)\n",
      "    Action 3: 3448 (13.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3094.5, -3094.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3095\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4583, 0.0193, 0.5224]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5648, 0.0207, 0.4144]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51146\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6077/10000 (60.8%)\n",
      "    Average reward: -0.200\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3923/10000 (39.2%)\n",
      "    Average reward: +0.200\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19711 (73.9%)\n",
      "    Action 1: 6970 (26.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4030 (16.5%)\n",
      "    Action 1: 14763 (60.3%)\n",
      "    Action 2: 2137 (8.7%)\n",
      "    Action 3: 3535 (14.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1995.5, 1995.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.829 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.868 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.848\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1996\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}, {'exploitability': 0.373075}, {'exploitability': 0.280675}, {'exploitability': 0.30422499999999997}, {'exploitability': 0.289825}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47005/50000 [45:09<02:10, 22.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 303897/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 304433/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47998/50000 [45:54<01:29, 22.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 310303/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 310844/2000000\n",
      "P1 SL Buffer Size:  310303\n",
      "P1 SL buffer distribution [104489. 137295.  28793.  39726.]\n",
      "P1 actions distribution [0.33673216 0.44245463 0.09278995 0.12802325]\n",
      "P2 SL Buffer Size:  310844\n",
      "P2 SL buffer distribution [100891. 147275.  26986.  35692.]\n",
      "P2 actions distribution [0.32457117 0.47379071 0.08681525 0.11482287]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.6247,  2.2591, -0.3747,  1.4098]])\n",
      "Player 0 Prediction: tensor([[6.4352e-02, 9.3535e-01, 2.9336e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 2.2301,  2.5705, -2.6263,  2.1838]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.6812e-01, 7.2262e-04, 1.3115e-01]])\n",
      "Player 1 Prediction: tensor([[-2.0832, -2.0580, -2.8285,  0.2926]])\n",
      "Player 0 Prediction: tensor([[0.9710, 0.0000, 0.0290, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47998/50000 [46:05<01:29, 22.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55502\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5722/10000 (57.2%)\n",
      "    Average reward: -0.147\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4278/10000 (42.8%)\n",
      "    Average reward: +0.147\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9312 (33.0%)\n",
      "    Action 1: 12771 (45.3%)\n",
      "    Action 2: 2838 (10.1%)\n",
      "    Action 3: 3261 (11.6%)\n",
      "  Player 1:\n",
      "    Action 0: 7480 (27.4%)\n",
      "    Action 1: 14306 (52.4%)\n",
      "    Action 2: 2294 (8.4%)\n",
      "    Action 3: 3240 (11.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1472.5, 1472.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.000 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.023\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1472\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1392, 0.0175, 0.8433]])\n",
      "Player 0 Prediction: tensor([[0.0396, 0.2237, 0.7366, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53044\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5196/10000 (52.0%)\n",
      "    Average reward: +0.375\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4804/10000 (48.0%)\n",
      "    Average reward: -0.375\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4177 (16.4%)\n",
      "    Action 1: 14039 (55.0%)\n",
      "    Action 2: 2558 (10.0%)\n",
      "    Action 3: 4755 (18.6%)\n",
      "  Player 1:\n",
      "    Action 0: 19092 (69.4%)\n",
      "    Action 1: 8423 (30.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3748.0, -3748.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.902 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.889 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.895\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3748\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8413, 0.1550, 0.0037, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.1087, -0.9240, -0.9378,  0.1189]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0594, 0.0208, 0.9198]])\n",
      "Player 0 Prediction: tensor([[-1.1089, -1.9845, -1.2991, -0.6165]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53299\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5851/10000 (58.5%)\n",
      "    Average reward: +0.391\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4149/10000 (41.5%)\n",
      "    Average reward: -0.391\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10162 (37.3%)\n",
      "    Action 1: 10297 (37.8%)\n",
      "    Action 2: 2633 (9.7%)\n",
      "    Action 3: 4182 (15.3%)\n",
      "  Player 1:\n",
      "    Action 0: 7377 (28.3%)\n",
      "    Action 1: 12973 (49.8%)\n",
      "    Action 2: 1891 (7.3%)\n",
      "    Action 3: 3784 (14.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3911.0, -3911.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3911\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[2.3300e-01, 7.6698e-01, 2.2591e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.5604e-01, 5.4509e-05, 1.4390e-01]])\n",
      "Player 1 Prediction: tensor([[0.5878, 0.3985, 0.0137, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51245\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5933/10000 (59.3%)\n",
      "    Average reward: -0.331\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4067/10000 (40.7%)\n",
      "    Average reward: +0.331\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19709 (73.6%)\n",
      "    Action 1: 7058 (26.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4081 (16.7%)\n",
      "    Action 1: 14620 (59.7%)\n",
      "    Action 2: 2160 (8.8%)\n",
      "    Action 3: 3617 (14.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3310.5, 3310.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.832 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.875 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.854\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3311\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}, {'exploitability': 0.373075}, {'exploitability': 0.280675}, {'exploitability': 0.30422499999999997}, {'exploitability': 0.289825}, {'exploitability': 0.269175}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49003/50000 [47:08<00:43, 22.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 316828/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 317374/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [47:54<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[8.5366e-02, 9.1463e-01, 1.2075e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.0980,  0.9251, -1.1830,  0.2591]])\n",
      "Player 0 Prediction: tensor([[1.0000e+00, 0.0000e+00, 4.3140e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 4.7427,  5.7934, -3.0773,  4.8057]])\n",
      "Player 0 Prediction: tensor([[0.3256, 0.6660, 0.0084, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 7.0234,  8.7366, -4.9536,  7.8181]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54005\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5305/10000 (53.0%)\n",
      "    Average reward: -0.252\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4695/10000 (46.9%)\n",
      "    Average reward: +0.252\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8204 (30.2%)\n",
      "    Action 1: 13036 (47.9%)\n",
      "    Action 2: 3037 (11.2%)\n",
      "    Action 3: 2913 (10.7%)\n",
      "  Player 1:\n",
      "    Action 0: 7022 (26.2%)\n",
      "    Action 1: 14488 (54.0%)\n",
      "    Action 2: 2063 (7.7%)\n",
      "    Action 3: 3242 (12.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2523.0, 2523.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.986 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.008\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2523\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8211, 0.0011, 0.1777]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6958, 0.0010, 0.3032]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52837\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5295/10000 (52.9%)\n",
      "    Average reward: +0.443\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4705/10000 (47.0%)\n",
      "    Average reward: -0.443\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4003 (15.7%)\n",
      "    Action 1: 14131 (55.5%)\n",
      "    Action 2: 2552 (10.0%)\n",
      "    Action 3: 4785 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 19089 (69.8%)\n",
      "    Action 1: 8277 (30.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4429.5, -4429.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.891 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.884 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.888\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.4430\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.0141,  2.0696, -0.9743,  1.3358]])\n",
      "Player 1 Prediction: tensor([[5.2555e-01, 4.7403e-01, 4.1444e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.5936,  1.5370, -2.4351,  2.2989]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6438, 0.0055, 0.3507]])\n",
      "Player 0 Prediction: tensor([[ 0.8012,  2.1444, -4.2042,  0.8041]])\n",
      "Player 1 Prediction: tensor([[0.0473, 0.1168, 0.8359, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51248\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5912/10000 (59.1%)\n",
      "    Average reward: +0.256\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4088/10000 (40.9%)\n",
      "    Average reward: -0.256\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9355 (35.6%)\n",
      "    Action 1: 9575 (36.4%)\n",
      "    Action 2: 2510 (9.5%)\n",
      "    Action 3: 4855 (18.5%)\n",
      "  Player 1:\n",
      "    Action 0: 7228 (29.0%)\n",
      "    Action 1: 11903 (47.7%)\n",
      "    Action 2: 1672 (6.7%)\n",
      "    Action 3: 4150 (16.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2556.0, -2556.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2556\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.2836e-01, 5.5907e-05, 7.1585e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 7.3979e-01, 4.1732e-04, 2.5979e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51343\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6002/10000 (60.0%)\n",
      "    Average reward: -0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3998/10000 (40.0%)\n",
      "    Average reward: +0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19710 (73.7%)\n",
      "    Action 1: 7033 (26.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3966 (16.1%)\n",
      "    Action 1: 14635 (59.5%)\n",
      "    Action 2: 2294 (9.3%)\n",
      "    Action 3: 3705 (15.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2883.5, 2883.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.831 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.870 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.851\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2883\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.988775}, {'exploitability': 1.073275}, {'exploitability': 0.9801}, {'exploitability': 0.7918000000000001}, {'exploitability': 0.87235}, {'exploitability': 0.887025}, {'exploitability': 0.6987}, {'exploitability': 0.5196000000000001}, {'exploitability': 0.5558}, {'exploitability': 0.624875}, {'exploitability': 0.536}, {'exploitability': 0.35965}, {'exploitability': 0.429775}, {'exploitability': 0.364975}, {'exploitability': 0.358325}, {'exploitability': 0.318625}, {'exploitability': 0.40259999999999996}, {'exploitability': 0.3351}, {'exploitability': 0.370875}, {'exploitability': 0.373075}, {'exploitability': 0.280675}, {'exploitability': 0.30422499999999997}, {'exploitability': 0.289825}, {'exploitability': 0.269175}, {'exploitability': 0.25395}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d08d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x358adec20>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x358adec20>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 3\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x358adec20>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 3\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x392ab3a30>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x392ab3a30>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c058ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-NStep\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e9e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/50000 [00:00<16:54, 49.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 65/200000\n",
      "   Player 0 SL buffer: 4/2000000\n",
      "   Player 1 RL buffer: 59/200000\n",
      "   Player 1 SL buffer: 1/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1004/50000 [00:28<25:34, 31.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0019 â†’ 0.0019\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 63098/200000\n",
      "   Player 0 SL buffer: 6688/2000000\n",
      "   Player 1 RL buffer: 65024/200000\n",
      "   Player 1 SL buffer: 6901/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:04<31:01, 25.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0013 â†’ 0.0013\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 125693/200000\n",
      "   Player 0 SL buffer: 12855/2000000\n",
      "   Player 1 RL buffer: 130429/200000\n",
      "   Player 1 SL buffer: 13131/2000000\n",
      "P1 SL Buffer Size:  12855\n",
      "P1 SL buffer distribution [4030. 4714.  843. 3268.]\n",
      "P1 actions distribution [0.31349669 0.36670556 0.0655776  0.25422015]\n",
      "P2 SL Buffer Size:  13131\n",
      "P2 SL buffer distribution [4348. 3808. 1258. 3717.]\n",
      "P2 actions distribution [0.33112482 0.29000076 0.09580382 0.2830706 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.0981,  1.1902, -0.5269,  0.8025]])\n",
      "Player 0 Prediction: tensor([[0.1098, 0.8221, 0.0681, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.8187,  2.3780, -2.0367,  1.9532]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5051, 0.0894, 0.4055]])\n",
      "Player 1 Prediction: tensor([[ 1.1754,  2.6243, -2.9960,  1.7406]])\n",
      "Player 0 Prediction: tensor([[0.8088, 0.0000, 0.1912, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:15<31:01, 25.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48717\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5071/10000 (50.7%)\n",
      "    Average reward: -1.204\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4929/10000 (49.3%)\n",
      "    Average reward: +1.204\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7388 (30.2%)\n",
      "    Action 1: 10083 (41.2%)\n",
      "    Action 2: 1818 (7.4%)\n",
      "    Action 3: 5161 (21.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8809 (36.3%)\n",
      "    Action 1: 7357 (30.3%)\n",
      "    Action 2: 2342 (9.7%)\n",
      "    Action 3: 5759 (23.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-12040.0, 12040.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.051\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.2040\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3352, 0.0770, 0.5878]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6292, 0.0961, 0.2748]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53461\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5137/10000 (51.4%)\n",
      "    Average reward: -0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4863/10000 (48.6%)\n",
      "    Average reward: +0.190\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5999 (23.4%)\n",
      "    Action 1: 13649 (53.3%)\n",
      "    Action 2: 1592 (6.2%)\n",
      "    Action 3: 4366 (17.1%)\n",
      "  Player 1:\n",
      "    Action 0: 18622 (66.9%)\n",
      "    Action 1: 9233 (33.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1896.0, 1896.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.974 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.916 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.945\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1896\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.4974,  0.0643, -0.4651,  0.1287]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2556, 0.0898, 0.6545]])\n",
      "Player 0 Prediction: tensor([[ 0.0818,  0.4285, -0.9849,  0.0810]])\n",
      "Player 1 Prediction: tensor([[0.8350, 0.0000, 0.1650, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.4881,  1.7437, -2.8493,  0.8327]])\n",
      "Player 1 Prediction: tensor([[0.3871, 0.4970, 0.1159, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.6011,  3.1536, -5.2568,  1.6845]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48527\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6816/10000 (68.2%)\n",
      "    Average reward: +1.171\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3184/10000 (31.8%)\n",
      "    Average reward: -1.171\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7219 (30.7%)\n",
      "    Action 1: 11505 (49.0%)\n",
      "    Action 2: 1017 (4.3%)\n",
      "    Action 3: 3754 (16.0%)\n",
      "  Player 1:\n",
      "    Action 0: 9498 (37.9%)\n",
      "    Action 1: 8374 (33.5%)\n",
      "    Action 2: 2822 (11.3%)\n",
      "    Action 3: 4338 (17.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [11712.5, -11712.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.043\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.1712\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3151, 0.0838, 0.6011]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6092, 0.1067, 0.2841]])\n",
      "Player 1 Prediction: tensor([[0.5316, 0.3771, 0.0913, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47626\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6916/10000 (69.2%)\n",
      "    Average reward: +0.309\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3084/10000 (30.8%)\n",
      "    Average reward: -0.309\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14640 (61.1%)\n",
      "    Action 1: 9321 (38.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6964 (29.4%)\n",
      "    Action 1: 9736 (41.1%)\n",
      "    Action 2: 2463 (10.4%)\n",
      "    Action 3: 4502 (19.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3089.0, -3089.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.964 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.005\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3089\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3006/50000 [02:15<28:11, 27.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0011 â†’ 0.0011\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 188396/200000\n",
      "   Player 0 SL buffer: 18991/2000000\n",
      "   Player 1 RL buffer: 195726/200000\n",
      "   Player 1 SL buffer: 19770/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3999/50000 [02:53<27:51, 27.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0009 â†’ 0.0009\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 25011/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 26015/2000000\n",
      "P1 SL Buffer Size:  25011\n",
      "P1 SL buffer distribution [7851. 9379. 2153. 5628.]\n",
      "P1 actions distribution [0.31390188 0.374995   0.08608212 0.22502099]\n",
      "P2 SL Buffer Size:  26015\n",
      "P2 SL buffer distribution [8646. 7890. 3256. 6223.]\n",
      "P2 actions distribution [0.33234672 0.30328657 0.12515856 0.23920815]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.0612, -0.1784, -0.6011,  0.0203]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7450, 0.0496, 0.2054]])\n",
      "Player 1 Prediction: tensor([[ 2.2879,  2.6934, -1.0574,  1.4925]])\n",
      "Player 0 Prediction: tensor([[0.1034, 0.8327, 0.0639, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 5.4927,  5.0451, -2.9821,  3.9391]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3999/50000 [03:05<27:51, 27.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40420\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5592/10000 (55.9%)\n",
      "    Average reward: -0.916\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4408/10000 (44.1%)\n",
      "    Average reward: +0.916\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5725 (29.4%)\n",
      "    Action 1: 8327 (42.8%)\n",
      "    Action 2: 1953 (10.0%)\n",
      "    Action 3: 3461 (17.8%)\n",
      "  Player 1:\n",
      "    Action 0: 7504 (35.8%)\n",
      "    Action 1: 6181 (29.5%)\n",
      "    Action 2: 3205 (15.3%)\n",
      "    Action 3: 4064 (19.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-9163.5, 9163.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.047\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.9163\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.6839, 0.2893, 0.0268, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0679, 0.9070, 0.0251, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7209, 0.0631, 0.2160]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51043\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4605/10000 (46.1%)\n",
      "    Average reward: -0.202\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5395/10000 (53.9%)\n",
      "    Average reward: +0.202\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5619 (22.5%)\n",
      "    Action 1: 12526 (50.3%)\n",
      "    Action 2: 2689 (10.8%)\n",
      "    Action 3: 4089 (16.4%)\n",
      "  Player 1:\n",
      "    Action 0: 17587 (67.3%)\n",
      "    Action 1: 8533 (32.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2023.5, 2023.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.983 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.912 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.947\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2024\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5726, 0.3791, 0.0483, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.2385, -0.0173, -0.8906, -0.1379]])\n",
      "Player 1 Prediction: tensor([[0.9325, 0.0000, 0.0675, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 4.7867,  5.4334, -2.9447,  3.9239]])\n",
      "Player 1 Prediction: tensor([[0.2946, 0.6403, 0.0651, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40396\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6014/10000 (60.1%)\n",
      "    Average reward: +0.873\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3986/10000 (39.9%)\n",
      "    Average reward: -0.873\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4756 (24.0%)\n",
      "    Action 1: 8645 (43.7%)\n",
      "    Action 2: 2842 (14.4%)\n",
      "    Action 3: 3547 (17.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7644 (37.1%)\n",
      "    Action 1: 6975 (33.8%)\n",
      "    Action 2: 3147 (15.3%)\n",
      "    Action 3: 2840 (13.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [8731.0, -8731.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.8731\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5189, 0.2196, 0.2615, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45126\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7029/10000 (70.3%)\n",
      "    Average reward: +0.204\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2971/10000 (29.7%)\n",
      "    Average reward: -0.204\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14165 (62.8%)\n",
      "    Action 1: 8387 (37.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6262 (27.7%)\n",
      "    Action 1: 9219 (40.8%)\n",
      "    Action 2: 3211 (14.2%)\n",
      "    Action 3: 3882 (17.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2036.5, -2036.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.952 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.996\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2036\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5004/50000 [03:54<27:14, 27.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 31032/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 32227/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6000/50000 [04:31<28:02, 26.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 36818/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 38554/2000000\n",
      "P1 SL Buffer Size:  36818\n",
      "P1 SL buffer distribution [11417. 13630.  3966.  7805.]\n",
      "P1 actions distribution [0.31009289 0.37019936 0.10771905 0.2119887 ]\n",
      "P2 SL Buffer Size:  38554\n",
      "P2 SL buffer distribution [12755. 11914.  5272.  8613.]\n",
      "P2 actions distribution [0.33083467 0.30902111 0.13674327 0.22340094]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8945, 0.0845, 0.0210, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.1911,  0.0530, -0.9429,  0.2719]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5529, 0.0923, 0.3548]])\n",
      "Player 1 Prediction: tensor([[ 0.0521,  0.4460, -0.9701, -0.4915]])\n",
      "Player 0 Prediction: tensor([[0.9451, 0.0000, 0.0549, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 38866\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5520/10000 (55.2%)\n",
      "    Average reward: -0.718\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4480/10000 (44.8%)\n",
      "    Average reward: +0.718\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6030 (31.2%)\n",
      "    Action 1: 7339 (38.0%)\n",
      "    Action 2: 2558 (13.3%)\n",
      "    Action 3: 3370 (17.5%)\n",
      "  Player 1:\n",
      "    Action 0: 6342 (32.4%)\n",
      "    Action 1: 6592 (33.7%)\n",
      "    Action 2: 2981 (15.2%)\n",
      "    Action 3: 3654 (18.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7182.5, 7182.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7183\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8945, 0.0845, 0.0210, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2883, 0.6665, 0.0452, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5547, 0.1065, 0.3388]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6000/50000 [04:45<28:02, 26.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48724\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4353/10000 (43.5%)\n",
      "    Average reward: -0.148\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5647/10000 (56.5%)\n",
      "    Average reward: +0.148\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5656 (23.3%)\n",
      "    Action 1: 11145 (45.9%)\n",
      "    Action 2: 3444 (14.2%)\n",
      "    Action 3: 4013 (16.5%)\n",
      "  Player 1:\n",
      "    Action 0: 16101 (65.8%)\n",
      "    Action 1: 8365 (34.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1478.5, 1478.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.005 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.927 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.966\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1479\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3387, 0.1311, 0.5302, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-1.1744, -1.0559, -1.0196, -0.6342]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1245, 0.2945, 0.5810]])\n",
      "Player 0 Prediction: tensor([[-2.1126, -1.7351, -0.9775, -0.9675]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 35549\n",
      "Average episode length: 3.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5785/10000 (57.9%)\n",
      "    Average reward: +0.581\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4215/10000 (42.1%)\n",
      "    Average reward: -0.581\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4607 (26.2%)\n",
      "    Action 1: 6615 (37.6%)\n",
      "    Action 2: 3140 (17.8%)\n",
      "    Action 3: 3235 (18.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6218 (34.6%)\n",
      "    Action 1: 5966 (33.2%)\n",
      "    Action 2: 3042 (16.9%)\n",
      "    Action 3: 2726 (15.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5812.5, -5812.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.047\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5813\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2564, 0.0697, 0.6739]])\n",
      "Player 1 Prediction: tensor([[0.2999, 0.3267, 0.3734, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43257\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7033/10000 (70.3%)\n",
      "    Average reward: +0.016\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2967/10000 (29.7%)\n",
      "    Average reward: -0.016\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13791 (64.4%)\n",
      "    Action 1: 7617 (35.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5374 (24.6%)\n",
      "    Action 1: 8746 (40.0%)\n",
      "    Action 2: 3839 (17.6%)\n",
      "    Action 3: 3890 (17.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [158.0, -158.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.939 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.983\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0158\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7004/50000 [05:35<27:09, 26.39it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 42481/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 44575/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [06:14<26:42, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 48078/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 50862/2000000\n",
      "P1 SL Buffer Size:  48078\n",
      "P1 SL buffer distribution [14794. 17074.  6203. 10007.]\n",
      "P1 actions distribution [0.30770831 0.35513125 0.12901951 0.20814094]\n",
      "P2 SL Buffer Size:  50862\n",
      "P2 SL buffer distribution [16409. 16304.  7392. 10757.]\n",
      "P2 actions distribution [0.32261806 0.32055365 0.14533443 0.21149385]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[-1.0335, -1.4886, -0.5027, -0.5863]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [06:25<26:42, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 36619\n",
      "Average episode length: 3.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5635/10000 (56.4%)\n",
      "    Average reward: -0.556\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4365/10000 (43.6%)\n",
      "    Average reward: +0.556\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6473 (35.8%)\n",
      "    Action 1: 6573 (36.3%)\n",
      "    Action 2: 2735 (15.1%)\n",
      "    Action 3: 2305 (12.7%)\n",
      "  Player 1:\n",
      "    Action 0: 3628 (19.6%)\n",
      "    Action 1: 8163 (44.0%)\n",
      "    Action 2: 3413 (18.4%)\n",
      "    Action 3: 3329 (18.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5563.0, 5563.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.982 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5563\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9037, 0.0740, 0.0223, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.5187, 0.4426, 0.0387, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.3812, 0.5014, 0.1174, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47152\n",
      "Average episode length: 4.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4332/10000 (43.3%)\n",
      "    Average reward: +0.063\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5668/10000 (56.7%)\n",
      "    Average reward: -0.063\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5247 (22.3%)\n",
      "    Action 1: 10253 (43.6%)\n",
      "    Action 2: 3787 (16.1%)\n",
      "    Action 3: 4254 (18.1%)\n",
      "  Player 1:\n",
      "    Action 0: 15286 (64.7%)\n",
      "    Action 1: 8325 (35.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [633.5, -633.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.005 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.936 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.971\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0634\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2404, 0.0879, 0.6718, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 32825\n",
      "Average episode length: 3.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5754/10000 (57.5%)\n",
      "    Average reward: +0.521\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4246/10000 (42.5%)\n",
      "    Average reward: -0.521\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5250 (31.7%)\n",
      "    Action 1: 5010 (30.3%)\n",
      "    Action 2: 3229 (19.5%)\n",
      "    Action 3: 3071 (18.5%)\n",
      "  Player 1:\n",
      "    Action 0: 4554 (28.0%)\n",
      "    Action 1: 5740 (35.3%)\n",
      "    Action 2: 3262 (20.1%)\n",
      "    Action 3: 2709 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5210.5, -5210.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.046\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5211\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7904, 0.0062, 0.2034]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8750, 0.0144, 0.1106]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42450\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7016/10000 (70.2%)\n",
      "    Average reward: -0.114\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2984/10000 (29.8%)\n",
      "    Average reward: +0.114\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13347 (64.0%)\n",
      "    Action 1: 7507 (36.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4986 (23.1%)\n",
      "    Action 1: 8448 (39.1%)\n",
      "    Action 2: 4221 (19.5%)\n",
      "    Action 3: 3941 (18.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1137.5, 1137.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.943 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.018 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.980\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1138\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9004/50000 [07:17<26:58, 25.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 53717/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 57288/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9999/50000 [07:57<26:16, 25.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 59412/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 63575/2000000\n",
      "P1 SL Buffer Size:  59412\n",
      "P1 SL buffer distribution [17999. 20495.  8915. 12003.]\n",
      "P1 actions distribution [0.30295227 0.34496398 0.15005386 0.20202989]\n",
      "P2 SL Buffer Size:  63575\n",
      "P2 SL buffer distribution [19599. 21161.  9838. 12977.]\n",
      "P2 actions distribution [0.30828156 0.33285096 0.15474636 0.20412112]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9108, 0.0643, 0.0249, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.8228, -1.4471, -1.0146, -0.7227]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4443, 0.0564, 0.4993]])\n",
      "Player 1 Prediction: tensor([[-0.9886, -2.2841, -0.9839, -0.7153]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 34904\n",
      "Average episode length: 3.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5498/10000 (55.0%)\n",
      "    Average reward: -0.528\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4502/10000 (45.0%)\n",
      "    Average reward: +0.528\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5152 (30.0%)\n",
      "    Action 1: 6040 (35.2%)\n",
      "    Action 2: 3102 (18.1%)\n",
      "    Action 3: 2862 (16.7%)\n",
      "  Player 1:\n",
      "    Action 0: 5380 (30.3%)\n",
      "    Action 1: 6100 (34.4%)\n",
      "    Action 2: 3360 (18.9%)\n",
      "    Action 3: 2908 (16.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5278.0, 5278.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.051\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5278\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3842, 0.0650, 0.5508, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45452\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4080/10000 (40.8%)\n",
      "    Average reward: +0.106\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5920/10000 (59.2%)\n",
      "    Average reward: -0.106\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5012 (21.8%)\n",
      "    Action 1: 9327 (40.6%)\n",
      "    Action 2: 4314 (18.8%)\n",
      "    Action 3: 4300 (18.7%)\n",
      "  Player 1:\n",
      "    Action 0: 14251 (63.3%)\n",
      "    Action 1: 8248 (36.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1061.5, -1061.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.948 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.978\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1061\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.3825, -0.2812, -0.5408, -0.0389]])\n",
      "Player 1 Prediction: tensor([[0.2341, 0.1535, 0.6124, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.2821, -0.4866, -1.9206, -0.2965]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1439, 0.4917, 0.3644]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9999/50000 [08:15<26:16, 25.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29497\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5149/10000 (51.5%)\n",
      "    Average reward: +0.174\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4851/10000 (48.5%)\n",
      "    Average reward: -0.174\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2045 (13.6%)\n",
      "    Action 1: 4872 (32.5%)\n",
      "    Action 2: 4161 (27.7%)\n",
      "    Action 3: 3923 (26.2%)\n",
      "  Player 1:\n",
      "    Action 0: 4127 (28.5%)\n",
      "    Action 1: 5056 (34.9%)\n",
      "    Action 2: 3057 (21.1%)\n",
      "    Action 3: 2256 (15.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1745.0, -1745.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.919 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.982\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1745\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1669, 0.8281, 0.0050, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.3574, 0.6360, 0.0066, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.2137, 0.7656, 0.0207, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42441\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7048/10000 (70.5%)\n",
      "    Average reward: -0.161\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2952/10000 (29.5%)\n",
      "    Average reward: +0.161\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13850 (66.0%)\n",
      "    Action 1: 7137 (34.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3932 (18.3%)\n",
      "    Action 1: 8799 (41.0%)\n",
      "    Action 2: 4426 (20.6%)\n",
      "    Action 3: 4297 (20.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1613.0, 1613.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.925 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.976 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.950\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1613\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11005/50000 [09:01<25:33, 25.43it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 65021/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 69789/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11998/50000 [09:40<24:51, 25.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 70625/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 75994/2000000\n",
      "P1 SL Buffer Size:  70625\n",
      "P1 SL buffer distribution [20484. 24397. 11841. 13903.]\n",
      "P1 actions distribution [0.29003894 0.34544425 0.16766018 0.19685664]\n",
      "P2 SL Buffer Size:  75994\n",
      "P2 SL buffer distribution [22456. 26061. 12532. 14945.]\n",
      "P2 actions distribution [0.29549701 0.34293497 0.16490776 0.19666026]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8714, 0.0945, 0.0341, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.0890,  1.8367, -0.9485,  1.0866]])\n",
      "Player 0 Prediction: tensor([[0.5953, 0.3167, 0.0880, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.4186,  2.5770, -1.8409,  1.6362]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1509, 0.0870, 0.7621]])\n",
      "Player 1 Prediction: tensor([[ 1.1527,  2.4093, -2.9552,  1.1080]])\n",
      "Player 0 Prediction: tensor([[0.4243, 0.4948, 0.0809, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.6854,  2.7025, -4.7789,  1.4549]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 32817\n",
      "Average episode length: 3.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5567/10000 (55.7%)\n",
      "    Average reward: -0.318\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4433/10000 (44.3%)\n",
      "    Average reward: +0.318\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5593 (34.4%)\n",
      "    Action 1: 5694 (35.0%)\n",
      "    Action 2: 3404 (20.9%)\n",
      "    Action 3: 1579 (9.7%)\n",
      "  Player 1:\n",
      "    Action 0: 2939 (17.8%)\n",
      "    Action 1: 7859 (47.5%)\n",
      "    Action 2: 3844 (23.2%)\n",
      "    Action 3: 1905 (11.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3179.0, 3179.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.953 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.006\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3179\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4003, 0.0424, 0.5573]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4880, 0.1621, 0.3499]])\n",
      "Player 0 Prediction: tensor([[0.2519, 0.4834, 0.2648, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11998/50000 [09:55<24:51, 25.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44677\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3973/10000 (39.7%)\n",
      "    Average reward: +0.140\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6027/10000 (60.3%)\n",
      "    Average reward: -0.140\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4376 (19.4%)\n",
      "    Action 1: 9095 (40.3%)\n",
      "    Action 2: 4600 (20.4%)\n",
      "    Action 3: 4502 (19.9%)\n",
      "  Player 1:\n",
      "    Action 0: 14166 (64.1%)\n",
      "    Action 1: 7938 (35.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1398.5, -1398.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.987 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.942 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.965\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1399\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.9208,  1.2664, -0.5393,  0.9641]])\n",
      "Player 1 Prediction: tensor([[0.7580, 0.1752, 0.0668, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.2487,  1.9100, -1.9353,  1.5639]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5321, 0.0633, 0.4047]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29165\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5445/10000 (54.4%)\n",
      "    Average reward: +0.320\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4555/10000 (45.6%)\n",
      "    Average reward: -0.320\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2127 (14.3%)\n",
      "    Action 1: 6139 (41.4%)\n",
      "    Action 2: 3911 (26.4%)\n",
      "    Action 3: 2661 (17.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4469 (31.2%)\n",
      "    Action 1: 4983 (34.8%)\n",
      "    Action 2: 3433 (24.0%)\n",
      "    Action 3: 1442 (10.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3199.0, -3199.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.928 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.991\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3199\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7187, 0.2378, 0.0435, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1417, 0.1695, 0.6888]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42558\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7086/10000 (70.9%)\n",
      "    Average reward: -0.180\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2914/10000 (29.1%)\n",
      "    Average reward: +0.180\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13720 (65.5%)\n",
      "    Action 1: 7211 (34.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3741 (17.3%)\n",
      "    Action 1: 8705 (40.3%)\n",
      "    Action 2: 4727 (21.9%)\n",
      "    Action 3: 4454 (20.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1804.0, 1804.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.929 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.966 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.948\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1804\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13003/50000 [10:45<24:59, 24.68it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 76383/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 82011/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [11:25<23:11, 25.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 82072/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 88079/2000000\n",
      "P1 SL Buffer Size:  82072\n",
      "P1 SL buffer distribution [22602. 28856. 14998. 15616.]\n",
      "P1 actions distribution [0.27539234 0.35159372 0.18274198 0.19027196]\n",
      "P2 SL Buffer Size:  88079\n",
      "P2 SL buffer distribution [25346. 30379. 15580. 16774.]\n",
      "P2 actions distribution [0.28776439 0.34490628 0.17688666 0.19044267]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[-0.0360, -0.3641, -0.4552,  0.0345]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3770, 0.0374, 0.5857]])\n",
      "Player 1 Prediction: tensor([[-0.6918, -1.7028, -0.9971, -0.7395]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4852, 0.1590, 0.3557]])\n",
      "Player 1 Prediction: tensor([[-3.4351, -5.1402, -1.9966, -3.2314]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [11:35<23:11, 25.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 33277\n",
      "Average episode length: 3.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5592/10000 (55.9%)\n",
      "    Average reward: -0.326\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4408/10000 (44.1%)\n",
      "    Average reward: +0.326\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3139 (19.2%)\n",
      "    Action 1: 5988 (36.6%)\n",
      "    Action 2: 3058 (18.7%)\n",
      "    Action 3: 4159 (25.4%)\n",
      "  Player 1:\n",
      "    Action 0: 6856 (40.5%)\n",
      "    Action 1: 2853 (16.8%)\n",
      "    Action 2: 3571 (21.1%)\n",
      "    Action 3: 3653 (21.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3256.5, 3256.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.988 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.961 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.974\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3256\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3770, 0.0374, 0.5857]])\n",
      "Player 0 Prediction: tensor([[0.1003, 0.1870, 0.7126, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43690\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3844/10000 (38.4%)\n",
      "    Average reward: +0.152\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6156/10000 (61.6%)\n",
      "    Average reward: -0.152\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4101 (18.4%)\n",
      "    Action 1: 8828 (39.7%)\n",
      "    Action 2: 4803 (21.6%)\n",
      "    Action 3: 4512 (20.3%)\n",
      "  Player 1:\n",
      "    Action 0: 13816 (64.4%)\n",
      "    Action 1: 7630 (35.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1519.0, -1519.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.979 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.939 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.959\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1519\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6906, 0.2664, 0.0430, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.8683,  1.0010, -1.0395,  1.0006]])\n",
      "Player 1 Prediction: tensor([[0.7095, 0.1571, 0.1333, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.6307,  1.9692, -1.9722,  1.8419]])\n",
      "Player 1 Prediction: tensor([[0.0604, 0.1405, 0.7991, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29056\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4889/10000 (48.9%)\n",
      "    Average reward: +0.134\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5111/10000 (51.1%)\n",
      "    Average reward: -0.134\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5150 (34.5%)\n",
      "    Action 1: 2867 (19.2%)\n",
      "    Action 2: 4637 (31.1%)\n",
      "    Action 3: 2267 (15.2%)\n",
      "  Player 1:\n",
      "    Action 0: 2491 (17.6%)\n",
      "    Action 1: 5075 (35.9%)\n",
      "    Action 2: 3240 (22.9%)\n",
      "    Action 3: 3329 (23.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1339.5, -1339.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.987 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.972 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.979\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1340\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1301, 0.0573, 0.8126, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42187\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7096/10000 (71.0%)\n",
      "    Average reward: -0.216\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2904/10000 (29.0%)\n",
      "    Average reward: +0.216\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13665 (66.2%)\n",
      "    Action 1: 6987 (33.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3472 (16.1%)\n",
      "    Action 1: 8724 (40.5%)\n",
      "    Action 2: 4893 (22.7%)\n",
      "    Action 3: 4446 (20.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2165.0, 2165.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.953 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.938\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2165\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15005/50000 [12:28<23:01, 25.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 87827/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 94325/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15998/50000 [13:08<25:55, 21.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 93672/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 100576/2000000\n",
      "P1 SL Buffer Size:  93672\n",
      "P1 SL buffer distribution [24758. 33304. 18374. 17236.]\n",
      "P1 actions distribution [0.26430524 0.35553847 0.19615253 0.18400376]\n",
      "P2 SL Buffer Size:  100576\n",
      "P2 SL buffer distribution [28359. 34924. 18821. 18472.]\n",
      "P2 actions distribution [0.28196588 0.3472399  0.18713212 0.18366211]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7388, 0.2159, 0.0452, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.2942,  1.5134, -0.8833,  1.0787]])\n",
      "Player 0 Prediction: tensor([[0.4574, 0.2712, 0.2714, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.6084,  1.8770, -1.8545,  1.6164]])\n",
      "Player 0 Prediction: tensor([[0.0465, 0.1577, 0.7958, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28596\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5553/10000 (55.5%)\n",
      "    Average reward: -0.252\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4447/10000 (44.5%)\n",
      "    Average reward: +0.252\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3748 (26.6%)\n",
      "    Action 1: 4796 (34.0%)\n",
      "    Action 2: 3756 (26.6%)\n",
      "    Action 3: 1799 (12.8%)\n",
      "  Player 1:\n",
      "    Action 0: 3225 (22.2%)\n",
      "    Action 1: 5363 (37.0%)\n",
      "    Action 2: 4283 (29.5%)\n",
      "    Action 3: 1626 (11.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2517.5, 2517.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.013 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.025\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2517\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.7388, 0.2159, 0.0452, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.4574, 0.2712, 0.2714, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15998/50000 [13:26<25:55, 21.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42894\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3874/10000 (38.7%)\n",
      "    Average reward: +0.229\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6126/10000 (61.3%)\n",
      "    Average reward: -0.229\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3499 (16.0%)\n",
      "    Action 1: 8818 (40.3%)\n",
      "    Action 2: 4919 (22.5%)\n",
      "    Action 3: 4622 (21.1%)\n",
      "  Player 1:\n",
      "    Action 0: 13802 (65.6%)\n",
      "    Action 1: 7234 (34.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2289.5, -2289.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.928 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.940\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2289\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1309, 0.8676, 0.0015, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-1.6156, -2.1654, -1.0217, -1.1805]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27919\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5246/10000 (52.5%)\n",
      "    Average reward: +0.163\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4754/10000 (47.5%)\n",
      "    Average reward: -0.163\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2885 (20.4%)\n",
      "    Action 1: 5683 (40.2%)\n",
      "    Action 2: 4414 (31.2%)\n",
      "    Action 3: 1172 (8.3%)\n",
      "  Player 1:\n",
      "    Action 0: 3704 (26.9%)\n",
      "    Action 1: 4637 (33.7%)\n",
      "    Action 2: 3862 (28.1%)\n",
      "    Action 3: 1562 (11.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1630.0, -1630.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.996 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.017\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1630\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6743, 0.2867, 0.0390, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6562, 0.1272, 0.2166, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1771, 0.1220, 0.7009]])\n",
      "Player 1 Prediction: tensor([[0.4863, 0.2536, 0.2601, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42012\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7227/10000 (72.3%)\n",
      "    Average reward: -0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2773/10000 (27.7%)\n",
      "    Average reward: +0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13584 (66.1%)\n",
      "    Action 1: 6952 (33.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3209 (14.9%)\n",
      "    Action 1: 8637 (40.2%)\n",
      "    Action 2: 5077 (23.6%)\n",
      "    Action 3: 4553 (21.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1570.5, 1570.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.938 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.931\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1570\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17003/50000 [14:16<22:42, 24.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 99744/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 106684/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17998/50000 [14:58<22:27, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 105820/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 112726/2000000\n",
      "P1 SL Buffer Size:  105820\n",
      "P1 SL buffer distribution [27788. 37351. 21964. 18717.]\n",
      "P1 actions distribution [0.26259686 0.3529673  0.20756001 0.17687583]\n",
      "P2 SL Buffer Size:  112726\n",
      "P2 SL buffer distribution [31105. 39342. 22060. 20219.]\n",
      "P2 actions distribution [0.27593457 0.34900555 0.19569576 0.17936412]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.2277, 0.7695, 0.0028, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-1.0355, -2.7943, -0.9955, -1.0524]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28834\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5519/10000 (55.2%)\n",
      "    Average reward: -0.194\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4481/10000 (44.8%)\n",
      "    Average reward: +0.194\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3810 (26.7%)\n",
      "    Action 1: 4843 (33.9%)\n",
      "    Action 2: 3727 (26.1%)\n",
      "    Action 3: 1899 (13.3%)\n",
      "  Player 1:\n",
      "    Action 0: 3307 (22.7%)\n",
      "    Action 1: 5418 (37.2%)\n",
      "    Action 2: 4169 (28.6%)\n",
      "    Action 3: 1661 (11.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1941.5, 1941.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.027\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1941\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.7602, 0.2004, 0.0394, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.3898, 0.2197, 0.3906, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1545, 0.0963, 0.7492]])\n",
      "Player 0 Prediction: tensor([[0.3669, 0.4217, 0.2114, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17998/50000 [15:16<22:27, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42217\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3719/10000 (37.2%)\n",
      "    Average reward: +0.234\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6281/10000 (62.8%)\n",
      "    Average reward: -0.234\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3591 (16.6%)\n",
      "    Action 1: 8262 (38.1%)\n",
      "    Action 2: 5221 (24.1%)\n",
      "    Action 3: 4606 (21.2%)\n",
      "  Player 1:\n",
      "    Action 0: 13275 (64.6%)\n",
      "    Action 1: 7262 (35.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2335.5, -2335.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.960 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.937 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.949\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2336\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6949, 0.2729, 0.0322, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.8648, -1.1153, -0.9655, -0.6500]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7068, 0.0103, 0.2829]])\n",
      "Player 0 Prediction: tensor([[-3.2433, -4.8610, -1.0110, -2.6180]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30116\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5089/10000 (50.9%)\n",
      "    Average reward: +0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4911/10000 (49.1%)\n",
      "    Average reward: -0.137\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4129 (27.1%)\n",
      "    Action 1: 4520 (29.7%)\n",
      "    Action 2: 4511 (29.6%)\n",
      "    Action 3: 2067 (13.6%)\n",
      "  Player 1:\n",
      "    Action 0: 3610 (24.2%)\n",
      "    Action 1: 5030 (33.8%)\n",
      "    Action 2: 3414 (22.9%)\n",
      "    Action 3: 2835 (19.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1368.5, -1368.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1368\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.1622e-01, 4.3983e-04, 8.3338e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3728, 0.0053, 0.6219]])\n",
      "Player 1 Prediction: tensor([[0.3098, 0.6413, 0.0490, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42283\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7121/10000 (71.2%)\n",
      "    Average reward: -0.210\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2879/10000 (28.8%)\n",
      "    Average reward: +0.210\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13692 (66.2%)\n",
      "    Action 1: 6986 (33.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3272 (15.1%)\n",
      "    Action 1: 8710 (40.3%)\n",
      "    Action 2: 5028 (23.3%)\n",
      "    Action 3: 4595 (21.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2101.5, 2101.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.941 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.932\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2102\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19003/50000 [16:06<20:33, 25.14it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 111711/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 118948/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19999/50000 [16:48<21:16, 23.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 117520/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 125154/2000000\n",
      "P1 SL Buffer Size:  117520\n",
      "P1 SL buffer distribution [30445. 41645. 25489. 19941.]\n",
      "P1 actions distribution [0.25906229 0.35436521 0.21689074 0.16968176]\n",
      "P2 SL Buffer Size:  125154\n",
      "P2 SL buffer distribution [33950. 44000. 25360. 21844.]\n",
      "P2 actions distribution [0.2712658  0.35156687 0.20263036 0.17453697]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.2278, 0.7699, 0.0023, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.6951,  1.0573, -1.0019,  0.6878]])\n",
      "Player 0 Prediction: tensor([[0.9978, 0.0000, 0.0022, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.2462,  2.4815, -3.2528,  1.5230]])\n",
      "Player 0 Prediction: tensor([[0.1414, 0.8532, 0.0055, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.2105,  2.8837, -4.8792,  2.4679]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 31415\n",
      "Average episode length: 3.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5124/10000 (51.2%)\n",
      "    Average reward: -0.284\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4876/10000 (48.8%)\n",
      "    Average reward: +0.284\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3741 (23.8%)\n",
      "    Action 1: 5691 (36.2%)\n",
      "    Action 2: 4087 (26.0%)\n",
      "    Action 3: 2182 (13.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4678 (29.8%)\n",
      "    Action 1: 5726 (36.4%)\n",
      "    Action 2: 3632 (23.1%)\n",
      "    Action 3: 1678 (10.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2841.5, 2841.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.037\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2842\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.2278, 0.7699, 0.0023, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2468, 0.0066, 0.7466]])\n",
      "Player 0 Prediction: tensor([[0.3424, 0.5896, 0.0680, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42107\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3703/10000 (37.0%)\n",
      "    Average reward: +0.211\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6297/10000 (63.0%)\n",
      "    Average reward: -0.211\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3466 (16.1%)\n",
      "    Action 1: 8302 (38.5%)\n",
      "    Action 2: 5202 (24.1%)\n",
      "    Action 3: 4584 (21.3%)\n",
      "  Player 1:\n",
      "    Action 0: 13367 (65.0%)\n",
      "    Action 1: 7186 (35.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2107.0, -2107.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.954 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.934 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.944\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2107\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.1290, -0.2487, -0.4975, -0.1974]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.2036e-01, 3.1337e-04, 7.9328e-02]])\n",
      "Player 0 Prediction: tensor([[-1.2410, -1.7804, -1.0007, -1.1042]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19999/50000 [17:06<21:16, 23.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30361\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5227/10000 (52.3%)\n",
      "    Average reward: +0.078\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4773/10000 (47.7%)\n",
      "    Average reward: -0.078\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4263 (28.0%)\n",
      "    Action 1: 5572 (36.6%)\n",
      "    Action 2: 4315 (28.4%)\n",
      "    Action 3: 1054 (6.9%)\n",
      "  Player 1:\n",
      "    Action 0: 3955 (26.1%)\n",
      "    Action 1: 4981 (32.9%)\n",
      "    Action 2: 3670 (24.2%)\n",
      "    Action 3: 2551 (16.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [780.0, -780.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.033 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0780\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.2036e-01, 3.1337e-04, 7.9328e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8114, 0.0091, 0.1795]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41750\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7143/10000 (71.4%)\n",
      "    Average reward: -0.260\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2857/10000 (28.6%)\n",
      "    Average reward: +0.260\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13474 (66.1%)\n",
      "    Action 1: 6903 (33.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3061 (14.3%)\n",
      "    Action 1: 8415 (39.4%)\n",
      "    Action 2: 5218 (24.4%)\n",
      "    Action 3: 4679 (21.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2602.0, 2602.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.931 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.927\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2602\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21003/50000 [17:57<21:03, 22.94it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 123653/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 131620/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21999/50000 [18:37<18:15, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 130165/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 138263/2000000\n",
      "P1 SL Buffer Size:  130165\n",
      "P1 SL buffer distribution [33624. 46131. 28974. 21436.]\n",
      "P1 actions distribution [0.25831829 0.35440403 0.2225944  0.16468329]\n",
      "P2 SL Buffer Size:  138263\n",
      "P2 SL buffer distribution [37115. 48940. 28527. 23681.]\n",
      "P2 actions distribution [0.26843769 0.3539631  0.20632418 0.17127503]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.0827,  1.1744, -0.5725,  0.9859]])\n",
      "Player 0 Prediction: tensor([[0.2682, 0.7124, 0.0193, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.3714,  2.1573, -1.6712,  2.0106]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7730, 0.0045, 0.2225]])\n",
      "Player 1 Prediction: tensor([[ 0.3240,  1.0344, -2.6073,  1.1587]])\n",
      "Player 0 Prediction: tensor([[0.9911, 0.0000, 0.0089, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 32584\n",
      "Average episode length: 3.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4038/10000 (40.4%)\n",
      "    Average reward: -0.274\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5962/10000 (59.6%)\n",
      "    Average reward: +0.274\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4627 (27.5%)\n",
      "    Action 1: 5830 (34.6%)\n",
      "    Action 2: 5155 (30.6%)\n",
      "    Action 3: 1243 (7.4%)\n",
      "  Player 1:\n",
      "    Action 0: 2834 (18.0%)\n",
      "    Action 1: 9062 (57.6%)\n",
      "    Action 2: 2553 (16.2%)\n",
      "    Action 3: 1280 (8.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2742.0, 2742.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.904 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.973\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2742\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0669, 0.0831, 0.8500]])\n",
      "Player 0 Prediction: tensor([[0.0352, 0.3519, 0.6129, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42088\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3811/10000 (38.1%)\n",
      "    Average reward: +0.293\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6189/10000 (61.9%)\n",
      "    Average reward: -0.293\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3243 (15.1%)\n",
      "    Action 1: 8639 (40.1%)\n",
      "    Action 2: 5099 (23.7%)\n",
      "    Action 3: 4540 (21.1%)\n",
      "  Player 1:\n",
      "    Action 0: 13588 (66.1%)\n",
      "    Action 1: 6979 (33.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2931.5, -2931.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.940 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.932\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2932\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0906, 0.0338, 0.8757, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21999/50000 [18:56<18:15, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 33890\n",
      "Average episode length: 3.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5634/10000 (56.3%)\n",
      "    Average reward: +0.265\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4366/10000 (43.7%)\n",
      "    Average reward: -0.265\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5900 (34.6%)\n",
      "    Action 1: 3915 (22.9%)\n",
      "    Action 2: 3787 (22.2%)\n",
      "    Action 3: 3466 (20.3%)\n",
      "  Player 1:\n",
      "    Action 0: 3598 (21.4%)\n",
      "    Action 1: 6306 (37.5%)\n",
      "    Action 2: 2812 (16.7%)\n",
      "    Action 3: 4106 (24.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2647.5, -2647.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.017 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.012\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2647\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4395, 0.0124, 0.5481]])\n",
      "Player 1 Prediction: tensor([[0.0533, 0.1151, 0.8316, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41810\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7161/10000 (71.6%)\n",
      "    Average reward: -0.221\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2839/10000 (28.4%)\n",
      "    Average reward: +0.221\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13708 (67.1%)\n",
      "    Action 1: 6735 (32.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2921 (13.7%)\n",
      "    Action 1: 8661 (40.5%)\n",
      "    Action 2: 5149 (24.1%)\n",
      "    Action 3: 4636 (21.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2208.0, 2208.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.914 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.917\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2208\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23003/50000 [19:41<17:00, 26.44it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 136928/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 144978/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [20:18<16:13, 26.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 144176/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 151558/2000000\n",
      "P1 SL Buffer Size:  144176\n",
      "P1 SL buffer distribution [37766. 51828. 31717. 22865.]\n",
      "P1 actions distribution [0.26194374 0.35947731 0.21998807 0.15859089]\n",
      "P2 SL Buffer Size:  151558\n",
      "P2 SL buffer distribution [39972. 54805. 31306. 25475.]\n",
      "P2 actions distribution [0.26374061 0.36161074 0.20656118 0.16808746]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[-0.7142, -0.7956, -0.4501, -0.8366]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 32787\n",
      "Average episode length: 3.3 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5753/10000 (57.5%)\n",
      "    Average reward: -0.203\n",
      "    Reward range: -6.0 to +6.0\n",
      "  Player 1:\n",
      "    Wins: 4247/10000 (42.5%)\n",
      "    Average reward: +0.203\n",
      "    Reward range: -6.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2872 (18.0%)\n",
      "    Action 1: 6121 (38.3%)\n",
      "    Action 2: 2742 (17.1%)\n",
      "    Action 3: 4256 (26.6%)\n",
      "  Player 1:\n",
      "    Action 0: 6223 (37.1%)\n",
      "    Action 1: 2044 (12.2%)\n",
      "    Action 2: 4218 (25.1%)\n",
      "    Action 3: 4311 (25.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2030.5, 2030.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.975 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.901 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.938\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2031\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0883, 0.0684, 0.8433]])\n",
      "Player 0 Prediction: tensor([[0.0351, 0.4372, 0.5277, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42443\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3754/10000 (37.5%)\n",
      "    Average reward: +0.269\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6246/10000 (62.5%)\n",
      "    Average reward: -0.269\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3637 (16.7%)\n",
      "    Action 1: 8392 (38.5%)\n",
      "    Action 2: 5194 (23.9%)\n",
      "    Action 3: 4548 (20.9%)\n",
      "  Player 1:\n",
      "    Action 0: 13392 (64.8%)\n",
      "    Action 1: 7280 (35.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2691.0, -2691.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.961 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.936 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.949\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2691\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.2367, -0.2689, -0.5274, -0.1849]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.0452e-01, 1.9840e-04, 9.5277e-02]])\n",
      "Player 0 Prediction: tensor([[-1.0324, -1.5551, -1.0085, -0.9265]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23999/50000 [20:37<16:13, 26.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 32302\n",
      "Average episode length: 3.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5939/10000 (59.4%)\n",
      "    Average reward: +0.163\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4061/10000 (40.6%)\n",
      "    Average reward: -0.163\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3240 (20.5%)\n",
      "    Action 1: 6800 (43.0%)\n",
      "    Action 2: 3410 (21.6%)\n",
      "    Action 3: 2351 (14.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4159 (25.2%)\n",
      "    Action 1: 5867 (35.6%)\n",
      "    Action 2: 4144 (25.1%)\n",
      "    Action 3: 2331 (14.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1626.0, -1626.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.992 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.012\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1626\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0741, 0.0532, 0.8727]])\n",
      "Player 1 Prediction: tensor([[0.0239, 0.3616, 0.6145, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42419\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7144/10000 (71.4%)\n",
      "    Average reward: -0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2856/10000 (28.6%)\n",
      "    Average reward: +0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13682 (66.2%)\n",
      "    Action 1: 6994 (33.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2957 (13.6%)\n",
      "    Action 1: 8696 (40.0%)\n",
      "    Action 2: 5255 (24.2%)\n",
      "    Action 3: 4835 (22.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2536.5, 2536.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.922\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2536\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25003/50000 [21:22<16:52, 24.68it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 151789/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 158501/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [22:05<18:04, 22.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 159375/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 165325/2000000\n",
      "P1 SL Buffer Size:  159375\n",
      "P1 SL buffer distribution [42744. 58313. 33686. 24632.]\n",
      "P1 actions distribution [0.26819765 0.36588549 0.21136314 0.15455373]\n",
      "P2 SL Buffer Size:  165325\n",
      "P2 SL buffer distribution [43509. 59949. 34154. 27713.]\n",
      "P2 actions distribution [0.26317254 0.36261303 0.20658703 0.1676274 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.8057,  0.9237, -0.5852,  0.8477]])\n",
      "Player 0 Prediction: tensor([[0.4428, 0.1283, 0.4288, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.2843,  3.1332, -2.5502,  3.6583]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5818, 0.0625, 0.3558]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [22:17<18:04, 22.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30686\n",
      "Average episode length: 3.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4881/10000 (48.8%)\n",
      "    Average reward: -0.263\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5119/10000 (51.2%)\n",
      "    Average reward: +0.263\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4546 (29.3%)\n",
      "    Action 1: 5348 (34.5%)\n",
      "    Action 2: 4161 (26.8%)\n",
      "    Action 3: 1460 (9.4%)\n",
      "  Player 1:\n",
      "    Action 0: 2388 (15.7%)\n",
      "    Action 1: 7043 (46.4%)\n",
      "    Action 2: 3706 (24.4%)\n",
      "    Action 3: 2034 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2626.0, 2626.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.934 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.991\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2626\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1282, 0.0592, 0.8126]])\n",
      "Player 0 Prediction: tensor([[0.0117, 0.0407, 0.9475, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42742\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3854/10000 (38.5%)\n",
      "    Average reward: +0.289\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6146/10000 (61.5%)\n",
      "    Average reward: -0.289\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3660 (16.8%)\n",
      "    Action 1: 8634 (39.5%)\n",
      "    Action 2: 5081 (23.3%)\n",
      "    Action 3: 4456 (20.4%)\n",
      "  Player 1:\n",
      "    Action 0: 13621 (65.1%)\n",
      "    Action 1: 7290 (34.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2891.0, -2891.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.961 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.933 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.947\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2891\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6416, 0.3485, 0.0099, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.5100, -0.3968, -0.9182, -0.5884]])\n",
      "Player 1 Prediction: tensor([[0.6150, 0.0628, 0.3223, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-2.5556, -2.7939, -2.1130, -2.4868]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 35476\n",
      "Average episode length: 3.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6679/10000 (66.8%)\n",
      "    Average reward: +0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3321/10000 (33.2%)\n",
      "    Average reward: -0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3938 (23.1%)\n",
      "    Action 1: 9467 (55.5%)\n",
      "    Action 2: 2928 (17.2%)\n",
      "    Action 3: 737 (4.3%)\n",
      "  Player 1:\n",
      "    Action 0: 5227 (28.4%)\n",
      "    Action 1: 6259 (34.0%)\n",
      "    Action 2: 4892 (26.6%)\n",
      "    Action 3: 2028 (11.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2862.0, -2862.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.960 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2862\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6416, 0.3485, 0.0099, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6150, 0.0628, 0.3223, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0755, 0.5809, 0.3436, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42521\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7159/10000 (71.6%)\n",
      "    Average reward: -0.214\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2841/10000 (28.4%)\n",
      "    Average reward: +0.214\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13689 (66.1%)\n",
      "    Action 1: 7020 (33.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2968 (13.6%)\n",
      "    Action 1: 8689 (39.8%)\n",
      "    Action 2: 5314 (24.4%)\n",
      "    Action 3: 4841 (22.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2143.0, 2143.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.922\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2143\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27003/50000 [23:14<16:57, 22.60it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 166755/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 172243/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27999/50000 [23:58<15:58, 22.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 173955/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 179052/2000000\n",
      "P1 SL Buffer Size:  173955\n",
      "P1 SL buffer distribution [46919. 65129. 35533. 26374.]\n",
      "P1 actions distribution [0.26971918 0.37440143 0.20426547 0.15161392]\n",
      "P2 SL Buffer Size:  179052\n",
      "P2 SL buffer distribution [46727. 65089. 36855. 30381.]\n",
      "P2 actions distribution [0.26096888 0.36352009 0.20583406 0.16967697]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.3213, 0.6771, 0.0016, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.5648, -0.6681, -1.0008, -0.5050]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8420, 0.0028, 0.1552]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 38612\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4407/10000 (44.1%)\n",
      "    Average reward: -0.340\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5593/10000 (55.9%)\n",
      "    Average reward: +0.340\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5209 (26.6%)\n",
      "    Action 1: 7434 (37.9%)\n",
      "    Action 2: 4100 (20.9%)\n",
      "    Action 3: 2876 (14.7%)\n",
      "  Player 1:\n",
      "    Action 0: 4899 (25.8%)\n",
      "    Action 1: 8090 (42.6%)\n",
      "    Action 2: 2519 (13.3%)\n",
      "    Action 3: 3485 (18.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3401.0, 3401.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3401\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4632, 0.0173, 0.5195]])\n",
      "Player 0 Prediction: tensor([[0.2333, 0.3942, 0.3725, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43032\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3844/10000 (38.4%)\n",
      "    Average reward: +0.221\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6156/10000 (61.6%)\n",
      "    Average reward: -0.221\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3516 (16.0%)\n",
      "    Action 1: 9027 (41.2%)\n",
      "    Action 2: 5020 (22.9%)\n",
      "    Action 3: 4344 (19.8%)\n",
      "  Player 1:\n",
      "    Action 0: 14023 (66.4%)\n",
      "    Action 1: 7102 (33.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2210.5, -2210.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.936\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2210\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.1028,  0.7463, -0.4836,  0.9138]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.1069e-01, 2.1874e-04, 1.8909e-01]])\n",
      "Player 0 Prediction: tensor([[ 0.1289,  0.0214, -0.7614,  0.3649]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.7657e-01, 3.3246e-04, 1.2310e-01]])\n",
      "Player 0 Prediction: tensor([[ 0.5049,  1.2673, -1.2000,  1.6873]])\n",
      "Player 1 Prediction: tensor([[0.8816, 0.0000, 0.1184, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27999/50000 [24:17<15:58, 22.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 34328\n",
      "Average episode length: 3.4 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5280/10000 (52.8%)\n",
      "    Average reward: +0.190\n",
      "    Reward range: -6.0 to +6.0\n",
      "  Player 1:\n",
      "    Wins: 4720/10000 (47.2%)\n",
      "    Average reward: -0.190\n",
      "    Reward range: -6.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7451 (42.3%)\n",
      "    Action 1: 1937 (11.0%)\n",
      "    Action 2: 3903 (22.2%)\n",
      "    Action 3: 4305 (24.5%)\n",
      "  Player 1:\n",
      "    Action 0: 2310 (13.8%)\n",
      "    Action 1: 7041 (42.1%)\n",
      "    Action 2: 2467 (14.7%)\n",
      "    Action 3: 4914 (29.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1901.0, -1901.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.875 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.898\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1901\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0705, 0.1077, 0.8218, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42972\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7087/10000 (70.9%)\n",
      "    Average reward: -0.244\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2913/10000 (29.1%)\n",
      "    Average reward: +0.244\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13703 (65.5%)\n",
      "    Action 1: 7215 (34.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3050 (13.8%)\n",
      "    Action 1: 8759 (39.7%)\n",
      "    Action 2: 5272 (23.9%)\n",
      "    Action 3: 4973 (22.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2438.5, 2438.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.929 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.927\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2439\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29003/50000 [25:08<15:37, 22.39it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 181014/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 185640/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [25:53<14:54, 22.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 188200/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 192500/2000000\n",
      "P1 SL Buffer Size:  188200\n",
      "P1 SL buffer distribution [50975. 71919. 37423. 27883.]\n",
      "P1 actions distribution [0.27085547 0.38214134 0.19884697 0.14815622]\n",
      "P2 SL Buffer Size:  192500\n",
      "P2 SL buffer distribution [49686. 70326. 39553. 32935.]\n",
      "P2 actions distribution [0.25810909 0.36532987 0.20547013 0.17109091]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.0622,  0.9128, -0.5679,  0.5900]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2189, 0.0556, 0.7256]])\n",
      "Player 1 Prediction: tensor([[-0.0082,  0.2389, -0.9742,  0.0361]])\n",
      "Player 0 Prediction: tensor([[0.0092, 0.0221, 0.9687, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [26:07<14:54, 22.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 37486\n",
      "Average episode length: 3.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5053/10000 (50.5%)\n",
      "    Average reward: -0.234\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4947/10000 (49.5%)\n",
      "    Average reward: +0.234\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4953 (26.3%)\n",
      "    Action 1: 7127 (37.8%)\n",
      "    Action 2: 3950 (20.9%)\n",
      "    Action 3: 2830 (15.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7300 (39.2%)\n",
      "    Action 1: 7314 (39.3%)\n",
      "    Action 2: 3189 (17.1%)\n",
      "    Action 3: 823 (4.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2336.0, 2336.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.048\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2336\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1602, 0.2015, 0.6383, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1682, 0.2474, 0.5844]])\n",
      "Player 0 Prediction: tensor([[0.0068, 0.0292, 0.9640, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43212\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3906/10000 (39.1%)\n",
      "    Average reward: +0.235\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6094/10000 (60.9%)\n",
      "    Average reward: -0.235\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3413 (15.5%)\n",
      "    Action 1: 9347 (42.6%)\n",
      "    Action 2: 4978 (22.7%)\n",
      "    Action 3: 4226 (19.2%)\n",
      "  Player 1:\n",
      "    Action 0: 14353 (67.5%)\n",
      "    Action 1: 6895 (32.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2348.5, -2348.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.942 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.909 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.926\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2349\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.9507,  0.8692, -0.4755,  0.8368]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1403, 0.0474, 0.8123]])\n",
      "Player 0 Prediction: tensor([[ 2.4661,  2.4887, -0.8165,  1.8959]])\n",
      "Player 1 Prediction: tensor([[0.0113, 0.0544, 0.9343, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41145\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6910/10000 (69.1%)\n",
      "    Average reward: +0.410\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3090/10000 (30.9%)\n",
      "    Average reward: -0.410\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7422 (37.2%)\n",
      "    Action 1: 7530 (37.7%)\n",
      "    Action 2: 2029 (10.2%)\n",
      "    Action 3: 2978 (14.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4833 (22.8%)\n",
      "    Action 1: 7940 (37.5%)\n",
      "    Action 2: 4105 (19.4%)\n",
      "    Action 3: 4308 (20.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4104.5, -4104.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.017 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4104\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5848, 0.4089, 0.0063, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6568, 0.0584, 0.2848, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0133, 0.0266, 0.9601, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42703\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7010/10000 (70.1%)\n",
      "    Average reward: -0.276\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2990/10000 (29.9%)\n",
      "    Average reward: +0.276\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13744 (66.1%)\n",
      "    Action 1: 7043 (33.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3001 (13.7%)\n",
      "    Action 1: 8875 (40.5%)\n",
      "    Action 2: 5176 (23.6%)\n",
      "    Action 3: 4864 (22.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2763.5, 2763.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.922\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2763\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31003/50000 [27:04<13:50, 22.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 195507/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 199285/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31999/50000 [27:49<13:29, 22.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 202613/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 206357/2000000\n",
      "P1 SL Buffer Size:  202613\n",
      "P1 SL buffer distribution [55243. 78707. 39228. 29435.]\n",
      "P1 actions distribution [0.27265279 0.38845977 0.19361048 0.14527696]\n",
      "P2 SL Buffer Size:  206357\n",
      "P2 SL buffer distribution [53140. 76260. 41924. 35033.]\n",
      "P2 actions distribution [0.25751489 0.36955373 0.20316248 0.1697689 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.6637, 0.3255, 0.0108, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.4033, -0.1821, -1.0327, -0.4240]])\n",
      "Player 0 Prediction: tensor([[0.5215, 0.1048, 0.3737, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41537\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4000/10000 (40.0%)\n",
      "    Average reward: -0.401\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6000/10000 (60.0%)\n",
      "    Average reward: +0.401\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5878 (27.5%)\n",
      "    Action 1: 8090 (37.9%)\n",
      "    Action 2: 4444 (20.8%)\n",
      "    Action 3: 2948 (13.8%)\n",
      "  Player 1:\n",
      "    Action 0: 5650 (28.0%)\n",
      "    Action 1: 9570 (47.4%)\n",
      "    Action 2: 1884 (9.3%)\n",
      "    Action 3: 3073 (15.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4014.5, 4014.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4014\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3465, 0.6518, 0.0017, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2334, 0.7566, 0.0100, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1322, 0.8082, 0.0595, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43744\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 3918/10000 (39.2%)\n",
      "    Average reward: +0.223\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 6082/10000 (60.8%)\n",
      "    Average reward: -0.223\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3456 (15.6%)\n",
      "    Action 1: 9548 (43.1%)\n",
      "    Action 2: 4887 (22.0%)\n",
      "    Action 3: 4278 (19.3%)\n",
      "  Player 1:\n",
      "    Action 0: 14531 (67.4%)\n",
      "    Action 1: 7044 (32.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2230.0, -2230.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.941 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.911 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.926\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2230\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.5701,  0.5077, -0.6087,  0.5733]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1539, 0.0458, 0.8002]])\n",
      "Player 0 Prediction: tensor([[-0.4232, -0.4939, -0.9386, -0.1475]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1297, 0.2295, 0.6408]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31999/50000 [28:08<13:29, 22.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39648\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7077/10000 (70.8%)\n",
      "    Average reward: +0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2923/10000 (29.2%)\n",
      "    Average reward: -0.359\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5710 (30.2%)\n",
      "    Action 1: 9899 (52.3%)\n",
      "    Action 2: 1616 (8.5%)\n",
      "    Action 3: 1702 (9.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5540 (26.7%)\n",
      "    Action 1: 7488 (36.1%)\n",
      "    Action 2: 4954 (23.9%)\n",
      "    Action 3: 2739 (13.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3589.0, -3589.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.025\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3589\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1539, 0.0458, 0.8002]])\n",
      "Player 1 Prediction: tensor([[0.0061, 0.0398, 0.9540, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42739\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6943/10000 (69.4%)\n",
      "    Average reward: -0.344\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3057/10000 (30.6%)\n",
      "    Average reward: +0.344\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 13810 (66.2%)\n",
      "    Action 1: 7061 (33.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3011 (13.8%)\n",
      "    Action 1: 8776 (40.1%)\n",
      "    Action 2: 5204 (23.8%)\n",
      "    Action 3: 4877 (22.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3437.5, 3437.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.923 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.922 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.923\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3438\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33004/50000 [29:01<12:49, 22.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 209609/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 213593/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [29:47<12:18, 21.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 216557/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 220711/2000000\n",
      "P1 SL Buffer Size:  216557\n",
      "P1 SL buffer distribution [59226. 85408. 40874. 31049.]\n",
      "P1 actions distribution [0.2734892  0.39439039 0.18874476 0.14337565]\n",
      "P2 SL Buffer Size:  220711\n",
      "P2 SL buffer distribution [57018. 82158. 44042. 37493.]\n",
      "P2 actions distribution [0.25833783 0.37224243 0.19954601 0.16987373]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.3613, 0.6371, 0.0016, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.0639,  0.8790, -0.9855,  0.9930]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8677, 0.0010, 0.1313]])\n",
      "Player 1 Prediction: tensor([[-0.2297,  0.9042, -2.0377,  1.1007]])\n",
      "Player 0 Prediction: tensor([[0.9588, 0.0000, 0.0412, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [29:58<12:18, 21.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42701\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4520/10000 (45.2%)\n",
      "    Average reward: -0.317\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5480/10000 (54.8%)\n",
      "    Average reward: +0.317\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4218 (19.3%)\n",
      "    Action 1: 9470 (43.4%)\n",
      "    Action 2: 3964 (18.2%)\n",
      "    Action 3: 4162 (19.1%)\n",
      "  Player 1:\n",
      "    Action 0: 8722 (41.8%)\n",
      "    Action 1: 6402 (30.7%)\n",
      "    Action 2: 2799 (13.4%)\n",
      "    Action 3: 2964 (14.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3168.0, 3168.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.981 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3168\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3613, 0.6371, 0.0016, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2306, 0.7606, 0.0088, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2519, 0.0124, 0.7357]])\n",
      "Player 0 Prediction: tensor([[0.5678, 0.3919, 0.0403, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43929\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4033/10000 (40.3%)\n",
      "    Average reward: +0.242\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5967/10000 (59.7%)\n",
      "    Average reward: -0.242\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3387 (15.3%)\n",
      "    Action 1: 9873 (44.5%)\n",
      "    Action 2: 4760 (21.5%)\n",
      "    Action 3: 4167 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 14872 (68.4%)\n",
      "    Action 1: 6870 (31.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2421.5, -2421.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.934 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.900 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.917\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2422\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2665, 0.7326, 0.0009, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.7084, -1.0663, -1.0054, -0.6109]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8331, 0.0017, 0.1652]])\n",
      "Player 0 Prediction: tensor([[-3.8235, -5.4762, -1.9727, -2.1602]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40682\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6862/10000 (68.6%)\n",
      "    Average reward: +0.374\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3138/10000 (31.4%)\n",
      "    Average reward: -0.374\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5215 (26.7%)\n",
      "    Action 1: 9274 (47.5%)\n",
      "    Action 2: 2131 (10.9%)\n",
      "    Action 3: 2905 (14.9%)\n",
      "  Player 1:\n",
      "    Action 0: 5652 (26.7%)\n",
      "    Action 1: 7796 (36.8%)\n",
      "    Action 2: 4390 (20.7%)\n",
      "    Action 3: 3319 (15.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3744.5, -3744.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.029\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3745\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 7.4442e-01, 2.3729e-04, 2.5534e-01]])\n",
      "Player 1 Prediction: tensor([[0.2176, 0.7601, 0.0223, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43260\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6960/10000 (69.6%)\n",
      "    Average reward: -0.337\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3040/10000 (30.4%)\n",
      "    Average reward: +0.337\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14082 (66.3%)\n",
      "    Action 1: 7160 (33.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2886 (13.1%)\n",
      "    Action 1: 8959 (40.7%)\n",
      "    Action 2: 5090 (23.1%)\n",
      "    Action 3: 5083 (23.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3373.5, 3373.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.922 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.912 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.917\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3373\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35003/50000 [31:00<11:32, 21.65it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 223646/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 227901/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35998/50000 [31:49<11:18, 20.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 230478/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 235380/2000000\n",
      "P1 SL Buffer Size:  230478\n",
      "P1 SL buffer distribution [63215. 91842. 42562. 32859.]\n",
      "P1 actions distribution [0.27427781 0.39848489 0.18466838 0.14256892]\n",
      "P2 SL Buffer Size:  235380\n",
      "P2 SL buffer distribution [61172. 88162. 46140. 39906.]\n",
      "P2 actions distribution [0.25988614 0.37455179 0.19602345 0.16953862]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.9471,  0.7455, -0.5344,  0.7877]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.4358e-01, 4.2815e-04, 2.5599e-01]])\n",
      "Player 1 Prediction: tensor([[ 0.4537,  0.8644, -0.9556,  0.8803]])\n",
      "Player 0 Prediction: tensor([[9.9928e-01, 0.0000e+00, 7.1956e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.7733, -0.2589, -2.8182,  0.1293]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2573, 0.0135, 0.7292]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44594\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4695/10000 (46.9%)\n",
      "    Average reward: -0.377\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5305/10000 (53.0%)\n",
      "    Average reward: +0.377\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5444 (23.7%)\n",
      "    Action 1: 9250 (40.4%)\n",
      "    Action 2: 3702 (16.1%)\n",
      "    Action 3: 4527 (19.7%)\n",
      "  Player 1:\n",
      "    Action 0: 7348 (33.9%)\n",
      "    Action 1: 7131 (32.9%)\n",
      "    Action 2: 2397 (11.1%)\n",
      "    Action 3: 4795 (22.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3767.0, 3767.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3767\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1172, 0.3304, 0.5524, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5934, 0.1054, 0.3012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35998/50000 [32:08<11:18, 20.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43941\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4014/10000 (40.1%)\n",
      "    Average reward: +0.239\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5986/10000 (59.9%)\n",
      "    Average reward: -0.239\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3139 (14.2%)\n",
      "    Action 1: 9996 (45.2%)\n",
      "    Action 2: 4734 (21.4%)\n",
      "    Action 3: 4266 (19.3%)\n",
      "  Player 1:\n",
      "    Action 0: 15031 (68.9%)\n",
      "    Action 1: 6775 (31.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2391.0, -2391.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.918 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.894 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.906\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2391\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.3097, 0.6893, 0.0010, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.5744, -0.2640, -1.0510, -0.2424]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.7574e-01, 2.7848e-04, 3.2398e-01]])\n",
      "Player 0 Prediction: tensor([[-1.3664, -1.2908, -1.0545, -0.6829]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 37624\n",
      "Average episode length: 3.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6819/10000 (68.2%)\n",
      "    Average reward: +0.292\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3181/10000 (31.8%)\n",
      "    Average reward: -0.292\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3725 (20.5%)\n",
      "    Action 1: 9545 (52.4%)\n",
      "    Action 2: 2238 (12.3%)\n",
      "    Action 3: 2698 (14.8%)\n",
      "  Player 1:\n",
      "    Action 0: 5252 (27.0%)\n",
      "    Action 1: 7191 (37.0%)\n",
      "    Action 2: 4747 (24.4%)\n",
      "    Action 3: 2228 (11.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2920.0, -2920.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.957 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.999\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2920\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0923, 0.1162, 0.7915, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43389\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6929/10000 (69.3%)\n",
      "    Average reward: -0.313\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3071/10000 (30.7%)\n",
      "    Average reward: +0.313\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14222 (66.7%)\n",
      "    Action 1: 7092 (33.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3100 (14.0%)\n",
      "    Action 1: 9251 (41.9%)\n",
      "    Action 2: 4934 (22.4%)\n",
      "    Action 3: 4790 (21.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3127.5, 3127.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.918 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.924 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.921\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3127\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37003/50000 [33:05<09:57, 21.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 237482/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 242558/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37998/50000 [33:55<09:33, 20.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 244390/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 249716/2000000\n",
      "P1 SL Buffer Size:  244390\n",
      "P1 SL buffer distribution [66771. 98786. 44118. 34715.]\n",
      "P1 actions distribution [0.27321494 0.40421458 0.18052293 0.14204755]\n",
      "P2 SL Buffer Size:  249716\n",
      "P2 SL buffer distribution [65574. 93775. 47997. 42370.]\n",
      "P2 actions distribution [0.26259431 0.3755266  0.19220635 0.16967275]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5890, 0.4042, 0.0068, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.4085,  1.1529, -0.6268,  1.0139]])\n",
      "Player 0 Prediction: tensor([[0.5885, 0.0853, 0.3262, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37998/50000 [34:08<09:33, 20.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44876\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4129/10000 (41.3%)\n",
      "    Average reward: -0.397\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5871/10000 (58.7%)\n",
      "    Average reward: +0.397\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7251 (31.2%)\n",
      "    Action 1: 8793 (37.8%)\n",
      "    Action 2: 4309 (18.5%)\n",
      "    Action 3: 2903 (12.5%)\n",
      "  Player 1:\n",
      "    Action 0: 4663 (21.6%)\n",
      "    Action 1: 11040 (51.1%)\n",
      "    Action 2: 2251 (10.4%)\n",
      "    Action 3: 3666 (17.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3968.0, 3968.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.972 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.014\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3968\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 7.5496e-01, 3.6227e-04, 2.4468e-01]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3388, 0.0064, 0.6547]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44121\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4099/10000 (41.0%)\n",
      "    Average reward: +0.222\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5901/10000 (59.0%)\n",
      "    Average reward: -0.222\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3207 (14.4%)\n",
      "    Action 1: 10227 (46.1%)\n",
      "    Action 2: 4589 (20.7%)\n",
      "    Action 3: 4171 (18.8%)\n",
      "  Player 1:\n",
      "    Action 0: 15220 (69.4%)\n",
      "    Action 1: 6707 (30.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2217.5, -2217.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.918 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.888 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.903\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2218\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.6135, -0.4555, -0.4470, -0.3683]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 33420\n",
      "Average episode length: 3.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6020/10000 (60.2%)\n",
      "    Average reward: +0.259\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3980/10000 (39.8%)\n",
      "    Average reward: -0.259\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3704 (22.5%)\n",
      "    Action 1: 7491 (45.5%)\n",
      "    Action 2: 2978 (18.1%)\n",
      "    Action 3: 2282 (13.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4811 (28.4%)\n",
      "    Action 1: 6303 (37.2%)\n",
      "    Action 2: 3783 (22.3%)\n",
      "    Action 3: 2068 (12.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2594.5, -2594.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2595\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4926, 0.5030, 0.0044, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6943, 0.0817, 0.2240, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43739\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6812/10000 (68.1%)\n",
      "    Average reward: -0.346\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3188/10000 (31.9%)\n",
      "    Average reward: +0.346\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14610 (67.7%)\n",
      "    Action 1: 6982 (32.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2947 (13.3%)\n",
      "    Action 1: 9572 (43.2%)\n",
      "    Action 2: 4856 (21.9%)\n",
      "    Action 3: 4772 (21.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3457.0, 3457.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.908 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.910 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.909\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3457\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39005/50000 [35:09<07:55, 23.14it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 251095/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 257360/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39998/50000 [35:54<07:14, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 258035/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 264841/2000000\n",
      "P1 SL Buffer Size:  258035\n",
      "P1 SL buffer distribution [ 70324. 105160.  45751.  36800.]\n",
      "P1 actions distribution [0.27253667 0.40754161 0.1773054  0.14261631]\n",
      "P2 SL Buffer Size:  264841\n",
      "P2 SL buffer distribution [70616. 98919. 49989. 45317.]\n",
      "P2 actions distribution [0.26663545 0.37350335 0.18875099 0.17111021]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1032, 0.4060, 0.4908, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39998/50000 [36:08<07:14, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42591\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4331/10000 (43.3%)\n",
      "    Average reward: -0.340\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5669/10000 (56.7%)\n",
      "    Average reward: +0.340\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5547 (25.6%)\n",
      "    Action 1: 9102 (42.0%)\n",
      "    Action 2: 4159 (19.2%)\n",
      "    Action 3: 2878 (13.3%)\n",
      "  Player 1:\n",
      "    Action 0: 6072 (29.0%)\n",
      "    Action 1: 9062 (43.3%)\n",
      "    Action 2: 2606 (12.5%)\n",
      "    Action 3: 3165 (15.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3398.5, 3398.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.035\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3398\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3723, 0.6265, 0.0013, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 8.4948e-01, 4.9547e-04, 1.5002e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44381\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4196/10000 (42.0%)\n",
      "    Average reward: +0.255\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5804/10000 (58.0%)\n",
      "    Average reward: -0.255\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3070 (13.8%)\n",
      "    Action 1: 10589 (47.7%)\n",
      "    Action 2: 4460 (20.1%)\n",
      "    Action 3: 4089 (18.4%)\n",
      "  Player 1:\n",
      "    Action 0: 15643 (70.5%)\n",
      "    Action 1: 6530 (29.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2546.0, -2546.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.904 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.874 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.889\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2546\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1294, 0.1366, 0.7341, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39315\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6863/10000 (68.6%)\n",
      "    Average reward: +0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3137/10000 (31.4%)\n",
      "    Average reward: -0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4591 (24.0%)\n",
      "    Action 1: 10660 (55.8%)\n",
      "    Action 2: 1856 (9.7%)\n",
      "    Action 3: 1990 (10.4%)\n",
      "  Player 1:\n",
      "    Action 0: 5620 (27.8%)\n",
      "    Action 1: 7722 (38.2%)\n",
      "    Action 2: 4836 (23.9%)\n",
      "    Action 3: 2040 (10.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2865.0, -2865.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.964 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.004\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2865\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4710, 0.5251, 0.0038, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6869, 0.0978, 0.2153, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0102, 0.0284, 0.9614, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43559\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6877/10000 (68.8%)\n",
      "    Average reward: -0.333\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3123/10000 (31.2%)\n",
      "    Average reward: +0.333\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14283 (66.6%)\n",
      "    Action 1: 7152 (33.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3021 (13.7%)\n",
      "    Action 1: 9325 (42.1%)\n",
      "    Action 2: 4923 (22.3%)\n",
      "    Action 3: 4855 (21.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3331.5, 3331.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.919 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.918 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.918\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3332\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}, {'exploitability': 0.313175}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41003/50000 [37:04<06:17, 23.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 264737/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 272209/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [37:46<07:01, 18.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 271759/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 279491/2000000\n",
      "P1 SL Buffer Size:  271759\n",
      "P1 SL buffer distribution [ 74129. 111462.  47455.  38713.]\n",
      "P1 actions distribution [0.27277477 0.41015017 0.17462163 0.14245342]\n",
      "P2 SL Buffer Size:  279491\n",
      "P2 SL buffer distribution [ 74889. 104247.  52061.  48294.]\n",
      "P2 actions distribution [0.26794781 0.37298875 0.18627076 0.17279268]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5618, 0.4330, 0.0052, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.3882, -0.2997, -1.0022, -0.3472]])\n",
      "Player 0 Prediction: tensor([[0.9829, 0.0000, 0.0171, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-1.5073, -1.7409, -2.9283, -0.7258]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6096, 0.0722, 0.3182]])\n",
      "Player 1 Prediction: tensor([[-4.0356, -5.8454, -3.0351, -2.0026]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41999/50000 [37:59<07:01, 18.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 46156\n",
      "Average episode length: 4.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4966/10000 (49.7%)\n",
      "    Average reward: -0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5034/10000 (50.3%)\n",
      "    Average reward: +0.288\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4856 (20.7%)\n",
      "    Action 1: 9675 (41.2%)\n",
      "    Action 2: 3425 (14.6%)\n",
      "    Action 3: 5532 (23.6%)\n",
      "  Player 1:\n",
      "    Action 0: 9255 (40.8%)\n",
      "    Action 1: 5877 (25.9%)\n",
      "    Action 2: 2594 (11.4%)\n",
      "    Action 3: 4942 (21.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2877.0, 2877.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.997 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.033 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2877\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3928, 0.6059, 0.0013, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1950, 0.7972, 0.0079, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.3614, 0.2473, 0.3913, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44341\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4099/10000 (41.0%)\n",
      "    Average reward: +0.212\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5901/10000 (59.0%)\n",
      "    Average reward: -0.212\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3240 (14.5%)\n",
      "    Action 1: 10296 (46.2%)\n",
      "    Action 2: 4524 (20.3%)\n",
      "    Action 3: 4224 (19.0%)\n",
      "  Player 1:\n",
      "    Action 0: 15251 (69.1%)\n",
      "    Action 1: 6806 (30.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2122.0, -2122.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.919 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.892 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.905\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2122\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.5162, -0.4171, -0.4768, -0.2023]])\n",
      "Player 1 Prediction: tensor([[0.6764, 0.1172, 0.2064, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.2975,  2.1864, -1.9499,  1.8207]])\n",
      "Player 1 Prediction: tensor([[0.0090, 0.0286, 0.9624, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40549\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6938/10000 (69.4%)\n",
      "    Average reward: +0.347\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3062/10000 (30.6%)\n",
      "    Average reward: -0.347\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4504 (23.0%)\n",
      "    Action 1: 10943 (55.9%)\n",
      "    Action 2: 2146 (11.0%)\n",
      "    Action 3: 1984 (10.1%)\n",
      "  Player 1:\n",
      "    Action 0: 6222 (29.7%)\n",
      "    Action 1: 7873 (37.5%)\n",
      "    Action 2: 4632 (22.1%)\n",
      "    Action 3: 2245 (10.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3475.0, -3475.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.957 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.004\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3475\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1520, 0.1385, 0.7095, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43861\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6799/10000 (68.0%)\n",
      "    Average reward: -0.423\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3201/10000 (32.0%)\n",
      "    Average reward: +0.423\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14236 (65.9%)\n",
      "    Action 1: 7380 (34.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3058 (13.7%)\n",
      "    Action 1: 9206 (41.4%)\n",
      "    Action 2: 4934 (22.2%)\n",
      "    Action 3: 5047 (22.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4229.5, 4229.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.926 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.920 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.923\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4229\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}, {'exploitability': 0.313175}, {'exploitability': 0.3176}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43004/50000 [39:01<05:15, 22.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 278465/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 286723/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44000/50000 [39:47<04:58, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 285264/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 294112/2000000\n",
      "P1 SL Buffer Size:  285264\n",
      "P1 SL buffer distribution [ 77209. 117700.  49137.  41218.]\n",
      "P1 actions distribution [0.27065806 0.41260026 0.17225097 0.14449072]\n",
      "P2 SL Buffer Size:  294112\n",
      "P2 SL buffer distribution [ 78960. 110035.  53790.  51327.]\n",
      "P2 actions distribution [0.26846915 0.37412618 0.18288951 0.17451515]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[-0.1498, -0.0760, -0.6869, -0.0960]])\n",
      "Player 0 Prediction: tensor([[0.4844, 0.0876, 0.4280, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44000/50000 [39:59<04:58, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44028\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4874/10000 (48.7%)\n",
      "    Average reward: -0.293\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5126/10000 (51.3%)\n",
      "    Average reward: +0.293\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4595 (20.6%)\n",
      "    Action 1: 9882 (44.3%)\n",
      "    Action 2: 3460 (15.5%)\n",
      "    Action 3: 4380 (19.6%)\n",
      "  Player 1:\n",
      "    Action 0: 8285 (38.2%)\n",
      "    Action 1: 6358 (29.3%)\n",
      "    Action 2: 2577 (11.9%)\n",
      "    Action 3: 4491 (20.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2933.5, 2933.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.990 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.020\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2933\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2874, 0.0345, 0.6781]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1836, 0.2273, 0.5890]])\n",
      "Player 0 Prediction: tensor([[0.0062, 0.0141, 0.9796, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44480\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4331/10000 (43.3%)\n",
      "    Average reward: +0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5669/10000 (56.7%)\n",
      "    Average reward: -0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3005 (13.5%)\n",
      "    Action 1: 10889 (49.0%)\n",
      "    Action 2: 4213 (19.0%)\n",
      "    Action 3: 4104 (18.5%)\n",
      "  Player 1:\n",
      "    Action 0: 15884 (71.3%)\n",
      "    Action 1: 6385 (28.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2775.0, -2775.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.895 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.864 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.880\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2775\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.5962, -0.5466, -0.4889, -0.2482]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 35328\n",
      "Average episode length: 3.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5749/10000 (57.5%)\n",
      "    Average reward: +0.182\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4251/10000 (42.5%)\n",
      "    Average reward: -0.182\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2106 (12.0%)\n",
      "    Action 1: 9288 (52.7%)\n",
      "    Action 2: 3315 (18.8%)\n",
      "    Action 3: 2910 (16.5%)\n",
      "  Player 1:\n",
      "    Action 0: 6484 (36.6%)\n",
      "    Action 1: 5952 (33.6%)\n",
      "    Action 2: 3693 (20.9%)\n",
      "    Action 3: 1580 (8.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1820.0, -1820.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.853 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.956\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1820\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1608, 0.1533, 0.6859, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44553\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6868/10000 (68.7%)\n",
      "    Average reward: -0.343\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3132/10000 (31.3%)\n",
      "    Average reward: +0.343\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14472 (65.8%)\n",
      "    Action 1: 7512 (34.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3208 (14.2%)\n",
      "    Action 1: 9439 (41.8%)\n",
      "    Action 2: 4847 (21.5%)\n",
      "    Action 3: 5075 (22.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3434.0, 3434.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.926 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.926 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.926\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.3434\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}, {'exploitability': 0.313175}, {'exploitability': 0.3176}, {'exploitability': 0.237675}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45003/50000 [41:00<03:45, 22.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 292076/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 301092/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [41:48<03:11, 20.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 298809/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 308244/2000000\n",
      "P1 SL Buffer Size:  298809\n",
      "P1 SL buffer distribution [ 80426. 124018.  50652.  43713.]\n",
      "P1 actions distribution [0.26915521 0.41504105 0.16951297 0.14629077]\n",
      "P2 SL Buffer Size:  308244\n",
      "P2 SL buffer distribution [ 82601. 116122.  55429.  54092.]\n",
      "P2 actions distribution [0.26797277 0.37672104 0.17982183 0.17548436]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45999/50000 [41:59<03:11, 20.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.8577,  0.7854, -0.5573,  0.7832]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2733, 0.0280, 0.6987]])\n",
      "Player 1 Prediction: tensor([[ 1.1098,  0.9478, -0.9771,  0.8434]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.7227e-01, 9.1846e-04, 2.6808e-02]])\n",
      "Player 1 Prediction: tensor([[ 0.4648,  1.1376, -1.9941,  1.8880]])\n",
      "Player 0 Prediction: tensor([[0.7642, 0.0000, 0.2358, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44654\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4805/10000 (48.0%)\n",
      "    Average reward: -0.312\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5195/10000 (51.9%)\n",
      "    Average reward: +0.312\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4813 (21.1%)\n",
      "    Action 1: 9830 (43.1%)\n",
      "    Action 2: 3690 (16.2%)\n",
      "    Action 3: 4449 (19.5%)\n",
      "  Player 1:\n",
      "    Action 0: 8282 (37.9%)\n",
      "    Action 1: 7315 (33.4%)\n",
      "    Action 2: 2482 (11.3%)\n",
      "    Action 3: 3793 (17.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3119.0, 3119.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.997 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3119\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1065, 0.4371, 0.4564, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45019\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4298/10000 (43.0%)\n",
      "    Average reward: +0.269\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5702/10000 (57.0%)\n",
      "    Average reward: -0.269\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2945 (13.1%)\n",
      "    Action 1: 10986 (49.0%)\n",
      "    Action 2: 4168 (18.6%)\n",
      "    Action 3: 4332 (19.3%)\n",
      "  Player 1:\n",
      "    Action 0: 16012 (70.9%)\n",
      "    Action 1: 6576 (29.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2694.5, -2694.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.889 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.870 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.880\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2695\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[-0.4356, -0.2534, -0.5184, -0.2265]])\n",
      "Player 1 Prediction: tensor([[0.6177, 0.1903, 0.1920, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.5605,  2.2883, -1.9472,  1.8691]])\n",
      "Player 1 Prediction: tensor([[0.0082, 0.0311, 0.9606, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41819\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6743/10000 (67.4%)\n",
      "    Average reward: +0.274\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3257/10000 (32.6%)\n",
      "    Average reward: -0.274\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3878 (19.1%)\n",
      "    Action 1: 11945 (58.8%)\n",
      "    Action 2: 2130 (10.5%)\n",
      "    Action 3: 2358 (11.6%)\n",
      "  Player 1:\n",
      "    Action 0: 6858 (31.9%)\n",
      "    Action 1: 8029 (37.3%)\n",
      "    Action 2: 4596 (21.4%)\n",
      "    Action 3: 2025 (9.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2744.5, -2744.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.907 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.981\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2745\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 7.6956e-01, 1.4872e-04, 2.3029e-01]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.4521e-01, 2.8485e-04, 3.5450e-01]])\n",
      "Player 1 Prediction: tensor([[0.0436, 0.9092, 0.0472, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44602\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6756/10000 (67.6%)\n",
      "    Average reward: -0.405\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3244/10000 (32.4%)\n",
      "    Average reward: +0.405\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14768 (66.8%)\n",
      "    Action 1: 7343 (33.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2994 (13.3%)\n",
      "    Action 1: 9731 (43.3%)\n",
      "    Action 2: 4698 (20.9%)\n",
      "    Action 3: 5068 (22.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4048.0, 4048.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.917 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.910 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.914\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4048\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}, {'exploitability': 0.313175}, {'exploitability': 0.3176}, {'exploitability': 0.237675}, {'exploitability': 0.293175}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47002/50000 [43:07<02:29, 20.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 305841/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 315174/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48000/50000 [43:56<01:35, 20.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 312795/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 322073/2000000\n",
      "P1 SL Buffer Size:  312795\n",
      "P1 SL buffer distribution [ 84532. 129207.  52177.  46879.]\n",
      "P1 actions distribution [0.27024729 0.41307246 0.16680893 0.14987132]\n",
      "P2 SL Buffer Size:  322073\n",
      "P2 SL buffer distribution [ 86687. 121472.  56983.  56931.]\n",
      "P2 actions distribution [0.26915327 0.37715673 0.17692573 0.17676427]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5072, 0.4894, 0.0034, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.6003,  0.9074, -0.7384,  0.5359]])\n",
      "Player 0 Prediction: tensor([[0.6085, 0.1069, 0.2846, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-3.8582e-01, -1.4987e-01, -1.3670e+00,  3.9915e-04]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9543, 0.0046, 0.0412]])\n",
      "Player 1 Prediction: tensor([[-1.9819, -2.4830, -2.0596, -0.9254]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48000/50000 [44:09<01:35, 20.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43144\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4428/10000 (44.3%)\n",
      "    Average reward: -0.341\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5572/10000 (55.7%)\n",
      "    Average reward: +0.341\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7101 (32.1%)\n",
      "    Action 1: 8560 (38.7%)\n",
      "    Action 2: 3914 (17.7%)\n",
      "    Action 3: 2547 (11.5%)\n",
      "  Player 1:\n",
      "    Action 0: 3524 (16.8%)\n",
      "    Action 1: 10719 (51.0%)\n",
      "    Action 2: 2153 (10.2%)\n",
      "    Action 3: 4626 (22.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3408.0, 3408.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.927 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.992\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3408\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1191, 0.4316, 0.4493, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.5413, 0.0868, 0.3719, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45400\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4258/10000 (42.6%)\n",
      "    Average reward: +0.249\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5742/10000 (57.4%)\n",
      "    Average reward: -0.249\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3240 (14.2%)\n",
      "    Action 1: 10816 (47.6%)\n",
      "    Action 2: 4236 (18.6%)\n",
      "    Action 3: 4454 (19.6%)\n",
      "  Player 1:\n",
      "    Action 0: 15792 (69.7%)\n",
      "    Action 1: 6862 (30.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2488.5, -2488.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.910 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.898\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2488\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.4548,  0.8029, -0.5294,  0.6173]])\n",
      "Player 1 Prediction: tensor([[0.4682, 0.0601, 0.4716, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40008\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6591/10000 (65.9%)\n",
      "    Average reward: +0.300\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3409/10000 (34.1%)\n",
      "    Average reward: -0.300\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4539 (23.0%)\n",
      "    Action 1: 8610 (43.6%)\n",
      "    Action 2: 1809 (9.2%)\n",
      "    Action 3: 4778 (24.2%)\n",
      "  Player 1:\n",
      "    Action 0: 5856 (28.9%)\n",
      "    Action 1: 7770 (38.3%)\n",
      "    Action 2: 3383 (16.7%)\n",
      "    Action 3: 3263 (16.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2997.5, -2997.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.010 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.029\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2998\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4275, 0.0023, 0.5702]])\n",
      "Player 1 Prediction: tensor([[0.4695, 0.2661, 0.2643, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44809\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6789/10000 (67.9%)\n",
      "    Average reward: -0.409\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3211/10000 (32.1%)\n",
      "    Average reward: +0.409\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14573 (65.8%)\n",
      "    Action 1: 7574 (34.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3066 (13.5%)\n",
      "    Action 1: 9587 (42.3%)\n",
      "    Action 2: 4826 (21.3%)\n",
      "    Action 3: 5183 (22.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4089.0, 4089.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.927 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.915 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.921\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4089\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}, {'exploitability': 0.313175}, {'exploitability': 0.3176}, {'exploitability': 0.237675}, {'exploitability': 0.293175}, {'exploitability': 0.320275}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49003/50000 [45:10<00:45, 21.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 319627/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 328955/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [46:00<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.3612, 0.6380, 0.0008, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.5521,  1.0086, -0.9731,  0.7150]])\n",
      "Player 0 Prediction: tensor([[9.9971e-01, 0.0000e+00, 2.9235e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.7822, -0.0607, -2.3400,  0.0263]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4244, 0.0129, 0.5627]])\n",
      "Player 1 Prediction: tensor([[-2.2157, -2.6945, -2.8170, -0.9776]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 37688\n",
      "Average episode length: 3.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5544/10000 (55.4%)\n",
      "    Average reward: -0.238\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4456/10000 (44.6%)\n",
      "    Average reward: +0.238\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6190 (32.8%)\n",
      "    Action 1: 7370 (39.0%)\n",
      "    Action 2: 3256 (17.2%)\n",
      "    Action 3: 2072 (11.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2824 (15.0%)\n",
      "    Action 1: 8647 (46.0%)\n",
      "    Action 2: 3567 (19.0%)\n",
      "    Action 3: 3762 (20.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2380.0, 2380.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.926 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.992\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2380\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2683, 0.0258, 0.7059]])\n",
      "Player 0 Prediction: tensor([[0.0178, 0.0213, 0.9609, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45286\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4448/10000 (44.5%)\n",
      "    Average reward: +0.342\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5552/10000 (55.5%)\n",
      "    Average reward: -0.342\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3063 (13.6%)\n",
      "    Action 1: 10945 (48.5%)\n",
      "    Action 2: 4072 (18.0%)\n",
      "    Action 3: 4508 (20.0%)\n",
      "  Player 1:\n",
      "    Action 0: 15986 (70.4%)\n",
      "    Action 1: 6712 (29.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3423.0, -3423.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.897 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.876 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.887\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.3423\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1753, 0.1957, 0.6291, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42090\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6749/10000 (67.5%)\n",
      "    Average reward: +0.293\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3251/10000 (32.5%)\n",
      "    Average reward: -0.293\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4061 (19.8%)\n",
      "    Action 1: 10261 (49.9%)\n",
      "    Action 2: 1812 (8.8%)\n",
      "    Action 3: 4421 (21.5%)\n",
      "  Player 1:\n",
      "    Action 0: 6866 (31.9%)\n",
      "    Action 1: 8066 (37.5%)\n",
      "    Action 2: 3563 (16.5%)\n",
      "    Action 3: 3040 (14.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2932.0, -2932.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.963 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.010\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2932\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1753, 0.1957, 0.6291, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45112\n",
      "Average episode length: 4.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6710/10000 (67.1%)\n",
      "    Average reward: -0.424\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3290/10000 (32.9%)\n",
      "    Average reward: +0.424\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14872 (66.4%)\n",
      "    Action 1: 7513 (33.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3101 (13.6%)\n",
      "    Action 1: 9882 (43.5%)\n",
      "    Action 2: 4593 (20.2%)\n",
      "    Action 3: 5151 (22.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4239.0, 4239.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.915 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.918\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.4239\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 1.187625}, {'exploitability': 0.894725}, {'exploitability': 0.64975}, {'exploitability': 0.538675}, {'exploitability': 0.35115}, {'exploitability': 0.3189}, {'exploitability': 0.2298}, {'exploitability': 0.20737499999999998}, {'exploitability': 0.16549999999999998}, {'exploitability': 0.181075}, {'exploitability': 0.269475}, {'exploitability': 0.18282500000000002}, {'exploitability': 0.2744}, {'exploitability': 0.2651}, {'exploitability': 0.322025}, {'exploitability': 0.380175}, {'exploitability': 0.345625}, {'exploitability': 0.33435}, {'exploitability': 0.328125}, {'exploitability': 0.313175}, {'exploitability': 0.3176}, {'exploitability': 0.237675}, {'exploitability': 0.293175}, {'exploitability': 0.320275}, {'exploitability': 0.2656}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb5bf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x3711c2e00>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x3711c2e00>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.5\n",
      "Using         per_beta                      : 0.5\n",
      "Using         per_beta_final                : 1.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x3711c2e00>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.5\n",
      "Using         per_beta                      : 0.5\n",
      "Using         per_beta_final                : 1.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x3711c2aa0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x3711c2aa0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321e87e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-PER\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a00b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['best_response', 'best_response']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/50000 [00:00<16:02, 51.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 63/200000\n",
      "   Player 0 SL buffer: 4/2000000\n",
      "   Player 1 RL buffer: 64/200000\n",
      "   Player 1 SL buffer: 10/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1004/50000 [00:29<28:40, 28.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0019 â†’ 0.0019\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 64178/200000\n",
      "   Player 0 SL buffer: 7318/2000000\n",
      "   Player 1 RL buffer: 63948/200000\n",
      "   Player 1 SL buffer: 7214/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:04<27:36, 28.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0013 â†’ 0.0013\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 128763/200000\n",
      "   Player 0 SL buffer: 13649/2000000\n",
      "   Player 1 RL buffer: 127364/200000\n",
      "   Player 1 SL buffer: 13651/2000000\n",
      "P1 SL Buffer Size:  13649\n",
      "P1 SL buffer distribution [5106. 7723.   23.  797.]\n",
      "P1 actions distribution [0.37409334 0.565829   0.00168511 0.05839256]\n",
      "P2 SL Buffer Size:  13651\n",
      "P2 SL buffer distribution [4357. 8340.   48.  906.]\n",
      "P2 actions distribution [0.31917076 0.61094425 0.00351623 0.06636876]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.3588, 0.6271, 0.0141, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.2222,  0.5493, -1.2328,  0.5609]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9686, 0.0082, 0.0232]])\n",
      "Player 1 Prediction: tensor([[-0.4809, -0.6424, -2.2558,  0.6025]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58432\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5656/10000 (56.6%)\n",
      "    Average reward: -0.486\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4344/10000 (43.4%)\n",
      "    Average reward: +0.486\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11727 (40.2%)\n",
      "    Action 1: 16461 (56.5%)\n",
      "    Action 2: 596 (2.0%)\n",
      "    Action 3: 374 (1.3%)\n",
      "  Player 1:\n",
      "    Action 0: 9332 (31.9%)\n",
      "    Action 1: 16652 (56.9%)\n",
      "    Action 2: 809 (2.8%)\n",
      "    Action 3: 2481 (8.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4861.0, 4861.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.994 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.989 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.991\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4861\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9548, 0.0124, 0.0327]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9233, 0.0239, 0.0528]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1998/50000 [01:19<27:36, 28.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48448\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5667/10000 (56.7%)\n",
      "    Average reward: -0.099\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4333/10000 (43.3%)\n",
      "    Average reward: +0.099\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3090 (13.9%)\n",
      "    Action 1: 18087 (81.1%)\n",
      "    Action 2: 465 (2.1%)\n",
      "    Action 3: 666 (3.0%)\n",
      "  Player 1:\n",
      "    Action 0: 22972 (87.9%)\n",
      "    Action 1: 3168 (12.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-992.5, 992.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.640 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Tails\n",
      "  Player 1 strategy entropy: 0.533 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.587\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0993\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.1709,  0.4738, -0.8496,  0.2079]])\n",
      "Player 1 Prediction: tensor([[0.1161, 0.8727, 0.0112, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.0851, -0.1113, -1.9369,  0.4647]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9323, 0.0151, 0.0526]])\n",
      "Player 0 Prediction: tensor([[-3.3267, -2.3182, -2.8820,  0.3694]])\n",
      "Player 1 Prediction: tensor([[0.9750, 0.0000, 0.0250, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59398\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6288/10000 (62.9%)\n",
      "    Average reward: -0.012\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3712/10000 (37.1%)\n",
      "    Average reward: +0.012\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11859 (40.9%)\n",
      "    Action 1: 14316 (49.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 2819 (9.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10847 (35.7%)\n",
      "    Action 1: 18162 (59.7%)\n",
      "    Action 2: 654 (2.2%)\n",
      "    Action 3: 741 (2.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-119.0, 119.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.975 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.002\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0119\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2069, 0.7781, 0.0150, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.1746, 0.8109, 0.0145, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6679, 0.3142, 0.0179, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 46596\n",
      "Average episode length: 4.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6187/10000 (61.9%)\n",
      "    Average reward: +0.087\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3813/10000 (38.1%)\n",
      "    Average reward: -0.087\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 23405 (92.6%)\n",
      "    Action 1: 1866 (7.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 1516 (7.1%)\n",
      "    Action 1: 18413 (86.3%)\n",
      "    Action 2: 435 (2.0%)\n",
      "    Action 3: 961 (4.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [868.0, -868.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.380 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.454 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Tails\n",
      "  Average strategy entropy: 0.417\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0868\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3007/50000 [02:06<25:32, 30.66it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0011 â†’ 0.0011\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 193112/200000\n",
      "   Player 0 SL buffer: 20253/2000000\n",
      "   Player 1 RL buffer: 191014/200000\n",
      "   Player 1 SL buffer: 20481/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3998/50000 [02:37<24:16, 31.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0009 â†’ 0.0009\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 27069/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 27303/2000000\n",
      "P1 SL Buffer Size:  27069\n",
      "P1 SL buffer distribution [ 9547. 14691.  1002.  1829.]\n",
      "P1 actions distribution [0.35269127 0.54272415 0.03701651 0.06756807]\n",
      "P2 SL Buffer Size:  27303\n",
      "P2 SL buffer distribution [ 9061. 15196.  1077.  1969.]\n",
      "P2 actions distribution [0.33186829 0.55656888 0.03944621 0.07211662]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5531, 0.4369, 0.0099, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.5188,  2.0138, -1.3888,  0.9144]])\n",
      "Player 0 Prediction: tensor([[0.9908, 0.0000, 0.0092, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.2532, -0.2850, -2.8893,  0.9969]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8019, 0.0393, 0.1587]])\n",
      "Player 1 Prediction: tensor([[-4.2042, -4.2323, -3.3079,  0.7218]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 3998/50000 [02:49<24:16, 31.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57559\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5702/10000 (57.0%)\n",
      "    Average reward: -0.910\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4298/10000 (43.0%)\n",
      "    Average reward: +0.910\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8619 (31.6%)\n",
      "    Action 1: 16749 (61.3%)\n",
      "    Action 2: 661 (2.4%)\n",
      "    Action 3: 1280 (4.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10327 (34.1%)\n",
      "    Action 1: 14555 (48.1%)\n",
      "    Action 2: 3043 (10.1%)\n",
      "    Action 3: 2325 (7.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-9098.0, 9098.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.958 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.997\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.9098\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9520, 0.0106, 0.0375]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9217, 0.0224, 0.0559]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49412\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5727/10000 (57.3%)\n",
      "    Average reward: -0.081\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4273/10000 (42.7%)\n",
      "    Average reward: +0.081\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3048 (13.4%)\n",
      "    Action 1: 18119 (79.5%)\n",
      "    Action 2: 507 (2.2%)\n",
      "    Action 3: 1118 (4.9%)\n",
      "  Player 1:\n",
      "    Action 0: 23099 (86.8%)\n",
      "    Action 1: 3521 (13.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-806.0, 806.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.651 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.564 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.607\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0806\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.8186,  2.6722, -0.8775,  0.8608]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9305, 0.0127, 0.0568]])\n",
      "Player 0 Prediction: tensor([[ 2.5213,  3.2408, -1.5442,  1.0204]])\n",
      "Player 1 Prediction: tensor([[0.9881, 0.0000, 0.0119, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 3.2989,  3.1757, -2.9025,  1.4629]])\n",
      "Player 1 Prediction: tensor([[0.0428, 0.9271, 0.0302, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.9997,  2.2652, -4.2832,  1.1584]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 62179\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5706/10000 (57.1%)\n",
      "    Average reward: +1.051\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4294/10000 (42.9%)\n",
      "    Average reward: -1.051\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11411 (35.1%)\n",
      "    Action 1: 14849 (45.7%)\n",
      "    Action 2: 2926 (9.0%)\n",
      "    Action 3: 3335 (10.3%)\n",
      "  Player 1:\n",
      "    Action 0: 10172 (34.3%)\n",
      "    Action 1: 17462 (58.9%)\n",
      "    Action 2: 781 (2.6%)\n",
      "    Action 3: 1243 (4.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [10511.0, -10511.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.979 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.013\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -1.0511\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8522, 0.0269, 0.1209]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8056, 0.0399, 0.1545]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48901\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6216/10000 (62.2%)\n",
      "    Average reward: +0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3784/10000 (37.8%)\n",
      "    Average reward: -0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22704 (86.7%)\n",
      "    Action 1: 3468 (13.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2921 (12.9%)\n",
      "    Action 1: 17667 (77.7%)\n",
      "    Action 2: 583 (2.6%)\n",
      "    Action 3: 1558 (6.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [765.0, -765.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.564 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.663 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.614\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0765\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5008/50000 [03:36<20:28, 36.63it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 34182/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 34081/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6000/50000 [04:04<21:30, 34.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0008 â†’ 0.0008\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 40930/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 40881/2000000\n",
      "P1 SL Buffer Size:  40930\n",
      "P1 SL buffer distribution [14430. 21058.  2205.  3237.]\n",
      "P1 actions distribution [0.35255314 0.51448815 0.05387247 0.07908624]\n",
      "P2 SL Buffer Size:  40881\n",
      "P2 SL buffer distribution [13827. 21567.  2315.  3172.]\n",
      "P2 actions distribution [0.33822558 0.52755559 0.05662777 0.07759106]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.2406,  1.9991, -0.7791,  0.9238]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9741, 0.0092, 0.0167]])\n",
      "Player 1 Prediction: tensor([[ 1.9200,  2.3247, -1.3494,  1.0862]])\n",
      "Player 0 Prediction: tensor([[0.9907, 0.0000, 0.0093, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.1474, -0.1212, -2.8362,  0.8075]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7187, 0.0650, 0.2163]])\n",
      "Player 1 Prediction: tensor([[-4.6921, -5.2007, -3.2594,  0.3378]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60812\n",
      "Average episode length: 6.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5329/10000 (53.3%)\n",
      "    Average reward: -1.038\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4671/10000 (46.7%)\n",
      "    Average reward: +1.038\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9774 (33.2%)\n",
      "    Action 1: 16461 (55.9%)\n",
      "    Action 2: 1523 (5.2%)\n",
      "    Action 3: 1704 (5.8%)\n",
      "  Player 1:\n",
      "    Action 0: 11177 (35.7%)\n",
      "    Action 1: 14602 (46.6%)\n",
      "    Action 2: 2673 (8.5%)\n",
      "    Action 3: 2898 (9.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-10380.5, 10380.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.997 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -1.0380\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9868, 0.0057, 0.0075]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9593, 0.0193, 0.0214]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6000/50000 [04:19<21:30, 34.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51595\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5638/10000 (56.4%)\n",
      "    Average reward: -0.052\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4362/10000 (43.6%)\n",
      "    Average reward: +0.052\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3838 (16.0%)\n",
      "    Action 1: 18114 (75.7%)\n",
      "    Action 2: 807 (3.4%)\n",
      "    Action 3: 1155 (4.8%)\n",
      "  Player 1:\n",
      "    Action 0: 23151 (83.6%)\n",
      "    Action 1: 4530 (16.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-523.0, 523.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.727 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.643 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.685\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0523\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4340, 0.5527, 0.0133, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.9586,  2.7117, -1.4979,  1.1107]])\n",
      "Player 1 Prediction: tensor([[0.9926, 0.0000, 0.0074, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.7750,  2.0294, -3.1835,  1.5110]])\n",
      "Player 1 Prediction: tensor([[0.0605, 0.8196, 0.1199, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.9306,  1.5872, -4.7170,  1.2331]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 61730\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6030/10000 (60.3%)\n",
      "    Average reward: +0.970\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3970/10000 (39.7%)\n",
      "    Average reward: -0.970\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 11147 (35.1%)\n",
      "    Action 1: 14815 (46.6%)\n",
      "    Action 2: 2634 (8.3%)\n",
      "    Action 3: 3173 (10.0%)\n",
      "  Player 1:\n",
      "    Action 0: 10296 (34.4%)\n",
      "    Action 1: 16471 (55.0%)\n",
      "    Action 2: 1761 (5.9%)\n",
      "    Action 3: 1433 (4.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [9704.5, -9704.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.004 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.9705\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2329, 0.7262, 0.0409, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6523, 0.1253, 0.2224]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49755\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6325/10000 (63.2%)\n",
      "    Average reward: +0.103\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3675/10000 (36.8%)\n",
      "    Average reward: -0.103\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22535 (85.3%)\n",
      "    Action 1: 3875 (14.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3247 (13.9%)\n",
      "    Action 1: 17582 (75.3%)\n",
      "    Action 2: 985 (4.2%)\n",
      "    Action 3: 1531 (6.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1027.0, -1027.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.602 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.704 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.653\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1027\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7004/50000 [05:04<20:17, 35.32it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 47790/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 47924/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [05:32<19:57, 35.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0007 â†’ 0.0007\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 54298/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 54755/2000000\n",
      "P1 SL Buffer Size:  54298\n",
      "P1 SL buffer distribution [18899. 27432.  3390.  4577.]\n",
      "P1 actions distribution [0.3480607  0.50521198 0.06243324 0.08429408]\n",
      "P2 SL Buffer Size:  54755\n",
      "P2 SL buffer distribution [18849. 28053.  3461.  4392.]\n",
      "P2 actions distribution [0.34424253 0.51233677 0.06320884 0.08021185]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.7698,  1.4084, -0.8246,  0.7808]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9965, 0.0016, 0.0019]])\n",
      "Player 1 Prediction: tensor([[ 1.3218,  1.6517, -1.3739,  0.9729]])\n",
      "Player 0 Prediction: tensor([[9.9912e-01, 0.0000e+00, 8.8030e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-1.3871, -1.0063, -2.8738, -0.4257]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9085, 0.0185, 0.0730]])\n",
      "Player 1 Prediction: tensor([[-5.1415, -5.3449, -2.8496, -1.2436]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 61503\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5399/10000 (54.0%)\n",
      "    Average reward: -0.906\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4601/10000 (46.0%)\n",
      "    Average reward: +0.906\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9868 (32.8%)\n",
      "    Action 1: 16232 (54.0%)\n",
      "    Action 2: 1916 (6.4%)\n",
      "    Action 3: 2042 (6.8%)\n",
      "  Player 1:\n",
      "    Action 0: 11254 (35.8%)\n",
      "    Action 1: 14757 (46.9%)\n",
      "    Action 2: 2732 (8.7%)\n",
      "    Action 3: 2702 (8.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-9060.0, 9060.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.008 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.025\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.9060\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9626, 0.0132, 0.0243]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8689, 0.0677, 0.0633]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [05:49<19:57, 35.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52860\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5556/10000 (55.6%)\n",
      "    Average reward: -0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4444/10000 (44.4%)\n",
      "    Average reward: +0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4101 (16.7%)\n",
      "    Action 1: 18265 (74.4%)\n",
      "    Action 2: 914 (3.7%)\n",
      "    Action 3: 1271 (5.2%)\n",
      "  Player 1:\n",
      "    Action 0: 23312 (82.3%)\n",
      "    Action 1: 4997 (17.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1265.5, 1265.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.749 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.672 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.711\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1265\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.1751,  1.8581, -0.6878,  0.8137]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9200, 0.0166, 0.0634]])\n",
      "Player 0 Prediction: tensor([[ 1.3556,  2.2807, -1.4046,  0.8868]])\n",
      "Player 1 Prediction: tensor([[0.9884, 0.0000, 0.0116, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.1319,  1.7185, -3.1044,  1.1256]])\n",
      "Player 1 Prediction: tensor([[0.0673, 0.3643, 0.5685, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 62297\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6024/10000 (60.2%)\n",
      "    Average reward: +0.766\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3976/10000 (39.8%)\n",
      "    Average reward: -0.766\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10521 (33.3%)\n",
      "    Action 1: 15313 (48.5%)\n",
      "    Action 2: 2709 (8.6%)\n",
      "    Action 3: 3038 (9.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10947 (35.6%)\n",
      "    Action 1: 16111 (52.5%)\n",
      "    Action 2: 2154 (7.0%)\n",
      "    Action 3: 1504 (4.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [7665.0, -7665.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.027\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.7665\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9925, 0.0025, 0.0051]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9520, 0.0251, 0.0229]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 51386\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6318/10000 (63.2%)\n",
      "    Average reward: +0.101\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3682/10000 (36.8%)\n",
      "    Average reward: -0.101\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22663 (83.3%)\n",
      "    Action 1: 4532 (16.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3710 (15.3%)\n",
      "    Action 1: 17570 (72.6%)\n",
      "    Action 2: 1066 (4.4%)\n",
      "    Action 3: 1845 (7.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1010.5, -1010.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.650 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.750 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.700\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1011\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9006/50000 [06:33<19:03, 35.85it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 60804/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 61388/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9999/50000 [07:04<22:35, 29.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 67308/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 67865/2000000\n",
      "P1 SL Buffer Size:  67308\n",
      "P1 SL buffer distribution [23201. 34048.  4479.  5580.]\n",
      "P1 actions distribution [0.344699   0.50585369 0.06654484 0.08290248]\n",
      "P2 SL Buffer Size:  67865\n",
      "P2 SL buffer distribution [23543. 34620.  4635.  5067.]\n",
      "P2 actions distribution [0.34690931 0.51013041 0.06829736 0.07466293]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.6253, 0.3628, 0.0119, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.0818,  1.4611, -1.3857,  0.9214]])\n",
      "Player 0 Prediction: tensor([[0.9905, 0.0000, 0.0095, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-1.7289, -0.9601, -3.0185, -1.0058]])\n",
      "Player 0 Prediction: tensor([[0.0400, 0.2620, 0.6980, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9999/50000 [07:19<22:35, 29.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 62134\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4711/10000 (47.1%)\n",
      "    Average reward: -0.754\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5289/10000 (52.9%)\n",
      "    Average reward: +0.754\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10350 (33.4%)\n",
      "    Action 1: 15920 (51.4%)\n",
      "    Action 2: 2990 (9.7%)\n",
      "    Action 3: 1699 (5.5%)\n",
      "  Player 1:\n",
      "    Action 0: 10753 (34.5%)\n",
      "    Action 1: 17407 (55.8%)\n",
      "    Action 2: 2764 (8.9%)\n",
      "    Action 3: 251 (0.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7537.5, 7537.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.022 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.999 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.010\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7538\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9930, 0.0023, 0.0048]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9336, 0.0282, 0.0382]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53809\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5591/10000 (55.9%)\n",
      "    Average reward: -0.091\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4409/10000 (44.1%)\n",
      "    Average reward: +0.091\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4258 (17.0%)\n",
      "    Action 1: 18349 (73.3%)\n",
      "    Action 2: 1015 (4.1%)\n",
      "    Action 3: 1424 (5.7%)\n",
      "  Player 1:\n",
      "    Action 0: 23390 (81.3%)\n",
      "    Action 1: 5373 (18.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-912.5, 912.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.763 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.695 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.729\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0912\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.7030,  2.2800, -0.7199,  1.3195]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9694e-01, 9.2873e-04, 2.1284e-03]])\n",
      "Player 0 Prediction: tensor([[ 1.8818,  2.8419, -1.3887,  1.4030]])\n",
      "Player 1 Prediction: tensor([[9.9938e-01, 0.0000e+00, 6.2064e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.7608,  2.3243, -3.0445,  1.3955]])\n",
      "Player 1 Prediction: tensor([[0.1156, 0.7821, 0.1022, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.7753,  2.0009, -4.2022,  0.9783]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 62443\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6094/10000 (60.9%)\n",
      "    Average reward: +0.642\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3906/10000 (39.1%)\n",
      "    Average reward: -0.642\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10403 (33.1%)\n",
      "    Action 1: 16275 (51.7%)\n",
      "    Action 2: 2690 (8.6%)\n",
      "    Action 3: 2090 (6.6%)\n",
      "  Player 1:\n",
      "    Action 0: 11261 (36.3%)\n",
      "    Action 1: 15710 (50.7%)\n",
      "    Action 2: 2617 (8.4%)\n",
      "    Action 3: 1397 (4.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6416.5, -6416.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6417\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.4403, 0.5559, 0.0038, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7302, 0.0508, 0.2191]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52236\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6270/10000 (62.7%)\n",
      "    Average reward: +0.081\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3730/10000 (37.3%)\n",
      "    Average reward: -0.081\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22466 (81.8%)\n",
      "    Action 1: 5001 (18.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4060 (16.4%)\n",
      "    Action 1: 17446 (70.4%)\n",
      "    Action 2: 1145 (4.6%)\n",
      "    Action 3: 2118 (8.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [806.0, -806.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.685 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.784 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.734\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0806\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11005/50000 [08:08<20:19, 31.98it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0006 â†’ 0.0006\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 74043/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 74558/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11997/50000 [08:39<20:08, 31.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 80430/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 80694/2000000\n",
      "P1 SL Buffer Size:  80430\n",
      "P1 SL buffer distribution [27515. 41077.  5676.  6162.]\n",
      "P1 actions distribution [0.34209872 0.51071739 0.07057068 0.0766132 ]\n",
      "P2 SL Buffer Size:  80694\n",
      "P2 SL buffer distribution [27988. 41606.  5649.  5451.]\n",
      "P2 actions distribution [0.34684115 0.51560215 0.0700052  0.06755149]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.6945,  2.2155, -0.7903,  1.7832]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9940e-01, 2.7353e-04, 3.2922e-04]])\n",
      "Player 1 Prediction: tensor([[ 1.9663,  2.6828, -1.3803,  1.7628]])\n",
      "Player 0 Prediction: tensor([[9.9987e-01, 0.0000e+00, 1.3291e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.1946,  1.4772, -2.9850,  0.8222]])\n",
      "Player 0 Prediction: tensor([[0.2527, 0.6897, 0.0577, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 0.8553,  0.7563, -4.2184,  0.8381]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11997/50000 [08:49<20:08, 31.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59964\n",
      "Average episode length: 6.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4486/10000 (44.9%)\n",
      "    Average reward: -0.769\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5514/10000 (55.1%)\n",
      "    Average reward: +0.769\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10471 (34.8%)\n",
      "    Action 1: 14985 (49.8%)\n",
      "    Action 2: 3208 (10.7%)\n",
      "    Action 3: 1411 (4.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10241 (34.3%)\n",
      "    Action 1: 16170 (54.1%)\n",
      "    Action 2: 1892 (6.3%)\n",
      "    Action 3: 1586 (5.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-7693.5, 7693.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.009 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.020\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.7693\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9950, 0.0016, 0.0035]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9754, 0.0127, 0.0120]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54189\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5501/10000 (55.0%)\n",
      "    Average reward: -0.159\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4499/10000 (45.0%)\n",
      "    Average reward: +0.159\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4497 (17.8%)\n",
      "    Action 1: 18347 (72.6%)\n",
      "    Action 2: 1055 (4.2%)\n",
      "    Action 3: 1386 (5.5%)\n",
      "  Player 1:\n",
      "    Action 0: 23315 (80.7%)\n",
      "    Action 1: 5589 (19.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1594.0, 1594.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.779 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.708 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Average strategy entropy: 0.744\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1594\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.5210,  1.2983, -0.6883,  0.3308]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9792e-01, 6.0061e-04, 1.4755e-03]])\n",
      "Player 0 Prediction: tensor([[ 0.6813,  1.5264, -1.3833,  0.3142]])\n",
      "Player 1 Prediction: tensor([[9.9957e-01, 0.0000e+00, 4.3020e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-1.6585, -0.6728, -3.0108, -0.7968]])\n",
      "Player 1 Prediction: tensor([[0.1262, 0.7738, 0.1000, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-4.9281, -4.0672, -4.2428, -1.9417]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 62148\n",
      "Average episode length: 6.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6297/10000 (63.0%)\n",
      "    Average reward: +0.658\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3703/10000 (37.0%)\n",
      "    Average reward: -0.658\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10447 (33.4%)\n",
      "    Action 1: 17020 (54.4%)\n",
      "    Action 2: 2742 (8.8%)\n",
      "    Action 3: 1066 (3.4%)\n",
      "  Player 1:\n",
      "    Action 0: 11284 (36.5%)\n",
      "    Action 1: 15602 (50.5%)\n",
      "    Action 2: 3050 (9.9%)\n",
      "    Action 3: 937 (3.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6584.0, -6584.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.017\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6584\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9792e-01, 6.0061e-04, 1.4755e-03]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9397, 0.0197, 0.0406]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52426\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6334/10000 (63.3%)\n",
      "    Average reward: +0.087\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3666/10000 (36.7%)\n",
      "    Average reward: -0.087\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22292 (81.1%)\n",
      "    Action 1: 5196 (18.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4130 (16.6%)\n",
      "    Action 1: 17282 (69.3%)\n",
      "    Action 2: 1352 (5.4%)\n",
      "    Action 3: 2174 (8.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [875.0, -875.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.699 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.796 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.748\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0875\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13007/50000 [09:43<17:56, 34.38it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 86889/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 87031/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [10:12<19:21, 31.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 93223/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 93227/2000000\n",
      "P1 SL Buffer Size:  93223\n",
      "P1 SL buffer distribution [31766. 48037.  6679.  6741.]\n",
      "P1 actions distribution [0.34075282 0.51529129 0.07164541 0.07231048]\n",
      "P2 SL Buffer Size:  93227\n",
      "P2 SL buffer distribution [32204. 48360.  6526.  6137.]\n",
      "P2 actions distribution [0.34543641 0.51873384 0.07000118 0.06582857]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7211, 0.2660, 0.0129, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.8726,  2.0783, -1.0392,  1.8645]])\n",
      "Player 0 Prediction: tensor([[0.0096, 0.9806, 0.0097, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 2.1706,  1.7169, -2.2007,  1.5396]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5132, 0.0272, 0.4595]])\n",
      "Player 1 Prediction: tensor([[ 1.1544,  1.5435, -2.9772,  0.8328]])\n",
      "Player 0 Prediction: tensor([[0.8291, 0.0000, 0.1709, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59523\n",
      "Average episode length: 6.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4839/10000 (48.4%)\n",
      "    Average reward: -0.653\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5161/10000 (51.6%)\n",
      "    Average reward: +0.653\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10817 (36.1%)\n",
      "    Action 1: 14776 (49.3%)\n",
      "    Action 2: 3001 (10.0%)\n",
      "    Action 3: 1375 (4.6%)\n",
      "  Player 1:\n",
      "    Action 0: 9790 (33.1%)\n",
      "    Action 1: 15800 (53.5%)\n",
      "    Action 2: 2323 (7.9%)\n",
      "    Action 3: 1641 (5.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6525.5, 6525.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.022\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6525\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9951, 0.0015, 0.0034]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9726, 0.0152, 0.0122]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [10:29<19:21, 31.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54707\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5483/10000 (54.8%)\n",
      "    Average reward: -0.114\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4517/10000 (45.2%)\n",
      "    Average reward: +0.114\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4626 (18.0%)\n",
      "    Action 1: 18158 (70.8%)\n",
      "    Action 2: 1253 (4.9%)\n",
      "    Action 3: 1615 (6.3%)\n",
      "  Player 1:\n",
      "    Action 0: 23130 (79.6%)\n",
      "    Action 1: 5925 (20.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1136.0, 1136.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.798 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.730 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.764\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1136\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[8.7010e-01, 1.2954e-01, 3.5969e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.5188,  1.3551, -1.4028,  0.1164]])\n",
      "Player 1 Prediction: tensor([[9.9967e-01, 0.0000e+00, 3.2726e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.7121,  1.2662, -3.0656,  0.3228]])\n",
      "Player 1 Prediction: tensor([[0.4337, 0.5324, 0.0339, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60749\n",
      "Average episode length: 6.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6476/10000 (64.8%)\n",
      "    Average reward: +0.635\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3524/10000 (35.2%)\n",
      "    Average reward: -0.635\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9600 (31.7%)\n",
      "    Action 1: 16626 (54.9%)\n",
      "    Action 2: 2068 (6.8%)\n",
      "    Action 3: 1972 (6.5%)\n",
      "  Player 1:\n",
      "    Action 0: 11703 (38.4%)\n",
      "    Action 1: 14780 (48.5%)\n",
      "    Action 2: 3205 (10.5%)\n",
      "    Action 3: 795 (2.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [6355.0, -6355.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.000 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.018\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.6355\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8821, 0.0137, 0.1042]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3442, 0.2120, 0.4438]])\n",
      "Player 1 Prediction: tensor([[0.4841, 0.1236, 0.3923, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52463\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6309/10000 (63.1%)\n",
      "    Average reward: +0.093\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3691/10000 (36.9%)\n",
      "    Average reward: -0.093\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 22027 (80.3%)\n",
      "    Action 1: 5389 (19.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4042 (16.1%)\n",
      "    Action 1: 17072 (68.2%)\n",
      "    Action 2: 1573 (6.3%)\n",
      "    Action 3: 2360 (9.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [927.0, -927.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.715 (max=1.0 for random)\n",
      "    â†’ Strongly prefers Heads\n",
      "  Player 1 strategy entropy: 0.802 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.758\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0927\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15004/50000 [11:16<19:07, 30.49it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 99541/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 99348/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15999/50000 [11:49<18:58, 29.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 105987/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 105565/2000000\n",
      "P1 SL Buffer Size:  105987\n",
      "P1 SL buffer distribution [35883. 54831.  7577.  7696.]\n",
      "P1 actions distribution [0.33856039 0.51733703 0.0714899  0.07261268]\n",
      "P2 SL Buffer Size:  105565\n",
      "P2 SL buffer distribution [36417. 54944.  7338.  6866.]\n",
      "P2 actions distribution [0.34497229 0.52047554 0.06951168 0.0650405 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.8707, 0.1278, 0.0015, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.4644,  2.0978, -0.7583,  1.5185]])\n",
      "Player 0 Prediction: tensor([[0.0309, 0.9663, 0.0027, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.7686,  2.1568, -1.9482,  1.0331]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9147, 0.0050, 0.0803]])\n",
      "Player 1 Prediction: tensor([[-0.1402, -1.1386, -3.2847,  1.9848]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 61176\n",
      "Average episode length: 6.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4464/10000 (44.6%)\n",
      "    Average reward: -0.572\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5536/10000 (55.4%)\n",
      "    Average reward: +0.572\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 12453 (40.1%)\n",
      "    Action 1: 14291 (46.1%)\n",
      "    Action 2: 3361 (10.8%)\n",
      "    Action 3: 926 (3.0%)\n",
      "  Player 1:\n",
      "    Action 0: 10254 (34.0%)\n",
      "    Action 1: 16734 (55.5%)\n",
      "    Action 2: 1497 (5.0%)\n",
      "    Action 3: 1660 (5.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5715.0, 5715.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.044 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.022\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5715\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8707, 0.1278, 0.0015, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0309, 0.9663, 0.0027, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5221, 0.0120, 0.4659]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 15999/50000 [12:09<18:58, 29.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54695\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5489/10000 (54.9%)\n",
      "    Average reward: -0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4511/10000 (45.1%)\n",
      "    Average reward: +0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4792 (18.7%)\n",
      "    Action 1: 18067 (70.4%)\n",
      "    Action 2: 1271 (5.0%)\n",
      "    Action 3: 1541 (6.0%)\n",
      "  Player 1:\n",
      "    Action 0: 22992 (79.2%)\n",
      "    Action 1: 6032 (20.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1076.5, 1076.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.809 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.737 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.773\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.1076\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[8.8398e-01, 1.1576e-01, 2.6376e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.7254,  1.1804, -0.9514,  0.0630]])\n",
      "Player 1 Prediction: tensor([[3.2757e-02, 9.6649e-01, 7.5024e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.6727,  1.1181, -2.1864,  0.0588]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9815, 0.0012, 0.0173]])\n",
      "Player 0 Prediction: tensor([[-5.0090, -4.5646, -3.0592, -2.7541]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60407\n",
      "Average episode length: 6.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6295/10000 (62.9%)\n",
      "    Average reward: +0.473\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3705/10000 (37.0%)\n",
      "    Average reward: -0.473\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9684 (32.1%)\n",
      "    Action 1: 16230 (53.8%)\n",
      "    Action 2: 2159 (7.2%)\n",
      "    Action 3: 2068 (6.9%)\n",
      "  Player 1:\n",
      "    Action 0: 11599 (38.3%)\n",
      "    Action 1: 14777 (48.8%)\n",
      "    Action 2: 2964 (9.8%)\n",
      "    Action 3: 926 (3.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4735.0, -4735.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4735\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[8.8398e-01, 1.1576e-01, 2.6376e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[3.2757e-02, 9.6649e-01, 7.5024e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9482e-01, 6.8940e-04, 4.4883e-03]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52528\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6244/10000 (62.4%)\n",
      "    Average reward: -0.009\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3756/10000 (37.6%)\n",
      "    Average reward: +0.009\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21896 (79.7%)\n",
      "    Action 1: 5571 (20.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4101 (16.4%)\n",
      "    Action 1: 16879 (67.4%)\n",
      "    Action 2: 1601 (6.4%)\n",
      "    Action 3: 2480 (9.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-94.0, 94.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.728 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.811 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.769\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: 0.0094\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17006/50000 [12:54<18:03, 30.44it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0005 â†’ 0.0005\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 112235/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 111742/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17998/50000 [13:28<18:19, 29.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 118544/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 118063/2000000\n",
      "P1 SL Buffer Size:  118544\n",
      "P1 SL buffer distribution [39984. 61316.  8629.  8615.]\n",
      "P1 actions distribution [0.33729248 0.51724254 0.07279154 0.07267344]\n",
      "P2 SL Buffer Size:  118063\n",
      "P2 SL buffer distribution [40826. 61384.  8181.  7672.]\n",
      "P2 actions distribution [0.34579843 0.5199258  0.06929351 0.06498226]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.7409,  0.1502, -0.8726,  0.3022]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9874, 0.0020, 0.0106]])\n",
      "Player 1 Prediction: tensor([[ 0.6550,  0.8624, -1.3707,  0.6970]])\n",
      "Player 0 Prediction: tensor([[9.9912e-01, 0.0000e+00, 8.8299e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.8056, -0.9328, -2.8969, -0.9762]])\n",
      "Player 0 Prediction: tensor([[0.5355, 0.2773, 0.1872, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17998/50000 [13:39<18:19, 29.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58726\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4881/10000 (48.8%)\n",
      "    Average reward: -0.668\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5119/10000 (51.2%)\n",
      "    Average reward: +0.668\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10387 (35.4%)\n",
      "    Action 1: 14567 (49.6%)\n",
      "    Action 2: 2948 (10.0%)\n",
      "    Action 3: 1480 (5.0%)\n",
      "  Player 1:\n",
      "    Action 0: 10303 (35.1%)\n",
      "    Action 1: 15103 (51.5%)\n",
      "    Action 2: 2264 (7.7%)\n",
      "    Action 3: 1674 (5.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6680.0, 6680.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6680\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8502, 0.0144, 0.1354]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7120, 0.1204, 0.1677]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54685\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5576/10000 (55.8%)\n",
      "    Average reward: -0.032\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4424/10000 (44.2%)\n",
      "    Average reward: +0.032\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4755 (18.5%)\n",
      "    Action 1: 17921 (69.9%)\n",
      "    Action 2: 1242 (4.8%)\n",
      "    Action 3: 1719 (6.7%)\n",
      "  Player 1:\n",
      "    Action 0: 22939 (79.0%)\n",
      "    Action 1: 6109 (21.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-318.5, 318.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.812 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.742 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.777\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0319\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.2869,  1.2021, -0.7052, -0.1071]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6698, 0.0206, 0.3096]])\n",
      "Player 0 Prediction: tensor([[ 0.3755,  1.2704, -1.4042, -0.2032]])\n",
      "Player 1 Prediction: tensor([[0.9937, 0.0000, 0.0063, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 5.0181,  4.5353, -2.7786,  2.3192]])\n",
      "Player 1 Prediction: tensor([[0.0355, 0.1619, 0.8026, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 59490\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6030/10000 (60.3%)\n",
      "    Average reward: +0.520\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3970/10000 (39.7%)\n",
      "    Average reward: -0.520\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9817 (32.8%)\n",
      "    Action 1: 15463 (51.6%)\n",
      "    Action 2: 2680 (8.9%)\n",
      "    Action 3: 1994 (6.7%)\n",
      "  Player 1:\n",
      "    Action 0: 11262 (38.1%)\n",
      "    Action 1: 14552 (49.3%)\n",
      "    Action 2: 2630 (8.9%)\n",
      "    Action 3: 1092 (3.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5195.5, -5195.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.020 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.027\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5195\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6653, 0.3130, 0.0218, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3357, 0.1664, 0.4979]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52430\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6342/10000 (63.4%)\n",
      "    Average reward: +0.126\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3658/10000 (36.6%)\n",
      "    Average reward: -0.126\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21676 (79.0%)\n",
      "    Action 1: 5745 (21.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4141 (16.6%)\n",
      "    Action 1: 16652 (66.6%)\n",
      "    Action 2: 1572 (6.3%)\n",
      "    Action 3: 2644 (10.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1262.5, -1262.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.741 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.820 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.780\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.1263\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19003/50000 [14:32<17:06, 30.19it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 124734/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 124324/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [15:06<17:49, 28.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 131229/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 130870/2000000\n",
      "P1 SL Buffer Size:  131229\n",
      "P1 SL buffer distribution [44026. 67869.  9838.  9496.]\n",
      "P1 actions distribution [0.33548987 0.51717989 0.07496819 0.07236205]\n",
      "P2 SL Buffer Size:  130870\n",
      "P2 SL buffer distribution [45337. 67497.  9376.  8660.]\n",
      "P2 actions distribution [0.34642775 0.51575609 0.07164362 0.06617254]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 2.3785,  1.9502, -0.8140,  1.7653]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7384, 0.0218, 0.2398]])\n",
      "Player 1 Prediction: tensor([[ 1.6728,  2.4077, -1.3722,  1.6915]])\n",
      "Player 0 Prediction: tensor([[0.9947, 0.0000, 0.0053, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.2915,  1.3030, -3.0058,  0.7730]])\n",
      "Player 0 Prediction: tensor([[0.0304, 0.1700, 0.7996, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [15:19<17:49, 28.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58881\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5185/10000 (51.8%)\n",
      "    Average reward: -0.603\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4815/10000 (48.1%)\n",
      "    Average reward: +0.603\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10022 (34.2%)\n",
      "    Action 1: 14917 (50.9%)\n",
      "    Action 2: 2647 (9.0%)\n",
      "    Action 3: 1719 (5.9%)\n",
      "  Player 1:\n",
      "    Action 0: 10413 (35.2%)\n",
      "    Action 1: 14171 (47.9%)\n",
      "    Action 2: 2607 (8.8%)\n",
      "    Action 3: 2385 (8.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6030.0, 6030.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.032\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6030\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8854, 0.1132, 0.0014, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0266, 0.9710, 0.0023, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9617, 0.0026, 0.0357]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54892\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5556/10000 (55.6%)\n",
      "    Average reward: -0.018\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4444/10000 (44.4%)\n",
      "    Average reward: +0.018\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4845 (18.8%)\n",
      "    Action 1: 17657 (68.6%)\n",
      "    Action 2: 1288 (5.0%)\n",
      "    Action 3: 1950 (7.6%)\n",
      "  Player 1:\n",
      "    Action 0: 22699 (77.9%)\n",
      "    Action 1: 6453 (22.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-175.5, 175.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.827 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.763 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.795\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0175\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.1673e-01, 8.3167e-02, 1.0281e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.8894,  2.1593, -0.7477,  1.7718]])\n",
      "Player 1 Prediction: tensor([[2.9518e-02, 9.7010e-01, 3.8621e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 2.2708,  2.2272, -2.1882,  1.7426]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8677e-01, 6.6873e-04, 1.2558e-02]])\n",
      "Player 0 Prediction: tensor([[ 0.0999,  0.7884, -3.1733,  0.5236]])\n",
      "Player 1 Prediction: tensor([[0.9861, 0.0000, 0.0139, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58783\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6056/10000 (60.6%)\n",
      "    Average reward: +0.517\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3944/10000 (39.4%)\n",
      "    Average reward: -0.517\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9743 (33.0%)\n",
      "    Action 1: 15171 (51.4%)\n",
      "    Action 2: 2629 (8.9%)\n",
      "    Action 3: 1987 (6.7%)\n",
      "  Player 1:\n",
      "    Action 0: 11149 (38.1%)\n",
      "    Action 1: 14114 (48.2%)\n",
      "    Action 2: 2614 (8.9%)\n",
      "    Action 3: 1376 (4.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5169.5, -5169.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.030\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5170\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9759, 0.0017, 0.0224]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8599, 0.0288, 0.1113]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52598\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6259/10000 (62.6%)\n",
      "    Average reward: +0.045\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3741/10000 (37.4%)\n",
      "    Average reward: -0.045\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21412 (78.0%)\n",
      "    Action 1: 6034 (22.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4328 (17.2%)\n",
      "    Action 1: 16444 (65.4%)\n",
      "    Action 2: 1633 (6.5%)\n",
      "    Action 3: 2747 (10.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [455.0, -455.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.760 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.838 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.799\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 1 average reward: -0.0455\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21006/50000 [16:14<16:44, 28.88it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 137503/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 137241/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21999/50000 [16:49<16:34, 28.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 143752/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 143885/2000000\n",
      "P1 SL Buffer Size:  143752\n",
      "P1 SL buffer distribution [48133. 74242. 11045. 10332.]\n",
      "P1 actions distribution [0.3348336  0.5164589  0.07683371 0.07187378]\n",
      "P2 SL Buffer Size:  143885\n",
      "P2 SL buffer distribution [49982. 73588. 10620.  9695.]\n",
      "P2 actions distribution [0.34737464 0.51143622 0.07380894 0.0673802 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.2310e-01, 7.6856e-02, 4.4493e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.1479,  2.0118, -0.7350,  1.1370]])\n",
      "Player 0 Prediction: tensor([[6.2051e-02, 9.3784e-01, 1.0541e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.6340,  2.0307, -1.8870,  0.8869]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9133e-01, 3.6015e-04, 8.3140e-03]])\n",
      "Player 1 Prediction: tensor([[-4.7820, -3.7547, -3.0573, -1.2504]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21999/50000 [16:59<16:34, 28.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58609\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5300/10000 (53.0%)\n",
      "    Average reward: -0.527\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4700/10000 (47.0%)\n",
      "    Average reward: +0.527\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9969 (34.3%)\n",
      "    Action 1: 14851 (51.1%)\n",
      "    Action 2: 2512 (8.6%)\n",
      "    Action 3: 1743 (6.0%)\n",
      "  Player 1:\n",
      "    Action 0: 10385 (35.2%)\n",
      "    Action 1: 13927 (47.2%)\n",
      "    Action 2: 2758 (9.3%)\n",
      "    Action 3: 2464 (8.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5265.5, 5265.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.025 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5265\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8515, 0.1331, 0.0154, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0074, 0.9813, 0.0113, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2036, 0.0184, 0.7780]])\n",
      "Player 0 Prediction: tensor([[0.0562, 0.1168, 0.8270, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54611\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5519/10000 (55.2%)\n",
      "    Average reward: -0.031\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4481/10000 (44.8%)\n",
      "    Average reward: +0.031\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4808 (18.8%)\n",
      "    Action 1: 17647 (69.1%)\n",
      "    Action 2: 1260 (4.9%)\n",
      "    Action 3: 1836 (7.2%)\n",
      "  Player 1:\n",
      "    Action 0: 22719 (78.2%)\n",
      "    Action 1: 6341 (21.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-306.0, 306.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.822 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.757 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.790\n",
      "  âš ï¸  Players may be using deterministic strategies\n",
      "   Player 0 average reward: -0.0306\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.1071, -0.2669, -0.8691, -0.0851]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4818, 0.0289, 0.4893]])\n",
      "Player 0 Prediction: tensor([[-2.3129, -2.4521, -1.1138, -1.2341]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57976\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5822/10000 (58.2%)\n",
      "    Average reward: +0.525\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4178/10000 (41.8%)\n",
      "    Average reward: -0.525\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9584 (32.7%)\n",
      "    Action 1: 14605 (49.8%)\n",
      "    Action 2: 3142 (10.7%)\n",
      "    Action 3: 2005 (6.8%)\n",
      "  Player 1:\n",
      "    Action 0: 10772 (37.6%)\n",
      "    Action 1: 13873 (48.4%)\n",
      "    Action 2: 2484 (8.7%)\n",
      "    Action 3: 1511 (5.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5250.5, -5250.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.028 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.033\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5251\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9769, 0.0015, 0.0217]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3696, 0.0895, 0.5409]])\n",
      "Player 1 Prediction: tensor([[0.4536, 0.0758, 0.4705, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52381\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6374/10000 (63.7%)\n",
      "    Average reward: +0.088\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3626/10000 (36.3%)\n",
      "    Average reward: -0.088\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21109 (77.4%)\n",
      "    Action 1: 6150 (22.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4256 (16.9%)\n",
      "    Action 1: 16130 (64.2%)\n",
      "    Action 2: 1780 (7.1%)\n",
      "    Action 3: 2956 (11.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [876.0, -876.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.770 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.844 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.807\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0876\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23004/50000 [17:55<15:39, 28.72it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 150114/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 150351/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23998/50000 [18:31<15:47, 27.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 156454/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 156643/2000000\n",
      "P1 SL Buffer Size:  156454\n",
      "P1 SL buffer distribution [52187. 80357. 12504. 11406.]\n",
      "P1 actions distribution [0.3335613  0.51361423 0.07992125 0.07290322]\n",
      "P2 SL Buffer Size:  156643\n",
      "P2 SL buffer distribution [54439. 79231. 12077. 10896.]\n",
      "P2 actions distribution [0.34753548 0.5058062  0.07709888 0.06955944]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.8449,  1.9748, -0.4214,  1.2933]])\n",
      "Player 0 Prediction: tensor([[0.0065, 0.9814, 0.0121, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.6359,  1.9602, -1.8238,  0.8802]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6004, 0.0222, 0.3774]])\n",
      "Player 1 Prediction: tensor([[ 4.0473,  4.7046, -2.6384,  1.9888]])\n",
      "Player 0 Prediction: tensor([[0.0297, 0.1595, 0.8108, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 6.3611,  6.5483, -4.2512,  3.0505]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58291\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5611/10000 (56.1%)\n",
      "    Average reward: -0.642\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4389/10000 (43.9%)\n",
      "    Average reward: +0.642\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9282 (32.6%)\n",
      "    Action 1: 15041 (52.8%)\n",
      "    Action 2: 2308 (8.1%)\n",
      "    Action 3: 1882 (6.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10520 (35.3%)\n",
      "    Action 1: 12641 (42.5%)\n",
      "    Action 2: 3465 (11.6%)\n",
      "    Action 3: 3152 (10.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6421.0, 6421.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.014 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6421\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8606, 0.1238, 0.0156, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0065, 0.9814, 0.0121, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1748, 0.0222, 0.8031]])\n",
      "Player 0 Prediction: tensor([[0.0519, 0.1007, 0.8474, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23998/50000 [18:50<15:47, 27.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54597\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5475/10000 (54.8%)\n",
      "    Average reward: -0.072\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4525/10000 (45.2%)\n",
      "    Average reward: +0.072\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4898 (19.1%)\n",
      "    Action 1: 17407 (67.9%)\n",
      "    Action 2: 1381 (5.4%)\n",
      "    Action 3: 1959 (7.6%)\n",
      "  Player 1:\n",
      "    Action 0: 22373 (77.3%)\n",
      "    Action 1: 6579 (22.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-721.0, 721.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.836 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.773 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.804\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0721\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7615, 0.2242, 0.0143, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 1.6993,  2.0004, -0.6669,  1.7104]])\n",
      "Player 1 Prediction: tensor([[0.0031, 0.9848, 0.0121, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 2.1383,  2.0997, -2.1512,  1.7734]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5605, 0.0626, 0.3769]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58117\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5695/10000 (57.0%)\n",
      "    Average reward: +0.561\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4305/10000 (43.0%)\n",
      "    Average reward: -0.561\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9649 (32.7%)\n",
      "    Action 1: 13676 (46.4%)\n",
      "    Action 2: 3429 (11.6%)\n",
      "    Action 3: 2745 (9.3%)\n",
      "  Player 1:\n",
      "    Action 0: 10476 (36.6%)\n",
      "    Action 1: 14062 (49.1%)\n",
      "    Action 2: 2380 (8.3%)\n",
      "    Action 3: 1700 (5.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5611.0, -5611.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5611\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9770, 0.0015, 0.0216]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3346, 0.0897, 0.5756]])\n",
      "Player 1 Prediction: tensor([[0.4039, 0.0627, 0.5333, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52476\n",
      "Average episode length: 5.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6220/10000 (62.2%)\n",
      "    Average reward: -0.022\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3780/10000 (37.8%)\n",
      "    Average reward: +0.022\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 21094 (77.2%)\n",
      "    Action 1: 6232 (22.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4170 (16.6%)\n",
      "    Action 1: 16095 (64.0%)\n",
      "    Action 2: 1855 (7.4%)\n",
      "    Action 3: 3030 (12.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-221.0, 221.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.775 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.842 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.808\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0221\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25004/50000 [19:45<16:05, 25.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 162585/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 163161/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [20:24<15:18, 26.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 169244/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 169740/2000000\n",
      "P1 SL Buffer Size:  169244\n",
      "P1 SL buffer distribution [56266. 86324. 13949. 12705.]\n",
      "P1 actions distribution [0.33245492 0.51005649 0.08241947 0.07506913]\n",
      "P2 SL Buffer Size:  169740\n",
      "P2 SL buffer distribution [59244. 85150. 13538. 11808.]\n",
      "P2 actions distribution [0.34902793 0.50164958 0.07975728 0.06956522]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.9025, 0.0965, 0.0010, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.5424, -0.0938, -0.8855, -0.1064]])\n",
      "Player 0 Prediction: tensor([[0.0222, 0.9755, 0.0023, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-0.1713, -0.8772, -2.0447, -0.2883]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9678, 0.0031, 0.0291]])\n",
      "Player 1 Prediction: tensor([[-4.6246, -5.8254, -2.9433, -1.4562]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [20:40<15:18, 26.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 60930\n",
      "Average episode length: 6.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5580/10000 (55.8%)\n",
      "    Average reward: -0.516\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4420/10000 (44.2%)\n",
      "    Average reward: +0.516\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9221 (30.9%)\n",
      "    Action 1: 16108 (53.9%)\n",
      "    Action 2: 2402 (8.0%)\n",
      "    Action 3: 2139 (7.2%)\n",
      "  Player 1:\n",
      "    Action 0: 11843 (38.1%)\n",
      "    Action 1: 14040 (45.2%)\n",
      "    Action 2: 3373 (10.9%)\n",
      "    Action 3: 1804 (5.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5158.5, 5158.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.004 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.026\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5159\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8749, 0.1100, 0.0151, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0060, 0.9794, 0.0145, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5626, 0.0291, 0.4083]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54363\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5498/10000 (55.0%)\n",
      "    Average reward: +0.029\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4502/10000 (45.0%)\n",
      "    Average reward: -0.029\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4941 (19.3%)\n",
      "    Action 1: 17093 (66.9%)\n",
      "    Action 2: 1495 (5.8%)\n",
      "    Action 3: 2037 (8.0%)\n",
      "  Player 1:\n",
      "    Action 0: 22152 (76.9%)\n",
      "    Action 1: 6645 (23.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [288.0, -288.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.847 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.779 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.813\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0288\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.1256,  1.8949, -0.6515,  1.4794]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3908, 0.0370, 0.5722]])\n",
      "Player 0 Prediction: tensor([[ 1.5413,  2.2639, -1.2234,  1.5609]])\n",
      "Player 1 Prediction: tensor([[0.9932, 0.0000, 0.0068, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 4.7942,  4.1703, -2.8312,  2.8164]])\n",
      "Player 1 Prediction: tensor([[0.0774, 0.1677, 0.7549, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57855\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5736/10000 (57.4%)\n",
      "    Average reward: +0.485\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4264/10000 (42.6%)\n",
      "    Average reward: -0.485\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10059 (33.9%)\n",
      "    Action 1: 12546 (42.3%)\n",
      "    Action 2: 3314 (11.2%)\n",
      "    Action 3: 3739 (12.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10118 (35.9%)\n",
      "    Action 1: 14064 (49.9%)\n",
      "    Action 2: 2046 (7.3%)\n",
      "    Action 3: 1969 (7.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4854.0, -4854.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.043\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4854\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9798, 0.0012, 0.0190]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8170, 0.0192, 0.1638]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52592\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6235/10000 (62.4%)\n",
      "    Average reward: -0.012\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3765/10000 (37.6%)\n",
      "    Average reward: +0.012\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20817 (76.4%)\n",
      "    Action 1: 6436 (23.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4215 (16.6%)\n",
      "    Action 1: 15846 (62.5%)\n",
      "    Action 2: 2060 (8.1%)\n",
      "    Action 3: 3218 (12.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-117.0, 117.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.789 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.854 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.821\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0117\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27003/50000 [21:47<15:57, 24.03it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 175443/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 176536/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28000/50000 [22:33<14:04, 26.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 181983/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 182862/2000000\n",
      "P1 SL Buffer Size:  181983\n",
      "P1 SL buffer distribution [60263. 92097. 15402. 14221.]\n",
      "P1 actions distribution [0.33114632 0.50607474 0.08463428 0.07814466]\n",
      "P2 SL Buffer Size:  182862\n",
      "P2 SL buffer distribution [64110. 90760. 14998. 12994.]\n",
      "P2 actions distribution [0.35059225 0.49633057 0.08201813 0.07105905]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.3674e-01, 6.3239e-02, 2.3671e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.3221,  1.9510, -0.8465,  1.5260]])\n",
      "Player 0 Prediction: tensor([[4.9341e-02, 9.5056e-01, 1.0156e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 2.0800,  1.5167, -2.0635,  1.6596]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.8820e-01, 3.5033e-04, 1.1452e-02]])\n",
      "Player 1 Prediction: tensor([[ 0.0573,  1.0952, -3.0862,  0.8240]])\n",
      "Player 0 Prediction: tensor([[0.9724, 0.0000, 0.0276, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28000/50000 [22:50<14:04, 26.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57764\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5557/10000 (55.6%)\n",
      "    Average reward: -0.604\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4443/10000 (44.4%)\n",
      "    Average reward: +0.604\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8843 (31.4%)\n",
      "    Action 1: 14878 (52.9%)\n",
      "    Action 2: 2249 (8.0%)\n",
      "    Action 3: 2175 (7.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10756 (36.3%)\n",
      "    Action 1: 11864 (40.1%)\n",
      "    Action 2: 3286 (11.1%)\n",
      "    Action 3: 3713 (12.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-6035.5, 6035.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.035\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.6036\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[9.0529e-01, 9.3886e-02, 8.2166e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0202, 0.9775, 0.0023, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5985, 0.0170, 0.3844]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54156\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5500/10000 (55.0%)\n",
      "    Average reward: +0.077\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4500/10000 (45.0%)\n",
      "    Average reward: -0.077\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4976 (19.5%)\n",
      "    Action 1: 16964 (66.4%)\n",
      "    Action 2: 1594 (6.2%)\n",
      "    Action 3: 2022 (7.9%)\n",
      "  Player 1:\n",
      "    Action 0: 21903 (76.6%)\n",
      "    Action 1: 6697 (23.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [771.5, -771.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.852 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.785 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.819\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0771\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.4283e-01, 5.7153e-02, 1.6821e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.4705,  0.8951, -0.7435, -0.4750]])\n",
      "Player 1 Prediction: tensor([[1.8931e-02, 9.8089e-01, 1.8240e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.6197,  0.8667, -2.0588, -0.1475]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9747e-01, 2.6368e-04, 2.2648e-03]])\n",
      "Player 0 Prediction: tensor([[ 5.6580,  4.8558, -2.8931,  2.1947]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58711\n",
      "Average episode length: 5.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5715/10000 (57.1%)\n",
      "    Average reward: +0.458\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4285/10000 (42.9%)\n",
      "    Average reward: -0.458\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9105 (30.8%)\n",
      "    Action 1: 13621 (46.0%)\n",
      "    Action 2: 3399 (11.5%)\n",
      "    Action 3: 3478 (11.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10871 (37.3%)\n",
      "    Action 1: 14060 (48.3%)\n",
      "    Action 2: 2305 (7.9%)\n",
      "    Action 3: 1872 (6.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4584.0, -4584.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.038 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4584\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4014, 0.0352, 0.5634]])\n",
      "Player 1 Prediction: tensor([[0.4616, 0.3722, 0.1662, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52777\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6223/10000 (62.2%)\n",
      "    Average reward: -0.040\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3777/10000 (37.8%)\n",
      "    Average reward: +0.040\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20678 (75.8%)\n",
      "    Action 1: 6594 (24.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4311 (16.9%)\n",
      "    Action 1: 15771 (61.8%)\n",
      "    Action 2: 2104 (8.2%)\n",
      "    Action 3: 3319 (13.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-402.5, 402.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.798 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.862 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.830\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0403\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29005/50000 [23:49<12:47, 27.35it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0004 â†’ 0.0004\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 188477/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 189331/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29998/50000 [24:27<12:31, 26.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 194975/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 195698/2000000\n",
      "P1 SL Buffer Size:  194975\n",
      "P1 SL buffer distribution [64439. 97830. 16886. 15820.]\n",
      "P1 actions distribution [0.33049878 0.50175664 0.08660598 0.08113861]\n",
      "P2 SL Buffer Size:  195698\n",
      "P2 SL buffer distribution [68668. 95907. 16452. 14671.]\n",
      "P2 actions distribution [0.35088759 0.49007655 0.08406831 0.07496755]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.3328, -0.1263, -0.6586,  0.1504]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9766, 0.0090, 0.0143]])\n",
      "Player 1 Prediction: tensor([[ 0.1952,  0.3103, -0.9729,  0.4514]])\n",
      "Player 0 Prediction: tensor([[9.9955e-01, 0.0000e+00, 4.5291e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 5.6763,  4.7975, -2.9083,  3.7638]])\n",
      "Player 0 Prediction: tensor([[0.0633, 0.1875, 0.7492, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 5.8224,  5.1892, -4.4712,  3.7244]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29998/50000 [24:40<12:31, 26.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57711\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5517/10000 (55.2%)\n",
      "    Average reward: -0.533\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4483/10000 (44.8%)\n",
      "    Average reward: +0.533\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8970 (31.7%)\n",
      "    Action 1: 14635 (51.7%)\n",
      "    Action 2: 2340 (8.3%)\n",
      "    Action 3: 2369 (8.4%)\n",
      "  Player 1:\n",
      "    Action 0: 10479 (35.6%)\n",
      "    Action 1: 11730 (39.9%)\n",
      "    Action 2: 3320 (11.3%)\n",
      "    Action 3: 3868 (13.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-5327.5, 5327.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.017 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.5327\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9766, 0.0090, 0.0143]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8113, 0.1010, 0.0877]])\n",
      "Player 0 Prediction: tensor([[0.1473, 0.4269, 0.4259, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54107\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5464/10000 (54.6%)\n",
      "    Average reward: +0.133\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4536/10000 (45.4%)\n",
      "    Average reward: -0.133\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5000 (19.5%)\n",
      "    Action 1: 16587 (64.7%)\n",
      "    Action 2: 1817 (7.1%)\n",
      "    Action 3: 2225 (8.7%)\n",
      "  Player 1:\n",
      "    Action 0: 21610 (75.9%)\n",
      "    Action 1: 6868 (24.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1328.5, -1328.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.866 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.797 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.832\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1328\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.0854, -0.2603, -0.7051, -0.2340]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9195e-01, 2.9862e-04, 7.7533e-03]])\n",
      "Player 0 Prediction: tensor([[-0.1155,  0.1644, -1.2118,  0.1521]])\n",
      "Player 1 Prediction: tensor([[9.9994e-01, 0.0000e+00, 6.0975e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 5.5283,  4.8758, -2.9885,  3.8812]])\n",
      "Player 1 Prediction: tensor([[0.1463, 0.8159, 0.0378, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57854\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5650/10000 (56.5%)\n",
      "    Average reward: +0.387\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4350/10000 (43.5%)\n",
      "    Average reward: -0.387\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9873 (33.4%)\n",
      "    Action 1: 12581 (42.6%)\n",
      "    Action 2: 3417 (11.6%)\n",
      "    Action 3: 3676 (12.4%)\n",
      "  Player 1:\n",
      "    Action 0: 10180 (36.0%)\n",
      "    Action 1: 13903 (49.1%)\n",
      "    Action 2: 2158 (7.6%)\n",
      "    Action 3: 2066 (7.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3868.5, -3868.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.044\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3869\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8230e-01, 9.2500e-04, 1.6779e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2735, 0.0645, 0.6620]])\n",
      "Player 1 Prediction: tensor([[0.3164, 0.0448, 0.6389, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52919\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6234/10000 (62.3%)\n",
      "    Average reward: -0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3766/10000 (37.7%)\n",
      "    Average reward: +0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20398 (74.9%)\n",
      "    Action 1: 6836 (25.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4406 (17.2%)\n",
      "    Action 1: 15463 (60.2%)\n",
      "    Action 2: 2262 (8.8%)\n",
      "    Action 3: 3554 (13.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-758.5, 758.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.813 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.877 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.845\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0759\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31002/50000 [25:43<15:06, 20.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 201631/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 201978/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [26:47<19:26, 15.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 207962/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 208474/2000000\n",
      "P1 SL Buffer Size:  207962\n",
      "P1 SL buffer distribution [ 68543. 103579.  18359.  17481.]\n",
      "P1 actions distribution [0.32959387 0.49806695 0.08828055 0.08405863]\n",
      "P2 SL Buffer Size:  208474\n",
      "P2 SL buffer distribution [ 73178. 100936.  17909.  16451.]\n",
      "P2 actions distribution [0.35101739 0.48416589 0.0859052  0.07891152]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.6541,  1.7712, -0.2837,  1.3170]])\n",
      "Player 0 Prediction: tensor([[4.7631e-02, 9.5222e-01, 1.4777e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.4956,  1.7374, -1.5971,  0.9843]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.6909e-01, 3.5416e-04, 3.0558e-02]])\n",
      "Player 1 Prediction: tensor([[-1.0699,  0.5956, -2.8772, -0.4869]])\n",
      "Player 0 Prediction: tensor([[0.0664, 0.8992, 0.0344, 0.0000]])\n",
      "Player 1 Prediction: tensor([[-4.3417, -2.9652, -4.5406, -1.1224]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [27:00<19:26, 15.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57651\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5540/10000 (55.4%)\n",
      "    Average reward: -0.498\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4460/10000 (44.6%)\n",
      "    Average reward: +0.498\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8816 (31.2%)\n",
      "    Action 1: 14467 (51.2%)\n",
      "    Action 2: 2450 (8.7%)\n",
      "    Action 3: 2505 (8.9%)\n",
      "  Player 1:\n",
      "    Action 0: 10412 (35.4%)\n",
      "    Action 1: 11632 (39.5%)\n",
      "    Action 2: 3356 (11.4%)\n",
      "    Action 3: 4013 (13.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4980.5, 4980.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.019 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4980\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3710, 0.1181, 0.5109]])\n",
      "Player 0 Prediction: tensor([[0.2287, 0.2536, 0.5177, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54473\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5493/10000 (54.9%)\n",
      "    Average reward: +0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4507/10000 (45.1%)\n",
      "    Average reward: -0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5120 (19.8%)\n",
      "    Action 1: 16600 (64.1%)\n",
      "    Action 2: 1873 (7.2%)\n",
      "    Action 3: 2285 (8.8%)\n",
      "  Player 1:\n",
      "    Action 0: 21504 (75.2%)\n",
      "    Action 1: 7091 (24.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1514.0, -1514.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.873 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.808 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.841\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1514\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.2941e-01, 8.7045e-01, 1.3927e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[-0.1304,  0.1784, -1.1632,  0.1299]])\n",
      "Player 1 Prediction: tensor([[0.9987, 0.0000, 0.0013, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-2.1950, -2.4291, -2.6869, -1.3755]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9478, 0.0011, 0.0511]])\n",
      "Player 0 Prediction: tensor([[-3.8945, -4.9581, -2.9353, -1.5719]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 58497\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5623/10000 (56.2%)\n",
      "    Average reward: +0.360\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4377/10000 (43.8%)\n",
      "    Average reward: -0.360\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8951 (30.4%)\n",
      "    Action 1: 13250 (45.0%)\n",
      "    Action 2: 3466 (11.8%)\n",
      "    Action 3: 3802 (12.9%)\n",
      "  Player 1:\n",
      "    Action 0: 10676 (36.8%)\n",
      "    Action 1: 13736 (47.3%)\n",
      "    Action 2: 2300 (7.9%)\n",
      "    Action 3: 2316 (8.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3600.0, -3600.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.041\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3600\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8305e-01, 7.9475e-04, 1.6157e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2773, 0.0585, 0.6643]])\n",
      "Player 1 Prediction: tensor([[0.2916, 0.0433, 0.6651, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 52946\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6252/10000 (62.5%)\n",
      "    Average reward: -0.060\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3748/10000 (37.5%)\n",
      "    Average reward: +0.060\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20382 (74.8%)\n",
      "    Action 1: 6850 (25.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4317 (16.8%)\n",
      "    Action 1: 15394 (59.9%)\n",
      "    Action 2: 2364 (9.2%)\n",
      "    Action 3: 3639 (14.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-596.5, 596.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.814 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.875 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.845\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0597\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33004/50000 [28:16<13:44, 20.61it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 214406/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 214886/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33999/50000 [29:09<16:40, 16.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 220740/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 221568/2000000\n",
      "P1 SL Buffer Size:  220740\n",
      "P1 SL buffer distribution [ 72345. 109321.  19821.  19253.]\n",
      "P1 actions distribution [0.32773852 0.4952478  0.08979342 0.08722026]\n",
      "P2 SL Buffer Size:  221568\n",
      "P2 SL buffer distribution [ 77872. 106117.  19376.  18203.]\n",
      "P2 actions distribution [0.35145869 0.47893649 0.08744945 0.08215537]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33999/50000 [29:20<16:40, 16.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.6720,  1.7360, -0.2595,  1.3458]])\n",
      "Player 0 Prediction: tensor([[0.0035, 0.9881, 0.0084, 0.0000]])\n",
      "Player 1 Prediction: tensor([[ 1.4882,  1.7035, -1.5626,  1.0123]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1022, 0.0250, 0.8728]])\n",
      "Player 1 Prediction: tensor([[ 2.1667,  2.3473, -2.7147,  2.0978]])\n",
      "Player 0 Prediction: tensor([[0.0315, 0.0856, 0.8829, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57598\n",
      "Average episode length: 5.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5620/10000 (56.2%)\n",
      "    Average reward: -0.446\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4380/10000 (43.8%)\n",
      "    Average reward: +0.446\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8872 (31.4%)\n",
      "    Action 1: 14543 (51.5%)\n",
      "    Action 2: 2355 (8.3%)\n",
      "    Action 3: 2466 (8.7%)\n",
      "  Player 1:\n",
      "    Action 0: 10386 (35.4%)\n",
      "    Action 1: 11576 (39.4%)\n",
      "    Action 2: 3389 (11.5%)\n",
      "    Action 3: 4011 (13.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4457.0, 4457.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.018 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.039\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4457\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[9.2262e-01, 7.6975e-02, 4.0532e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9919, 0.0055, 0.0026]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54362\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5418/10000 (54.2%)\n",
      "    Average reward: +0.165\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4582/10000 (45.8%)\n",
      "    Average reward: -0.165\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5089 (19.7%)\n",
      "    Action 1: 16439 (63.6%)\n",
      "    Action 2: 1967 (7.6%)\n",
      "    Action 3: 2361 (9.1%)\n",
      "  Player 1:\n",
      "    Action 0: 21380 (75.0%)\n",
      "    Action 1: 7126 (25.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1649.5, -1649.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.877 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.811 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.844\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1650\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.4797e-01, 5.2019e-02, 9.3650e-06, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.4052,  1.7033, -0.5204,  1.7281]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7720, 0.0030, 0.2251]])\n",
      "Player 0 Prediction: tensor([[-1.1843, -0.4081, -1.2869,  0.4060]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56463\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5688/10000 (56.9%)\n",
      "    Average reward: +0.300\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4312/10000 (43.1%)\n",
      "    Average reward: -0.300\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7865 (27.7%)\n",
      "    Action 1: 12320 (43.3%)\n",
      "    Action 2: 3381 (11.9%)\n",
      "    Action 3: 4854 (17.1%)\n",
      "  Player 1:\n",
      "    Action 0: 10544 (37.6%)\n",
      "    Action 1: 12475 (44.5%)\n",
      "    Action 2: 2509 (8.9%)\n",
      "    Action 3: 2515 (9.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3002.0, -3002.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.036 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.043\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3002\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3373, 0.0346, 0.6281]])\n",
      "Player 1 Prediction: tensor([[0.6357, 0.2209, 0.1433, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53102\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6155/10000 (61.6%)\n",
      "    Average reward: -0.159\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3845/10000 (38.5%)\n",
      "    Average reward: +0.159\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 20201 (74.1%)\n",
      "    Action 1: 7073 (25.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4347 (16.8%)\n",
      "    Action 1: 15112 (58.5%)\n",
      "    Action 2: 2463 (9.5%)\n",
      "    Action 3: 3906 (15.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1589.0, 1589.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.826 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.855\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1589\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35002/50000 [30:51<14:43, 16.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 227026/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 228305/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [31:51<14:11, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 233356/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 234832/2000000\n",
      "P1 SL Buffer Size:  233356\n",
      "P1 SL buffer distribution [ 75856. 114751.  21295.  21454.]\n",
      "P1 actions distribution [0.32506557 0.49174223 0.09125542 0.09193678]\n",
      "P2 SL Buffer Size:  234832\n",
      "P2 SL buffer distribution [ 82713. 111497.  20797.  19825.]\n",
      "P2 actions distribution [0.35222201 0.47479475 0.08856118 0.08442205]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [32:01<14:11, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.5575e-01, 4.4233e-02, 1.8048e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.8068,  1.6556, -0.4066,  1.0504]])\n",
      "Player 0 Prediction: tensor([[4.0985e-02, 9.5883e-01, 1.8905e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.5096,  1.6790, -1.5301,  1.0390]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.5214e-01, 2.3930e-04, 4.7616e-02]])\n",
      "Player 1 Prediction: tensor([[-5.5575, -3.9848, -3.0380, -1.3433]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57233\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5532/10000 (55.3%)\n",
      "    Average reward: -0.466\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4468/10000 (44.7%)\n",
      "    Average reward: +0.466\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8767 (31.1%)\n",
      "    Action 1: 14153 (50.3%)\n",
      "    Action 2: 2430 (8.6%)\n",
      "    Action 3: 2797 (9.9%)\n",
      "  Player 1:\n",
      "    Action 0: 10448 (35.9%)\n",
      "    Action 1: 11418 (39.3%)\n",
      "    Action 2: 3239 (11.1%)\n",
      "    Action 3: 3981 (13.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4659.0, 4659.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4659\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[9.2903e-01, 7.0636e-02, 3.3174e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0148, 0.9837, 0.0016, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9361, 0.0030, 0.0610]])\n",
      "Player 0 Prediction: tensor([[0.7065, 0.1642, 0.1293, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54372\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5330/10000 (53.3%)\n",
      "    Average reward: +0.096\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4670/10000 (46.7%)\n",
      "    Average reward: -0.096\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5113 (19.7%)\n",
      "    Action 1: 16123 (62.2%)\n",
      "    Action 2: 2053 (7.9%)\n",
      "    Action 3: 2621 (10.1%)\n",
      "  Player 1:\n",
      "    Action 0: 21130 (74.2%)\n",
      "    Action 1: 7332 (25.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [960.5, -960.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.888 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.823 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.856\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0960\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.1526,  0.8589, -0.5399, -0.1505]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3184, 0.0337, 0.6479]])\n",
      "Player 0 Prediction: tensor([[ 4.2737,  3.8817, -0.9100,  2.4507]])\n",
      "Player 1 Prediction: tensor([[0.0860, 0.1426, 0.7713, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56163\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5802/10000 (58.0%)\n",
      "    Average reward: +0.296\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4198/10000 (42.0%)\n",
      "    Average reward: -0.296\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8724 (30.3%)\n",
      "    Action 1: 10888 (37.9%)\n",
      "    Action 2: 3190 (11.1%)\n",
      "    Action 3: 5962 (20.7%)\n",
      "  Player 1:\n",
      "    Action 0: 9731 (35.5%)\n",
      "    Action 1: 12694 (46.3%)\n",
      "    Action 2: 2105 (7.7%)\n",
      "    Action 3: 2869 (10.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2955.5, -2955.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.045 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.049\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2955\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8551, 0.1416, 0.0033, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0016, 0.9922, 0.0062, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3649, 0.0128, 0.6223]])\n",
      "Player 1 Prediction: tensor([[0.0452, 0.1156, 0.8392, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53529\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6091/10000 (60.9%)\n",
      "    Average reward: -0.237\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3909/10000 (39.1%)\n",
      "    Average reward: +0.237\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19903 (72.9%)\n",
      "    Action 1: 7417 (27.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4549 (17.4%)\n",
      "    Action 1: 14912 (56.9%)\n",
      "    Action 2: 2604 (9.9%)\n",
      "    Action 3: 4144 (15.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2371.5, 2371.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.844 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.901 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.872\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2371\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37004/50000 [33:32<12:51, 16.85it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 239792/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 241195/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38000/50000 [34:31<12:08, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 246187/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 247666/2000000\n",
      "P1 SL Buffer Size:  246187\n",
      "P1 SL buffer distribution [ 79654. 119703.  22774.  24056.]\n",
      "P1 actions distribution [0.3235508  0.48622795 0.09250692 0.09771434]\n",
      "P2 SL Buffer Size:  247666\n",
      "P2 SL buffer distribution [ 87375. 116572.  22176.  21543.]\n",
      "P2 actions distribution [0.35279368 0.47068229 0.08953994 0.08698408]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38000/50000 [34:42<12:08, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.3280e-01, 6.6922e-02, 2.7408e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.1872,  2.0605, -1.0536,  1.6591]])\n",
      "Player 0 Prediction: tensor([[9.9973e-01, 0.0000e+00, 2.7302e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.9270,  2.2616, -3.1323,  1.9467]])\n",
      "Player 0 Prediction: tensor([[0.0520, 0.1428, 0.8052, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 57313\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5513/10000 (55.1%)\n",
      "    Average reward: -0.436\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4487/10000 (44.9%)\n",
      "    Average reward: +0.436\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8727 (30.9%)\n",
      "    Action 1: 13989 (49.5%)\n",
      "    Action 2: 2528 (9.0%)\n",
      "    Action 3: 3001 (10.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10390 (35.7%)\n",
      "    Action 1: 11598 (39.9%)\n",
      "    Action 2: 3189 (11.0%)\n",
      "    Action 3: 3891 (13.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4356.0, 4356.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.043\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4356\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9015, 0.0019, 0.0966]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9585, 0.0028, 0.0387]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54549\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5441/10000 (54.4%)\n",
      "    Average reward: +0.235\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4559/10000 (45.6%)\n",
      "    Average reward: -0.235\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5206 (20.0%)\n",
      "    Action 1: 15972 (61.3%)\n",
      "    Action 2: 2075 (8.0%)\n",
      "    Action 3: 2788 (10.7%)\n",
      "  Player 1:\n",
      "    Action 0: 20942 (73.5%)\n",
      "    Action 1: 7566 (26.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2349.0, -2349.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.897 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.835 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.866\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2349\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 0.0134, -0.3447, -0.6493, -0.2730]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.5487e-01, 3.3934e-04, 4.4791e-02]])\n",
      "Player 0 Prediction: tensor([[-0.2316,  0.0590, -1.0685,  0.1089]])\n",
      "Player 1 Prediction: tensor([[9.9995e-01, 0.0000e+00, 5.2377e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 5.3078,  4.7001, -2.8646,  3.6961]])\n",
      "Player 1 Prediction: tensor([[0.0953, 0.8795, 0.0252, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 6.6077,  5.4664, -4.7722,  3.4633]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56196\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5830/10000 (58.3%)\n",
      "    Average reward: +0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4170/10000 (41.7%)\n",
      "    Average reward: -0.278\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8091 (28.4%)\n",
      "    Action 1: 11146 (39.2%)\n",
      "    Action 2: 3219 (11.3%)\n",
      "    Action 3: 6011 (21.1%)\n",
      "  Player 1:\n",
      "    Action 0: 10064 (36.3%)\n",
      "    Action 1: 12408 (44.7%)\n",
      "    Action 2: 2216 (8.0%)\n",
      "    Action 3: 3041 (11.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2777.0, -2777.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.048\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2777\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8803, 0.1173, 0.0024, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0015, 0.9930, 0.0055, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3458, 0.0344, 0.6198]])\n",
      "Player 1 Prediction: tensor([[0.0200, 0.0555, 0.9245, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53603\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6127/10000 (61.3%)\n",
      "    Average reward: -0.201\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3873/10000 (38.7%)\n",
      "    Average reward: +0.201\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19744 (72.3%)\n",
      "    Action 1: 7575 (27.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4730 (18.0%)\n",
      "    Action 1: 14739 (56.1%)\n",
      "    Action 2: 2576 (9.8%)\n",
      "    Action 3: 4239 (16.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2014.0, 2014.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.852 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.913 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.882\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2014\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39002/50000 [36:08<10:51, 16.88it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 252357/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 254181/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40000/50000 [37:14<10:37, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 258639/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 260340/2000000\n",
      "P1 SL Buffer Size:  258639\n",
      "P1 SL buffer distribution [ 83214. 124608.  24176.  26641.]\n",
      "P1 actions distribution [0.32173802 0.48178349 0.09347392 0.10300457]\n",
      "P2 SL Buffer Size:  260340\n",
      "P2 SL buffer distribution [ 92042. 121435.  23423.  23440.]\n",
      "P2 actions distribution [0.35354536 0.46644772 0.08997081 0.09003611]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.5745e-01, 4.2539e-02, 1.2416e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.4447, -0.0042, -0.5903,  0.0666]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7225, 0.0148, 0.2627]])\n",
      "Player 1 Prediction: tensor([[-3.2548, -3.1937, -1.3766, -2.0815]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40000/50000 [37:32<10:37, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56639\n",
      "Average episode length: 5.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5391/10000 (53.9%)\n",
      "    Average reward: -0.332\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4609/10000 (46.1%)\n",
      "    Average reward: +0.332\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9395 (33.9%)\n",
      "    Action 1: 12602 (45.4%)\n",
      "    Action 2: 2262 (8.2%)\n",
      "    Action 3: 3494 (12.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10684 (37.0%)\n",
      "    Action 1: 10490 (36.3%)\n",
      "    Action 2: 2807 (9.7%)\n",
      "    Action 3: 4905 (17.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3318.5, 3318.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.054\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3318\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[9.5745e-01, 4.2539e-02, 1.2416e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[3.6414e-02, 9.6343e-01, 1.6019e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.5276e-01, 1.0734e-04, 4.7128e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54625\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5427/10000 (54.3%)\n",
      "    Average reward: +0.193\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4573/10000 (45.7%)\n",
      "    Average reward: -0.193\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5190 (19.9%)\n",
      "    Action 1: 15738 (60.3%)\n",
      "    Action 2: 2105 (8.1%)\n",
      "    Action 3: 3049 (11.7%)\n",
      "  Player 1:\n",
      "    Action 0: 20803 (72.9%)\n",
      "    Action 1: 7740 (27.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1926.5, -1926.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.903 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.843 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.873\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1926\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.0775,  1.8255, -0.5657,  1.6259]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.5083e-01, 3.1183e-04, 4.8861e-02]])\n",
      "Player 0 Prediction: tensor([[ 1.3321,  2.2999, -1.0227,  1.7015]])\n",
      "Player 1 Prediction: tensor([[9.9995e-01, 0.0000e+00, 4.4988e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 0.0073,  0.4668, -2.5646,  0.9501]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.2229e-01, 6.8519e-04, 1.7703e-01]])\n",
      "Player 0 Prediction: tensor([[ 0.2207,  0.2376, -2.7925,  0.8469]])\n",
      "Player 1 Prediction: tensor([[0.9933, 0.0000, 0.0067, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55982\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5636/10000 (56.4%)\n",
      "    Average reward: +0.220\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4364/10000 (43.6%)\n",
      "    Average reward: -0.220\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8131 (28.6%)\n",
      "    Action 1: 11157 (39.3%)\n",
      "    Action 2: 3358 (11.8%)\n",
      "    Action 3: 5739 (20.2%)\n",
      "  Player 1:\n",
      "    Action 0: 10039 (36.4%)\n",
      "    Action 1: 12231 (44.3%)\n",
      "    Action 2: 2264 (8.2%)\n",
      "    Action 3: 3063 (11.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2195.5, -2195.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.049\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2195\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.4974e-01, 8.5021e-01, 4.0633e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2398, 0.0526, 0.7077]])\n",
      "Player 1 Prediction: tensor([[0.2535, 0.0254, 0.7211, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53883\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6130/10000 (61.3%)\n",
      "    Average reward: -0.231\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3870/10000 (38.7%)\n",
      "    Average reward: +0.231\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19656 (71.7%)\n",
      "    Action 1: 7750 (28.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4781 (18.1%)\n",
      "    Action 1: 14639 (55.3%)\n",
      "    Action 2: 2643 (10.0%)\n",
      "    Action 3: 4414 (16.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2307.0, 2307.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.859 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.919 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.889\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2307\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}, {'exploitability': 0.2757}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41001/50000 [39:42<13:50, 10.83it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 264813/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 266719/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42000/50000 [41:17<11:50, 11.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 270946/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 273333/2000000\n",
      "P1 SL Buffer Size:  270946\n",
      "P1 SL buffer distribution [ 86869. 129213.  25671.  29193.]\n",
      "P1 actions distribution [0.3206137  0.47689577 0.09474582 0.10774472]\n",
      "P2 SL Buffer Size:  273333\n",
      "P2 SL buffer distribution [ 96739. 126155.  24788.  25651.]\n",
      "P2 actions distribution [0.3539236  0.46154325 0.09068792 0.09384524]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.4102e-01, 5.8796e-02, 1.8241e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[-0.4547, -0.0102, -0.5757,  0.0653]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9562, 0.0281, 0.0156]])\n",
      "Player 1 Prediction: tensor([[-3.1892, -3.6140, -1.3541, -1.2717]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42000/50000 [41:33<11:50, 11.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56188\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5300/10000 (53.0%)\n",
      "    Average reward: -0.429\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4700/10000 (47.0%)\n",
      "    Average reward: +0.429\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9326 (33.7%)\n",
      "    Action 1: 12269 (44.3%)\n",
      "    Action 2: 2350 (8.5%)\n",
      "    Action 3: 3729 (13.5%)\n",
      "  Player 1:\n",
      "    Action 0: 10215 (35.8%)\n",
      "    Action 1: 10248 (35.9%)\n",
      "    Action 2: 3062 (10.7%)\n",
      "    Action 3: 4989 (17.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4285.5, 4285.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4285\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2367, 0.0944, 0.6689]])\n",
      "Player 0 Prediction: tensor([[0.4584, 0.4188, 0.1229, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54672\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5434/10000 (54.3%)\n",
      "    Average reward: +0.221\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4566/10000 (45.7%)\n",
      "    Average reward: -0.221\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5279 (20.2%)\n",
      "    Action 1: 15528 (59.3%)\n",
      "    Action 2: 2164 (8.3%)\n",
      "    Action 3: 3195 (12.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20580 (72.2%)\n",
      "    Action 1: 7926 (27.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2205.0, -2205.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.913 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.853 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.883\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2205\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 2.0378,  1.7942, -0.5622,  1.6056]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2789, 0.0434, 0.6777]])\n",
      "Player 0 Prediction: tensor([[-1.3293, -0.5232, -1.2177,  0.3792]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4022, 0.0635, 0.5343]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55820\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5696/10000 (57.0%)\n",
      "    Average reward: +0.213\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4304/10000 (43.0%)\n",
      "    Average reward: -0.213\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8161 (28.9%)\n",
      "    Action 1: 11148 (39.4%)\n",
      "    Action 2: 3271 (11.6%)\n",
      "    Action 3: 5688 (20.1%)\n",
      "  Player 1:\n",
      "    Action 0: 9998 (36.3%)\n",
      "    Action 1: 12084 (43.9%)\n",
      "    Action 2: 2378 (8.6%)\n",
      "    Action 3: 3092 (11.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2133.5, -2133.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2134\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8893, 0.1089, 0.0018, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0013, 0.9941, 0.0047, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3281, 0.0308, 0.6412]])\n",
      "Player 1 Prediction: tensor([[0.0186, 0.0474, 0.9340, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53975\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6103/10000 (61.0%)\n",
      "    Average reward: -0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3897/10000 (39.0%)\n",
      "    Average reward: +0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19232 (70.5%)\n",
      "    Action 1: 8047 (29.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5029 (18.8%)\n",
      "    Action 1: 14194 (53.2%)\n",
      "    Action 2: 2765 (10.4%)\n",
      "    Action 3: 4708 (17.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2395.5, 2395.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.875 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.938 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.907\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2396\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}, {'exploitability': 0.2757}, {'exploitability': 0.32095}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43002/50000 [44:13<12:50,  9.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 277123/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 279781/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43999/50000 [46:14<08:54, 11.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 283433/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 286221/2000000\n",
      "P1 SL Buffer Size:  283433\n",
      "P1 SL buffer distribution [ 90453. 134086.  27114.  31780.]\n",
      "P1 actions distribution [0.31913362 0.47307829 0.09566282 0.11212526]\n",
      "P2 SL Buffer Size:  286221\n",
      "P2 SL buffer distribution [101343. 130773.  26215.  27890.]\n",
      "P2 actions distribution [0.35407255 0.4568952  0.09159007 0.09744219]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[9.5942e-01, 4.0575e-02, 6.2881e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 0.8633,  1.6425, -0.6953,  1.4367]])\n",
      "Player 0 Prediction: tensor([[3.2150e-02, 9.6776e-01, 9.2012e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.7855,  1.3781, -1.8879,  1.7090]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.6585e-01, 4.5828e-05, 3.4106e-02]])\n",
      "Player 1 Prediction: tensor([[-0.7629,  0.6803, -3.1520,  0.5477]])\n",
      "Player 0 Prediction: tensor([[0.9819, 0.0000, 0.0181, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43999/50000 [46:33<08:54, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56142\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5339/10000 (53.4%)\n",
      "    Average reward: -0.390\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4661/10000 (46.6%)\n",
      "    Average reward: +0.390\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9239 (33.4%)\n",
      "    Action 1: 12233 (44.3%)\n",
      "    Action 2: 2287 (8.3%)\n",
      "    Action 3: 3868 (14.0%)\n",
      "  Player 1:\n",
      "    Action 0: 10258 (36.0%)\n",
      "    Action 1: 10161 (35.6%)\n",
      "    Action 2: 3033 (10.6%)\n",
      "    Action 3: 5063 (17.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3898.0, 3898.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3898\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2300, 0.0864, 0.6836]])\n",
      "Player 0 Prediction: tensor([[0.0257, 0.3065, 0.6678, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54817\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5439/10000 (54.4%)\n",
      "    Average reward: +0.229\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4561/10000 (45.6%)\n",
      "    Average reward: -0.229\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5337 (20.3%)\n",
      "    Action 1: 15545 (59.2%)\n",
      "    Action 2: 2123 (8.1%)\n",
      "    Action 3: 3257 (12.4%)\n",
      "  Player 1:\n",
      "    Action 0: 20498 (71.8%)\n",
      "    Action 1: 8057 (28.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2294.5, -2294.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.915 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.858 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.887\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2294\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.9915,  1.7819, -0.5599,  1.5702]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9828, 0.0011, 0.0161]])\n",
      "Player 0 Prediction: tensor([[ 1.1887,  2.2198, -1.0082,  1.6349]])\n",
      "Player 1 Prediction: tensor([[9.9946e-01, 0.0000e+00, 5.3910e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 4.5075,  4.3189, -2.6485,  3.0786]])\n",
      "Player 1 Prediction: tensor([[0.8796, 0.0941, 0.0263, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55343\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5566/10000 (55.7%)\n",
      "    Average reward: +0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4434/10000 (44.3%)\n",
      "    Average reward: -0.127\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7994 (28.5%)\n",
      "    Action 1: 10849 (38.7%)\n",
      "    Action 2: 3408 (12.2%)\n",
      "    Action 3: 5774 (20.6%)\n",
      "  Player 1:\n",
      "    Action 0: 9998 (36.6%)\n",
      "    Action 1: 11785 (43.1%)\n",
      "    Action 2: 2267 (8.3%)\n",
      "    Action 3: 3268 (12.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1273.0, -1273.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1273\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8857, 0.1127, 0.0016, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0011, 0.9950, 0.0039, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3006, 0.0067, 0.6927]])\n",
      "Player 1 Prediction: tensor([[0.0340, 0.1060, 0.8600, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54244\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6206/10000 (62.1%)\n",
      "    Average reward: -0.197\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3794/10000 (37.9%)\n",
      "    Average reward: +0.197\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18982 (69.5%)\n",
      "    Action 1: 8326 (30.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5239 (19.4%)\n",
      "    Action 1: 13978 (51.9%)\n",
      "    Action 2: 2864 (10.6%)\n",
      "    Action 3: 4855 (18.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1967.5, 1967.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.887 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.951 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.919\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1968\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}, {'exploitability': 0.2757}, {'exploitability': 0.32095}, {'exploitability': 0.25855}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45002/50000 [48:55<11:07,  7.49it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 289401/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 292608/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/50000 [50:28<05:00, 13.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 295344/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 298907/2000000\n",
      "P1 SL Buffer Size:  295344\n",
      "P1 SL buffer distribution [ 93873. 138569.  28555.  34347.]\n",
      "P1 actions distribution [0.31784292 0.46917831 0.09668387 0.1162949 ]\n",
      "P2 SL Buffer Size:  298907\n",
      "P2 SL buffer distribution [105899. 135290.  27597.  30121.]\n",
      "P2 actions distribution [0.35428745 0.4526157  0.09232638 0.10077047]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.2956, -0.1544, -0.5552,  0.1833]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2222, 0.0814, 0.6965]])\n",
      "Player 1 Prediction: tensor([[-2.2318, -2.2144, -1.0013, -0.4973]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.0637, 0.6040, 0.3323]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/50000 [50:43<05:00, 13.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 56044\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5470/10000 (54.7%)\n",
      "    Average reward: -0.334\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4530/10000 (45.3%)\n",
      "    Average reward: +0.334\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9226 (33.5%)\n",
      "    Action 1: 12116 (43.9%)\n",
      "    Action 2: 2255 (8.2%)\n",
      "    Action 3: 3984 (14.4%)\n",
      "  Player 1:\n",
      "    Action 0: 10237 (36.0%)\n",
      "    Action 1: 10119 (35.6%)\n",
      "    Action 2: 3070 (10.8%)\n",
      "    Action 3: 5037 (17.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3343.5, 3343.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3343\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.9292, 0.0695, 0.0012, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0027, 0.9941, 0.0031, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3245, 0.0089, 0.6666]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54767\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5362/10000 (53.6%)\n",
      "    Average reward: +0.213\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4638/10000 (46.4%)\n",
      "    Average reward: -0.213\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5373 (20.4%)\n",
      "    Action 1: 15201 (57.8%)\n",
      "    Action 2: 2262 (8.6%)\n",
      "    Action 3: 3476 (13.2%)\n",
      "  Player 1:\n",
      "    Action 0: 20200 (71.0%)\n",
      "    Action 1: 8255 (29.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2131.5, -2131.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.925 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.869 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.897\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2132\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.8939, 0.1048, 0.0013, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.1462,  0.7382, -0.6070, -0.1625]])\n",
      "Player 1 Prediction: tensor([[0.0010, 0.9956, 0.0034, 0.0000]])\n",
      "Player 0 Prediction: tensor([[ 0.1647,  0.8333, -1.8891, -0.0026]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2738, 0.0053, 0.7209]])\n",
      "Player 0 Prediction: tensor([[ 1.4055,  0.5438, -2.8216,  0.6894]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55032\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5601/10000 (56.0%)\n",
      "    Average reward: +0.147\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4399/10000 (44.0%)\n",
      "    Average reward: -0.147\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8020 (28.8%)\n",
      "    Action 1: 10727 (38.6%)\n",
      "    Action 2: 3263 (11.7%)\n",
      "    Action 3: 5790 (20.8%)\n",
      "  Player 1:\n",
      "    Action 0: 9902 (36.4%)\n",
      "    Action 1: 11509 (42.3%)\n",
      "    Action 2: 2372 (8.7%)\n",
      "    Action 3: 3449 (12.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1467.0, -1467.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.047 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.052\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1467\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.6380e-01, 3.6200e-02, 1.3235e-06, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[1.4311e-02, 9.8563e-01, 5.9326e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8123e-01, 2.4375e-05, 1.8746e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54626\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6085/10000 (60.9%)\n",
      "    Average reward: -0.300\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3915/10000 (39.1%)\n",
      "    Average reward: +0.300\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18738 (68.4%)\n",
      "    Action 1: 8662 (31.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5499 (20.2%)\n",
      "    Action 1: 13749 (50.5%)\n",
      "    Action 2: 2879 (10.6%)\n",
      "    Action 3: 5099 (18.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2996.0, 2996.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.900 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.964 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.932\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2996\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}, {'exploitability': 0.2757}, {'exploitability': 0.32095}, {'exploitability': 0.25855}, {'exploitability': 0.240525}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47001/50000 [52:48<04:39, 10.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 301532/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 305213/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [54:35<03:11, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 307731/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 311826/2000000\n",
      "P1 SL Buffer Size:  307731\n",
      "P1 SL buffer distribution [ 97556. 143158.  30070.  36947.]\n",
      "P1 actions distribution [0.31701714 0.465205   0.09771521 0.12006265]\n",
      "P2 SL Buffer Size:  311826\n",
      "P2 SL buffer distribution [110553. 139797.  29007.  32469.]\n",
      "P2 actions distribution [0.35453426 0.44831733 0.09302303 0.10412538]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [54:53<03:11, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 0.2612, -0.1976, -0.5571,  0.1509]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6812, 0.0010, 0.3177]])\n",
      "Player 1 Prediction: tensor([[-3.0416, -2.5248, -0.9413, -1.4134]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55686\n",
      "Average episode length: 5.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5329/10000 (53.3%)\n",
      "    Average reward: -0.430\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4671/10000 (46.7%)\n",
      "    Average reward: +0.430\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9151 (33.4%)\n",
      "    Action 1: 11849 (43.2%)\n",
      "    Action 2: 2419 (8.8%)\n",
      "    Action 3: 4009 (14.6%)\n",
      "  Player 1:\n",
      "    Action 0: 10148 (35.9%)\n",
      "    Action 1: 10102 (35.7%)\n",
      "    Action 2: 3059 (10.8%)\n",
      "    Action 3: 4949 (17.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4303.5, 4303.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.056\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4304\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9565, 0.0115, 0.0320]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9951, 0.0022, 0.0027]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54878\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5312/10000 (53.1%)\n",
      "    Average reward: +0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4688/10000 (46.9%)\n",
      "    Average reward: -0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5429 (20.6%)\n",
      "    Action 1: 15023 (56.9%)\n",
      "    Action 2: 2282 (8.6%)\n",
      "    Action 3: 3652 (13.8%)\n",
      "  Player 1:\n",
      "    Action 0: 20045 (70.4%)\n",
      "    Action 1: 8447 (29.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1806.5, -1806.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.932 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.877 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.904\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1807\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[ 1.9009,  1.7142, -0.5548,  1.5126]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2306, 0.0488, 0.7206]])\n",
      "Player 0 Prediction: tensor([[ 1.0868,  2.1297, -1.0007,  1.5745]])\n",
      "Player 1 Prediction: tensor([[0.9982, 0.0000, 0.0018, 0.0000]])\n",
      "Player 0 Prediction: tensor([[-0.3632,  0.2676, -2.5137,  0.7310]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3149, 0.0226, 0.6624]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53786\n",
      "Average episode length: 5.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5691/10000 (56.9%)\n",
      "    Average reward: +0.203\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4309/10000 (43.1%)\n",
      "    Average reward: -0.203\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7960 (29.2%)\n",
      "    Action 1: 10739 (39.5%)\n",
      "    Action 2: 3350 (12.3%)\n",
      "    Action 3: 5168 (19.0%)\n",
      "  Player 1:\n",
      "    Action 0: 9916 (37.3%)\n",
      "    Action 1: 10856 (40.9%)\n",
      "    Action 2: 2482 (9.3%)\n",
      "    Action 3: 3315 (12.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2031.5, -2031.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.053\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2031\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.6638e-01, 3.3615e-02, 9.4447e-07, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[1.3805e-02, 9.8615e-01, 4.4053e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 6.5739e-01, 3.3612e-04, 3.4228e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54654\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6105/10000 (61.1%)\n",
      "    Average reward: -0.257\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3895/10000 (39.0%)\n",
      "    Average reward: +0.257\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18697 (68.2%)\n",
      "    Action 1: 8708 (31.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5624 (20.6%)\n",
      "    Action 1: 13672 (50.2%)\n",
      "    Action 2: 2835 (10.4%)\n",
      "    Action 3: 5118 (18.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2569.0, 2569.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.902 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.969 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.936\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2569\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}, {'exploitability': 0.2757}, {'exploitability': 0.32095}, {'exploitability': 0.25855}, {'exploitability': 0.240525}, {'exploitability': 0.31675}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49005/50000 [56:22<00:40, 24.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0003 â†’ 0.0003\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 314236/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 318197/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [57:24<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[ 1.7138,  1.6792, -0.6906,  1.7504]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9565, 0.0108, 0.0327]])\n",
      "Player 1 Prediction: tensor([[ 1.0844,  1.9906, -0.9959,  1.6272]])\n",
      "Player 0 Prediction: tensor([[9.9988e-01, 0.0000e+00, 1.2268e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[ 1.7069,  2.1028, -3.0975,  1.7288]])\n",
      "Player 0 Prediction: tensor([[0.0347, 0.0939, 0.8714, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55184\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5488/10000 (54.9%)\n",
      "    Average reward: -0.344\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4512/10000 (45.1%)\n",
      "    Average reward: +0.344\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8984 (33.0%)\n",
      "    Action 1: 11531 (42.4%)\n",
      "    Action 2: 2431 (8.9%)\n",
      "    Action 3: 4269 (15.7%)\n",
      "  Player 1:\n",
      "    Action 0: 9971 (35.7%)\n",
      "    Action 1: 9821 (35.1%)\n",
      "    Action 2: 3103 (11.1%)\n",
      "    Action 3: 5074 (18.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3442.0, 3442.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.057\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3442\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9565, 0.0108, 0.0327]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7208, 0.1470, 0.1322]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 55095\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5311/10000 (53.1%)\n",
      "    Average reward: +0.207\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4689/10000 (46.9%)\n",
      "    Average reward: -0.207\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5517 (20.8%)\n",
      "    Action 1: 14895 (56.1%)\n",
      "    Action 2: 2372 (8.9%)\n",
      "    Action 3: 3790 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 19881 (69.7%)\n",
      "    Action 1: 8640 (30.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2072.0, -2072.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.939 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.885 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.912\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.2072\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[3.4657e-01, 6.5338e-01, 4.3993e-05, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[ 1.1246,  1.5296, -0.4455,  1.6014]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7707, 0.0108, 0.2185]])\n",
      "Player 0 Prediction: tensor([[ 2.6921,  3.0410, -1.5867,  2.3853]])\n",
      "Player 1 Prediction: tensor([[0.9751, 0.0000, 0.0249, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 53427\n",
      "Average episode length: 5.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5608/10000 (56.1%)\n",
      "    Average reward: +0.093\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4392/10000 (43.9%)\n",
      "    Average reward: -0.093\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7983 (29.5%)\n",
      "    Action 1: 10428 (38.6%)\n",
      "    Action 2: 3362 (12.4%)\n",
      "    Action 3: 5274 (19.5%)\n",
      "  Player 1:\n",
      "    Action 0: 9860 (37.4%)\n",
      "    Action 1: 10603 (40.2%)\n",
      "    Action 2: 2481 (9.4%)\n",
      "    Action 3: 3436 (13.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [931.5, -931.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.054\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0931\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[9.6722e-01, 3.2775e-02, 6.9493e-07, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[1.3468e-02, 9.8650e-01, 3.3505e-05, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.4185e-01, 7.9600e-06, 5.8139e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 54787\n",
      "Average episode length: 5.5 steps\n",
      "Episode length range: 2 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6163/10000 (61.6%)\n",
      "    Average reward: -0.232\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3837/10000 (38.4%)\n",
      "    Average reward: +0.232\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 18810 (68.3%)\n",
      "    Action 1: 8712 (31.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5613 (20.6%)\n",
      "    Action 1: 13745 (50.4%)\n",
      "    Action 2: 2895 (10.6%)\n",
      "    Action 3: 5012 (18.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2316.0, 2316.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.901 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.968 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.934\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2316\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.23709999999999998}, {'exploitability': 0.98045}, {'exploitability': 1.0042499999999999}, {'exploitability': 0.8362499999999999}, {'exploitability': 0.6977}, {'exploitability': 0.713875}, {'exploitability': 0.644025}, {'exploitability': 0.5225}, {'exploitability': 0.5937749999999999}, {'exploitability': 0.559975}, {'exploitability': 0.5258}, {'exploitability': 0.6016}, {'exploitability': 0.500625}, {'exploitability': 0.530975}, {'exploitability': 0.4598}, {'exploitability': 0.429025}, {'exploitability': 0.37295}, {'exploitability': 0.380725}, {'exploitability': 0.35665}, {'exploitability': 0.2757}, {'exploitability': 0.32095}, {'exploitability': 0.25855}, {'exploitability': 0.240525}, {'exploitability': 0.31675}, {'exploitability': 0.218675}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a11e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x35e1ba9b0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x35e1ba9b0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.06\n",
      "Using         eg_epsilon                    : 0.0\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : True\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.5\n",
      "Using         per_beta                      : 0.5\n",
      "Using         per_beta_final                : 1.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 3\n",
      "Using         atom_size                     : 51\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x35e1ba9b0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.06\n",
      "Using         eg_epsilon                    : 0.0\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : True\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.5\n",
      "Using         per_beta                      : 0.5\n",
      "Using         per_beta_final                : 1.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 3\n",
      "Using         atom_size                     : 51\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x390016260>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x390016260>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e3d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-Rainbow\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8fac96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/50000 [00:00<2:17:45,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 61/200000\n",
      "   Player 0 SL buffer: 10/2000000\n",
      "   Player 1 RL buffer: 63/200000\n",
      "   Player 1 SL buffer: 6/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1003/50000 [01:01<46:56, 17.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 64875/200000\n",
      "   Player 0 SL buffer: 7209/2000000\n",
      "   Player 1 RL buffer: 63247/200000\n",
      "   Player 1 SL buffer: 6802/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1999/50000 [02:28<1:08:52, 11.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 129984/200000\n",
      "   Player 0 SL buffer: 13549/2000000\n",
      "   Player 1 RL buffer: 126138/200000\n",
      "   Player 1 SL buffer: 13094/2000000\n",
      "P1 SL Buffer Size:  13549\n",
      "P1 SL buffer distribution [6974. 4500.  120. 1955.]\n",
      "P1 actions distribution [0.51472433 0.33212783 0.00885674 0.14429109]\n",
      "P2 SL Buffer Size:  13094\n",
      "P2 SL buffer distribution [5874. 5597.   41. 1582.]\n",
      "P2 actions distribution [0.44860241 0.42744769 0.00313121 0.1208187 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7026, 0.2783, 0.0191, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.5290e-04, 1.4157e-04, 3.0309e-04, 1.8416e-04, 2.0543e-04,\n",
      "          2.6430e-04, 2.4201e-04, 5.2354e-03, 8.6208e-03, 3.8661e-04,\n",
      "          6.3926e-02, 9.0772e-03, 6.9685e-03, 9.1871e-03, 3.4191e-04,\n",
      "          8.9136e-02, 4.6315e-03, 1.6274e-02, 1.0595e-02, 4.5660e-04,\n",
      "          7.0242e-02, 1.0661e-03, 4.2435e-03, 4.1481e-03, 1.5285e-04,\n",
      "          1.3995e-01, 2.7355e-04, 1.9740e-03, 1.1606e-03, 1.2115e-03,\n",
      "          1.2041e-01, 3.8087e-04, 1.1754e-02, 1.3879e-02, 6.5350e-03,\n",
      "          2.0480e-01, 5.9053e-04, 1.8378e-02, 1.4099e-02, 4.5518e-03,\n",
      "          1.2527e-01, 6.1996e-04, 1.4989e-02, 1.0992e-02, 4.6820e-04,\n",
      "          1.8470e-04, 3.2289e-04, 1.3984e-04, 2.6978e-04, 1.9130e-04,\n",
      "          3.1077e-04],\n",
      "         [1.3827e-04, 1.5666e-04, 3.3364e-04, 1.6681e-04, 2.4735e-04,\n",
      "          2.8796e-04, 1.3355e-04, 5.6902e-03, 3.8595e-02, 3.1675e-04,\n",
      "          1.2774e-02, 9.2663e-03, 2.6816e-02, 6.6659e-02, 3.0065e-04,\n",
      "          2.4527e-02, 5.6127e-03, 2.8474e-02, 2.6441e-02, 4.2307e-04,\n",
      "          7.2771e-03, 1.4997e-03, 8.1925e-04, 8.4500e-04, 9.7993e-05,\n",
      "          1.6048e-01, 2.9151e-04, 3.6103e-03, 2.5361e-03, 1.3838e-03,\n",
      "          1.6768e-01, 2.7692e-04, 2.5496e-02, 3.0957e-02, 5.7803e-03,\n",
      "          9.1216e-02, 5.4172e-04, 8.7591e-02, 5.1637e-02, 8.5216e-03,\n",
      "          3.0898e-02, 2.9440e-04, 6.0638e-02, 1.0259e-02, 2.7920e-04,\n",
      "          2.9075e-04, 2.9247e-04, 2.1096e-04, 3.4618e-04, 2.5125e-04,\n",
      "          3.3623e-04],\n",
      "         [1.0102e-04, 7.3498e-05, 1.6168e-04, 9.4337e-05, 6.0383e-05,\n",
      "          1.0846e-04, 7.5182e-05, 4.8208e-04, 6.0033e-04, 1.0481e-04,\n",
      "          1.1074e-03, 7.3985e-04, 3.1446e-03, 4.9554e-03, 1.1383e-04,\n",
      "          1.3418e-02, 5.1243e-04, 2.2084e-02, 1.2117e-02, 9.0964e-05,\n",
      "          2.6759e-02, 1.2956e-04, 4.6043e-01, 4.3330e-01, 2.1821e-03,\n",
      "          1.7234e-03, 8.5398e-05, 3.7573e-04, 2.1791e-04, 3.2772e-04,\n",
      "          1.8309e-03, 9.5792e-05, 8.9530e-04, 1.3823e-03, 4.4699e-04,\n",
      "          2.3873e-03, 1.8648e-04, 1.0985e-03, 1.0518e-03, 5.0583e-04,\n",
      "          1.7326e-03, 8.4556e-05, 1.2853e-03, 5.8731e-04, 1.6829e-04,\n",
      "          5.7434e-05, 9.6749e-05, 6.4066e-05, 1.5710e-04, 7.9340e-05,\n",
      "          1.2539e-04],\n",
      "         [5.2295e-04, 5.0743e-04, 1.5109e-03, 8.5596e-04, 5.1693e-04,\n",
      "          6.6289e-04, 7.8810e-04, 2.2380e-03, 5.1948e-03, 1.4691e-03,\n",
      "          1.5376e-02, 6.8812e-03, 1.8051e-02, 3.6651e-02, 7.5985e-04,\n",
      "          1.5732e-02, 8.6097e-03, 3.8491e-02, 3.4324e-02, 1.0131e-03,\n",
      "          4.5921e-02, 1.3081e-03, 3.9705e-02, 5.1884e-02, 7.6627e-04,\n",
      "          1.4873e-01, 9.4193e-04, 2.8089e-02, 1.9424e-02, 1.9547e-03,\n",
      "          1.4844e-01, 1.7188e-03, 7.3474e-02, 6.1699e-02, 8.3705e-03,\n",
      "          6.2684e-02, 2.4324e-03, 3.4818e-02, 2.4189e-02, 5.7921e-03,\n",
      "          2.5319e-02, 1.6281e-03, 8.3068e-03, 6.5984e-03, 1.0519e-03,\n",
      "          5.3005e-04, 1.1082e-03, 8.3965e-04, 7.6298e-04, 4.7059e-04,\n",
      "          8.9181e-04]]])\n",
      "Player 0 Prediction: tensor([[0.9397, 0.0000, 0.0603, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.9474e-04, 1.5121e-04, 2.3115e-04, 2.2291e-04, 2.3636e-04,\n",
      "          2.8555e-04, 2.2675e-04, 3.0827e-02, 3.7830e-02, 2.3840e-04,\n",
      "          6.6589e-03, 1.7761e-03, 8.2451e-02, 9.1105e-02, 3.3667e-04,\n",
      "          1.4571e-03, 1.5819e-03, 2.9288e-02, 2.5616e-02, 2.6917e-04,\n",
      "          1.1329e-03, 6.0295e-04, 8.3595e-04, 6.3588e-04, 2.4740e-04,\n",
      "          1.8804e-01, 2.0461e-04, 1.0193e-03, 8.1452e-04, 8.0980e-04,\n",
      "          1.4808e-03, 2.1632e-04, 3.0156e-02, 4.5561e-02, 1.9783e-03,\n",
      "          3.7886e-03, 4.4452e-04, 1.7595e-01, 1.3195e-01, 2.0615e-03,\n",
      "          1.1795e-02, 4.0624e-04, 5.5973e-02, 3.1097e-02, 2.7724e-04,\n",
      "          2.3379e-04, 3.3553e-04, 2.5042e-04, 2.6850e-04, 2.5475e-04,\n",
      "          1.8730e-04],\n",
      "         [1.1716e-04, 9.7440e-05, 1.8656e-04, 1.2989e-04, 2.3143e-04,\n",
      "          1.7321e-04, 1.3716e-04, 5.0411e-02, 6.4508e-02, 1.5047e-04,\n",
      "          4.3474e-03, 2.2247e-03, 6.1792e-02, 6.7646e-02, 2.0875e-04,\n",
      "          2.4494e-03, 1.9284e-03, 5.2206e-03, 7.2999e-03, 1.8585e-04,\n",
      "          9.4347e-04, 7.6258e-04, 1.4219e-04, 1.1879e-04, 8.2695e-05,\n",
      "          2.0064e-01, 1.8369e-04, 7.5991e-04, 6.9813e-04, 4.4480e-04,\n",
      "          1.3357e-03, 1.4336e-04, 2.2675e-02, 3.0870e-02, 1.6942e-03,\n",
      "          4.6754e-03, 2.2938e-04, 1.2273e-01, 1.2149e-01, 2.5873e-03,\n",
      "          9.1018e-03, 2.1347e-04, 1.2733e-01, 7.9515e-02, 1.9275e-04,\n",
      "          1.8018e-04, 1.3480e-04, 2.1284e-04, 1.1596e-04, 1.8082e-04,\n",
      "          1.7867e-04],\n",
      "         [4.5802e-04, 4.5629e-04, 3.3739e-04, 2.5778e-04, 2.6149e-04,\n",
      "          4.3902e-04, 2.8265e-04, 3.9444e-03, 5.8966e-03, 3.4525e-04,\n",
      "          1.9445e-03, 8.6863e-04, 3.7829e-02, 3.0920e-02, 4.4940e-04,\n",
      "          1.3454e-02, 9.1327e-04, 3.7764e-01, 3.4719e-01, 3.0938e-04,\n",
      "          5.3034e-02, 4.6572e-04, 2.2465e-02, 2.1083e-02, 7.9213e-03,\n",
      "          9.8200e-03, 4.7631e-04, 7.1636e-04, 5.5338e-04, 7.7352e-04,\n",
      "          6.0765e-04, 3.9409e-04, 7.2574e-03, 6.9676e-03, 8.6640e-04,\n",
      "          7.9402e-04, 5.2093e-04, 1.1521e-02, 8.2214e-03, 9.9605e-04,\n",
      "          3.4709e-03, 3.5974e-04, 9.1509e-03, 4.9468e-03, 3.8626e-04,\n",
      "          3.0089e-04, 3.0101e-04, 4.1408e-04, 4.0857e-04, 2.6339e-04,\n",
      "          3.4140e-04],\n",
      "         [8.8571e-04, 7.4035e-04, 1.0454e-03, 9.4895e-04, 8.7170e-04,\n",
      "          6.1881e-04, 5.0301e-04, 1.4301e-02, 1.4836e-02, 1.1659e-03,\n",
      "          4.7264e-03, 2.2961e-03, 5.8298e-02, 5.1165e-02, 1.1922e-03,\n",
      "          3.8793e-03, 4.8052e-03, 9.8406e-02, 1.0160e-01, 7.0579e-04,\n",
      "          6.9559e-03, 1.4074e-03, 8.9111e-03, 6.1132e-03, 1.4512e-03,\n",
      "          1.9470e-01, 1.1583e-03, 1.0682e-02, 6.0723e-03, 1.4415e-03,\n",
      "          1.0536e-02, 9.0910e-04, 8.6788e-02, 1.0369e-01, 3.0939e-03,\n",
      "          8.3166e-03, 1.7520e-03, 6.0858e-02, 5.8847e-02, 4.1990e-03,\n",
      "          1.1062e-02, 1.2892e-03, 2.1079e-02, 2.0583e-02, 7.8265e-04,\n",
      "          7.2949e-04, 1.0226e-03, 8.9422e-04, 5.6892e-04, 6.1140e-04,\n",
      "          5.1149e-04]]])\n",
      "Player 0 Prediction: tensor([[0.5608, 0.4005, 0.0386, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1999/50000 [02:44<1:08:52, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 48283\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5403/10000 (54.0%)\n",
      "    Average reward: -0.410\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4597/10000 (46.0%)\n",
      "    Average reward: +0.410\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 9060 (37.6%)\n",
      "    Action 1: 11050 (45.9%)\n",
      "    Action 2: 931 (3.9%)\n",
      "    Action 3: 3038 (12.6%)\n",
      "  Player 1:\n",
      "    Action 0: 11592 (47.9%)\n",
      "    Action 1: 8002 (33.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 4610 (19.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4099.5, 4099.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.037 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.041\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4099\n",
      "   Testing specific player: 0\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6682, 0.0488, 0.2830]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6797, 0.0510, 0.2693]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50945\n",
      "Average episode length: 5.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5500/10000 (55.0%)\n",
      "    Average reward: -0.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4500/10000 (45.0%)\n",
      "    Average reward: +0.129\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8559 (34.2%)\n",
      "    Action 1: 12316 (49.2%)\n",
      "    Action 2: 889 (3.5%)\n",
      "    Action 3: 3290 (13.1%)\n",
      "  Player 1:\n",
      "    Action 0: 17383 (67.1%)\n",
      "    Action 1: 8508 (32.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1290.0, 1290.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.033 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.913 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.973\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1290\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[2.4964e-04, 3.6436e-04, 4.0763e-04, 2.8030e-04, 5.4623e-04,\n",
      "          3.4314e-04, 2.3668e-04, 4.9625e-03, 1.0706e-02, 5.7322e-04,\n",
      "          1.9238e-02, 8.0650e-03, 1.3003e-02, 2.6220e-02, 1.1047e-03,\n",
      "          4.0193e-02, 1.6504e-02, 3.5112e-02, 5.5137e-02, 6.0774e-04,\n",
      "          2.7726e-02, 4.8714e-03, 3.2921e-02, 3.8223e-02, 2.3978e-03,\n",
      "          1.2985e-01, 5.2491e-04, 3.8130e-02, 4.7976e-02, 2.0371e-03,\n",
      "          7.7036e-02, 6.2231e-04, 3.2031e-02, 2.8818e-02, 2.0737e-02,\n",
      "          1.2144e-01, 9.7377e-04, 6.1806e-02, 2.1091e-02, 1.4317e-02,\n",
      "          3.7670e-02, 1.8009e-03, 1.2875e-02, 7.6510e-03, 3.8437e-04,\n",
      "          3.8507e-04, 3.5413e-04, 5.4908e-04, 2.6624e-04, 4.1259e-04,\n",
      "          2.6640e-04],\n",
      "         [2.4704e-04, 2.3792e-04, 1.9938e-04, 1.7301e-04, 4.6823e-04,\n",
      "          1.3771e-04, 1.6079e-04, 8.0927e-03, 2.1970e-02, 8.2509e-04,\n",
      "          1.8846e-02, 8.1562e-03, 2.1556e-02, 3.5460e-02, 3.4676e-04,\n",
      "          3.7636e-02, 6.0423e-03, 1.0859e-02, 2.9520e-02, 3.1739e-04,\n",
      "          1.2960e-01, 6.5403e-03, 1.3706e-03, 1.3469e-03, 1.1277e-03,\n",
      "          9.9459e-02, 3.4486e-04, 5.3963e-02, 5.0694e-02, 1.4166e-03,\n",
      "          3.8427e-02, 2.2148e-04, 3.7365e-02, 2.4052e-02, 5.4950e-03,\n",
      "          7.0563e-02, 3.2616e-04, 1.3570e-01, 3.2111e-02, 1.5189e-02,\n",
      "          2.2904e-02, 6.3950e-04, 5.6465e-02, 1.1684e-02, 1.2524e-04,\n",
      "          3.2896e-04, 2.6276e-04, 2.1497e-04, 3.1407e-04, 2.0491e-04,\n",
      "          2.8891e-04],\n",
      "         [3.6774e-05, 3.5356e-05, 3.7956e-05, 2.0860e-05, 2.4596e-05,\n",
      "          2.3682e-05, 1.9867e-05, 1.6911e-04, 1.2355e-04, 4.1656e-05,\n",
      "          1.9660e-04, 1.9343e-04, 8.0161e-04, 2.1944e-03, 2.7914e-05,\n",
      "          2.2405e-03, 1.8850e-04, 6.3164e-03, 8.7863e-03, 2.7360e-05,\n",
      "          6.8532e-03, 8.8098e-05, 5.5399e-02, 2.2491e-01, 6.8566e-01,\n",
      "          6.6891e-04, 4.1750e-05, 5.6181e-04, 2.8006e-04, 3.9455e-05,\n",
      "          1.7929e-04, 4.0032e-05, 5.5333e-04, 3.1840e-04, 1.2511e-04,\n",
      "          3.4785e-04, 3.6851e-05, 6.4471e-04, 4.8329e-04, 3.6673e-04,\n",
      "          2.7192e-04, 7.6580e-05, 2.2281e-04, 1.1102e-04, 2.0048e-05,\n",
      "          4.6965e-05, 3.6109e-05, 5.1199e-05, 2.0398e-05, 1.2602e-05,\n",
      "          2.1108e-05],\n",
      "         [5.1997e-04, 1.0463e-03, 6.5181e-04, 5.2278e-04, 1.4964e-03,\n",
      "          5.7884e-04, 5.8035e-04, 6.3766e-03, 6.0312e-03, 1.0024e-03,\n",
      "          4.3998e-03, 2.6730e-03, 2.3057e-02, 3.4780e-02, 7.6271e-04,\n",
      "          8.0644e-03, 3.3139e-03, 4.9710e-02, 5.3564e-02, 7.6531e-04,\n",
      "          1.6448e-02, 2.4897e-03, 4.2843e-02, 6.5941e-02, 4.8354e-03,\n",
      "          1.5210e-01, 8.7785e-04, 1.1388e-01, 9.6927e-02, 1.8023e-03,\n",
      "          2.6280e-02, 6.5063e-04, 9.6376e-02, 3.3348e-02, 3.6655e-03,\n",
      "          1.6309e-02, 9.3257e-04, 5.7394e-02, 3.1672e-02, 5.0998e-03,\n",
      "          7.8822e-03, 1.4166e-03, 1.2307e-02, 4.9348e-03, 5.8031e-04,\n",
      "          4.9766e-04, 3.7749e-04, 1.0939e-03, 3.7443e-04, 4.5063e-04,\n",
      "          3.1510e-04]]])\n",
      "Player 1 Prediction: tensor([[0.4248, 0.5553, 0.0199, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.2565e-04, 1.3971e-04, 3.0773e-04, 2.4445e-04, 1.5978e-04,\n",
      "          4.1020e-04, 1.8344e-04, 2.0496e-02, 6.0067e-02, 3.8225e-04,\n",
      "          1.1291e-02, 2.7314e-03, 4.2680e-02, 6.2140e-02, 4.2323e-04,\n",
      "          1.9145e-02, 3.1256e-03, 3.6868e-02, 4.0223e-02, 3.9055e-04,\n",
      "          5.3247e-03, 1.2015e-03, 9.6342e-04, 7.5922e-04, 2.4711e-04,\n",
      "          2.0315e-01, 2.8107e-04, 1.3000e-03, 1.0363e-03, 8.1476e-04,\n",
      "          2.9120e-03, 3.4361e-04, 4.3730e-02, 4.7738e-02, 3.8763e-03,\n",
      "          4.3140e-02, 4.2004e-04, 1.2713e-01, 7.4042e-02, 3.0869e-03,\n",
      "          1.6247e-02, 4.3412e-04, 7.8322e-02, 3.9625e-02, 4.8738e-04,\n",
      "          1.6359e-04, 3.6403e-04, 2.8979e-04, 2.6904e-04, 3.0371e-04,\n",
      "          2.5426e-04],\n",
      "         [2.6080e-04, 1.6073e-04, 1.4548e-04, 2.2630e-04, 1.7981e-04,\n",
      "          3.8951e-04, 2.4960e-04, 2.7056e-02, 4.4483e-02, 4.2791e-04,\n",
      "          8.4947e-02, 4.5432e-03, 3.6881e-02, 3.4172e-02, 5.1850e-04,\n",
      "          3.2227e-02, 2.2389e-03, 5.3919e-03, 7.2409e-03, 2.4619e-04,\n",
      "          5.4628e-03, 1.1750e-03, 1.4085e-04, 2.3914e-04, 2.2653e-04,\n",
      "          2.6411e-01, 1.9081e-04, 1.7428e-03, 2.0931e-03, 6.5821e-04,\n",
      "          1.0267e-02, 2.3091e-04, 2.3398e-02, 2.2917e-02, 2.6107e-03,\n",
      "          5.6354e-02, 2.1643e-04, 5.6288e-02, 5.1599e-02, 4.3318e-03,\n",
      "          9.4682e-02, 2.1419e-04, 6.8499e-02, 4.8750e-02, 2.3597e-04,\n",
      "          1.6807e-04, 2.9817e-04, 1.5446e-04, 3.0316e-04, 2.2096e-04,\n",
      "          2.4031e-04],\n",
      "         [1.2215e-04, 6.2982e-05, 7.1856e-05, 6.8796e-05, 7.1464e-05,\n",
      "          1.2908e-04, 6.5824e-05, 1.1794e-03, 1.4367e-03, 1.0751e-04,\n",
      "          7.9382e-04, 4.7447e-04, 8.2070e-03, 6.2860e-03, 1.2768e-04,\n",
      "          8.4665e-03, 3.7910e-04, 4.4586e-02, 5.1338e-02, 5.7693e-05,\n",
      "          8.4216e-01, 2.2058e-04, 3.7327e-03, 5.9533e-03, 5.8794e-03,\n",
      "          3.1406e-03, 1.2513e-04, 2.0174e-04, 1.3463e-04, 1.5311e-04,\n",
      "          3.1992e-04, 1.0172e-04, 1.3939e-03, 1.3487e-03, 3.0394e-04,\n",
      "          1.3062e-03, 7.1985e-05, 1.4851e-03, 2.6221e-03, 2.7503e-04,\n",
      "          1.1224e-03, 1.0918e-04, 1.6295e-03, 1.4276e-03, 8.3806e-05,\n",
      "          8.9788e-05, 1.3454e-04, 1.8275e-04, 9.0451e-05, 7.7819e-05,\n",
      "          8.8969e-05],\n",
      "         [7.4386e-04, 6.5146e-04, 7.0551e-04, 7.7359e-04, 7.7437e-04,\n",
      "          1.4936e-03, 5.8754e-04, 1.6889e-02, 2.7456e-02, 8.9484e-04,\n",
      "          1.2122e-02, 2.5670e-03, 4.3832e-02, 5.1114e-02, 1.2147e-03,\n",
      "          1.1495e-02, 3.1521e-03, 4.0712e-02, 3.7141e-02, 8.2476e-04,\n",
      "          4.2119e-02, 2.2068e-03, 3.4658e-03, 2.5526e-03, 8.5664e-04,\n",
      "          2.4396e-01, 6.2288e-04, 8.8494e-03, 9.6206e-03, 1.4983e-03,\n",
      "          4.4301e-02, 7.5953e-04, 7.2764e-02, 4.7871e-02, 3.3231e-03,\n",
      "          3.1727e-02, 1.0068e-03, 6.4957e-02, 6.4160e-02, 2.6899e-03,\n",
      "          1.9346e-02, 6.3268e-04, 4.5012e-02, 2.4314e-02, 1.3129e-03,\n",
      "          4.6642e-04, 9.9738e-04, 9.5740e-04, 1.0123e-03, 9.3794e-04,\n",
      "          5.5069e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8641, 0.0232, 0.1126]])\n",
      "Player 0 Prediction: tensor([[[2.2187e-04, 1.3095e-04, 4.3868e-04, 1.9821e-04, 1.8750e-04,\n",
      "          3.2123e-04, 1.2734e-04, 2.0089e-02, 2.4261e-02, 3.1720e-04,\n",
      "          4.0783e-03, 1.7278e-03, 1.0001e-01, 9.5987e-02, 2.9627e-04,\n",
      "          1.8270e-03, 2.3855e-03, 2.2253e-02, 1.5720e-02, 2.2993e-04,\n",
      "          1.3123e-03, 8.9125e-04, 5.0754e-04, 5.6212e-04, 1.9852e-04,\n",
      "          2.0559e-01, 2.0498e-04, 7.8155e-04, 8.1969e-04, 5.1103e-04,\n",
      "          2.0255e-03, 2.6454e-04, 8.7627e-03, 1.3159e-02, 3.0277e-03,\n",
      "          9.2304e-03, 3.4480e-04, 2.0308e-01, 1.7675e-01, 2.2417e-03,\n",
      "          1.3280e-02, 5.4238e-04, 3.5107e-02, 2.8322e-02, 2.4235e-04,\n",
      "          2.1298e-04, 3.3297e-04, 1.6763e-04, 3.0773e-04, 2.8585e-04,\n",
      "          1.3280e-04],\n",
      "         [1.8001e-04, 1.3576e-04, 1.5509e-04, 1.7726e-04, 1.9691e-04,\n",
      "          2.2925e-04, 1.5082e-04, 7.3252e-02, 7.1364e-02, 4.3216e-04,\n",
      "          4.0600e-03, 2.9210e-03, 4.9600e-02, 3.3918e-02, 2.0672e-04,\n",
      "          2.9186e-03, 1.4042e-03, 4.5507e-03, 8.2770e-03, 2.5886e-04,\n",
      "          1.5059e-03, 1.4421e-03, 8.1413e-05, 1.6487e-04, 1.9275e-04,\n",
      "          2.0367e-01, 1.3931e-04, 7.8604e-04, 1.4721e-03, 5.2959e-04,\n",
      "          3.4060e-03, 2.4212e-04, 1.2182e-02, 1.0103e-02, 1.6281e-03,\n",
      "          5.2724e-03, 1.4562e-04, 9.6210e-02, 8.0587e-02, 2.3196e-03,\n",
      "          1.0077e-02, 2.2952e-04, 1.8056e-01, 1.3134e-01, 9.5859e-05,\n",
      "          1.8845e-04, 2.9519e-04, 8.6867e-05, 3.0309e-04, 1.5871e-04,\n",
      "          1.9977e-04],\n",
      "         [5.0737e-04, 3.6266e-04, 4.4986e-04, 3.5952e-04, 3.7988e-04,\n",
      "          4.0025e-04, 2.1732e-04, 4.1894e-03, 5.1824e-03, 3.5149e-04,\n",
      "          1.4392e-03, 1.3580e-03, 4.7826e-02, 3.9132e-02, 4.0566e-04,\n",
      "          1.1168e-02, 1.0106e-03, 3.6905e-01, 3.5493e-01, 2.7294e-04,\n",
      "          3.4802e-02, 1.0600e-03, 9.7887e-03, 1.6662e-02, 2.0826e-02,\n",
      "          1.2366e-02, 5.7005e-04, 7.1947e-04, 5.8818e-04, 3.8558e-04,\n",
      "          5.5425e-04, 4.7744e-04, 3.5235e-03, 3.5485e-03, 8.7111e-04,\n",
      "          1.4183e-03, 2.5789e-04, 1.2412e-02, 1.7108e-02, 1.0035e-03,\n",
      "          2.2401e-03, 5.0484e-04, 8.4381e-03, 8.3138e-03, 2.5763e-04,\n",
      "          4.3088e-04, 5.3319e-04, 4.8845e-04, 3.0371e-04, 2.3822e-04,\n",
      "          3.1695e-04],\n",
      "         [8.4298e-04, 9.3791e-04, 6.3628e-04, 1.0502e-03, 7.3945e-04,\n",
      "          1.2163e-03, 3.7839e-04, 2.5914e-02, 3.8325e-02, 8.0062e-04,\n",
      "          4.1949e-03, 2.7173e-03, 6.1931e-02, 6.1421e-02, 8.2703e-04,\n",
      "          4.8242e-03, 1.9801e-03, 5.3326e-02, 5.5870e-02, 7.8615e-04,\n",
      "          5.9024e-03, 1.9868e-03, 3.6938e-03, 2.4607e-03, 5.8635e-04,\n",
      "          2.1878e-01, 9.0090e-04, 7.2405e-03, 7.5449e-03, 9.7280e-04,\n",
      "          1.5877e-02, 7.1803e-04, 4.9837e-02, 2.7359e-02, 3.1013e-03,\n",
      "          6.0770e-03, 9.6359e-04, 1.2339e-01, 9.6904e-02, 3.3812e-03,\n",
      "          9.8004e-03, 8.9598e-04, 5.8180e-02, 2.9034e-02, 1.4538e-03,\n",
      "          7.5212e-04, 7.2960e-04, 7.2169e-04, 7.8818e-04, 7.2464e-04,\n",
      "          5.2684e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9610, 0.0000, 0.0390, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 47765\n",
      "Average episode length: 4.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5558/10000 (55.6%)\n",
      "    Average reward: +0.596\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4442/10000 (44.4%)\n",
      "    Average reward: -0.596\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5035 (21.5%)\n",
      "    Action 1: 13314 (56.8%)\n",
      "    Action 2: 1716 (7.3%)\n",
      "    Action 3: 3395 (14.5%)\n",
      "  Player 1:\n",
      "    Action 0: 11938 (49.1%)\n",
      "    Action 1: 10878 (44.8%)\n",
      "    Action 2: 682 (2.8%)\n",
      "    Action 3: 807 (3.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5957.5, -5957.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.940 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.023 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.982\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5958\n",
      "   Testing specific player: 1\n",
      "   At training step: 2000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8149, 0.0274, 0.1577]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8047, 0.0323, 0.1630]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 2000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 50218\n",
      "Average episode length: 5.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6198/10000 (62.0%)\n",
      "    Average reward: +0.032\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3802/10000 (38.0%)\n",
      "    Average reward: -0.032\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 19963 (75.7%)\n",
      "    Action 1: 6393 (24.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6090 (25.5%)\n",
      "    Action 1: 14929 (62.6%)\n",
      "    Action 2: 659 (2.8%)\n",
      "    Action 3: 2184 (9.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [323.5, -323.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.799 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.926 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.863\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0323\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3003/50000 [04:32<55:35, 14.09it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 3000:\n",
      "   Player 0 RL buffer: 194827/200000\n",
      "   Player 0 SL buffer: 19497/2000000\n",
      "   Player 1 RL buffer: 189297/200000\n",
      "   Player 1 SL buffer: 18931/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4000/50000 [05:45<49:53, 15.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 4000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 25061/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 24302/2000000\n",
      "P1 SL Buffer Size:  25061\n",
      "P1 SL buffer distribution [11270.  9203.  1153.  3435.]\n",
      "P1 actions distribution [0.44970273 0.36722397 0.04600774 0.13706556]\n",
      "P2 SL Buffer Size:  24302\n",
      "P2 SL buffer distribution [ 9749. 10409.   766.  3378.]\n",
      "P2 actions distribution [0.4011604  0.42831866 0.03152004 0.13900091]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4000/50000 [05:56<49:53, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[3.8072e-04, 1.4057e-04, 3.6222e-04, 4.7484e-04, 3.4431e-04,\n",
      "          3.3333e-04, 5.6834e-04, 4.2412e-03, 1.1121e-02, 4.0170e-03,\n",
      "          3.1537e-02, 1.2691e-02, 2.8386e-02, 7.4226e-02, 1.6076e-03,\n",
      "          1.0511e-01, 3.3953e-02, 6.1973e-02, 7.5860e-02, 9.0377e-04,\n",
      "          2.3057e-02, 2.4501e-03, 2.1864e-02, 1.8802e-02, 1.1966e-03,\n",
      "          1.5968e-01, 3.3009e-04, 2.2917e-02, 2.5245e-02, 4.8260e-03,\n",
      "          5.1294e-02, 2.0492e-04, 5.1313e-02, 2.5073e-02, 1.2793e-02,\n",
      "          3.7480e-02, 3.9596e-04, 3.3597e-02, 1.4708e-02, 1.2406e-02,\n",
      "          1.5104e-02, 3.9697e-03, 7.0558e-03, 3.6170e-03, 2.9515e-04,\n",
      "          1.7130e-04, 5.7598e-04, 3.5896e-04, 2.0848e-04, 6.5288e-04,\n",
      "          1.2192e-04],\n",
      "         [2.5942e-04, 1.2031e-04, 2.1435e-04, 3.0122e-04, 2.3841e-04,\n",
      "          2.1895e-04, 2.6800e-04, 1.1812e-02, 5.3763e-02, 1.5979e-03,\n",
      "          4.8416e-02, 1.3075e-02, 1.9637e-02, 6.1055e-02, 5.1302e-04,\n",
      "          1.8248e-01, 2.3077e-02, 2.2468e-02, 1.9792e-02, 3.8186e-04,\n",
      "          6.8185e-02, 2.2377e-03, 5.7197e-04, 5.1255e-04, 7.9186e-04,\n",
      "          1.6300e-01, 2.8375e-04, 1.7931e-02, 2.2952e-02, 2.6431e-03,\n",
      "          2.6054e-02, 1.1702e-04, 2.2475e-02, 1.2027e-02, 9.0269e-03,\n",
      "          6.1195e-02, 1.3049e-04, 2.8948e-02, 1.2532e-02, 1.5682e-02,\n",
      "          2.2417e-02, 3.3721e-03, 3.5769e-02, 9.9698e-03, 1.9250e-04,\n",
      "          1.3273e-04, 3.8673e-04, 2.5296e-04, 1.3214e-04, 2.6199e-04,\n",
      "          1.2977e-04],\n",
      "         [3.0617e-05, 1.3647e-05, 2.4795e-05, 2.5780e-05, 2.2620e-05,\n",
      "          2.7193e-05, 4.1961e-05, 1.2719e-04, 1.9545e-04, 1.0979e-04,\n",
      "          2.1865e-04, 2.2880e-04, 2.7088e-03, 4.1627e-03, 4.6637e-05,\n",
      "          8.8036e-03, 2.5621e-04, 7.7103e-03, 5.6631e-03, 3.8006e-05,\n",
      "          6.1551e-03, 6.1320e-05, 5.7367e-02, 1.7027e-01, 7.3179e-01,\n",
      "          3.2028e-04, 3.4468e-05, 2.7417e-04, 2.2043e-04, 2.2122e-04,\n",
      "          1.7950e-04, 1.6361e-05, 3.2371e-04, 3.1801e-04, 1.7877e-04,\n",
      "          1.7236e-04, 2.0519e-05, 2.4730e-04, 3.2411e-04, 1.8041e-04,\n",
      "          2.2183e-04, 1.4519e-04, 2.1739e-04, 1.0381e-04, 2.9027e-05,\n",
      "          2.0965e-05, 4.1014e-05, 2.7440e-05, 1.6817e-05, 2.5869e-05,\n",
      "          1.8567e-05],\n",
      "         [4.8316e-04, 3.6063e-04, 6.7264e-04, 1.0763e-03, 5.2854e-04,\n",
      "          4.0644e-04, 6.1928e-04, 2.5775e-03, 5.7193e-03, 1.8898e-03,\n",
      "          7.3848e-03, 4.2341e-03, 2.6817e-02, 8.5127e-02, 1.2490e-03,\n",
      "          1.3836e-02, 7.2922e-03, 1.0528e-01, 1.0843e-01, 1.0364e-03,\n",
      "          1.0412e-02, 1.3496e-03, 3.2033e-02, 3.1260e-02, 4.1040e-03,\n",
      "          1.9728e-01, 6.3643e-04, 5.5508e-02, 4.3912e-02, 2.0205e-03,\n",
      "          6.7740e-03, 3.2650e-04, 8.9637e-02, 4.6513e-02, 4.2260e-03,\n",
      "          5.8498e-03, 3.3468e-04, 4.8820e-02, 2.2279e-02, 5.2300e-03,\n",
      "          4.8365e-03, 2.9284e-03, 3.5847e-03, 2.2064e-03, 5.0518e-04,\n",
      "          2.4993e-04, 8.1886e-04, 4.9389e-04, 3.0877e-04, 3.8962e-04,\n",
      "          1.4353e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 44058\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6120/10000 (61.2%)\n",
      "    Average reward: -0.452\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3880/10000 (38.8%)\n",
      "    Average reward: +0.452\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 10796 (49.0%)\n",
      "    Action 1: 8692 (39.4%)\n",
      "    Action 2: 829 (3.8%)\n",
      "    Action 3: 1732 (7.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4756 (21.6%)\n",
      "    Action 1: 12207 (55.5%)\n",
      "    Action 2: 2344 (10.7%)\n",
      "    Action 3: 2702 (12.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-4519.5, 4519.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.949 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.992\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.4520\n",
      "   Testing specific player: 0\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7746, 0.0368, 0.1886]])\n",
      "Player 0 Prediction: tensor([[0.4764, 0.4999, 0.0237, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49256\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5564/10000 (55.6%)\n",
      "    Average reward: -0.010\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4436/10000 (44.4%)\n",
      "    Average reward: +0.010\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7881 (32.6%)\n",
      "    Action 1: 11916 (49.3%)\n",
      "    Action 2: 988 (4.1%)\n",
      "    Action 3: 3397 (14.0%)\n",
      "  Player 1:\n",
      "    Action 0: 16977 (67.7%)\n",
      "    Action 1: 8097 (32.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-95.5, 95.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.030 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.908 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.969\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0095\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6886, 0.2760, 0.0354, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[5.7430e-05, 1.1985e-04, 8.7961e-05, 1.5168e-04, 2.3526e-04,\n",
      "          1.7083e-04, 1.4572e-04, 8.5998e-04, 2.4681e-03, 1.8567e-03,\n",
      "          3.9249e-02, 1.3405e-02, 1.6368e-03, 4.6415e-03, 7.0580e-04,\n",
      "          6.5844e-02, 1.8744e-02, 2.0560e-02, 1.7641e-02, 2.2155e-04,\n",
      "          2.8490e-02, 1.8047e-03, 1.1164e-02, 7.3973e-03, 1.5186e-04,\n",
      "          2.3317e-01, 8.5469e-05, 6.7701e-03, 7.5055e-03, 1.8992e-03,\n",
      "          3.6019e-02, 1.5895e-04, 5.2358e-02, 3.0029e-02, 4.1268e-02,\n",
      "          1.4865e-01, 4.2909e-04, 1.1240e-02, 8.6482e-03, 2.5934e-02,\n",
      "          1.4006e-01, 4.1544e-03, 7.6126e-03, 5.4034e-03, 1.0665e-04,\n",
      "          9.4011e-05, 4.7262e-05, 1.9395e-04, 1.3135e-04, 9.1049e-05,\n",
      "          1.2985e-04],\n",
      "         [4.7493e-05, 4.8680e-05, 8.8702e-05, 1.1905e-04, 2.0700e-04,\n",
      "          1.2986e-04, 8.4606e-05, 3.2731e-03, 4.1812e-02, 8.7990e-04,\n",
      "          1.0903e-02, 1.0629e-02, 2.0495e-02, 3.9847e-02, 1.4639e-04,\n",
      "          3.5196e-02, 4.9168e-03, 2.1838e-02, 3.6655e-02, 1.6454e-04,\n",
      "          5.1272e-02, 1.8404e-03, 6.7702e-04, 6.1727e-04, 1.7602e-04,\n",
      "          1.6235e-01, 6.3071e-05, 1.0673e-02, 1.0013e-02, 8.6166e-04,\n",
      "          1.2756e-02, 7.6320e-05, 6.9483e-02, 2.4437e-02, 1.0548e-02,\n",
      "          8.5747e-02, 1.8205e-04, 8.8187e-02, 6.1916e-02, 2.2974e-02,\n",
      "          4.7167e-02, 8.2711e-04, 9.4298e-02, 1.4769e-02, 4.9382e-05,\n",
      "          6.3977e-05, 5.8887e-05, 1.2435e-04, 9.3604e-05, 1.1469e-04,\n",
      "          1.0108e-04],\n",
      "         [3.8844e-06, 5.2119e-06, 6.5841e-06, 6.0744e-06, 7.0992e-06,\n",
      "          1.1603e-05, 6.3626e-06, 1.9030e-05, 3.7860e-05, 3.6599e-05,\n",
      "          7.5916e-05, 1.2208e-04, 3.6382e-04, 8.2251e-04, 1.2870e-05,\n",
      "          2.2473e-03, 7.8540e-05, 1.6413e-03, 1.6973e-03, 7.3328e-06,\n",
      "          1.4147e-03, 1.8567e-05, 4.3680e-01, 5.1059e-01, 4.2039e-02,\n",
      "          2.4011e-04, 5.7368e-06, 8.2217e-05, 4.2536e-05, 1.9410e-05,\n",
      "          3.8709e-05, 3.8789e-06, 2.8432e-04, 1.0554e-04, 1.2834e-04,\n",
      "          1.3222e-04, 8.8756e-06, 8.9405e-05, 1.6291e-04, 1.2025e-04,\n",
      "          2.5261e-04, 5.3674e-05, 7.6637e-05, 4.6229e-05, 4.5659e-06,\n",
      "          4.1252e-06, 3.2328e-06, 1.2361e-05, 5.1788e-06, 3.1858e-06,\n",
      "          3.7828e-06],\n",
      "         [9.7389e-05, 2.1524e-04, 2.0163e-04, 2.0072e-04, 4.9199e-04,\n",
      "          4.4906e-04, 2.4893e-04, 1.6730e-03, 3.5209e-03, 1.1443e-03,\n",
      "          2.2570e-03, 2.7107e-03, 1.7233e-02, 3.6910e-02, 2.7104e-04,\n",
      "          5.0834e-03, 1.8665e-03, 6.3143e-02, 7.8089e-02, 3.7178e-04,\n",
      "          5.0993e-03, 7.3502e-04, 5.9348e-02, 4.8181e-02, 1.0464e-03,\n",
      "          1.5467e-01, 1.5573e-04, 5.8739e-02, 5.2394e-02, 9.1853e-04,\n",
      "          5.8337e-03, 2.0654e-04, 1.8285e-01, 8.0387e-02, 4.9294e-03,\n",
      "          6.5422e-03, 3.3495e-04, 6.0473e-02, 2.8822e-02, 5.8785e-03,\n",
      "          1.1651e-02, 1.8531e-03, 6.6260e-03, 4.8323e-03, 1.9954e-04,\n",
      "          8.4971e-05, 1.1657e-04, 4.4692e-04, 1.6984e-04, 1.3333e-04,\n",
      "          1.7072e-04]]])\n",
      "Player 1 Prediction: tensor([[0.6059, 0.3591, 0.0350, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.3613e-05, 5.0798e-06, 8.7712e-06, 1.0910e-05, 5.9928e-06,\n",
      "          1.0245e-05, 6.2396e-06, 7.7953e-04, 1.5450e-03, 3.0884e-05,\n",
      "          6.6971e-03, 6.6810e-04, 3.3647e-04, 8.0165e-04, 2.2298e-05,\n",
      "          2.9932e-01, 7.4821e-04, 2.3082e-04, 2.1322e-04, 1.2619e-05,\n",
      "          3.2506e-03, 5.7903e-05, 2.1314e-05, 2.4774e-05, 2.1581e-06,\n",
      "          1.4388e-01, 6.8508e-06, 1.2836e-05, 1.6341e-05, 3.9729e-05,\n",
      "          2.4615e-03, 1.4192e-05, 4.0938e-04, 4.0627e-04, 5.6094e-04,\n",
      "          5.1399e-01, 2.2788e-05, 1.7541e-03, 1.2190e-03, 1.6444e-03,\n",
      "          1.2281e-02, 4.8792e-05, 4.0994e-03, 2.2564e-03, 7.1722e-06,\n",
      "          6.8288e-06, 5.5042e-06, 4.7888e-06, 1.3658e-05, 7.3137e-06,\n",
      "          9.9290e-06],\n",
      "         [2.1540e-05, 1.0773e-05, 1.4404e-05, 2.3949e-05, 2.2151e-05,\n",
      "          4.0035e-05, 1.5569e-05, 2.9926e-03, 8.5686e-03, 6.8637e-05,\n",
      "          9.0268e-02, 3.4300e-03, 2.1648e-03, 2.0991e-03, 4.5848e-05,\n",
      "          1.2199e-01, 5.6079e-04, 7.4003e-04, 1.1420e-03, 1.9675e-05,\n",
      "          8.2979e-04, 1.2443e-04, 1.2321e-05, 2.3965e-05, 5.4846e-06,\n",
      "          1.8069e-01, 1.0865e-05, 5.5086e-05, 6.0165e-05, 4.4171e-05,\n",
      "          2.0476e-02, 3.2875e-05, 1.3095e-03, 7.8378e-04, 3.7993e-04,\n",
      "          3.2602e-01, 3.5330e-05, 4.5204e-03, 4.1831e-03, 8.0383e-03,\n",
      "          1.9419e-01, 3.3131e-05, 1.6712e-02, 7.0711e-03, 1.3208e-05,\n",
      "          1.8481e-05, 2.3331e-05, 8.8734e-06, 1.5408e-05, 1.7606e-05,\n",
      "          2.4796e-05],\n",
      "         [6.9792e-06, 4.6069e-06, 5.1794e-06, 4.4129e-06, 4.5676e-06,\n",
      "          6.9392e-06, 4.1993e-06, 8.4153e-05, 8.3877e-05, 1.4099e-05,\n",
      "          3.1717e-04, 1.0356e-04, 7.4074e-04, 5.1185e-04, 1.1475e-05,\n",
      "          2.2721e-02, 5.0796e-05, 5.7487e-04, 5.5844e-04, 4.0052e-06,\n",
      "          9.6006e-01, 1.9632e-05, 3.8462e-03, 5.9757e-03, 3.3178e-04,\n",
      "          7.1949e-04, 6.5606e-06, 5.7550e-06, 3.8809e-06, 1.2615e-05,\n",
      "          1.4126e-04, 6.2622e-06, 4.3714e-05, 3.3151e-05, 4.3756e-05,\n",
      "          1.8725e-03, 7.0790e-06, 5.0870e-05, 6.8768e-05, 6.8014e-05,\n",
      "          5.8096e-04, 2.1268e-05, 1.1567e-04, 1.2480e-04, 3.6607e-06,\n",
      "          3.6200e-06, 6.0363e-06, 5.9505e-06, 4.7820e-06, 3.9290e-06,\n",
      "          3.2005e-06],\n",
      "         [1.1894e-04, 1.9355e-04, 1.9093e-04, 1.0860e-04, 1.2580e-04,\n",
      "          2.8752e-04, 6.6063e-05, 5.1224e-03, 4.6629e-03, 3.7510e-04,\n",
      "          2.0605e-02, 2.0233e-03, 6.0888e-03, 7.9324e-03, 2.6341e-04,\n",
      "          8.1168e-02, 2.1476e-03, 6.3309e-03, 6.4508e-03, 1.7272e-04,\n",
      "          1.0477e-01, 5.1925e-04, 2.3479e-03, 1.7939e-03, 1.3659e-04,\n",
      "          1.6517e-01, 1.1999e-04, 2.1810e-03, 2.4122e-03, 2.2024e-04,\n",
      "          2.5405e-01, 3.1282e-04, 9.5624e-03, 7.9546e-03, 2.3925e-03,\n",
      "          1.8346e-01, 2.3902e-04, 1.0428e-02, 9.0788e-03, 5.1954e-03,\n",
      "          7.3381e-02, 3.0349e-04, 9.4099e-03, 9.0633e-03, 1.2052e-04,\n",
      "          9.7024e-05, 2.4580e-04, 1.4840e-04, 1.9158e-04, 1.3083e-04,\n",
      "          1.2197e-04]]])\n",
      "Player 1 Prediction: tensor([[0.6779, 0.2877, 0.0344, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39410\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5196/10000 (52.0%)\n",
      "    Average reward: +0.565\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4804/10000 (48.0%)\n",
      "    Average reward: -0.565\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8499 (41.0%)\n",
      "    Action 1: 7078 (34.1%)\n",
      "    Action 2: 2390 (11.5%)\n",
      "    Action 3: 2762 (13.3%)\n",
      "  Player 1:\n",
      "    Action 0: 7319 (39.2%)\n",
      "    Action 1: 9160 (49.0%)\n",
      "    Action 2: 417 (2.2%)\n",
      "    Action 3: 1785 (9.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [5645.5, -5645.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.034 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.045\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.5645\n",
      "   Testing specific player: 1\n",
      "   At training step: 4000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4298, 0.0389, 0.5312]])\n",
      "Player 1 Prediction: tensor([[0.7742, 0.1997, 0.0261, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 4000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 49167\n",
      "Average episode length: 4.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6124/10000 (61.2%)\n",
      "    Average reward: -0.011\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3876/10000 (38.8%)\n",
      "    Average reward: +0.011\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 17999 (71.0%)\n",
      "    Action 1: 7362 (29.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 7141 (30.0%)\n",
      "    Action 1: 13053 (54.8%)\n",
      "    Action 2: 533 (2.2%)\n",
      "    Action 3: 3079 (12.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-112.5, 112.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.869 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.996 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.933\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0112\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 5003/50000 [07:35<46:45, 16.04it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 5000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 30769/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 29768/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5999/50000 [08:43<47:53, 15.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 6000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 36481/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 35320/2000000\n",
      "P1 SL Buffer Size:  36481\n",
      "P1 SL buffer distribution [15267. 14009.  2560.  4645.]\n",
      "P1 actions distribution [0.41849182 0.38400811 0.07017351 0.12732655]\n",
      "P2 SL Buffer Size:  35320\n",
      "P2 SL buffer distribution [13099. 15443.  2018.  4760.]\n",
      "P2 actions distribution [0.37086636 0.43723103 0.05713477 0.13476784]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.5324, 0.2190, 0.2486, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 5999/50000 [08:56<47:53, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40295\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5939/10000 (59.4%)\n",
      "    Average reward: -0.389\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4061/10000 (40.6%)\n",
      "    Average reward: +0.389\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8847 (45.0%)\n",
      "    Action 1: 7994 (40.7%)\n",
      "    Action 2: 1331 (6.8%)\n",
      "    Action 3: 1486 (7.6%)\n",
      "  Player 1:\n",
      "    Action 0: 5389 (26.1%)\n",
      "    Action 1: 10464 (50.7%)\n",
      "    Action 2: 2297 (11.1%)\n",
      "    Action 3: 2487 (12.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-3889.0, 3889.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.003 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.3889\n",
      "   Testing specific player: 0\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8521, 0.0293, 0.1185]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9230, 0.0163, 0.0607]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45581\n",
      "Average episode length: 4.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5154/10000 (51.5%)\n",
      "    Average reward: -0.046\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4846/10000 (48.5%)\n",
      "    Average reward: +0.046\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7435 (32.7%)\n",
      "    Action 1: 10483 (46.1%)\n",
      "    Action 2: 1590 (7.0%)\n",
      "    Action 3: 3242 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 15478 (67.8%)\n",
      "    Action 1: 7353 (32.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-465.0, 465.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.907 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.974\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0465\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5518, 0.2445, 0.2038, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 38526\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5296/10000 (53.0%)\n",
      "    Average reward: +0.467\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4704/10000 (47.0%)\n",
      "    Average reward: -0.467\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 8018 (40.4%)\n",
      "    Action 1: 7813 (39.3%)\n",
      "    Action 2: 2466 (12.4%)\n",
      "    Action 3: 1568 (7.9%)\n",
      "  Player 1:\n",
      "    Action 0: 7345 (39.4%)\n",
      "    Action 1: 8735 (46.8%)\n",
      "    Action 2: 962 (5.2%)\n",
      "    Action 3: 1619 (8.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [4667.0, -4667.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.042 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.4667\n",
      "   Testing specific player: 1\n",
      "   At training step: 6000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.2576, 0.7164, 0.0260, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9024, 0.0118, 0.0858]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 6000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 45966\n",
      "Average episode length: 4.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6290/10000 (62.9%)\n",
      "    Average reward: -0.047\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3710/10000 (37.1%)\n",
      "    Average reward: +0.047\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 16690 (71.5%)\n",
      "    Action 1: 6662 (28.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 6613 (29.2%)\n",
      "    Action 1: 11684 (51.7%)\n",
      "    Action 2: 1165 (5.2%)\n",
      "    Action 3: 3152 (13.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-474.0, 474.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.863 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.937\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.0474\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7002/50000 [10:39<49:00, 14.62it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 7000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 42509/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 40899/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [11:55<56:02, 12.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 8000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 48470/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 46788/2000000\n",
      "P1 SL Buffer Size:  48470\n",
      "P1 SL buffer distribution [19432. 19099.  4172.  5767.]\n",
      "P1 actions distribution [0.40090778 0.39403755 0.08607386 0.11898081]\n",
      "P2 SL Buffer Size:  46788\n",
      "P2 SL buffer distribution [16884. 20315.  3523.  6066.]\n",
      "P2 actions distribution [0.36086176 0.43419253 0.07529708 0.12964863]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8000/50000 [12:06<56:02, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[1.1011e-04, 1.3547e-04, 1.3973e-04, 1.8167e-04, 1.6333e-04,\n",
      "          1.9065e-04, 1.0640e-04, 2.9107e-03, 8.5736e-03, 6.1852e-03,\n",
      "          9.6457e-03, 7.2477e-03, 8.5234e-03, 2.5288e-02, 1.2831e-03,\n",
      "          1.9887e-02, 9.4473e-03, 2.8875e-02, 3.7722e-02, 2.6758e-04,\n",
      "          7.2139e-03, 1.8706e-03, 1.6815e-02, 1.7529e-02, 7.8824e-04,\n",
      "          2.6258e-01, 1.3283e-04, 4.9642e-02, 4.3902e-02, 2.4443e-03,\n",
      "          1.6466e-02, 2.4661e-04, 1.1930e-01, 8.6826e-02, 7.7822e-03,\n",
      "          2.7574e-02, 5.7752e-04, 6.2809e-02, 2.1759e-02, 8.8487e-03,\n",
      "          2.5047e-02, 9.9897e-03, 3.0522e-02, 1.1424e-02, 1.7854e-04,\n",
      "          1.4572e-04, 1.4757e-04, 1.5585e-04, 1.2018e-04, 1.8157e-04,\n",
      "          9.9595e-05],\n",
      "         [7.3081e-05, 1.0073e-04, 9.6861e-05, 1.3142e-04, 9.4663e-05,\n",
      "          9.8355e-05, 6.4968e-05, 6.3053e-03, 2.5694e-02, 3.0441e-03,\n",
      "          2.2130e-02, 9.9706e-03, 5.2866e-03, 2.1943e-02, 2.5472e-04,\n",
      "          7.4468e-02, 8.3614e-03, 4.4081e-03, 6.5060e-03, 1.4585e-04,\n",
      "          1.3046e-02, 1.2293e-03, 1.9759e-04, 2.7805e-04, 4.9558e-04,\n",
      "          2.2167e-01, 8.6863e-05, 3.9239e-02, 3.4928e-02, 7.4946e-04,\n",
      "          3.5785e-02, 1.5099e-04, 2.0584e-02, 1.6577e-02, 1.6649e-02,\n",
      "          1.4849e-01, 1.1752e-04, 4.8982e-02, 1.8311e-02, 3.7949e-02,\n",
      "          7.1213e-02, 6.2057e-03, 6.1527e-02, 1.5677e-02, 1.2583e-04,\n",
      "          1.1494e-04, 9.6950e-05, 9.3838e-05, 9.0322e-05, 7.8482e-05,\n",
      "          8.3921e-05],\n",
      "         [7.3774e-06, 1.3085e-05, 8.9272e-06, 5.8731e-06, 6.7555e-06,\n",
      "          1.1411e-05, 9.3590e-06, 7.1344e-05, 8.3881e-05, 5.5503e-05,\n",
      "          5.2644e-05, 1.1864e-04, 1.1361e-03, 1.2871e-03, 1.6669e-05,\n",
      "          1.5840e-03, 7.5968e-05, 2.8914e-03, 2.2335e-03, 9.9684e-06,\n",
      "          4.1059e-03, 3.8247e-05, 1.5606e-02, 1.6233e-01, 8.0536e-01,\n",
      "          2.1255e-04, 1.0645e-05, 3.4352e-04, 2.5314e-04, 5.9037e-05,\n",
      "          7.3201e-05, 1.4004e-05, 2.9447e-04, 3.0106e-04, 1.0883e-04,\n",
      "          1.0811e-04, 1.7702e-05, 2.1537e-04, 1.6011e-04, 1.0869e-04,\n",
      "          2.1473e-04, 9.7058e-05, 1.5257e-04, 6.4286e-05, 1.4652e-05,\n",
      "          1.2172e-05, 5.9459e-06, 5.4715e-06, 9.5723e-06, 8.6556e-06,\n",
      "          7.1207e-06],\n",
      "         [1.3278e-04, 2.7799e-04, 2.2490e-04, 2.4403e-04, 1.5923e-04,\n",
      "          1.7260e-04, 1.7400e-04, 1.3800e-03, 2.4555e-03, 1.1248e-03,\n",
      "          1.5634e-03, 2.0108e-03, 1.2979e-02, 3.4467e-02, 4.1802e-04,\n",
      "          2.2355e-03, 2.2885e-03, 3.7725e-02, 5.6267e-02, 2.8593e-04,\n",
      "          1.4315e-03, 6.5525e-04, 1.2988e-02, 2.6677e-02, 2.0218e-03,\n",
      "          1.8041e-01, 2.6730e-04, 1.6171e-01, 8.8302e-02, 5.4742e-04,\n",
      "          4.3171e-03, 4.1403e-04, 1.3082e-01, 1.1954e-01, 3.0696e-03,\n",
      "          3.5042e-03, 2.9231e-04, 5.8173e-02, 3.0009e-02, 2.5273e-03,\n",
      "          6.0584e-03, 2.0139e-03, 4.4799e-03, 2.1113e-03, 2.1992e-04,\n",
      "          1.4996e-04, 1.7895e-04, 1.5097e-04, 1.3514e-04, 1.6447e-04,\n",
      "          7.0399e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3323, 0.6557, 0.0121, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[3.7063e-06, 1.8007e-06, 4.1460e-06, 5.0733e-06, 4.8725e-06,\n",
      "          2.3728e-06, 4.0524e-06, 9.5830e-04, 9.0095e-04, 2.7733e-05,\n",
      "          3.4316e-03, 8.8274e-04, 2.8949e-04, 3.4098e-04, 9.1995e-06,\n",
      "          3.1381e-01, 8.3228e-04, 3.9747e-05, 8.5686e-05, 6.2346e-06,\n",
      "          1.0976e-03, 1.2687e-05, 5.5152e-06, 7.0532e-06, 1.9402e-06,\n",
      "          1.6815e-01, 3.5219e-06, 6.1694e-06, 5.3721e-06, 1.2177e-05,\n",
      "          1.0518e-03, 6.6469e-06, 1.4012e-04, 2.3097e-04, 3.5079e-04,\n",
      "          4.8806e-01, 9.7751e-06, 1.0414e-03, 4.6856e-04, 1.3821e-03,\n",
      "          1.0379e-02, 3.9534e-05, 3.1885e-03, 2.6892e-03, 2.5396e-06,\n",
      "          3.5614e-06, 3.4459e-06, 2.1606e-06, 3.2668e-06, 4.0518e-06,\n",
      "          2.9284e-06],\n",
      "         [2.7230e-06, 3.2995e-06, 3.4837e-06, 3.6740e-06, 4.5097e-06,\n",
      "          3.5131e-06, 3.5350e-06, 1.7877e-03, 1.4285e-03, 4.3864e-05,\n",
      "          5.4331e-02, 1.2120e-02, 8.8020e-04, 8.3499e-04, 1.0233e-05,\n",
      "          1.3307e-01, 1.7391e-04, 7.0363e-05, 6.9381e-05, 8.4641e-06,\n",
      "          3.1139e-05, 1.9043e-05, 1.3905e-06, 3.2242e-06, 8.5562e-07,\n",
      "          3.1287e-01, 4.3101e-06, 1.3526e-05, 1.0399e-05, 3.8803e-05,\n",
      "          2.4743e-02, 5.7259e-06, 2.6268e-04, 2.4331e-04, 1.7608e-04,\n",
      "          2.5771e-01, 8.7550e-06, 8.4881e-04, 7.3040e-04, 1.5379e-02,\n",
      "          1.7620e-01, 6.3832e-05, 2.8682e-03, 2.8935e-03, 3.9682e-06,\n",
      "          4.4320e-06, 2.8666e-06, 2.3645e-06, 2.6749e-06, 4.4574e-06,\n",
      "          6.3022e-06],\n",
      "         [1.7039e-06, 2.6708e-06, 1.8991e-06, 3.3043e-06, 1.2495e-06,\n",
      "          2.4256e-06, 2.5641e-06, 2.9515e-05, 1.6359e-05, 1.1233e-05,\n",
      "          2.4444e-04, 9.5089e-05, 4.2471e-04, 4.2902e-04, 6.3672e-06,\n",
      "          1.8679e-02, 2.3256e-05, 1.2312e-04, 1.5676e-04, 2.6882e-06,\n",
      "          9.7569e-01, 2.7213e-06, 5.4481e-04, 1.2016e-03, 1.4586e-04,\n",
      "          2.3584e-04, 1.6401e-06, 3.2952e-06, 2.8056e-06, 4.4077e-06,\n",
      "          1.5315e-04, 2.8488e-06, 1.8104e-05, 1.4407e-05, 3.1114e-05,\n",
      "          1.1171e-03, 4.9164e-06, 1.8915e-05, 2.3499e-05, 1.3919e-04,\n",
      "          2.5888e-04, 7.1593e-06, 4.8606e-05, 5.7293e-05, 2.5718e-06,\n",
      "          1.9442e-06, 3.0655e-06, 2.2578e-06, 1.5757e-06, 1.2084e-06,\n",
      "          3.5303e-06],\n",
      "         [9.6765e-05, 4.2413e-05, 9.8879e-05, 9.1138e-05, 7.6473e-05,\n",
      "          6.7764e-05, 5.0922e-05, 1.7721e-03, 2.4267e-03, 4.1324e-04,\n",
      "          2.6057e-02, 7.3849e-03, 1.3117e-03, 2.8647e-03, 1.9122e-04,\n",
      "          8.6493e-02, 5.6092e-03, 1.2799e-03, 1.7418e-03, 8.8185e-05,\n",
      "          1.0238e-01, 1.6095e-04, 3.8190e-04, 7.4626e-04, 1.0979e-04,\n",
      "          2.0338e-01, 5.7907e-05, 1.1926e-03, 1.6664e-03, 1.2172e-04,\n",
      "          3.2511e-01, 1.3703e-04, 2.6573e-03, 2.9513e-03, 1.8826e-03,\n",
      "          1.0342e-01, 1.7228e-04, 2.7705e-03, 3.1613e-03, 1.5104e-02,\n",
      "          8.7849e-02, 3.9301e-04, 3.1314e-03, 2.4526e-03, 6.6800e-05,\n",
      "          4.2345e-05, 9.4078e-05, 8.6670e-05, 5.2482e-05, 2.8474e-05,\n",
      "          7.8415e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3446, 0.6479, 0.0075, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 35859\n",
      "Average episode length: 3.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5764/10000 (57.6%)\n",
      "    Average reward: -0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4236/10000 (42.4%)\n",
      "    Average reward: +0.286\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7192 (39.9%)\n",
      "    Action 1: 7040 (39.0%)\n",
      "    Action 2: 1851 (10.3%)\n",
      "    Action 3: 1946 (10.8%)\n",
      "  Player 1:\n",
      "    Action 0: 5638 (31.6%)\n",
      "    Action 1: 7358 (41.3%)\n",
      "    Action 2: 2688 (15.1%)\n",
      "    Action 3: 2146 (12.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2857.0, 2857.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2857\n",
      "   Testing specific player: 0\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.1580, 0.1622, 0.6798]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.3329, 0.1787, 0.4884]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43502\n",
      "Average episode length: 4.4 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5115/10000 (51.1%)\n",
      "    Average reward: +0.078\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4885/10000 (48.9%)\n",
      "    Average reward: -0.078\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6615 (30.3%)\n",
      "    Action 1: 10167 (46.6%)\n",
      "    Action 2: 1956 (9.0%)\n",
      "    Action 3: 3064 (14.1%)\n",
      "  Player 1:\n",
      "    Action 0: 15103 (69.6%)\n",
      "    Action 1: 6597 (30.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [783.5, -783.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.035 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.886 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.961\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0784\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1872, 0.7889, 0.0239, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 32216\n",
      "Average episode length: 3.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4586/10000 (45.9%)\n",
      "    Average reward: +0.362\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5414/10000 (54.1%)\n",
      "    Average reward: -0.362\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4088 (24.6%)\n",
      "    Action 1: 6630 (40.0%)\n",
      "    Action 2: 4501 (27.1%)\n",
      "    Action 3: 1376 (8.3%)\n",
      "  Player 1:\n",
      "    Action 0: 6051 (38.7%)\n",
      "    Action 1: 6654 (42.6%)\n",
      "    Action 2: 1530 (9.8%)\n",
      "    Action 3: 1386 (8.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3617.5, -3617.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.041\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3618\n",
      "   Testing specific player: 1\n",
      "   At training step: 8000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5723, 0.3693, 0.0584, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.5843, 0.3973, 0.0184, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.6738, 0.3080, 0.0181, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 8000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 43396\n",
      "Average episode length: 4.3 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6340/10000 (63.4%)\n",
      "    Average reward: -0.150\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3660/10000 (36.6%)\n",
      "    Average reward: +0.150\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15882 (72.6%)\n",
      "    Action 1: 5995 (27.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5868 (27.3%)\n",
      "    Action 1: 10780 (50.1%)\n",
      "    Action 2: 1653 (7.7%)\n",
      "    Action 3: 3218 (15.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1500.5, 1500.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.847 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.011 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.929\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1500\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9003/50000 [13:50<39:14, 17.41it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 9000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 54328/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 52583/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9998/50000 [14:49<30:36, 21.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 10000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 60077/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 58030/2000000\n",
      "P1 SL Buffer Size:  60077\n",
      "P1 SL buffer distribution [22919. 23902.  6490.  6766.]\n",
      "P1 actions distribution [0.38149375 0.39785608 0.10802803 0.11262213]\n",
      "P2 SL Buffer Size:  58030\n",
      "P2 SL buffer distribution [20309. 24818.  5624.  7279.]\n",
      "P2 actions distribution [0.34997415 0.42767534 0.09691539 0.12543512]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.2333, 0.7475, 0.0192, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.6057e-05, 2.3875e-05, 1.3601e-05, 1.6577e-05, 1.6388e-05,\n",
      "          6.7108e-06, 1.0016e-05, 8.5761e-04, 7.0554e-04, 2.1799e-04,\n",
      "          1.0592e-01, 1.9757e-02, 4.4614e-04, 2.3254e-04, 7.3294e-05,\n",
      "          2.8553e-01, 4.1990e-02, 2.1007e-03, 1.5396e-03, 4.6977e-05,\n",
      "          4.7813e-02, 3.9080e-04, 1.3681e-03, 1.4056e-03, 1.4606e-05,\n",
      "          1.6930e-01, 1.8812e-05, 2.5643e-04, 2.9951e-04, 1.6651e-04,\n",
      "          4.6008e-02, 2.7045e-05, 1.7158e-03, 9.6353e-04, 2.3967e-02,\n",
      "          1.5294e-01, 7.0448e-05, 2.7381e-04, 5.0829e-04, 5.7055e-03,\n",
      "          8.5028e-02, 2.8940e-04, 5.5512e-04, 1.2320e-03, 3.7651e-05,\n",
      "          2.0240e-05, 3.3202e-05, 8.1874e-06, 1.2745e-05, 1.6302e-05,\n",
      "          1.7764e-05],\n",
      "         [2.2827e-05, 2.5247e-05, 2.3341e-05, 1.3482e-05, 3.0789e-05,\n",
      "          2.0290e-05, 8.6436e-06, 2.8365e-02, 3.4355e-01, 2.4540e-04,\n",
      "          9.4596e-04, 2.6571e-03, 3.1034e-02, 7.5234e-02, 6.2597e-05,\n",
      "          3.3864e-03, 1.4965e-03, 9.2001e-03, 1.6045e-02, 3.4756e-05,\n",
      "          3.7692e-04, 4.1990e-04, 3.3500e-04, 1.0542e-04, 1.1324e-05,\n",
      "          1.8440e-01, 3.8603e-05, 4.3662e-04, 8.5547e-04, 4.3152e-04,\n",
      "          2.9643e-02, 3.6297e-05, 1.8499e-02, 1.6671e-02, 1.7858e-03,\n",
      "          3.7442e-03, 1.1837e-04, 4.6650e-02, 2.6777e-02, 2.3540e-03,\n",
      "          2.4655e-03, 3.9699e-04, 1.3327e-01, 1.7598e-02, 1.8229e-05,\n",
      "          2.9013e-05, 2.6391e-05, 1.3789e-05, 2.9757e-05, 3.7370e-05,\n",
      "          2.7636e-05],\n",
      "         [2.3357e-06, 1.4266e-06, 1.8834e-06, 1.0556e-06, 8.6448e-07,\n",
      "          7.7836e-07, 6.8382e-07, 2.2740e-05, 3.8054e-05, 4.0268e-06,\n",
      "          4.3198e-05, 3.2216e-05, 1.9150e-04, 3.1693e-04, 2.9944e-06,\n",
      "          6.9335e-04, 2.8311e-05, 5.8275e-04, 3.6019e-04, 1.7456e-06,\n",
      "          5.2611e-04, 3.2710e-06, 4.6025e-01, 5.3632e-01, 1.2281e-04,\n",
      "          6.5847e-05, 1.4086e-06, 9.7050e-06, 8.3081e-06, 6.3060e-06,\n",
      "          3.4841e-05, 1.8289e-06, 2.6845e-05, 2.2017e-05, 4.7778e-05,\n",
      "          2.9186e-05, 3.3621e-06, 2.6831e-05, 3.6042e-05, 2.2104e-05,\n",
      "          2.6484e-05, 5.5417e-06, 3.3364e-05, 2.4670e-05, 1.7416e-06,\n",
      "          1.0479e-06, 1.9060e-06, 1.0306e-06, 2.7565e-06, 2.0318e-06,\n",
      "          2.2685e-06],\n",
      "         [7.1050e-05, 1.0726e-04, 1.2988e-04, 1.1984e-04, 6.0058e-05,\n",
      "          3.0989e-05, 6.6052e-05, 2.0342e-03, 6.4530e-03, 5.4221e-04,\n",
      "          3.1262e-03, 1.8821e-03, 1.5798e-02, 2.9127e-02, 2.0857e-04,\n",
      "          4.1862e-03, 7.2532e-03, 2.4857e-02, 4.4499e-02, 1.3470e-04,\n",
      "          4.7971e-03, 2.8166e-04, 5.0994e-02, 4.8559e-02, 1.0155e-04,\n",
      "          5.3594e-01, 7.8602e-05, 1.8859e-02, 5.6810e-02, 4.2313e-04,\n",
      "          1.8399e-02, 2.5944e-04, 3.9682e-02, 3.1271e-02, 4.1309e-03,\n",
      "          2.2864e-03, 3.4867e-04, 2.5792e-02, 9.1618e-03, 1.6616e-03,\n",
      "          2.7931e-03, 6.9072e-04, 3.6128e-03, 1.5678e-03, 1.2368e-04,\n",
      "          8.2203e-05, 1.6528e-04, 9.1762e-05, 1.2194e-04, 8.7807e-05,\n",
      "          1.3841e-04]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9624, 0.0059, 0.0317]])\n",
      "Player 1 Prediction: tensor([[[2.0662e-06, 7.7085e-07, 2.5523e-06, 3.7414e-06, 2.3805e-06,\n",
      "          2.2037e-06, 2.5548e-06, 1.3147e-03, 7.5789e-04, 1.5327e-05,\n",
      "          3.0737e-03, 3.7223e-04, 2.6659e-04, 2.1362e-04, 3.7675e-06,\n",
      "          5.5062e-01, 2.1368e-04, 3.8044e-05, 5.1705e-05, 2.9448e-06,\n",
      "          3.0213e-04, 1.1132e-05, 4.1621e-06, 5.0522e-06, 8.1954e-07,\n",
      "          1.0055e-01, 2.1197e-06, 9.5880e-06, 1.1889e-05, 5.9290e-06,\n",
      "          1.8782e-04, 2.8921e-06, 1.0261e-04, 7.6229e-05, 6.5770e-05,\n",
      "          3.3701e-01, 6.0411e-06, 1.6694e-04, 1.5842e-04, 2.6522e-04,\n",
      "          2.0874e-03, 1.1262e-05, 6.8922e-04, 1.2934e-03, 1.6724e-06,\n",
      "          1.9326e-06, 1.8004e-06, 2.8028e-06, 1.3922e-06, 9.2691e-07,\n",
      "          1.6307e-06],\n",
      "         [1.6169e-06, 1.5487e-06, 2.7236e-06, 4.1924e-06, 3.4590e-06,\n",
      "          2.8624e-06, 3.9478e-06, 1.5397e-03, 4.2979e-04, 2.7117e-05,\n",
      "          4.2793e-01, 3.5993e-03, 3.3571e-04, 2.3367e-04, 3.4999e-06,\n",
      "          1.9554e-02, 1.3333e-04, 2.9693e-05, 5.3955e-05, 3.3746e-06,\n",
      "          3.5407e-05, 1.0397e-05, 8.8435e-07, 2.0419e-06, 7.2483e-07,\n",
      "          1.7735e-01, 2.1480e-06, 1.8524e-05, 2.7170e-05, 1.4326e-05,\n",
      "          7.1262e-03, 2.1376e-06, 1.7516e-04, 1.5453e-04, 4.0661e-05,\n",
      "          5.9128e-02, 2.6078e-06, 1.5664e-04, 1.7113e-04, 4.7682e-03,\n",
      "          2.9593e-01, 1.7894e-05, 2.3074e-04, 7.2250e-04, 1.9402e-06,\n",
      "          3.9579e-06, 1.7218e-06, 1.8442e-06, 2.2631e-06, 1.6566e-06,\n",
      "          6.0603e-06],\n",
      "         [2.7609e-06, 3.1090e-06, 4.6399e-06, 5.9244e-06, 4.6623e-06,\n",
      "          3.8191e-06, 6.9130e-06, 1.0122e-04, 3.9080e-05, 1.9859e-05,\n",
      "          1.1187e-03, 1.4470e-04, 7.8806e-04, 8.1084e-04, 6.8382e-06,\n",
      "          3.3411e-02, 4.8790e-05, 2.4833e-04, 3.0769e-04, 3.4833e-06,\n",
      "          9.5768e-01, 9.2570e-06, 4.5165e-04, 1.3800e-03, 1.9430e-04,\n",
      "          1.0526e-03, 2.9491e-06, 7.3679e-06, 1.0391e-05, 5.6526e-06,\n",
      "          8.5111e-05, 4.3369e-06, 2.5279e-05, 2.5768e-05, 2.3916e-05,\n",
      "          1.0511e-03, 1.0375e-05, 1.7741e-05, 1.4629e-05, 2.0017e-04,\n",
      "          5.2319e-04, 6.0976e-06, 5.2997e-05, 6.2009e-05, 1.9987e-06,\n",
      "          2.1820e-06, 3.5099e-06, 3.9269e-06, 3.1176e-06, 1.2216e-06,\n",
      "          4.6681e-06],\n",
      "         [6.9308e-05, 3.1288e-05, 1.5505e-04, 1.2693e-04, 1.3602e-04,\n",
      "          8.9451e-05, 1.7242e-04, 3.7239e-03, 2.7575e-03, 4.4091e-04,\n",
      "          8.3115e-02, 4.6402e-03, 1.7944e-03, 3.2125e-03, 1.4969e-04,\n",
      "          9.7425e-02, 3.9093e-03, 2.2687e-03, 2.2109e-03, 9.0628e-05,\n",
      "          5.8465e-02, 1.6867e-04, 7.1750e-04, 1.3199e-03, 9.3786e-05,\n",
      "          5.1430e-01, 7.2893e-05, 2.1322e-03, 4.4732e-03, 1.2663e-04,\n",
      "          8.0767e-02, 1.3937e-04, 3.2777e-03, 3.0215e-03, 1.3680e-03,\n",
      "          5.6493e-02, 1.5884e-04, 1.9723e-03, 1.5283e-03, 8.6817e-03,\n",
      "          4.9722e-02, 1.7873e-04, 1.6934e-03, 2.2085e-03, 4.3220e-05,\n",
      "          4.7909e-05, 6.3841e-05, 8.6778e-05, 5.2258e-05, 2.8055e-05,\n",
      "          7.4532e-05]]])\n",
      "Player 0 Prediction: tensor([[0.9970, 0.0000, 0.0030, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 9998/50000 [15:07<30:36, 21.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 34834\n",
      "Average episode length: 3.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5911/10000 (59.1%)\n",
      "    Average reward: -0.115\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4089/10000 (40.9%)\n",
      "    Average reward: +0.115\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 7194 (40.4%)\n",
      "    Action 1: 6807 (38.2%)\n",
      "    Action 2: 2057 (11.6%)\n",
      "    Action 3: 1743 (9.8%)\n",
      "  Player 1:\n",
      "    Action 0: 4976 (29.2%)\n",
      "    Action 1: 7391 (43.4%)\n",
      "    Action 2: 2821 (16.6%)\n",
      "    Action 3: 1845 (10.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1147.0, 1147.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.041 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1147\n",
      "   Testing specific player: 0\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.2333, 0.7475, 0.0192, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.2781, 0.7101, 0.0118, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9426, 0.0025, 0.0549]])\n",
      "Player 0 Prediction: tensor([[0.2336, 0.7638, 0.0026, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42210\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5062/10000 (50.6%)\n",
      "    Average reward: +0.153\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4938/10000 (49.4%)\n",
      "    Average reward: -0.153\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 6172 (29.1%)\n",
      "    Action 1: 9860 (46.4%)\n",
      "    Action 2: 2152 (10.1%)\n",
      "    Action 3: 3061 (14.4%)\n",
      "  Player 1:\n",
      "    Action 0: 14836 (70.8%)\n",
      "    Action 1: 6129 (29.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1530.5, -1530.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.872 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.952\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1530\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[5.2955e-05, 2.0395e-04, 1.3786e-04, 1.0416e-04, 1.4499e-04,\n",
      "          6.9978e-05, 1.0303e-04, 2.2881e-03, 6.2383e-03, 5.6117e-03,\n",
      "          1.9022e-02, 9.5786e-03, 3.2813e-03, 7.9294e-03, 3.0930e-03,\n",
      "          1.4063e-02, 5.4734e-03, 2.6421e-02, 2.5203e-02, 3.9150e-04,\n",
      "          3.2996e-03, 2.3389e-03, 2.3277e-02, 2.8692e-02, 4.2736e-04,\n",
      "          2.0724e-01, 1.2677e-04, 5.0651e-02, 6.0630e-02, 2.1945e-03,\n",
      "          1.6123e-02, 2.0239e-04, 1.7837e-01, 1.0568e-01, 1.4065e-02,\n",
      "          4.1534e-02, 1.0599e-03, 4.1680e-02, 1.8503e-02, 1.0963e-02,\n",
      "          2.0776e-02, 6.6442e-03, 2.4795e-02, 1.0566e-02, 1.2324e-04,\n",
      "          1.6900e-04, 9.2074e-05, 1.0079e-04, 5.6683e-05, 1.0068e-04,\n",
      "          1.1337e-04],\n",
      "         [3.2876e-05, 9.2685e-05, 7.0788e-05, 6.3121e-05, 1.0026e-04,\n",
      "          3.2542e-05, 5.7344e-05, 4.4335e-03, 2.3683e-02, 7.5049e-04,\n",
      "          2.1394e-02, 9.6871e-03, 4.1367e-03, 1.2062e-02, 4.0161e-04,\n",
      "          5.7095e-02, 3.4591e-03, 3.8490e-03, 3.8148e-03, 1.6971e-04,\n",
      "          1.1314e-02, 1.9455e-03, 2.3718e-04, 4.1626e-04, 3.3490e-04,\n",
      "          1.4859e-01, 6.4133e-05, 6.8515e-02, 7.2456e-02, 1.5295e-03,\n",
      "          4.5971e-02, 4.2555e-05, 1.3975e-02, 9.9728e-03, 1.9636e-02,\n",
      "          2.2624e-01, 1.4454e-04, 3.7836e-02, 1.4858e-02, 2.6681e-02,\n",
      "          5.0791e-02, 9.3382e-04, 8.7741e-02, 1.3930e-02, 5.8462e-05,\n",
      "          8.2269e-05, 5.6779e-05, 3.9651e-05, 5.2640e-05, 6.4229e-05,\n",
      "          1.0090e-04],\n",
      "         [1.7009e-06, 4.4027e-06, 4.0549e-06, 3.0192e-06, 3.1909e-06,\n",
      "          3.2550e-06, 3.4143e-06, 1.9468e-05, 2.3768e-05, 2.2432e-05,\n",
      "          3.9209e-05, 7.2006e-05, 1.5069e-04, 5.5826e-04, 1.5628e-05,\n",
      "          8.3361e-04, 3.1506e-05, 1.2703e-03, 7.7961e-04, 6.2127e-06,\n",
      "          6.1114e-04, 1.7657e-05, 9.6685e-03, 2.1638e-01, 7.6821e-01,\n",
      "          1.1837e-04, 3.2185e-06, 1.6063e-04, 1.3697e-04, 1.3068e-05,\n",
      "          3.6213e-05, 3.1959e-06, 1.3668e-04, 8.4336e-05, 5.7133e-05,\n",
      "          6.9777e-05, 7.7171e-06, 8.3305e-05, 8.6620e-05, 7.8146e-05,\n",
      "          6.8372e-05, 3.0329e-05, 5.6614e-05, 1.5852e-05, 3.3363e-06,\n",
      "          7.6632e-06, 3.4059e-06, 3.5746e-06, 2.2171e-06, 1.7946e-06,\n",
      "          2.8586e-06],\n",
      "         [5.9397e-05, 3.0480e-04, 2.3851e-04, 1.3563e-04, 2.1261e-04,\n",
      "          9.0343e-05, 1.4016e-04, 1.3238e-03, 2.6453e-03, 7.2522e-04,\n",
      "          1.3577e-03, 1.4807e-03, 8.2068e-03, 1.8231e-02, 5.0642e-04,\n",
      "          1.3784e-03, 1.0064e-03, 4.4410e-02, 3.6338e-02, 3.3230e-04,\n",
      "          1.1632e-03, 7.7262e-04, 1.6711e-02, 5.2155e-02, 1.5586e-03,\n",
      "          1.6859e-01, 1.2487e-04, 1.6786e-01, 1.9394e-01, 1.0935e-03,\n",
      "          5.5579e-03, 1.0383e-04, 1.4102e-01, 5.5488e-02, 2.9746e-03,\n",
      "          3.2511e-03, 2.5304e-04, 3.1460e-02, 2.1286e-02, 1.6203e-03,\n",
      "          2.6840e-03, 9.6191e-04, 6.0237e-03, 3.4883e-03, 1.2110e-04,\n",
      "          1.4164e-04, 8.1730e-05, 1.3004e-04, 7.9824e-05, 7.2035e-05,\n",
      "          1.0232e-04]]])\n",
      "Player 1 Prediction: tensor([[0.6222, 0.3598, 0.0180, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.9419e-05, 1.7259e-05, 4.3064e-05, 2.3181e-05, 1.9572e-05,\n",
      "          1.7916e-05, 2.1449e-05, 3.4584e-02, 7.6870e-02, 4.3353e-04,\n",
      "          4.6183e-04, 5.9234e-04, 1.7499e-02, 3.5358e-02, 1.5920e-04,\n",
      "          1.1641e-03, 1.3830e-04, 1.0946e-02, 8.5760e-03, 9.4019e-05,\n",
      "          1.2188e-04, 1.8000e-04, 8.8522e-05, 1.1054e-04, 1.5376e-05,\n",
      "          2.5715e-01, 2.6880e-05, 1.8156e-04, 2.6050e-04, 1.5930e-04,\n",
      "          3.6163e-04, 2.8112e-05, 3.1772e-02, 2.0876e-02, 4.1582e-04,\n",
      "          3.5612e-03, 1.2327e-04, 9.6423e-02, 5.8049e-02, 1.4140e-03,\n",
      "          5.2285e-04, 2.2257e-04, 2.5416e-01, 8.6558e-02, 4.0268e-05,\n",
      "          3.0665e-05, 2.4109e-05, 1.7445e-05, 1.2984e-05, 1.8212e-05,\n",
      "          3.4273e-05],\n",
      "         [1.7321e-05, 3.2029e-05, 3.2145e-05, 2.8153e-05, 3.7422e-05,\n",
      "          2.9885e-05, 4.3680e-05, 7.3146e-03, 9.3388e-03, 1.9116e-04,\n",
      "          7.5847e-02, 7.9318e-03, 8.4576e-03, 8.2657e-03, 1.8784e-04,\n",
      "          1.8583e-02, 2.7535e-04, 5.3343e-04, 3.9521e-04, 9.8008e-05,\n",
      "          2.7967e-04, 2.1159e-04, 7.6516e-06, 4.1242e-05, 2.6379e-05,\n",
      "          5.8145e-01, 2.2676e-05, 4.5400e-04, 1.0630e-03, 1.9746e-04,\n",
      "          1.1634e-02, 2.2203e-05, 5.2209e-03, 4.2557e-03, 1.8930e-03,\n",
      "          6.0316e-02, 4.9014e-05, 2.1649e-02, 2.0914e-02, 2.2638e-02,\n",
      "          7.8996e-02, 9.5136e-05, 2.9540e-02, 2.1169e-02, 3.7943e-05,\n",
      "          3.9936e-05, 2.7159e-05, 1.4153e-05, 2.3561e-05, 1.9576e-05,\n",
      "          5.1254e-05],\n",
      "         [1.8931e-05, 2.3165e-05, 2.4091e-05, 2.0551e-05, 3.1872e-05,\n",
      "          2.5526e-05, 2.6767e-05, 1.0138e-03, 8.4760e-04, 1.2360e-04,\n",
      "          3.6805e-04, 5.2754e-04, 9.3213e-03, 1.0270e-02, 9.4669e-05,\n",
      "          6.2247e-03, 8.1216e-05, 2.2175e-02, 1.1902e-02, 4.1544e-05,\n",
      "          9.0854e-01, 1.0912e-04, 6.0818e-04, 7.9382e-03, 8.8568e-03,\n",
      "          2.1423e-03, 3.0403e-05, 8.1621e-05, 1.2068e-04, 7.2449e-05,\n",
      "          2.4711e-04, 2.5565e-05, 6.6261e-04, 6.4155e-04, 1.9736e-04,\n",
      "          5.2555e-04, 3.8532e-05, 7.3369e-04, 1.6202e-03, 3.9546e-04,\n",
      "          3.5310e-04, 9.8651e-05, 1.6389e-03, 9.3088e-04, 2.5649e-05,\n",
      "          6.6573e-05, 2.4032e-05, 3.2019e-05, 1.7804e-05, 1.6997e-05,\n",
      "          4.4975e-05],\n",
      "         [1.4420e-04, 2.6552e-04, 4.8997e-04, 3.4152e-04, 2.3152e-04,\n",
      "          2.5387e-04, 1.8805e-04, 2.8941e-02, 3.5904e-02, 7.7857e-04,\n",
      "          8.2306e-03, 3.4396e-03, 3.3492e-02, 3.8558e-02, 8.0763e-04,\n",
      "          4.1390e-03, 1.2094e-03, 2.0195e-02, 1.0395e-02, 7.1201e-04,\n",
      "          1.3541e-02, 1.2018e-03, 1.3482e-03, 2.3353e-03, 2.6261e-04,\n",
      "          3.3784e-01, 1.8498e-04, 7.0055e-03, 1.4388e-02, 9.1642e-04,\n",
      "          5.4832e-02, 2.0991e-04, 6.7862e-02, 3.5247e-02, 4.0942e-03,\n",
      "          9.2571e-03, 5.1242e-04, 4.2975e-02, 4.2351e-02, 5.1359e-03,\n",
      "          1.3157e-02, 4.6461e-04, 9.7632e-02, 5.6644e-02, 4.0459e-04,\n",
      "          3.5412e-04, 2.3747e-04, 1.9532e-04, 2.2287e-04, 1.6572e-04,\n",
      "          3.0549e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8811, 0.0059, 0.1131]])\n",
      "Player 0 Prediction: tensor([[[4.1055e-06, 8.3709e-06, 1.6582e-05, 7.3224e-06, 7.0369e-06,\n",
      "          4.0971e-06, 4.7385e-06, 2.9827e-03, 2.8350e-03, 1.6513e-04,\n",
      "          1.6379e-04, 1.6771e-04, 7.2272e-02, 6.8907e-02, 4.4754e-05,\n",
      "          1.5714e-05, 4.3966e-05, 1.8344e-03, 9.5999e-04, 1.5850e-05,\n",
      "          6.6117e-06, 5.2487e-05, 1.2817e-05, 2.0230e-05, 2.6537e-06,\n",
      "          2.6817e-01, 5.4192e-06, 4.6750e-05, 6.4307e-05, 4.7416e-05,\n",
      "          7.4449e-05, 7.6445e-06, 2.8642e-03, 2.2279e-03, 1.5508e-04,\n",
      "          7.4611e-05, 2.2962e-05, 2.7076e-01, 2.7947e-01, 3.9196e-04,\n",
      "          5.0066e-04, 1.1399e-04, 1.2813e-02, 1.1588e-02, 8.3520e-06,\n",
      "          1.8551e-05, 8.1367e-06, 2.7823e-06, 3.9602e-06, 5.2404e-06,\n",
      "          5.1394e-06],\n",
      "         [2.6130e-06, 5.9096e-06, 8.1779e-06, 4.8506e-06, 9.0955e-06,\n",
      "          3.2802e-06, 5.5150e-06, 7.1479e-02, 6.4843e-02, 2.7874e-05,\n",
      "          1.4750e-04, 2.6093e-04, 2.3875e-03, 2.2714e-03, 2.6696e-05,\n",
      "          3.8744e-05, 2.2392e-05, 1.5951e-04, 1.3985e-04, 1.6186e-05,\n",
      "          3.7443e-06, 4.8758e-05, 1.5467e-06, 4.5253e-06, 3.8959e-06,\n",
      "          2.5282e-01, 2.3420e-06, 3.6247e-05, 1.2730e-04, 3.4796e-05,\n",
      "          1.8161e-04, 3.0021e-06, 1.8609e-03, 9.6900e-04, 2.0008e-04,\n",
      "          1.4600e-04, 6.4741e-06, 2.3227e-02, 2.2722e-02, 6.4179e-04,\n",
      "          4.0013e-04, 2.2317e-05, 3.0142e-01, 2.5322e-01, 4.3281e-06,\n",
      "          8.7682e-06, 4.0539e-06, 1.6796e-06, 5.0886e-06, 3.1200e-06,\n",
      "          7.6491e-06],\n",
      "         [2.7548e-05, 5.7839e-05, 7.0421e-05, 7.9643e-05, 1.1219e-04,\n",
      "          6.0945e-05, 5.4178e-05, 3.5286e-03, 2.7620e-03, 1.4872e-04,\n",
      "          2.0081e-04, 6.0740e-04, 3.0289e-02, 3.4294e-02, 2.1543e-04,\n",
      "          1.9800e-03, 1.1293e-04, 5.8678e-01, 2.5987e-01, 9.7200e-05,\n",
      "          1.8273e-03, 2.7871e-04, 1.8597e-03, 2.0919e-02, 6.6043e-03,\n",
      "          9.0961e-03, 8.5688e-05, 1.7710e-04, 3.0695e-04, 1.3352e-04,\n",
      "          1.4841e-04, 4.9889e-05, 1.3370e-03, 8.3376e-04, 4.6042e-04,\n",
      "          1.2008e-04, 6.8741e-05, 8.1638e-03, 1.2046e-02, 5.2119e-04,\n",
      "          3.0403e-04, 2.4202e-04, 7.5818e-03, 4.9194e-03, 5.6864e-05,\n",
      "          2.8437e-04, 4.9561e-05, 4.7517e-05, 4.0464e-05, 2.8315e-05,\n",
      "          6.1080e-05],\n",
      "         [5.6143e-05, 2.5878e-04, 2.3720e-04, 1.9991e-04, 1.3497e-04,\n",
      "          1.0902e-04, 7.1169e-05, 4.6084e-02, 6.5495e-02, 3.3155e-04,\n",
      "          6.4854e-04, 5.7329e-04, 2.9455e-02, 4.0643e-02, 3.5800e-04,\n",
      "          2.2317e-04, 2.1013e-04, 1.6474e-02, 1.1154e-02, 2.6892e-04,\n",
      "          1.7521e-04, 4.2238e-04, 6.0312e-04, 1.0264e-03, 4.7732e-05,\n",
      "          2.6020e-01, 8.8459e-05, 4.2539e-03, 7.7006e-03, 3.9664e-04,\n",
      "          4.6171e-03, 5.9393e-05, 4.7008e-02, 2.0444e-02, 1.4755e-03,\n",
      "          3.3506e-04, 1.8617e-04, 1.1320e-01, 8.5991e-02, 1.7058e-03,\n",
      "          1.0520e-03, 3.0800e-04, 1.3769e-01, 9.7054e-02, 1.9693e-04,\n",
      "          3.3078e-04, 1.1193e-04, 7.5005e-05, 6.3599e-05, 5.2844e-05,\n",
      "          1.4461e-04]]])\n",
      "Player 1 Prediction: tensor([[0.9963, 0.0000, 0.0037, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30757\n",
      "Average episode length: 3.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4843/10000 (48.4%)\n",
      "    Average reward: +0.385\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5157/10000 (51.6%)\n",
      "    Average reward: -0.385\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4045 (25.6%)\n",
      "    Action 1: 6257 (39.5%)\n",
      "    Action 2: 4318 (27.3%)\n",
      "    Action 3: 1202 (7.6%)\n",
      "  Player 1:\n",
      "    Action 0: 5538 (37.1%)\n",
      "    Action 1: 6068 (40.6%)\n",
      "    Action 2: 1926 (12.9%)\n",
      "    Action 3: 1403 (9.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [3851.0, -3851.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.032 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.046\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.3851\n",
      "   Testing specific player: 1\n",
      "   At training step: 10000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1499, 0.8354, 0.0147, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9625, 0.0029, 0.0345]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 10000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42233\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6484/10000 (64.8%)\n",
      "    Average reward: -0.122\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3516/10000 (35.2%)\n",
      "    Average reward: +0.122\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15333 (72.6%)\n",
      "    Action 1: 5792 (27.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5648 (26.8%)\n",
      "    Action 1: 10406 (49.3%)\n",
      "    Action 2: 1940 (9.2%)\n",
      "    Action 3: 3114 (14.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1216.5, 1216.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.847 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.012 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.930\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1216\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11004/50000 [16:19<33:07, 19.62it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 11000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 65742/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 63736/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11999/50000 [17:12<34:14, 18.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 12000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 71430/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 69326/2000000\n",
      "P1 SL Buffer Size:  71430\n",
      "P1 SL buffer distribution [25690. 28488.  9504.  7748.]\n",
      "P1 actions distribution [0.35965281 0.39882402 0.13305334 0.10846983]\n",
      "P2 SL Buffer Size:  69326\n",
      "P2 SL buffer distribution [23003. 29264.  8660.  8399.]\n",
      "P2 actions distribution [0.33180913 0.42212157 0.12491706 0.12115224]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1707, 0.0601, 0.7691, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 11999/50000 [17:27<34:14, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29343\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6027/10000 (60.3%)\n",
      "    Average reward: -0.291\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3973/10000 (39.7%)\n",
      "    Average reward: +0.291\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5254 (36.3%)\n",
      "    Action 1: 5247 (36.2%)\n",
      "    Action 2: 2369 (16.4%)\n",
      "    Action 3: 1615 (11.1%)\n",
      "  Player 1:\n",
      "    Action 0: 3162 (21.3%)\n",
      "    Action 1: 5437 (36.6%)\n",
      "    Action 2: 4227 (28.4%)\n",
      "    Action 3: 2032 (13.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2913.5, 2913.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.006 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2913\n",
      "   Testing specific player: 0\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1808, 0.8047, 0.0146, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9418, 0.0074, 0.0508]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41368\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4966/10000 (49.7%)\n",
      "    Average reward: +0.185\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5034/10000 (50.3%)\n",
      "    Average reward: -0.185\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5600 (26.9%)\n",
      "    Action 1: 9776 (46.9%)\n",
      "    Action 2: 2452 (11.8%)\n",
      "    Action 3: 3023 (14.5%)\n",
      "  Player 1:\n",
      "    Action 0: 14729 (71.8%)\n",
      "    Action 1: 5788 (28.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1849.5, -1849.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.022 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.858 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.940\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1850\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[6.5824e-05, 1.6927e-04, 1.1129e-04, 9.0868e-05, 1.1592e-04,\n",
      "          1.2563e-04, 4.8332e-05, 2.8817e-03, 7.2357e-03, 2.7645e-03,\n",
      "          1.0979e-02, 7.3019e-03, 6.4274e-03, 1.2635e-02, 2.3356e-03,\n",
      "          1.1949e-02, 6.0507e-03, 1.7505e-02, 2.8179e-02, 1.8973e-04,\n",
      "          3.0290e-03, 2.0326e-03, 3.1216e-02, 2.0606e-02, 4.6556e-04,\n",
      "          2.1715e-01, 1.1223e-04, 6.9493e-02, 7.1282e-02, 1.6168e-03,\n",
      "          1.3373e-02, 2.9405e-04, 1.3980e-01, 1.1055e-01, 1.0526e-02,\n",
      "          3.4402e-02, 1.2731e-03, 5.9038e-02, 1.9481e-02, 1.0537e-02,\n",
      "          1.8534e-02, 4.9518e-03, 3.0505e-02, 1.1769e-02, 7.7530e-05,\n",
      "          9.6108e-05, 1.1767e-04, 1.3713e-04, 1.2278e-04, 1.3644e-04,\n",
      "          1.0897e-04],\n",
      "         [4.3259e-05, 5.6042e-05, 4.6778e-05, 4.0706e-05, 8.4229e-05,\n",
      "          4.6690e-05, 3.7812e-05, 4.0948e-03, 2.5608e-02, 5.3751e-04,\n",
      "          1.3668e-02, 7.0565e-03, 5.6111e-03, 1.5282e-02, 1.7675e-04,\n",
      "          4.7666e-02, 3.6664e-03, 1.7037e-03, 2.1351e-03, 6.7713e-05,\n",
      "          8.8783e-03, 1.1010e-03, 1.6632e-04, 2.8223e-04, 3.5020e-04,\n",
      "          1.8064e-01, 5.7026e-05, 1.0605e-01, 9.8569e-02, 6.8465e-04,\n",
      "          3.6660e-02, 6.3801e-05, 7.0497e-03, 6.9766e-03, 1.6045e-02,\n",
      "          1.7546e-01, 1.2041e-04, 3.7287e-02, 1.1333e-02, 2.4582e-02,\n",
      "          4.8545e-02, 8.0607e-04, 9.6080e-02, 1.4172e-02, 3.8325e-05,\n",
      "          5.0667e-05, 6.7018e-05, 3.9490e-05, 6.7899e-05, 6.1911e-05,\n",
      "          8.1364e-05],\n",
      "         [2.0260e-06, 2.8163e-06, 2.2473e-06, 2.0609e-06, 1.7104e-06,\n",
      "          3.9878e-06, 2.0807e-06, 1.1230e-05, 1.6906e-05, 1.3994e-05,\n",
      "          2.6140e-05, 4.7839e-05, 1.6960e-04, 5.8092e-04, 8.1126e-06,\n",
      "          5.0979e-04, 2.8793e-05, 7.0019e-04, 6.5262e-04, 2.9745e-06,\n",
      "          6.2685e-04, 9.4869e-06, 8.6919e-03, 1.4104e-01, 8.4585e-01,\n",
      "          9.5627e-05, 2.6811e-06, 1.5698e-04, 9.9847e-05, 7.2362e-06,\n",
      "          2.5647e-05, 4.7486e-06, 1.1523e-04, 5.6194e-05, 3.2791e-05,\n",
      "          5.0825e-05, 5.9144e-06, 6.0143e-05, 5.9360e-05, 6.1063e-05,\n",
      "          6.4718e-05, 2.1194e-05, 4.6441e-05, 1.4188e-05, 2.0502e-06,\n",
      "          2.7759e-06, 3.8332e-06, 2.7191e-06, 2.6346e-06, 1.6819e-06,\n",
      "          1.6873e-06],\n",
      "         [9.4434e-05, 2.3144e-04, 1.9296e-04, 9.9627e-05, 1.7304e-04,\n",
      "          1.4545e-04, 1.1768e-04, 1.4120e-03, 2.8769e-03, 6.1635e-04,\n",
      "          8.5924e-04, 1.0657e-03, 1.3048e-02, 2.6238e-02, 2.9661e-04,\n",
      "          9.4438e-04, 1.4436e-03, 3.6239e-02, 3.3803e-02, 2.3336e-04,\n",
      "          1.2108e-03, 5.2173e-04, 1.9390e-02, 3.6436e-02, 1.5343e-03,\n",
      "          2.0303e-01, 1.5208e-04, 2.1645e-01, 1.7116e-01, 8.2802e-04,\n",
      "          4.5708e-03, 1.7548e-04, 1.0179e-01, 5.4926e-02, 2.3814e-03,\n",
      "          2.9513e-03, 2.4445e-04, 3.0037e-02, 1.6159e-02, 1.2478e-03,\n",
      "          3.0926e-03, 8.1874e-04, 6.4106e-03, 3.6841e-03, 1.0276e-04,\n",
      "          7.2327e-05, 6.7092e-05, 1.2408e-04, 1.2435e-04, 9.9084e-05,\n",
      "          7.6462e-05]]])\n",
      "Player 1 Prediction: tensor([[0.5990, 0.3413, 0.0596, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.1152e-05, 1.2285e-05, 3.4231e-05, 1.4496e-05, 1.0694e-05,\n",
      "          3.0525e-05, 8.0638e-06, 2.5921e-02, 6.8374e-02, 1.7181e-04,\n",
      "          4.0327e-04, 3.6608e-04, 2.4915e-02, 3.8838e-02, 9.1913e-05,\n",
      "          8.8880e-04, 1.3797e-04, 5.1902e-03, 7.8486e-03, 2.8530e-05,\n",
      "          1.0394e-04, 1.0911e-04, 8.6027e-05, 5.3676e-05, 1.2035e-05,\n",
      "          3.5526e-01, 1.9588e-05, 2.1080e-04, 1.3979e-04, 8.6751e-05,\n",
      "          2.2244e-04, 5.0896e-05, 2.0754e-02, 1.9040e-02, 2.3615e-04,\n",
      "          2.1970e-03, 1.0912e-04, 9.8714e-02, 4.6110e-02, 9.9682e-04,\n",
      "          8.8906e-04, 1.2800e-04, 1.9893e-01, 8.2096e-02, 1.5642e-05,\n",
      "          1.2759e-05, 2.2980e-05, 1.8777e-05, 2.6872e-05, 2.3031e-05,\n",
      "          2.6067e-05],\n",
      "         [2.4658e-05, 1.7178e-05, 2.1083e-05, 1.5436e-05, 2.3019e-05,\n",
      "          3.9877e-05, 2.7719e-05, 4.8542e-03, 7.0271e-03, 1.3400e-04,\n",
      "          5.6452e-02, 5.2232e-03, 1.1883e-02, 9.2045e-03, 6.7998e-05,\n",
      "          1.7227e-02, 3.1266e-04, 2.2260e-04, 2.2967e-04, 2.7623e-05,\n",
      "          1.9351e-04, 1.0465e-04, 5.5805e-06, 2.3461e-05, 2.5805e-05,\n",
      "          6.2488e-01, 1.9081e-05, 6.7755e-04, 7.8168e-04, 7.9929e-05,\n",
      "          8.6487e-03, 4.3291e-05, 3.4014e-03, 3.6755e-03, 1.1609e-03,\n",
      "          3.7972e-02, 3.1858e-05, 2.0632e-02, 1.8627e-02, 2.0295e-02,\n",
      "          1.0596e-01, 6.4560e-05, 2.2359e-02, 1.7126e-02, 1.9173e-05,\n",
      "          2.2122e-05, 3.0986e-05, 1.4598e-05, 3.1884e-05, 2.2269e-05,\n",
      "          3.6015e-05],\n",
      "         [1.8172e-05, 1.0747e-05, 1.1918e-05, 1.0275e-05, 1.2043e-05,\n",
      "          3.0097e-05, 1.3951e-05, 4.1401e-04, 5.0043e-04, 6.8258e-05,\n",
      "          2.4146e-04, 2.7668e-04, 9.6433e-03, 8.4614e-03, 4.4822e-05,\n",
      "          4.7255e-03, 8.3014e-05, 8.4930e-03, 9.6301e-03, 1.4273e-05,\n",
      "          9.3937e-01, 4.8993e-05, 5.7427e-04, 3.4588e-03, 6.2453e-03,\n",
      "          1.7000e-03, 2.3913e-05, 7.5750e-05, 3.9787e-05, 3.2429e-05,\n",
      "          1.4865e-04, 3.6092e-05, 5.5737e-04, 4.5793e-04, 9.6313e-05,\n",
      "          3.0053e-04, 2.0933e-05, 5.0850e-04, 1.0001e-03, 3.2185e-04,\n",
      "          4.2558e-04, 4.4818e-05, 9.6686e-04, 7.1881e-04, 9.4884e-06,\n",
      "          1.7571e-05, 2.4574e-05, 2.1498e-05, 1.7804e-05, 1.5720e-05,\n",
      "          2.0339e-05],\n",
      "         [2.2937e-04, 1.8512e-04, 3.8338e-04, 1.9894e-04, 1.4666e-04,\n",
      "          3.8562e-04, 1.2109e-04, 2.1343e-02, 3.1793e-02, 6.8436e-04,\n",
      "          5.8327e-03, 2.2838e-03, 4.7114e-02, 4.8968e-02, 3.9868e-04,\n",
      "          3.4713e-03, 1.9144e-03, 1.2351e-02, 1.0057e-02, 3.5609e-04,\n",
      "          1.5830e-02, 6.1106e-04, 1.4209e-03, 1.0696e-03, 2.5005e-04,\n",
      "          3.9294e-01, 1.9452e-04, 9.7103e-03, 8.8341e-03, 5.9697e-04,\n",
      "          4.0371e-02, 3.8709e-04, 5.2272e-02, 3.9430e-02, 2.8445e-03,\n",
      "          8.3754e-03, 3.8616e-04, 3.8828e-02, 3.1696e-02, 3.4287e-03,\n",
      "          2.2670e-02, 3.3168e-04, 7.7041e-02, 6.0765e-02, 2.1380e-04,\n",
      "          1.6450e-04, 1.6716e-04, 1.7018e-04, 3.7519e-04, 2.2089e-04,\n",
      "          1.9237e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8738, 0.0086, 0.1176]])\n",
      "Player 0 Prediction: tensor([[[4.2221e-06, 4.7666e-06, 1.2365e-05, 5.0167e-06, 4.3187e-06,\n",
      "          6.1023e-06, 1.0115e-06, 3.3824e-03, 1.9095e-03, 2.3457e-05,\n",
      "          1.6993e-04, 9.7291e-05, 8.7034e-02, 6.6543e-02, 2.4349e-05,\n",
      "          2.2441e-05, 5.6336e-05, 1.0533e-03, 1.3951e-03, 4.4077e-06,\n",
      "          1.0699e-05, 4.6351e-05, 1.0545e-05, 9.9426e-06, 1.9999e-06,\n",
      "          4.1361e-01, 3.3085e-06, 2.9740e-05, 3.7426e-05, 2.5014e-05,\n",
      "          3.7313e-05, 8.7525e-06, 1.7797e-03, 2.3506e-03, 7.4074e-05,\n",
      "          5.1678e-05, 2.9644e-05, 2.1605e-01, 1.9266e-01, 2.1868e-04,\n",
      "          5.2390e-04, 5.4540e-05, 5.3192e-03, 5.2685e-03, 2.5859e-06,\n",
      "          4.9651e-06, 8.9878e-06, 4.3008e-06, 1.1968e-05, 5.3787e-06,\n",
      "          3.7462e-06],\n",
      "         [5.3076e-06, 2.5493e-06, 4.8869e-06, 2.6658e-06, 7.0036e-06,\n",
      "          3.9162e-06, 4.1178e-06, 9.2668e-02, 7.3246e-02, 2.0543e-05,\n",
      "          1.4890e-04, 2.0127e-04, 3.2084e-03, 3.4067e-03, 5.6529e-06,\n",
      "          6.6558e-05, 3.3973e-05, 1.0634e-04, 1.3737e-04, 4.3335e-06,\n",
      "          4.2192e-06, 2.7873e-05, 7.7180e-07, 4.1363e-06, 5.0256e-06,\n",
      "          4.8736e-01, 2.4412e-06, 4.0995e-05, 1.4545e-04, 8.9817e-06,\n",
      "          1.0501e-04, 5.2874e-06, 1.4560e-03, 1.0398e-03, 1.1590e-04,\n",
      "          5.6037e-05, 5.3886e-06, 2.1567e-02, 2.2944e-02, 3.3014e-04,\n",
      "          4.3108e-04, 1.8962e-05, 1.6726e-01, 1.2374e-01, 2.6071e-06,\n",
      "          5.8863e-06, 7.5030e-06, 2.2668e-06, 5.0750e-06, 3.1136e-06,\n",
      "          5.6131e-06],\n",
      "         [5.2023e-05, 3.1748e-05, 4.0262e-05, 5.7174e-05, 5.7946e-05,\n",
      "          9.2458e-05, 4.8617e-05, 2.0648e-03, 2.1054e-03, 1.1969e-04,\n",
      "          3.1196e-04, 5.4763e-04, 3.1730e-02, 4.2911e-02, 1.3901e-04,\n",
      "          3.7334e-03, 2.8704e-04, 3.8264e-01, 4.4527e-01, 7.0487e-05,\n",
      "          5.5217e-03, 1.8168e-04, 3.1077e-03, 2.0754e-02, 9.9464e-03,\n",
      "          1.4406e-02, 1.2247e-04, 2.1145e-04, 1.4700e-04, 8.9558e-05,\n",
      "          1.3452e-04, 8.4132e-05, 2.9456e-03, 9.6047e-04, 2.7035e-04,\n",
      "          9.3381e-05, 6.2553e-05, 5.9522e-03, 1.1951e-02, 5.1828e-04,\n",
      "          5.0602e-04, 1.6533e-04, 5.1890e-03, 3.9731e-03, 3.8887e-05,\n",
      "          6.9648e-05, 1.2535e-04, 5.0299e-05, 4.6342e-05, 3.2693e-05,\n",
      "          3.5189e-05],\n",
      "         [1.2647e-04, 1.4561e-04, 1.6000e-04, 8.6299e-05, 8.5931e-05,\n",
      "          1.4709e-04, 5.4948e-05, 5.4456e-02, 5.9373e-02, 2.7002e-04,\n",
      "          4.8350e-04, 3.4060e-04, 3.9371e-02, 6.4707e-02, 1.2609e-04,\n",
      "          2.7043e-04, 5.2632e-04, 1.2682e-02, 1.3319e-02, 2.0751e-04,\n",
      "          3.2934e-04, 1.8865e-04, 6.3468e-04, 7.3383e-04, 4.4572e-05,\n",
      "          4.1113e-01, 1.2203e-04, 5.1231e-03, 5.4388e-03, 2.7269e-04,\n",
      "          1.9704e-03, 9.3532e-05, 3.1816e-02, 2.2592e-02, 8.7435e-04,\n",
      "          2.2442e-04, 1.5458e-04, 7.2684e-02, 5.8348e-02, 5.9051e-04,\n",
      "          1.7462e-03, 2.0102e-04, 6.6796e-02, 7.0348e-02, 1.5820e-04,\n",
      "          1.1340e-04, 5.7522e-05, 6.6842e-05, 8.8500e-05, 6.1284e-05,\n",
      "          5.9714e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29221\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4903/10000 (49.0%)\n",
      "    Average reward: +0.270\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5097/10000 (51.0%)\n",
      "    Average reward: -0.270\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3642 (24.3%)\n",
      "    Action 1: 5766 (38.5%)\n",
      "    Action 2: 4388 (29.3%)\n",
      "    Action 3: 1192 (8.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4924 (34.6%)\n",
      "    Action 1: 5668 (39.8%)\n",
      "    Action 2: 2348 (16.5%)\n",
      "    Action 3: 1293 (9.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2695.5, -2695.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.042\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2696\n",
      "   Testing specific player: 1\n",
      "   At training step: 12000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4684, 0.0156, 0.5160]])\n",
      "Player 1 Prediction: tensor([[0.9161, 0.0718, 0.0121, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 12000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41245\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6488/10000 (64.9%)\n",
      "    Average reward: -0.204\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3512/10000 (35.1%)\n",
      "    Average reward: +0.204\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15102 (73.3%)\n",
      "    Action 1: 5502 (26.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 5094 (24.7%)\n",
      "    Action 1: 10098 (48.9%)\n",
      "    Action 2: 2300 (11.1%)\n",
      "    Action 3: 3149 (15.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2040.0, 2040.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.837 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.003 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.920\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2040\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 13003/50000 [18:43<31:20, 19.68it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 13000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 77000/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 75059/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [19:43<30:53, 19.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 14000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 82877/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 80587/2000000\n",
      "P1 SL Buffer Size:  82877\n",
      "P1 SL buffer distribution [28319. 32831. 12887.  8840.]\n",
      "P1 actions distribution [0.34169914 0.39614127 0.15549549 0.10666409]\n",
      "P2 SL Buffer Size:  80587\n",
      "P2 SL buffer distribution [25492. 33466. 12046.  9583.]\n",
      "P2 actions distribution [0.31632894 0.4152779  0.1494782  0.11891496]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1507, 0.8400, 0.0093, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[3.6558e-05, 1.5771e-05, 1.6131e-05, 1.6074e-05, 1.5079e-05,\n",
      "          3.9979e-06, 1.9384e-05, 4.6944e-04, 3.3724e-04, 1.8264e-04,\n",
      "          1.1766e-01, 2.9382e-02, 2.8261e-04, 2.1870e-04, 8.0457e-05,\n",
      "          3.6823e-01, 3.5940e-02, 4.4984e-03, 4.0529e-03, 1.8445e-05,\n",
      "          1.6460e-01, 1.2093e-03, 4.8536e-03, 3.9500e-03, 8.5353e-06,\n",
      "          3.6876e-02, 1.1138e-05, 2.5935e-04, 2.1033e-04, 1.7618e-04,\n",
      "          4.4673e-02, 3.1236e-05, 9.0431e-04, 7.5385e-04, 1.2736e-02,\n",
      "          1.3890e-01, 5.7993e-05, 8.6704e-05, 9.3956e-05, 5.0398e-03,\n",
      "          2.2459e-02, 1.4780e-04, 1.8193e-04, 1.9221e-04, 2.3222e-05,\n",
      "          1.3122e-05, 2.4643e-05, 1.0442e-05, 1.1300e-05, 1.8743e-05,\n",
      "          1.4056e-05],\n",
      "         [2.3719e-05, 2.2049e-05, 3.4581e-05, 2.8663e-05, 4.5143e-05,\n",
      "          1.8553e-05, 1.6488e-05, 3.6494e-02, 2.4510e-01, 2.6216e-04,\n",
      "          1.4501e-03, 4.1539e-03, 1.0690e-01, 2.0038e-01, 8.0260e-05,\n",
      "          7.0643e-03, 3.0549e-03, 5.7529e-02, 9.0082e-02, 5.3802e-05,\n",
      "          5.2139e-03, 5.5534e-04, 5.0555e-04, 2.3293e-04, 1.1416e-05,\n",
      "          6.9749e-02, 3.4852e-05, 1.5690e-03, 1.4322e-03, 5.3979e-04,\n",
      "          1.3675e-02, 2.7326e-05, 1.1722e-02, 8.4377e-03, 1.3635e-03,\n",
      "          1.6283e-03, 8.7856e-05, 3.7703e-02, 1.7614e-02, 2.3324e-03,\n",
      "          8.7443e-04, 3.5850e-04, 6.2151e-02, 9.1726e-03, 1.4178e-05,\n",
      "          3.2356e-05, 3.4503e-05, 2.0544e-05, 5.8433e-05, 2.6733e-05,\n",
      "          2.5493e-05],\n",
      "         [5.7042e-07, 4.0126e-07, 7.4737e-07, 4.8470e-07, 3.4878e-07,\n",
      "          3.4105e-07, 3.3947e-07, 7.7693e-06, 8.2516e-06, 2.6185e-06,\n",
      "          1.0862e-05, 1.3538e-05, 1.3399e-04, 1.1117e-04, 8.7176e-07,\n",
      "          2.8078e-04, 1.2528e-05, 1.7285e-04, 1.8494e-04, 4.0691e-07,\n",
      "          4.5816e-04, 2.2113e-06, 4.2221e-01, 5.7617e-01, 1.2211e-04,\n",
      "          9.8503e-06, 3.8953e-07, 2.1550e-06, 1.9506e-06, 1.8886e-06,\n",
      "          1.3354e-05, 3.4381e-07, 4.0580e-06, 4.3474e-06, 7.3495e-06,\n",
      "          7.5930e-06, 8.4603e-07, 5.2493e-06, 5.3690e-06, 6.6973e-06,\n",
      "          5.5873e-06, 9.5217e-07, 4.9485e-06, 3.2374e-06, 4.2639e-07,\n",
      "          3.6488e-07, 4.3269e-07, 6.4255e-07, 8.1831e-07, 3.5922e-07,\n",
      "          6.2418e-07],\n",
      "         [3.9742e-05, 8.5640e-05, 1.4177e-04, 1.0867e-04, 7.3946e-05,\n",
      "          3.4609e-05, 7.7972e-05, 1.6069e-03, 2.8067e-03, 6.1410e-04,\n",
      "          2.4370e-03, 2.9903e-03, 2.1786e-02, 3.2827e-02, 1.7522e-04,\n",
      "          3.3785e-03, 7.4255e-03, 7.3981e-02, 1.0466e-01, 1.4231e-04,\n",
      "          1.6543e-02, 4.9902e-04, 1.6386e-01, 1.9560e-01, 1.2831e-04,\n",
      "          2.5169e-01, 5.4363e-05, 1.7414e-02, 2.6630e-02, 3.0250e-04,\n",
      "          4.6645e-03, 1.6344e-04, 1.9018e-02, 1.8751e-02, 2.1063e-03,\n",
      "          9.7771e-04, 1.7810e-04, 1.2248e-02, 7.4318e-03, 1.5205e-03,\n",
      "          1.5475e-03, 3.3686e-04, 1.5036e-03, 8.6827e-04, 6.7574e-05,\n",
      "          6.8261e-05, 6.7760e-05, 1.2084e-04, 8.6524e-05, 9.3374e-05,\n",
      "          7.0395e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 13999/50000 [19:57<30:53, 19.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27009\n",
      "Average episode length: 2.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6069/10000 (60.7%)\n",
      "    Average reward: -0.242\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3931/10000 (39.3%)\n",
      "    Average reward: +0.242\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4520 (34.2%)\n",
      "    Action 1: 4653 (35.2%)\n",
      "    Action 2: 2700 (20.4%)\n",
      "    Action 3: 1345 (10.2%)\n",
      "  Player 1:\n",
      "    Action 0: 3115 (22.6%)\n",
      "    Action 1: 5123 (37.1%)\n",
      "    Action 2: 4544 (32.9%)\n",
      "    Action 3: 1009 (7.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2416.5, 2416.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.038\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2417\n",
      "   Testing specific player: 0\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1507, 0.8400, 0.0093, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9463, 0.0071, 0.0466]])\n",
      "Player 0 Prediction: tensor([[0.3563, 0.6414, 0.0024, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40614\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4634/10000 (46.3%)\n",
      "    Average reward: +0.085\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5366/10000 (53.7%)\n",
      "    Average reward: -0.085\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5375 (26.1%)\n",
      "    Action 1: 9248 (45.0%)\n",
      "    Action 2: 2808 (13.6%)\n",
      "    Action 3: 3141 (15.3%)\n",
      "  Player 1:\n",
      "    Action 0: 14273 (71.2%)\n",
      "    Action 1: 5769 (28.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [854.5, -854.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.024 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.866 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.945\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0854\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6958, 0.2421, 0.0621, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[7.4887e-06, 1.0364e-05, 2.5291e-05, 1.6733e-05, 7.7315e-06,\n",
      "          6.0698e-06, 1.4883e-05, 8.0045e-04, 4.5732e-04, 1.4484e-04,\n",
      "          1.2232e-01, 2.9808e-02, 2.8394e-04, 4.7565e-04, 1.5190e-04,\n",
      "          4.1210e-01, 3.3562e-02, 5.2453e-03, 3.8371e-03, 1.2544e-05,\n",
      "          1.6821e-01, 2.3069e-03, 4.3949e-03, 1.5922e-03, 4.0647e-06,\n",
      "          4.6242e-02, 1.0712e-05, 1.6354e-04, 2.6290e-04, 1.6957e-04,\n",
      "          3.2397e-02, 2.4707e-05, 7.3143e-04, 9.6953e-04, 1.1487e-02,\n",
      "          7.8868e-02, 1.0944e-04, 1.4645e-04, 1.6225e-04, 1.3787e-02,\n",
      "          2.8042e-02, 2.2231e-04, 1.5922e-04, 1.3188e-04, 4.1190e-05,\n",
      "          2.4017e-05, 3.5463e-06, 8.0121e-06, 9.9280e-06, 1.2795e-05,\n",
      "          8.3310e-06],\n",
      "         [2.0936e-05, 9.1926e-06, 1.2533e-05, 3.4768e-05, 2.6375e-05,\n",
      "          4.2391e-05, 4.3089e-05, 2.8054e-02, 1.6176e-01, 3.3394e-05,\n",
      "          8.8349e-04, 1.1049e-02, 1.2074e-01, 2.4963e-01, 9.4132e-05,\n",
      "          8.4040e-03, 7.9985e-04, 3.3001e-02, 9.5004e-02, 2.0742e-05,\n",
      "          3.0421e-03, 2.4323e-04, 3.9171e-04, 2.9453e-04, 1.3639e-05,\n",
      "          7.2877e-02, 3.7369e-05, 1.8268e-03, 1.9786e-03, 4.1215e-04,\n",
      "          1.5606e-02, 1.1780e-05, 8.2912e-03, 8.4186e-03, 5.7494e-04,\n",
      "          1.5585e-03, 1.6160e-05, 5.7255e-02, 1.8778e-02, 4.0425e-03,\n",
      "          6.6279e-04, 5.7142e-05, 8.1029e-02, 1.2769e-02, 9.0379e-06,\n",
      "          1.8536e-05, 1.0396e-05, 1.4467e-05, 5.7980e-05, 2.5669e-05,\n",
      "          1.3977e-05],\n",
      "         [5.3153e-07, 6.0709e-07, 2.6045e-07, 2.4735e-07, 3.2320e-07,\n",
      "          5.5026e-07, 2.5168e-07, 3.5257e-06, 4.5008e-06, 1.0829e-06,\n",
      "          6.0214e-06, 9.3382e-06, 9.5907e-05, 3.7820e-05, 6.6223e-07,\n",
      "          3.0476e-04, 1.9744e-05, 9.7080e-05, 1.9824e-04, 2.8388e-07,\n",
      "          7.6657e-04, 2.3139e-06, 5.8690e-01, 4.1126e-01, 1.8366e-04,\n",
      "          2.4778e-05, 3.1079e-07, 5.8966e-06, 2.6851e-06, 1.5805e-06,\n",
      "          1.7820e-05, 7.7494e-08, 4.3307e-06, 1.7099e-06, 8.0705e-06,\n",
      "          9.2115e-06, 4.3021e-07, 2.2699e-06, 7.6682e-06, 4.2802e-06,\n",
      "          1.1345e-05, 5.7766e-07, 3.2590e-06, 1.3655e-06, 3.0776e-07,\n",
      "          8.1758e-07, 3.0471e-07, 3.3703e-07, 3.8659e-07, 2.6547e-07,\n",
      "          1.1218e-07],\n",
      "         [6.4459e-05, 2.3843e-04, 5.3736e-05, 2.6800e-05, 1.1608e-04,\n",
      "          5.4777e-05, 3.1484e-04, 2.2826e-03, 2.7592e-03, 1.9156e-04,\n",
      "          6.1095e-03, 1.3344e-03, 7.0205e-03, 3.8223e-02, 8.4948e-05,\n",
      "          6.1081e-03, 5.3389e-03, 9.8462e-02, 1.0143e-01, 1.1421e-04,\n",
      "          1.8464e-02, 1.0880e-03, 7.4217e-02, 7.8440e-02, 3.0891e-05,\n",
      "          4.6433e-01, 6.7897e-05, 1.0742e-02, 1.1639e-02, 1.8343e-04,\n",
      "          6.5472e-03, 1.2921e-04, 1.1098e-02, 1.3032e-02, 4.7083e-03,\n",
      "          1.3720e-03, 1.3814e-04, 2.2572e-02, 3.9340e-03, 9.7910e-04,\n",
      "          1.4722e-03, 1.4208e-04, 1.2799e-03, 2.5040e-03, 9.4033e-05,\n",
      "          5.3061e-05, 5.4150e-05, 4.5822e-05, 7.1164e-05, 1.5660e-04,\n",
      "          9.2282e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29072\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5586/10000 (55.9%)\n",
      "    Average reward: +0.253\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4414/10000 (44.1%)\n",
      "    Average reward: -0.253\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2843 (19.4%)\n",
      "    Action 1: 7261 (49.5%)\n",
      "    Action 2: 3540 (24.1%)\n",
      "    Action 3: 1022 (7.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4957 (34.4%)\n",
      "    Action 1: 5765 (40.0%)\n",
      "    Action 2: 3166 (22.0%)\n",
      "    Action 3: 518 (3.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2528.5, -2528.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.961 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.010\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2529\n",
      "   Testing specific player: 1\n",
      "   At training step: 14000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6958, 0.2421, 0.0621, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6854, 0.0232, 0.2913]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 14000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40295\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6696/10000 (67.0%)\n",
      "    Average reward: -0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3304/10000 (33.0%)\n",
      "    Average reward: +0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14681 (73.3%)\n",
      "    Action 1: 5360 (26.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4784 (23.6%)\n",
      "    Action 1: 9635 (47.6%)\n",
      "    Action 2: 2729 (13.5%)\n",
      "    Action 3: 3106 (15.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1569.0, 1569.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.838 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 1.002 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.920\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1569\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 15002/50000 [21:18<35:23, 16.48it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 15000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 88854/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 86175/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16000/50000 [22:21<32:56, 17.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 16000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 94721/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 91917/2000000\n",
      "P1 SL Buffer Size:  94721\n",
      "P1 SL buffer distribution [30928. 37312. 16424. 10057.]\n",
      "P1 actions distribution [0.32651682 0.39391476 0.17339344 0.10617498]\n",
      "P2 SL Buffer Size:  91917\n",
      "P2 SL buffer distribution [28038. 37462. 15674. 10743.]\n",
      "P2 actions distribution [0.30503607 0.40756335 0.17052341 0.11687718]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[6.7277e-05, 5.4895e-05, 6.7586e-05, 9.8493e-05, 5.5821e-05,\n",
      "          1.0426e-04, 8.3253e-05, 3.5090e-03, 7.9869e-03, 4.9445e-03,\n",
      "          2.4285e-02, 1.0643e-02, 7.7447e-03, 1.8669e-02, 7.9279e-04,\n",
      "          6.2263e-02, 1.2224e-02, 1.2727e-02, 1.9632e-02, 1.7077e-04,\n",
      "          5.9023e-03, 1.6812e-03, 1.0549e-01, 1.3802e-01, 4.8413e-04,\n",
      "          1.6878e-01, 8.6080e-05, 1.1237e-01, 1.0156e-01, 1.6335e-03,\n",
      "          1.2010e-02, 1.6583e-04, 5.5635e-02, 4.2296e-02, 5.5829e-03,\n",
      "          2.1167e-02, 2.7034e-04, 1.6748e-02, 5.0998e-03, 3.7032e-03,\n",
      "          6.3241e-03, 2.2860e-03, 3.8108e-03, 2.2638e-03, 7.8173e-05,\n",
      "          7.8310e-05, 8.9005e-05, 7.4432e-05, 7.9322e-05, 7.3384e-05,\n",
      "          3.4769e-05],\n",
      "         [4.0421e-05, 4.0878e-05, 3.8817e-05, 4.6451e-05, 4.6301e-05,\n",
      "          2.5563e-05, 3.5302e-05, 1.4906e-02, 9.0926e-02, 1.2520e-03,\n",
      "          1.4539e-02, 9.0365e-03, 1.5560e-02, 7.6715e-02, 1.2991e-04,\n",
      "          2.5097e-02, 5.3860e-03, 2.8207e-03, 4.1851e-03, 6.6724e-05,\n",
      "          1.8490e-02, 1.2599e-03, 2.5907e-04, 4.8843e-04, 2.9873e-04,\n",
      "          1.6380e-01, 6.6128e-05, 2.0399e-01, 2.2236e-01, 9.1586e-04,\n",
      "          1.0854e-02, 6.9758e-05, 9.6898e-03, 6.2080e-03, 5.7709e-03,\n",
      "          1.8865e-02, 6.0750e-05, 2.7938e-02, 7.0448e-03, 4.4927e-03,\n",
      "          8.5227e-03, 1.3096e-03, 2.1787e-02, 4.2818e-03, 4.1029e-05,\n",
      "          5.2052e-05, 3.7846e-05, 2.9097e-05, 3.6605e-05, 3.5118e-05,\n",
      "          5.8543e-05],\n",
      "         [1.2712e-06, 1.1100e-06, 7.8533e-07, 8.1518e-07, 5.4724e-07,\n",
      "          1.3961e-06, 1.0431e-06, 1.2271e-05, 2.5394e-05, 1.0439e-05,\n",
      "          1.2181e-05, 1.6633e-05, 2.5961e-04, 3.9234e-04, 1.5800e-06,\n",
      "          2.8383e-04, 1.3054e-05, 3.2721e-04, 3.3457e-04, 1.2510e-06,\n",
      "          2.4453e-04, 4.0319e-06, 9.1947e-03, 2.7261e-01, 7.1579e-01,\n",
      "          5.4687e-05, 1.5313e-06, 1.1090e-04, 6.6906e-05, 8.4557e-06,\n",
      "          6.3821e-06, 2.1100e-06, 3.4200e-05, 4.5317e-05, 1.7583e-05,\n",
      "          1.1192e-05, 2.6790e-06, 2.3397e-05, 2.0857e-05, 7.0274e-06,\n",
      "          1.3116e-05, 1.1248e-05, 1.2617e-05, 8.1810e-06, 1.6012e-06,\n",
      "          1.3490e-06, 9.7656e-07, 8.3596e-07, 1.2398e-06, 7.3150e-07,\n",
      "          1.2183e-06],\n",
      "         [4.1373e-05, 1.1731e-04, 8.7635e-05, 1.0922e-04, 5.9968e-05,\n",
      "          9.1872e-05, 7.0591e-05, 1.5485e-03, 2.6759e-03, 3.2104e-04,\n",
      "          9.7407e-04, 7.3380e-04, 1.9984e-02, 7.5118e-02, 2.0225e-04,\n",
      "          8.9140e-04, 1.2881e-03, 3.9582e-02, 7.0949e-02, 6.6384e-05,\n",
      "          6.9048e-04, 4.3350e-04, 2.1041e-02, 6.3538e-02, 1.2710e-03,\n",
      "          2.9365e-01, 8.6020e-05, 1.4126e-01, 1.1311e-01, 2.1822e-04,\n",
      "          1.2325e-03, 1.0252e-04, 6.3483e-02, 3.8732e-02, 8.1638e-04,\n",
      "          4.0890e-04, 7.8980e-05, 3.1493e-02, 9.9024e-03, 5.2204e-04,\n",
      "          5.9805e-04, 5.5396e-04, 9.4045e-04, 5.0205e-04, 8.5059e-05,\n",
      "          5.8801e-05, 8.9326e-05, 5.9606e-05, 4.4478e-05, 5.1059e-05,\n",
      "          3.4469e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9689, 0.0014, 0.0298]])\n",
      "Player 1 Prediction: tensor([[[1.5681e-05, 6.2776e-06, 1.3109e-05, 9.4761e-06, 1.1347e-05,\n",
      "          4.5593e-06, 9.7197e-06, 5.9699e-04, 4.8959e-04, 1.5715e-04,\n",
      "          1.6484e-01, 2.5316e-02, 2.4275e-04, 1.5169e-04, 6.4907e-05,\n",
      "          3.8572e-01, 2.4553e-02, 1.1758e-03, 1.2365e-03, 1.6372e-05,\n",
      "          3.3410e-02, 6.1745e-04, 1.9094e-03, 1.6112e-03, 4.9422e-06,\n",
      "          1.1405e-01, 8.8889e-06, 3.2785e-04, 2.4735e-04, 1.6362e-04,\n",
      "          4.6346e-02, 2.6659e-05, 1.1381e-03, 1.3361e-03, 1.1956e-02,\n",
      "          1.3384e-01, 3.2846e-05, 1.7744e-04, 2.1111e-04, 8.2048e-03,\n",
      "          3.8824e-02, 9.1728e-05, 3.2460e-04, 4.3620e-04, 1.4903e-05,\n",
      "          8.7639e-06, 1.3827e-05, 9.4772e-06, 9.6531e-06, 6.2333e-06,\n",
      "          5.2668e-06],\n",
      "         [9.8093e-06, 4.9814e-06, 1.7337e-05, 9.5449e-06, 2.5352e-05,\n",
      "          5.2969e-06, 5.8041e-06, 3.8656e-02, 4.0666e-01, 8.6882e-05,\n",
      "          3.5772e-04, 2.3655e-03, 4.2634e-02, 1.2470e-01, 3.1999e-05,\n",
      "          1.1871e-03, 7.6468e-04, 7.6292e-03, 1.0048e-02, 2.1230e-05,\n",
      "          4.7542e-04, 2.5282e-04, 1.5322e-04, 8.0982e-05, 2.6847e-06,\n",
      "          1.6527e-01, 1.3843e-05, 9.8741e-04, 1.3197e-03, 1.4165e-04,\n",
      "          5.9978e-03, 1.7151e-05, 1.0083e-02, 6.9772e-03, 6.0082e-04,\n",
      "          6.9226e-04, 2.4498e-05, 6.8019e-02, 3.4927e-02, 9.4938e-04,\n",
      "          4.3824e-04, 9.4186e-05, 5.9672e-02, 7.5146e-03, 6.4106e-06,\n",
      "          2.4610e-05, 7.5126e-06, 6.2821e-06, 1.1860e-05, 1.0032e-05,\n",
      "          1.5268e-05],\n",
      "         [9.3674e-07, 3.4564e-07, 6.0331e-07, 5.4522e-07, 4.4458e-07,\n",
      "          4.1243e-07, 4.0145e-07, 7.6054e-06, 1.7568e-05, 2.3946e-06,\n",
      "          1.2763e-05, 1.3677e-05, 2.4618e-04, 2.5718e-04, 9.0211e-07,\n",
      "          3.8132e-04, 1.5466e-05, 2.0592e-04, 2.2043e-04, 6.0871e-07,\n",
      "          2.1241e-04, 1.5334e-06, 4.6025e-01, 5.3789e-01, 7.3885e-05,\n",
      "          4.1918e-05, 4.8059e-07, 4.7113e-06, 2.9334e-06, 1.6809e-06,\n",
      "          1.0415e-05, 9.1581e-07, 1.0459e-05, 1.2967e-05, 1.5935e-05,\n",
      "          1.2033e-05, 1.8621e-06, 1.4686e-05, 1.2164e-05, 8.1155e-06,\n",
      "          1.1164e-05, 1.2152e-06, 1.0310e-05, 8.4539e-06, 6.2881e-07,\n",
      "          4.9675e-07, 5.7156e-07, 6.8263e-07, 9.9581e-07, 4.8138e-07,\n",
      "          5.2108e-07],\n",
      "         [2.1383e-05, 5.8306e-05, 1.3999e-04, 6.8602e-05, 5.0659e-05,\n",
      "          3.0828e-05, 6.9038e-05, 1.8820e-03, 3.2979e-03, 2.1941e-04,\n",
      "          2.3880e-03, 1.4662e-03, 1.9364e-02, 3.3554e-02, 1.2822e-04,\n",
      "          1.8674e-03, 4.6946e-03, 1.8979e-02, 3.1841e-02, 5.7507e-05,\n",
      "          4.2138e-03, 3.3165e-04, 6.7018e-02, 9.4317e-02, 5.7201e-05,\n",
      "          5.6460e-01, 3.6290e-05, 2.9709e-02, 3.1140e-02, 1.3572e-04,\n",
      "          1.1425e-02, 9.4454e-05, 2.2082e-02, 1.6212e-02, 2.5617e-03,\n",
      "          7.3155e-04, 1.3722e-04, 1.9580e-02, 9.5578e-03, 1.1329e-03,\n",
      "          1.5594e-03, 2.6017e-04, 1.4938e-03, 9.8553e-04, 5.4501e-05,\n",
      "          9.3407e-05, 5.9151e-05, 8.2527e-05, 3.5714e-05, 5.6010e-05,\n",
      "          6.8901e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16000/50000 [22:37<32:56, 17.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28053\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6000/10000 (60.0%)\n",
      "    Average reward: -0.224\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4000/10000 (40.0%)\n",
      "    Average reward: +0.224\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4411 (31.8%)\n",
      "    Action 1: 5049 (36.4%)\n",
      "    Action 2: 2829 (20.4%)\n",
      "    Action 3: 1591 (11.5%)\n",
      "  Player 1:\n",
      "    Action 0: 3034 (21.4%)\n",
      "    Action 1: 5078 (35.8%)\n",
      "    Action 2: 4274 (30.2%)\n",
      "    Action 3: 1787 (12.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2238.0, 2238.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.007 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.031\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2238\n",
      "   Testing specific player: 0\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.7786, 0.1755, 0.0459, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.4523, 0.1684, 0.3793, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4080, 0.0076, 0.5844]])\n",
      "Player 0 Prediction: tensor([[0.5841, 0.4081, 0.0078, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40040\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4576/10000 (45.8%)\n",
      "    Average reward: +0.100\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5424/10000 (54.2%)\n",
      "    Average reward: -0.100\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4841 (23.9%)\n",
      "    Action 1: 9255 (45.6%)\n",
      "    Action 2: 3067 (15.1%)\n",
      "    Action 3: 3117 (15.4%)\n",
      "  Player 1:\n",
      "    Action 0: 14270 (72.2%)\n",
      "    Action 1: 5490 (27.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1004.5, -1004.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.010 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.852 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.931\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1004\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0928, 0.9017, 0.0054, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[6.7698e-06, 1.6441e-05, 8.1687e-06, 2.4499e-05, 1.1954e-05,\n",
      "          1.2382e-05, 2.1376e-05, 3.6842e-04, 3.4724e-04, 1.1846e-04,\n",
      "          1.1958e-01, 2.3589e-02, 4.1375e-04, 4.1774e-04, 1.7746e-04,\n",
      "          3.5608e-01, 3.3337e-02, 4.6552e-03, 7.0177e-03, 2.2173e-05,\n",
      "          2.3065e-01, 2.3722e-03, 4.2742e-03, 4.6304e-03, 2.3633e-06,\n",
      "          4.5622e-02, 1.1089e-05, 2.1518e-04, 4.1814e-04, 1.8524e-04,\n",
      "          4.0004e-02, 2.8178e-05, 7.0549e-04, 8.9160e-04, 1.0495e-02,\n",
      "          7.8845e-02, 9.3399e-05, 1.9835e-04, 1.7527e-04, 8.0940e-03,\n",
      "          2.5377e-02, 1.1165e-04, 1.1825e-04, 1.6526e-04, 1.2087e-05,\n",
      "          1.8646e-05, 1.4844e-05, 9.3344e-06, 1.2706e-05, 1.5018e-05,\n",
      "          1.0875e-05],\n",
      "         [1.6090e-05, 1.2358e-05, 1.0875e-05, 2.3587e-05, 2.0971e-05,\n",
      "          1.4830e-05, 2.3446e-05, 4.0813e-02, 2.5147e-01, 9.1785e-05,\n",
      "          6.5929e-04, 2.9376e-03, 1.0622e-01, 2.6086e-01, 4.8361e-05,\n",
      "          3.8939e-03, 1.8782e-03, 3.4107e-02, 7.7538e-02, 1.8259e-05,\n",
      "          2.5530e-03, 3.8465e-04, 2.5634e-04, 1.4073e-04, 4.8533e-06,\n",
      "          7.3500e-02, 1.5921e-05, 1.6128e-03, 1.6541e-03, 1.7999e-04,\n",
      "          9.3653e-03, 1.4849e-05, 4.6844e-03, 5.0863e-03, 1.2052e-03,\n",
      "          1.2921e-03, 3.2839e-05, 4.7711e-02, 2.2808e-02, 1.5638e-03,\n",
      "          4.7634e-04, 8.4558e-05, 3.8741e-02, 5.8628e-03, 1.1818e-05,\n",
      "          1.8803e-05, 1.1620e-05, 7.7719e-06, 1.2411e-05, 2.7563e-05,\n",
      "          1.3730e-05],\n",
      "         [1.8741e-07, 2.6620e-07, 2.1727e-07, 1.6027e-07, 2.4932e-07,\n",
      "          2.0947e-07, 3.3400e-07, 2.7880e-06, 2.7542e-06, 6.4336e-07,\n",
      "          4.1080e-06, 5.8408e-06, 3.0461e-05, 4.9434e-05, 7.6583e-07,\n",
      "          2.3453e-04, 9.4491e-06, 8.7255e-05, 1.2627e-04, 1.7370e-07,\n",
      "          2.3451e-04, 2.4342e-06, 5.3524e-01, 4.6377e-01, 1.4604e-04,\n",
      "          1.4659e-05, 2.0713e-07, 2.8752e-06, 3.2230e-06, 4.5025e-07,\n",
      "          4.8302e-06, 8.3083e-08, 2.7801e-06, 9.8740e-07, 3.8881e-06,\n",
      "          4.5252e-06, 2.8334e-07, 1.9098e-06, 1.4460e-06, 3.7343e-06,\n",
      "          3.6620e-06, 9.8210e-07, 1.7099e-06, 1.2981e-06, 1.5711e-07,\n",
      "          1.9458e-07, 1.7256e-07, 1.5239e-07, 1.4377e-07, 1.6919e-07,\n",
      "          1.1219e-07],\n",
      "         [5.4420e-05, 1.5119e-04, 6.7619e-05, 1.0171e-04, 1.0760e-04,\n",
      "          1.5624e-04, 1.4265e-04, 2.6163e-03, 3.0638e-03, 2.0484e-04,\n",
      "          3.6129e-03, 2.3090e-03, 1.5314e-02, 3.1590e-02, 1.3596e-04,\n",
      "          3.7058e-03, 3.2777e-03, 1.1985e-01, 1.0633e-01, 1.0372e-04,\n",
      "          1.7541e-02, 4.5403e-04, 1.1487e-01, 9.1286e-02, 7.5745e-05,\n",
      "          3.8816e-01, 7.4645e-05, 1.7795e-02, 1.6077e-02, 2.4007e-04,\n",
      "          7.5473e-03, 6.1182e-05, 9.8407e-03, 8.5480e-03, 2.4454e-03,\n",
      "          1.0038e-03, 1.4494e-04, 1.6068e-02, 8.0831e-03, 2.1593e-03,\n",
      "          1.8107e-03, 1.5672e-04, 1.0677e-03, 1.0922e-03, 9.8453e-05,\n",
      "          6.9508e-05, 8.8907e-05, 5.1595e-05, 5.1540e-05, 1.0517e-04,\n",
      "          4.4406e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27161\n",
      "Average episode length: 2.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5070/10000 (50.7%)\n",
      "    Average reward: +0.169\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4930/10000 (49.3%)\n",
      "    Average reward: -0.169\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3054 (22.1%)\n",
      "    Action 1: 5122 (37.0%)\n",
      "    Action 2: 4388 (31.7%)\n",
      "    Action 3: 1279 (9.2%)\n",
      "  Player 1:\n",
      "    Action 0: 4261 (32.0%)\n",
      "    Action 1: 4919 (36.9%)\n",
      "    Action 2: 2908 (21.8%)\n",
      "    Action 3: 1230 (9.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1689.5, -1689.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.012 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.034\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1689\n",
      "   Testing specific player: 1\n",
      "   At training step: 16000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4229, 0.0248, 0.5523]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6814, 0.0236, 0.2950]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 16000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39522\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6721/10000 (67.2%)\n",
      "    Average reward: -0.200\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3279/10000 (32.8%)\n",
      "    Average reward: +0.200\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14303 (73.3%)\n",
      "    Action 1: 5221 (26.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4491 (22.5%)\n",
      "    Action 1: 9347 (46.7%)\n",
      "    Action 2: 3086 (15.4%)\n",
      "    Action 3: 3074 (15.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2001.0, 2001.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.838 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.997 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.917\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2001\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17004/50000 [24:03<33:04, 16.62it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 17000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 100795/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 97732/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17999/50000 [25:04<31:58, 16.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 18000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 106620/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 103567/2000000\n",
      "P1 SL Buffer Size:  106620\n",
      "P1 SL buffer distribution [33514. 42509. 19820. 10777.]\n",
      "P1 actions distribution [0.31433127 0.3986963  0.18589383 0.1010786 ]\n",
      "P2 SL Buffer Size:  103567\n",
      "P2 SL buffer distribution [30606. 42032. 19266. 11663.]\n",
      "P2 actions distribution [0.29551884 0.40584356 0.18602451 0.11261309]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.1101, 0.0415, 0.8484, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 17999/50000 [25:17<31:58, 16.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27474\n",
      "Average episode length: 2.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5349/10000 (53.5%)\n",
      "    Average reward: -0.256\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4651/10000 (46.5%)\n",
      "    Average reward: +0.256\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4218 (30.8%)\n",
      "    Action 1: 5280 (38.6%)\n",
      "    Action 2: 3578 (26.1%)\n",
      "    Action 3: 617 (4.5%)\n",
      "  Player 1:\n",
      "    Action 0: 2461 (17.9%)\n",
      "    Action 1: 6505 (47.2%)\n",
      "    Action 2: 3675 (26.7%)\n",
      "    Action 3: 1140 (8.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2556.5, 2556.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.955 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.004\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2556\n",
      "   Testing specific player: 0\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.1101, 0.0415, 0.8484, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39935\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4507/10000 (45.1%)\n",
      "    Average reward: +0.092\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5493/10000 (54.9%)\n",
      "    Average reward: -0.092\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4429 (21.9%)\n",
      "    Action 1: 9400 (46.5%)\n",
      "    Action 2: 3218 (15.9%)\n",
      "    Action 3: 3165 (15.7%)\n",
      "  Player 1:\n",
      "    Action 0: 14440 (73.2%)\n",
      "    Action 1: 5283 (26.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [925.0, -925.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.994 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.838 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.916\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0925\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[4.6823e-05, 8.3000e-05, 5.0950e-05, 5.5284e-05, 1.2713e-04,\n",
      "          6.2062e-05, 6.5593e-05, 2.9598e-03, 6.0407e-03, 2.9973e-03,\n",
      "          3.2207e-02, 1.1535e-02, 8.6753e-03, 1.8276e-02, 2.0860e-03,\n",
      "          5.9613e-02, 1.2943e-02, 1.5011e-02, 2.1002e-02, 1.9844e-04,\n",
      "          7.8355e-03, 1.5061e-03, 1.1668e-01, 1.2263e-01, 4.1516e-04,\n",
      "          1.8171e-01, 1.5285e-04, 1.1400e-01, 8.5962e-02, 1.1831e-03,\n",
      "          7.3359e-03, 1.6418e-04, 5.4401e-02, 2.9545e-02, 6.7317e-03,\n",
      "          2.4363e-02, 1.0480e-03, 2.3076e-02, 8.4908e-03, 3.8277e-03,\n",
      "          6.6841e-03, 2.1358e-03, 3.5011e-03, 2.0061e-03, 7.9104e-05,\n",
      "          1.0484e-04, 8.9459e-05, 8.4628e-05, 9.2778e-05, 6.7557e-05,\n",
      "          6.3860e-05],\n",
      "         [3.0725e-05, 3.2366e-05, 1.8943e-05, 3.2082e-05, 3.6078e-05,\n",
      "          2.2326e-05, 2.1817e-05, 1.1173e-02, 5.9089e-02, 2.8287e-04,\n",
      "          1.4159e-02, 6.3960e-03, 1.9676e-02, 6.9435e-02, 1.3849e-04,\n",
      "          1.6841e-02, 4.0499e-03, 2.3211e-03, 2.4082e-03, 4.2460e-05,\n",
      "          2.9882e-02, 8.3458e-04, 1.4053e-04, 3.2012e-04, 2.9776e-04,\n",
      "          1.6088e-01, 4.4686e-05, 2.3782e-01, 2.5079e-01, 7.2459e-04,\n",
      "          1.1072e-02, 3.6691e-05, 3.8485e-03, 2.1793e-03, 4.0585e-03,\n",
      "          1.5187e-02, 1.0533e-04, 2.4199e-02, 5.8223e-03, 6.4120e-03,\n",
      "          8.9894e-03, 3.0235e-04, 2.3874e-02, 5.7474e-03, 2.3850e-05,\n",
      "          2.5238e-05, 3.1720e-05, 2.7137e-05, 3.3935e-05, 3.8778e-05,\n",
      "          4.7634e-05],\n",
      "         [5.5639e-07, 7.0825e-07, 9.9325e-07, 5.4920e-07, 5.4636e-07,\n",
      "          8.0646e-07, 6.3073e-07, 5.9165e-06, 7.8539e-06, 4.7409e-06,\n",
      "          7.8991e-06, 1.2432e-05, 5.9587e-05, 2.7256e-04, 2.6200e-06,\n",
      "          1.9549e-04, 1.0397e-05, 2.3780e-04, 1.8195e-04, 8.5482e-07,\n",
      "          2.2250e-04, 3.0890e-06, 4.9232e-03, 2.1110e-01, 7.8249e-01,\n",
      "          2.1307e-05, 5.2086e-07, 8.8700e-05, 6.0200e-05, 1.7019e-06,\n",
      "          4.1721e-06, 5.6051e-07, 1.4904e-05, 9.4567e-06, 4.7058e-06,\n",
      "          5.9099e-06, 1.4509e-06, 1.2354e-05, 8.1361e-06, 7.4781e-06,\n",
      "          6.2301e-06, 4.5633e-06, 3.9814e-06, 1.8928e-06, 6.5531e-07,\n",
      "          8.9825e-07, 8.4975e-07, 9.2353e-07, 6.1406e-07, 3.0483e-07,\n",
      "          4.6914e-07],\n",
      "         [6.0110e-05, 1.0018e-04, 8.4310e-05, 5.8344e-05, 1.1479e-04,\n",
      "          4.7590e-05, 9.2962e-05, 1.8823e-03, 2.5694e-03, 3.9836e-04,\n",
      "          8.7974e-04, 1.1014e-03, 2.0863e-02, 5.5612e-02, 2.8048e-04,\n",
      "          8.2708e-04, 1.4499e-03, 4.3885e-02, 4.7523e-02, 1.2524e-04,\n",
      "          1.8392e-03, 3.8286e-04, 2.0252e-02, 9.4125e-02, 1.8915e-03,\n",
      "          3.0021e-01, 1.1568e-04, 1.7116e-01, 1.2963e-01, 7.2377e-04,\n",
      "          1.8164e-03, 7.8437e-05, 4.8894e-02, 2.0554e-02, 7.7792e-04,\n",
      "          7.2485e-04, 1.7345e-04, 1.5476e-02, 9.0500e-03, 3.9029e-04,\n",
      "          8.0840e-04, 4.5961e-04, 1.2061e-03, 8.7141e-04, 6.0764e-05,\n",
      "          3.9425e-05, 5.8422e-05, 9.8576e-05, 8.1225e-05, 5.0745e-05,\n",
      "          4.1303e-05]]])\n",
      "Player 1 Prediction: tensor([[0.1410, 0.8118, 0.0472, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[3.5133e-05, 9.5991e-06, 2.2210e-05, 1.3285e-05, 1.7752e-05,\n",
      "          2.2703e-05, 1.3857e-05, 6.2932e-02, 1.1696e-01, 2.8210e-04,\n",
      "          9.8132e-04, 1.0581e-03, 1.1329e-01, 1.7004e-01, 1.0638e-04,\n",
      "          5.3380e-03, 4.1635e-04, 6.6532e-03, 8.7566e-03, 3.7647e-05,\n",
      "          3.3161e-04, 1.0542e-04, 1.6723e-04, 2.0781e-04, 1.4334e-05,\n",
      "          2.7321e-01, 4.4112e-05, 3.5006e-04, 1.8391e-04, 1.1021e-04,\n",
      "          1.9653e-04, 3.2628e-05, 8.6469e-03, 6.8569e-03, 2.0143e-04,\n",
      "          2.0943e-03, 1.6712e-04, 8.3164e-02, 4.9303e-02, 6.2684e-04,\n",
      "          3.5493e-04, 1.1152e-04, 5.5161e-02, 3.1203e-02, 2.2663e-05,\n",
      "          2.5258e-05, 3.0045e-05, 1.5980e-05, 3.0530e-05, 1.4440e-05,\n",
      "          2.6394e-05],\n",
      "         [2.8495e-05, 1.3961e-05, 1.0930e-05, 1.4515e-05, 1.5616e-05,\n",
      "          2.7352e-05, 1.6669e-05, 2.8302e-02, 3.6940e-02, 9.2974e-05,\n",
      "          9.2373e-02, 7.7818e-03, 4.9649e-02, 4.9661e-02, 6.9420e-05,\n",
      "          7.4967e-03, 4.7557e-04, 3.4123e-04, 3.0936e-04, 2.2690e-05,\n",
      "          4.8154e-04, 8.6636e-05, 4.4741e-06, 2.5813e-05, 2.6294e-05,\n",
      "          6.1229e-01, 2.5756e-05, 1.1241e-03, 1.4187e-03, 1.4297e-04,\n",
      "          3.9954e-03, 2.1815e-05, 2.6516e-03, 2.2916e-03, 3.3923e-04,\n",
      "          5.4627e-03, 4.5192e-05, 2.2177e-02, 1.8185e-02, 8.9738e-03,\n",
      "          2.1648e-02, 4.1533e-05, 1.2040e-02, 1.2714e-02, 1.5813e-05,\n",
      "          2.0466e-05, 2.4841e-05, 1.1173e-05, 2.7299e-05, 1.5318e-05,\n",
      "          2.8292e-05],\n",
      "         [1.3749e-05, 7.5770e-06, 9.6123e-06, 5.9592e-06, 1.1950e-05,\n",
      "          1.0979e-05, 6.6239e-06, 6.9488e-04, 7.5551e-04, 4.6920e-05,\n",
      "          1.7950e-04, 1.9083e-04, 9.7715e-03, 1.0247e-02, 2.6762e-05,\n",
      "          3.5530e-03, 7.0165e-05, 7.6373e-03, 7.2432e-03, 7.7802e-06,\n",
      "          9.4111e-01, 3.8038e-05, 4.7608e-04, 7.4546e-03, 7.2968e-03,\n",
      "          7.1867e-04, 1.6301e-05, 6.0915e-05, 4.1729e-05, 2.2238e-05,\n",
      "          6.6266e-05, 9.3239e-06, 1.6095e-04, 1.6685e-04, 2.7958e-05,\n",
      "          8.6611e-05, 1.6562e-05, 3.1276e-04, 4.9315e-04, 1.0688e-04,\n",
      "          9.2815e-05, 2.4180e-05, 2.8000e-04, 3.4495e-04, 6.2229e-06,\n",
      "          1.5542e-05, 1.2949e-05, 1.8407e-05, 1.1532e-05, 5.6705e-06,\n",
      "          1.2548e-05],\n",
      "         [1.7685e-04, 9.6226e-05, 1.6620e-04, 1.1256e-04, 1.4021e-04,\n",
      "          1.5785e-04, 7.0750e-05, 4.3390e-02, 5.2070e-02, 4.1713e-04,\n",
      "          6.4114e-03, 3.1905e-03, 1.0714e-01, 1.2742e-01, 3.3424e-04,\n",
      "          3.2651e-03, 2.1848e-03, 1.2223e-02, 1.1831e-02, 1.8241e-04,\n",
      "          3.0984e-02, 4.8779e-04, 9.3321e-04, 1.8508e-03, 3.0551e-04,\n",
      "          4.0583e-01, 1.9643e-04, 6.0014e-03, 5.2202e-03, 5.4072e-04,\n",
      "          2.3990e-02, 1.6093e-04, 2.2594e-02, 1.4322e-02, 9.8911e-04,\n",
      "          2.1564e-03, 4.0498e-04, 2.6444e-02, 2.8339e-02, 1.7215e-03,\n",
      "          5.6450e-03, 2.0508e-04, 2.7182e-02, 2.1354e-02, 1.5123e-04,\n",
      "          1.2000e-04, 1.8666e-04, 1.4937e-04, 3.1718e-04, 1.0480e-04,\n",
      "          1.3105e-04]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9383e-01, 5.6426e-04, 5.6031e-03]])\n",
      "Player 0 Prediction: tensor([[[3.6022e-06, 2.0975e-06, 6.7997e-06, 5.4469e-06, 3.5930e-06,\n",
      "          3.3961e-06, 2.4811e-06, 2.4184e-03, 2.1403e-03, 4.7692e-05,\n",
      "          3.4430e-04, 2.5577e-04, 2.4709e-01, 2.9092e-01, 1.9496e-05,\n",
      "          3.2885e-05, 1.0441e-04, 7.1829e-04, 7.2759e-04, 6.0819e-06,\n",
      "          2.5660e-05, 2.7961e-05, 2.3428e-05, 2.5591e-05, 1.3596e-06,\n",
      "          1.5865e-01, 3.3553e-06, 3.6440e-05, 4.3405e-05, 1.9250e-05,\n",
      "          3.0978e-05, 6.1814e-06, 4.8041e-04, 4.3981e-04, 7.2568e-05,\n",
      "          5.1380e-05, 1.8804e-05, 1.7774e-01, 1.1461e-01, 1.5126e-04,\n",
      "          1.8148e-04, 2.8814e-05, 1.0324e-03, 1.4157e-03, 2.8128e-06,\n",
      "          5.7623e-06, 4.7262e-06, 1.9041e-06, 7.4886e-06, 3.7026e-06,\n",
      "          2.6114e-06],\n",
      "         [1.6858e-06, 1.3340e-06, 1.8055e-06, 1.6507e-06, 2.2345e-06,\n",
      "          2.7531e-06, 1.5139e-06, 3.0393e-01, 2.8069e-01, 7.0630e-06,\n",
      "          6.4933e-05, 1.0906e-04, 7.2419e-03, 1.5539e-02, 4.4250e-06,\n",
      "          1.0625e-05, 3.0682e-05, 7.4245e-05, 1.5060e-04, 2.3128e-06,\n",
      "          3.1915e-06, 8.3026e-06, 6.9264e-07, 2.7321e-06, 1.7725e-06,\n",
      "          1.1598e-01, 1.8290e-06, 3.7350e-05, 7.4964e-05, 1.1604e-05,\n",
      "          4.0989e-05, 1.5175e-06, 7.5719e-04, 6.2709e-04, 1.8882e-05,\n",
      "          3.5373e-06, 3.2380e-06, 1.3614e-02, 7.7398e-03, 5.6530e-05,\n",
      "          3.9867e-05, 7.3074e-06, 1.4062e-01, 1.1246e-01, 1.3838e-06,\n",
      "          4.1885e-06, 1.9873e-06, 6.5356e-07, 3.7151e-06, 1.8140e-06,\n",
      "          1.8253e-06],\n",
      "         [4.2072e-05, 2.6368e-05, 3.4025e-05, 3.8187e-05, 6.7283e-05,\n",
      "          2.6568e-05, 2.7777e-05, 4.9464e-03, 4.3136e-03, 7.1057e-05,\n",
      "          1.6699e-04, 2.8599e-04, 4.7302e-02, 4.0761e-02, 9.8910e-05,\n",
      "          2.2373e-03, 1.7099e-04, 4.5822e-01, 3.4994e-01, 2.8503e-05,\n",
      "          4.8163e-03, 1.2602e-04, 4.0688e-03, 5.3952e-02, 6.0940e-03,\n",
      "          4.6550e-03, 5.4126e-05, 1.4626e-04, 1.1992e-04, 7.7432e-05,\n",
      "          1.0416e-04, 2.9521e-05, 7.8866e-04, 4.4299e-04, 8.1383e-05,\n",
      "          2.8459e-05, 3.5614e-05, 4.2204e-03, 5.0013e-03, 1.6655e-04,\n",
      "          1.2496e-04, 6.8232e-05, 3.1242e-03, 2.5834e-03, 3.0608e-05,\n",
      "          7.3663e-05, 4.7844e-05, 5.8119e-05, 3.2373e-05, 1.6164e-05,\n",
      "          2.3388e-05],\n",
      "         [5.7515e-05, 7.3673e-05, 1.0859e-04, 6.9917e-05, 7.8165e-05,\n",
      "          9.1531e-05, 3.9820e-05, 6.2295e-02, 1.4203e-01, 9.9107e-05,\n",
      "          6.0676e-04, 4.2690e-04, 1.2646e-01, 1.9266e-01, 9.9726e-05,\n",
      "          2.2038e-04, 4.4570e-04, 1.3076e-02, 1.3452e-02, 8.9343e-05,\n",
      "          4.3321e-04, 2.0294e-04, 6.4200e-04, 9.5708e-04, 5.9257e-05,\n",
      "          2.0292e-01, 7.3906e-05, 3.0586e-03, 2.8673e-03, 1.6162e-04,\n",
      "          1.6153e-03, 7.9219e-05, 1.7817e-02, 1.0651e-02, 5.7514e-04,\n",
      "          8.7926e-05, 1.4129e-04, 7.9716e-02, 5.5602e-02, 4.6711e-04,\n",
      "          2.9397e-04, 7.7850e-05, 4.0740e-02, 2.7740e-02, 1.5245e-04,\n",
      "          7.3209e-05, 6.1600e-05, 5.7682e-05, 9.2754e-05, 4.8822e-05,\n",
      "          5.4442e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27398\n",
      "Average episode length: 2.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5930/10000 (59.3%)\n",
      "    Average reward: +0.197\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4070/10000 (40.7%)\n",
      "    Average reward: -0.197\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 2406 (17.9%)\n",
      "    Action 1: 6919 (51.4%)\n",
      "    Action 2: 3339 (24.8%)\n",
      "    Action 3: 804 (6.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4377 (31.4%)\n",
      "    Action 1: 5190 (37.3%)\n",
      "    Action 2: 3904 (28.0%)\n",
      "    Action 3: 459 (3.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1969.0, -1969.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.938 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.997\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1969\n",
      "   Testing specific player: 1\n",
      "   At training step: 18000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0781, 0.1551, 0.7667]])\n",
      "Player 1 Prediction: tensor([[0.5760, 0.0512, 0.3729, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 18000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39502\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6723/10000 (67.2%)\n",
      "    Average reward: -0.202\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3277/10000 (32.8%)\n",
      "    Average reward: +0.202\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14364 (73.6%)\n",
      "    Action 1: 5160 (26.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4350 (21.8%)\n",
      "    Action 1: 9422 (47.2%)\n",
      "    Action 2: 3146 (15.7%)\n",
      "    Action 3: 3060 (15.3%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2015.0, 2015.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.833 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.990 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.912\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.2015\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19003/50000 [26:46<28:44, 17.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 19000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 112676/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 109574/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [27:44<27:03, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 20000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 119037/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 115519/2000000\n",
      "P1 SL Buffer Size:  119037\n",
      "P1 SL buffer distribution [36710. 48062. 23118. 11147.]\n",
      "P1 actions distribution [0.30839151 0.40375682 0.19420852 0.09364315]\n",
      "P2 SL Buffer Size:  115519\n",
      "P2 SL buffer distribution [33247. 46744. 23080. 12448.]\n",
      "P2 actions distribution [0.28780547 0.40464339 0.19979397 0.10775717]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[3.5970e-05, 5.6299e-05, 3.9084e-05, 5.0357e-05, 3.2483e-05,\n",
      "          4.6934e-05, 4.1063e-05, 7.6609e-04, 3.1464e-03, 2.5851e-03,\n",
      "          8.5261e-03, 2.2949e-03, 4.3717e-03, 7.1475e-03, 4.5592e-04,\n",
      "          4.8618e-03, 5.2342e-03, 1.3213e-02, 2.0700e-02, 1.9779e-04,\n",
      "          2.4964e-03, 2.2190e-03, 5.6688e-02, 5.6068e-02, 4.0925e-04,\n",
      "          2.3363e-01, 9.4874e-05, 7.0881e-02, 1.1132e-01, 9.8815e-04,\n",
      "          1.1344e-02, 5.2707e-05, 1.4229e-01, 1.0690e-01, 2.9849e-03,\n",
      "          1.3134e-02, 2.3623e-04, 4.5512e-02, 2.3549e-02, 3.8539e-03,\n",
      "          1.5275e-02, 3.7566e-03, 1.7659e-02, 4.5313e-03, 3.4144e-05,\n",
      "          5.6888e-05, 3.8570e-05, 5.1562e-05, 4.4870e-05, 7.1160e-05,\n",
      "          2.8324e-05],\n",
      "         [3.1113e-05, 2.6267e-05, 1.9547e-05, 2.4878e-05, 3.5249e-05,\n",
      "          3.3031e-05, 1.6261e-05, 1.0476e-03, 9.0289e-03, 5.1365e-04,\n",
      "          7.5857e-03, 4.9507e-03, 4.7797e-03, 1.3274e-02, 6.9080e-05,\n",
      "          2.7560e-02, 3.7541e-03, 9.2670e-04, 1.0590e-03, 2.7038e-05,\n",
      "          5.4778e-03, 4.0086e-04, 1.1997e-04, 1.4263e-04, 1.9364e-04,\n",
      "          1.3146e-01, 2.2310e-05, 2.3985e-01, 3.0313e-01, 4.0925e-04,\n",
      "          1.5802e-02, 5.9834e-05, 4.1664e-03, 2.1761e-03, 9.4317e-03,\n",
      "          6.7417e-02, 3.2006e-05, 3.3323e-02, 9.4965e-03, 1.6652e-02,\n",
      "          4.8029e-02, 2.5157e-03, 2.8982e-02, 5.7442e-03, 4.3579e-05,\n",
      "          1.5644e-05, 3.5680e-05, 1.6689e-05, 1.9603e-05, 2.4871e-05,\n",
      "          4.5503e-05],\n",
      "         [2.8891e-07, 7.2232e-07, 8.0215e-07, 1.5011e-07, 2.8673e-07,\n",
      "          1.0648e-06, 1.0049e-06, 3.7140e-06, 3.5060e-06, 3.9718e-06,\n",
      "          3.5563e-06, 9.9446e-06, 3.9191e-05, 9.3478e-05, 2.1632e-06,\n",
      "          1.0046e-04, 4.8799e-06, 1.6726e-04, 1.8227e-04, 2.7221e-07,\n",
      "          3.1214e-04, 1.4836e-06, 2.1137e-03, 9.1917e-02, 9.0465e-01,\n",
      "          1.6908e-05, 5.0395e-07, 9.4302e-05, 6.7566e-05, 6.1005e-06,\n",
      "          1.1623e-05, 9.6795e-07, 2.6360e-05, 3.5393e-05, 1.0078e-05,\n",
      "          1.1055e-05, 7.1506e-07, 2.5955e-05, 1.4150e-05, 1.2351e-05,\n",
      "          1.0137e-05, 5.7290e-06, 1.8787e-05, 8.6572e-06, 1.2650e-06,\n",
      "          9.3305e-07, 9.0075e-07, 4.9159e-07, 6.0682e-07, 7.7911e-07,\n",
      "          9.1951e-07],\n",
      "         [1.1985e-04, 1.2304e-04, 1.2937e-04, 1.0693e-04, 9.3950e-05,\n",
      "          7.7402e-05, 2.8178e-05, 2.5793e-04, 9.7876e-04, 1.0406e-03,\n",
      "          5.0460e-04, 3.8030e-04, 1.9323e-02, 4.6998e-02, 5.4942e-04,\n",
      "          7.8448e-04, 1.0790e-03, 3.1686e-02, 4.2215e-02, 1.5052e-04,\n",
      "          4.0783e-04, 2.0546e-04, 8.0910e-03, 3.0845e-02, 1.3165e-03,\n",
      "          1.1176e-01, 1.2525e-04, 1.5190e-01, 8.4895e-02, 6.0299e-04,\n",
      "          3.0932e-03, 2.0568e-04, 2.0770e-01, 9.7129e-02, 1.4121e-03,\n",
      "          7.9717e-04, 9.5918e-05, 8.5398e-02, 6.0773e-02, 9.7571e-04,\n",
      "          2.2193e-03, 7.3773e-04, 1.6459e-03, 6.1540e-04, 6.3367e-05,\n",
      "          1.0164e-04, 6.2051e-05, 7.0738e-05, 6.9926e-05, 4.0553e-05,\n",
      "          1.8774e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3417, 0.1534, 0.5049, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/50000 [27:57<27:03, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 26910\n",
      "Average episode length: 2.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5994/10000 (59.9%)\n",
      "    Average reward: -0.196\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4006/10000 (40.1%)\n",
      "    Average reward: +0.196\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3641 (27.9%)\n",
      "    Action 1: 5110 (39.1%)\n",
      "    Action 2: 2967 (22.7%)\n",
      "    Action 3: 1336 (10.2%)\n",
      "  Player 1:\n",
      "    Action 0: 3311 (23.9%)\n",
      "    Action 1: 4539 (32.8%)\n",
      "    Action 2: 4494 (32.4%)\n",
      "    Action 3: 1512 (10.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1956.5, 1956.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.032\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1956\n",
      "   Testing specific player: 0\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.7880e-01, 4.6043e-04, 2.0740e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9381, 0.0036, 0.0583]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40034\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4515/10000 (45.1%)\n",
      "    Average reward: +0.143\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5485/10000 (54.9%)\n",
      "    Average reward: -0.143\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4260 (21.0%)\n",
      "    Action 1: 9523 (47.0%)\n",
      "    Action 2: 3344 (16.5%)\n",
      "    Action 3: 3154 (15.6%)\n",
      "  Player 1:\n",
      "    Action 0: 14533 (73.6%)\n",
      "    Action 1: 5220 (26.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1434.0, -1434.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.985 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.833 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.909\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1434\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.1126, 0.0385, 0.8489, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.9961e-05, 4.1833e-05, 2.3762e-05, 2.4393e-05, 3.6431e-05,\n",
      "          2.9695e-05, 2.5981e-05, 1.5291e-04, 2.2665e-04, 7.2857e-04,\n",
      "          4.1878e-02, 5.5496e-03, 2.3632e-04, 3.8689e-04, 5.4313e-04,\n",
      "          2.2209e-02, 7.7064e-03, 1.4979e-02, 1.5904e-02, 6.6223e-05,\n",
      "          4.2545e-03, 1.3738e-03, 5.5184e-02, 2.3043e-02, 4.2576e-05,\n",
      "          2.5262e-01, 2.2792e-05, 4.5645e-02, 3.5344e-02, 5.3397e-04,\n",
      "          1.1508e-02, 7.3572e-05, 1.2908e-01, 1.1280e-01, 1.4136e-02,\n",
      "          9.2589e-02, 2.9328e-04, 5.1264e-03, 2.6729e-03, 8.4362e-03,\n",
      "          9.0029e-02, 9.7174e-04, 1.9577e-03, 1.3072e-03, 3.8457e-05,\n",
      "          3.1119e-05, 1.4543e-05, 2.8675e-05, 2.2091e-05, 2.3824e-05,\n",
      "          2.0480e-05],\n",
      "         [1.1207e-05, 1.7622e-05, 1.8724e-05, 1.3936e-05, 2.0819e-05,\n",
      "          1.7197e-05, 1.3606e-05, 7.4709e-04, 3.6694e-03, 1.3758e-04,\n",
      "          3.6702e-03, 1.7154e-03, 4.8682e-03, 6.5082e-03, 3.8909e-05,\n",
      "          2.7618e-02, 2.2865e-03, 6.7188e-03, 7.3230e-03, 2.9274e-05,\n",
      "          7.1590e-03, 5.5743e-04, 2.6131e-04, 1.8168e-04, 3.5788e-05,\n",
      "          7.9334e-02, 1.4078e-05, 2.6777e-01, 2.6887e-01, 3.9005e-04,\n",
      "          8.9138e-03, 1.9526e-05, 2.5532e-02, 2.3195e-02, 6.9110e-03,\n",
      "          1.1328e-01, 4.8817e-05, 3.4486e-02, 1.8248e-02, 1.3083e-02,\n",
      "          3.1050e-02, 2.5670e-04, 3.0590e-02, 4.2695e-03, 1.0882e-05,\n",
      "          1.3056e-05, 1.2010e-05, 1.1645e-05, 1.6253e-05, 2.3439e-05,\n",
      "          1.5539e-05],\n",
      "         [3.0324e-07, 5.2888e-07, 4.5013e-07, 2.7373e-07, 2.5200e-07,\n",
      "          5.9202e-07, 2.9896e-07, 9.4135e-07, 7.5133e-07, 2.0848e-06,\n",
      "          8.5306e-06, 8.2897e-06, 2.4844e-05, 4.8688e-05, 1.3499e-06,\n",
      "          3.3450e-04, 8.4725e-06, 1.2627e-04, 1.2741e-04, 3.6160e-07,\n",
      "          9.5622e-05, 2.0889e-06, 3.0659e-01, 6.5205e-01, 4.0216e-02,\n",
      "          3.1914e-05, 5.1295e-07, 7.5004e-05, 5.3157e-05, 1.2146e-06,\n",
      "          3.3405e-06, 5.4362e-07, 5.0013e-05, 3.0925e-05, 1.1038e-05,\n",
      "          1.4827e-05, 9.6742e-07, 1.1894e-05, 1.3074e-05, 1.1157e-05,\n",
      "          2.7662e-05, 2.8497e-06, 4.5754e-06, 2.0685e-06, 4.2834e-07,\n",
      "          3.9905e-07, 3.5290e-07, 4.7273e-07, 3.9554e-07, 2.4333e-07,\n",
      "          1.9472e-07],\n",
      "         [1.7395e-05, 3.5998e-05, 3.9119e-05, 2.5058e-05, 4.1359e-05,\n",
      "          4.6851e-05, 3.2341e-05, 1.3335e-04, 1.1882e-04, 9.3552e-05,\n",
      "          4.1221e-04, 3.6469e-04, 3.2408e-03, 6.1692e-03, 4.4926e-05,\n",
      "          4.1027e-04, 4.7102e-04, 2.9258e-02, 3.9527e-02, 5.1537e-05,\n",
      "          3.1904e-04, 1.3434e-04, 4.6838e-02, 4.4744e-02, 1.8817e-04,\n",
      "          1.6839e-01, 3.1488e-05, 1.7878e-01, 1.9795e-01, 1.5580e-04,\n",
      "          9.4363e-04, 3.2215e-05, 1.6342e-01, 8.1619e-02, 6.3427e-04,\n",
      "          1.0605e-03, 5.8016e-05, 2.0706e-02, 9.8514e-03, 8.1360e-04,\n",
      "          1.3568e-03, 2.0691e-04, 6.8658e-04, 3.8633e-04, 3.6008e-05,\n",
      "          1.6405e-05, 1.7942e-05, 2.5605e-05, 2.1718e-05, 1.9878e-05,\n",
      "          1.9871e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0992, 0.0307, 0.8701, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.3814e-06, 2.7575e-07, 7.7859e-07, 8.6273e-07, 8.6347e-07,\n",
      "          4.6068e-07, 4.9180e-07, 1.7375e-04, 2.1021e-04, 3.0361e-06,\n",
      "          2.6063e-03, 4.6434e-04, 1.0998e-04, 1.0716e-04, 4.5732e-06,\n",
      "          3.2595e-01, 1.2630e-04, 4.0959e-06, 7.8260e-06, 1.0914e-06,\n",
      "          5.4510e-04, 1.3736e-05, 3.6260e-06, 4.6454e-06, 1.5454e-07,\n",
      "          8.2353e-02, 5.1934e-07, 4.6284e-06, 3.9679e-06, 6.5580e-06,\n",
      "          9.2615e-04, 1.2221e-06, 3.1622e-05, 2.5783e-05, 1.1128e-04,\n",
      "          5.8295e-01, 4.7353e-06, 1.7036e-04, 1.6103e-04, 4.9686e-04,\n",
      "          1.5045e-03, 4.3994e-06, 3.9168e-04, 5.1343e-04, 9.4512e-07,\n",
      "          4.8368e-07, 7.1706e-07, 4.9237e-07, 1.3139e-06, 3.3283e-07,\n",
      "          8.8886e-07],\n",
      "         [3.0730e-06, 1.8544e-06, 2.8332e-06, 1.7391e-06, 3.6675e-06,\n",
      "          2.9902e-06, 2.6913e-06, 5.2166e-04, 4.7981e-04, 8.6880e-06,\n",
      "          6.0365e-02, 5.1920e-03, 5.3792e-04, 4.8276e-04, 7.3532e-06,\n",
      "          3.0686e-01, 1.2441e-04, 3.2378e-05, 4.8768e-05, 2.1556e-06,\n",
      "          7.8626e-05, 2.9686e-05, 7.1957e-07, 1.6953e-06, 4.5967e-07,\n",
      "          1.9699e-01, 1.2028e-06, 9.7226e-05, 1.2735e-04, 1.6077e-05,\n",
      "          4.2655e-02, 3.0686e-06, 1.6096e-04, 1.2033e-04, 1.0435e-04,\n",
      "          2.3321e-01, 8.2941e-06, 7.1668e-04, 5.4163e-04, 2.9398e-02,\n",
      "          1.2026e-01, 5.6216e-06, 2.9468e-04, 4.8953e-04, 1.7822e-06,\n",
      "          1.2810e-06, 2.8276e-06, 1.0570e-06, 2.1981e-06, 1.5265e-06,\n",
      "          4.9429e-06],\n",
      "         [8.6799e-07, 3.1024e-07, 6.4659e-07, 2.4396e-07, 6.1503e-07,\n",
      "          4.0686e-07, 3.6364e-07, 9.1745e-06, 4.0009e-06, 1.4610e-06,\n",
      "          8.5960e-05, 4.6624e-05, 1.7025e-04, 8.7910e-05, 1.7380e-06,\n",
      "          1.8686e-02, 1.0389e-05, 1.7595e-05, 2.4361e-05, 2.5705e-07,\n",
      "          9.7692e-01, 2.7662e-06, 2.5924e-04, 2.5219e-03, 1.0057e-04,\n",
      "          2.4040e-04, 7.9104e-07, 1.9402e-06, 1.7155e-06, 1.8449e-06,\n",
      "          5.3965e-05, 7.0971e-07, 3.6103e-06, 2.7989e-06, 9.2347e-06,\n",
      "          5.4962e-04, 1.0682e-06, 5.8090e-06, 5.2238e-06, 2.8444e-05,\n",
      "          1.1195e-04, 1.6600e-06, 5.3358e-06, 1.1464e-05, 4.1431e-07,\n",
      "          3.3587e-07, 1.3030e-06, 6.6176e-07, 7.6895e-07, 2.2071e-07,\n",
      "          4.9784e-07],\n",
      "         [3.0569e-05, 1.7836e-05, 7.2957e-05, 2.1003e-05, 2.4775e-05,\n",
      "          4.0348e-05, 1.6563e-05, 6.7821e-04, 2.5491e-04, 4.0439e-05,\n",
      "          2.1871e-02, 3.0639e-03, 1.5442e-03, 1.5364e-03, 5.1049e-05,\n",
      "          7.0327e-02, 1.4094e-03, 2.6898e-04, 3.8317e-04, 3.7697e-05,\n",
      "          9.5397e-02, 1.0766e-04, 2.8661e-04, 4.0974e-04, 1.7210e-05,\n",
      "          3.6573e-01, 3.5457e-05, 1.2552e-03, 1.4298e-03, 8.4991e-05,\n",
      "          3.1273e-01, 3.2641e-05, 1.5228e-03, 8.1111e-04, 9.4187e-04,\n",
      "          7.6679e-02, 4.8958e-05, 1.1855e-03, 2.4048e-03, 4.2918e-03,\n",
      "          3.1303e-02, 4.5572e-05, 5.1134e-04, 8.4320e-04, 2.7723e-05,\n",
      "          1.8394e-05, 4.4848e-05, 1.9262e-05, 4.5049e-05, 1.1642e-05,\n",
      "          3.3592e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2658, 0.1796, 0.5546]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29146\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5370/10000 (53.7%)\n",
      "    Average reward: +0.239\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4630/10000 (46.3%)\n",
      "    Average reward: -0.239\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4549 (31.1%)\n",
      "    Action 1: 5754 (39.3%)\n",
      "    Action 2: 4127 (28.2%)\n",
      "    Action 3: 213 (1.5%)\n",
      "  Player 1:\n",
      "    Action 0: 4713 (32.5%)\n",
      "    Action 1: 4931 (34.0%)\n",
      "    Action 2: 3149 (21.7%)\n",
      "    Action 3: 1710 (11.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2388.5, -2388.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2389\n",
      "   Testing specific player: 1\n",
      "   At training step: 20000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.7784e-01, 6.3935e-04, 2.1522e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9714, 0.0013, 0.0272]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 20000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39056\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6900/10000 (69.0%)\n",
      "    Average reward: -0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3100/10000 (31.0%)\n",
      "    Average reward: +0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14242 (74.0%)\n",
      "    Action 1: 5015 (26.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4002 (20.2%)\n",
      "    Action 1: 9266 (46.8%)\n",
      "    Action 2: 3403 (17.2%)\n",
      "    Action 3: 3128 (15.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1505.5, 1505.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.827 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.979 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.903\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1505\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21004/50000 [29:14<25:22, 19.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 21000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 125315/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 121389/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22000/50000 [30:10<28:57, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 22000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 131641/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 127187/2000000\n",
      "P1 SL Buffer Size:  131641\n",
      "P1 SL buffer distribution [40434. 53261. 26597. 11349.]\n",
      "P1 actions distribution [0.30715355 0.40459279 0.20204192 0.08621174]\n",
      "P2 SL Buffer Size:  127187\n",
      "P2 SL buffer distribution [35625. 51786. 26790. 12986.]\n",
      "P2 actions distribution [0.28009938 0.40716425 0.21063473 0.10210163]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[5.1332e-05, 4.8410e-05, 5.1796e-05, 5.3522e-05, 4.4512e-05,\n",
      "          4.7541e-05, 5.5032e-05, 9.3538e-04, 2.9781e-03, 2.2827e-03,\n",
      "          1.2516e-02, 5.1158e-03, 3.5949e-03, 9.8910e-03, 8.3315e-04,\n",
      "          2.6659e-02, 8.3327e-03, 1.9516e-02, 4.0819e-02, 1.3168e-04,\n",
      "          1.8115e-02, 1.7014e-03, 3.0762e-01, 3.5279e-01, 3.2017e-04,\n",
      "          4.2229e-02, 4.2053e-05, 3.6537e-02, 3.3301e-02, 9.3078e-04,\n",
      "          1.0796e-02, 9.2026e-05, 1.7608e-02, 1.5224e-02, 3.4443e-03,\n",
      "          8.7782e-03, 1.7976e-04, 6.8466e-03, 1.8016e-03, 1.4117e-03,\n",
      "          2.7068e-03, 1.3553e-03, 1.2868e-03, 5.7127e-04, 4.0850e-05,\n",
      "          4.4462e-05, 6.2959e-05, 5.2908e-05, 5.6786e-05, 5.9996e-05,\n",
      "          3.7703e-05],\n",
      "         [4.6738e-05, 5.1886e-05, 4.2882e-05, 5.0635e-05, 3.8107e-05,\n",
      "          4.4011e-05, 4.5940e-05, 7.2369e-03, 4.7935e-02, 1.2044e-03,\n",
      "          2.3481e-02, 6.6495e-03, 3.1258e-02, 1.2319e-01, 1.9071e-04,\n",
      "          5.3241e-02, 9.1425e-03, 8.1417e-03, 1.9144e-02, 9.2432e-05,\n",
      "          1.2622e-01, 1.2543e-03, 5.1766e-04, 8.9400e-04, 4.8818e-04,\n",
      "          4.2830e-02, 6.3241e-05, 2.2507e-01, 1.8504e-01, 1.1156e-03,\n",
      "          1.5493e-02, 7.2633e-05, 3.3812e-03, 2.8334e-03, 4.4396e-03,\n",
      "          1.2257e-02, 6.8964e-05, 1.5602e-02, 3.2350e-03, 3.6082e-03,\n",
      "          6.7520e-03, 1.5406e-03, 1.3008e-02, 2.6701e-03, 3.3100e-05,\n",
      "          3.7955e-05, 5.4274e-05, 4.9463e-05, 5.2862e-05, 4.1364e-05,\n",
      "          5.7645e-05],\n",
      "         [3.8544e-07, 3.7090e-07, 3.0137e-07, 2.4945e-07, 2.1982e-07,\n",
      "          3.0173e-07, 4.0548e-07, 2.8297e-06, 4.8377e-06, 2.6839e-06,\n",
      "          3.4169e-06, 5.3078e-06, 6.6467e-05, 8.0320e-05, 8.8890e-07,\n",
      "          9.4447e-05, 3.5797e-06, 1.0893e-04, 1.5752e-04, 3.9536e-07,\n",
      "          1.8624e-04, 2.3645e-06, 8.0509e-03, 2.6521e-01, 7.2591e-01,\n",
      "          4.1585e-06, 4.9638e-07, 3.0369e-05, 1.7655e-05, 3.7979e-06,\n",
      "          3.1513e-06, 4.5235e-07, 5.7870e-06, 8.6802e-06, 4.2190e-06,\n",
      "          1.8606e-06, 6.5980e-07, 6.2605e-06, 5.2586e-06, 1.6920e-06,\n",
      "          2.9066e-06, 2.5479e-06, 2.9136e-06, 1.2495e-06, 3.9016e-07,\n",
      "          5.1354e-07, 3.2251e-07, 2.9809e-07, 3.8031e-07, 2.2614e-07,\n",
      "          4.6576e-07],\n",
      "         [4.8498e-05, 1.0052e-04, 8.3605e-05, 1.0102e-04, 6.8710e-05,\n",
      "          5.4637e-05, 6.4996e-05, 7.1827e-04, 1.5719e-03, 3.2204e-04,\n",
      "          7.4327e-04, 6.6066e-04, 3.2504e-02, 1.1993e-01, 2.5598e-04,\n",
      "          1.6018e-03, 1.0449e-03, 9.5413e-02, 2.2807e-01, 1.2903e-04,\n",
      "          2.4038e-03, 5.2352e-04, 4.0035e-02, 1.3874e-01, 1.6566e-03,\n",
      "          1.1401e-01, 7.5183e-05, 6.9483e-02, 4.0269e-02, 3.1805e-04,\n",
      "          6.2148e-04, 1.0721e-04, 3.6277e-02, 3.4247e-02, 7.9412e-04,\n",
      "          2.6703e-04, 8.5016e-05, 2.5432e-02, 8.8362e-03, 3.6009e-04,\n",
      "          3.2586e-04, 4.0189e-04, 5.4290e-04, 3.1649e-04, 6.3852e-05,\n",
      "          5.1065e-05, 6.8379e-05, 6.3167e-05, 6.1156e-05, 3.7792e-05,\n",
      "          3.4220e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22000/50000 [30:27<28:57, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 25856\n",
      "Average episode length: 2.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5308/10000 (53.1%)\n",
      "    Average reward: -0.117\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4692/10000 (46.9%)\n",
      "    Average reward: +0.117\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 3890 (30.2%)\n",
      "    Action 1: 4712 (36.6%)\n",
      "    Action 2: 3894 (30.2%)\n",
      "    Action 3: 385 (3.0%)\n",
      "  Player 1:\n",
      "    Action 0: 2667 (20.6%)\n",
      "    Action 1: 6203 (47.8%)\n",
      "    Action 2: 3686 (28.4%)\n",
      "    Action 3: 419 (3.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1174.5, 1174.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.978 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1174\n",
      "   Testing specific player: 0\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6768, 0.0084, 0.3148]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5690, 0.0306, 0.4004]])\n",
      "Player 0 Prediction: tensor([[0.5617, 0.4343, 0.0040, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40300\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4449/10000 (44.5%)\n",
      "    Average reward: +0.145\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5551/10000 (55.5%)\n",
      "    Average reward: -0.145\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4211 (20.6%)\n",
      "    Action 1: 9647 (47.3%)\n",
      "    Action 2: 3446 (16.9%)\n",
      "    Action 3: 3089 (15.1%)\n",
      "  Player 1:\n",
      "    Action 0: 14640 (73.5%)\n",
      "    Action 1: 5267 (26.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1454.0, -1454.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.981 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.834 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.907\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1454\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[5.0087e-05, 7.3315e-05, 5.0524e-05, 7.5230e-05, 7.5956e-05,\n",
      "          5.7596e-05, 6.6861e-05, 2.3023e-03, 5.7409e-03, 3.0286e-03,\n",
      "          2.1126e-02, 1.0862e-02, 9.9663e-03, 2.0505e-02, 2.5627e-03,\n",
      "          4.1467e-02, 1.1864e-02, 1.3432e-02, 2.3355e-02, 1.8805e-04,\n",
      "          6.5052e-03, 1.5652e-03, 1.4741e-01, 1.4519e-01, 3.0678e-04,\n",
      "          1.3493e-01, 1.1225e-04, 1.0782e-01, 1.1196e-01, 1.0651e-03,\n",
      "          1.1822e-02, 1.5545e-04, 4.7096e-02, 3.6765e-02, 5.8383e-03,\n",
      "          1.8803e-02, 9.6233e-04, 2.8542e-02, 1.0014e-02, 3.2164e-03,\n",
      "          6.3626e-03, 2.1662e-03, 2.7015e-03, 1.3286e-03, 6.6577e-05,\n",
      "          6.7485e-05, 7.8240e-05, 7.9122e-05, 8.6128e-05, 6.6874e-05,\n",
      "          6.7003e-05],\n",
      "         [2.4767e-05, 2.3447e-05, 1.5945e-05, 2.7543e-05, 2.7874e-05,\n",
      "          1.9289e-05, 2.3388e-05, 9.2977e-03, 5.4096e-02, 2.3458e-04,\n",
      "          1.1351e-02, 3.9183e-03, 2.1108e-02, 9.5951e-02, 1.3211e-04,\n",
      "          1.2480e-02, 4.0321e-03, 1.5338e-03, 2.5887e-03, 4.3479e-05,\n",
      "          2.1297e-02, 8.3074e-04, 1.5064e-04, 3.0881e-04, 1.9713e-04,\n",
      "          1.4192e-01, 4.9047e-05, 2.4651e-01, 2.7343e-01, 7.0774e-04,\n",
      "          1.1713e-02, 2.5787e-05, 3.0232e-03, 2.6418e-03, 4.1293e-03,\n",
      "          1.1382e-02, 7.1254e-05, 2.7927e-02, 5.6561e-03, 3.7357e-03,\n",
      "          7.1414e-03, 2.7387e-04, 1.6336e-02, 3.4116e-03, 2.4842e-05,\n",
      "          2.0728e-05, 2.9178e-05, 2.0055e-05, 3.5568e-05, 3.3206e-05,\n",
      "          3.2434e-05],\n",
      "         [4.9246e-07, 5.3827e-07, 7.0886e-07, 4.8057e-07, 4.9701e-07,\n",
      "          6.0221e-07, 6.4864e-07, 6.1157e-06, 6.5331e-06, 3.2878e-06,\n",
      "          6.2225e-06, 9.5019e-06, 6.5825e-05, 2.3914e-04, 2.1660e-06,\n",
      "          1.3943e-04, 1.1984e-05, 1.6499e-04, 1.6270e-04, 7.8262e-07,\n",
      "          1.5101e-04, 3.0082e-06, 4.9542e-03, 2.4251e-01, 7.5134e-01,\n",
      "          2.3108e-05, 5.4657e-07, 6.2425e-05, 4.6539e-05, 1.6936e-06,\n",
      "          4.4942e-06, 4.9542e-07, 1.1974e-05, 9.4345e-06, 4.6203e-06,\n",
      "          4.1434e-06, 1.2288e-06, 1.3793e-05, 9.4709e-06, 5.4720e-06,\n",
      "          6.3263e-06, 3.1413e-06, 3.5088e-06, 1.6236e-06, 5.2314e-07,\n",
      "          5.5967e-07, 7.6335e-07, 8.4753e-07, 5.3753e-07, 2.5466e-07,\n",
      "          3.7402e-07],\n",
      "         [4.3302e-05, 1.0349e-04, 9.1377e-05, 6.2319e-05, 1.0494e-04,\n",
      "          6.2238e-05, 8.9834e-05, 1.7015e-03, 3.5610e-03, 3.1088e-04,\n",
      "          6.2945e-04, 7.7792e-04, 2.7899e-02, 7.6202e-02, 2.5073e-04,\n",
      "          6.9693e-04, 1.4833e-03, 3.9161e-02, 5.5414e-02, 1.3550e-04,\n",
      "          1.5922e-03, 4.0314e-04, 2.2706e-02, 8.0228e-02, 1.7679e-03,\n",
      "          3.1045e-01, 9.2806e-05, 1.3025e-01, 1.3771e-01, 4.5681e-04,\n",
      "          1.9975e-03, 7.6106e-05, 4.3126e-02, 2.3533e-02, 9.0850e-04,\n",
      "          5.6094e-04, 1.7044e-04, 1.9471e-02, 1.1947e-02, 3.1422e-04,\n",
      "          6.4802e-04, 3.1098e-04, 1.1721e-03, 9.0397e-04, 6.8401e-05,\n",
      "          3.6253e-05, 5.5799e-05, 8.6143e-05, 8.1793e-05, 5.2868e-05,\n",
      "          4.3189e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0827, 0.1833, 0.7340]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28602\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5359/10000 (53.6%)\n",
      "    Average reward: +0.224\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4641/10000 (46.4%)\n",
      "    Average reward: -0.224\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4508 (31.4%)\n",
      "    Action 1: 5517 (38.4%)\n",
      "    Action 2: 4168 (29.0%)\n",
      "    Action 3: 178 (1.2%)\n",
      "  Player 1:\n",
      "    Action 0: 4384 (30.8%)\n",
      "    Action 1: 4915 (34.5%)\n",
      "    Action 2: 3298 (23.2%)\n",
      "    Action 3: 1634 (11.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2238.5, -2238.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.054\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2238\n",
      "   Testing specific player: 1\n",
      "   At training step: 22000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.0827, 0.1833, 0.7340]])\n",
      "Player 1 Prediction: tensor([[0.3613, 0.1020, 0.5367, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 22000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 38962\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6929/10000 (69.3%)\n",
      "    Average reward: -0.124\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3071/10000 (30.7%)\n",
      "    Average reward: +0.124\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14173 (73.9%)\n",
      "    Action 1: 5004 (26.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3806 (19.2%)\n",
      "    Action 1: 9228 (46.6%)\n",
      "    Action 2: 3503 (17.7%)\n",
      "    Action 3: 3248 (16.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1242.5, 1242.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.828 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.899\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1242\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23002/50000 [31:50<28:36, 15.73it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 23000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 137931/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 133171/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24000/50000 [32:54<26:24, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 24000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 144056/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 139269/2000000\n",
      "P1 SL Buffer Size:  144056\n",
      "P1 SL buffer distribution [44086. 58230. 30235. 11505.]\n",
      "P1 actions distribution [0.30603376 0.4042178  0.20988366 0.07986477]\n",
      "P2 SL Buffer Size:  139269\n",
      "P2 SL buffer distribution [38274. 57183. 30460. 13352.]\n",
      "P2 actions distribution [0.27482067 0.41059389 0.21871343 0.09587202]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.0818, 0.0281, 0.8900, 0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24000/50000 [33:08<26:24, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 25642\n",
      "Average episode length: 2.6 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5202/10000 (52.0%)\n",
      "    Average reward: -0.081\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4798/10000 (48.0%)\n",
      "    Average reward: +0.081\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4071 (31.6%)\n",
      "    Action 1: 4359 (33.9%)\n",
      "    Action 2: 4175 (32.5%)\n",
      "    Action 3: 258 (2.0%)\n",
      "  Player 1:\n",
      "    Action 0: 1335 (10.4%)\n",
      "    Action 1: 7014 (54.9%)\n",
      "    Action 2: 4069 (31.8%)\n",
      "    Action 3: 361 (2.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-805.5, 805.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.815 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.935\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.0805\n",
      "   Testing specific player: 0\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2068, 0.0819, 0.7113]])\n",
      "Player 0 Prediction: tensor([[0.4106, 0.0617, 0.5277, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40696\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4410/10000 (44.1%)\n",
      "    Average reward: +0.152\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5590/10000 (55.9%)\n",
      "    Average reward: -0.152\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4216 (20.5%)\n",
      "    Action 1: 9858 (47.9%)\n",
      "    Action 2: 3487 (17.0%)\n",
      "    Action 3: 3009 (14.6%)\n",
      "  Player 1:\n",
      "    Action 0: 14825 (73.7%)\n",
      "    Action 1: 5301 (26.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1521.0, -1521.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.977 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.832 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.905\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1521\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7065, 0.2576, 0.0359, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[4.6233e-06, 1.1456e-05, 1.2027e-05, 1.0044e-05, 7.1310e-06,\n",
      "          8.7832e-06, 7.2657e-06, 1.4522e-04, 1.2144e-04, 3.9635e-05,\n",
      "          5.0764e-02, 6.2728e-03, 1.9953e-04, 1.3119e-04, 4.3973e-05,\n",
      "          3.7453e-02, 7.0913e-03, 9.5745e-04, 9.4817e-04, 1.0769e-05,\n",
      "          9.9823e-03, 5.7860e-04, 5.6297e-04, 2.9433e-04, 1.3958e-06,\n",
      "          5.4601e-01, 5.5505e-06, 1.7317e-04, 2.7270e-04, 1.2733e-04,\n",
      "          7.0281e-02, 8.4974e-06, 2.7693e-03, 1.8773e-03, 1.1885e-02,\n",
      "          1.2778e-01, 4.1516e-05, 9.8510e-04, 9.2180e-04, 1.1408e-02,\n",
      "          1.0870e-01, 9.3300e-05, 4.3989e-04, 5.2844e-04, 6.0148e-06,\n",
      "          8.7073e-06, 3.2723e-06, 5.6390e-06, 4.5773e-06, 6.3093e-06,\n",
      "          3.6767e-06],\n",
      "         [9.0013e-06, 8.4723e-06, 1.2803e-05, 1.5425e-05, 1.2472e-05,\n",
      "          9.5027e-06, 1.1904e-05, 5.1211e-03, 2.6263e-02, 3.3033e-05,\n",
      "          3.3187e-04, 1.3468e-03, 2.4258e-02, 4.0764e-02, 1.8434e-05,\n",
      "          4.7003e-03, 5.8319e-04, 5.6250e-03, 6.6018e-03, 1.0065e-05,\n",
      "          2.8630e-04, 1.4955e-04, 4.2368e-05, 3.7492e-05, 3.3011e-06,\n",
      "          5.2083e-01, 5.1731e-06, 1.9927e-03, 2.6204e-03, 1.1908e-04,\n",
      "          2.8666e-02, 1.0595e-05, 1.2414e-02, 9.5621e-03, 1.6796e-03,\n",
      "          7.5422e-03, 1.6895e-05, 1.3363e-01, 6.7161e-02, 4.5764e-03,\n",
      "          2.9841e-03, 6.4224e-05, 7.3874e-02, 1.5951e-02, 6.0603e-06,\n",
      "          7.9721e-06, 4.9903e-06, 6.2167e-06, 7.9283e-06, 9.7252e-06,\n",
      "          5.8197e-06],\n",
      "         [7.5001e-07, 1.0234e-06, 1.1018e-06, 6.8487e-07, 9.6869e-07,\n",
      "          1.2013e-06, 6.9746e-07, 4.3152e-06, 4.3622e-06, 1.5955e-06,\n",
      "          1.4663e-05, 2.0767e-05, 1.4823e-04, 1.5649e-04, 1.7929e-06,\n",
      "          8.4800e-04, 2.3532e-05, 2.1271e-04, 2.6089e-04, 4.2549e-07,\n",
      "          7.2005e-04, 4.0412e-06, 5.1154e-01, 4.8499e-01, 3.7289e-04,\n",
      "          3.3357e-04, 1.0576e-06, 9.1604e-06, 1.0258e-05, 2.3584e-06,\n",
      "          4.5877e-05, 4.3984e-07, 2.9546e-05, 1.4577e-05, 2.8529e-05,\n",
      "          4.0697e-05, 1.0102e-06, 2.0522e-05, 2.9400e-05, 2.6698e-05,\n",
      "          3.9514e-05, 3.6136e-06, 1.3664e-05, 1.0604e-05, 5.1321e-07,\n",
      "          8.4432e-07, 4.5553e-07, 9.7298e-07, 5.5898e-07, 4.0795e-07,\n",
      "          2.9555e-07],\n",
      "         [2.5944e-05, 5.1572e-05, 4.8881e-05, 3.8919e-05, 4.9324e-05,\n",
      "          8.8663e-05, 3.0248e-05, 5.7978e-04, 5.3457e-04, 6.4323e-05,\n",
      "          1.5619e-03, 8.5976e-04, 5.7286e-03, 8.3701e-03, 4.1977e-05,\n",
      "          1.4801e-03, 1.1640e-03, 1.2128e-02, 1.7630e-02, 3.6473e-05,\n",
      "          2.6047e-03, 1.3921e-04, 1.6761e-02, 1.2834e-02, 3.0305e-05,\n",
      "          7.5661e-01, 3.8278e-05, 1.0442e-02, 1.2191e-02, 1.0385e-04,\n",
      "          2.2082e-02, 2.2470e-05, 3.0554e-02, 3.2004e-02, 2.4210e-03,\n",
      "          1.7617e-03, 3.8831e-05, 2.6331e-02, 1.3415e-02, 2.6461e-03,\n",
      "          3.2855e-03, 8.8805e-05, 1.4643e-03, 1.4522e-03, 3.3400e-05,\n",
      "          2.2503e-05, 2.2531e-05, 3.1150e-05, 2.2029e-05, 1.8464e-05,\n",
      "          1.7211e-05]]])\n",
      "Player 1 Prediction: tensor([[0.9801, 0.0000, 0.0199, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.3939e-06, 7.2587e-07, 2.2791e-06, 8.1423e-07, 1.0044e-06,\n",
      "          2.1997e-06, 5.8195e-07, 1.2444e-03, 2.7074e-03, 1.1943e-05,\n",
      "          6.9871e-06, 1.3222e-05, 5.1405e-02, 5.9692e-02, 4.6354e-06,\n",
      "          1.3658e-07, 8.0835e-06, 8.6334e-03, 6.9802e-03, 2.4669e-06,\n",
      "          3.2285e-07, 9.8921e-06, 5.1884e-06, 5.6842e-06, 6.8420e-07,\n",
      "          1.6569e-01, 1.1821e-06, 4.0180e-05, 2.8325e-05, 6.8139e-06,\n",
      "          8.2485e-06, 1.8833e-06, 1.3114e-02, 1.5033e-02, 1.6936e-05,\n",
      "          3.9268e-07, 7.0995e-06, 3.9291e-01, 2.5291e-01, 2.3204e-05,\n",
      "          7.4103e-05, 1.6239e-05, 1.9090e-02, 1.0274e-02, 1.5078e-06,\n",
      "          1.2334e-06, 1.4510e-06, 2.1319e-06, 1.3700e-06, 6.1536e-07,\n",
      "          8.7504e-07],\n",
      "         [1.5003e-06, 9.7156e-07, 2.1949e-06, 1.5340e-06, 1.6155e-06,\n",
      "          2.9340e-06, 1.3877e-06, 1.0556e-02, 2.2179e-02, 6.3866e-06,\n",
      "          2.5279e-05, 6.0844e-05, 2.4372e-02, 2.0594e-02, 3.5232e-06,\n",
      "          4.0802e-06, 2.2514e-05, 7.2466e-05, 6.3904e-05, 2.1945e-06,\n",
      "          7.7637e-07, 8.5004e-06, 3.9624e-07, 1.8626e-06, 1.8693e-06,\n",
      "          2.1446e-01, 1.4426e-06, 1.2838e-04, 2.1197e-04, 1.0888e-05,\n",
      "          2.5502e-05, 1.8473e-06, 1.4024e-02, 1.1848e-02, 6.0049e-05,\n",
      "          5.9838e-06, 2.8153e-06, 2.3124e-01, 1.9542e-01, 1.5495e-04,\n",
      "          1.4685e-04, 1.0180e-05, 1.7005e-01, 8.4195e-02, 1.0568e-06,\n",
      "          8.5590e-07, 2.2499e-06, 1.4875e-06, 2.5835e-06, 1.3443e-06,\n",
      "          1.0137e-06],\n",
      "         [1.8022e-05, 1.2975e-05, 1.8635e-05, 1.7800e-05, 2.2084e-05,\n",
      "          2.4872e-05, 9.1493e-06, 9.0075e-04, 7.9554e-04, 3.0247e-05,\n",
      "          1.3088e-05, 7.5462e-05, 5.6050e-02, 3.6324e-02, 4.9781e-05,\n",
      "          2.3963e-04, 4.5780e-05, 3.8712e-01, 4.5921e-01, 1.0852e-05,\n",
      "          7.8150e-04, 4.7238e-05, 1.2915e-03, 1.3548e-02, 4.3993e-03,\n",
      "          2.6806e-03, 2.7083e-05, 1.3904e-04, 9.9862e-05, 3.9257e-05,\n",
      "          2.3855e-05, 1.3026e-05, 2.3499e-03, 2.5303e-03, 4.8292e-05,\n",
      "          4.3075e-06, 1.1499e-05, 9.3287e-03, 1.4100e-02, 5.4167e-05,\n",
      "          6.9474e-05, 5.5383e-05, 4.8924e-03, 2.3617e-03, 7.7359e-06,\n",
      "          1.8860e-05, 1.4544e-05, 4.1106e-05, 1.2220e-05, 7.8963e-06,\n",
      "          9.7894e-06],\n",
      "         [2.0625e-05, 2.0393e-05, 3.2374e-05, 3.9645e-05, 2.7331e-05,\n",
      "          3.2067e-05, 1.0503e-05, 2.2695e-02, 4.0153e-02, 3.7151e-05,\n",
      "          4.3594e-05, 8.9024e-05, 6.0606e-02, 9.1967e-02, 4.0067e-05,\n",
      "          9.3070e-06, 8.4335e-05, 1.0140e-02, 1.0600e-02, 3.7295e-05,\n",
      "          2.1790e-05, 9.6100e-05, 1.6075e-04, 1.9416e-04, 2.0323e-05,\n",
      "          1.1566e-01, 2.3772e-05, 1.9533e-03, 2.1841e-03, 5.7832e-05,\n",
      "          4.0081e-04, 1.7124e-05, 9.7083e-02, 8.4948e-02, 1.9409e-04,\n",
      "          1.2416e-05, 5.7107e-05, 1.9193e-01, 8.2392e-02, 1.6590e-04,\n",
      "          2.1001e-04, 5.8874e-05, 1.1665e-01, 6.8657e-02, 4.0101e-05,\n",
      "          2.1364e-05, 2.1328e-05, 2.4149e-05, 2.7575e-05, 1.8706e-05,\n",
      "          1.5382e-05]]])\n",
      "Player 1 Prediction: tensor([[0.6313, 0.3610, 0.0077, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28316\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5216/10000 (52.2%)\n",
      "    Average reward: +0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4784/10000 (47.8%)\n",
      "    Average reward: -0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4478 (31.3%)\n",
      "    Action 1: 5443 (38.0%)\n",
      "    Action 2: 4284 (29.9%)\n",
      "    Action 3: 113 (0.8%)\n",
      "  Player 1:\n",
      "    Action 0: 4356 (31.1%)\n",
      "    Action 1: 4904 (35.0%)\n",
      "    Action 2: 3193 (22.8%)\n",
      "    Action 3: 1545 (11.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1810.5, -1810.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.055 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1810\n",
      "   Testing specific player: 1\n",
      "   At training step: 24000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1114, 0.2157, 0.6729]])\n",
      "Player 1 Prediction: tensor([[0.5142, 0.0524, 0.4333, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 24000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39331\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6935/10000 (69.3%)\n",
      "    Average reward: -0.119\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3065/10000 (30.6%)\n",
      "    Average reward: +0.119\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14467 (74.5%)\n",
      "    Action 1: 4957 (25.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3716 (18.7%)\n",
      "    Action 1: 9429 (47.4%)\n",
      "    Action 2: 3507 (17.6%)\n",
      "    Action 3: 3255 (16.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1187.0, 1187.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.819 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.963 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.891\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1187\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25002/50000 [34:52<28:28, 14.63it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 25000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 150574/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 145392/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [35:49<20:32, 19.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 26000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 157027/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 151968/2000000\n",
      "P1 SL Buffer Size:  157027\n",
      "P1 SL buffer distribution [47944. 63475. 33894. 11714.]\n",
      "P1 actions distribution [0.30532329 0.40422985 0.21584823 0.07459864]\n",
      "P2 SL Buffer Size:  151968\n",
      "P2 SL buffer distribution [41442. 62428. 34414. 13684.]\n",
      "P2 actions distribution [0.27270215 0.41079701 0.22645557 0.09004527]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/50000 [35:59<20:32, 19.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.3443, 0.6543, 0.0014, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.6800e-05, 2.1058e-05, 2.2867e-05, 2.5515e-05, 2.3977e-05,\n",
      "          1.3581e-05, 2.3576e-05, 4.5140e-04, 5.7211e-04, 6.7523e-04,\n",
      "          9.0520e-02, 1.0784e-02, 5.0890e-04, 8.3196e-04, 2.6990e-04,\n",
      "          1.5501e-01, 1.5171e-02, 1.2411e-02, 1.3454e-02, 7.9982e-05,\n",
      "          1.7051e-02, 1.4263e-03, 2.0070e-01, 9.7618e-02, 4.1432e-05,\n",
      "          1.3723e-01, 2.1635e-05, 2.3065e-02, 2.4525e-02, 7.7925e-04,\n",
      "          3.5963e-02, 4.1321e-05, 4.4392e-02, 3.1743e-02, 7.3659e-03,\n",
      "          6.2580e-02, 1.1479e-04, 1.0768e-03, 5.6907e-04, 3.3192e-03,\n",
      "          8.2943e-03, 3.9656e-04, 3.6355e-04, 2.6960e-04, 2.6067e-05,\n",
      "          3.0582e-05, 2.5031e-05, 1.5113e-05, 1.3087e-05, 3.0289e-05,\n",
      "          1.2991e-05],\n",
      "         [2.0689e-05, 1.8675e-05, 2.0315e-05, 2.6901e-05, 2.7004e-05,\n",
      "          1.6370e-05, 1.7457e-05, 7.3789e-03, 3.2676e-02, 4.0353e-04,\n",
      "          1.0779e-02, 2.6864e-03, 3.0356e-02, 9.8223e-02, 8.0533e-05,\n",
      "          1.7232e-02, 3.0282e-03, 1.2385e-02, 1.6127e-02, 3.6095e-05,\n",
      "          3.0591e-02, 9.2516e-04, 6.6127e-04, 4.0012e-04, 3.8632e-05,\n",
      "          1.8657e-01, 2.7431e-05, 1.8210e-01, 2.3771e-01, 6.2699e-04,\n",
      "          1.1503e-02, 2.0817e-05, 2.5082e-02, 2.2235e-02, 2.2016e-03,\n",
      "          5.3364e-03, 5.7749e-05, 3.3068e-02, 9.8043e-03, 1.4471e-03,\n",
      "          3.3569e-03, 4.8977e-04, 1.1688e-02, 2.3704e-03, 1.4583e-05,\n",
      "          2.9975e-05, 1.6357e-05, 1.3189e-05, 1.5915e-05, 2.6402e-05,\n",
      "          2.6752e-05],\n",
      "         [2.0761e-07, 1.3080e-07, 1.1743e-07, 1.2083e-07, 1.3904e-07,\n",
      "          1.0975e-07, 1.7137e-07, 1.8212e-06, 1.5806e-06, 8.2556e-07,\n",
      "          4.1598e-06, 2.8108e-06, 3.9729e-05, 4.5427e-05, 3.9265e-07,\n",
      "          1.0430e-04, 2.4270e-06, 6.8044e-05, 5.5847e-05, 1.8151e-07,\n",
      "          4.4007e-05, 9.1485e-07, 4.1826e-01, 5.6727e-01, 1.4021e-02,\n",
      "          1.0988e-05, 1.4939e-07, 1.5336e-05, 9.3059e-06, 1.3337e-06,\n",
      "          1.7391e-06, 1.7422e-07, 1.0423e-05, 9.1824e-06, 3.0227e-06,\n",
      "          2.0353e-06, 4.9359e-07, 3.9025e-06, 3.8743e-06, 1.0364e-06,\n",
      "          2.1790e-06, 7.8447e-07, 1.1976e-06, 6.9672e-07, 1.7512e-07,\n",
      "          1.9154e-07, 1.5470e-07, 9.3340e-08, 1.3903e-07, 1.4881e-07,\n",
      "          2.3131e-07],\n",
      "         [1.0797e-05, 2.9630e-05, 2.2933e-05, 3.0397e-05, 2.0083e-05,\n",
      "          1.0006e-05, 2.2681e-05, 1.9508e-04, 2.5580e-04, 8.8648e-05,\n",
      "          4.7057e-04, 2.2762e-04, 1.9422e-02, 4.0349e-02, 6.5243e-05,\n",
      "          4.7530e-04, 3.9659e-04, 5.6154e-02, 8.3048e-02, 3.2237e-05,\n",
      "          3.7899e-04, 1.3129e-04, 7.5340e-02, 7.3850e-02, 7.6351e-05,\n",
      "          3.9568e-01, 1.3414e-05, 5.9043e-02, 6.7354e-02, 1.1941e-04,\n",
      "          3.7122e-04, 3.0059e-05, 5.7449e-02, 4.0448e-02, 3.9298e-04,\n",
      "          1.7195e-04, 4.9030e-05, 2.0460e-02, 6.5486e-03, 1.5554e-04,\n",
      "          1.2651e-04, 1.0496e-04, 1.6738e-04, 9.4065e-05, 2.2574e-05,\n",
      "          1.6668e-05, 2.2196e-05, 1.2134e-05, 1.3634e-05, 1.3593e-05,\n",
      "          1.2635e-05]]])\n",
      "Player 0 Prediction: tensor([[0.1550, 0.8151, 0.0299, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[8.1066e-06, 9.5721e-06, 1.7303e-05, 1.3765e-05, 9.8972e-06,\n",
      "          1.3518e-05, 1.6935e-05, 4.2820e-02, 1.2217e-01, 9.0922e-05,\n",
      "          2.1432e-04, 5.5184e-04, 1.4390e-01, 2.7185e-01, 5.8130e-05,\n",
      "          2.7130e-03, 2.1316e-04, 1.0490e-02, 1.9384e-02, 4.4061e-05,\n",
      "          2.6602e-04, 7.6482e-05, 6.2372e-05, 1.7503e-04, 1.0409e-05,\n",
      "          1.3147e-01, 1.8073e-05, 1.6867e-04, 1.3563e-04, 5.7575e-05,\n",
      "          2.8832e-04, 2.1249e-05, 8.5636e-03, 1.0199e-02, 1.9926e-04,\n",
      "          1.0201e-03, 2.2401e-05, 1.0591e-01, 6.0556e-02, 3.4831e-04,\n",
      "          1.1425e-04, 7.6262e-05, 4.5661e-02, 1.9902e-02, 1.1764e-05,\n",
      "          1.5574e-05, 1.9972e-05, 8.8156e-06, 1.1352e-05, 1.5698e-05,\n",
      "          1.4717e-05],\n",
      "         [4.2774e-06, 6.6948e-06, 7.9116e-06, 8.0625e-06, 1.8780e-05,\n",
      "          6.7302e-06, 6.5426e-06, 2.9695e-02, 5.8920e-02, 1.0046e-04,\n",
      "          2.7696e-02, 2.1127e-03, 9.4394e-02, 1.7037e-01, 2.6053e-05,\n",
      "          1.4160e-03, 2.4998e-04, 1.8704e-04, 2.8381e-04, 1.7326e-05,\n",
      "          1.7928e-04, 6.4175e-05, 2.6272e-06, 6.6773e-06, 3.6472e-06,\n",
      "          5.0556e-01, 1.0056e-05, 3.2353e-04, 3.2532e-04, 4.8668e-05,\n",
      "          1.8924e-03, 6.1848e-06, 1.1173e-03, 6.9173e-04, 9.9405e-05,\n",
      "          2.8130e-03, 9.3594e-06, 3.9315e-02, 2.4636e-02, 3.3542e-03,\n",
      "          9.0720e-03, 9.5350e-05, 1.3777e-02, 1.1007e-02, 7.8968e-06,\n",
      "          9.1777e-06, 8.5563e-06, 1.3334e-05, 7.7672e-06, 9.0387e-06,\n",
      "          7.2205e-06],\n",
      "         [8.4134e-06, 4.1968e-06, 2.0238e-06, 2.8806e-06, 4.0788e-06,\n",
      "          4.9923e-06, 8.2371e-06, 4.1670e-04, 5.2506e-04, 1.0637e-05,\n",
      "          6.8834e-05, 5.6022e-05, 6.7080e-03, 8.3892e-03, 1.0875e-05,\n",
      "          4.9544e-04, 3.0150e-05, 4.9917e-03, 4.7411e-03, 5.9680e-06,\n",
      "          9.6595e-01, 6.9167e-06, 2.6775e-04, 4.0292e-03, 1.5217e-03,\n",
      "          5.7168e-04, 5.3903e-06, 1.9223e-05, 1.0483e-05, 8.9976e-06,\n",
      "          2.3730e-05, 5.3765e-06, 1.5533e-04, 4.6040e-05, 1.6772e-05,\n",
      "          3.3143e-05, 6.3248e-06, 3.3028e-04, 1.1950e-04, 2.3612e-05,\n",
      "          2.9205e-05, 1.0703e-05, 2.3558e-04, 6.4970e-05, 5.6462e-06,\n",
      "          2.5118e-06, 3.6373e-06, 3.4588e-06, 4.5345e-06, 1.2981e-06,\n",
      "          3.3692e-06],\n",
      "         [1.3113e-04, 2.2332e-04, 1.2654e-04, 2.8844e-04, 9.4132e-05,\n",
      "          1.4994e-04, 1.4870e-04, 2.4160e-02, 5.4675e-02, 6.8192e-04,\n",
      "          3.5091e-03, 2.4799e-03, 8.0422e-02, 1.4896e-01, 3.8951e-04,\n",
      "          3.2529e-03, 2.6698e-03, 4.1482e-02, 3.1020e-02, 2.3669e-04,\n",
      "          2.7722e-02, 1.8658e-04, 5.7662e-04, 1.4317e-03, 2.1412e-04,\n",
      "          3.6200e-01, 3.2926e-04, 2.1025e-03, 3.6627e-03, 3.2783e-04,\n",
      "          1.8336e-02, 3.1308e-04, 2.7029e-02, 1.9845e-02, 1.8669e-03,\n",
      "          2.5637e-03, 4.3926e-04, 6.5740e-02, 3.1992e-02, 2.7547e-03,\n",
      "          2.1493e-03, 2.7043e-04, 1.6116e-02, 1.6059e-02, 8.6800e-05,\n",
      "          5.5969e-05, 2.5159e-04, 2.0325e-04, 1.7311e-04, 4.4887e-05,\n",
      "          6.2791e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.8650e-01, 3.9518e-05, 1.3456e-02]])\n",
      "Player 1 Prediction: tensor([[[3.0433e-06, 3.7471e-06, 4.2681e-06, 5.5462e-06, 3.5180e-06,\n",
      "          2.4758e-06, 3.5622e-06, 5.4852e-03, 6.1643e-03, 1.5965e-05,\n",
      "          2.8988e-04, 7.5302e-05, 2.6583e-01, 2.3430e-01, 1.7169e-05,\n",
      "          3.1440e-05, 5.2485e-05, 1.7145e-03, 2.8872e-03, 1.2197e-05,\n",
      "          1.0929e-05, 4.0091e-05, 4.9721e-05, 9.7645e-05, 2.8374e-06,\n",
      "          2.2081e-01, 3.7496e-06, 1.0013e-04, 7.7001e-05, 3.9938e-05,\n",
      "          9.1603e-05, 7.7417e-06, 5.4188e-04, 1.0490e-03, 8.1310e-05,\n",
      "          8.1624e-06, 8.4881e-06, 1.0026e-01, 1.5350e-01, 4.8904e-05,\n",
      "          1.8261e-04, 2.0072e-05, 2.6785e-03, 3.3561e-03, 3.8034e-06,\n",
      "          8.8970e-06, 6.7650e-06, 1.9928e-06, 2.5990e-06, 5.7209e-06,\n",
      "          4.9376e-06],\n",
      "         [5.9155e-07, 5.8595e-07, 1.0578e-06, 6.5469e-07, 2.9560e-06,\n",
      "          4.2982e-07, 4.4445e-07, 3.2974e-01, 2.8692e-01, 8.1965e-06,\n",
      "          4.9145e-05, 1.2938e-05, 7.2359e-03, 1.4982e-02, 1.9494e-06,\n",
      "          6.0443e-07, 1.0428e-05, 3.6738e-05, 6.1492e-05, 7.7493e-07,\n",
      "          1.3605e-06, 1.2837e-05, 9.2867e-07, 9.3620e-07, 2.8327e-07,\n",
      "          1.8401e-01, 7.5816e-07, 2.6267e-05, 3.6135e-05, 6.8245e-06,\n",
      "          1.2711e-05, 5.6520e-07, 4.5563e-04, 3.7646e-04, 4.7421e-06,\n",
      "          8.0065e-07, 1.7613e-06, 1.4072e-02, 9.2086e-03, 1.1338e-05,\n",
      "          1.9550e-05, 1.0588e-05, 7.6701e-02, 7.5957e-02, 8.7599e-07,\n",
      "          1.2696e-06, 5.5826e-07, 1.4357e-06, 3.0098e-07, 1.1529e-06,\n",
      "          5.8119e-07],\n",
      "         [1.3154e-04, 2.9796e-05, 1.2281e-05, 1.4891e-05, 3.9881e-05,\n",
      "          3.4743e-05, 4.1646e-05, 9.2306e-03, 1.2497e-02, 5.8097e-05,\n",
      "          1.6140e-04, 1.0926e-04, 7.0599e-02, 7.3709e-02, 4.4280e-05,\n",
      "          9.8018e-04, 1.1124e-04, 3.7522e-01, 3.3722e-01, 2.1006e-05,\n",
      "          2.1246e-03, 3.4364e-05, 5.9944e-03, 7.0301e-02, 1.0786e-02,\n",
      "          1.2542e-02, 2.9096e-05, 2.5286e-04, 1.0924e-04, 7.3053e-05,\n",
      "          3.9992e-05, 3.5847e-05, 1.0399e-03, 5.0370e-04, 1.2140e-04,\n",
      "          1.3417e-05, 8.4824e-05, 4.9350e-03, 4.0764e-03, 3.8059e-05,\n",
      "          1.1935e-04, 6.3229e-05, 4.4795e-03, 1.7040e-03, 5.6576e-05,\n",
      "          4.6200e-05, 1.7331e-05, 2.3800e-05, 2.9649e-05, 1.7427e-05,\n",
      "          3.8971e-05],\n",
      "         [5.2567e-05, 5.6173e-05, 3.5401e-05, 9.9548e-05, 2.8137e-05,\n",
      "          3.8715e-05, 4.1773e-05, 3.6447e-02, 5.3496e-02, 1.7711e-04,\n",
      "          2.4752e-04, 1.6401e-04, 7.8417e-02, 1.2171e-01, 1.5250e-04,\n",
      "          6.1519e-05, 4.8211e-04, 1.8465e-02, 1.3360e-02, 5.7902e-05,\n",
      "          1.0998e-04, 1.1352e-04, 6.2379e-04, 1.5068e-03, 9.4796e-05,\n",
      "          5.2291e-01, 1.2348e-04, 2.8539e-03, 4.3243e-03, 2.0444e-04,\n",
      "          5.9173e-04, 9.3828e-05, 7.2534e-03, 1.2103e-02, 3.4279e-04,\n",
      "          3.1141e-05, 2.2112e-04, 5.1760e-02, 3.3486e-02, 2.5358e-04,\n",
      "          2.0809e-04, 8.8817e-05, 1.7424e-02, 1.9381e-02, 3.1577e-05,\n",
      "          2.9912e-05, 6.3813e-05, 8.7229e-05, 5.2052e-05, 2.1893e-05,\n",
      "          1.4041e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30021\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5707/10000 (57.1%)\n",
      "    Average reward: -0.177\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4293/10000 (42.9%)\n",
      "    Average reward: +0.177\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4986 (33.7%)\n",
      "    Action 1: 5430 (36.7%)\n",
      "    Action 2: 3123 (21.1%)\n",
      "    Action 3: 1273 (8.6%)\n",
      "  Player 1:\n",
      "    Action 0: 4609 (30.3%)\n",
      "    Action 1: 6028 (39.6%)\n",
      "    Action 2: 4413 (29.0%)\n",
      "    Action 3: 159 (1.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1774.5, 1774.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.055\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1774\n",
      "   Testing specific player: 0\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.8834e-01, 1.1041e-04, 1.1554e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.9336, 0.0020, 0.0644]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41312\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4477/10000 (44.8%)\n",
      "    Average reward: +0.167\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5523/10000 (55.2%)\n",
      "    Average reward: -0.167\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4197 (20.2%)\n",
      "    Action 1: 10221 (49.2%)\n",
      "    Action 2: 3391 (16.3%)\n",
      "    Action 3: 2971 (14.3%)\n",
      "  Player 1:\n",
      "    Action 0: 15253 (74.3%)\n",
      "    Action 1: 5279 (25.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1668.0, -1668.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.970 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.822 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.896\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1668\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7141, 0.2522, 0.0337, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.7051e-05, 3.9313e-05, 3.4609e-05, 2.2125e-05, 2.5567e-05,\n",
      "          2.8377e-05, 2.9198e-05, 1.7709e-04, 2.0620e-04, 7.9308e-04,\n",
      "          4.2591e-02, 4.4652e-03, 2.7879e-04, 5.0101e-04, 5.1068e-04,\n",
      "          2.3404e-02, 5.9245e-03, 2.2368e-02, 1.8430e-02, 8.6704e-05,\n",
      "          4.9876e-03, 2.2282e-03, 4.0629e-02, 1.2678e-02, 3.0869e-05,\n",
      "          1.8701e-01, 2.8607e-05, 5.5031e-02, 4.1318e-02, 9.5946e-04,\n",
      "          4.9575e-02, 5.4960e-05, 1.3385e-01, 1.0140e-01, 1.1263e-02,\n",
      "          9.1577e-02, 3.3205e-04, 7.8238e-03, 3.9539e-03, 1.0008e-02,\n",
      "          1.2056e-01, 1.2548e-03, 2.0938e-03, 1.2260e-03, 3.4054e-05,\n",
      "          4.2883e-05, 1.9319e-05, 2.8406e-05, 2.6434e-05, 2.2695e-05,\n",
      "          1.9392e-05],\n",
      "         [1.2513e-05, 1.2804e-05, 2.0542e-05, 1.3602e-05, 1.5237e-05,\n",
      "          1.0673e-05, 1.3387e-05, 6.1499e-04, 2.8250e-03, 1.2671e-04,\n",
      "          2.5571e-03, 1.7206e-03, 9.0082e-03, 1.0859e-02, 4.0691e-05,\n",
      "          2.9586e-02, 1.6766e-03, 8.3878e-03, 8.1742e-03, 3.2144e-05,\n",
      "          5.4982e-03, 6.1977e-04, 1.7264e-04, 1.6471e-04, 3.4234e-05,\n",
      "          6.1561e-02, 1.4060e-05, 3.1975e-01, 2.6347e-01, 4.3719e-04,\n",
      "          1.9025e-02, 1.3982e-05, 2.7827e-02, 2.0357e-02, 5.2949e-03,\n",
      "          7.2309e-02, 4.7674e-05, 4.1892e-02, 2.2029e-02, 7.7959e-03,\n",
      "          2.4397e-02, 3.3931e-04, 2.7193e-02, 3.9503e-03, 1.0783e-05,\n",
      "          1.5972e-05, 1.5282e-05, 9.1445e-06, 1.9139e-05, 1.6743e-05,\n",
      "          1.1972e-05],\n",
      "         [3.7529e-07, 5.4407e-07, 4.8853e-07, 2.1085e-07, 2.4630e-07,\n",
      "          5.4990e-07, 3.6316e-07, 9.7363e-07, 8.0565e-07, 1.8955e-06,\n",
      "          8.1189e-06, 9.2370e-06, 2.6671e-05, 5.7164e-05, 1.3730e-06,\n",
      "          2.8489e-04, 6.8790e-06, 1.1920e-04, 1.3785e-04, 4.2150e-07,\n",
      "          1.1146e-04, 3.0727e-06, 3.6801e-01, 5.9199e-01, 3.8876e-02,\n",
      "          2.5184e-05, 4.0929e-07, 6.8229e-05, 5.4750e-05, 1.9756e-06,\n",
      "          7.0198e-06, 3.9933e-07, 5.4460e-05, 3.3912e-05, 8.6998e-06,\n",
      "          1.3576e-05, 9.8140e-07, 1.1521e-05, 1.8877e-05, 1.2414e-05,\n",
      "          2.8262e-05, 3.6307e-06, 4.8476e-06, 2.1903e-06, 3.7507e-07,\n",
      "          6.5410e-07, 3.6581e-07, 4.1169e-07, 4.2148e-07, 2.1487e-07,\n",
      "          2.3998e-07],\n",
      "         [2.5114e-05, 3.7605e-05, 5.1591e-05, 2.1003e-05, 3.6027e-05,\n",
      "          3.7418e-05, 3.4410e-05, 1.4067e-04, 1.2712e-04, 1.0681e-04,\n",
      "          5.3213e-04, 2.9559e-04, 4.9826e-03, 1.0049e-02, 5.3929e-05,\n",
      "          4.7678e-04, 4.1619e-04, 3.2753e-02, 4.0600e-02, 5.8522e-05,\n",
      "          3.6460e-04, 2.0170e-04, 5.1643e-02, 4.5097e-02, 1.4461e-04,\n",
      "          1.3864e-01, 2.7718e-05, 1.8064e-01, 1.5739e-01, 2.4580e-04,\n",
      "          2.2838e-03, 2.9631e-05, 1.7380e-01, 1.1194e-01, 6.4682e-04,\n",
      "          8.3828e-04, 6.8779e-05, 2.9686e-02, 1.1950e-02, 7.8941e-04,\n",
      "          1.1670e-03, 2.3224e-04, 7.1230e-04, 4.5487e-04, 2.7620e-05,\n",
      "          1.8825e-05, 2.0697e-05, 2.7632e-05, 2.3855e-05, 1.9536e-05,\n",
      "          1.8855e-05]]])\n",
      "Player 1 Prediction: tensor([[0.2047, 0.1270, 0.6684, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28270\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5346/10000 (53.5%)\n",
      "    Average reward: +0.218\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4654/10000 (46.5%)\n",
      "    Average reward: -0.218\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4371 (30.8%)\n",
      "    Action 1: 5487 (38.6%)\n",
      "    Action 2: 4220 (29.7%)\n",
      "    Action 3: 121 (0.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4352 (30.9%)\n",
      "    Action 1: 4910 (34.9%)\n",
      "    Action 2: 3403 (24.2%)\n",
      "    Action 3: 1406 (10.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [2175.5, -2175.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.054 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.053\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.2175\n",
      "   Testing specific player: 1\n",
      "   At training step: 26000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.8898e-01, 2.0801e-04, 1.0811e-02]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7720, 0.0060, 0.2219]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 26000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39361\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6857/10000 (68.6%)\n",
      "    Average reward: -0.191\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3143/10000 (31.4%)\n",
      "    Average reward: +0.191\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14469 (74.5%)\n",
      "    Action 1: 4944 (25.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3707 (18.6%)\n",
      "    Action 1: 9424 (47.2%)\n",
      "    Action 2: 3630 (18.2%)\n",
      "    Action 3: 3187 (16.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1913.5, 1913.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.819 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.962 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.890\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1913\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27002/50000 [37:54<23:49, 16.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 27000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 163312/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 158449/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28000/50000 [38:56<19:39, 18.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 28000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 169858/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 165071/2000000\n",
      "P1 SL Buffer Size:  169858\n",
      "P1 SL buffer distribution [51856. 68598. 37514. 11890.]\n",
      "P1 actions distribution [0.3052903  0.40385498 0.22085507 0.06999965]\n",
      "P2 SL Buffer Size:  165071\n",
      "P2 SL buffer distribution [44981. 67860. 38265. 13965.]\n",
      "P2 actions distribution [0.27249487 0.41109583 0.23180934 0.08459996]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28000/50000 [39:10<19:39, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[4.6081e-05, 3.8611e-05, 5.3027e-05, 7.0011e-05, 4.7705e-05,\n",
      "          6.7035e-05, 5.9325e-05, 2.0004e-03, 5.5089e-03, 2.9274e-03,\n",
      "          1.3008e-02, 5.7142e-03, 8.4385e-03, 1.9358e-02, 9.2126e-04,\n",
      "          2.9218e-02, 1.0807e-02, 1.2468e-02, 1.6162e-02, 1.3802e-04,\n",
      "          7.5037e-03, 1.7362e-03, 1.8809e-01, 2.4146e-01, 3.3839e-04,\n",
      "          1.0786e-01, 6.3606e-05, 8.1501e-02, 7.8508e-02, 1.6834e-03,\n",
      "          3.2494e-02, 1.2350e-04, 3.2988e-02, 2.9406e-02, 3.1681e-03,\n",
      "          1.7428e-02, 2.2015e-04, 2.7576e-02, 8.9318e-03, 2.2162e-03,\n",
      "          4.0208e-03, 1.4444e-03, 2.6343e-03, 1.1734e-03, 5.8662e-05,\n",
      "          6.3697e-05, 6.1353e-05, 5.3677e-05, 5.0039e-05, 6.1345e-05,\n",
      "          2.8478e-05],\n",
      "         [2.7567e-05, 3.0115e-05, 2.8981e-05, 3.3545e-05, 2.4487e-05,\n",
      "          2.3280e-05, 2.8944e-05, 9.4612e-03, 5.4979e-02, 7.7213e-04,\n",
      "          1.0561e-02, 4.0005e-03, 2.5762e-02, 1.0014e-01, 1.0577e-04,\n",
      "          1.6133e-02, 5.2368e-03, 2.2267e-03, 3.4650e-03, 3.8076e-05,\n",
      "          4.3293e-02, 1.2679e-03, 1.5653e-04, 3.4924e-04, 2.0741e-04,\n",
      "          9.0798e-02, 4.2053e-05, 2.6056e-01, 2.5692e-01, 8.9862e-04,\n",
      "          1.7069e-02, 5.1958e-05, 4.0203e-03, 3.1167e-03, 2.5468e-03,\n",
      "          1.2650e-02, 4.4079e-05, 3.5912e-02, 8.2143e-03, 2.6305e-03,\n",
      "          4.7483e-03, 7.4839e-04, 1.7166e-02, 3.3119e-03, 3.1302e-05,\n",
      "          3.0541e-05, 2.8238e-05, 2.7628e-05, 2.6887e-05, 2.3660e-05,\n",
      "          3.1267e-05],\n",
      "         [5.1945e-07, 5.3904e-07, 4.7794e-07, 3.7596e-07, 3.1255e-07,\n",
      "          6.2129e-07, 5.9707e-07, 6.3890e-06, 1.2332e-05, 4.6677e-06,\n",
      "          5.8110e-06, 6.7209e-06, 1.4650e-04, 1.9166e-04, 1.0869e-06,\n",
      "          1.2348e-04, 6.9821e-06, 1.7695e-04, 1.4560e-04, 5.0864e-07,\n",
      "          2.1589e-04, 3.2576e-06, 6.1326e-03, 2.4401e-01, 7.4856e-01,\n",
      "          2.1260e-05, 6.8932e-07, 6.1793e-05, 4.0336e-05, 5.9430e-06,\n",
      "          7.4308e-06, 9.0702e-07, 1.3933e-05, 2.1024e-05, 6.1474e-06,\n",
      "          4.7707e-06, 9.7801e-07, 1.8764e-05, 2.1368e-05, 3.1735e-06,\n",
      "          5.5655e-06, 4.2611e-06, 6.9199e-06, 2.6930e-06, 6.6748e-07,\n",
      "          7.3563e-07, 4.7915e-07, 3.8198e-07, 5.1216e-07, 3.3706e-07,\n",
      "          6.6969e-07],\n",
      "         [4.4779e-05, 7.7713e-05, 7.3616e-05, 9.8972e-05, 5.5532e-05,\n",
      "          6.3410e-05, 5.8964e-05, 1.0129e-03, 1.7660e-03, 2.6937e-04,\n",
      "          6.8468e-04, 5.9850e-04, 4.7670e-02, 1.4415e-01, 2.1785e-04,\n",
      "          9.4135e-04, 1.1106e-03, 7.6513e-02, 1.1209e-01, 7.7427e-05,\n",
      "          9.2654e-04, 4.0960e-04, 1.5150e-02, 5.6932e-02, 1.0543e-03,\n",
      "          2.3214e-01, 6.8890e-05, 6.6629e-02, 4.9029e-02, 2.6487e-04,\n",
      "          1.2541e-03, 1.2369e-04, 6.0902e-02, 4.6268e-02, 5.1717e-04,\n",
      "          4.5952e-04, 6.8458e-05, 5.7600e-02, 2.0058e-02, 3.6969e-04,\n",
      "          3.3402e-04, 3.4976e-04, 8.4223e-04, 3.2651e-04, 7.5598e-05,\n",
      "          4.5394e-05, 6.4017e-05, 4.8669e-05, 4.4663e-05, 3.5753e-05,\n",
      "          2.5529e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.8921e-01, 6.9286e-05, 1.0724e-02]])\n",
      "Player 1 Prediction: tensor([[[1.1303e-05, 7.7466e-06, 1.2680e-05, 8.3798e-06, 1.0420e-05,\n",
      "          2.7527e-06, 7.5976e-06, 3.9431e-04, 3.9963e-04, 1.5424e-04,\n",
      "          1.3448e-01, 2.1258e-02, 3.4821e-04, 2.2284e-04, 4.7704e-05,\n",
      "          2.6276e-01, 3.8671e-02, 1.8168e-03, 1.1938e-03, 1.1091e-05,\n",
      "          5.2017e-02, 1.5007e-03, 2.4107e-03, 1.9559e-03, 2.9661e-06,\n",
      "          1.5649e-01, 6.5894e-06, 2.5709e-04, 2.5426e-04, 3.2032e-04,\n",
      "          1.1639e-01, 2.4616e-05, 1.0208e-03, 1.3654e-03, 1.0295e-02,\n",
      "          1.4331e-01, 2.7844e-05, 3.6787e-04, 5.0304e-04, 8.8686e-03,\n",
      "          4.0191e-02, 6.8351e-05, 2.2607e-04, 2.3748e-04, 1.8454e-05,\n",
      "          1.1558e-05, 1.0868e-05, 1.0773e-05, 4.7106e-06, 1.0025e-05,\n",
      "          5.6884e-06],\n",
      "         [6.0411e-06, 6.3947e-06, 1.4855e-05, 7.4836e-06, 9.8937e-06,\n",
      "          4.6824e-06, 4.9370e-06, 4.1556e-02, 2.4354e-01, 9.0297e-05,\n",
      "          3.0886e-04, 1.1122e-03, 7.6238e-02, 1.8940e-01, 1.7050e-05,\n",
      "          8.0183e-04, 8.8793e-04, 8.0091e-03, 9.3328e-03, 8.3644e-06,\n",
      "          8.6925e-04, 4.0986e-04, 8.2883e-05, 7.0591e-05, 1.6426e-06,\n",
      "          1.3454e-01, 8.3187e-06, 1.3203e-03, 1.1953e-03, 2.6424e-04,\n",
      "          3.4175e-02, 1.3581e-05, 4.4440e-03, 4.5248e-03, 2.6070e-04,\n",
      "          3.6286e-04, 1.6161e-05, 1.2623e-01, 5.2802e-02, 5.7730e-04,\n",
      "          2.5224e-04, 5.7483e-05, 5.8408e-02, 7.6826e-03, 7.8003e-06,\n",
      "          1.6581e-05, 7.1716e-06, 1.0656e-05, 7.9202e-06, 1.1650e-05,\n",
      "          7.8376e-06],\n",
      "         [3.0902e-07, 2.2819e-07, 4.2785e-07, 2.4865e-07, 1.8322e-07,\n",
      "          1.2675e-07, 2.3916e-07, 5.1367e-06, 8.7065e-06, 1.2713e-06,\n",
      "          6.5668e-06, 6.0902e-06, 1.0768e-04, 1.1085e-04, 4.3678e-07,\n",
      "          1.4981e-04, 8.2622e-06, 1.3831e-04, 8.8452e-05, 1.5024e-07,\n",
      "          2.1444e-04, 2.0776e-06, 4.7119e-01, 5.2780e-01, 4.3170e-05,\n",
      "          2.3442e-05, 1.6022e-07, 2.8836e-06, 2.3282e-06, 2.0812e-06,\n",
      "          1.7083e-05, 3.7924e-07, 3.4491e-06, 5.7900e-06, 5.1737e-06,\n",
      "          4.8972e-06, 4.3095e-07, 1.5089e-05, 1.4574e-05, 4.3592e-06,\n",
      "          4.5566e-06, 4.3364e-07, 4.5063e-06, 2.5689e-06, 2.7251e-07,\n",
      "          3.0341e-07, 2.5190e-07, 3.9550e-07, 2.9017e-07, 3.2046e-07,\n",
      "          3.0540e-07],\n",
      "         [1.8088e-05, 3.7245e-05, 8.8696e-05, 4.5985e-05, 2.7380e-05,\n",
      "          1.3422e-05, 3.8323e-05, 9.0632e-04, 1.7275e-03, 2.2877e-04,\n",
      "          1.5921e-03, 1.0602e-03, 3.8857e-02, 4.5728e-02, 6.6346e-05,\n",
      "          1.7833e-03, 3.8849e-03, 2.7160e-02, 3.7607e-02, 4.4129e-05,\n",
      "          4.6868e-03, 3.7568e-04, 3.4774e-02, 6.2190e-02, 2.9238e-05,\n",
      "          6.1364e-01, 1.8178e-05, 1.2212e-02, 1.3216e-02, 2.2650e-04,\n",
      "          1.1460e-02, 1.1899e-04, 1.5830e-02, 1.7521e-02, 1.2199e-03,\n",
      "          8.0452e-04, 7.9319e-05, 3.0813e-02, 1.6313e-02, 7.3277e-04,\n",
      "          8.4005e-04, 1.1867e-04, 1.0345e-03, 5.2235e-04, 4.6691e-05,\n",
      "          6.0343e-05, 3.2365e-05, 7.0101e-05, 2.5841e-05, 3.6511e-05,\n",
      "          3.6983e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29884\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5721/10000 (57.2%)\n",
      "    Average reward: -0.146\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4279/10000 (42.8%)\n",
      "    Average reward: +0.146\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5266 (35.6%)\n",
      "    Action 1: 5188 (35.1%)\n",
      "    Action 2: 3179 (21.5%)\n",
      "    Action 3: 1161 (7.8%)\n",
      "  Player 1:\n",
      "    Action 0: 4298 (28.5%)\n",
      "    Action 1: 6228 (41.3%)\n",
      "    Action 2: 4485 (29.7%)\n",
      "    Action 3: 79 (0.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1455.5, 1455.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.052\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1456\n",
      "   Testing specific player: 0\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.3974, 0.6016, 0.0010, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1372, 0.8396, 0.0232, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.6508e-01, 5.2488e-05, 3.4864e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41501\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4360/10000 (43.6%)\n",
      "    Average reward: +0.121\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5640/10000 (56.4%)\n",
      "    Average reward: -0.121\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4146 (19.8%)\n",
      "    Action 1: 10180 (48.7%)\n",
      "    Action 2: 3577 (17.1%)\n",
      "    Action 3: 3018 (14.4%)\n",
      "  Player 1:\n",
      "    Action 0: 15234 (74.0%)\n",
      "    Action 1: 5346 (26.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1212.0, -1212.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.968 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.826 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.897\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1212\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[1.3707e-01, 8.6218e-01, 7.4371e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[6.0904e-06, 1.2843e-05, 7.0615e-06, 1.5546e-05, 7.5014e-06,\n",
      "          1.8109e-05, 1.6235e-05, 2.8896e-04, 2.6729e-04, 6.9625e-05,\n",
      "          1.0252e-01, 2.2344e-02, 4.5757e-04, 3.4105e-04, 1.4878e-04,\n",
      "          2.9841e-01, 2.8106e-02, 4.1444e-03, 4.5866e-03, 2.2600e-05,\n",
      "          2.6729e-01, 3.3632e-03, 8.1353e-03, 5.2270e-03, 3.8085e-06,\n",
      "          3.1585e-02, 1.4801e-05, 2.6766e-04, 3.9828e-04, 2.7263e-04,\n",
      "          9.1723e-02, 1.5258e-05, 7.6779e-04, 6.2912e-04, 1.1189e-02,\n",
      "          8.8289e-02, 1.0733e-04, 1.8660e-04, 1.9874e-04, 5.9255e-03,\n",
      "          2.2220e-02, 1.1335e-04, 9.0082e-05, 1.0876e-04, 2.3494e-05,\n",
      "          1.6899e-05, 1.1264e-05, 5.8173e-06, 1.0417e-05, 1.2693e-05,\n",
      "          8.1058e-06],\n",
      "         [2.0461e-05, 1.0482e-05, 1.0241e-05, 2.1131e-05, 2.5762e-05,\n",
      "          1.2889e-05, 2.3645e-05, 2.8965e-02, 1.5467e-01, 6.7481e-05,\n",
      "          6.2664e-04, 4.5168e-03, 1.4077e-01, 2.8899e-01, 4.6193e-05,\n",
      "          4.7860e-03, 1.3921e-03, 3.7710e-02, 6.2078e-02, 2.5011e-05,\n",
      "          1.0229e-02, 5.2655e-04, 2.6956e-04, 1.4550e-04, 8.8642e-06,\n",
      "          3.7618e-02, 1.5581e-05, 2.7468e-03, 3.4401e-03, 2.3492e-04,\n",
      "          5.7822e-02, 1.4161e-05, 6.3486e-03, 4.4911e-03, 7.2240e-04,\n",
      "          1.1110e-03, 5.0657e-05, 6.6525e-02, 2.9220e-02, 1.4169e-03,\n",
      "          4.1284e-04, 7.8173e-05, 4.6696e-02, 4.9747e-03, 1.9269e-05,\n",
      "          2.3207e-05, 1.0969e-05, 1.0398e-05, 1.6715e-05, 2.7399e-05,\n",
      "          1.1935e-05],\n",
      "         [1.4778e-07, 1.3325e-07, 1.9409e-07, 7.8629e-08, 2.3134e-07,\n",
      "          2.3237e-07, 2.5765e-07, 1.9878e-06, 1.7676e-06, 3.5225e-07,\n",
      "          2.6284e-06, 4.1994e-06, 2.4372e-05, 4.3288e-05, 5.3843e-07,\n",
      "          1.6669e-04, 5.7678e-06, 5.9454e-05, 6.2495e-05, 1.1911e-07,\n",
      "          3.3217e-04, 1.3783e-06, 5.1967e-01, 4.7939e-01, 1.8188e-04,\n",
      "          8.4157e-06, 2.1666e-07, 2.5180e-06, 1.7827e-06, 4.6088e-07,\n",
      "          9.2951e-06, 8.1965e-08, 2.1868e-06, 9.5813e-07, 1.6544e-06,\n",
      "          3.1051e-06, 2.7355e-07, 1.4763e-06, 1.6324e-06, 2.0216e-06,\n",
      "          2.4956e-06, 4.3232e-07, 1.3001e-06, 8.2583e-07, 1.9806e-07,\n",
      "          2.1236e-07, 9.6278e-08, 1.7570e-07, 1.2657e-07, 8.5528e-08,\n",
      "          4.9673e-08],\n",
      "         [5.5279e-05, 1.3995e-04, 5.3269e-05, 4.5904e-05, 1.2559e-04,\n",
      "          1.0452e-04, 9.9342e-05, 1.7523e-03, 2.3080e-03, 1.2570e-04,\n",
      "          4.0103e-03, 2.3444e-03, 2.0419e-02, 5.2600e-02, 1.2970e-04,\n",
      "          3.5296e-03, 3.0238e-03, 7.6648e-02, 1.3966e-01, 7.2074e-05,\n",
      "          2.7740e-02, 6.0356e-04, 1.4679e-01, 1.3294e-01, 6.8968e-05,\n",
      "          2.9200e-01, 7.2596e-05, 1.0648e-02, 1.2868e-02, 2.7019e-04,\n",
      "          1.2733e-02, 4.6102e-05, 1.0001e-02, 1.1853e-02, 1.7202e-03,\n",
      "          6.7586e-04, 1.2130e-04, 1.6892e-02, 9.7837e-03, 1.3152e-03,\n",
      "          1.3183e-03, 8.6138e-05, 1.1277e-03, 6.9413e-04, 9.3819e-05,\n",
      "          6.7568e-05, 6.3640e-05, 4.3764e-05, 3.3296e-05, 5.4658e-05,\n",
      "          2.8755e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27828\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5196/10000 (52.0%)\n",
      "    Average reward: +0.154\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4804/10000 (48.0%)\n",
      "    Average reward: -0.154\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4163 (29.7%)\n",
      "    Action 1: 5347 (38.2%)\n",
      "    Action 2: 4420 (31.6%)\n",
      "    Action 3: 78 (0.6%)\n",
      "  Player 1:\n",
      "    Action 0: 4182 (30.3%)\n",
      "    Action 1: 4966 (35.9%)\n",
      "    Action 2: 3464 (25.1%)\n",
      "    Action 3: 1208 (8.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1544.5, -1544.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.051 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.052\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1545\n",
      "   Testing specific player: 1\n",
      "   At training step: 28000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.1796, 0.2452, 0.5752]])\n",
      "Player 1 Prediction: tensor([[0.4562, 0.0581, 0.4857, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 28000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39378\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6871/10000 (68.7%)\n",
      "    Average reward: -0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3129/10000 (31.3%)\n",
      "    Average reward: +0.181\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14466 (74.6%)\n",
      "    Action 1: 4914 (25.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3680 (18.4%)\n",
      "    Action 1: 9511 (47.6%)\n",
      "    Action 2: 3718 (18.6%)\n",
      "    Action 3: 3089 (15.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1812.0, 1812.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.817 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.959 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.888\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1812\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29004/50000 [40:28<17:55, 19.52it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 29000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 176011/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 171584/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [41:27<19:37, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 30000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 182249/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 178288/2000000\n",
      "P1 SL Buffer Size:  182249\n",
      "P1 SL buffer distribution [55708. 73317. 41129. 12095.]\n",
      "P1 actions distribution [0.30566972 0.40229027 0.22567476 0.06636525]\n",
      "P2 SL Buffer Size:  178288\n",
      "P2 SL buffer distribution [48773. 72976. 42227. 14312.]\n",
      "P2 actions distribution [0.273563   0.40931527 0.23684712 0.08027461]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7610, 0.2159, 0.0231, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[3.2479e-05, 1.0233e-05, 2.3553e-05, 9.3061e-06, 7.9549e-06,\n",
      "          1.0592e-05, 1.0969e-05, 1.5100e-04, 1.3781e-04, 8.3734e-04,\n",
      "          7.9207e-02, 7.3141e-03, 1.6612e-04, 3.3104e-04, 1.0997e-04,\n",
      "          8.5110e-02, 9.4988e-03, 1.8257e-02, 2.7519e-02, 3.3682e-05,\n",
      "          6.5668e-02, 1.7617e-03, 4.2636e-01, 1.2419e-01, 4.9640e-05,\n",
      "          1.6062e-02, 6.0968e-06, 5.4628e-03, 6.9257e-03, 2.7821e-04,\n",
      "          3.1993e-02, 1.3316e-05, 1.1068e-02, 9.7383e-03, 5.3771e-03,\n",
      "          5.3504e-02, 6.0362e-05, 2.9775e-04, 1.7725e-04, 1.5927e-03,\n",
      "          9.7965e-03, 5.4136e-04, 1.4878e-04, 7.1082e-05, 7.8251e-06,\n",
      "          1.0357e-05, 8.8371e-06, 1.7581e-05, 1.0340e-05, 1.3900e-05,\n",
      "          6.5027e-06],\n",
      "         [4.0714e-05, 1.4274e-05, 3.3575e-05, 1.2393e-05, 1.0305e-05,\n",
      "          3.0193e-05, 1.2381e-05, 3.5079e-03, 1.2678e-02, 6.8170e-04,\n",
      "          1.5612e-02, 4.4993e-03, 4.1399e-02, 8.4257e-02, 3.4646e-05,\n",
      "          4.9602e-02, 3.8381e-03, 5.0381e-02, 7.4035e-02, 4.1745e-05,\n",
      "          2.4923e-01, 2.1129e-03, 1.4632e-03, 6.2938e-04, 8.5929e-05,\n",
      "          2.0393e-02, 1.8152e-05, 1.6181e-01, 1.4181e-01, 2.6711e-04,\n",
      "          1.4623e-02, 1.0051e-05, 1.1656e-02, 9.5974e-03, 3.2116e-03,\n",
      "          9.2504e-03, 7.8646e-05, 1.2058e-02, 5.1240e-03, 1.6081e-03,\n",
      "          4.1094e-03, 1.2455e-03, 7.7002e-03, 1.0479e-03, 5.4769e-06,\n",
      "          1.4740e-05, 1.2544e-05, 3.1678e-05, 3.1261e-05, 2.7210e-05,\n",
      "          1.7640e-05],\n",
      "         [1.1401e-07, 2.2284e-08, 9.5474e-08, 2.7248e-08, 3.0006e-08,\n",
      "          6.6868e-08, 4.0686e-08, 2.8286e-07, 2.2846e-07, 6.4551e-07,\n",
      "          1.1521e-06, 1.9759e-06, 1.5010e-05, 8.8983e-06, 6.9541e-08,\n",
      "          4.9703e-05, 6.2357e-07, 2.3452e-05, 2.6878e-05, 7.0285e-08,\n",
      "          4.2803e-05, 9.7777e-07, 5.0453e-01, 4.8223e-01, 1.3048e-02,\n",
      "          6.4628e-07, 3.1731e-08, 2.8786e-06, 2.1295e-06, 2.9637e-07,\n",
      "          7.4078e-07, 3.6125e-08, 2.0291e-06, 1.6224e-06, 8.3285e-07,\n",
      "          7.0285e-07, 1.2598e-07, 7.6542e-07, 9.5993e-07, 2.4529e-07,\n",
      "          7.6297e-07, 4.3522e-07, 2.6521e-07, 1.6643e-07, 3.5044e-08,\n",
      "          2.8756e-08, 5.8951e-08, 6.8060e-08, 6.2018e-08, 3.0952e-08,\n",
      "          3.0826e-08],\n",
      "         [1.3998e-05, 2.7194e-05, 4.8996e-05, 1.0824e-05, 8.8592e-06,\n",
      "          1.4374e-05, 1.3018e-05, 1.0859e-04, 1.2371e-04, 2.0722e-04,\n",
      "          4.4476e-04, 2.5189e-04, 2.2147e-02, 3.5582e-02, 2.1690e-05,\n",
      "          8.3883e-04, 3.8149e-04, 8.0128e-02, 1.3941e-01, 2.8554e-05,\n",
      "          1.2766e-03, 4.6007e-04, 2.8564e-01, 2.4763e-01, 1.3577e-04,\n",
      "          4.8206e-02, 7.0610e-06, 3.5682e-02, 2.8322e-02, 8.9531e-05,\n",
      "          2.2433e-04, 1.1056e-05, 3.0928e-02, 2.4836e-02, 4.9981e-04,\n",
      "          2.0160e-04, 5.5799e-05, 1.0512e-02, 4.8125e-03, 1.0132e-04,\n",
      "          1.1560e-04, 1.8849e-04, 1.0317e-04, 6.5345e-05, 1.3255e-05,\n",
      "          6.2782e-06, 1.2090e-05, 2.7303e-05, 1.9875e-05, 8.4708e-06,\n",
      "          6.2097e-06]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29999/50000 [41:40<19:37, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28658\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6146/10000 (61.5%)\n",
      "    Average reward: -0.148\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3854/10000 (38.5%)\n",
      "    Average reward: +0.148\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5122 (36.7%)\n",
      "    Action 1: 4988 (35.8%)\n",
      "    Action 2: 2718 (19.5%)\n",
      "    Action 3: 1117 (8.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4669 (31.7%)\n",
      "    Action 1: 4797 (32.6%)\n",
      "    Action 2: 4601 (31.3%)\n",
      "    Action 3: 646 (4.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1480.0, 1480.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.057\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1480\n",
      "   Testing specific player: 0\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.4717, 0.5275, 0.0008, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.6940e-01, 4.9699e-04, 3.0107e-02]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41810\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4298/10000 (43.0%)\n",
      "    Average reward: +0.082\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5702/10000 (57.0%)\n",
      "    Average reward: -0.082\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4262 (20.2%)\n",
      "    Action 1: 10288 (48.9%)\n",
      "    Action 2: 3596 (17.1%)\n",
      "    Action 3: 2913 (13.8%)\n",
      "  Player 1:\n",
      "    Action 0: 15330 (73.9%)\n",
      "    Action 1: 5421 (26.1%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [815.5, -815.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.829 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.900\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0815\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[4.3621e-05, 6.4653e-05, 5.5263e-05, 5.1380e-05, 7.0034e-05,\n",
      "          5.0018e-05, 4.0381e-05, 1.4024e-03, 3.2940e-03, 1.9600e-03,\n",
      "          3.7185e-03, 2.5238e-03, 9.1737e-03, 2.3257e-02, 1.3208e-03,\n",
      "          4.9786e-03, 2.1634e-03, 1.7617e-02, 2.6322e-02, 1.7350e-04,\n",
      "          1.3696e-03, 8.9756e-04, 3.0973e-02, 2.4506e-02, 3.5496e-04,\n",
      "          1.8262e-01, 8.0732e-05, 1.2594e-01, 1.2192e-01, 1.2339e-03,\n",
      "          5.1506e-02, 1.6151e-04, 9.2155e-02, 7.0802e-02, 4.2831e-03,\n",
      "          1.2394e-02, 8.0161e-04, 1.1010e-01, 3.5430e-02, 3.7668e-03,\n",
      "          5.8531e-03, 3.1499e-03, 1.5245e-02, 5.7348e-03, 5.0946e-05,\n",
      "          6.5563e-05, 7.3640e-05, 7.9364e-05, 5.2174e-05, 5.9393e-05,\n",
      "          5.7293e-05],\n",
      "         [1.3184e-05, 2.3027e-05, 1.5592e-05, 2.1989e-05, 2.4887e-05,\n",
      "          1.7680e-05, 1.1077e-05, 1.2056e-03, 6.8092e-03, 1.6897e-04,\n",
      "          4.1477e-03, 2.2223e-03, 7.7500e-03, 1.9395e-02, 8.1368e-05,\n",
      "          1.9941e-02, 1.5750e-03, 6.2954e-04, 7.9224e-04, 3.0158e-05,\n",
      "          5.3295e-03, 5.7993e-04, 4.1335e-05, 7.5387e-05, 1.1813e-04,\n",
      "          1.6730e-01, 2.4877e-05, 3.1489e-01, 3.0234e-01, 4.6861e-04,\n",
      "          1.7836e-02, 2.0497e-05, 1.8954e-03, 1.3516e-03, 4.1606e-03,\n",
      "          3.8714e-02, 4.6567e-05, 2.5623e-02, 7.9078e-03, 6.7345e-03,\n",
      "          1.4272e-02, 3.9748e-04, 2.0853e-02, 4.0068e-03, 1.3007e-05,\n",
      "          1.6253e-05, 2.4002e-05, 1.5147e-05, 2.2789e-05, 2.3731e-05,\n",
      "          2.6205e-05],\n",
      "         [1.1217e-06, 9.7048e-07, 8.7944e-07, 8.7273e-07, 7.8856e-07,\n",
      "          1.3262e-06, 7.1521e-07, 3.7906e-06, 4.9416e-06, 4.2089e-06,\n",
      "          6.8790e-06, 1.3845e-05, 1.4339e-04, 4.1041e-04, 2.9621e-06,\n",
      "          2.0597e-04, 9.7907e-06, 3.4482e-04, 3.0665e-04, 1.2475e-06,\n",
      "          3.4839e-04, 3.8302e-06, 2.2622e-03, 1.1116e-01, 8.8424e-01,\n",
      "          4.8725e-05, 8.0351e-07, 1.1365e-04, 9.7991e-05, 3.3591e-06,\n",
      "          1.8748e-05, 1.4984e-06, 3.3331e-05, 2.1826e-05, 8.6370e-06,\n",
      "          1.0776e-05, 2.1430e-06, 4.5791e-05, 4.3860e-05, 1.7106e-05,\n",
      "          1.8266e-05, 9.1287e-06, 1.7358e-05, 4.4748e-06, 7.7379e-07,\n",
      "          1.1696e-06, 1.3623e-06, 1.8990e-06, 8.5600e-07, 6.7951e-07,\n",
      "          6.0019e-07],\n",
      "         [6.1235e-05, 1.2477e-04, 1.2088e-04, 7.1331e-05, 1.2019e-04,\n",
      "          9.1496e-05, 7.0952e-05, 8.0686e-04, 1.7412e-03, 3.6417e-04,\n",
      "          5.6419e-04, 8.9949e-04, 3.7755e-02, 6.6124e-02, 2.5545e-04,\n",
      "          7.6586e-04, 9.0971e-04, 4.4634e-02, 5.2357e-02, 1.5161e-04,\n",
      "          1.0646e-03, 5.0773e-04, 8.0722e-03, 2.2960e-02, 2.0114e-03,\n",
      "          1.9072e-01, 8.9960e-05, 1.4423e-01, 1.2341e-01, 7.3912e-04,\n",
      "          7.2093e-03, 1.3181e-04, 1.2211e-01, 6.6642e-02, 1.2300e-03,\n",
      "          1.5281e-03, 2.1117e-04, 5.6707e-02, 3.2410e-02, 8.0062e-04,\n",
      "          1.2521e-03, 7.6222e-04, 4.7763e-03, 1.9213e-03, 7.1752e-05,\n",
      "          4.8550e-05, 7.2342e-05, 1.2017e-04, 7.6256e-05, 6.5838e-05,\n",
      "          5.2846e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9072e-01, 1.2600e-04, 9.1501e-03]])\n",
      "Player 0 Prediction: tensor([[[5.0835e-06, 7.9688e-06, 9.8865e-06, 6.8319e-06, 4.7501e-06,\n",
      "          7.3935e-06, 4.3410e-06, 1.0398e-04, 9.8536e-05, 4.8441e-05,\n",
      "          3.6911e-02, 5.5734e-03, 1.6677e-04, 1.3782e-04, 5.5401e-05,\n",
      "          4.4599e-02, 4.9878e-03, 7.8540e-04, 9.1266e-04, 1.2855e-05,\n",
      "          1.0259e-02, 8.7575e-04, 4.8265e-04, 1.4731e-04, 8.2348e-07,\n",
      "          4.9289e-01, 4.1652e-06, 2.2548e-04, 2.1454e-04, 2.0099e-04,\n",
      "          1.6193e-01, 1.0939e-05, 1.7855e-03, 1.8695e-03, 7.0086e-03,\n",
      "          1.3548e-01, 4.0027e-05, 1.1899e-03, 1.0404e-03, 1.1726e-02,\n",
      "          7.7320e-02, 1.0480e-04, 3.7434e-04, 3.3980e-04, 7.5518e-06,\n",
      "          8.6352e-06, 3.5868e-06, 5.5828e-06, 5.1730e-06, 6.5985e-06,\n",
      "          3.9817e-06],\n",
      "         [9.7190e-06, 4.6854e-06, 8.1755e-06, 8.5862e-06, 6.7335e-06,\n",
      "          5.6905e-06, 1.0034e-05, 2.6745e-03, 2.0840e-02, 2.8866e-05,\n",
      "          1.9899e-04, 1.3130e-03, 2.6636e-02, 4.0212e-02, 1.4099e-05,\n",
      "          3.7157e-03, 5.0700e-04, 3.4112e-03, 4.1877e-03, 1.1232e-05,\n",
      "          2.5393e-04, 1.8005e-04, 3.0784e-05, 2.5043e-05, 2.5027e-06,\n",
      "          4.8272e-01, 3.4343e-06, 1.9288e-03, 2.2746e-03, 1.1427e-04,\n",
      "          1.1238e-01, 1.3771e-05, 9.3351e-03, 7.4506e-03, 8.7981e-04,\n",
      "          5.0571e-03, 2.2111e-05, 1.1136e-01, 6.7014e-02, 4.5146e-03,\n",
      "          1.9736e-03, 4.5459e-05, 7.8502e-02, 1.0067e-02, 6.0311e-06,\n",
      "          6.6444e-06, 6.3003e-06, 4.0913e-06, 7.7479e-06, 7.7040e-06,\n",
      "          6.1047e-06],\n",
      "         [6.9497e-07, 8.8930e-07, 1.2089e-06, 6.0258e-07, 6.8746e-07,\n",
      "          8.4664e-07, 4.9764e-07, 3.8060e-06, 4.2762e-06, 2.3702e-06,\n",
      "          1.3773e-05, 2.3877e-05, 1.5818e-04, 1.5148e-04, 2.8286e-06,\n",
      "          8.6581e-04, 2.0622e-05, 1.5233e-04, 2.0904e-04, 4.8656e-07,\n",
      "          1.0051e-03, 7.0235e-06, 5.2877e-01, 4.6746e-01, 3.7459e-04,\n",
      "          3.8502e-04, 1.0349e-06, 9.6939e-06, 7.8781e-06, 3.7624e-06,\n",
      "          8.7474e-05, 6.7999e-07, 3.3622e-05, 2.0531e-05, 2.5571e-05,\n",
      "          4.6263e-05, 1.4313e-06, 2.4258e-05, 3.5566e-05, 2.5064e-05,\n",
      "          2.9574e-05, 4.7143e-06, 1.9550e-05, 7.8437e-06, 5.3327e-07,\n",
      "          9.7499e-07, 7.3257e-07, 6.7556e-07, 9.0284e-07, 4.4358e-07,\n",
      "          4.6095e-07],\n",
      "         [3.6290e-05, 5.0465e-05, 3.8954e-05, 4.0894e-05, 5.2672e-05,\n",
      "          6.1387e-05, 3.2663e-05, 4.3597e-04, 3.9977e-04, 7.2638e-05,\n",
      "          1.9775e-03, 8.0687e-04, 6.3239e-03, 1.5061e-02, 5.2190e-05,\n",
      "          2.0333e-03, 1.0226e-03, 1.5460e-02, 2.2887e-02, 6.0038e-05,\n",
      "          2.6819e-03, 2.7774e-04, 1.5648e-02, 9.8707e-03, 2.0015e-05,\n",
      "          7.0300e-01, 4.9088e-05, 9.2915e-03, 9.6658e-03, 1.0610e-04,\n",
      "          4.9482e-02, 3.0088e-05, 4.0156e-02, 2.7077e-02, 2.6157e-03,\n",
      "          2.0152e-03, 5.0290e-05, 3.5366e-02, 1.6169e-02, 2.7668e-03,\n",
      "          3.1459e-03, 8.7865e-05, 1.5498e-03, 1.7576e-03, 4.0927e-05,\n",
      "          3.4378e-05, 2.7864e-05, 2.3033e-05, 3.3717e-05, 2.6184e-05,\n",
      "          2.5923e-05]]])\n",
      "Player 1 Prediction: tensor([[0.9937, 0.0000, 0.0063, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.0694e-06, 5.1882e-07, 2.0075e-06, 1.8506e-06, 1.0083e-06,\n",
      "          2.5213e-06, 1.3221e-06, 1.4589e-03, 2.0845e-03, 1.1061e-05,\n",
      "          1.2875e-05, 1.5166e-05, 9.1943e-02, 9.4720e-02, 4.8321e-06,\n",
      "          6.4757e-07, 1.2576e-05, 1.3389e-02, 1.2943e-02, 3.0144e-06,\n",
      "          1.2085e-06, 1.4461e-05, 7.1280e-06, 6.0981e-06, 6.0855e-07,\n",
      "          3.3844e-01, 2.1215e-06, 2.7265e-05, 1.5704e-05, 2.0590e-05,\n",
      "          2.7417e-05, 2.2221e-06, 1.2017e-02, 1.3980e-02, 1.7122e-05,\n",
      "          6.8807e-07, 8.2316e-06, 2.2343e-01, 1.8517e-01, 2.0093e-05,\n",
      "          6.1466e-05, 2.4518e-05, 6.2410e-03, 3.8409e-03, 1.5872e-06,\n",
      "          3.1620e-06, 2.9612e-06, 2.9670e-06, 1.2106e-06, 1.4157e-06,\n",
      "          1.0594e-06],\n",
      "         [1.1478e-06, 4.9722e-07, 1.3040e-06, 3.1900e-06, 1.0981e-06,\n",
      "          3.0346e-06, 2.3388e-06, 9.6887e-03, 1.6282e-02, 5.0227e-06,\n",
      "          4.0180e-05, 5.5259e-05, 3.2783e-02, 2.5696e-02, 2.5753e-06,\n",
      "          5.3602e-06, 2.4206e-05, 3.1840e-04, 2.9190e-04, 2.7763e-06,\n",
      "          2.2978e-06, 9.6902e-06, 4.3820e-07, 2.0277e-06, 1.3537e-06,\n",
      "          4.7919e-01, 1.9021e-06, 7.1072e-05, 1.1357e-04, 2.2007e-05,\n",
      "          4.1413e-05, 2.2102e-06, 1.0813e-02, 1.0938e-02, 4.3893e-05,\n",
      "          3.6931e-06, 2.5231e-06, 1.4542e-01, 1.7135e-01, 8.8275e-05,\n",
      "          6.9764e-05, 1.2407e-05, 6.1309e-02, 3.5275e-02, 7.8590e-07,\n",
      "          2.0235e-06, 3.2411e-06, 1.6950e-06, 1.8823e-06, 2.1320e-06,\n",
      "          9.7513e-07],\n",
      "         [1.1231e-05, 4.0971e-06, 9.5800e-06, 1.5155e-05, 9.9508e-06,\n",
      "          1.8822e-05, 9.9777e-06, 4.5750e-04, 5.1825e-04, 1.8989e-05,\n",
      "          1.7983e-05, 4.4624e-05, 3.6497e-02, 2.3011e-02, 2.1580e-05,\n",
      "          2.6522e-04, 4.6680e-05, 4.1253e-01, 4.8877e-01, 8.0604e-06,\n",
      "          1.9259e-03, 4.3925e-05, 1.7143e-03, 1.1120e-02, 1.7628e-03,\n",
      "          2.5910e-03, 2.3756e-05, 5.9630e-05, 5.1790e-05, 4.1203e-05,\n",
      "          2.9700e-05, 1.1130e-05, 1.3926e-03, 1.4991e-03, 3.3780e-05,\n",
      "          2.9221e-06, 8.0574e-06, 3.5979e-03, 8.8059e-03, 3.0904e-05,\n",
      "          4.1517e-05, 3.3271e-05, 1.8368e-03, 9.5608e-04, 3.8528e-06,\n",
      "          2.3883e-05, 1.9420e-05, 3.1756e-05, 6.8351e-06, 1.0357e-05,\n",
      "          6.0686e-06],\n",
      "         [1.7942e-05, 9.9893e-06, 2.2924e-05, 6.2113e-05, 1.7093e-05,\n",
      "          4.6637e-05, 1.9096e-05, 2.0865e-02, 2.7457e-02, 3.8890e-05,\n",
      "          1.0572e-04, 1.1051e-04, 8.1023e-02, 1.0777e-01, 3.4753e-05,\n",
      "          2.4782e-05, 1.1836e-04, 2.1224e-02, 2.3245e-02, 3.2368e-05,\n",
      "          7.4382e-05, 1.2328e-04, 3.0379e-04, 3.5252e-04, 1.6782e-05,\n",
      "          2.0982e-01, 3.6397e-05, 1.1553e-03, 9.3995e-04, 9.2904e-05,\n",
      "          5.5159e-04, 2.5820e-05, 7.9527e-02, 7.6512e-02, 1.8245e-04,\n",
      "          1.0173e-05, 5.6686e-05, 1.3259e-01, 9.0706e-02, 1.4225e-04,\n",
      "          1.8645e-04, 6.3774e-05, 8.4354e-02, 3.9688e-02, 3.6278e-05,\n",
      "          5.6171e-05, 3.8837e-05, 3.7912e-05, 2.5042e-05, 4.1266e-05,\n",
      "          1.4531e-05]]])\n",
      "Player 1 Prediction: tensor([[0.2817, 0.7168, 0.0014, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28027\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5235/10000 (52.3%)\n",
      "    Average reward: +0.166\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4765/10000 (47.6%)\n",
      "    Average reward: -0.166\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4292 (30.4%)\n",
      "    Action 1: 5385 (38.1%)\n",
      "    Action 2: 4383 (31.0%)\n",
      "    Action 3: 61 (0.4%)\n",
      "  Player 1:\n",
      "    Action 0: 4172 (30.0%)\n",
      "    Action 1: 4988 (35.9%)\n",
      "    Action 2: 3562 (25.6%)\n",
      "    Action 3: 1184 (8.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1656.5, -1656.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.052\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1656\n",
      "   Testing specific player: 1\n",
      "   At training step: 30000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.9072e-01, 1.2600e-04, 9.1501e-03]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.9043, 0.0028, 0.0929]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 30000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39423\n",
      "Average episode length: 3.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6983/10000 (69.8%)\n",
      "    Average reward: -0.162\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3017/10000 (30.2%)\n",
      "    Average reward: +0.162\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14580 (75.0%)\n",
      "    Action 1: 4851 (25.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3630 (18.2%)\n",
      "    Action 1: 9569 (47.9%)\n",
      "    Action 2: 3787 (18.9%)\n",
      "    Action 3: 3006 (15.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1620.5, 1620.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.811 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.956 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.883\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1620\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31004/50000 [43:00<17:53, 17.69it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 31000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 188674/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 184889/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [44:55<29:25, 10.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 32000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 194995/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 191554/2000000\n",
      "P1 SL Buffer Size:  194995\n",
      "P1 SL buffer distribution [59823. 78026. 44828. 12318.]\n",
      "P1 actions distribution [0.30679248 0.40014359 0.22989307 0.06317085]\n",
      "P2 SL Buffer Size:  191554\n",
      "P2 SL buffer distribution [52705. 77817. 46149. 14883.]\n",
      "P2 actions distribution [0.27514435 0.40624054 0.24091901 0.07769611]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/50000 [45:12<29:25, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[3.5493e-05, 6.3733e-05, 7.2401e-05, 7.3476e-05, 5.5658e-05,\n",
      "          3.9884e-05, 4.0089e-05, 6.6016e-04, 2.2520e-03, 1.4549e-03,\n",
      "          1.0484e-02, 5.2327e-03, 3.3846e-03, 1.0809e-02, 9.6552e-04,\n",
      "          1.7343e-02, 5.3172e-03, 1.5323e-02, 2.3037e-02, 9.2249e-05,\n",
      "          2.6132e-02, 1.6696e-03, 3.3021e-01, 3.6648e-01, 2.8778e-04,\n",
      "          2.0727e-02, 5.0954e-05, 3.1889e-02, 2.7484e-02, 9.1040e-04,\n",
      "          3.6671e-02, 8.5496e-05, 1.5986e-02, 1.0955e-02, 3.0529e-03,\n",
      "          1.1019e-02, 1.1370e-04, 9.0967e-03, 3.3440e-03, 1.2376e-03,\n",
      "          2.5036e-03, 1.1643e-03, 1.3043e-03, 5.6720e-04, 5.3029e-05,\n",
      "          1.9932e-05, 6.5696e-05, 2.6250e-05, 3.7326e-05, 6.5358e-05,\n",
      "          5.7621e-05],\n",
      "         [2.0481e-05, 4.2166e-05, 3.7791e-05, 4.1560e-05, 3.2486e-05,\n",
      "          3.3439e-05, 2.0449e-05, 3.6536e-03, 2.3310e-02, 5.3075e-04,\n",
      "          1.2121e-02, 4.9436e-03, 2.2782e-02, 7.6878e-02, 1.0332e-04,\n",
      "          3.5092e-02, 4.5759e-03, 5.4653e-03, 1.1981e-02, 5.7700e-05,\n",
      "          2.6093e-01, 1.4981e-03, 3.0398e-04, 5.1537e-04, 3.2671e-04,\n",
      "          1.6379e-02, 6.7910e-05, 2.4459e-01, 2.1169e-01, 5.0174e-04,\n",
      "          1.3023e-02, 5.1049e-05, 2.0004e-03, 1.3576e-03, 2.4912e-03,\n",
      "          1.0009e-02, 2.9423e-05, 1.1774e-02, 2.7098e-03, 2.3141e-03,\n",
      "          3.8308e-03, 8.0487e-04, 8.9939e-03, 1.8807e-03, 3.1605e-05,\n",
      "          1.5308e-05, 3.9142e-05, 2.1517e-05, 2.3680e-05, 2.5244e-05,\n",
      "          4.4772e-05],\n",
      "         [1.9812e-07, 3.7249e-07, 3.3404e-07, 2.4747e-07, 2.5111e-07,\n",
      "          3.2996e-07, 2.3519e-07, 1.8577e-06, 2.7077e-06, 1.7718e-06,\n",
      "          3.0374e-06, 4.0113e-06, 4.9820e-05, 5.6393e-05, 6.4158e-07,\n",
      "          5.1524e-05, 2.5377e-06, 8.6454e-05, 9.7301e-05, 2.0046e-07,\n",
      "          2.0638e-04, 1.6912e-06, 6.7131e-03, 2.3351e-01, 7.5912e-01,\n",
      "          2.2799e-06, 6.4408e-07, 1.7842e-05, 1.4984e-05, 2.9946e-06,\n",
      "          4.9385e-06, 4.2339e-07, 5.3774e-06, 5.5970e-06, 2.8604e-06,\n",
      "          1.6763e-06, 3.1474e-07, 5.0276e-06, 7.5026e-06, 1.4443e-06,\n",
      "          1.8908e-06, 1.2909e-06, 2.4799e-06, 1.3858e-06, 5.2436e-07,\n",
      "          2.8480e-07, 3.0750e-07, 1.3805e-07, 1.5131e-07, 2.2768e-07,\n",
      "          7.7336e-07],\n",
      "         [3.1085e-05, 1.6633e-04, 1.0559e-04, 1.2003e-04, 1.0742e-04,\n",
      "          7.5013e-05, 3.7028e-05, 6.4650e-04, 1.6610e-03, 2.9275e-04,\n",
      "          7.7791e-04, 8.8819e-04, 4.4279e-02, 1.5278e-01, 2.7762e-04,\n",
      "          1.2895e-03, 9.3843e-04, 1.1398e-01, 1.8410e-01, 1.0769e-04,\n",
      "          3.0026e-03, 5.6793e-04, 4.6872e-02, 1.5968e-01, 1.7924e-03,\n",
      "          5.8080e-02, 9.9281e-05, 4.3233e-02, 2.8430e-02, 2.8081e-04,\n",
      "          9.9302e-04, 1.8240e-04, 5.6770e-02, 4.1474e-02, 8.1051e-04,\n",
      "          3.9363e-04, 5.8447e-05, 3.7008e-02, 1.5430e-02, 3.4072e-04,\n",
      "          3.0258e-04, 2.7548e-04, 5.3675e-04, 3.1111e-04, 1.0037e-04,\n",
      "          3.4183e-05, 1.0148e-04, 4.8956e-05, 4.2219e-05, 4.2004e-05,\n",
      "          4.6110e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29337\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5639/10000 (56.4%)\n",
      "    Average reward: -0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4361/10000 (43.6%)\n",
      "    Average reward: +0.151\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5143 (35.3%)\n",
      "    Action 1: 5114 (35.1%)\n",
      "    Action 2: 3330 (22.9%)\n",
      "    Action 3: 979 (6.7%)\n",
      "  Player 1:\n",
      "    Action 0: 4147 (28.1%)\n",
      "    Action 1: 5991 (40.6%)\n",
      "    Action 2: 4545 (30.8%)\n",
      "    Action 3: 88 (0.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1513.5, 1513.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.043 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.052\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1514\n",
      "   Testing specific player: 0\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.9040e-01, 3.2297e-05, 9.5699e-03]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.8862, 0.0010, 0.1128]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42032\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4349/10000 (43.5%)\n",
      "    Average reward: +0.118\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5651/10000 (56.5%)\n",
      "    Average reward: -0.118\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4308 (20.3%)\n",
      "    Action 1: 10392 (48.9%)\n",
      "    Action 2: 3670 (17.3%)\n",
      "    Action 3: 2862 (13.5%)\n",
      "  Player 1:\n",
      "    Action 0: 15360 (73.8%)\n",
      "    Action 1: 5440 (26.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1182.0, -1182.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.971 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.829 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.900\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1182\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0641, 0.0198, 0.9161, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.4715e-05, 3.2181e-05, 1.2418e-05, 2.0216e-05, 2.0477e-05,\n",
      "          2.0588e-05, 1.6628e-05, 8.4302e-05, 1.6724e-04, 7.4326e-04,\n",
      "          3.0125e-02, 5.8481e-03, 3.3268e-04, 6.4606e-04, 6.0274e-04,\n",
      "          6.7077e-02, 1.4485e-02, 4.8087e-02, 5.3524e-02, 4.4689e-05,\n",
      "          4.2679e-02, 2.6894e-03, 4.0424e-01, 1.3168e-01, 4.4969e-05,\n",
      "          2.1762e-02, 2.3285e-05, 2.3353e-02, 2.5689e-02, 1.1267e-03,\n",
      "          4.4339e-02, 7.6980e-05, 2.0756e-02, 1.5587e-02, 4.6873e-03,\n",
      "          2.4096e-02, 2.3019e-04, 4.0413e-04, 2.7240e-04, 2.7494e-03,\n",
      "          1.0386e-02, 7.8719e-04, 1.9854e-04, 5.4083e-05, 3.7824e-05,\n",
      "          1.6033e-05, 2.1062e-05, 1.9753e-05, 3.7773e-05, 1.7219e-05,\n",
      "          2.7289e-05],\n",
      "         [1.6859e-05, 1.2913e-05, 1.4078e-05, 1.8812e-05, 4.1571e-05,\n",
      "          1.4077e-05, 1.6699e-05, 1.5484e-03, 5.8041e-03, 1.4485e-04,\n",
      "          4.6543e-03, 1.4738e-03, 3.3934e-02, 7.0058e-02, 6.9077e-05,\n",
      "          2.5904e-02, 4.5124e-03, 2.4390e-02, 3.2504e-02, 2.1511e-05,\n",
      "          2.3860e-01, 1.9597e-03, 9.6751e-04, 5.7975e-04, 5.3022e-05,\n",
      "          1.5710e-02, 6.2877e-05, 2.1537e-01, 2.3419e-01, 7.6785e-04,\n",
      "          1.7647e-02, 2.7329e-05, 9.3836e-03, 7.7610e-03, 1.5060e-03,\n",
      "          1.1898e-02, 2.5967e-05, 1.8688e-02, 6.0271e-03, 1.5987e-03,\n",
      "          3.2363e-03, 1.9873e-04, 7.7748e-03, 6.8525e-04, 9.9302e-06,\n",
      "          2.1392e-05, 9.9238e-06, 1.7428e-05, 1.7774e-05, 3.7772e-05,\n",
      "          1.4756e-05],\n",
      "         [1.6589e-07, 1.0365e-07, 8.7517e-08, 4.1837e-08, 7.7199e-08,\n",
      "          1.5795e-07, 1.3326e-07, 5.2219e-07, 3.4851e-07, 1.0371e-06,\n",
      "          3.9273e-06, 1.8763e-06, 1.0247e-05, 1.5035e-05, 6.5047e-07,\n",
      "          8.6602e-05, 3.1156e-06, 6.0742e-05, 6.3147e-05, 1.1924e-07,\n",
      "          7.5226e-05, 1.0459e-06, 3.4284e-01, 6.2393e-01, 3.2845e-02,\n",
      "          1.5215e-06, 1.1714e-07, 2.2133e-05, 1.4018e-05, 6.6383e-07,\n",
      "          1.4764e-06, 1.4360e-07, 5.0873e-06, 4.2546e-06, 1.3571e-06,\n",
      "          1.1985e-06, 1.5929e-07, 1.1443e-06, 1.5684e-06, 1.9432e-06,\n",
      "          1.5948e-06, 7.0645e-07, 3.7055e-07, 2.3783e-07, 1.6259e-07,\n",
      "          1.0577e-07, 1.4384e-07, 1.5987e-07, 1.5662e-07, 6.3551e-08,\n",
      "          4.2342e-08],\n",
      "         [1.8377e-05, 2.6032e-05, 4.9056e-05, 1.2020e-05, 4.2347e-05,\n",
      "          1.5108e-05, 5.5935e-05, 1.6250e-04, 1.0754e-04, 1.1114e-04,\n",
      "          3.6151e-04, 2.7110e-04, 7.7444e-03, 2.3011e-02, 7.2426e-05,\n",
      "          3.8156e-04, 3.4032e-04, 9.3777e-02, 1.4505e-01, 4.0103e-05,\n",
      "          1.7664e-03, 1.3234e-04, 2.2332e-01, 2.7237e-01, 2.9041e-04,\n",
      "          5.2348e-02, 2.6360e-05, 5.4960e-02, 5.2933e-02, 1.9740e-04,\n",
      "          3.5623e-04, 2.2873e-05, 3.5024e-02, 1.8703e-02, 3.6251e-04,\n",
      "          1.1760e-04, 5.2164e-05, 1.1320e-02, 3.0426e-03, 1.6399e-04,\n",
      "          3.5221e-04, 1.5926e-04, 1.1838e-04, 7.3701e-05, 3.3387e-05,\n",
      "          1.1826e-05, 1.0708e-05, 3.6273e-05, 1.4215e-05, 2.9807e-05,\n",
      "          8.3190e-06]]])\n",
      "Player 1 Prediction: tensor([[0.0785, 0.0433, 0.8783, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28857\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5415/10000 (54.1%)\n",
      "    Average reward: +0.138\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4585/10000 (45.9%)\n",
      "    Average reward: -0.138\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4384 (30.6%)\n",
      "    Action 1: 5384 (37.6%)\n",
      "    Action 2: 4090 (28.5%)\n",
      "    Action 3: 476 (3.3%)\n",
      "  Player 1:\n",
      "    Action 0: 4400 (30.3%)\n",
      "    Action 1: 5162 (35.5%)\n",
      "    Action 2: 3730 (25.7%)\n",
      "    Action 3: 1231 (8.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1384.5, -1384.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.053\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1384\n",
      "   Testing specific player: 1\n",
      "   At training step: 32000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.7489, 0.2250, 0.0261, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.1697, 0.1098, 0.7205, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 32000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39575\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6928/10000 (69.3%)\n",
      "    Average reward: -0.178\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3072/10000 (30.7%)\n",
      "    Average reward: +0.178\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14513 (74.4%)\n",
      "    Action 1: 4981 (25.6%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3814 (19.0%)\n",
      "    Action 1: 9435 (47.0%)\n",
      "    Action 2: 3811 (19.0%)\n",
      "    Action 3: 3021 (15.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1779.0, 1779.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.820 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.967 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.894\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1779\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33002/50000 [47:14<20:03, 14.12it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 33000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 201398/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 198125/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [48:21<14:47, 18.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 34000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 207694/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 204827/2000000\n",
      "P1 SL Buffer Size:  207694\n",
      "P1 SL buffer distribution [63941. 82549. 48621. 12583.]\n",
      "P1 actions distribution [0.30786157 0.39745491 0.2340992  0.06058432]\n",
      "P2 SL Buffer Size:  204827\n",
      "P2 SL buffer distribution [56125. 83062. 49886. 15754.]\n",
      "P2 actions distribution [0.27401173 0.40552271 0.24355188 0.07691369]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34000/50000 [48:33<14:47, 18.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.0500, 0.0179, 0.9321, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30302\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5843/10000 (58.4%)\n",
      "    Average reward: -0.195\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4157/10000 (41.6%)\n",
      "    Average reward: +0.195\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5662 (37.4%)\n",
      "    Action 1: 5573 (36.8%)\n",
      "    Action 2: 2707 (17.9%)\n",
      "    Action 3: 1198 (7.9%)\n",
      "  Player 1:\n",
      "    Action 0: 4044 (26.7%)\n",
      "    Action 1: 5362 (35.4%)\n",
      "    Action 2: 4211 (27.8%)\n",
      "    Action 3: 1545 (10.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1948.0, 1948.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.061 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.039 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.050\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.1948\n",
      "   Testing specific player: 0\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.7638, 0.2153, 0.0208, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1727, 0.0795, 0.7478, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7116, 0.0010, 0.2874]])\n",
      "Player 0 Prediction: tensor([[0.8589, 0.1402, 0.0009, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42061\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4347/10000 (43.5%)\n",
      "    Average reward: +0.135\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5653/10000 (56.5%)\n",
      "    Average reward: -0.135\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4166 (19.7%)\n",
      "    Action 1: 10647 (50.4%)\n",
      "    Action 2: 3614 (17.1%)\n",
      "    Action 3: 2697 (12.8%)\n",
      "  Player 1:\n",
      "    Action 0: 15647 (74.7%)\n",
      "    Action 1: 5290 (25.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1345.5, -1345.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.960 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.815 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.888\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1346\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[6.5486e-05, 3.1020e-05, 4.7186e-05, 4.6754e-05, 1.0584e-04,\n",
      "          8.3341e-05, 2.3570e-05, 1.6826e-03, 2.9364e-03, 1.7196e-03,\n",
      "          3.6057e-03, 1.6187e-03, 6.7253e-03, 2.5935e-02, 2.0743e-03,\n",
      "          3.2806e-03, 1.3024e-03, 9.9661e-03, 2.2772e-02, 7.1434e-05,\n",
      "          8.8622e-04, 3.5039e-04, 1.8880e-02, 2.1351e-02, 1.6691e-04,\n",
      "          1.8171e-01, 9.2430e-05, 1.1062e-01, 1.4393e-01, 2.0015e-03,\n",
      "          7.4592e-02, 6.4885e-05, 8.3086e-02, 6.0771e-02, 2.4433e-03,\n",
      "          1.1458e-02, 5.4279e-04, 1.2327e-01, 4.2684e-02, 2.1977e-03,\n",
      "          6.2072e-03, 2.6821e-03, 1.5591e-02, 9.9370e-03, 9.1643e-05,\n",
      "          4.0802e-05, 9.7601e-05, 4.3905e-05, 1.8412e-05, 7.0336e-05,\n",
      "          3.6058e-05],\n",
      "         [7.8609e-06, 3.6948e-05, 8.2912e-06, 1.7433e-05, 1.1690e-05,\n",
      "          1.3402e-05, 2.0264e-05, 1.0784e-03, 5.4207e-03, 1.6166e-04,\n",
      "          2.5597e-03, 3.5442e-03, 8.1446e-03, 1.6540e-02, 6.6199e-05,\n",
      "          1.7495e-02, 2.1954e-03, 5.8189e-04, 7.1653e-04, 4.7861e-05,\n",
      "          5.9770e-03, 3.8800e-04, 3.7658e-05, 4.6354e-05, 1.0860e-04,\n",
      "          1.6759e-01, 1.0774e-05, 3.2637e-01, 3.0726e-01, 1.7968e-04,\n",
      "          1.6616e-02, 3.5727e-05, 1.2253e-03, 1.2430e-03, 5.1464e-03,\n",
      "          3.8404e-02, 5.6845e-05, 2.4611e-02, 6.6641e-03, 9.1341e-03,\n",
      "          1.0972e-02, 5.5882e-04, 1.5564e-02, 3.0227e-03, 6.3896e-06,\n",
      "          7.9651e-06, 1.2229e-05, 9.2068e-06, 1.3772e-05, 1.8915e-05,\n",
      "          4.2497e-05],\n",
      "         [4.3741e-07, 1.3049e-06, 9.1716e-07, 2.5658e-07, 1.0992e-06,\n",
      "          5.3481e-07, 2.9575e-07, 4.2822e-06, 6.1012e-06, 3.1952e-06,\n",
      "          2.9711e-06, 1.3141e-05, 1.0290e-04, 4.0639e-04, 1.2194e-06,\n",
      "          1.9725e-04, 4.0852e-06, 3.2576e-04, 1.7090e-04, 4.9185e-07,\n",
      "          5.1642e-04, 4.9715e-06, 1.0341e-03, 1.2743e-01, 8.6938e-01,\n",
      "          3.6400e-05, 4.3467e-07, 6.9059e-05, 4.8015e-05, 4.0645e-06,\n",
      "          1.0739e-05, 1.3305e-06, 2.2887e-05, 2.6517e-05, 5.9134e-06,\n",
      "          6.1930e-06, 1.2145e-06, 3.1582e-05, 4.9408e-05, 1.6639e-05,\n",
      "          1.7165e-05, 4.7760e-06, 1.7399e-05, 6.0165e-06, 3.3856e-07,\n",
      "          8.4171e-07, 1.2590e-06, 2.2553e-06, 2.0144e-06, 2.3231e-07,\n",
      "          3.6263e-07],\n",
      "         [6.2114e-05, 1.0403e-04, 8.9272e-05, 9.7129e-05, 7.9535e-05,\n",
      "          4.6267e-05, 3.9642e-05, 5.2381e-04, 1.4601e-03, 2.8820e-04,\n",
      "          8.2320e-04, 4.3153e-04, 4.3555e-02, 6.9302e-02, 2.1772e-04,\n",
      "          9.0986e-04, 1.9577e-03, 5.2625e-02, 5.9360e-02, 2.1392e-04,\n",
      "          6.9127e-04, 6.1538e-04, 5.7096e-03, 2.3876e-02, 2.7616e-03,\n",
      "          1.9111e-01, 7.5140e-05, 1.2516e-01, 8.9893e-02, 6.5980e-04,\n",
      "          1.5956e-02, 5.8067e-05, 1.4950e-01, 5.6053e-02, 2.5038e-03,\n",
      "          1.7576e-03, 3.3661e-04, 6.9651e-02, 2.1975e-02, 3.4158e-04,\n",
      "          1.5080e-03, 3.6351e-04, 4.8622e-03, 1.8708e-03, 9.1003e-05,\n",
      "          5.1372e-05, 6.2356e-05, 1.3012e-04, 7.9450e-05, 7.5849e-05,\n",
      "          3.9044e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 9.7830e-01, 1.0686e-04, 2.1593e-02]])\n",
      "Player 0 Prediction: tensor([[[1.0172e-05, 2.2118e-06, 8.5113e-06, 9.4765e-06, 6.6861e-06,\n",
      "          1.4702e-05, 3.8535e-06, 1.3376e-04, 7.7749e-05, 4.0947e-05,\n",
      "          3.8832e-02, 3.7540e-03, 1.0867e-04, 1.5689e-04, 9.3748e-05,\n",
      "          3.0103e-02, 3.1267e-03, 3.4602e-04, 7.4791e-04, 5.2388e-06,\n",
      "          7.5018e-03, 3.5939e-04, 2.3221e-04, 1.4761e-04, 4.3754e-07,\n",
      "          5.0113e-01, 8.7704e-06, 1.5916e-04, 2.7937e-04, 3.2191e-04,\n",
      "          1.6178e-01, 4.6318e-06, 1.9950e-03, 1.2270e-03, 4.3933e-03,\n",
      "          1.3516e-01, 2.3760e-05, 1.2476e-03, 1.1703e-03, 7.9872e-03,\n",
      "          9.6109e-02, 7.2982e-05, 3.7930e-04, 6.8432e-04, 1.3294e-05,\n",
      "          8.4127e-06, 6.3414e-06, 1.8005e-06, 1.7039e-06, 7.8359e-06,\n",
      "          2.6267e-06],\n",
      "         [4.7997e-06, 1.1495e-05, 6.2432e-06, 1.1971e-05, 2.7296e-06,\n",
      "          7.3028e-06, 2.3567e-05, 3.1690e-03, 1.4260e-02, 3.8329e-05,\n",
      "          1.6281e-04, 2.5202e-03, 2.1263e-02, 3.2777e-02, 1.6115e-05,\n",
      "          2.9178e-03, 5.8499e-04, 3.8827e-03, 5.5226e-03, 2.6542e-05,\n",
      "          3.4028e-04, 1.8150e-04, 2.7369e-05, 1.2843e-05, 1.6708e-06,\n",
      "          4.6586e-01, 2.2773e-06, 2.0922e-03, 2.0299e-03, 4.4064e-05,\n",
      "          2.1061e-01, 3.4742e-05, 7.7752e-03, 7.0418e-03, 1.1308e-03,\n",
      "          4.7183e-03, 1.9389e-05, 1.0367e-01, 4.8394e-02, 5.6055e-03,\n",
      "          1.5259e-03, 1.4476e-04, 4.3494e-02, 7.9931e-03, 2.1170e-06,\n",
      "          3.7489e-06, 2.4200e-06, 2.2567e-06, 4.9077e-06, 7.6327e-06,\n",
      "          1.8366e-05],\n",
      "         [4.1153e-07, 1.2697e-06, 1.5295e-06, 1.6887e-07, 1.6220e-06,\n",
      "          4.3845e-07, 3.2613e-07, 5.1652e-06, 6.6060e-06, 1.2986e-06,\n",
      "          8.6440e-06, 2.8100e-05, 1.2640e-04, 2.1552e-04, 8.3825e-07,\n",
      "          1.1690e-03, 8.2164e-06, 2.7697e-04, 1.8181e-04, 2.9279e-07,\n",
      "          2.2266e-03, 1.4465e-05, 3.8463e-01, 6.1004e-01, 3.3409e-04,\n",
      "          3.1363e-04, 6.6496e-07, 6.6746e-06, 4.8481e-06, 6.4281e-06,\n",
      "          7.4574e-05, 8.7900e-07, 2.7703e-05, 2.5338e-05, 1.3516e-05,\n",
      "          2.8229e-05, 5.8753e-07, 1.8216e-05, 4.7055e-05, 4.6644e-05,\n",
      "          5.3966e-05, 2.3937e-06, 2.0626e-05, 1.7850e-05, 2.8266e-07,\n",
      "          1.0102e-06, 5.8433e-07, 1.6044e-06, 2.6283e-06, 1.8334e-07,\n",
      "          2.5009e-07],\n",
      "         [3.0755e-05, 2.7824e-05, 3.1391e-05, 5.7240e-05, 1.5973e-05,\n",
      "          3.3234e-05, 1.3236e-05, 2.5430e-04, 3.4847e-04, 5.3167e-05,\n",
      "          2.9466e-03, 3.9225e-04, 7.6264e-03, 1.1431e-02, 5.4743e-05,\n",
      "          2.0431e-03, 2.7729e-03, 1.6003e-02, 1.9920e-02, 8.2641e-05,\n",
      "          1.3577e-03, 3.2576e-04, 7.7212e-03, 7.4923e-03, 3.6771e-05,\n",
      "          6.5051e-01, 3.5158e-05, 7.1203e-03, 5.1976e-03, 1.5826e-04,\n",
      "          1.1641e-01, 1.3960e-05, 4.5155e-02, 2.6056e-02, 4.2764e-03,\n",
      "          2.5827e-03, 9.3687e-05, 4.5056e-02, 9.0370e-03, 9.9933e-04,\n",
      "          3.5960e-03, 4.9495e-05, 1.5545e-03, 8.2977e-04, 4.1872e-05,\n",
      "          3.6528e-05, 2.5164e-05, 2.8497e-05, 2.4909e-05, 2.7271e-05,\n",
      "          1.5760e-05]]])\n",
      "Player 1 Prediction: tensor([[0.9903, 0.0000, 0.0097, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.5441e-06, 7.1127e-07, 1.6922e-06, 7.0178e-07, 1.9399e-06,\n",
      "          1.5686e-06, 6.7604e-07, 1.1276e-03, 1.5282e-03, 2.1198e-05,\n",
      "          1.1568e-05, 1.0529e-05, 1.0147e-01, 1.1208e-01, 5.8282e-06,\n",
      "          4.4965e-07, 9.4162e-06, 1.4200e-02, 1.2087e-02, 1.5264e-06,\n",
      "          1.0170e-06, 7.9714e-06, 5.2430e-06, 6.5892e-06, 3.6439e-07,\n",
      "          3.1671e-01, 1.2780e-06, 2.0528e-05, 1.2064e-05, 1.0582e-05,\n",
      "          2.6006e-05, 1.1216e-06, 1.0001e-02, 1.4559e-02, 1.6075e-05,\n",
      "          3.2075e-07, 1.1383e-05, 2.0880e-01, 1.9632e-01, 1.0711e-05,\n",
      "          5.0571e-05, 1.1150e-05, 6.4675e-03, 4.3732e-03, 3.9260e-06,\n",
      "          1.0087e-06, 2.8454e-06, 1.7237e-06, 1.2278e-06, 1.1853e-06,\n",
      "          8.7555e-07],\n",
      "         [1.1460e-06, 1.3324e-06, 1.0367e-06, 1.2134e-06, 1.5630e-06,\n",
      "          1.5566e-06, 1.6378e-06, 7.9644e-03, 1.4371e-02, 1.0776e-05,\n",
      "          4.3092e-05, 4.9077e-05, 2.8908e-02, 1.9636e-02, 2.6622e-06,\n",
      "          4.6304e-06, 2.6222e-05, 3.3550e-04, 2.1800e-04, 2.4411e-06,\n",
      "          2.4206e-06, 8.2226e-06, 4.4713e-07, 2.1174e-06, 1.0311e-06,\n",
      "          5.5759e-01, 6.9559e-07, 4.8356e-05, 6.9264e-05, 6.9768e-06,\n",
      "          3.2930e-05, 1.6500e-06, 8.2493e-03, 1.0638e-02, 5.3012e-05,\n",
      "          2.0245e-06, 4.0384e-06, 1.2144e-01, 1.4374e-01, 5.9069e-05,\n",
      "          3.8742e-05, 7.3037e-06, 5.6428e-02, 2.9993e-02, 1.2312e-06,\n",
      "          6.2015e-07, 2.2293e-06, 1.0841e-06, 2.4627e-06, 1.5761e-06,\n",
      "          1.0905e-06],\n",
      "         [9.7128e-06, 9.1721e-06, 8.6202e-06, 4.0831e-06, 1.8722e-05,\n",
      "          7.8230e-06, 4.4103e-06, 3.7264e-04, 4.9595e-04, 3.2178e-05,\n",
      "          1.6115e-05, 4.1911e-05, 3.2446e-02, 2.4302e-02, 1.6256e-05,\n",
      "          2.4949e-04, 3.6925e-05, 5.2961e-01, 3.7534e-01, 4.4476e-06,\n",
      "          1.9447e-03, 4.5270e-05, 1.1954e-03, 1.3078e-02, 1.4885e-03,\n",
      "          2.5845e-03, 1.0369e-05, 3.7438e-05, 2.7072e-05, 1.8678e-05,\n",
      "          2.0033e-05, 7.4906e-06, 9.9542e-04, 1.6388e-03, 3.4952e-05,\n",
      "          1.4401e-06, 1.0986e-05, 3.2576e-03, 7.6061e-03, 2.3090e-05,\n",
      "          2.9598e-05, 1.3720e-05, 1.8768e-03, 9.5010e-04, 5.5608e-06,\n",
      "          9.5554e-06, 1.4641e-05, 3.0585e-05, 1.4512e-05, 5.7975e-06,\n",
      "          5.0698e-06],\n",
      "         [2.1392e-05, 1.8080e-05, 1.8204e-05, 2.5882e-05, 2.4344e-05,\n",
      "          2.0081e-05, 8.5904e-06, 1.4774e-02, 2.3085e-02, 7.8929e-05,\n",
      "          1.3369e-04, 8.4481e-05, 7.9300e-02, 9.7878e-02, 3.4495e-05,\n",
      "          2.3878e-05, 1.5306e-04, 2.5806e-02, 2.1658e-02, 3.0382e-05,\n",
      "          5.7892e-05, 1.0530e-04, 2.9019e-04, 4.5272e-04, 1.7359e-05,\n",
      "          2.8924e-01, 1.6827e-05, 8.3554e-04, 5.4446e-04, 4.3338e-05,\n",
      "          4.9002e-04, 1.3247e-05, 6.1543e-02, 7.1216e-02, 2.4587e-04,\n",
      "          7.1034e-06, 1.1403e-04, 1.1011e-01, 7.5906e-02, 7.2239e-05,\n",
      "          1.3466e-04, 2.8074e-05, 9.2238e-02, 3.2870e-02, 7.5448e-05,\n",
      "          2.1806e-05, 2.9396e-05, 3.0307e-05, 3.4917e-05, 3.0448e-05,\n",
      "          1.1505e-05]]])\n",
      "Player 1 Prediction: tensor([[0.2832, 0.7150, 0.0018, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.0216e-05, 6.1030e-06, 1.1507e-05, 8.8197e-06, 1.3377e-05,\n",
      "          1.6580e-05, 4.6266e-06, 3.2697e-02, 2.4543e-02, 5.4093e-05,\n",
      "          3.1331e-04, 5.2760e-05, 1.8781e-03, 2.0132e-03, 2.8370e-05,\n",
      "          1.3183e-04, 3.2849e-05, 3.7344e-04, 4.8939e-04, 1.0423e-05,\n",
      "          1.1547e-05, 2.6880e-05, 1.0525e-05, 1.5241e-05, 3.2288e-06,\n",
      "          7.4286e-01, 1.9296e-05, 5.1851e-05, 4.3904e-05, 6.1648e-05,\n",
      "          4.0597e-04, 7.9606e-06, 9.9602e-04, 7.9241e-04, 8.0862e-05,\n",
      "          1.3053e-04, 3.4487e-05, 2.0590e-03, 2.5802e-03, 1.5865e-04,\n",
      "          1.3295e-03, 6.0786e-05, 7.7734e-02, 1.0774e-01, 2.1696e-05,\n",
      "          6.3494e-06, 2.7946e-05, 6.6303e-06, 4.6889e-06, 1.6925e-05,\n",
      "          6.7807e-06],\n",
      "         [1.5306e-05, 4.0214e-05, 1.4379e-05, 2.5837e-05, 1.3440e-05,\n",
      "          1.8534e-05, 3.5047e-05, 5.0479e-03, 3.8255e-03, 9.6003e-05,\n",
      "          2.0368e-03, 9.8884e-04, 5.7370e-03, 3.4445e-03, 2.8569e-05,\n",
      "          1.4986e-03, 2.4451e-04, 5.5596e-04, 5.0265e-04, 6.5643e-05,\n",
      "          4.5107e-05, 9.4570e-05, 6.0938e-06, 1.9563e-05, 1.3160e-05,\n",
      "          8.9485e-01, 1.3709e-05, 5.4004e-04, 4.2779e-04, 4.4931e-05,\n",
      "          2.4172e-03, 3.6765e-05, 1.6043e-03, 2.1126e-03, 7.1720e-04,\n",
      "          2.4474e-03, 5.2709e-05, 5.4242e-03, 1.1726e-02, 4.5122e-03,\n",
      "          6.8127e-03, 1.1116e-04, 1.9455e-02, 2.2111e-02, 1.2774e-05,\n",
      "          1.2201e-05, 3.4987e-05, 1.2121e-05, 2.6227e-05, 2.9521e-05,\n",
      "          3.8270e-05],\n",
      "         [3.5884e-04, 4.2083e-04, 3.4781e-04, 1.1940e-04, 5.6075e-04,\n",
      "          2.3763e-04, 1.2710e-04, 9.8783e-03, 9.1989e-03, 7.8386e-04,\n",
      "          1.6010e-03, 2.4753e-03, 2.6496e-01, 2.3114e-01, 3.1350e-04,\n",
      "          4.0592e-02, 8.5271e-04, 5.1021e-02, 3.4322e-02, 1.9323e-04,\n",
      "          9.5368e-02, 1.7393e-03, 6.9991e-03, 6.2369e-02, 2.5590e-02,\n",
      "          5.9219e-02, 4.1053e-04, 1.1014e-03, 7.3611e-04, 9.7326e-04,\n",
      "          1.9633e-03, 4.5621e-04, 2.5660e-03, 8.4458e-03, 1.2890e-03,\n",
      "          1.2598e-03, 4.2855e-04, 3.3718e-03, 1.2559e-02, 2.7100e-03,\n",
      "          8.1009e-03, 3.8564e-04, 1.9094e-02, 3.0143e-02, 2.1894e-04,\n",
      "          3.8803e-04, 6.7110e-04, 6.6982e-04, 6.8458e-04, 3.3756e-04,\n",
      "          2.5536e-04],\n",
      "         [2.5399e-04, 2.0350e-04, 1.6319e-04, 4.3613e-04, 1.6429e-04,\n",
      "          1.8931e-04, 6.2828e-05, 1.7695e-02, 1.5070e-02, 4.1571e-04,\n",
      "          7.9012e-03, 8.4186e-04, 3.7439e-02, 3.3756e-02, 2.9834e-04,\n",
      "          2.7997e-03, 1.8063e-03, 9.2092e-03, 8.1401e-03, 4.0279e-04,\n",
      "          1.7818e-03, 9.2612e-04, 1.1159e-03, 1.5058e-03, 3.1066e-04,\n",
      "          5.5730e-01, 3.2353e-04, 2.7767e-03, 2.6084e-03, 3.7491e-04,\n",
      "          2.2843e-02, 1.8113e-04, 2.6626e-02, 3.0144e-02, 3.4556e-03,\n",
      "          2.1121e-03, 7.1927e-04, 1.5278e-02, 1.4631e-02, 2.0600e-03,\n",
      "          1.4052e-02, 2.3131e-04, 8.8137e-02, 7.1094e-02, 4.8105e-04,\n",
      "          2.4317e-04, 4.3677e-04, 2.0009e-04, 2.2033e-04, 3.4722e-04,\n",
      "          2.3498e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28155\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5339/10000 (53.4%)\n",
      "    Average reward: +0.112\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4661/10000 (46.6%)\n",
      "    Average reward: -0.112\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4216 (29.9%)\n",
      "    Action 1: 5237 (37.2%)\n",
      "    Action 2: 4310 (30.6%)\n",
      "    Action 3: 323 (2.3%)\n",
      "  Player 1:\n",
      "    Action 0: 4141 (29.4%)\n",
      "    Action 1: 4959 (35.2%)\n",
      "    Action 2: 3704 (26.3%)\n",
      "    Action 3: 1265 (9.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1117.5, -1117.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.052 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.051\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1118\n",
      "   Testing specific player: 1\n",
      "   At training step: 34000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5456, 0.0092, 0.4453]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3169, 0.0408, 0.6423]])\n",
      "Player 1 Prediction: tensor([[0.6059, 0.2614, 0.1327, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 34000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39803\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 7012/10000 (70.1%)\n",
      "    Average reward: -0.118\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 2988/10000 (29.9%)\n",
      "    Average reward: +0.118\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14332 (73.3%)\n",
      "    Action 1: 5211 (26.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 4043 (20.0%)\n",
      "    Action 1: 9318 (46.0%)\n",
      "    Action 2: 3912 (19.3%)\n",
      "    Action 3: 2987 (14.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1179.0, 1179.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.837 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.979 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.908\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1179\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35004/50000 [49:41<11:32, 21.66it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 35000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 214004/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 211437/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [51:25<22:54, 10.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 36000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 220422/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 217904/2000000\n",
      "P1 SL Buffer Size:  220422\n",
      "P1 SL buffer distribution [68303. 86670. 52415. 13034.]\n",
      "P1 actions distribution [0.30987379 0.39320032 0.23779387 0.05913203]\n",
      "P2 SL Buffer Size:  217904\n",
      "P2 SL buffer distribution [59080. 88608. 53406. 16810.]\n",
      "P2 actions distribution [0.27112857 0.40663779 0.24508958 0.07714406]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[5.7609e-01, 4.2345e-01, 4.6166e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[7.9092e-05, 1.8598e-05, 5.0803e-05, 1.9160e-05, 3.8035e-05,\n",
      "          1.9188e-05, 1.0352e-05, 2.1332e-04, 5.1790e-04, 5.1147e-04,\n",
      "          8.4276e-02, 1.4030e-02, 6.2507e-04, 5.9251e-04, 1.4507e-04,\n",
      "          1.1335e-01, 9.9330e-03, 1.1038e-02, 1.4366e-02, 2.0649e-05,\n",
      "          6.2141e-02, 1.0478e-03, 2.4269e-01, 8.8333e-02, 3.3249e-05,\n",
      "          1.0668e-01, 1.8718e-05, 1.5915e-02, 1.5555e-02, 1.5991e-03,\n",
      "          7.4790e-02, 2.7178e-05, 2.6487e-02, 1.5179e-02, 1.0090e-02,\n",
      "          7.6698e-02, 1.0094e-04, 5.6993e-04, 5.5273e-04, 3.9989e-03,\n",
      "          6.5650e-03, 5.6575e-04, 3.0633e-04, 1.0508e-04, 1.6161e-05,\n",
      "          9.2529e-06, 1.5104e-05, 1.2455e-05, 1.1087e-05, 1.8400e-05,\n",
      "          1.0950e-05],\n",
      "         [1.4452e-05, 7.8129e-06, 1.2898e-05, 2.1676e-05, 1.0226e-05,\n",
      "          1.2427e-05, 3.1283e-05, 7.0846e-03, 3.5522e-02, 2.8053e-04,\n",
      "          7.1579e-03, 2.9209e-03, 2.6842e-02, 6.8812e-02, 6.1822e-05,\n",
      "          1.4950e-02, 1.9136e-03, 1.2908e-02, 1.5674e-02, 2.0595e-05,\n",
      "          1.7967e-01, 4.9291e-03, 3.8691e-04, 3.2840e-04, 4.3887e-05,\n",
      "          1.1639e-01, 2.3035e-05, 2.1145e-01, 1.7853e-01, 1.2001e-03,\n",
      "          2.4134e-02, 1.7139e-05, 1.6207e-02, 1.5346e-02, 1.9192e-03,\n",
      "          4.7499e-03, 6.5152e-05, 2.2368e-02, 1.2755e-02, 1.1192e-03,\n",
      "          1.9904e-03, 6.0812e-04, 1.0117e-02, 1.2240e-03, 4.6634e-05,\n",
      "          8.2147e-06, 6.7748e-06, 1.0040e-05, 4.2901e-05, 2.5100e-05,\n",
      "          3.3692e-05],\n",
      "         [7.8740e-08, 3.8165e-08, 2.3930e-07, 3.3921e-08, 1.6183e-07,\n",
      "          1.5377e-07, 3.8896e-08, 6.0870e-07, 9.2387e-07, 1.4330e-06,\n",
      "          2.4939e-06, 1.2660e-06, 2.0025e-05, 3.7246e-05, 1.4931e-07,\n",
      "          6.6459e-05, 1.5495e-06, 5.5944e-05, 3.9684e-05, 3.6152e-08,\n",
      "          8.2966e-05, 5.2130e-07, 4.6651e-01, 5.2683e-01, 6.3099e-03,\n",
      "          5.8200e-06, 2.8614e-07, 5.7304e-06, 3.1898e-06, 7.0710e-07,\n",
      "          1.0467e-06, 3.3633e-07, 6.0226e-06, 6.7142e-06, 8.3608e-07,\n",
      "          9.8649e-07, 2.1488e-07, 2.5141e-06, 1.8056e-06, 7.7241e-07,\n",
      "          1.2706e-06, 1.9650e-07, 4.5136e-07, 5.1438e-07, 2.6535e-07,\n",
      "          1.3824e-07, 6.0681e-08, 7.8358e-08, 7.5426e-08, 4.4627e-08,\n",
      "          6.5047e-08],\n",
      "         [8.4147e-06, 1.9302e-05, 1.7538e-05, 1.2694e-05, 1.5930e-05,\n",
      "          5.9596e-06, 9.7992e-06, 3.7087e-04, 3.5549e-04, 9.3650e-05,\n",
      "          3.1759e-04, 1.6350e-04, 4.0364e-02, 7.2580e-02, 4.3017e-05,\n",
      "          3.8857e-04, 4.4131e-04, 6.2750e-02, 1.2789e-01, 3.3522e-05,\n",
      "          4.2719e-04, 3.2304e-04, 7.4475e-02, 7.0138e-02, 1.0940e-04,\n",
      "          3.4688e-01, 4.9003e-05, 3.1818e-02, 3.8339e-02, 7.5378e-05,\n",
      "          2.9332e-04, 1.0705e-04, 4.0204e-02, 3.5685e-02, 4.0858e-04,\n",
      "          1.7348e-04, 4.9953e-05, 3.7121e-02, 1.6787e-02, 5.2241e-05,\n",
      "          7.7889e-05, 1.5698e-04, 1.2310e-04, 9.2595e-05, 1.9869e-05,\n",
      "          2.5784e-05, 1.6492e-05, 6.7717e-06, 2.9101e-05, 2.6307e-05,\n",
      "          2.2824e-05]]])\n",
      "Player 0 Prediction: tensor([[0.1106, 0.8762, 0.0132, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.9240e-05, 1.1278e-05, 1.3122e-05, 1.9331e-05, 1.4725e-05,\n",
      "          1.8969e-05, 1.6264e-05, 4.1349e-02, 1.4507e-01, 7.3913e-05,\n",
      "          2.5648e-04, 6.6386e-04, 1.9462e-01, 2.2519e-01, 5.7238e-05,\n",
      "          2.0004e-03, 2.3104e-04, 1.5770e-02, 2.6329e-02, 2.1607e-05,\n",
      "          5.5724e-04, 6.6448e-05, 5.1930e-05, 8.5431e-05, 1.0338e-05,\n",
      "          1.3295e-01, 9.9386e-06, 1.2629e-04, 1.2658e-04, 1.1077e-04,\n",
      "          5.1326e-04, 8.1811e-06, 7.6400e-03, 4.7400e-03, 2.2954e-04,\n",
      "          1.4451e-03, 2.8383e-05, 7.7181e-02, 5.7853e-02, 3.0642e-04,\n",
      "          1.0495e-04, 1.2032e-04, 5.0444e-02, 1.3462e-02, 6.7840e-06,\n",
      "          7.2140e-06, 2.3581e-05, 1.5297e-05, 9.0587e-06, 9.4760e-06,\n",
      "          7.6870e-06],\n",
      "         [4.1639e-06, 6.5645e-06, 3.7477e-06, 1.2616e-05, 6.9760e-06,\n",
      "          5.2573e-06, 2.6387e-05, 2.7022e-02, 6.3338e-02, 4.1188e-05,\n",
      "          2.0744e-02, 2.9955e-03, 1.0446e-01, 1.3747e-01, 3.2744e-05,\n",
      "          1.8278e-03, 2.2118e-04, 2.1477e-04, 2.4525e-04, 2.9532e-05,\n",
      "          6.4675e-04, 1.8127e-04, 1.2500e-06, 7.8587e-06, 4.7672e-06,\n",
      "          5.2891e-01, 6.4883e-06, 4.8509e-04, 3.2811e-04, 1.0095e-04,\n",
      "          3.6171e-03, 5.2125e-06, 7.9842e-04, 4.9170e-04, 1.4311e-04,\n",
      "          2.9460e-03, 1.2156e-05, 3.6625e-02, 3.4304e-02, 2.9175e-03,\n",
      "          8.3398e-03, 7.6401e-05, 1.3184e-02, 7.0778e-03, 1.4608e-05,\n",
      "          6.2755e-06, 8.4518e-06, 1.0108e-05, 1.4665e-05, 8.6042e-06,\n",
      "          1.2263e-05],\n",
      "         [9.6548e-07, 1.6954e-06, 1.7052e-06, 1.1270e-06, 1.5771e-06,\n",
      "          2.5154e-06, 1.4691e-06, 7.2082e-05, 1.5164e-04, 7.5290e-06,\n",
      "          2.8281e-05, 1.4150e-05, 1.6894e-03, 2.9235e-03, 4.3315e-06,\n",
      "          1.9108e-04, 8.9544e-06, 2.3990e-03, 2.1007e-03, 2.1699e-06,\n",
      "          9.8822e-01, 3.7614e-06, 7.1062e-05, 1.2708e-03, 2.3386e-04,\n",
      "          2.1959e-04, 2.2535e-06, 4.0592e-06, 3.3840e-06, 3.1795e-06,\n",
      "          1.0161e-05, 2.5563e-06, 4.0127e-05, 1.8245e-05, 3.0318e-06,\n",
      "          8.7066e-06, 1.5392e-06, 1.2455e-04, 3.6968e-05, 1.3093e-05,\n",
      "          1.2803e-05, 2.0376e-06, 5.2755e-05, 2.9843e-05, 1.4799e-06,\n",
      "          1.2459e-06, 1.4987e-06, 2.0430e-06, 1.0064e-06, 3.1635e-07,\n",
      "          5.9104e-07],\n",
      "         [8.7091e-05, 1.5648e-04, 6.1611e-05, 1.4436e-04, 7.4723e-05,\n",
      "          5.9503e-05, 7.5426e-05, 3.5236e-02, 6.2928e-02, 2.6944e-04,\n",
      "          2.9068e-03, 1.2101e-03, 8.8683e-02, 1.6959e-01, 2.4322e-04,\n",
      "          2.4442e-03, 2.4123e-03, 2.8338e-02, 4.3347e-02, 3.1077e-04,\n",
      "          2.2927e-02, 3.5904e-04, 4.0104e-04, 7.6445e-04, 2.3882e-04,\n",
      "          3.8004e-01, 2.4004e-04, 8.6210e-04, 1.7113e-03, 1.4587e-04,\n",
      "          1.0514e-02, 2.5020e-04, 1.5998e-02, 1.0449e-02, 1.2007e-03,\n",
      "          2.1067e-03, 1.9877e-04, 5.6534e-02, 3.1284e-02, 6.6110e-04,\n",
      "          1.4966e-03, 3.2300e-04, 1.2485e-02, 9.5502e-03, 4.8461e-05,\n",
      "          1.0384e-04, 2.0632e-04, 7.3589e-05, 1.0089e-04, 6.5724e-05,\n",
      "          8.3961e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/50000 [51:43<22:54, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30203\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5080/10000 (50.8%)\n",
      "    Average reward: -0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4920/10000 (49.2%)\n",
      "    Average reward: +0.254\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5684 (37.1%)\n",
      "    Action 1: 5289 (34.5%)\n",
      "    Action 2: 3517 (22.9%)\n",
      "    Action 3: 849 (5.5%)\n",
      "  Player 1:\n",
      "    Action 0: 2669 (18.0%)\n",
      "    Action 1: 7161 (48.2%)\n",
      "    Action 2: 3780 (25.4%)\n",
      "    Action 3: 1254 (8.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2540.5, 2540.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.952 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.006\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2540\n",
      "   Testing specific player: 0\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.4944, 0.0416, 0.4639]])\n",
      "Player 0 Prediction: tensor([[0.3789, 0.0442, 0.5770, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42404\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4295/10000 (43.0%)\n",
      "    Average reward: +0.122\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5705/10000 (57.0%)\n",
      "    Average reward: -0.122\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4356 (20.4%)\n",
      "    Action 1: 10487 (49.1%)\n",
      "    Action 2: 3687 (17.3%)\n",
      "    Action 3: 2833 (13.3%)\n",
      "  Player 1:\n",
      "    Action 0: 15482 (73.6%)\n",
      "    Action 1: 5559 (26.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1216.0, -1216.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.972 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.833 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.902\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1216\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[3.1202e-05, 3.5726e-05, 3.5117e-05, 4.0315e-05, 5.3690e-05,\n",
      "          3.6473e-05, 4.2699e-05, 1.5440e-03, 3.7043e-03, 1.8152e-03,\n",
      "          1.0110e-02, 5.4526e-03, 1.0536e-02, 2.6150e-02, 1.7196e-03,\n",
      "          1.6256e-02, 5.9388e-03, 1.3070e-02, 1.6920e-02, 1.3414e-04,\n",
      "          7.5307e-03, 1.2505e-03, 2.2365e-01, 2.2017e-01, 2.1121e-04,\n",
      "          9.0678e-02, 6.0945e-05, 1.0221e-01, 1.0443e-01, 1.9928e-03,\n",
      "          3.3886e-02, 1.2410e-04, 2.9703e-02, 1.9358e-02, 2.6665e-03,\n",
      "          9.6805e-03, 5.5877e-04, 2.2298e-02, 7.2280e-03, 1.7255e-03,\n",
      "          2.6805e-03, 1.9238e-03, 1.2638e-03, 7.5965e-04, 4.2191e-05,\n",
      "          4.9626e-05, 6.1471e-05, 5.1387e-05, 5.1662e-05, 4.1676e-05,\n",
      "          3.7282e-05],\n",
      "         [1.6514e-05, 1.6843e-05, 8.6536e-06, 1.7395e-05, 2.1302e-05,\n",
      "          9.9565e-06, 1.5907e-05, 8.1299e-03, 5.5688e-02, 1.3521e-04,\n",
      "          6.5847e-03, 2.2956e-03, 3.5246e-02, 1.4724e-01, 7.3074e-05,\n",
      "          7.0502e-03, 2.4188e-03, 7.9232e-04, 1.1047e-03, 2.6697e-05,\n",
      "          3.5941e-02, 1.0143e-03, 9.4674e-05, 2.4944e-04, 1.4273e-04,\n",
      "          7.5848e-02, 2.4600e-05, 2.7284e-01, 2.7565e-01, 6.1634e-04,\n",
      "          8.9384e-03, 1.4721e-05, 1.0078e-03, 9.6089e-04, 2.0548e-03,\n",
      "          5.4833e-03, 4.2340e-05, 3.1206e-02, 5.4469e-03, 1.4246e-03,\n",
      "          2.4255e-03, 2.1640e-04, 9.3631e-03, 1.9830e-03, 1.5662e-05,\n",
      "          1.3716e-05, 1.5831e-05, 1.7501e-05, 1.9097e-05, 2.1122e-05,\n",
      "          2.2261e-05],\n",
      "         [2.4910e-07, 2.6960e-07, 3.7550e-07, 2.6585e-07, 2.1615e-07,\n",
      "          3.4853e-07, 3.0376e-07, 3.3649e-06, 3.6334e-06, 2.1478e-06,\n",
      "          2.5478e-06, 3.5506e-06, 4.9690e-05, 1.7461e-04, 1.2700e-06,\n",
      "          6.8709e-05, 5.5047e-06, 1.0214e-04, 7.9577e-05, 3.9408e-07,\n",
      "          1.1691e-04, 1.9818e-06, 3.2030e-03, 3.0551e-01, 6.9053e-01,\n",
      "          1.0132e-05, 3.1563e-07, 4.3957e-05, 3.1860e-05, 1.4672e-06,\n",
      "          4.4182e-06, 2.8562e-07, 5.4745e-06, 3.3205e-06, 1.8121e-06,\n",
      "          1.8663e-06, 5.8326e-07, 1.1715e-05, 7.2300e-06, 2.6060e-06,\n",
      "          2.6872e-06, 2.1577e-06, 1.8617e-06, 8.3007e-07, 2.6847e-07,\n",
      "          3.1855e-07, 3.7427e-07, 5.4409e-07, 2.7968e-07, 1.3839e-07,\n",
      "          1.9097e-07],\n",
      "         [4.0147e-05, 8.7152e-05, 7.0101e-05, 5.1986e-05, 1.0685e-04,\n",
      "          4.8503e-05, 7.7209e-05, 1.4439e-03, 3.4783e-03, 2.6733e-04,\n",
      "          5.4179e-04, 5.6826e-04, 5.6592e-02, 1.7031e-01, 1.8288e-04,\n",
      "          5.8503e-04, 1.1074e-03, 6.4962e-02, 7.3920e-02, 9.3911e-05,\n",
      "          1.4838e-03, 5.6830e-04, 2.0842e-02, 9.2440e-02, 1.5917e-03,\n",
      "          2.3857e-01, 8.1096e-05, 7.2877e-02, 8.2108e-02, 7.6748e-04,\n",
      "          1.7049e-03, 7.8730e-05, 4.2577e-02, 2.1222e-02, 6.2339e-04,\n",
      "          4.0069e-04, 1.5772e-04, 2.7547e-02, 1.7002e-02, 2.2829e-04,\n",
      "          3.4147e-04, 3.2218e-04, 8.1595e-04, 7.5084e-04, 6.4944e-05,\n",
      "          3.0681e-05, 4.1821e-05, 7.8559e-05, 7.0557e-05, 4.8414e-05,\n",
      "          3.3560e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2419, 0.2887, 0.4695]])\n",
      "Player 0 Prediction: tensor([[[3.2849e-06, 1.2091e-05, 5.2791e-06, 1.0456e-05, 1.7532e-05,\n",
      "          1.0891e-05, 1.1180e-05, 1.1548e-04, 8.9046e-05, 2.0675e-04,\n",
      "          1.1621e-01, 7.0026e-03, 2.8783e-04, 4.8739e-04, 2.1198e-04,\n",
      "          1.2683e-01, 8.9350e-03, 2.6281e-02, 3.4076e-02, 5.0547e-05,\n",
      "          1.8191e-02, 2.3859e-03, 1.7948e-01, 3.8306e-02, 9.2588e-06,\n",
      "          2.8258e-01, 7.5078e-06, 1.3785e-02, 1.0495e-02, 4.5846e-04,\n",
      "          1.2211e-02, 2.2733e-05, 4.5791e-02, 4.1668e-02, 3.4154e-03,\n",
      "          9.4746e-03, 1.4614e-04, 3.0144e-04, 1.6961e-04, 1.4046e-03,\n",
      "          1.8564e-02, 1.6040e-04, 2.1215e-05, 2.3916e-05, 2.2826e-05,\n",
      "          1.2829e-05, 5.2467e-06, 9.8730e-06, 5.2926e-06, 1.0931e-05,\n",
      "          4.8859e-06],\n",
      "         [6.6373e-06, 1.1053e-05, 4.2248e-06, 1.2137e-05, 2.3350e-05,\n",
      "          7.1337e-06, 1.2317e-05, 4.8802e-03, 1.1858e-02, 3.2798e-05,\n",
      "          7.8828e-03, 1.3242e-03, 4.2398e-02, 9.6252e-02, 1.4893e-05,\n",
      "          2.3165e-03, 1.1821e-03, 6.3520e-02, 6.2047e-02, 2.8395e-05,\n",
      "          3.1173e-02, 1.3381e-03, 5.0288e-04, 3.3851e-04, 1.7263e-05,\n",
      "          2.6806e-01, 6.5683e-06, 1.2529e-01, 1.0289e-01, 2.3312e-04,\n",
      "          4.1717e-03, 6.1858e-06, 4.5360e-02, 5.8097e-02, 1.0637e-03,\n",
      "          7.2326e-04, 2.7238e-05, 4.4505e-02, 1.8377e-02, 4.2919e-04,\n",
      "          6.0232e-04, 7.6524e-05, 2.2684e-03, 5.6392e-04, 1.6095e-05,\n",
      "          6.6395e-06, 3.3172e-06, 1.3935e-05, 4.8065e-06, 1.7956e-05,\n",
      "          6.4987e-06],\n",
      "         [8.7432e-09, 3.8451e-08, 1.9041e-08, 1.8212e-08, 2.0690e-08,\n",
      "          4.8222e-08, 1.8034e-08, 1.6471e-07, 7.5421e-08, 1.5315e-07,\n",
      "          1.0768e-06, 3.8208e-07, 2.2392e-06, 5.4329e-06, 7.5632e-08,\n",
      "          2.9949e-05, 8.3143e-07, 2.6215e-05, 2.1886e-05, 4.6909e-08,\n",
      "          1.2338e-05, 5.0147e-07, 5.9748e-01, 4.0124e-01, 1.1562e-03,\n",
      "          5.3415e-06, 2.7762e-08, 4.3331e-06, 3.0933e-06, 9.2521e-08,\n",
      "          4.0075e-07, 1.2314e-08, 3.5615e-06, 1.8551e-06, 2.7983e-07,\n",
      "          1.3279e-07, 6.4082e-08, 5.4385e-07, 6.8885e-07, 2.2844e-07,\n",
      "          6.9488e-07, 7.6994e-08, 6.8884e-08, 5.3856e-08, 5.2966e-08,\n",
      "          1.7769e-08, 8.8159e-09, 4.5195e-08, 1.4362e-08, 1.7795e-08,\n",
      "          9.3225e-09],\n",
      "         [2.8066e-06, 1.4844e-05, 4.6391e-06, 9.3433e-06, 2.8969e-05,\n",
      "          1.6922e-05, 1.0824e-05, 1.3325e-04, 1.0754e-04, 1.7744e-05,\n",
      "          3.5473e-04, 9.3875e-05, 6.9563e-03, 1.7400e-02, 9.2089e-06,\n",
      "          1.5754e-04, 1.6121e-04, 5.0698e-02, 6.6611e-02, 1.4499e-05,\n",
      "          2.3839e-04, 1.5063e-04, 1.4316e-01, 1.3453e-01, 3.7805e-05,\n",
      "          4.2248e-01, 9.2688e-06, 3.9977e-02, 4.2185e-02, 5.4599e-05,\n",
      "          8.3581e-05, 7.8976e-06, 3.4381e-02, 3.1808e-02, 8.0834e-05,\n",
      "          2.3891e-05, 4.3878e-05, 4.9716e-03, 2.7021e-03, 6.1267e-05,\n",
      "          8.1347e-05, 2.0450e-05, 3.0074e-05, 2.2858e-05, 1.9763e-05,\n",
      "          3.0504e-06, 2.4461e-06, 1.1308e-05, 4.2616e-06, 9.3766e-06,\n",
      "          2.1008e-06]]])\n",
      "Player 1 Prediction: tensor([[0.4081, 0.0822, 0.5097, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28632\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5399/10000 (54.0%)\n",
      "    Average reward: +0.164\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4601/10000 (46.0%)\n",
      "    Average reward: -0.164\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4667 (32.4%)\n",
      "    Action 1: 5400 (37.5%)\n",
      "    Action 2: 4236 (29.4%)\n",
      "    Action 3: 93 (0.6%)\n",
      "  Player 1:\n",
      "    Action 0: 4127 (29.0%)\n",
      "    Action 1: 5197 (36.5%)\n",
      "    Action 2: 3722 (26.1%)\n",
      "    Action 3: 1190 (8.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1638.0, -1638.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.057 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.049 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.053\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1638\n",
      "   Testing specific player: 1\n",
      "   At training step: 36000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0512, 0.0172, 0.9315, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 36000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39816\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6986/10000 (69.9%)\n",
      "    Average reward: -0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3014/10000 (30.1%)\n",
      "    Average reward: +0.157\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14553 (74.1%)\n",
      "    Action 1: 5086 (25.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3868 (19.2%)\n",
      "    Action 1: 9517 (47.2%)\n",
      "    Action 2: 3854 (19.1%)\n",
      "    Action 3: 2938 (14.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1566.0, 1566.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.825 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.968 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.897\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1566\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37001/50000 [54:04<21:05, 10.27it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 37000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 226804/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 224260/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38000/50000 [55:40<20:20,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 38000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 233323/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 230616/2000000\n",
      "P1 SL Buffer Size:  233323\n",
      "P1 SL buffer distribution [72878. 90625. 56211. 13609.]\n",
      "P1 actions distribution [0.31234812 0.38841006 0.24091495 0.05832687]\n",
      "P2 SL Buffer Size:  230616\n",
      "P2 SL buffer distribution [61746. 93969. 56989. 17912.]\n",
      "P2 actions distribution [0.26774378 0.40746956 0.24711642 0.07767024]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[2.4412e-05, 5.8551e-05, 4.1470e-05, 4.8414e-05, 4.5014e-05,\n",
      "          5.1687e-05, 8.0795e-05, 1.0041e-03, 4.1744e-03, 2.0111e-03,\n",
      "          1.1842e-02, 5.4583e-03, 6.9632e-03, 1.9784e-02, 7.7710e-04,\n",
      "          2.9149e-02, 6.7316e-03, 8.3662e-03, 1.2818e-02, 1.3657e-04,\n",
      "          1.5385e-02, 3.2878e-03, 2.5205e-01, 2.8081e-01, 3.2106e-04,\n",
      "          7.1766e-02, 1.9546e-05, 5.5778e-02, 4.8700e-02, 1.4234e-03,\n",
      "          6.3972e-02, 2.3510e-04, 2.2307e-02, 1.6740e-02, 2.8146e-03,\n",
      "          1.7137e-02, 2.9424e-04, 1.9559e-02, 9.0184e-03, 1.5972e-03,\n",
      "          2.1363e-03, 1.2863e-03, 2.2676e-03, 1.1853e-03, 5.6774e-05,\n",
      "          4.7004e-05, 6.9525e-05, 3.4064e-05, 2.8097e-05, 8.9301e-05,\n",
      "          1.2792e-05],\n",
      "         [6.9912e-06, 4.5927e-05, 1.8126e-05, 1.5539e-05, 1.3647e-05,\n",
      "          1.3089e-05, 3.9409e-05, 4.8286e-03, 3.3465e-02, 3.9946e-04,\n",
      "          1.1186e-02, 4.4153e-03, 1.2173e-02, 5.6343e-02, 8.2571e-05,\n",
      "          1.4586e-02, 3.5757e-03, 1.4532e-03, 2.5888e-03, 3.1486e-05,\n",
      "          1.8038e-01, 2.8735e-03, 1.3564e-04, 2.4229e-04, 2.4915e-04,\n",
      "          6.2826e-02, 3.3520e-05, 2.7648e-01, 2.7062e-01, 1.0222e-03,\n",
      "          9.9843e-03, 2.9411e-05, 2.4353e-03, 1.6319e-03, 1.6045e-03,\n",
      "          9.0844e-03, 1.4746e-05, 1.6723e-02, 4.0873e-03, 1.0093e-03,\n",
      "          2.7983e-03, 4.6669e-04, 8.0432e-03, 1.7963e-03, 1.7289e-05,\n",
      "          2.0318e-05, 2.0587e-05, 2.9514e-05, 1.9999e-05, 1.3695e-05,\n",
      "          2.7812e-05],\n",
      "         [2.5940e-07, 3.6084e-07, 2.5269e-07, 1.4276e-07, 8.3799e-08,\n",
      "          4.1272e-07, 3.5000e-07, 5.2151e-06, 6.0688e-06, 4.4412e-06,\n",
      "          3.9197e-06, 3.9842e-06, 8.7513e-05, 9.9217e-05, 9.4169e-07,\n",
      "          7.0232e-05, 3.1040e-06, 1.4312e-04, 1.2404e-04, 1.8878e-07,\n",
      "          4.1015e-04, 2.2240e-06, 5.1564e-03, 2.3422e-01, 7.5949e-01,\n",
      "          1.3310e-05, 4.1662e-07, 3.6721e-05, 2.7570e-05, 9.9510e-06,\n",
      "          4.6148e-06, 9.6438e-07, 6.9606e-06, 1.3524e-05, 4.2232e-06,\n",
      "          3.8415e-06, 3.2809e-07, 1.0046e-05, 1.7984e-05, 2.1947e-06,\n",
      "          4.2982e-06, 2.0149e-06, 3.1943e-06, 1.4725e-06, 4.2827e-07,\n",
      "          6.0768e-07, 1.1030e-07, 6.3985e-07, 1.1781e-07, 3.9588e-07,\n",
      "          2.6321e-07],\n",
      "         [3.2062e-05, 5.5758e-05, 4.9571e-05, 7.4422e-05, 4.0533e-05,\n",
      "          1.8026e-05, 3.2210e-05, 7.5037e-04, 1.0610e-03, 2.0644e-04,\n",
      "          6.7013e-04, 4.9927e-04, 6.4071e-02, 2.0130e-01, 4.6758e-04,\n",
      "          1.0214e-03, 7.1167e-04, 7.8218e-02, 1.2781e-01, 8.6287e-05,\n",
      "          1.8534e-03, 2.8860e-04, 1.1414e-02, 5.4177e-02, 7.0739e-04,\n",
      "          1.8702e-01, 5.1505e-05, 3.5796e-02, 2.5926e-02, 3.7748e-04,\n",
      "          1.0839e-03, 1.0358e-04, 4.4947e-02, 2.9276e-02, 7.7020e-04,\n",
      "          4.2958e-04, 7.5939e-05, 1.0539e-01, 2.0828e-02, 2.6889e-04,\n",
      "          2.8521e-04, 5.6866e-04, 5.8964e-04, 2.1519e-04, 1.1445e-04,\n",
      "          6.9108e-05, 2.5433e-05, 8.8650e-05, 4.3930e-05, 2.6430e-05,\n",
      "          1.4379e-05]]])\n",
      "Player 0 Prediction: tensor([[0.1314, 0.8557, 0.0130, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[6.4596e-06, 1.1992e-05, 8.6970e-06, 1.0139e-05, 9.6218e-06,\n",
      "          1.2109e-05, 2.4429e-05, 4.1653e-02, 1.3850e-01, 1.2303e-04,\n",
      "          2.3421e-04, 4.7390e-04, 1.4114e-01, 2.7063e-01, 7.8916e-05,\n",
      "          2.6844e-03, 3.0298e-04, 1.8839e-02, 2.3289e-02, 2.7709e-05,\n",
      "          4.8571e-04, 9.8465e-05, 3.9636e-05, 1.1078e-04, 1.0859e-05,\n",
      "          1.2260e-01, 9.6065e-06, 9.6617e-05, 1.2969e-04, 9.0471e-05,\n",
      "          4.8707e-04, 2.5820e-05, 5.9771e-03, 5.8287e-03, 1.5199e-04,\n",
      "          1.4729e-03, 3.6880e-05, 9.5000e-02, 6.2997e-02, 2.6720e-04,\n",
      "          1.1248e-04, 9.3098e-05, 4.6260e-02, 1.9455e-02, 1.5994e-05,\n",
      "          1.1856e-05, 1.9221e-05, 1.6599e-05, 9.4840e-06, 1.7633e-05,\n",
      "          5.0181e-06],\n",
      "         [2.7715e-06, 1.3352e-05, 5.6832e-06, 5.6815e-06, 1.0067e-05,\n",
      "          4.6603e-06, 1.6127e-05, 1.9234e-02, 5.3009e-02, 5.7658e-05,\n",
      "          3.0945e-02, 3.2334e-03, 9.3714e-02, 1.5783e-01, 3.3331e-05,\n",
      "          1.5484e-03, 2.9422e-04, 2.0485e-04, 2.6031e-04, 2.2274e-05,\n",
      "          6.0065e-04, 9.2250e-05, 1.3439e-06, 6.2549e-06, 4.6406e-06,\n",
      "          5.4429e-01, 1.0138e-05, 3.6877e-04, 4.2402e-04, 9.5168e-05,\n",
      "          2.6093e-03, 1.0284e-05, 7.9062e-04, 5.0850e-04, 1.0978e-04,\n",
      "          3.5350e-03, 7.3868e-06, 3.3436e-02, 2.4564e-02, 2.4537e-03,\n",
      "          8.6047e-03, 4.9179e-05, 1.0131e-02, 6.7818e-03, 8.9484e-06,\n",
      "          1.1399e-05, 8.0484e-06, 1.5270e-05, 8.2745e-06, 8.0264e-06,\n",
      "          9.3950e-06],\n",
      "         [9.0637e-07, 2.3257e-06, 9.3133e-07, 7.6122e-07, 5.7625e-07,\n",
      "          1.1635e-06, 1.8351e-06, 7.6957e-05, 1.2497e-04, 6.3937e-06,\n",
      "          2.4680e-05, 1.4732e-05, 1.5429e-03, 2.1085e-03, 5.6469e-06,\n",
      "          1.5936e-04, 7.7496e-06, 2.3859e-03, 2.0547e-03, 1.9797e-06,\n",
      "          9.8932e-01, 3.3179e-06, 4.8615e-05, 1.1612e-03, 4.0197e-04,\n",
      "          1.8067e-04, 1.3006e-06, 4.6026e-06, 4.6629e-06, 6.6798e-06,\n",
      "          1.3561e-05, 2.5766e-06, 2.2785e-05, 1.6365e-05, 4.0652e-06,\n",
      "          1.0777e-05, 1.2881e-06, 1.1040e-04, 5.4728e-05, 1.2101e-05,\n",
      "          1.1397e-05, 3.0185e-06, 5.0213e-05, 1.8786e-05, 1.1702e-06,\n",
      "          9.4795e-07, 7.0273e-07, 2.6774e-06, 7.7396e-07, 7.5962e-07,\n",
      "          6.6039e-07],\n",
      "         [8.8072e-05, 1.6703e-04, 9.2096e-05, 1.2829e-04, 8.9892e-05,\n",
      "          5.0236e-05, 8.3679e-05, 2.0099e-02, 4.7386e-02, 3.6639e-04,\n",
      "          4.4310e-03, 1.5491e-03, 8.1844e-02, 1.7690e-01, 4.6560e-04,\n",
      "          3.3256e-03, 2.2633e-03, 4.3581e-02, 4.6690e-02, 1.9849e-04,\n",
      "          4.2830e-02, 1.6289e-04, 3.5963e-04, 9.1067e-04, 1.6327e-04,\n",
      "          3.5888e-01, 1.2451e-04, 9.2368e-04, 1.2957e-03, 3.0638e-04,\n",
      "          1.5050e-02, 2.0407e-04, 1.6219e-02, 1.0023e-02, 1.1585e-03,\n",
      "          2.5362e-03, 2.1688e-04, 7.1378e-02, 2.3799e-02, 1.2829e-03,\n",
      "          1.8123e-03, 3.1398e-04, 1.2796e-02, 6.7981e-03, 1.2571e-04,\n",
      "          9.2184e-05, 9.6521e-05, 1.8159e-04, 6.5596e-05, 4.9127e-05,\n",
      "          4.4921e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29230\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5096/10000 (51.0%)\n",
      "    Average reward: -0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4904/10000 (49.0%)\n",
      "    Average reward: +0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5583 (37.7%)\n",
      "    Action 1: 4827 (32.6%)\n",
      "    Action 2: 3695 (24.9%)\n",
      "    Action 3: 710 (4.8%)\n",
      "  Player 1:\n",
      "    Action 0: 2374 (16.5%)\n",
      "    Action 1: 7112 (49.3%)\n",
      "    Action 2: 3909 (27.1%)\n",
      "    Action 3: 1020 (7.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2400.5, 2400.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.931 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.995\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2401\n",
      "   Testing specific player: 0\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7968, 0.0018, 0.2014]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2686, 0.0516, 0.6798]])\n",
      "Player 0 Prediction: tensor([[0.5515, 0.3538, 0.0947, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42254\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4270/10000 (42.7%)\n",
      "    Average reward: +0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5730/10000 (57.3%)\n",
      "    Average reward: -0.076\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4330 (20.3%)\n",
      "    Action 1: 10497 (49.3%)\n",
      "    Action 2: 3735 (17.5%)\n",
      "    Action 3: 2735 (12.8%)\n",
      "  Player 1:\n",
      "    Action 0: 15474 (73.8%)\n",
      "    Action 1: 5483 (26.2%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [763.5, -763.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.970 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.829 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.900\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0764\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.6846, 0.2946, 0.0208, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[1.5578e-05, 2.9189e-05, 1.4322e-05, 2.2353e-05, 1.1917e-05,\n",
      "          2.0276e-05, 9.5104e-06, 1.1804e-04, 1.0974e-04, 2.6336e-04,\n",
      "          2.8523e-02, 2.8653e-03, 2.6100e-04, 5.7738e-04, 2.7275e-04,\n",
      "          1.7232e-02, 3.9333e-03, 1.9978e-02, 1.6229e-02, 3.6609e-05,\n",
      "          5.5228e-03, 1.5148e-03, 1.7513e-02, 7.4429e-03, 3.0948e-05,\n",
      "          2.0186e-01, 2.3044e-05, 3.9382e-02, 3.3526e-02, 8.7308e-04,\n",
      "          2.6859e-01, 7.7312e-05, 6.2550e-02, 6.3310e-02, 8.6498e-03,\n",
      "          7.5452e-02, 3.4247e-04, 4.1398e-03, 1.9103e-03, 1.3046e-02,\n",
      "          1.0073e-01, 7.8045e-04, 1.2831e-03, 8.0202e-04, 2.8170e-05,\n",
      "          2.3211e-05, 9.1275e-06, 2.4219e-05, 1.1919e-05, 1.5032e-05,\n",
      "          1.2749e-05],\n",
      "         [2.2704e-05, 1.4333e-05, 1.5426e-05, 1.6710e-05, 1.3836e-05,\n",
      "          2.1321e-05, 7.0887e-06, 5.1835e-04, 3.3912e-03, 8.5431e-05,\n",
      "          2.3914e-03, 1.2100e-03, 5.9716e-03, 1.0849e-02, 4.4957e-05,\n",
      "          2.3110e-02, 9.1234e-04, 7.5326e-03, 7.2264e-03, 1.5157e-05,\n",
      "          8.8604e-03, 1.4808e-03, 1.5306e-04, 7.7527e-05, 4.0505e-05,\n",
      "          1.2846e-01, 1.6667e-05, 2.6128e-01, 2.9873e-01, 5.1298e-04,\n",
      "          4.3630e-02, 1.7864e-05, 1.5775e-02, 1.4584e-02, 3.9159e-03,\n",
      "          4.9694e-02, 5.6076e-05, 4.2108e-02, 1.6245e-02, 7.2165e-03,\n",
      "          1.8000e-02, 1.5801e-04, 2.1601e-02, 3.9402e-03, 1.5813e-05,\n",
      "          6.9521e-06, 1.2451e-05, 1.0627e-05, 1.2964e-05, 8.0189e-06,\n",
      "          8.8441e-06],\n",
      "         [4.6624e-07, 9.7116e-07, 4.6481e-07, 5.9809e-07, 2.2524e-07,\n",
      "          5.6654e-07, 3.5637e-07, 7.6188e-07, 9.2395e-07, 1.7042e-06,\n",
      "          1.3577e-05, 5.0148e-06, 3.5549e-05, 6.7504e-05, 1.6125e-06,\n",
      "          2.6100e-04, 7.7766e-06, 1.1633e-04, 1.6616e-04, 4.2986e-07,\n",
      "          3.8377e-04, 2.3478e-06, 2.7661e-01, 6.8774e-01, 3.4209e-02,\n",
      "          3.9647e-05, 5.0384e-07, 6.9382e-05, 4.4225e-05, 1.3010e-06,\n",
      "          1.5468e-05, 1.0810e-06, 4.3604e-05, 4.4814e-05, 6.5280e-06,\n",
      "          1.3163e-05, 1.6982e-06, 1.0928e-05, 2.3147e-05, 1.4836e-05,\n",
      "          3.2599e-05, 4.1134e-06, 4.0268e-06, 1.6958e-06, 5.9557e-07,\n",
      "          4.6359e-07, 4.1643e-07, 3.3753e-07, 1.9086e-07, 1.5238e-07,\n",
      "          1.4691e-07],\n",
      "         [2.6413e-05, 7.7562e-05, 2.1121e-05, 3.3222e-05, 4.6849e-05,\n",
      "          4.5325e-05, 3.2648e-05, 1.6463e-04, 1.4998e-04, 5.0484e-05,\n",
      "          4.7244e-04, 3.5054e-04, 5.3233e-03, 2.1508e-02, 7.4896e-05,\n",
      "          4.7605e-04, 2.2506e-04, 4.0252e-02, 4.6974e-02, 2.6760e-05,\n",
      "          4.7914e-04, 2.0666e-04, 3.0843e-02, 2.8224e-02, 1.4649e-04,\n",
      "          1.6919e-01, 3.6410e-05, 1.2965e-01, 1.1921e-01, 2.4284e-04,\n",
      "          3.5072e-03, 5.2717e-05, 2.3461e-01, 1.0537e-01, 8.7844e-04,\n",
      "          6.9002e-04, 5.7245e-05, 4.1274e-02, 1.5201e-02, 9.8136e-04,\n",
      "          1.1755e-03, 1.3129e-04, 9.8368e-04, 3.6776e-04, 3.0781e-05,\n",
      "          8.3441e-06, 2.4982e-05, 2.4304e-05, 1.9782e-05, 1.6980e-05,\n",
      "          2.3278e-05]]])\n",
      "Player 1 Prediction: tensor([[0.1686, 0.1027, 0.7287, 0.0000]])\n",
      "Player 0 Prediction: tensor([[[2.6195e-06, 9.9839e-07, 3.4520e-07, 1.5937e-06, 2.4810e-07,\n",
      "          1.2781e-06, 2.2737e-07, 1.1858e-04, 1.3138e-04, 1.8913e-06,\n",
      "          1.6890e-03, 1.0019e-04, 5.7571e-05, 7.2927e-05, 4.0637e-06,\n",
      "          3.0691e-01, 3.0858e-05, 4.3408e-06, 6.4055e-06, 6.5036e-07,\n",
      "          6.1017e-04, 1.2776e-05, 1.4668e-06, 1.3372e-06, 1.1971e-07,\n",
      "          7.6974e-02, 7.2115e-07, 2.0844e-06, 3.2077e-06, 3.9637e-06,\n",
      "          2.1355e-03, 3.3119e-06, 1.4424e-05, 1.5771e-05, 4.7485e-05,\n",
      "          6.0412e-01, 7.4380e-06, 9.5178e-05, 1.0506e-04, 7.1071e-04,\n",
      "          5.6100e-03, 1.9132e-06, 2.1399e-04, 1.7392e-04, 1.1546e-06,\n",
      "          1.4052e-07, 8.6401e-07, 2.8183e-07, 2.0990e-07, 1.8852e-07,\n",
      "          3.7038e-07],\n",
      "         [7.8673e-06, 3.9509e-06, 6.6068e-07, 6.6781e-06, 9.3705e-07,\n",
      "          3.8065e-06, 1.6436e-06, 1.8392e-04, 1.1088e-04, 3.0638e-06,\n",
      "          3.8426e-02, 2.0639e-03, 3.9306e-04, 4.1390e-04, 7.3693e-06,\n",
      "          6.8566e-02, 6.0356e-05, 5.9525e-05, 4.4123e-05, 1.4656e-06,\n",
      "          9.6689e-05, 1.4603e-05, 4.3034e-07, 2.0167e-06, 4.0097e-07,\n",
      "          2.1830e-01, 1.4617e-06, 4.8196e-05, 7.0003e-05, 5.0094e-06,\n",
      "          1.5761e-01, 1.2578e-05, 1.2153e-04, 1.1974e-04, 4.3514e-05,\n",
      "          3.5710e-01, 9.0751e-06, 6.2465e-04, 5.7424e-04, 2.3542e-02,\n",
      "          1.3065e-01, 3.2295e-06, 3.2280e-04, 3.5455e-04, 2.3344e-06,\n",
      "          7.9537e-07, 2.7682e-06, 6.0144e-07, 7.5905e-07, 5.6379e-07,\n",
      "          1.6995e-06],\n",
      "         [2.0059e-06, 5.8609e-07, 1.1292e-07, 3.4982e-07, 1.6039e-07,\n",
      "          7.4931e-07, 8.7927e-08, 3.0740e-06, 1.3469e-06, 4.9398e-07,\n",
      "          3.9727e-05, 1.5481e-05, 4.7357e-05, 4.3555e-05, 1.0152e-06,\n",
      "          7.2736e-03, 3.2072e-06, 1.6336e-05, 1.1857e-05, 9.1584e-08,\n",
      "          9.9060e-01, 2.3905e-06, 2.4089e-04, 6.4488e-04, 4.1543e-05,\n",
      "          1.1698e-04, 5.2122e-07, 1.0751e-06, 9.4141e-07, 6.5925e-07,\n",
      "          1.1544e-04, 1.1780e-06, 2.0028e-06, 1.6563e-06, 3.8945e-06,\n",
      "          5.8755e-04, 1.2909e-06, 2.4853e-06, 2.7333e-06, 3.6859e-05,\n",
      "          1.2256e-04, 3.8795e-07, 3.8817e-06, 4.4773e-06, 3.5821e-07,\n",
      "          9.9525e-08, 7.3764e-07, 3.6534e-07, 1.4908e-07, 1.0385e-07,\n",
      "          1.9397e-07],\n",
      "         [9.1769e-05, 6.3333e-05, 2.1357e-05, 6.4926e-05, 9.3382e-06,\n",
      "          7.7053e-05, 5.4572e-06, 3.4292e-04, 1.4138e-04, 2.9790e-05,\n",
      "          3.7899e-02, 1.1105e-03, 1.1108e-03, 9.1233e-04, 6.7307e-05,\n",
      "          7.6513e-02, 8.4494e-04, 5.2170e-04, 4.2533e-04, 1.5626e-05,\n",
      "          1.1537e-01, 1.3239e-04, 3.3627e-04, 5.4131e-04, 1.8677e-05,\n",
      "          2.0108e-01, 3.7582e-05, 6.8171e-04, 5.7883e-04, 2.2673e-05,\n",
      "          4.0600e-01, 1.1477e-04, 7.0809e-04, 1.1974e-03, 8.2940e-04,\n",
      "          8.4585e-02, 9.2767e-05, 6.9991e-04, 1.4563e-03, 5.2359e-03,\n",
      "          5.8442e-02, 2.0632e-05, 5.7789e-04, 8.6254e-04, 3.0951e-05,\n",
      "          9.8963e-06, 3.7589e-05, 1.2046e-05, 7.4472e-06, 7.0540e-06,\n",
      "          7.7505e-06]]])\n",
      "Player 1 Prediction: tensor([[0.5390, 0.2346, 0.2264, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28318\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5214/10000 (52.1%)\n",
      "    Average reward: +0.141\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4786/10000 (47.9%)\n",
      "    Average reward: -0.141\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5762 (40.3%)\n",
      "    Action 1: 3582 (25.0%)\n",
      "    Action 2: 4275 (29.9%)\n",
      "    Action 3: 684 (4.8%)\n",
      "  Player 1:\n",
      "    Action 0: 3195 (22.8%)\n",
      "    Action 1: 5693 (40.6%)\n",
      "    Action 2: 3313 (23.6%)\n",
      "    Action 3: 1814 (12.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1406.0, -1406.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.029 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.014 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.021\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1406\n",
      "   Testing specific player: 1\n",
      "   At training step: 38000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5702, 0.0076, 0.4222]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7448, 0.0484, 0.2068]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 38000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39994\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6972/10000 (69.7%)\n",
      "    Average reward: -0.165\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3028/10000 (30.3%)\n",
      "    Average reward: +0.165\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14729 (74.5%)\n",
      "    Action 1: 5035 (25.5%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3782 (18.7%)\n",
      "    Action 1: 9727 (48.1%)\n",
      "    Action 2: 3815 (18.9%)\n",
      "    Action 3: 2906 (14.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1653.0, 1653.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.819 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.960 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.889\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1653\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39005/50000 [57:12<08:18, 22.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 39000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 239795/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 237165/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39998/50000 [57:57<07:33, 22.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 40000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 246157/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 243855/2000000\n",
      "P1 SL Buffer Size:  246157\n",
      "P1 SL buffer distribution [77870. 93968. 60084. 14235.]\n",
      "P1 actions distribution [0.31634282 0.38174011 0.24408812 0.05782895]\n",
      "P2 SL Buffer Size:  243855\n",
      "P2 SL buffer distribution [64379. 99915. 60498. 19063.]\n",
      "P2 actions distribution [0.26400525 0.40973119 0.24809005 0.0781735 ]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39998/50000 [58:10<07:33, 22.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7977, 0.1853, 0.0170, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.5453e-05, 1.7093e-05, 2.3993e-05, 1.3719e-05, 1.8473e-05,\n",
      "          1.0758e-05, 2.0230e-05, 1.7764e-04, 1.7282e-04, 5.1313e-04,\n",
      "          6.0125e-02, 5.7199e-03, 2.1191e-04, 3.8765e-04, 1.8463e-04,\n",
      "          7.1748e-02, 1.0301e-02, 2.0773e-02, 2.1389e-02, 3.1204e-05,\n",
      "          8.8381e-02, 5.3905e-03, 3.9596e-01, 1.3951e-01, 2.6748e-05,\n",
      "          1.1724e-02, 1.3419e-05, 7.3140e-03, 1.0952e-02, 9.8866e-04,\n",
      "          4.5179e-02, 2.2941e-05, 1.6713e-02, 1.1692e-02, 4.4528e-03,\n",
      "          5.7703e-02, 5.3751e-05, 4.2207e-04, 2.2920e-04, 1.9410e-03,\n",
      "          8.6422e-03, 3.9691e-04, 2.3067e-04, 8.3713e-05, 1.9475e-05,\n",
      "          1.2160e-05, 2.0825e-05, 1.2734e-05, 1.3014e-05, 1.6616e-05,\n",
      "          1.7860e-05],\n",
      "         [1.2578e-05, 1.8029e-05, 1.3483e-05, 2.0877e-05, 1.1368e-05,\n",
      "          1.7379e-05, 1.0700e-05, 1.5134e-03, 6.8536e-03, 3.4183e-04,\n",
      "          1.5917e-02, 2.2843e-03, 1.3938e-02, 3.2269e-02, 7.6544e-05,\n",
      "          2.7141e-02, 3.1863e-03, 2.4710e-02, 3.3248e-02, 2.0737e-05,\n",
      "          3.4963e-01, 5.5822e-03, 9.9762e-04, 3.9450e-04, 5.0093e-05,\n",
      "          8.5288e-03, 2.0233e-05, 2.0588e-01, 2.1243e-01, 5.3769e-04,\n",
      "          8.8086e-03, 1.5908e-05, 1.1603e-02, 1.0222e-02, 1.0551e-03,\n",
      "          5.0724e-03, 3.7008e-05, 6.4584e-03, 2.7649e-03, 9.6380e-04,\n",
      "          3.1192e-03, 3.6779e-04, 3.2358e-03, 5.0408e-04, 9.8192e-06,\n",
      "          1.5400e-05, 1.4340e-05, 1.6064e-05, 2.3811e-05, 1.9186e-05,\n",
      "          1.9351e-05],\n",
      "         [5.4446e-08, 4.5282e-08, 7.6117e-08, 4.7528e-08, 3.6011e-08,\n",
      "          4.5321e-08, 5.5024e-08, 2.4467e-07, 2.1781e-07, 4.5317e-07,\n",
      "          1.7238e-06, 1.6007e-06, 8.0850e-06, 7.4649e-06, 1.3999e-07,\n",
      "          4.6818e-05, 1.0997e-06, 1.8633e-05, 2.7706e-05, 6.7941e-08,\n",
      "          5.0121e-05, 1.1793e-06, 4.2435e-01, 5.6433e-01, 1.1129e-02,\n",
      "          6.5051e-07, 6.7378e-08, 4.0452e-06, 3.6775e-06, 1.0525e-06,\n",
      "          1.2357e-06, 7.7789e-08, 2.1009e-06, 2.5914e-06, 8.5499e-07,\n",
      "          8.6193e-07, 1.4659e-07, 8.7367e-07, 1.7051e-06, 5.3338e-07,\n",
      "          6.0044e-07, 3.5168e-07, 1.9320e-07, 2.0919e-07, 9.2193e-08,\n",
      "          7.0700e-08, 5.0194e-08, 6.7147e-08, 9.4107e-08, 4.3374e-08,\n",
      "          6.6727e-08],\n",
      "         [1.2654e-05, 2.7672e-05, 2.8872e-05, 2.2239e-05, 2.4239e-05,\n",
      "          1.0842e-05, 2.5593e-05, 9.2386e-05, 1.7252e-04, 1.0561e-04,\n",
      "          5.0505e-04, 3.1716e-04, 3.3210e-02, 4.0274e-02, 5.8739e-05,\n",
      "          6.2712e-04, 3.8343e-04, 1.0742e-01, 1.2588e-01, 4.0040e-05,\n",
      "          1.9392e-03, 6.4161e-04, 2.3155e-01, 2.6226e-01, 1.1738e-04,\n",
      "          4.3155e-02, 1.8050e-05, 3.2300e-02, 2.7246e-02, 1.5807e-04,\n",
      "          2.4102e-04, 3.6904e-05, 3.4201e-02, 2.5778e-02, 2.5487e-04,\n",
      "          1.5098e-04, 4.3914e-05, 1.9950e-02, 9.9230e-03, 1.9402e-04,\n",
      "          1.5768e-04, 1.5007e-04, 1.1934e-04, 6.5029e-05, 2.0590e-05,\n",
      "          1.7967e-05, 2.1972e-05, 2.1181e-05, 2.3619e-05, 1.1984e-05,\n",
      "          8.9866e-06]]])\n",
      "Player 0 Prediction: tensor([[0.1620, 0.0514, 0.7866, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29966\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4958/10000 (49.6%)\n",
      "    Average reward: -0.257\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5042/10000 (50.4%)\n",
      "    Average reward: +0.257\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5725 (37.6%)\n",
      "    Action 1: 5029 (33.0%)\n",
      "    Action 2: 3802 (24.9%)\n",
      "    Action 3: 685 (4.5%)\n",
      "  Player 1:\n",
      "    Action 0: 2774 (18.8%)\n",
      "    Action 1: 7238 (49.2%)\n",
      "    Action 2: 3716 (25.2%)\n",
      "    Action 3: 997 (6.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2571.0, 2571.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.058 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.957 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.008\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2571\n",
      "   Testing specific player: 0\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7948, 0.0017, 0.2035]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2759, 0.0501, 0.6740]])\n",
      "Player 0 Prediction: tensor([[0.5111, 0.3835, 0.1055, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42295\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4185/10000 (41.9%)\n",
      "    Average reward: +0.045\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5815/10000 (58.1%)\n",
      "    Average reward: -0.045\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4489 (21.0%)\n",
      "    Action 1: 10342 (48.4%)\n",
      "    Action 2: 3749 (17.6%)\n",
      "    Action 3: 2769 (13.0%)\n",
      "  Player 1:\n",
      "    Action 0: 15313 (73.1%)\n",
      "    Action 1: 5633 (26.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [447.5, -447.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.980 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.840 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.910\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0447\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[4.2325e-01, 5.7628e-01, 4.6343e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[8.6772e-06, 1.4423e-05, 8.4575e-06, 1.5783e-05, 2.9136e-05,\n",
      "          1.1778e-05, 2.8776e-05, 1.6986e-04, 2.1344e-04, 7.0894e-04,\n",
      "          9.5208e-02, 9.8721e-03, 6.7280e-04, 1.0665e-03, 5.9506e-04,\n",
      "          7.1313e-02, 1.3537e-02, 1.4321e-02, 1.8433e-02, 3.3662e-05,\n",
      "          3.5764e-02, 2.5704e-03, 1.9923e-01, 6.9106e-02, 1.4989e-05,\n",
      "          1.9659e-01, 2.5780e-05, 3.3999e-02, 2.8181e-02, 1.3382e-03,\n",
      "          5.9135e-02, 4.4261e-05, 3.2960e-02, 2.4157e-02, 9.5361e-03,\n",
      "          3.4868e-02, 2.7607e-04, 1.3155e-03, 7.2018e-04, 5.2716e-03,\n",
      "          3.7874e-02, 4.4770e-04, 1.0594e-04, 8.8555e-05, 1.4129e-05,\n",
      "          2.9807e-05, 1.6029e-05, 1.6401e-05, 1.6241e-05, 1.5566e-05,\n",
      "          1.7859e-05],\n",
      "         [7.1302e-06, 8.0363e-06, 7.2753e-06, 7.0418e-06, 1.1022e-05,\n",
      "          5.6226e-06, 1.0495e-05, 5.5421e-03, 3.0760e-02, 8.7388e-05,\n",
      "          3.9654e-03, 1.3454e-03, 5.9414e-02, 1.3979e-01, 3.7644e-05,\n",
      "          3.9084e-03, 1.6266e-03, 8.0115e-03, 1.0122e-02, 1.4835e-05,\n",
      "          5.7314e-02, 1.7618e-03, 2.6691e-04, 2.1475e-04, 1.9861e-05,\n",
      "          1.5532e-01, 1.2098e-05, 2.0471e-01, 1.7541e-01, 5.7843e-04,\n",
      "          1.8200e-02, 1.5919e-05, 1.3540e-02, 1.1197e-02, 1.2764e-03,\n",
      "          2.6098e-03, 4.0517e-05, 5.9186e-02, 1.9085e-02, 1.0166e-03,\n",
      "          1.5877e-03, 1.3685e-04, 9.7822e-03, 1.9335e-03, 7.8553e-06,\n",
      "          1.4266e-05, 1.4477e-05, 1.1490e-05, 1.0793e-05, 2.0347e-05,\n",
      "          1.3717e-05],\n",
      "         [4.1183e-08, 5.3539e-08, 5.6451e-08, 2.9663e-08, 4.2432e-08,\n",
      "          6.1039e-08, 7.3533e-08, 4.2698e-07, 2.7900e-07, 2.6351e-07,\n",
      "          1.4661e-06, 1.0553e-06, 7.4727e-06, 2.1374e-05, 1.7534e-07,\n",
      "          3.7390e-05, 1.8059e-06, 2.7060e-05, 2.7162e-05, 3.9285e-08,\n",
      "          4.0395e-05, 8.2918e-07, 3.0717e-01, 6.8332e-01, 9.3062e-03,\n",
      "          5.5256e-06, 5.1112e-08, 8.4656e-06, 5.1949e-06, 1.9299e-07,\n",
      "          1.0339e-06, 5.4831e-08, 4.6169e-06, 2.0828e-06, 8.3460e-07,\n",
      "          5.5472e-07, 1.4232e-07, 1.2454e-06, 1.6265e-06, 6.9150e-07,\n",
      "          1.0312e-06, 3.8186e-07, 1.8274e-07, 1.1903e-07, 6.6834e-08,\n",
      "          5.1701e-08, 5.2414e-08, 8.9849e-08, 4.1079e-08, 3.4422e-08,\n",
      "          3.2977e-08],\n",
      "         [8.9167e-06, 2.1482e-05, 1.5633e-05, 1.3233e-05, 3.6461e-05,\n",
      "          1.9953e-05, 3.7859e-05, 2.5661e-04, 3.3213e-04, 8.2423e-05,\n",
      "          4.0955e-04, 2.6204e-04, 1.7262e-02, 4.3542e-02, 4.3363e-05,\n",
      "          2.2787e-04, 4.3578e-04, 6.5453e-02, 9.2370e-02, 2.5955e-05,\n",
      "          6.7645e-04, 1.5732e-04, 8.2141e-02, 1.0435e-01, 1.5971e-04,\n",
      "          3.4783e-01, 1.8521e-05, 6.3664e-02, 6.1184e-02, 1.7418e-04,\n",
      "          5.7074e-04, 1.8197e-05, 5.2910e-02, 3.4983e-02, 3.2770e-04,\n",
      "          1.0489e-04, 4.5253e-05, 1.8953e-02, 1.0000e-02, 2.0136e-04,\n",
      "          2.1966e-04, 8.0365e-05, 1.4149e-04, 1.2096e-04, 1.9586e-05,\n",
      "          1.1891e-05, 1.3120e-05, 3.1765e-05, 1.5502e-05, 1.3945e-05,\n",
      "          9.8621e-06]]])\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.4120e-01, 1.2324e-05, 1.5879e-01]])\n",
      "Player 0 Prediction: tensor([[[2.6838e-06, 2.6816e-06, 2.7091e-06, 3.1708e-06, 7.4819e-06,\n",
      "          3.5427e-06, 9.2616e-06, 1.0399e-03, 1.0477e-03, 1.1738e-04,\n",
      "          3.6442e-03, 8.0097e-04, 7.4614e-03, 6.7929e-03, 6.4061e-05,\n",
      "          1.8958e-04, 8.9673e-04, 2.1418e-01, 2.3595e-01, 5.8513e-06,\n",
      "          5.7366e-04, 3.8145e-04, 2.2059e-03, 1.1735e-03, 6.8461e-07,\n",
      "          3.2882e-01, 9.3047e-06, 3.7291e-04, 2.4130e-04, 1.3747e-04,\n",
      "          4.7983e-04, 1.0369e-05, 9.5136e-02, 8.9772e-02, 6.8041e-04,\n",
      "          1.2351e-05, 6.9017e-05, 3.2121e-03, 2.4051e-03, 2.2412e-04,\n",
      "          1.2757e-03, 4.6392e-05, 2.3470e-04, 2.6404e-04, 4.9332e-06,\n",
      "          9.3178e-06, 6.2594e-06, 6.0319e-06, 4.2571e-06, 4.1462e-06,\n",
      "          6.1160e-06],\n",
      "         [9.9623e-07, 6.5429e-07, 9.9747e-07, 8.9732e-07, 1.3075e-06,\n",
      "          8.0673e-07, 2.1133e-06, 1.0492e-02, 2.3352e-02, 8.9039e-06,\n",
      "          8.8298e-05, 4.9579e-05, 2.0192e-01, 2.3226e-01, 3.6288e-06,\n",
      "          3.3624e-06, 3.8436e-05, 1.0452e-02, 1.0936e-02, 1.5363e-06,\n",
      "          1.0922e-04, 5.7164e-05, 1.0844e-05, 1.4783e-05, 5.9645e-07,\n",
      "          1.4316e-01, 1.3823e-06, 4.9397e-04, 4.4933e-04, 3.1033e-05,\n",
      "          1.0319e-04, 1.9471e-06, 7.1708e-02, 7.1725e-02, 4.0830e-05,\n",
      "          3.2059e-07, 4.0923e-06, 1.1755e-01, 9.7129e-02, 1.1285e-05,\n",
      "          4.8093e-06, 9.0391e-06, 5.2940e-03, 2.4629e-03, 9.9653e-07,\n",
      "          1.6751e-06, 2.7154e-06, 2.0294e-06, 1.6273e-06, 3.6959e-06,\n",
      "          1.5147e-06],\n",
      "         [4.5840e-07, 3.3455e-07, 4.7257e-07, 2.2396e-07, 5.8612e-07,\n",
      "          5.8419e-07, 9.1614e-07, 2.2111e-05, 1.5060e-05, 1.9241e-06,\n",
      "          4.3392e-06, 3.2370e-06, 6.4799e-04, 1.1140e-03, 1.8687e-06,\n",
      "          3.8724e-05, 8.5162e-06, 1.0042e-02, 1.0878e-02, 2.9106e-07,\n",
      "          8.7941e-05, 7.2690e-06, 5.5376e-01, 4.2244e-01, 2.6031e-04,\n",
      "          1.2161e-04, 1.0437e-06, 1.0375e-05, 9.3632e-06, 1.1815e-06,\n",
      "          1.9357e-06, 4.0887e-07, 2.0178e-04, 1.4668e-04, 4.0542e-06,\n",
      "          8.6210e-08, 1.0173e-06, 4.3163e-05, 9.8106e-05, 1.2958e-06,\n",
      "          1.4527e-06, 1.7216e-06, 8.7521e-06, 7.1420e-06, 4.8263e-07,\n",
      "          6.1351e-07, 4.4650e-07, 1.4816e-06, 3.6884e-07, 6.3351e-07,\n",
      "          4.0706e-07],\n",
      "         [4.6826e-06, 4.4575e-06, 5.2461e-06, 7.7630e-06, 1.2141e-05,\n",
      "          1.0803e-05, 1.5720e-05, 2.8529e-03, 3.0637e-03, 3.2529e-05,\n",
      "          9.8123e-05, 5.4305e-05, 3.4233e-02, 6.3574e-02, 1.8516e-05,\n",
      "          8.9384e-06, 9.6238e-05, 7.8803e-02, 1.1038e-01, 1.0308e-05,\n",
      "          6.3938e-05, 5.1287e-05, 1.2516e-02, 9.8722e-03, 9.5060e-06,\n",
      "          5.2156e-01, 9.7627e-06, 3.2585e-03, 2.9507e-03, 3.0717e-05,\n",
      "          3.6570e-05, 7.2817e-06, 5.0304e-02, 6.1452e-02, 5.7005e-05,\n",
      "          7.8074e-07, 4.2844e-05, 2.4910e-02, 1.7420e-02, 3.9047e-05,\n",
      "          2.3690e-05, 1.6514e-05, 1.3350e-03, 6.6738e-04, 1.6380e-05,\n",
      "          1.0005e-05, 9.5625e-06, 2.1111e-05, 9.2665e-06, 1.1154e-05,\n",
      "          4.6907e-06]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27323\n",
      "Average episode length: 2.7 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4833/10000 (48.3%)\n",
      "    Average reward: +0.097\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5167/10000 (51.7%)\n",
      "    Average reward: -0.097\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5613 (40.1%)\n",
      "    Action 1: 2826 (20.2%)\n",
      "    Action 2: 4509 (32.2%)\n",
      "    Action 3: 1054 (7.5%)\n",
      "  Player 1:\n",
      "    Action 0: 3495 (26.2%)\n",
      "    Action 1: 5272 (39.6%)\n",
      "    Action 2: 2805 (21.1%)\n",
      "    Action 3: 1749 (13.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [968.5, -968.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.995 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.036 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0969\n",
      "   Testing specific player: 1\n",
      "   At training step: 40000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000e+00, 8.3183e-01, 9.3061e-05, 1.6808e-01]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5585, 0.0033, 0.4381]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 40000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 39920\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6953/10000 (69.5%)\n",
      "    Average reward: -0.173\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3047/10000 (30.5%)\n",
      "    Average reward: +0.173\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14634 (74.2%)\n",
      "    Action 1: 5090 (25.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3813 (18.9%)\n",
      "    Action 1: 9606 (47.6%)\n",
      "    Action 2: 3847 (19.0%)\n",
      "    Action 3: 2930 (14.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1726.0, 1726.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.824 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.964 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.894\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1726\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}, {'exploitability': 0.176975}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41002/50000 [59:20<07:07, 21.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 41000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 252595/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 250487/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41998/50000 [1:00:06<06:11, 21.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 42000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 258935/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 257056/2000000\n",
      "P1 SL Buffer Size:  258935\n",
      "P1 SL buffer distribution [82972. 96940. 64029. 14994.]\n",
      "P1 actions distribution [0.32043563 0.37437967 0.24727827 0.05790642]\n",
      "P2 SL Buffer Size:  257056\n",
      "P2 SL buffer distribution [ 66758. 106088.  63871.  20339.]\n",
      "P2 actions distribution [0.25970217 0.41270385 0.24847115 0.07912284]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41998/50000 [1:00:20<06:11, 21.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.7975, 0.1859, 0.0166, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[1.4588e-05, 1.4758e-05, 1.7788e-05, 1.4602e-05, 1.1834e-05,\n",
      "          9.1625e-06, 1.2255e-05, 1.1630e-04, 1.3799e-04, 3.8989e-04,\n",
      "          6.8425e-02, 4.5169e-03, 1.5090e-04, 2.6937e-04, 1.4924e-04,\n",
      "          4.2222e-02, 6.9352e-03, 1.6040e-02, 2.0409e-02, 2.8327e-05,\n",
      "          1.1623e-01, 3.7174e-03, 4.0363e-01, 1.7155e-01, 2.1327e-05,\n",
      "          1.2935e-02, 1.1013e-05, 5.6337e-03, 7.2995e-03, 1.2451e-03,\n",
      "          2.9896e-02, 2.0426e-05, 1.1783e-02, 1.2007e-02, 3.0218e-03,\n",
      "          4.8010e-02, 4.3198e-05, 2.9975e-04, 1.7902e-04, 2.1691e-03,\n",
      "          9.6205e-03, 4.0023e-04, 1.8062e-04, 1.1615e-04, 1.3253e-05,\n",
      "          9.8280e-06, 1.2679e-05, 1.3481e-05, 1.2095e-05, 1.6017e-05,\n",
      "          8.8261e-06],\n",
      "         [1.1349e-05, 1.2016e-05, 1.5379e-05, 2.1154e-05, 1.1656e-05,\n",
      "          1.5959e-05, 1.1371e-05, 1.2878e-03, 7.0900e-03, 3.1970e-04,\n",
      "          1.6071e-02, 4.5922e-03, 1.5785e-02, 3.6411e-02, 5.8755e-05,\n",
      "          2.9714e-02, 4.8628e-03, 3.2269e-02, 4.1327e-02, 2.2661e-05,\n",
      "          3.0581e-01, 7.5298e-03, 7.9379e-04, 4.4837e-04, 6.1082e-05,\n",
      "          1.0234e-02, 2.5145e-05, 2.1388e-01, 2.0966e-01, 6.5823e-04,\n",
      "          8.9099e-03, 2.0388e-05, 1.4027e-02, 1.1591e-02, 1.5804e-03,\n",
      "          5.3242e-03, 2.9141e-05, 6.5036e-03, 3.0816e-03, 1.2590e-03,\n",
      "          2.9019e-03, 4.4677e-04, 4.5476e-03, 6.5443e-04, 8.6037e-06,\n",
      "          1.2427e-05, 1.4015e-05, 1.3863e-05, 2.1090e-05, 1.5291e-05,\n",
      "          1.9066e-05],\n",
      "         [3.8826e-08, 3.1912e-08, 4.9739e-08, 3.2443e-08, 2.8132e-08,\n",
      "          2.7473e-08, 3.7940e-08, 2.2449e-07, 1.6430e-07, 2.6007e-07,\n",
      "          1.2660e-06, 1.1852e-06, 9.1738e-06, 8.3204e-06, 9.7562e-08,\n",
      "          4.8580e-05, 6.2610e-07, 1.6032e-05, 2.0503e-05, 3.9496e-08,\n",
      "          4.5558e-05, 1.1236e-06, 4.9024e-01, 4.9450e-01, 1.5094e-02,\n",
      "          3.5376e-07, 3.9024e-08, 2.8076e-06, 2.5596e-06, 6.4066e-07,\n",
      "          7.6023e-07, 4.9798e-08, 1.8703e-06, 1.9838e-06, 7.6401e-07,\n",
      "          3.2630e-07, 1.1446e-07, 7.1151e-07, 1.0351e-06, 3.2794e-07,\n",
      "          5.1557e-07, 2.8197e-07, 1.8949e-07, 1.1907e-07, 4.0397e-08,\n",
      "          4.6437e-08, 4.7424e-08, 3.4434e-08, 5.7094e-08, 3.5840e-08,\n",
      "          4.5676e-08],\n",
      "         [9.8071e-06, 2.5983e-05, 2.5622e-05, 2.2460e-05, 1.9184e-05,\n",
      "          9.4850e-06, 2.2707e-05, 9.4887e-05, 1.5948e-04, 1.0994e-04,\n",
      "          5.8015e-04, 2.7314e-04, 3.5130e-02, 5.6699e-02, 5.5554e-05,\n",
      "          7.2937e-04, 5.2318e-04, 9.1571e-02, 1.5162e-01, 2.6934e-05,\n",
      "          1.5953e-03, 6.6356e-04, 1.8232e-01, 2.5720e-01, 1.5835e-04,\n",
      "          4.5729e-02, 1.1537e-05, 2.9150e-02, 2.6037e-02, 1.9432e-04,\n",
      "          2.7578e-04, 2.4378e-05, 4.3849e-02, 3.3101e-02, 3.0794e-04,\n",
      "          1.7516e-04, 3.2209e-05, 2.3572e-02, 1.7195e-02, 1.3538e-04,\n",
      "          1.2051e-04, 1.5367e-04, 1.0875e-04, 7.2720e-05, 2.0159e-05,\n",
      "          1.4568e-05, 1.8266e-05, 1.7911e-05, 1.7952e-05, 1.3248e-05,\n",
      "          1.0151e-05]]])\n",
      "Player 0 Prediction: tensor([[0.1568, 0.0425, 0.8007, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30508\n",
      "Average episode length: 3.1 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5028/10000 (50.3%)\n",
      "    Average reward: -0.235\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4972/10000 (49.7%)\n",
      "    Average reward: +0.235\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5430 (35.2%)\n",
      "    Action 1: 5305 (34.4%)\n",
      "    Action 2: 3614 (23.4%)\n",
      "    Action 3: 1064 (6.9%)\n",
      "  Player 1:\n",
      "    Action 0: 2771 (18.4%)\n",
      "    Action 1: 6658 (44.1%)\n",
      "    Action 2: 3673 (24.3%)\n",
      "    Action 3: 1993 (13.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2354.0, 2354.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.060 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.970 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.015\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2354\n",
      "   Testing specific player: 0\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0391, 0.0114, 0.9495, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42304\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4232/10000 (42.3%)\n",
      "    Average reward: +0.080\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5768/10000 (57.7%)\n",
      "    Average reward: -0.080\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4453 (20.9%)\n",
      "    Action 1: 10366 (48.6%)\n",
      "    Action 2: 3804 (17.8%)\n",
      "    Action 3: 2715 (12.7%)\n",
      "  Player 1:\n",
      "    Action 0: 15341 (73.2%)\n",
      "    Action 1: 5625 (26.8%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [804.5, -804.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.978 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.839 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.908\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.0804\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0423, 0.0158, 0.9419, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28317\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5035/10000 (50.3%)\n",
      "    Average reward: +0.139\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4965/10000 (49.6%)\n",
      "    Average reward: -0.139\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5305 (37.2%)\n",
      "    Action 1: 4038 (28.3%)\n",
      "    Action 2: 4501 (31.5%)\n",
      "    Action 3: 428 (3.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3453 (24.6%)\n",
      "    Action 1: 5651 (40.2%)\n",
      "    Action 2: 3294 (23.5%)\n",
      "    Action 3: 1647 (11.7%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1387.5, -1387.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.046 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.036\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1388\n",
      "   Testing specific player: 1\n",
      "   At training step: 42000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0423, 0.0158, 0.9419, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 42000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40248\n",
      "Average episode length: 4.0 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6946/10000 (69.5%)\n",
      "    Average reward: -0.145\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3054/10000 (30.5%)\n",
      "    Average reward: +0.145\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 14902 (74.6%)\n",
      "    Action 1: 5067 (25.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3683 (18.2%)\n",
      "    Action 1: 9848 (48.6%)\n",
      "    Action 2: 3812 (18.8%)\n",
      "    Action 3: 2936 (14.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1446.5, 1446.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.817 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.953 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.885\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1447\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}, {'exploitability': 0.176975}, {'exploitability': 0.187075}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43005/50000 [1:01:28<05:26, 21.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 43000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 265239/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 263515/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43998/50000 [1:02:18<04:54, 20.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 44000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 271455/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 270053/2000000\n",
      "P1 SL Buffer Size:  271455\n",
      "P1 SL buffer distribution [87922. 99627. 68137. 15769.]\n",
      "P1 actions distribution [0.32389162 0.36701111 0.25100661 0.05809066]\n",
      "P2 SL Buffer Size:  270053\n",
      "P2 SL buffer distribution [ 69072. 112363.  67064.  21554.]\n",
      "P2 actions distribution [0.25577202 0.41607758 0.24833644 0.07981396]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43998/50000 [1:02:31<04:54, 20.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[0.0356, 0.0087, 0.9557, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29171\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4956/10000 (49.6%)\n",
      "    Average reward: -0.226\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5044/10000 (50.4%)\n",
      "    Average reward: +0.226\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5695 (38.4%)\n",
      "    Action 1: 4558 (30.7%)\n",
      "    Action 2: 3918 (26.4%)\n",
      "    Action 3: 679 (4.6%)\n",
      "  Player 1:\n",
      "    Action 0: 2399 (16.8%)\n",
      "    Action 1: 7010 (48.9%)\n",
      "    Action 2: 3737 (26.1%)\n",
      "    Action 3: 1175 (8.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2261.5, 2261.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.053 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.936 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.995\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2261\n",
      "   Testing specific player: 0\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.8215, 0.1614, 0.0170, 0.0000]])\n",
      "Player 0 Prediction: tensor([[0.1434, 0.0353, 0.8213, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42223\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4190/10000 (41.9%)\n",
      "    Average reward: +0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5810/10000 (58.1%)\n",
      "    Average reward: -0.108\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4640 (21.8%)\n",
      "    Action 1: 10047 (47.1%)\n",
      "    Action 2: 3895 (18.3%)\n",
      "    Action 3: 2748 (12.9%)\n",
      "  Player 1:\n",
      "    Action 0: 15059 (72.1%)\n",
      "    Action 1: 5834 (27.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1079.5, -1079.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.990 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.854 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.922\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1080\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[2.2246e-05, 5.5740e-05, 2.9096e-05, 4.3233e-05, 5.8508e-05,\n",
      "          3.0063e-05, 3.9187e-05, 1.1387e-03, 2.5635e-03, 1.2413e-03,\n",
      "          9.1885e-03, 4.2600e-03, 1.3901e-02, 4.1048e-02, 9.3317e-04,\n",
      "          1.5371e-02, 4.1425e-03, 1.1623e-02, 1.8135e-02, 1.0948e-04,\n",
      "          7.1608e-03, 1.4158e-03, 2.2492e-01, 2.8465e-01, 1.5612e-04,\n",
      "          8.0495e-02, 6.5804e-05, 7.6388e-02, 8.2149e-02, 3.7573e-03,\n",
      "          2.5182e-02, 1.1061e-04, 2.3104e-02, 1.3618e-02, 2.6597e-03,\n",
      "          9.0311e-03, 4.5248e-04, 2.4856e-02, 6.7969e-03, 1.7784e-03,\n",
      "          4.2375e-03, 1.5839e-03, 6.9560e-04, 4.7279e-04, 3.6617e-05,\n",
      "          4.7155e-05, 3.4029e-05, 6.5073e-05, 5.8322e-05, 4.3489e-05,\n",
      "          4.2606e-05],\n",
      "         [1.9981e-05, 1.2015e-05, 8.6505e-06, 1.9721e-05, 1.4342e-05,\n",
      "          1.5551e-05, 1.0653e-05, 8.7852e-03, 4.7929e-02, 1.4897e-04,\n",
      "          6.1962e-03, 1.8178e-03, 4.6075e-02, 1.6927e-01, 5.6104e-05,\n",
      "          6.2947e-03, 2.7893e-03, 5.9568e-04, 1.0914e-03, 1.6393e-05,\n",
      "          5.8357e-02, 2.4423e-03, 8.4958e-05, 2.7604e-04, 1.2502e-04,\n",
      "          7.1731e-02, 2.6976e-05, 2.3900e-01, 2.5880e-01, 8.8062e-04,\n",
      "          6.7005e-03, 1.8287e-05, 7.8442e-04, 7.2880e-04, 1.5427e-03,\n",
      "          4.3932e-03, 3.3584e-05, 4.1246e-02, 6.9927e-03, 1.2549e-03,\n",
      "          2.8491e-03, 1.1663e-04, 8.0675e-03, 2.2680e-03, 2.3452e-05,\n",
      "          1.2063e-05, 1.3865e-05, 1.3088e-05, 2.5618e-05, 2.0824e-05,\n",
      "          1.6164e-05],\n",
      "         [2.7685e-07, 1.6418e-07, 1.9827e-07, 1.9091e-07, 2.4333e-07,\n",
      "          2.8993e-07, 2.7141e-07, 2.7728e-06, 2.1660e-06, 1.5521e-06,\n",
      "          2.9847e-06, 3.2077e-06, 4.2704e-05, 1.5535e-04, 8.4761e-07,\n",
      "          4.2622e-05, 3.1442e-06, 9.4616e-05, 8.2436e-05, 2.7277e-07,\n",
      "          1.6307e-04, 2.2180e-06, 2.8919e-03, 2.3326e-01, 7.6315e-01,\n",
      "          7.5065e-06, 1.5597e-07, 2.9075e-05, 2.0309e-05, 1.5166e-06,\n",
      "          2.7875e-06, 2.0618e-07, 3.9885e-06, 2.6752e-06, 1.2458e-06,\n",
      "          1.6191e-06, 5.4701e-07, 9.6674e-06, 6.7439e-06, 2.2105e-06,\n",
      "          2.3537e-06, 1.7179e-06, 1.1470e-06, 5.9141e-07, 2.2618e-07,\n",
      "          2.2872e-07, 2.7431e-07, 3.7825e-07, 1.8443e-07, 1.0659e-07,\n",
      "          1.0722e-07],\n",
      "         [3.2452e-05, 1.0274e-04, 7.7303e-05, 4.8078e-05, 8.3334e-05,\n",
      "          4.4079e-05, 8.3145e-05, 1.2760e-03, 3.3151e-03, 3.2392e-04,\n",
      "          6.1532e-04, 5.6328e-04, 7.1637e-02, 2.4475e-01, 2.6432e-04,\n",
      "          4.3656e-04, 9.5487e-04, 8.0372e-02, 8.7104e-02, 1.0490e-04,\n",
      "          1.7696e-03, 7.5758e-04, 2.0981e-02, 8.4761e-02, 2.0152e-03,\n",
      "          1.7914e-01, 8.5669e-05, 5.3741e-02, 4.7869e-02, 8.9621e-04,\n",
      "          1.3414e-03, 5.6003e-05, 3.4052e-02, 1.8247e-02, 5.4048e-04,\n",
      "          3.9205e-04, 1.5613e-04, 3.6977e-02, 2.1425e-02, 1.5819e-04,\n",
      "          3.2640e-04, 3.4958e-04, 8.5830e-04, 5.7870e-04, 5.1801e-05,\n",
      "          2.5502e-05, 5.8876e-05, 7.9845e-05, 5.7055e-05, 3.3845e-05,\n",
      "          2.5996e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.3861, 0.2511, 0.3628]])\n",
      "Player 0 Prediction: tensor([[[4.0772e-06, 1.6266e-05, 6.0689e-06, 9.4372e-06, 8.9908e-06,\n",
      "          6.1232e-06, 1.4153e-05, 9.3357e-05, 9.7216e-05, 1.7707e-04,\n",
      "          8.2567e-02, 5.6069e-03, 3.6647e-04, 5.8199e-04, 1.2382e-04,\n",
      "          3.8962e-02, 5.6216e-03, 2.0096e-02, 2.6104e-02, 2.2739e-05,\n",
      "          7.2175e-03, 2.1060e-03, 1.1181e-01, 4.7416e-02, 4.7780e-06,\n",
      "          4.2592e-01, 9.7647e-06, 1.5838e-02, 1.7457e-02, 1.9108e-03,\n",
      "          9.8766e-03, 1.9580e-05, 5.3073e-02, 3.9245e-02, 5.6422e-03,\n",
      "          7.5853e-03, 6.9350e-05, 6.5782e-04, 3.5054e-04, 3.6066e-03,\n",
      "          6.9342e-02, 2.2235e-04, 2.7198e-05, 4.1283e-05, 1.0706e-05,\n",
      "          1.5730e-05, 3.8039e-06, 1.0159e-05, 1.2284e-05, 1.2342e-05,\n",
      "          7.0900e-06],\n",
      "         [8.5433e-06, 2.5660e-06, 4.7005e-06, 6.6561e-06, 3.0914e-06,\n",
      "          5.5579e-06, 5.6918e-06, 5.8876e-03, 1.3253e-02, 3.3735e-05,\n",
      "          2.3946e-03, 5.2504e-04, 6.0931e-02, 1.2604e-01, 1.0435e-05,\n",
      "          7.8381e-04, 9.7101e-04, 8.1590e-03, 1.2232e-02, 4.7198e-06,\n",
      "          1.2775e-02, 2.3408e-03, 1.7181e-04, 1.7516e-04, 8.3296e-06,\n",
      "          2.6322e-01, 5.8722e-06, 1.2182e-01, 1.2041e-01, 5.1874e-04,\n",
      "          2.9854e-03, 6.1115e-06, 6.3876e-02, 6.1938e-02, 6.9471e-04,\n",
      "          3.4977e-04, 7.6699e-06, 7.8907e-02, 3.2780e-02, 5.6432e-04,\n",
      "          9.5538e-04, 2.2882e-05, 2.9775e-03, 1.1775e-03, 1.0816e-05,\n",
      "          4.2976e-06, 3.2504e-06, 4.6078e-06, 8.4697e-06, 1.3200e-05,\n",
      "          2.7256e-06],\n",
      "         [2.3240e-08, 1.8627e-08, 1.7409e-08, 1.5922e-08, 1.9438e-08,\n",
      "          2.8796e-08, 3.1071e-08, 2.3922e-07, 6.6655e-08, 1.3786e-07,\n",
      "          1.0972e-06, 4.6773e-07, 3.3991e-06, 8.3613e-06, 7.7591e-08,\n",
      "          1.9034e-05, 4.1482e-07, 2.5059e-05, 2.4308e-05, 2.4659e-08,\n",
      "          1.1017e-05, 4.8398e-07, 5.0822e-01, 4.8952e-01, 2.1431e-03,\n",
      "          6.1270e-06, 1.5455e-08, 4.9793e-06, 3.1201e-06, 2.4583e-07,\n",
      "          2.5427e-07, 1.0283e-08, 4.1406e-06, 2.4323e-06, 2.9839e-07,\n",
      "          1.3966e-07, 3.4169e-08, 7.7989e-07, 1.2330e-06, 4.0890e-07,\n",
      "          8.5040e-07, 1.0826e-07, 6.7711e-08, 7.4598e-08, 3.5043e-08,\n",
      "          1.9885e-08, 1.3236e-08, 3.0837e-08, 1.4204e-08, 1.4899e-08,\n",
      "          6.7089e-09],\n",
      "         [3.4319e-06, 1.1539e-05, 9.7693e-06, 7.0015e-06, 8.2033e-06,\n",
      "          8.6986e-06, 1.4247e-05, 1.4878e-04, 1.4703e-04, 2.6735e-05,\n",
      "          2.1811e-04, 8.6065e-05, 1.2758e-02, 3.4932e-02, 2.0115e-05,\n",
      "          5.6152e-05, 1.3073e-04, 5.2556e-02, 6.1689e-02, 1.4487e-05,\n",
      "          1.0800e-04, 1.3908e-04, 5.9837e-02, 5.7058e-02, 4.7619e-05,\n",
      "          4.9255e-01, 1.0297e-05, 5.3263e-02, 5.3502e-02, 1.1287e-04,\n",
      "          1.0668e-04, 4.2111e-06, 5.5200e-02, 4.5059e-02, 1.0606e-04,\n",
      "          2.3269e-05, 1.6363e-05, 1.3045e-02, 6.6723e-03, 6.5197e-05,\n",
      "          9.0971e-05, 2.8922e-05, 3.8759e-05, 3.7847e-05, 8.0172e-06,\n",
      "          2.7973e-06, 5.8282e-06, 7.8308e-06, 4.4339e-06, 5.1529e-06,\n",
      "          1.9997e-06]]])\n",
      "Player 1 Prediction: tensor([[0.3054, 0.1325, 0.5620, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27502\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4634/10000 (46.3%)\n",
      "    Average reward: +0.068\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5366/10000 (53.7%)\n",
      "    Average reward: -0.068\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5429 (38.5%)\n",
      "    Action 1: 3302 (23.4%)\n",
      "    Action 2: 4947 (35.1%)\n",
      "    Action 3: 417 (3.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3342 (24.9%)\n",
      "    Action 1: 5481 (40.9%)\n",
      "    Action 2: 2928 (21.8%)\n",
      "    Action 3: 1656 (12.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [679.5, -679.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.021 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.027 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.024\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0679\n",
      "   Testing specific player: 1\n",
      "   At training step: 44000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[4.7214e-01, 5.2754e-01, 3.2079e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5730, 0.0023, 0.4247]])\n",
      "Player 1 Prediction: tensor([[0.8281, 0.1653, 0.0066, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 44000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40890\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6893/10000 (68.9%)\n",
      "    Average reward: -0.144\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3107/10000 (31.1%)\n",
      "    Average reward: +0.144\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15334 (75.1%)\n",
      "    Action 1: 5074 (24.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3546 (17.3%)\n",
      "    Action 1: 10236 (50.0%)\n",
      "    Action 2: 3657 (17.9%)\n",
      "    Action 3: 3043 (14.9%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1436.0, 1436.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.809 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.938 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.874\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1436\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}, {'exploitability': 0.176975}, {'exploitability': 0.187075}, {'exploitability': 0.14705}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45004/50000 [1:03:39<04:17, 19.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 45000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 277679/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 276424/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/50000 [1:04:28<03:10, 20.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 46000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 283926/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 282935/2000000\n",
      "P1 SL Buffer Size:  283926\n",
      "P1 SL buffer distribution [ 92632. 102392.  72475.  16427.]\n",
      "P1 actions distribution [0.32625402 0.36062918 0.25526017 0.05785662]\n",
      "P2 SL Buffer Size:  282935\n",
      "P2 SL buffer distribution [ 71322. 118508.  70219.  22886.]\n",
      "P2 actions distribution [0.2520791  0.41885239 0.24818068 0.08088784]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/50000 [1:04:41<03:10, 20.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 1 Prediction: tensor([[[3.8096e-05, 2.7408e-05, 3.7580e-05, 5.3917e-05, 4.6530e-05,\n",
      "          6.2943e-05, 5.4353e-05, 1.1441e-03, 2.5728e-03, 2.1451e-03,\n",
      "          1.0889e-02, 5.3265e-03, 7.4188e-03, 1.7352e-02, 7.1017e-04,\n",
      "          2.6941e-02, 5.6008e-03, 8.0261e-03, 1.2422e-02, 1.2678e-04,\n",
      "          2.3912e-02, 2.8105e-03, 2.5361e-01, 2.6351e-01, 2.5066e-04,\n",
      "          7.8963e-02, 6.6670e-05, 7.2632e-02, 6.6689e-02, 3.6830e-03,\n",
      "          5.1206e-02, 9.6246e-05, 1.8635e-02, 1.4129e-02, 3.3794e-03,\n",
      "          1.6227e-02, 1.7615e-04, 1.3838e-02, 6.3916e-03, 1.7125e-03,\n",
      "          2.4005e-03, 1.5891e-03, 1.9091e-03, 8.8117e-04, 4.5950e-05,\n",
      "          4.1080e-05, 4.8167e-05, 4.4279e-05, 5.2089e-05, 5.2171e-05,\n",
      "          2.8115e-05],\n",
      "         [2.2271e-05, 1.8978e-05, 1.5196e-05, 2.3319e-05, 2.0820e-05,\n",
      "          1.9341e-05, 2.3455e-05, 3.5983e-03, 1.5529e-02, 4.6508e-04,\n",
      "          2.1531e-02, 8.0270e-03, 8.0928e-03, 2.6477e-02, 5.7785e-05,\n",
      "          2.3194e-02, 3.3428e-03, 1.0675e-03, 2.1661e-03, 3.2081e-05,\n",
      "          2.1663e-01, 6.8174e-03, 1.0219e-04, 2.5275e-04, 1.5672e-04,\n",
      "          4.0646e-02, 4.0038e-05, 2.6606e-01, 2.9998e-01, 8.7987e-04,\n",
      "          9.0880e-03, 2.9917e-05, 2.0317e-03, 1.3298e-03, 1.9429e-03,\n",
      "          1.3587e-02, 2.8123e-05, 9.6725e-03, 2.7186e-03, 1.7025e-03,\n",
      "          5.8064e-03, 5.1656e-04, 4.8444e-03, 1.2758e-03, 1.6967e-05,\n",
      "          1.6103e-05, 1.6503e-05, 2.0987e-05, 2.2303e-05, 1.6416e-05,\n",
      "          2.9036e-05],\n",
      "         [3.2838e-07, 2.9637e-07, 2.7846e-07, 2.4952e-07, 2.2917e-07,\n",
      "          5.2076e-07, 4.7960e-07, 3.0570e-06, 4.9981e-06, 2.5236e-06,\n",
      "          5.0554e-06, 6.4694e-06, 7.2607e-05, 8.4793e-05, 6.5849e-07,\n",
      "          9.7920e-05, 3.2964e-06, 8.5494e-05, 7.2804e-05, 4.1427e-07,\n",
      "          4.4817e-04, 4.3169e-06, 4.6900e-03, 2.1084e-01, 7.8343e-01,\n",
      "          1.2241e-05, 6.0308e-07, 3.4760e-05, 2.6795e-05, 5.2704e-06,\n",
      "          5.2078e-06, 4.8079e-07, 7.4707e-06, 9.3325e-06, 4.5038e-06,\n",
      "          3.8903e-06, 5.4910e-07, 9.7254e-06, 1.3253e-05, 1.9671e-06,\n",
      "          3.8218e-06, 3.1350e-06, 2.9309e-06, 1.4988e-06, 3.4691e-07,\n",
      "          3.9953e-07, 2.9091e-07, 2.7315e-07, 3.8487e-07, 2.6077e-07,\n",
      "          5.2643e-07],\n",
      "         [4.2945e-05, 5.8454e-05, 5.9243e-05, 9.3128e-05, 7.0034e-05,\n",
      "          7.0192e-05, 6.9259e-05, 7.1746e-04, 1.0612e-03, 2.2038e-04,\n",
      "          1.0389e-03, 8.9727e-04, 6.8718e-02, 1.8886e-01, 2.0656e-04,\n",
      "          1.3325e-03, 7.4494e-04, 7.3007e-02, 1.5426e-01, 9.3815e-05,\n",
      "          1.7624e-03, 8.0028e-04, 1.0297e-02, 4.1578e-02, 1.0818e-03,\n",
      "          1.9724e-01, 7.4960e-05, 3.4959e-02, 2.6192e-02, 3.5732e-04,\n",
      "          1.0955e-03, 9.7505e-05, 3.7901e-02, 2.8035e-02, 5.6537e-04,\n",
      "          5.4446e-04, 5.5750e-05, 8.7731e-02, 3.5832e-02, 3.0212e-04,\n",
      "          3.3106e-04, 3.4686e-04, 5.6812e-04, 2.9096e-04, 7.1425e-05,\n",
      "          3.5047e-05, 5.3383e-05, 5.1732e-05, 5.6364e-05, 3.8108e-05,\n",
      "          3.3904e-05]]])\n",
      "Player 0 Prediction: tensor([[0.0521, 0.0437, 0.9042, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 29377\n",
      "Average episode length: 2.9 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4922/10000 (49.2%)\n",
      "    Average reward: -0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5078/10000 (50.8%)\n",
      "    Average reward: +0.240\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5909 (39.3%)\n",
      "    Action 1: 4467 (29.7%)\n",
      "    Action 2: 3970 (26.4%)\n",
      "    Action 3: 673 (4.5%)\n",
      "  Player 1:\n",
      "    Action 0: 2258 (15.7%)\n",
      "    Action 1: 7140 (49.7%)\n",
      "    Action 2: 3807 (26.5%)\n",
      "    Action 3: 1153 (8.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2396.0, 2396.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.050 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.921 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.985\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2396\n",
      "   Testing specific player: 0\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.5325, 0.1244, 0.3431]])\n",
      "Player 0 Prediction: tensor([[0.3757, 0.0349, 0.5895, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 42067\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4286/10000 (42.9%)\n",
      "    Average reward: +0.168\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5714/10000 (57.1%)\n",
      "    Average reward: -0.168\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4679 (22.0%)\n",
      "    Action 1: 9984 (47.0%)\n",
      "    Action 2: 3881 (18.3%)\n",
      "    Action 3: 2718 (12.8%)\n",
      "  Player 1:\n",
      "    Action 0: 14979 (72.0%)\n",
      "    Action 1: 5826 (28.0%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1684.0, -1684.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.993 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.855 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.924\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1684\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[1.7528e-05, 1.1032e-04, 1.4749e-05, 1.0092e-04, 5.7302e-05,\n",
      "          6.3353e-05, 4.6227e-05, 7.6792e-04, 1.6028e-03, 7.1353e-04,\n",
      "          6.6929e-03, 2.3396e-03, 8.2291e-03, 2.1633e-02, 8.4672e-04,\n",
      "          1.9105e-02, 2.6893e-03, 2.0625e-02, 3.8120e-02, 1.0728e-04,\n",
      "          3.1827e-02, 1.0027e-03, 3.4840e-01, 3.4245e-01, 1.9757e-04,\n",
      "          1.6657e-02, 4.9498e-05, 3.7591e-02, 3.7762e-02, 2.3816e-03,\n",
      "          2.6684e-02, 6.6863e-05, 8.1604e-03, 4.5740e-03, 2.2372e-03,\n",
      "          5.8544e-03, 3.3541e-04, 2.7023e-03, 1.2211e-03, 1.7113e-03,\n",
      "          1.8774e-03, 1.4874e-03, 3.2997e-04, 2.0411e-04, 5.1862e-05,\n",
      "          1.7249e-05, 4.0598e-05, 6.7028e-05, 4.8915e-05, 2.6901e-05,\n",
      "          9.7942e-05],\n",
      "         [1.1953e-05, 3.4397e-05, 7.3145e-06, 3.2589e-05, 4.4804e-05,\n",
      "          2.5866e-05, 1.4886e-05, 3.8045e-03, 1.7799e-02, 9.8728e-05,\n",
      "          8.0521e-03, 2.2628e-03, 2.3802e-02, 7.4424e-02, 3.7313e-05,\n",
      "          1.8469e-02, 1.8608e-03, 1.9774e-03, 3.3586e-03, 3.0486e-05,\n",
      "          4.1973e-01, 2.5466e-03, 2.5817e-04, 4.2421e-04, 1.2699e-04,\n",
      "          1.3774e-02, 2.5272e-05, 1.9629e-01, 1.8194e-01, 6.7166e-04,\n",
      "          5.3155e-03, 6.9477e-06, 3.5120e-04, 2.2251e-04, 2.0336e-03,\n",
      "          5.2851e-03, 4.3362e-05, 5.7747e-03, 1.8587e-03, 9.1921e-04,\n",
      "          1.4477e-03, 2.4512e-04, 3.6480e-03, 7.1673e-04, 2.4856e-05,\n",
      "          7.8733e-06, 1.4095e-05, 3.6094e-05, 3.8651e-05, 3.1839e-05,\n",
      "          4.0981e-05],\n",
      "         [2.7751e-07, 4.1509e-07, 1.4596e-07, 3.6863e-07, 5.0972e-07,\n",
      "          8.2069e-07, 2.1388e-07, 2.6271e-06, 2.2341e-06, 2.5299e-06,\n",
      "          4.0183e-06, 2.8933e-06, 5.6550e-05, 9.4976e-05, 6.8088e-07,\n",
      "          9.4603e-05, 5.4668e-06, 9.8541e-05, 6.5553e-05, 6.0226e-07,\n",
      "          5.9724e-04, 2.7873e-06, 6.0195e-03, 3.0756e-01, 6.8529e-01,\n",
      "          3.2770e-06, 1.8981e-07, 2.6691e-05, 2.8720e-05, 8.8829e-07,\n",
      "          4.8629e-06, 2.7047e-07, 3.1752e-06, 1.9483e-06, 2.3433e-06,\n",
      "          1.5043e-06, 1.2405e-06, 4.9381e-06, 4.5067e-06, 3.5880e-06,\n",
      "          2.4818e-06, 2.6137e-06, 1.5514e-06, 5.6035e-07, 2.5975e-07,\n",
      "          1.8115e-07, 3.1900e-07, 8.8045e-07, 2.6686e-07, 2.7179e-07,\n",
      "          2.4927e-07],\n",
      "         [4.0438e-05, 3.1456e-04, 5.7426e-05, 8.5022e-05, 2.0999e-04,\n",
      "          7.0013e-05, 1.2098e-04, 1.4287e-03, 1.6123e-03, 3.2783e-04,\n",
      "          7.9522e-04, 5.7840e-04, 5.1182e-02, 1.6270e-01, 9.6369e-05,\n",
      "          1.3491e-03, 6.8195e-04, 9.1360e-02, 1.7368e-01, 1.5662e-04,\n",
      "          5.8533e-03, 4.5413e-04, 7.2619e-02, 2.6441e-01, 2.9478e-03,\n",
      "          4.2888e-02, 4.0110e-05, 4.0869e-02, 3.5887e-02, 4.1308e-04,\n",
      "          1.1884e-03, 3.4430e-05, 1.7401e-02, 8.3823e-03, 5.3521e-04,\n",
      "          3.4645e-04, 1.8982e-04, 9.8153e-03, 6.3320e-03, 1.6264e-04,\n",
      "          3.1813e-04, 3.3337e-04, 5.8442e-04, 5.6640e-04, 9.0129e-05,\n",
      "          3.1260e-05, 3.4297e-05, 2.1343e-04, 6.7345e-05, 8.2696e-05,\n",
      "          5.4369e-05]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 27842\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4701/10000 (47.0%)\n",
      "    Average reward: +0.105\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5299/10000 (53.0%)\n",
      "    Average reward: -0.105\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5775 (40.4%)\n",
      "    Action 1: 2995 (20.9%)\n",
      "    Action 2: 4845 (33.9%)\n",
      "    Action 3: 692 (4.8%)\n",
      "  Player 1:\n",
      "    Action 0: 3171 (23.4%)\n",
      "    Action 1: 5666 (41.9%)\n",
      "    Action 2: 2874 (21.2%)\n",
      "    Action 3: 1824 (13.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1048.0, -1048.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.016 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.009\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1048\n",
      "   Testing specific player: 1\n",
      "   At training step: 46000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4220, 0.2330, 0.3450]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.4653, 0.3060, 0.2287]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 46000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40531\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6958/10000 (69.6%)\n",
      "    Average reward: -0.143\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3042/10000 (30.4%)\n",
      "    Average reward: +0.143\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15037 (74.7%)\n",
      "    Action 1: 5080 (25.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3580 (17.5%)\n",
      "    Action 1: 9996 (49.0%)\n",
      "    Action 2: 3820 (18.7%)\n",
      "    Action 3: 3018 (14.8%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1432.5, 1432.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.815 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.945 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.880\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1432\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}, {'exploitability': 0.176975}, {'exploitability': 0.187075}, {'exploitability': 0.14705}, {'exploitability': 0.17220000000000002}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47002/50000 [1:05:49<02:22, 21.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 47000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 290143/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 289421/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [1:06:37<01:38, 20.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 48000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 296156/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 296096/2000000\n",
      "P1 SL Buffer Size:  296156\n",
      "P1 SL buffer distribution [ 96992. 105060.  77011.  17093.]\n",
      "P1 actions distribution [0.32750307 0.35474547 0.26003525 0.0577162 ]\n",
      "P2 SL Buffer Size:  296096\n",
      "P2 SL buffer distribution [ 73621. 125022.  73341.  24112.]\n",
      "P2 actions distribution [0.24863895 0.42223468 0.24769332 0.08143305]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[6.9559e-01, 3.0401e-01, 3.9439e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[2.0140e-05, 2.1990e-05, 2.0716e-05, 1.6067e-05, 1.5780e-05,\n",
      "          1.0759e-05, 1.9826e-05, 1.3324e-04, 1.4147e-04, 5.3747e-04,\n",
      "          4.8001e-02, 5.9434e-03, 1.9454e-04, 6.5305e-04, 2.2346e-04,\n",
      "          6.1593e-02, 7.1623e-03, 1.7119e-02, 2.1723e-02, 3.6794e-05,\n",
      "          1.2772e-01, 5.2048e-03, 4.0672e-01, 1.1136e-01, 3.4127e-05,\n",
      "          1.0814e-02, 1.6679e-05, 1.1218e-02, 1.5822e-02, 1.7370e-03,\n",
      "          4.9170e-02, 2.7010e-05, 1.6761e-02, 1.3490e-02, 4.1667e-03,\n",
      "          4.7739e-02, 6.0651e-05, 3.2561e-04, 2.3640e-04, 2.4967e-03,\n",
      "          1.0316e-02, 5.5328e-04, 1.8734e-04, 1.1134e-04, 2.1775e-05,\n",
      "          1.6868e-05, 1.6538e-05, 1.7920e-05, 1.8846e-05, 2.1039e-05,\n",
      "          1.3254e-05],\n",
      "         [1.2307e-05, 1.6115e-05, 1.7266e-05, 1.7157e-05, 1.1303e-05,\n",
      "          1.7206e-05, 1.3284e-05, 9.0500e-04, 3.3980e-03, 2.6876e-04,\n",
      "          2.7385e-02, 6.8687e-03, 8.3158e-03, 1.6743e-02, 5.2525e-05,\n",
      "          4.3426e-02, 3.5539e-03, 1.8628e-02, 2.1597e-02, 2.5246e-05,\n",
      "          3.7437e-01, 1.3450e-02, 6.1918e-04, 2.8927e-04, 4.8247e-05,\n",
      "          5.0772e-03, 2.4195e-05, 1.9783e-01, 2.0570e-01, 6.3319e-04,\n",
      "          8.1088e-03, 1.6841e-05, 8.0468e-03, 6.4801e-03, 1.3047e-03,\n",
      "          7.3003e-03, 3.7206e-05, 4.4917e-03, 1.7825e-03, 2.2392e-03,\n",
      "          7.3260e-03, 4.2265e-04, 2.5974e-03, 4.1150e-04, 9.0009e-06,\n",
      "          1.6528e-05, 1.2363e-05, 1.2882e-05, 2.5440e-05, 1.7262e-05,\n",
      "          1.8474e-05],\n",
      "         [8.5334e-08, 7.3137e-08, 9.3481e-08, 5.5398e-08, 5.4141e-08,\n",
      "          5.2205e-08, 8.9581e-08, 3.7459e-07, 2.4110e-07, 4.6399e-07,\n",
      "          2.4089e-06, 2.8025e-06, 1.2650e-05, 1.2158e-05, 2.1387e-07,\n",
      "          7.0569e-05, 1.1910e-06, 2.3020e-05, 2.5139e-05, 8.1984e-08,\n",
      "          9.4690e-05, 2.7940e-06, 4.5990e-01, 5.2865e-01, 1.1169e-02,\n",
      "          6.4987e-07, 8.6544e-08, 7.0749e-06, 5.0508e-06, 1.1060e-06,\n",
      "          1.3063e-06, 7.5971e-08, 3.1110e-06, 2.7541e-06, 1.2957e-06,\n",
      "          9.3877e-07, 1.9430e-07, 1.2981e-06, 1.3883e-06, 8.3298e-07,\n",
      "          1.2104e-06, 4.4345e-07, 3.3417e-07, 1.9052e-07, 9.8716e-08,\n",
      "          9.3900e-08, 6.8962e-08, 8.6228e-08, 1.1868e-07, 6.5298e-08,\n",
      "          1.0583e-07],\n",
      "         [1.2048e-05, 4.4288e-05, 4.2133e-05, 2.9385e-05, 2.3870e-05,\n",
      "          1.2877e-05, 3.4940e-05, 1.0919e-04, 1.4912e-04, 1.2393e-04,\n",
      "          7.2199e-04, 4.6668e-04, 3.4000e-02, 5.5320e-02, 8.2036e-05,\n",
      "          1.0369e-03, 5.8058e-04, 1.0519e-01, 1.0658e-01, 3.8184e-05,\n",
      "          2.5402e-03, 1.0652e-03, 2.2511e-01, 2.3557e-01, 1.3014e-04,\n",
      "          3.9290e-02, 1.9708e-05, 3.2043e-02, 3.6020e-02, 2.2340e-04,\n",
      "          2.7705e-04, 3.4696e-05, 4.2575e-02, 3.2058e-02, 4.4279e-04,\n",
      "          2.1689e-04, 5.0873e-05, 3.3790e-02, 1.3023e-02, 2.2866e-04,\n",
      "          1.9794e-04, 1.6851e-04, 1.1478e-04, 6.5844e-05, 2.8928e-05,\n",
      "          1.6979e-05, 1.9222e-05, 2.3509e-05, 2.6070e-05, 1.7208e-05,\n",
      "          1.4878e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3484, 0.6394, 0.0122, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[5.1368e-07, 4.3860e-07, 2.0296e-07, 3.8849e-07, 4.0102e-07,\n",
      "          9.8182e-08, 6.0034e-07, 8.5840e-05, 5.6698e-05, 3.3829e-06,\n",
      "          3.8286e-03, 3.6223e-04, 3.1104e-05, 2.9404e-05, 3.9617e-06,\n",
      "          7.5464e-01, 1.6222e-04, 1.4375e-05, 3.0996e-05, 5.0023e-07,\n",
      "          8.1871e-03, 2.0791e-05, 5.3571e-06, 6.6582e-06, 1.9389e-07,\n",
      "          5.2046e-03, 5.1784e-07, 1.2615e-06, 1.4139e-06, 6.6297e-06,\n",
      "          9.7448e-04, 4.2654e-07, 2.4681e-06, 3.0057e-06, 5.1284e-05,\n",
      "          2.2306e-01, 1.2436e-06, 1.1947e-05, 9.6664e-06, 2.0243e-04,\n",
      "          2.8839e-03, 3.2428e-06, 4.3355e-05, 5.5749e-05, 3.5011e-07,\n",
      "          3.9379e-07, 5.6414e-07, 3.8015e-07, 5.0695e-07, 2.8542e-07,\n",
      "          7.0517e-07],\n",
      "         [2.3044e-06, 2.6478e-06, 3.8652e-06, 3.9132e-06, 2.7187e-06,\n",
      "          3.0331e-06, 3.6590e-06, 1.2532e-03, 8.5476e-04, 2.5989e-05,\n",
      "          3.6655e-01, 3.0008e-02, 1.8156e-03, 1.8920e-03, 1.1141e-05,\n",
      "          2.1840e-01, 5.0282e-04, 2.9075e-04, 5.7632e-04, 6.0806e-06,\n",
      "          1.5822e-03, 3.6212e-04, 2.4668e-06, 4.5759e-06, 9.0934e-07,\n",
      "          1.2608e-02, 4.2801e-06, 5.7694e-05, 5.8760e-05, 5.2376e-05,\n",
      "          7.7425e-02, 3.1739e-06, 1.1013e-04, 6.3576e-05, 7.0971e-05,\n",
      "          1.0118e-01, 7.6655e-06, 4.3743e-04, 2.3410e-04, 2.6516e-02,\n",
      "          1.5637e-01, 2.3255e-05, 2.7879e-04, 3.0772e-04, 2.2400e-06,\n",
      "          6.7473e-06, 2.9500e-06, 2.9883e-06, 4.5591e-06, 3.6626e-06,\n",
      "          8.1906e-06],\n",
      "         [1.9578e-07, 1.4423e-07, 1.9831e-07, 1.7930e-07, 2.4542e-07,\n",
      "          6.3296e-08, 3.7804e-07, 3.2560e-06, 7.5904e-07, 4.9144e-07,\n",
      "          7.7886e-05, 4.6279e-05, 4.4477e-05, 3.3105e-05, 5.7266e-07,\n",
      "          4.2675e-03, 5.4520e-06, 1.0148e-05, 1.6654e-05, 2.6617e-07,\n",
      "          9.9258e-01, 2.8338e-06, 3.0367e-04, 2.4379e-03, 1.7665e-05,\n",
      "          6.0823e-06, 9.0274e-08, 3.3533e-07, 3.7498e-07, 3.2677e-07,\n",
      "          2.4346e-05, 9.6047e-08, 3.5330e-07, 2.7796e-07, 3.2710e-06,\n",
      "          6.9384e-05, 2.9979e-07, 9.5856e-07, 8.2724e-07, 1.8401e-05,\n",
      "          1.6840e-05, 3.7373e-07, 1.0154e-06, 7.3113e-07, 1.3653e-07,\n",
      "          9.8322e-08, 1.1201e-07, 2.3977e-07, 1.3875e-07, 6.9077e-08,\n",
      "          3.3123e-07],\n",
      "         [2.1610e-05, 1.2044e-05, 3.8750e-05, 3.3766e-05, 1.0525e-05,\n",
      "          8.9221e-06, 5.1305e-05, 2.5880e-04, 3.8937e-04, 6.0118e-05,\n",
      "          3.4267e-02, 5.8836e-03, 1.6299e-03, 2.1890e-03, 7.1321e-05,\n",
      "          1.1908e-01, 5.9138e-03, 1.2078e-03, 8.4407e-04, 2.9553e-05,\n",
      "          7.0959e-01, 2.6262e-04, 9.7091e-04, 2.3635e-03, 1.9449e-05,\n",
      "          3.7463e-02, 2.1587e-05, 1.8218e-04, 3.3463e-04, 5.0520e-05,\n",
      "          4.5066e-02, 2.4275e-05, 3.0019e-04, 3.6309e-04, 8.0975e-04,\n",
      "          1.5717e-02, 6.2306e-05, 9.7546e-04, 7.3887e-04, 2.1390e-03,\n",
      "          1.0001e-02, 4.9106e-05, 1.6247e-04, 2.1525e-04, 6.3361e-06,\n",
      "          8.7842e-06, 1.5264e-05, 3.0082e-05, 2.1778e-05, 8.6540e-06,\n",
      "          1.9054e-05]]])\n",
      "Player 0 Prediction: tensor([[3.5363e-01, 6.4622e-01, 1.5146e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[2.7773e-06, 4.4619e-06, 2.0354e-06, 3.3345e-06, 2.9003e-06,\n",
      "          9.0735e-07, 3.1467e-06, 3.0894e-04, 1.7754e-04, 1.9338e-05,\n",
      "          7.2534e-01, 2.7248e-04, 1.5139e-04, 1.2078e-04, 1.9203e-05,\n",
      "          4.2683e-03, 5.2728e-04, 5.0015e-04, 1.1931e-03, 3.8568e-06,\n",
      "          9.6108e-04, 1.0746e-04, 3.5150e-04, 2.2671e-04, 3.0938e-06,\n",
      "          1.8845e-02, 1.4037e-06, 4.0766e-05, 4.3652e-05, 4.5078e-05,\n",
      "          3.4822e-04, 3.0602e-06, 9.9042e-05, 1.7184e-04, 9.7157e-05,\n",
      "          7.5309e-04, 5.6462e-06, 4.4515e-05, 3.2873e-05, 1.3860e-04,\n",
      "          2.4432e-01, 2.0870e-05, 1.5960e-04, 2.3353e-04, 2.7926e-06,\n",
      "          3.3231e-06, 4.7303e-06, 4.3215e-06, 3.8684e-06, 2.1618e-06,\n",
      "          3.0664e-06],\n",
      "         [4.5072e-05, 2.7793e-05, 8.9074e-05, 6.3125e-05, 5.1953e-05,\n",
      "          4.4089e-05, 5.1114e-05, 4.0805e-02, 3.5671e-02, 2.9689e-04,\n",
      "          4.6953e-01, 6.8334e-03, 4.2232e-02, 3.5346e-02, 1.0828e-04,\n",
      "          6.0736e-03, 3.0966e-03, 2.8652e-02, 2.4868e-02, 7.1956e-05,\n",
      "          3.2730e-03, 5.2041e-03, 2.2440e-04, 2.4149e-04, 3.7956e-05,\n",
      "          7.7073e-02, 4.9641e-05, 3.2738e-03, 3.8998e-03, 6.7625e-04,\n",
      "          4.5566e-03, 6.0088e-05, 2.6903e-02, 3.8507e-02, 5.5810e-04,\n",
      "          9.5852e-04, 1.8159e-04, 1.0169e-02, 7.3035e-03, 4.1232e-03,\n",
      "          8.8797e-02, 3.2899e-04, 1.1501e-02, 1.7713e-02, 4.5834e-05,\n",
      "          7.2073e-05, 4.9286e-05, 5.8849e-05, 7.1109e-05, 4.4200e-05,\n",
      "          8.7192e-05],\n",
      "         [2.9215e-05, 1.3167e-05, 3.7496e-05, 2.8388e-05, 3.9662e-05,\n",
      "          7.2049e-06, 2.6904e-05, 4.3682e-04, 1.1774e-04, 4.5895e-05,\n",
      "          6.6154e-03, 5.1727e-04, 7.3821e-03, 2.4319e-03, 5.0520e-05,\n",
      "          1.7619e-01, 3.0824e-04, 5.0318e-03, 9.0114e-03, 3.8061e-05,\n",
      "          1.4597e-02, 2.2418e-04, 2.3266e-01, 5.3343e-01, 5.8941e-03,\n",
      "          5.6979e-04, 1.0106e-05, 1.9803e-04, 1.4418e-04, 5.1118e-05,\n",
      "          1.2048e-04, 1.0933e-05, 1.6575e-04, 2.5482e-04, 1.7929e-04,\n",
      "          4.8269e-05, 3.1684e-05, 1.3511e-04, 2.1277e-04, 1.6410e-04,\n",
      "          1.9593e-03, 6.0612e-05, 2.0026e-04, 1.7063e-04, 2.4140e-05,\n",
      "          1.4256e-05, 1.1932e-05, 2.7672e-05, 2.3862e-05, 1.3000e-05,\n",
      "          2.9925e-05],\n",
      "         [1.0949e-04, 1.4157e-04, 3.3281e-04, 3.8428e-04, 1.7510e-04,\n",
      "          8.5696e-05, 2.1245e-04, 3.0425e-03, 7.8599e-03, 4.0480e-04,\n",
      "          6.0834e-02, 2.4553e-03, 2.6411e-02, 4.5857e-02, 4.1759e-04,\n",
      "          9.7941e-03, 6.7986e-03, 3.9770e-02, 1.7376e-02, 1.7911e-04,\n",
      "          1.4830e-02, 1.8685e-03, 7.0688e-02, 9.9579e-02, 4.2518e-04,\n",
      "          4.6455e-01, 1.0493e-04, 1.2521e-02, 1.9421e-02, 4.4369e-04,\n",
      "          2.0769e-03, 1.4000e-04, 8.0611e-03, 2.1820e-02, 1.2959e-03,\n",
      "          5.8061e-04, 5.8219e-04, 1.0526e-02, 9.9716e-03, 8.2632e-04,\n",
      "          2.9245e-02, 4.3030e-04, 2.5561e-03, 3.8256e-03, 7.0060e-05,\n",
      "          8.7207e-05, 1.8028e-04, 3.2827e-04, 1.5297e-04, 7.8388e-05,\n",
      "          9.0683e-05]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [1:06:51<01:38, 20.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 30232\n",
      "Average episode length: 3.0 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5038/10000 (50.4%)\n",
      "    Average reward: -0.256\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4962/10000 (49.6%)\n",
      "    Average reward: +0.256\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5423 (35.5%)\n",
      "    Action 1: 5074 (33.2%)\n",
      "    Action 2: 3648 (23.9%)\n",
      "    Action 3: 1131 (7.4%)\n",
      "  Player 1:\n",
      "    Action 0: 2871 (19.2%)\n",
      "    Action 1: 5972 (39.9%)\n",
      "    Action 2: 3474 (23.2%)\n",
      "    Action 3: 2639 (17.6%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2556.5, 2556.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.059 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.986 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.022\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2556\n",
      "   Testing specific player: 0\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 9.8440e-01, 2.5076e-05, 1.5572e-02]])\n",
      "Player 0 Prediction: tensor([[0.0000e+00, 6.7195e-01, 3.4105e-04, 3.2771e-01]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41643\n",
      "Average episode length: 4.2 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4252/10000 (42.5%)\n",
      "    Average reward: +0.192\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5748/10000 (57.5%)\n",
      "    Average reward: -0.192\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4624 (22.0%)\n",
      "    Action 1: 9846 (46.8%)\n",
      "    Action 2: 3962 (18.8%)\n",
      "    Action 3: 2599 (12.4%)\n",
      "  Player 1:\n",
      "    Action 0: 14869 (72.1%)\n",
      "    Action 1: 5743 (27.9%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1916.5, -1916.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.993 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.854 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.923\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1916\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[[4.8143e-05, 6.7885e-05, 4.7166e-05, 4.1839e-05, 6.0600e-05,\n",
      "          5.1259e-05, 3.3003e-05, 7.5118e-04, 1.5128e-03, 1.1279e-03,\n",
      "          7.0724e-03, 2.8531e-03, 6.9169e-03, 2.3578e-02, 8.4583e-04,\n",
      "          8.5636e-03, 1.7759e-03, 1.5476e-02, 2.0690e-02, 1.6483e-04,\n",
      "          2.8783e-03, 1.1027e-03, 1.5345e-02, 1.2831e-02, 1.8492e-04,\n",
      "          2.0628e-01, 5.5053e-05, 8.6130e-02, 7.5708e-02, 8.1840e-03,\n",
      "          2.4801e-01, 1.2866e-04, 6.0036e-02, 4.3130e-02, 5.7478e-03,\n",
      "          2.9870e-02, 4.7924e-04, 5.0290e-02, 1.9015e-02, 1.0363e-02,\n",
      "          2.0854e-02, 2.5501e-03, 5.6160e-03, 3.1742e-03, 4.6861e-05,\n",
      "          4.6969e-05, 5.7545e-05, 6.9901e-05, 4.0177e-05, 5.8039e-05,\n",
      "          4.2139e-05],\n",
      "         [1.1505e-05, 1.3377e-05, 8.7309e-06, 1.2743e-05, 1.4761e-05,\n",
      "          1.1177e-05, 9.5990e-06, 6.2100e-04, 3.8585e-03, 9.5474e-05,\n",
      "          4.0358e-03, 2.0469e-03, 4.9622e-03, 1.0519e-02, 4.0023e-05,\n",
      "          1.4269e-02, 1.1405e-03, 3.8367e-04, 5.1123e-04, 2.2997e-05,\n",
      "          9.6834e-03, 1.1309e-03, 2.0050e-05, 3.7618e-05, 6.5249e-05,\n",
      "          2.4776e-01, 1.2120e-05, 3.1219e-01, 2.7771e-01, 7.9214e-04,\n",
      "          1.9535e-02, 1.5148e-05, 1.0612e-03, 6.7660e-04, 2.5444e-03,\n",
      "          3.3147e-02, 3.0967e-05, 1.5282e-02, 4.9059e-03, 5.9011e-03,\n",
      "          1.3561e-02, 1.9334e-04, 8.9649e-03, 2.1109e-03, 7.7969e-06,\n",
      "          9.5019e-06, 1.7976e-05, 8.8427e-06, 1.3280e-05, 1.4496e-05,\n",
      "          1.6910e-05],\n",
      "         [1.2130e-06, 1.1302e-06, 9.2202e-07, 8.7784e-07, 7.4403e-07,\n",
      "          1.3282e-06, 6.5545e-07, 3.3463e-06, 4.2156e-06, 4.9638e-06,\n",
      "          1.0540e-05, 1.7429e-05, 1.1370e-04, 3.3995e-04, 2.6416e-06,\n",
      "          2.4551e-04, 9.3912e-06, 2.8303e-04, 2.7567e-04, 1.3334e-06,\n",
      "          1.0001e-03, 6.1351e-06, 2.5453e-03, 1.1531e-01, 8.7925e-01,\n",
      "          7.6077e-05, 6.9570e-07, 1.2026e-04, 8.0668e-05, 9.7651e-06,\n",
      "          3.3614e-05, 1.5754e-06, 3.1923e-05, 1.9999e-05, 1.1854e-05,\n",
      "          1.7648e-05, 1.8792e-06, 4.4936e-05, 3.7460e-05, 2.3030e-05,\n",
      "          2.7087e-05, 8.5697e-06, 1.1510e-05, 3.9440e-06, 6.5907e-07,\n",
      "          1.1229e-06, 1.7905e-06, 1.6331e-06, 9.3464e-07, 7.1228e-07,\n",
      "          6.1037e-07],\n",
      "         [7.5998e-05, 1.2904e-04, 1.0878e-04, 6.6766e-05, 1.2108e-04,\n",
      "          8.7145e-05, 7.0474e-05, 5.5243e-04, 1.0017e-03, 2.7699e-04,\n",
      "          9.9619e-04, 1.0439e-03, 3.1868e-02, 6.9425e-02, 1.7482e-04,\n",
      "          8.9320e-04, 9.1734e-04, 5.2572e-02, 7.1298e-02, 1.8710e-04,\n",
      "          2.0478e-03, 7.8039e-04, 6.6280e-03, 2.1663e-02, 1.4150e-03,\n",
      "          2.6846e-01, 8.7393e-05, 1.1920e-01, 8.6787e-02, 1.4928e-03,\n",
      "          9.8405e-03, 1.1228e-04, 1.0022e-01, 4.9130e-02, 1.7161e-03,\n",
      "          1.8736e-03, 1.4915e-04, 5.7802e-02, 3.0276e-02, 1.0760e-03,\n",
      "          2.2731e-03, 5.8735e-04, 2.3722e-03, 1.6830e-03, 6.0274e-05,\n",
      "          4.2991e-05, 6.9581e-05, 1.0469e-04, 6.6170e-05, 6.4509e-05,\n",
      "          5.4735e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.6780, 0.0043, 0.3177]])\n",
      "Player 0 Prediction: tensor([[[3.5065e-06, 4.2027e-06, 5.0583e-06, 4.6311e-06, 2.4353e-06,\n",
      "          4.0342e-06, 3.3489e-06, 4.2760e-05, 3.7462e-05, 2.1980e-05,\n",
      "          4.5401e-02, 5.2537e-03, 8.2956e-05, 9.0906e-05, 2.0758e-05,\n",
      "          5.4305e-02, 3.3279e-03, 5.0844e-04, 5.3608e-04, 8.9312e-06,\n",
      "          1.5917e-02, 8.8311e-04, 1.4598e-04, 6.6116e-05, 4.2230e-07,\n",
      "          2.8604e-01, 2.4260e-06, 5.7117e-05, 6.1299e-05, 6.1619e-04,\n",
      "          2.3499e-01, 6.4071e-06, 1.0068e-03, 7.0566e-04, 8.9941e-03,\n",
      "          1.6349e-01, 2.0279e-05, 4.1745e-04, 4.2665e-04, 2.3353e-02,\n",
      "          1.5276e-01, 4.1026e-05, 1.3171e-04, 1.7983e-04, 3.6490e-06,\n",
      "          5.4970e-06, 2.3048e-06, 2.5460e-06, 3.5411e-06, 4.1202e-06,\n",
      "          2.5523e-06],\n",
      "         [4.6747e-06, 2.1179e-06, 3.8423e-06, 4.4122e-06, 3.2231e-06,\n",
      "          3.1050e-06, 4.2786e-06, 1.4002e-03, 8.0954e-03, 1.4698e-05,\n",
      "          1.3317e-04, 7.5776e-04, 8.6547e-03, 1.6099e-02, 6.3669e-06,\n",
      "          1.6076e-03, 1.8766e-04, 1.8964e-03, 2.6461e-03, 4.8819e-06,\n",
      "          2.9311e-04, 3.1989e-04, 8.6118e-06, 7.4089e-06, 6.4914e-07,\n",
      "          2.9662e-01, 1.5802e-06, 5.2849e-04, 5.0351e-04, 1.3172e-04,\n",
      "          5.5171e-01, 5.5248e-06, 4.8158e-03, 3.0671e-03, 3.7359e-04,\n",
      "          2.2498e-03, 6.9549e-06, 4.3738e-02, 2.4210e-02, 1.7436e-03,\n",
      "          8.1204e-04, 1.9236e-05, 2.2630e-02, 4.6492e-03, 2.3436e-06,\n",
      "          2.9822e-06, 2.9706e-06, 1.6402e-06, 3.8581e-06, 3.7760e-06,\n",
      "          3.1604e-06],\n",
      "         [8.6449e-07, 6.6610e-07, 8.0146e-07, 4.8143e-07, 6.4777e-07,\n",
      "          8.2334e-07, 5.4994e-07, 2.7152e-06, 2.9624e-06, 1.5666e-06,\n",
      "          1.8849e-05, 2.4817e-05, 1.0653e-04, 1.2778e-04, 1.7134e-06,\n",
      "          1.1193e-03, 1.9645e-05, 1.8505e-04, 2.3796e-04, 5.1533e-07,\n",
      "          2.6169e-03, 8.7793e-06, 5.8716e-01, 4.0734e-01, 1.8855e-04,\n",
      "          3.2121e-04, 7.7222e-07, 5.5849e-06, 4.7353e-06, 7.3866e-06,\n",
      "          2.3540e-04, 5.2699e-07, 2.6152e-05, 1.2452e-05, 2.3300e-05,\n",
      "          4.5684e-05, 9.2265e-07, 1.9373e-05, 2.2943e-05, 3.2523e-05,\n",
      "          4.2693e-05, 2.9749e-06, 1.0050e-05, 7.8953e-06, 5.1238e-07,\n",
      "          7.8033e-07, 5.7343e-07, 6.2316e-07, 6.3691e-07, 5.3322e-07,\n",
      "          3.3849e-07],\n",
      "         [3.1036e-05, 3.9161e-05, 3.2901e-05, 3.0299e-05, 3.6408e-05,\n",
      "          6.6462e-05, 2.6823e-05, 3.1478e-04, 2.8972e-04, 6.4710e-05,\n",
      "          3.2454e-03, 1.2417e-03, 5.4732e-03, 1.1402e-02, 4.2548e-05,\n",
      "          2.1739e-03, 1.1535e-03, 1.5684e-02, 2.5214e-02, 4.9910e-05,\n",
      "          4.8771e-03, 3.2574e-04, 8.8784e-03, 6.6739e-03, 1.5433e-05,\n",
      "          7.1290e-01, 3.3748e-05, 4.6403e-03, 3.6445e-03, 2.7830e-04,\n",
      "          7.7185e-02, 2.7595e-05, 2.5356e-02, 2.6098e-02, 2.6595e-03,\n",
      "          2.2545e-03, 3.8380e-05, 3.3476e-02, 1.4043e-02, 3.6297e-03,\n",
      "          4.1034e-03, 7.1933e-05, 9.4651e-04, 1.0557e-03, 2.8690e-05,\n",
      "          2.4303e-05, 3.0841e-05, 1.9887e-05, 2.3266e-05, 2.1792e-05,\n",
      "          2.2381e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5215, 0.0945, 0.3840]])\n",
      "Player 0 Prediction: tensor([[[2.9287e-07, 7.8199e-08, 1.0796e-07, 1.2456e-07, 5.5643e-08,\n",
      "          1.5925e-07, 6.3321e-08, 2.4879e-05, 2.6011e-05, 4.4774e-07,\n",
      "          2.5597e-03, 9.2894e-05, 1.4056e-05, 1.4689e-05, 3.3549e-07,\n",
      "          5.5658e-01, 2.6839e-05, 9.6733e-07, 1.4867e-06, 2.9968e-07,\n",
      "          3.0554e-04, 3.0275e-06, 1.1422e-07, 1.6001e-07, 1.4208e-08,\n",
      "          3.1465e-02, 9.3123e-08, 3.4739e-07, 3.8792e-07, 7.1271e-06,\n",
      "          2.5198e-03, 2.4916e-07, 2.4945e-06, 2.4597e-06, 2.6159e-05,\n",
      "          4.0395e-01, 5.6561e-07, 1.2294e-05, 1.7245e-05, 5.2164e-04,\n",
      "          1.7599e-03, 6.0909e-07, 3.0855e-05, 3.1202e-05, 1.0623e-07,\n",
      "          6.6783e-08, 1.0747e-07, 6.1178e-08, 1.4875e-07, 1.3097e-07,\n",
      "          9.6322e-08],\n",
      "         [1.1842e-06, 4.1428e-07, 4.9746e-07, 6.8110e-07, 5.2270e-07,\n",
      "          1.1242e-06, 1.1143e-06, 1.0970e-04, 8.9027e-05, 3.4818e-06,\n",
      "          3.6853e-02, 3.6388e-03, 2.7550e-04, 1.9765e-04, 1.4849e-06,\n",
      "          2.7648e-01, 4.2049e-05, 1.9961e-05, 2.1821e-05, 1.0026e-06,\n",
      "          2.7808e-05, 1.5708e-05, 1.0877e-07, 2.7850e-07, 9.5951e-08,\n",
      "          1.7731e-01, 3.2933e-07, 2.0115e-05, 3.0383e-05, 1.2391e-05,\n",
      "          3.2413e-01, 1.1997e-06, 6.7620e-05, 3.9878e-05, 2.0817e-05,\n",
      "          1.0243e-01, 1.4431e-06, 1.1762e-04, 9.8641e-05, 1.0245e-02,\n",
      "          6.7582e-02, 1.2805e-06, 4.1386e-05, 6.1262e-05, 4.4614e-07,\n",
      "          5.5580e-07, 1.0616e-06, 2.0063e-07, 5.8896e-07, 4.3935e-07,\n",
      "          1.0503e-06],\n",
      "         [9.3663e-08, 3.7827e-08, 3.0278e-08, 2.6850e-08, 4.6261e-08,\n",
      "          6.4418e-08, 2.9607e-08, 6.4066e-07, 3.8476e-07, 2.1499e-07,\n",
      "          1.9983e-05, 5.8724e-06, 1.0800e-05, 7.0028e-06, 9.0425e-08,\n",
      "          5.9756e-03, 1.0585e-06, 1.8238e-06, 1.8573e-06, 6.5694e-08,\n",
      "          9.9349e-01, 3.6337e-07, 2.7772e-05, 2.2586e-04, 5.1886e-06,\n",
      "          3.9283e-05, 3.8377e-08, 1.0993e-07, 8.3050e-08, 6.2361e-07,\n",
      "          3.4303e-05, 5.8601e-08, 3.3354e-07, 2.5683e-07, 8.6433e-07,\n",
      "          1.1207e-04, 6.5730e-08, 3.4788e-07, 3.8357e-07, 5.1815e-06,\n",
      "          2.4720e-05, 1.2618e-07, 3.8443e-07, 4.8614e-07, 2.9094e-08,\n",
      "          4.2116e-08, 1.0658e-07, 6.4150e-08, 3.6626e-08, 4.2708e-08,\n",
      "          4.2984e-08],\n",
      "         [1.3399e-05, 8.1600e-06, 1.5552e-05, 7.2918e-06, 4.6723e-06,\n",
      "          1.8857e-05, 3.2173e-06, 1.1664e-04, 6.7261e-05, 1.2951e-05,\n",
      "          3.7153e-02, 1.5736e-03, 5.6209e-04, 5.2181e-04, 1.0846e-05,\n",
      "          9.5195e-02, 7.8042e-04, 1.7093e-04, 1.2710e-04, 1.8554e-05,\n",
      "          1.2545e-01, 5.0241e-05, 6.4930e-05, 9.0819e-05, 4.2651e-06,\n",
      "          3.3650e-01, 6.8962e-06, 1.7204e-04, 1.6380e-04, 6.9728e-05,\n",
      "          3.2461e-01, 1.3785e-05, 4.5656e-04, 3.3920e-04, 6.3754e-04,\n",
      "          3.9660e-02, 8.2467e-06, 3.3798e-04, 4.9841e-04, 2.1142e-03,\n",
      "          3.2024e-02, 1.0460e-05, 8.5648e-05, 1.9347e-04, 3.6986e-06,\n",
      "          5.8768e-06, 1.5580e-05, 6.4445e-06, 6.2922e-06, 5.5070e-06,\n",
      "          8.5752e-06]]])\n",
      "Player 1 Prediction: tensor([[0.7128, 0.1594, 0.1277, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 28172\n",
      "Average episode length: 2.8 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4629/10000 (46.3%)\n",
      "    Average reward: +0.063\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5371/10000 (53.7%)\n",
      "    Average reward: -0.063\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 5497 (38.4%)\n",
      "    Action 1: 3470 (24.3%)\n",
      "    Action 2: 4946 (34.6%)\n",
      "    Action 3: 385 (2.7%)\n",
      "  Player 1:\n",
      "    Action 0: 3578 (25.8%)\n",
      "    Action 1: 5767 (41.6%)\n",
      "    Action 2: 2852 (20.6%)\n",
      "    Action 3: 1677 (12.1%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [627.5, -627.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.026 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.031 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.028\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.0628\n",
      "   Testing specific player: 1\n",
      "   At training step: 48000\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.5605, 0.4266, 0.0129, 0.0000]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.2891, 0.0570, 0.6539]])\n",
      "Player 1 Prediction: tensor([[0.3669, 0.1614, 0.4717, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 48000\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 40808\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6950/10000 (69.5%)\n",
      "    Average reward: -0.133\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3050/10000 (30.5%)\n",
      "    Average reward: +0.133\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15289 (75.3%)\n",
      "    Action 1: 5015 (24.7%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3544 (17.3%)\n",
      "    Action 1: 10236 (49.9%)\n",
      "    Action 2: 3755 (18.3%)\n",
      "    Action 3: 2969 (14.5%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1327.0, 1327.0]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.806 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.938 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.872\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1327\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}, {'exploitability': 0.176975}, {'exploitability': 0.187075}, {'exploitability': 0.14705}, {'exploitability': 0.17220000000000002}, {'exploitability': 0.1592}]\n",
      "Plotting test_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49003/50000 [1:08:06<00:49, 19.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0000 â†’ 0.0000\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 49000:\n",
      "   Player 0 RL buffer: 200000/200000\n",
      "   Player 0 SL buffer: 302193/2000000\n",
      "   Player 1 RL buffer: 200000/200000\n",
      "   Player 1 SL buffer: 302743/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [1:08:59<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'best_response']\n",
      "Player 0 Prediction: tensor([[7.0547e-01, 2.9397e-01, 5.6024e-04, 0.0000e+00]])\n",
      "Player 1 Prediction: tensor([[[1.7324e-05, 2.3145e-05, 2.1082e-05, 1.9964e-05, 6.8945e-06,\n",
      "          5.6234e-06, 9.0555e-06, 1.3760e-04, 8.8924e-05, 4.7290e-04,\n",
      "          4.5770e-02, 3.5553e-03, 1.9571e-04, 4.0958e-04, 1.0254e-04,\n",
      "          6.0145e-02, 6.9117e-03, 1.4872e-02, 1.6750e-02, 2.6700e-05,\n",
      "          1.2332e-01, 3.4675e-03, 4.1755e-01, 1.4741e-01, 3.0349e-05,\n",
      "          9.7496e-03, 1.0192e-05, 1.1177e-02, 1.1403e-02, 1.8328e-03,\n",
      "          3.6821e-02, 3.9972e-05, 1.4431e-02, 1.2398e-02, 3.3394e-03,\n",
      "          4.2790e-02, 4.0599e-05, 2.7584e-04, 2.1474e-04, 2.3125e-03,\n",
      "          1.0945e-02, 5.3703e-04, 1.5796e-04, 1.1017e-04, 1.4933e-05,\n",
      "          1.1742e-05, 8.8214e-06, 1.6140e-05, 2.1943e-05, 9.8688e-06,\n",
      "          1.1937e-05],\n",
      "         [1.1016e-05, 7.8675e-06, 3.0418e-05, 1.1255e-05, 1.1574e-05,\n",
      "          2.5545e-05, 1.7068e-05, 8.9584e-04, 3.2362e-03, 3.0098e-04,\n",
      "          2.3920e-02, 8.8219e-03, 1.0001e-02, 1.7043e-02, 4.3215e-05,\n",
      "          5.3530e-02, 6.3068e-03, 1.8893e-02, 2.1631e-02, 2.0418e-05,\n",
      "          3.6493e-01, 2.2662e-02, 6.4354e-04, 2.9441e-04, 4.8086e-05,\n",
      "          5.0401e-03, 3.6338e-05, 1.9025e-01, 2.0002e-01, 8.7901e-04,\n",
      "          7.2408e-03, 1.7729e-05, 7.5863e-03, 6.1724e-03, 1.0449e-03,\n",
      "          8.8126e-03, 4.4744e-05, 4.3235e-03, 1.7123e-03, 1.8064e-03,\n",
      "          8.5225e-03, 3.6283e-04, 2.2621e-03, 4.1931e-04, 6.6051e-06,\n",
      "          1.2826e-05, 1.1097e-05, 9.5760e-06, 3.8839e-05, 1.1649e-05,\n",
      "          1.1871e-05],\n",
      "         [1.0189e-07, 6.8181e-08, 5.1770e-08, 2.1756e-08, 5.2900e-08,\n",
      "          4.2321e-08, 4.6264e-08, 1.6570e-07, 1.8322e-07, 1.8017e-07,\n",
      "          1.6025e-06, 2.1734e-06, 9.3481e-06, 7.1133e-06, 1.5603e-07,\n",
      "          6.3326e-05, 7.6263e-07, 2.0966e-05, 2.1967e-05, 3.6643e-08,\n",
      "          6.4543e-05, 2.5233e-06, 4.7578e-01, 5.1510e-01, 8.9090e-03,\n",
      "          4.1427e-07, 4.4947e-08, 3.9496e-06, 3.9230e-06, 1.4650e-06,\n",
      "          6.7090e-07, 4.5636e-08, 1.8622e-06, 2.2219e-06, 1.0228e-06,\n",
      "          7.9172e-07, 9.0666e-08, 7.2529e-07, 1.0899e-06, 8.4345e-07,\n",
      "          9.2941e-07, 3.8716e-07, 2.1421e-07, 2.2289e-07, 5.8442e-08,\n",
      "          6.4223e-08, 5.3563e-08, 9.8687e-08, 6.0442e-08, 6.5613e-08,\n",
      "          1.0981e-07],\n",
      "         [8.9640e-06, 4.0386e-05, 2.8813e-05, 2.8943e-05, 2.0056e-05,\n",
      "          1.4224e-05, 3.4457e-05, 6.9426e-05, 1.4955e-04, 1.0105e-04,\n",
      "          6.9972e-04, 3.9160e-04, 2.9232e-02, 5.5783e-02, 6.4299e-05,\n",
      "          8.0734e-04, 8.6628e-04, 9.3880e-02, 1.1311e-01, 4.6365e-05,\n",
      "          2.3072e-03, 1.1286e-03, 2.2914e-01, 2.6303e-01, 1.2985e-04,\n",
      "          3.2429e-02, 8.7300e-06, 3.3559e-02, 3.4163e-02, 1.8176e-04,\n",
      "          2.6838e-04, 1.9129e-05, 4.0173e-02, 2.7409e-02, 3.0855e-04,\n",
      "          1.9066e-04, 3.5245e-05, 2.5934e-02, 1.3434e-02, 1.8017e-04,\n",
      "          2.0350e-04, 1.1771e-04, 1.0852e-04, 4.0150e-05, 2.7747e-05,\n",
      "          1.3347e-05, 2.2665e-05, 1.5523e-05, 1.5506e-05, 1.0115e-05,\n",
      "          1.4836e-05]]])\n",
      "Player 0 Prediction: tensor([[0.3848, 0.6042, 0.0110, 0.0000]])\n",
      "Player 1 Prediction: tensor([[[2.3587e-05, 3.8857e-05, 2.1781e-05, 3.8919e-05, 4.5279e-06,\n",
      "          8.4608e-06, 1.2371e-05, 9.3693e-02, 6.3101e-02, 2.1070e-04,\n",
      "          5.8778e-04, 2.3361e-04, 1.4515e-01, 2.4843e-01, 3.4575e-05,\n",
      "          5.9956e-03, 2.2517e-04, 7.8627e-02, 1.3332e-01, 2.4297e-05,\n",
      "          4.1823e-03, 1.5460e-04, 1.6447e-04, 3.4479e-04, 2.7343e-05,\n",
      "          7.4732e-02, 1.4598e-05, 2.0596e-04, 6.7591e-05, 3.1683e-04,\n",
      "          8.0538e-04, 5.2422e-05, 1.0071e-02, 8.6838e-03, 2.3910e-04,\n",
      "          1.7781e-03, 2.9040e-05, 4.3311e-02, 3.0227e-02, 4.1962e-04,\n",
      "          1.8632e-04, 2.9856e-04, 3.6536e-02, 1.7195e-02, 1.5717e-05,\n",
      "          1.2370e-05, 1.0749e-05, 3.4881e-05, 6.1489e-05, 6.1566e-06,\n",
      "          3.0044e-05],\n",
      "         [1.0208e-05, 5.3877e-06, 4.3360e-05, 6.1614e-06, 2.1203e-05,\n",
      "          3.5144e-05, 4.5543e-05, 2.0870e-02, 2.7063e-02, 1.4519e-04,\n",
      "          7.4759e-02, 2.2967e-02, 2.8494e-01, 2.8154e-01, 4.4167e-05,\n",
      "          1.4044e-02, 1.4734e-03, 9.4217e-04, 1.2997e-03, 3.4619e-05,\n",
      "          4.1774e-03, 1.3949e-03, 3.2420e-06, 7.3521e-06, 1.5912e-05,\n",
      "          1.7654e-01, 5.2817e-05, 5.9653e-04, 8.0661e-04, 2.8171e-04,\n",
      "          2.5176e-03, 1.6423e-05, 5.2884e-04, 3.0079e-04, 9.2487e-05,\n",
      "          4.0629e-03, 3.8646e-05, 2.3401e-02, 1.2348e-02, 4.7794e-03,\n",
      "          2.6369e-02, 1.0608e-04, 7.4452e-03, 3.7057e-03, 7.7358e-06,\n",
      "          1.4053e-05, 1.4671e-05, 1.6065e-05, 6.1390e-05, 8.5763e-06,\n",
      "          1.2043e-05],\n",
      "         [5.3123e-06, 5.3866e-06, 1.1501e-06, 4.2939e-07, 2.4919e-06,\n",
      "          1.6819e-06, 1.7499e-06, 2.6096e-05, 7.1452e-05, 1.3924e-06,\n",
      "          2.7394e-05, 4.5948e-05, 1.6224e-03, 1.0479e-03, 6.5575e-06,\n",
      "          5.3877e-04, 6.2472e-06, 2.3911e-03, 3.8973e-03, 1.3217e-06,\n",
      "          9.8655e-01, 1.7675e-05, 1.9262e-04, 2.9409e-03, 2.7075e-04,\n",
      "          4.1879e-05, 1.1833e-06, 3.3065e-06, 7.6606e-06, 1.9391e-05,\n",
      "          9.7573e-06, 9.7040e-07, 1.5248e-05, 7.4518e-06, 7.5480e-06,\n",
      "          1.3117e-05, 9.1225e-07, 4.4712e-05, 2.1638e-05, 4.8041e-05,\n",
      "          9.4749e-06, 7.1970e-06, 2.3240e-05, 3.1176e-05, 9.6745e-07,\n",
      "          1.0939e-06, 1.3006e-06, 7.3116e-06, 1.1363e-06, 1.0298e-06,\n",
      "          3.8563e-06],\n",
      "         [1.2024e-04, 3.3972e-04, 1.2591e-04, 2.4128e-04, 1.2899e-04,\n",
      "          1.7368e-04, 2.5309e-04, 9.9869e-03, 6.7929e-02, 3.8168e-04,\n",
      "          9.1531e-03, 2.9090e-03, 6.6755e-02, 1.5887e-01, 3.3006e-04,\n",
      "          4.5273e-03, 8.7216e-03, 8.4334e-02, 4.8298e-02, 4.4443e-04,\n",
      "          2.1879e-01, 1.0109e-03, 1.7268e-03, 4.6344e-03, 3.6542e-04,\n",
      "          1.7525e-01, 6.9371e-05, 1.2462e-03, 1.8417e-03, 3.0900e-04,\n",
      "          1.7817e-02, 6.6842e-05, 2.7442e-02, 1.2454e-02, 1.2020e-03,\n",
      "          1.7308e-03, 2.0197e-04, 2.9306e-02, 2.0166e-02, 1.7825e-03,\n",
      "          3.4066e-03, 2.9688e-04, 1.1506e-02, 2.5150e-03, 1.3372e-04,\n",
      "          6.9069e-05, 2.8195e-04, 1.0457e-04, 7.0331e-05, 2.4642e-05,\n",
      "          1.5923e-04]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 25506\n",
      "Average episode length: 2.6 steps\n",
      "Episode length range: 1 - 7\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 5915/10000 (59.2%)\n",
      "    Average reward: -0.226\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 4085/10000 (40.8%)\n",
      "    Average reward: +0.226\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4546 (36.7%)\n",
      "    Action 1: 3927 (31.7%)\n",
      "    Action 2: 3013 (24.3%)\n",
      "    Action 3: 910 (7.3%)\n",
      "  Player 1:\n",
      "    Action 0: 2682 (20.5%)\n",
      "    Action 1: 3833 (29.2%)\n",
      "    Action 2: 4601 (35.1%)\n",
      "    Action 3: 1994 (15.2%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-2255.5, 2255.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.056 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.987 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.022\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: -0.2256\n",
      "   Testing specific player: 0\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 0 Prediction: tensor([[0.0000, 0.6940, 0.0084, 0.2976]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.2999, 0.0550, 0.6451]])\n",
      "Player 0 Prediction: tensor([[0.4141, 0.3757, 0.2102, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41012\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4103/10000 (41.0%)\n",
      "    Average reward: +0.132\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5897/10000 (59.0%)\n",
      "    Average reward: -0.132\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4691 (22.5%)\n",
      "    Action 1: 9501 (45.6%)\n",
      "    Action 2: 4081 (19.6%)\n",
      "    Action 3: 2550 (12.2%)\n",
      "  Player 1:\n",
      "    Action 0: 14456 (71.6%)\n",
      "    Action 1: 5733 (28.4%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1319.5, -1319.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.001 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 0.861 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Average strategy entropy: 0.931\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 0 average reward: 0.1320\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['best_response', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[5.7676e-01, 4.2308e-01, 1.5976e-04, 0.0000e+00]])\n",
      "Player 0 Prediction: tensor([[[2.0810e-06, 4.4169e-06, 2.9658e-06, 4.4835e-06, 2.0908e-06,\n",
      "          2.5075e-06, 2.5622e-06, 3.5392e-05, 3.3995e-05, 1.5010e-05,\n",
      "          3.8491e-02, 4.9295e-03, 6.4734e-05, 7.6095e-05, 1.0810e-05,\n",
      "          5.2935e-02, 2.1685e-03, 2.9532e-04, 3.6102e-04, 4.6064e-06,\n",
      "          1.4386e-02, 6.5806e-04, 6.5676e-05, 4.9568e-05, 2.2997e-07,\n",
      "          2.4630e-01, 1.0417e-06, 5.1825e-05, 6.1461e-05, 2.7274e-04,\n",
      "          3.0458e-01, 2.7688e-06, 7.3694e-04, 4.6959e-04, 6.9584e-03,\n",
      "          1.4051e-01, 1.6485e-05, 4.2264e-04, 4.0911e-04, 2.6891e-02,\n",
      "          1.5735e-01, 4.0732e-05, 1.6113e-04, 1.4257e-04, 3.1417e-06,\n",
      "          2.2012e-06, 1.5176e-06, 2.7840e-06, 2.0589e-06, 2.0551e-06,\n",
      "          9.1640e-07],\n",
      "         [2.6685e-06, 9.8045e-07, 1.8159e-06, 2.7229e-06, 3.1597e-06,\n",
      "          1.7374e-06, 2.4921e-06, 9.9589e-04, 6.2569e-03, 1.4277e-05,\n",
      "          7.9876e-05, 6.6664e-04, 6.3031e-03, 1.2390e-02, 2.7394e-06,\n",
      "          1.2227e-03, 1.4557e-04, 9.3203e-04, 1.4433e-03, 3.8345e-06,\n",
      "          2.4954e-04, 2.3982e-04, 5.1657e-06, 4.6547e-06, 5.9703e-07,\n",
      "          1.8383e-01, 6.9699e-07, 3.9171e-04, 4.3346e-04, 6.5083e-05,\n",
      "          6.8164e-01, 3.3550e-06, 3.3126e-03, 1.8209e-03, 3.2735e-04,\n",
      "          1.6397e-03, 7.5974e-06, 4.1125e-02, 2.3769e-02, 9.0022e-04,\n",
      "          5.4626e-04, 1.3356e-05, 2.5285e-02, 3.8990e-03, 2.7914e-06,\n",
      "          1.3491e-06, 1.1399e-06, 1.1586e-06, 1.7646e-06, 1.3486e-06,\n",
      "          1.1743e-06],\n",
      "         [4.5579e-07, 4.4677e-07, 7.3810e-07, 5.2754e-07, 7.7045e-07,\n",
      "          5.5541e-07, 1.7558e-07, 1.9150e-06, 2.2082e-06, 1.8138e-06,\n",
      "          1.5331e-05, 1.6457e-05, 1.1974e-04, 1.1345e-04, 9.3652e-07,\n",
      "          8.9930e-04, 2.4580e-05, 1.7210e-04, 1.4370e-04, 3.0464e-07,\n",
      "          2.2307e-03, 8.4519e-06, 4.8535e-01, 5.0997e-01, 2.4493e-04,\n",
      "          2.4374e-04, 4.8867e-07, 3.0901e-06, 3.3705e-06, 4.6135e-06,\n",
      "          2.2766e-04, 4.3764e-07, 1.3537e-05, 1.1441e-05, 1.5825e-05,\n",
      "          3.9744e-05, 1.1837e-06, 1.7046e-05, 2.1852e-05, 2.9481e-05,\n",
      "          3.1956e-05, 1.2073e-06, 1.1344e-05, 5.2770e-06, 4.1810e-07,\n",
      "          4.8449e-07, 3.0305e-07, 3.1402e-07, 6.2821e-07, 2.3532e-07,\n",
      "          2.8462e-07],\n",
      "         [1.1126e-05, 6.1690e-05, 3.2834e-05, 5.6227e-05, 3.7988e-05,\n",
      "          2.8020e-05, 1.0245e-05, 4.0370e-04, 3.5102e-04, 5.0929e-05,\n",
      "          2.1915e-03, 1.0471e-03, 5.4519e-03, 1.0512e-02, 4.6659e-05,\n",
      "          1.5298e-03, 7.0756e-04, 1.5640e-02, 2.6063e-02, 1.9739e-05,\n",
      "          3.6912e-03, 3.3919e-04, 8.4519e-03, 7.3809e-03, 1.8166e-05,\n",
      "          6.8720e-01, 4.0228e-05, 3.9170e-03, 3.4789e-03, 1.9205e-04,\n",
      "          8.8734e-02, 1.1694e-05, 3.7706e-02, 2.0853e-02, 3.4312e-03,\n",
      "          1.9493e-03, 3.5335e-05, 3.6178e-02, 2.0129e-02, 6.0275e-03,\n",
      "          3.3027e-03, 8.3766e-05, 1.0381e-03, 1.4151e-03, 3.7921e-05,\n",
      "          1.3737e-05, 8.6565e-06, 2.3498e-05, 2.8266e-05, 1.4852e-05,\n",
      "          1.8036e-05]]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.5754, 0.0015, 0.4231]])\n",
      "Player 0 Prediction: tensor([[[1.5266e-07, 7.4178e-08, 1.1231e-07, 1.8631e-07, 8.5018e-08,\n",
      "          1.0506e-07, 3.9212e-08, 2.9283e-05, 2.9981e-05, 5.6104e-07,\n",
      "          2.4387e-03, 1.1253e-04, 1.1706e-05, 1.5143e-05, 2.5705e-07,\n",
      "          4.5327e-01, 2.7196e-05, 9.4843e-07, 1.4777e-06, 2.1657e-07,\n",
      "          3.6716e-04, 3.8462e-06, 1.3715e-07, 1.8611e-07, 1.8529e-08,\n",
      "          2.6846e-02, 6.9199e-08, 3.5214e-07, 4.4504e-07, 5.0664e-06,\n",
      "          2.5284e-03, 1.5684e-07, 3.7412e-06, 2.9298e-06, 3.5821e-05,\n",
      "          5.1221e-01, 7.6977e-07, 1.3940e-05, 1.7147e-05, 5.7572e-04,\n",
      "          1.3901e-03, 5.9775e-07, 2.8982e-05, 3.0623e-05, 1.3527e-07,\n",
      "          4.0217e-08, 6.6011e-08, 7.4412e-08, 1.1630e-07, 5.8532e-08,\n",
      "          7.5843e-08],\n",
      "         [7.3622e-07, 5.1105e-07, 5.1771e-07, 1.0057e-06, 8.5522e-07,\n",
      "          7.8229e-07, 5.4423e-07, 9.6220e-05, 1.0031e-04, 3.3662e-06,\n",
      "          2.6449e-02, 2.9101e-03, 3.0579e-04, 2.3368e-04, 1.1726e-06,\n",
      "          2.8308e-01, 4.1716e-05, 1.9108e-05, 1.8958e-05, 6.9752e-07,\n",
      "          3.8933e-05, 1.5885e-05, 9.7779e-08, 3.2889e-07, 1.3054e-07,\n",
      "          1.2400e-01, 2.6930e-07, 2.2672e-05, 3.3390e-05, 6.6636e-06,\n",
      "          3.7915e-01, 8.1372e-07, 6.5909e-05, 5.1245e-05, 2.8674e-05,\n",
      "          1.1411e-01, 1.8822e-06, 1.4117e-04, 1.2089e-04, 1.2098e-02,\n",
      "          5.6721e-02, 1.3778e-06, 5.8211e-05, 6.9305e-05, 7.5042e-07,\n",
      "          3.3628e-07, 4.5493e-07, 2.4328e-07, 5.7134e-07, 2.8613e-07,\n",
      "          7.4304e-07],\n",
      "         [6.6521e-08, 4.7086e-08, 3.9965e-08, 3.5580e-08, 6.7642e-08,\n",
      "          5.3113e-08, 1.8767e-08, 6.7770e-07, 4.5052e-07, 2.0257e-07,\n",
      "          1.8991e-05, 6.8619e-06, 1.2806e-05, 7.7593e-06, 7.4988e-08,\n",
      "          5.6359e-03, 1.3397e-06, 1.7044e-06, 1.9720e-06, 3.8111e-08,\n",
      "          9.9369e-01, 3.6068e-07, 4.0944e-05, 3.3320e-04, 9.0923e-06,\n",
      "          4.4438e-05, 4.5674e-08, 1.2857e-07, 1.0851e-07, 3.6123e-07,\n",
      "          4.4954e-05, 4.1826e-08, 4.9070e-07, 2.7112e-07, 9.3278e-07,\n",
      "          1.1096e-04, 1.1342e-07, 3.4499e-07, 4.8889e-07, 6.8526e-06,\n",
      "          2.2623e-05, 1.4187e-07, 3.8946e-07, 4.1523e-07, 4.9149e-08,\n",
      "          2.7895e-08, 4.5374e-08, 6.2317e-08, 4.9577e-08, 2.2762e-08,\n",
      "          3.8866e-08],\n",
      "         [7.1534e-06, 1.0536e-05, 1.2443e-05, 8.9919e-06, 8.1546e-06,\n",
      "          1.2781e-05, 1.7719e-06, 1.1456e-04, 6.0725e-05, 1.5146e-05,\n",
      "          2.6593e-02, 1.2351e-03, 5.6036e-04, 4.8666e-04, 9.6872e-06,\n",
      "          7.5035e-02, 6.4071e-04, 1.3191e-04, 1.2844e-04, 9.3091e-06,\n",
      "          1.2449e-01, 5.4255e-05, 5.4738e-05, 9.8499e-05, 4.1356e-06,\n",
      "          2.8947e-01, 6.5494e-06, 2.1542e-04, 1.8977e-04, 3.1833e-05,\n",
      "          4.1587e-01, 8.1499e-06, 4.8689e-04, 3.8643e-04, 5.7971e-04,\n",
      "          3.6763e-02, 1.0156e-05, 3.8974e-04, 6.5719e-04, 2.2810e-03,\n",
      "          2.2590e-02, 7.4134e-06, 9.8395e-05, 1.4041e-04, 6.7066e-06,\n",
      "          3.1128e-06, 5.6836e-06, 5.9396e-06, 7.7971e-06, 2.8233e-06,\n",
      "          4.6395e-06]]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 25130\n",
      "Average episode length: 2.5 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 4541/10000 (45.4%)\n",
      "    Average reward: +0.168\n",
      "    Reward range: -6.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 5459/10000 (54.6%)\n",
      "    Average reward: -0.168\n",
      "    Reward range: -7.0 to +6.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 4088 (31.2%)\n",
      "    Action 1: 2873 (22.0%)\n",
      "    Action 2: 4955 (37.9%)\n",
      "    Action 3: 1171 (8.9%)\n",
      "  Player 1:\n",
      "    Action 0: 3459 (28.7%)\n",
      "    Action 1: 4485 (37.2%)\n",
      "    Action 2: 2723 (22.6%)\n",
      "    Action 3: 1376 (11.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [1679.5, -1679.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 1.005 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Player 1 strategy entropy: 1.048 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 1.026\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: -0.1679\n",
      "   Testing specific player: 1\n",
      "   At training step: 49999\n",
      "ðŸŽ¯ Test policies: ['average_strategy', 'average_strategy']\n",
      "Player 1 Prediction: tensor([[0.0411, 0.0269, 0.9320, 0.0000]])\n",
      "\n",
      "ðŸ“Š TEST RESULTS SUMMARY\n",
      "==================================================\n",
      "Training step: 49999\n",
      "Episodes completed: 10000/10000\n",
      "Total steps: 41167\n",
      "Average episode length: 4.1 steps\n",
      "Episode length range: 1 - 8\n",
      "\n",
      "ðŸ† PLAYER PERFORMANCE:\n",
      "  Player 0:\n",
      "    Wins: 6883/10000 (68.8%)\n",
      "    Average reward: -0.167\n",
      "    Reward range: -7.0 to +7.0\n",
      "  Player 1:\n",
      "    Wins: 3117/10000 (31.2%)\n",
      "    Average reward: +0.167\n",
      "    Reward range: -7.0 to +7.0\n",
      "\n",
      "ðŸŽ² ACTION FREQUENCIES:\n",
      "  Player 0:\n",
      "    Action 0: 15277 (74.7%)\n",
      "    Action 1: 5171 (25.3%)\n",
      "    Action 2: 0 (0.0%)\n",
      "    Action 3: 0 (0.0%)\n",
      "  Player 1:\n",
      "    Action 0: 3617 (17.5%)\n",
      "    Action 1: 10336 (49.9%)\n",
      "    Action 2: 3780 (18.2%)\n",
      "    Action 3: 2986 (14.4%)\n",
      "\n",
      "âš–ï¸  GAME BALANCE ANALYSIS:\n",
      "  Total rewards per player: [-1668.5, 1668.5]\n",
      "  Sum of all rewards: 0.0 (should be ~0 for zero-sum games)\n",
      "  âœ… Game appears balanced (zero-sum)\n",
      "\n",
      "ðŸ§  STRATEGY ANALYSIS:\n",
      "  Player 0 strategy entropy: 0.816 (max=1.0 for random)\n",
      "    â†’ Mixed strategy\n",
      "  Player 1 strategy entropy: 0.940 (max=1.0 for random)\n",
      "    â†’ Playing nearly random strategy\n",
      "  Average strategy entropy: 0.878\n",
      "  âœ… Both players using mixed strategies (good for Nash equilibrium)\n",
      "   Player 1 average reward: 0.1668\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.50285}, {'exploitability': 0.50825}, {'exploitability': 0.4278}, {'exploitability': 0.32372500000000004}, {'exploitability': 0.2499}, {'exploitability': 0.28045}, {'exploitability': 0.24725000000000003}, {'exploitability': 0.196375}, {'exploitability': 0.226275}, {'exploitability': 0.21725}, {'exploitability': 0.17065}, {'exploitability': 0.1308}, {'exploitability': 0.1975}, {'exploitability': 0.15000000000000002}, {'exploitability': 0.156825}, {'exploitability': 0.1449}, {'exploitability': 0.153275}, {'exploitability': 0.208925}, {'exploitability': 0.19032500000000002}, {'exploitability': 0.176975}, {'exploitability': 0.187075}, {'exploitability': 0.14705}, {'exploitability': 0.17220000000000002}, {'exploitability': 0.1592}, {'exploitability': 0.19674999999999998}]\n",
      "Plotting test_score...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
