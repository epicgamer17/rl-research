{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7491063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 2,\n",
    "    \"stochastic\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.1,\n",
    "    \"reanalyze_noise\": False,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": False,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a659894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 20000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using default learning_rate                 : 0.001\n",
      "Using default clipnorm                      : 0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default loss_function                 : <class 'modules.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 8\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using default min_replay_buffer_size        : 8\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using default norm_type                     : none\n",
      "Using         world_model_cls               : <class 'modules.muzero_world_model.MuzeroWorldModel'>\n",
      "Using default norm_type                     : batch\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : [(24, 3, 1)]\n",
      "Using default conv_layers                   : []\n",
      "Using default dense_layer_widths            : []\n",
      "Using default representation_residual_layers: [(24, 3, 1)]\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: []\n",
      "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : []\n",
      "Using         reward_conv_layers            : [(16, 1, 1)]\n",
      "Using         reward_dense_layer_widths     : []\n",
      "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
      "Using         to_play_dense_layer_widths    : []\n",
      "Using         critic_conv_layers            : [(16, 1, 1)]\n",
      "Using         critic_dense_layer_widths     : []\n",
      "Using         actor_conv_layers             : [(16, 1, 1)]\n",
      "Using         actor_dense_layer_widths      : []\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using default to_play_loss_factor           : 1.0\n",
      "Using default weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.25\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 25\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using default temperature_updates           : [5]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default value_loss_function           : <modules.utils.MSELoss object at 0x3188b2110>\n",
      "Using default reward_loss_function          : <modules.utils.MSELoss object at 0x3188b20e0>\n",
      "Using         policy_loss_function          : <modules.utils.CategoricalCrossentropyLoss object at 0x103d5d810>\n",
      "Using default to_play_loss_function         : <modules.utils.CategoricalCrossentropyLoss object at 0x3188b2140>\n",
      "Using         n_step                        : 10\n",
      "Using default discount_factor               : 1.0\n",
      "Using default unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_use_initial_max_priority  : True\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using default num_workers                   : 4\n",
      "Using default lr_ratio                      : inf\n",
      "Using         transfer_interval             : 1\n",
      "Using         reanalyze_ratio               : 0.0\n",
      "Using         reanalyze_method              : mcts\n",
      "Using default reanalyze_tau                 : 0.3\n",
      "Using         injection_frac                : 0.0\n",
      "Using         reanalyze_noise               : True\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         gumbel                        : False\n",
      "Using         gumbel_m                      : 16\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using         consistency_loss_factor       : 0.0\n",
      "Using         projector_output_dim          : 128\n",
      "Using         projector_hidden_dim          : 128\n",
      "Using         predictor_output_dim          : 128\n",
      "Using         predictor_hidden_dim          : 64\n",
      "Using default mask_absorbing                : False\n",
      "Using         value_prefix                  : False\n",
      "Using default lstm_horizon_len              : 5\n",
      "Using default lstm_hidden_size              : 64\n",
      "Using default q_estimation_method           : v_mix\n",
      "Using         stochastic                    : False\n",
      "Using default use_true_chance_codes         : False\n",
      "Using default num_chance                    : 32\n",
      "Using default sigma_loss                    : <modules.utils.CategoricalCrossentropyLoss object at 0x3188b21a0>\n",
      "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
      "Using default afterstate_conv_layers        : []\n",
      "Using default afterstate_dense_layer_widths : []\n",
      "Using         chance_conv_layers            : [(16, 1, 1)]\n",
      "Using         chance_dense_layer_widths     : []\n",
      "Using default vqvae_commitment_cost_factor  : 1.0\n",
      "Using default action_embedding_dim          : 32\n",
      "Using default single_action_plane           : False\n",
      "[refactored_search_test] Using device: cpu\n",
      "Observation dimensions: (9, 3, 3)\n",
      "Num actions: 9 (Discrete: True)\n",
      "Making test env...\n",
      "Test env configured for video recording.\n",
      "MARL Agent 'refactored_search_test' initialized. Test agents: ['random', 'tictactoe_expert']\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "encoder input shape (8, 18, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "encoder input shape (8, 18, 3, 3)\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'cons_loss' with subkeys None\n",
      "Initializing stat 'q_loss' with subkeys None\n",
      "Initializing stat 'sigma_loss' with subkeys None\n",
      "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'num_codes' with subkeys None\n",
      "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "[Worker 1] Starting self-play...\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 2] Starting self-play...\n",
      "[Worker 3] Starting self-play...\n",
      "0\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7],\n",
      "        [0, 4, 2, 3, 7]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[-0.0522],\n",
      "        [-0.0522],\n",
      "        [-0.0522],\n",
      "        [-0.0522],\n",
      "        [-0.0522],\n",
      "        [-0.0522],\n",
      "        [-0.0522],\n",
      "        [-0.0522]], grad_fn=<AddmmBackward0>), tensor([[-0.0617],\n",
      "        [-0.0617],\n",
      "        [-0.0617],\n",
      "        [-0.0617],\n",
      "        [-0.0617],\n",
      "        [-0.0617],\n",
      "        [-0.0617],\n",
      "        [-0.0617]], grad_fn=<AddmmBackward0>), tensor([[0.0928],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0928],\n",
      "        [0.0928]], grad_fn=<AddmmBackward0>), tensor([[-0.0938],\n",
      "        [-0.0938],\n",
      "        [-0.0938],\n",
      "        [-0.0938],\n",
      "        [-0.0938],\n",
      "        [-0.0938],\n",
      "        [-0.0938],\n",
      "        [-0.0938]], grad_fn=<AddmmBackward0>), tensor([[-0.3959],\n",
      "        [-0.3959],\n",
      "        [-0.3959],\n",
      "        [-0.3959],\n",
      "        [-0.3959],\n",
      "        [-0.3959],\n",
      "        [-0.3959],\n",
      "        [-0.3959]], grad_fn=<AddmmBackward0>), tensor([[-0.4658],\n",
      "        [-0.4658],\n",
      "        [-0.4658],\n",
      "        [-0.4658],\n",
      "        [-0.4658],\n",
      "        [-0.4658],\n",
      "        [-0.4658],\n",
      "        [-0.4658]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.2811],\n",
      "        [-0.2811],\n",
      "        [-0.2811],\n",
      "        [-0.2811],\n",
      "        [-0.2811],\n",
      "        [-0.2811],\n",
      "        [-0.2811],\n",
      "        [-0.2811]], grad_fn=<AddmmBackward0>), tensor([[0.3443],\n",
      "        [0.3443],\n",
      "        [0.3443],\n",
      "        [0.3443],\n",
      "        [0.3443],\n",
      "        [0.3443],\n",
      "        [0.3443],\n",
      "        [0.3443]], grad_fn=<AddmmBackward0>), tensor([[0.3082],\n",
      "        [0.3082],\n",
      "        [0.3082],\n",
      "        [0.3082],\n",
      "        [0.3082],\n",
      "        [0.3082],\n",
      "        [0.3082],\n",
      "        [0.3082]], grad_fn=<AddmmBackward0>), tensor([[0.1673],\n",
      "        [0.1673],\n",
      "        [0.1673],\n",
      "        [0.1673],\n",
      "        [0.1673],\n",
      "        [0.1673],\n",
      "        [0.1673],\n",
      "        [0.1673]], grad_fn=<AddmmBackward0>), tensor([[-0.1524],\n",
      "        [-0.1524],\n",
      "        [-0.1524],\n",
      "        [-0.1524],\n",
      "        [-0.1524],\n",
      "        [-0.1524],\n",
      "        [-0.1524],\n",
      "        [-0.1524]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[0.3665, 0.6335],\n",
      "        [0.3665, 0.6335],\n",
      "        [0.3665, 0.6335],\n",
      "        [0.3665, 0.6335],\n",
      "        [0.3665, 0.6335],\n",
      "        [0.3665, 0.6335],\n",
      "        [0.3665, 0.6335],\n",
      "        [0.3665, 0.6335]], grad_fn=<SoftmaxBackward0>), tensor([[0.4780, 0.5220],\n",
      "        [0.4780, 0.5220],\n",
      "        [0.4780, 0.5220],\n",
      "        [0.4780, 0.5220],\n",
      "        [0.4780, 0.5220],\n",
      "        [0.4780, 0.5220],\n",
      "        [0.4780, 0.5220],\n",
      "        [0.4780, 0.5220]], grad_fn=<SoftmaxBackward0>), tensor([[0.6884, 0.3116],\n",
      "        [0.6884, 0.3116],\n",
      "        [0.6884, 0.3116],\n",
      "        [0.6884, 0.3116],\n",
      "        [0.6884, 0.3116],\n",
      "        [0.6884, 0.3116],\n",
      "        [0.6884, 0.3116],\n",
      "        [0.6884, 0.3116]], grad_fn=<SoftmaxBackward0>), tensor([[0.6177, 0.3823],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6177, 0.3823],\n",
      "        [0.6177, 0.3823]], grad_fn=<SoftmaxBackward0>), tensor([[0.6715, 0.3285],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.6715, 0.3285]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True]]) tensor([[True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 8, 6, 0, 4],\n",
      "        [0, 2, 1, 8, 0],\n",
      "        [8, 0, 7, 8, 4],\n",
      "        [5, 2, 6, 3, 4],\n",
      "        [7, 6, 2, 3, 5],\n",
      "        [7, 5, 2, 0, 4],\n",
      "        [2, 7, 0, 8, 4],\n",
      "        [3, 0, 7, 8, 4]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.4094],\n",
      "        [-0.8697],\n",
      "        [ 0.7375],\n",
      "        [ 0.6656],\n",
      "        [ 0.7686],\n",
      "        [ 0.5464],\n",
      "        [-0.1758],\n",
      "        [ 0.4951]], grad_fn=<AddmmBackward0>), tensor([[ 0.3449],\n",
      "        [ 0.4649],\n",
      "        [-0.0895],\n",
      "        [-0.2535],\n",
      "        [-0.4785],\n",
      "        [ 0.3010],\n",
      "        [ 0.4258],\n",
      "        [-0.1356]], grad_fn=<AddmmBackward0>), tensor([[-0.2802],\n",
      "        [-0.3191],\n",
      "        [ 0.3657],\n",
      "        [ 0.3595],\n",
      "        [ 0.4908],\n",
      "        [ 0.0860],\n",
      "        [-0.3460],\n",
      "        [-0.0327]], grad_fn=<AddmmBackward0>), tensor([[ 0.3567],\n",
      "        [ 0.3396],\n",
      "        [-0.1439],\n",
      "        [-0.1028],\n",
      "        [-0.2643],\n",
      "        [-0.2490],\n",
      "        [ 0.3271],\n",
      "        [ 0.1071]], grad_fn=<AddmmBackward0>), tensor([[-0.2806],\n",
      "        [-0.2323],\n",
      "        [ 0.3120],\n",
      "        [ 0.3238],\n",
      "        [ 0.4618],\n",
      "        [ 0.2344],\n",
      "        [-0.1881],\n",
      "        [-0.0834]], grad_fn=<AddmmBackward0>), tensor([[ 0.2888],\n",
      "        [ 0.3569],\n",
      "        [-0.0963],\n",
      "        [-0.1814],\n",
      "        [-0.2089],\n",
      "        [-0.2131],\n",
      "        [ 0.3409],\n",
      "        [-0.0493]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2201],\n",
      "        [-0.0332],\n",
      "        [ 0.4330],\n",
      "        [ 0.0273],\n",
      "        [ 0.0498],\n",
      "        [-0.0442],\n",
      "        [-0.0121],\n",
      "        [ 0.3141]], grad_fn=<AddmmBackward0>), tensor([[ 0.2174],\n",
      "        [ 0.1522],\n",
      "        [ 0.1988],\n",
      "        [ 0.1703],\n",
      "        [-0.0050],\n",
      "        [ 0.1602],\n",
      "        [ 0.0200],\n",
      "        [ 0.3047]], grad_fn=<AddmmBackward0>), tensor([[ 0.1395],\n",
      "        [ 0.0436],\n",
      "        [ 0.1695],\n",
      "        [ 0.2112],\n",
      "        [ 0.1085],\n",
      "        [ 0.1465],\n",
      "        [-0.0383],\n",
      "        [ 0.1173]], grad_fn=<AddmmBackward0>), tensor([[0.1523],\n",
      "        [0.1834],\n",
      "        [0.1766],\n",
      "        [0.1263],\n",
      "        [0.0714],\n",
      "        [0.0935],\n",
      "        [0.1889],\n",
      "        [0.1216]], grad_fn=<AddmmBackward0>), tensor([[0.1018],\n",
      "        [0.0349],\n",
      "        [0.2209],\n",
      "        [0.2014],\n",
      "        [0.1273],\n",
      "        [0.1378],\n",
      "        [0.1208],\n",
      "        [0.1611]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[0.9601, 0.0399],\n",
      "        [0.9774, 0.0226],\n",
      "        [0.0549, 0.9451],\n",
      "        [0.0101, 0.9899],\n",
      "        [0.0193, 0.9807],\n",
      "        [0.1016, 0.8984],\n",
      "        [0.9620, 0.0380],\n",
      "        [0.5699, 0.4301]], grad_fn=<SoftmaxBackward0>), tensor([[0.0198, 0.9802],\n",
      "        [0.0143, 0.9857],\n",
      "        [0.8836, 0.1164],\n",
      "        [0.9167, 0.0833],\n",
      "        [0.9377, 0.0623],\n",
      "        [0.8242, 0.1758],\n",
      "        [0.0308, 0.9692],\n",
      "        [0.4686, 0.5314]], grad_fn=<SoftmaxBackward0>), tensor([[0.9563, 0.0437],\n",
      "        [0.9624, 0.0376],\n",
      "        [0.1340, 0.8660],\n",
      "        [0.0858, 0.9142],\n",
      "        [0.0471, 0.9529],\n",
      "        [0.0574, 0.9426],\n",
      "        [0.9544, 0.0456],\n",
      "        [0.4597, 0.5403]], grad_fn=<SoftmaxBackward0>), tensor([[0.0222, 0.9778],\n",
      "        [0.0206, 0.9794],\n",
      "        [0.8819, 0.1181],\n",
      "        [0.8325, 0.1675],\n",
      "        [0.9365, 0.0635],\n",
      "        [0.9106, 0.0894],\n",
      "        [0.0354, 0.9646],\n",
      "        [0.2205, 0.7795]], grad_fn=<SoftmaxBackward0>), tensor([[0.8667, 0.1333],\n",
      "        [0.9274, 0.0726],\n",
      "        [0.0531, 0.9469],\n",
      "        [0.0320, 0.9680],\n",
      "        [0.0197, 0.9803],\n",
      "        [0.0350, 0.9650],\n",
      "        [0.8738, 0.1262],\n",
      "        [0.3362, 0.6638]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 8, 3, 1, 0],\n",
      "        [5, 0, 0, 3, 2],\n",
      "        [5, 7, 1, 4, 6],\n",
      "        [3, 8, 4, 0, 2],\n",
      "        [8, 1, 0, 0, 2],\n",
      "        [8, 5, 2, 3, 4],\n",
      "        [8, 1, 0, 3, 2],\n",
      "        [8, 4, 5, 6, 1]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[-0.3726],\n",
      "        [-0.1702],\n",
      "        [-0.1435],\n",
      "        [-0.3579],\n",
      "        [ 0.3425],\n",
      "        [ 0.2969],\n",
      "        [-0.1695],\n",
      "        [-0.1576]], grad_fn=<AddmmBackward0>), tensor([[ 0.1159],\n",
      "        [ 0.0971],\n",
      "        [-0.3155],\n",
      "        [ 0.2227],\n",
      "        [-0.2128],\n",
      "        [-0.2722],\n",
      "        [ 0.2667],\n",
      "        [ 0.3225]], grad_fn=<AddmmBackward0>), tensor([[-0.1289],\n",
      "        [-0.0217],\n",
      "        [ 0.0393],\n",
      "        [-0.0980],\n",
      "        [ 0.1489],\n",
      "        [ 0.0974],\n",
      "        [-0.1354],\n",
      "        [-0.2850]], grad_fn=<AddmmBackward0>), tensor([[-0.0180],\n",
      "        [-0.0442],\n",
      "        [ 0.1216],\n",
      "        [-0.0905],\n",
      "        [-0.0434],\n",
      "        [-0.0232],\n",
      "        [-0.0018],\n",
      "        [-0.0737]], grad_fn=<AddmmBackward0>), tensor([[ 0.0763],\n",
      "        [-0.0310],\n",
      "        [-0.0653],\n",
      "        [ 0.0152],\n",
      "        [-0.0775],\n",
      "        [ 0.0744],\n",
      "        [ 0.0107],\n",
      "        [ 0.0048]], grad_fn=<AddmmBackward0>), tensor([[ 0.0273],\n",
      "        [-0.0896],\n",
      "        [ 0.0936],\n",
      "        [-0.0100],\n",
      "        [ 0.0285],\n",
      "        [-0.0116],\n",
      "        [-0.0171],\n",
      "        [ 0.1169]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.1311],\n",
      "        [0.2850],\n",
      "        [0.1148],\n",
      "        [0.1720],\n",
      "        [0.2688],\n",
      "        [0.0206],\n",
      "        [0.2117],\n",
      "        [0.2116]], grad_fn=<AddmmBackward0>), tensor([[ 0.1906],\n",
      "        [-0.1166],\n",
      "        [ 0.1162],\n",
      "        [ 0.2722],\n",
      "        [ 0.1102],\n",
      "        [ 0.0546],\n",
      "        [ 0.2091],\n",
      "        [ 0.2430]], grad_fn=<AddmmBackward0>), tensor([[ 0.0428],\n",
      "        [-0.0374],\n",
      "        [ 0.1561],\n",
      "        [ 0.0411],\n",
      "        [ 0.1547],\n",
      "        [ 0.0259],\n",
      "        [ 0.0809],\n",
      "        [ 0.0016]], grad_fn=<AddmmBackward0>), tensor([[ 0.1597],\n",
      "        [ 0.0132],\n",
      "        [ 0.1396],\n",
      "        [-0.0118],\n",
      "        [ 0.0049],\n",
      "        [-0.0558],\n",
      "        [ 0.0499],\n",
      "        [ 0.0739]], grad_fn=<AddmmBackward0>), tensor([[ 0.0358],\n",
      "        [-0.0014],\n",
      "        [ 0.1120],\n",
      "        [-0.0328],\n",
      "        [ 0.0342],\n",
      "        [ 0.0669],\n",
      "        [ 0.0052],\n",
      "        [ 0.0764]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9857e-01, 1.4342e-03],\n",
      "        [9.7319e-02, 9.0268e-01],\n",
      "        [7.3704e-04, 9.9926e-01],\n",
      "        [9.4441e-01, 5.5590e-02],\n",
      "        [2.9643e-03, 9.9704e-01],\n",
      "        [1.8508e-04, 9.9981e-01],\n",
      "        [9.6944e-01, 3.0557e-02],\n",
      "        [9.9094e-01, 9.0595e-03]], grad_fn=<SoftmaxBackward0>), tensor([[0.0028, 0.9972],\n",
      "        [0.6589, 0.3411],\n",
      "        [0.9981, 0.0019],\n",
      "        [0.0160, 0.9840],\n",
      "        [0.9988, 0.0012],\n",
      "        [0.9964, 0.0036],\n",
      "        [0.0202, 0.9798],\n",
      "        [0.0102, 0.9898]], grad_fn=<SoftmaxBackward0>), tensor([[0.9863, 0.0137],\n",
      "        [0.1366, 0.8634],\n",
      "        [0.0022, 0.9978],\n",
      "        [0.9780, 0.0220],\n",
      "        [0.0088, 0.9912],\n",
      "        [0.0011, 0.9989],\n",
      "        [0.9282, 0.0718],\n",
      "        [0.9924, 0.0076]], grad_fn=<SoftmaxBackward0>), tensor([[0.0098, 0.9902],\n",
      "        [0.1544, 0.8456],\n",
      "        [0.9960, 0.0040],\n",
      "        [0.0200, 0.9800],\n",
      "        [0.9864, 0.0136],\n",
      "        [0.9859, 0.0141],\n",
      "        [0.0045, 0.9955],\n",
      "        [0.0059, 0.9941]], grad_fn=<SoftmaxBackward0>), tensor([[0.9903, 0.0097],\n",
      "        [0.7815, 0.2185],\n",
      "        [0.0028, 0.9972],\n",
      "        [0.9699, 0.0301],\n",
      "        [0.0059, 0.9941],\n",
      "        [0.0205, 0.9795],\n",
      "        [0.9825, 0.0175],\n",
      "        [0.9963, 0.0037]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 3, 8, 0, 2],\n",
      "        [8, 7, 6, 4, 0],\n",
      "        [1, 0, 6, 4, 2],\n",
      "        [8, 6, 2, 1, 0],\n",
      "        [6, 7, 5, 8, 0],\n",
      "        [8, 1, 4, 6, 2],\n",
      "        [3, 2, 0, 4, 5],\n",
      "        [1, 2, 0, 4, 2]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.7285],\n",
      "        [-0.2530],\n",
      "        [ 0.5759],\n",
      "        [-0.6410],\n",
      "        [ 0.2789],\n",
      "        [-0.1020],\n",
      "        [ 0.4939],\n",
      "        [ 0.2587]], grad_fn=<AddmmBackward0>), tensor([[-0.1830],\n",
      "        [ 0.2204],\n",
      "        [-0.1316],\n",
      "        [ 0.2742],\n",
      "        [ 0.1059],\n",
      "        [ 0.0989],\n",
      "        [-0.1609],\n",
      "        [ 0.2469]], grad_fn=<AddmmBackward0>), tensor([[ 0.4116],\n",
      "        [-0.1234],\n",
      "        [ 0.0783],\n",
      "        [-0.1986],\n",
      "        [-0.1891],\n",
      "        [ 0.0836],\n",
      "        [ 0.3098],\n",
      "        [ 0.1689]], grad_fn=<AddmmBackward0>), tensor([[-0.2824],\n",
      "        [ 0.1747],\n",
      "        [-0.0010],\n",
      "        [ 0.1363],\n",
      "        [ 0.2995],\n",
      "        [ 0.0916],\n",
      "        [-0.2628],\n",
      "        [-0.1112]], grad_fn=<AddmmBackward0>), tensor([[ 0.3177],\n",
      "        [-0.2845],\n",
      "        [ 0.0124],\n",
      "        [-0.1015],\n",
      "        [-0.2343],\n",
      "        [-0.0764],\n",
      "        [ 0.1721],\n",
      "        [ 0.0193]], grad_fn=<AddmmBackward0>), tensor([[-0.1802],\n",
      "        [ 0.0751],\n",
      "        [-0.2165],\n",
      "        [ 0.1255],\n",
      "        [ 0.0200],\n",
      "        [ 0.0761],\n",
      "        [-0.1347],\n",
      "        [ 0.0006]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2199],\n",
      "        [ 0.2802],\n",
      "        [ 0.5030],\n",
      "        [ 0.2378],\n",
      "        [ 0.2998],\n",
      "        [-0.0099],\n",
      "        [-0.0893],\n",
      "        [ 0.1955]], grad_fn=<AddmmBackward0>), tensor([[ 0.0289],\n",
      "        [ 0.2252],\n",
      "        [-0.0294],\n",
      "        [ 0.1794],\n",
      "        [ 0.2982],\n",
      "        [ 0.0675],\n",
      "        [-0.1282],\n",
      "        [ 0.1134]], grad_fn=<AddmmBackward0>), tensor([[ 0.2495],\n",
      "        [ 0.1466],\n",
      "        [ 0.1630],\n",
      "        [ 0.1871],\n",
      "        [ 0.0644],\n",
      "        [ 0.2109],\n",
      "        [-0.1289],\n",
      "        [ 0.1088]], grad_fn=<AddmmBackward0>), tensor([[ 0.0979],\n",
      "        [ 0.2668],\n",
      "        [-0.0704],\n",
      "        [ 0.1905],\n",
      "        [ 0.2431],\n",
      "        [ 0.2741],\n",
      "        [ 0.0014],\n",
      "        [ 0.0823]], grad_fn=<AddmmBackward0>), tensor([[ 0.1029],\n",
      "        [ 0.0087],\n",
      "        [ 0.0475],\n",
      "        [-0.0265],\n",
      "        [ 0.0273],\n",
      "        [ 0.2099],\n",
      "        [ 0.1422],\n",
      "        [ 0.1558]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[8.6212e-04, 9.9914e-01],\n",
      "        [9.9618e-01, 3.8210e-03],\n",
      "        [3.1996e-03, 9.9680e-01],\n",
      "        [9.9912e-01, 8.8200e-04],\n",
      "        [9.9217e-01, 7.8288e-03],\n",
      "        [9.9553e-01, 4.4678e-03],\n",
      "        [2.4698e-04, 9.9975e-01],\n",
      "        [2.9608e-01, 7.0392e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9821e-01, 1.7910e-03],\n",
      "        [6.1602e-03, 9.9384e-01],\n",
      "        [9.9298e-01, 7.0241e-03],\n",
      "        [5.1274e-04, 9.9949e-01],\n",
      "        [1.0657e-02, 9.8934e-01],\n",
      "        [3.5012e-03, 9.9650e-01],\n",
      "        [9.9942e-01, 5.7751e-04],\n",
      "        [9.6263e-01, 3.7368e-02]], grad_fn=<SoftmaxBackward0>), tensor([[1.1796e-03, 9.9882e-01],\n",
      "        [9.9463e-01, 5.3655e-03],\n",
      "        [1.5481e-02, 9.8452e-01],\n",
      "        [9.9756e-01, 2.4359e-03],\n",
      "        [9.9945e-01, 5.5113e-04],\n",
      "        [9.6815e-01, 3.1845e-02],\n",
      "        [9.7809e-04, 9.9902e-01],\n",
      "        [6.1134e-03, 9.9389e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9806e-01, 1.9365e-03],\n",
      "        [4.8374e-04, 9.9952e-01],\n",
      "        [9.7080e-01, 2.9205e-02],\n",
      "        [4.2913e-04, 9.9957e-01],\n",
      "        [1.3026e-04, 9.9987e-01],\n",
      "        [3.0665e-02, 9.6933e-01],\n",
      "        [9.9883e-01, 1.1686e-03],\n",
      "        [9.7986e-01, 2.0145e-02]], grad_fn=<SoftmaxBackward0>), tensor([[5.0254e-04, 9.9950e-01],\n",
      "        [9.9886e-01, 1.1380e-03],\n",
      "        [1.0208e-02, 9.8979e-01],\n",
      "        [9.9845e-01, 1.5511e-03],\n",
      "        [9.9885e-01, 1.1469e-03],\n",
      "        [9.9648e-01, 3.5247e-03],\n",
      "        [2.5704e-03, 9.9743e-01],\n",
      "        [2.9351e-02, 9.7065e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 3, 8, 7, 4],\n",
      "        [8, 0, 3, 7, 7],\n",
      "        [6, 3, 0, 7, 7],\n",
      "        [0, 2, 7, 5, 4],\n",
      "        [0, 7, 2, 6, 8],\n",
      "        [4, 8, 6, 7, 5],\n",
      "        [3, 1, 5, 2, 4],\n",
      "        [8, 4, 7, 2, 0]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1868],\n",
      "        [ 0.1628],\n",
      "        [ 0.2326],\n",
      "        [-0.0408],\n",
      "        [ 0.2842],\n",
      "        [ 0.1103],\n",
      "        [-0.5587],\n",
      "        [-0.6026]], grad_fn=<AddmmBackward0>), tensor([[-0.3934],\n",
      "        [ 0.4180],\n",
      "        [-0.0310],\n",
      "        [ 0.2744],\n",
      "        [ 0.0588],\n",
      "        [ 0.1274],\n",
      "        [ 0.3123],\n",
      "        [ 0.2877]], grad_fn=<AddmmBackward0>), tensor([[ 0.5032],\n",
      "        [ 0.1728],\n",
      "        [ 0.2256],\n",
      "        [-0.0514],\n",
      "        [-0.0525],\n",
      "        [ 0.1303],\n",
      "        [-0.1168],\n",
      "        [-0.0694]], grad_fn=<AddmmBackward0>), tensor([[-0.0474],\n",
      "        [ 0.2248],\n",
      "        [-0.0089],\n",
      "        [ 0.3862],\n",
      "        [ 0.0531],\n",
      "        [ 0.1252],\n",
      "        [ 0.5112],\n",
      "        [ 0.2773]], grad_fn=<AddmmBackward0>), tensor([[ 0.2904],\n",
      "        [ 0.0839],\n",
      "        [ 0.1252],\n",
      "        [ 0.2600],\n",
      "        [ 0.0610],\n",
      "        [ 0.1112],\n",
      "        [-0.2532],\n",
      "        [ 0.0834]], grad_fn=<AddmmBackward0>), tensor([[-0.0642],\n",
      "        [ 0.3384],\n",
      "        [ 0.0762],\n",
      "        [ 0.1547],\n",
      "        [ 0.2995],\n",
      "        [ 0.2646],\n",
      "        [ 0.0065],\n",
      "        [ 0.2485]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0908],\n",
      "        [ 0.2734],\n",
      "        [ 0.4732],\n",
      "        [ 0.1130],\n",
      "        [ 0.0036],\n",
      "        [ 0.0618],\n",
      "        [-0.0798],\n",
      "        [-0.0342]], grad_fn=<AddmmBackward0>), tensor([[0.0037],\n",
      "        [0.2205],\n",
      "        [0.1661],\n",
      "        [0.2568],\n",
      "        [0.1222],\n",
      "        [0.2078],\n",
      "        [0.2063],\n",
      "        [0.2207]], grad_fn=<AddmmBackward0>), tensor([[0.1025],\n",
      "        [0.0646],\n",
      "        [0.1362],\n",
      "        [0.0992],\n",
      "        [0.1053],\n",
      "        [0.0840],\n",
      "        [0.0223],\n",
      "        [0.0532]], grad_fn=<AddmmBackward0>), tensor([[0.1005],\n",
      "        [0.2465],\n",
      "        [0.0239],\n",
      "        [0.0965],\n",
      "        [0.0835],\n",
      "        [0.1312],\n",
      "        [0.2293],\n",
      "        [0.1601]], grad_fn=<AddmmBackward0>), tensor([[0.1901],\n",
      "        [0.0892],\n",
      "        [0.0900],\n",
      "        [0.0677],\n",
      "        [0.1446],\n",
      "        [0.0184],\n",
      "        [0.2556],\n",
      "        [0.0159]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[2.8849e-05, 9.9997e-01],\n",
      "        [9.9831e-01, 1.6886e-03],\n",
      "        [5.1379e-02, 9.4862e-01],\n",
      "        [9.8949e-01, 1.0514e-02],\n",
      "        [9.9566e-01, 4.3353e-03],\n",
      "        [9.7970e-01, 2.0295e-02],\n",
      "        [9.9941e-01, 5.8998e-04],\n",
      "        [9.9830e-01, 1.6959e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.9988e-01, 1.1834e-04],\n",
      "        [6.4658e-03, 9.9353e-01],\n",
      "        [9.9939e-01, 6.1102e-04],\n",
      "        [1.4880e-02, 9.8512e-01],\n",
      "        [3.0718e-02, 9.6928e-01],\n",
      "        [1.2071e-01, 8.7929e-01],\n",
      "        [5.0094e-04, 9.9950e-01],\n",
      "        [4.7188e-04, 9.9953e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.4746e-04, 9.9955e-01],\n",
      "        [9.9972e-01, 2.7548e-04],\n",
      "        [1.3376e-02, 9.8662e-01],\n",
      "        [9.9447e-01, 5.5288e-03],\n",
      "        [9.2476e-01, 7.5237e-02],\n",
      "        [9.6332e-01, 3.6681e-02],\n",
      "        [9.9980e-01, 2.0015e-04],\n",
      "        [9.9969e-01, 3.1282e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9886e-01, 1.1387e-03],\n",
      "        [4.3367e-03, 9.9566e-01],\n",
      "        [9.9133e-01, 8.6665e-03],\n",
      "        [6.3042e-03, 9.9370e-01],\n",
      "        [1.3146e-02, 9.8685e-01],\n",
      "        [1.2273e-01, 8.7727e-01],\n",
      "        [5.1227e-04, 9.9949e-01],\n",
      "        [6.0451e-03, 9.9395e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.1865e-04, 9.9968e-01],\n",
      "        [9.9561e-01, 4.3896e-03],\n",
      "        [6.6419e-02, 9.3358e-01],\n",
      "        [9.4323e-01, 5.6768e-02],\n",
      "        [9.9693e-01, 3.0727e-03],\n",
      "        [9.9446e-01, 5.5438e-03],\n",
      "        [9.9401e-01, 5.9926e-03],\n",
      "        [9.9536e-01, 4.6350e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 8, 0, 2, 3],\n",
      "        [1, 7, 2, 0, 1],\n",
      "        [3, 4, 8, 2, 0],\n",
      "        [7, 3, 2, 0, 1],\n",
      "        [6, 2, 1, 8, 0],\n",
      "        [6, 7, 4, 8, 3],\n",
      "        [4, 6, 7, 5, 2],\n",
      "        [8, 5, 0, 3, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.5669],\n",
      "        [-0.2727],\n",
      "        [-0.5495],\n",
      "        [ 0.4945],\n",
      "        [-0.5669],\n",
      "        [-0.2706],\n",
      "        [ 0.3690],\n",
      "        [-0.1474]], grad_fn=<AddmmBackward0>), tensor([[ 0.1727],\n",
      "        [ 0.2125],\n",
      "        [ 0.0729],\n",
      "        [ 0.0940],\n",
      "        [ 0.1261],\n",
      "        [ 0.1158],\n",
      "        [-0.2168],\n",
      "        [ 0.0715]], grad_fn=<AddmmBackward0>), tensor([[-0.0039],\n",
      "        [-0.0236],\n",
      "        [-0.4432],\n",
      "        [ 0.3684],\n",
      "        [-0.3947],\n",
      "        [-0.1115],\n",
      "        [ 0.0129],\n",
      "        [-0.0576]], grad_fn=<AddmmBackward0>), tensor([[ 0.0044],\n",
      "        [ 0.1813],\n",
      "        [-0.1015],\n",
      "        [ 0.0235],\n",
      "        [ 0.1018],\n",
      "        [ 0.0993],\n",
      "        [-0.0033],\n",
      "        [ 0.0472]], grad_fn=<AddmmBackward0>), tensor([[-0.0805],\n",
      "        [-0.1848],\n",
      "        [-0.3117],\n",
      "        [ 0.1943],\n",
      "        [-0.1572],\n",
      "        [-0.1087],\n",
      "        [ 0.2719],\n",
      "        [ 0.0156]], grad_fn=<AddmmBackward0>), tensor([[ 0.1915],\n",
      "        [ 0.1854],\n",
      "        [-0.0765],\n",
      "        [ 0.0436],\n",
      "        [-0.0682],\n",
      "        [ 0.2790],\n",
      "        [-0.0306],\n",
      "        [ 0.0477]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0543],\n",
      "        [ 0.1040],\n",
      "        [-0.0621],\n",
      "        [ 0.2400],\n",
      "        [-0.1122],\n",
      "        [-0.0680],\n",
      "        [ 0.1277],\n",
      "        [ 0.1895]], grad_fn=<AddmmBackward0>), tensor([[ 0.3492],\n",
      "        [ 0.2816],\n",
      "        [ 0.2065],\n",
      "        [-0.0060],\n",
      "        [ 0.0403],\n",
      "        [ 0.0596],\n",
      "        [-0.0735],\n",
      "        [ 0.3055]], grad_fn=<AddmmBackward0>), tensor([[-2.2081e-02],\n",
      "        [ 3.5031e-02],\n",
      "        [-3.1177e-04],\n",
      "        [ 3.3974e-01],\n",
      "        [ 6.7475e-02],\n",
      "        [-6.9310e-02],\n",
      "        [ 2.6402e-01],\n",
      "        [ 8.8220e-02]], grad_fn=<AddmmBackward0>), tensor([[ 0.1823],\n",
      "        [-0.0461],\n",
      "        [ 0.2464],\n",
      "        [-0.0389],\n",
      "        [ 0.2955],\n",
      "        [ 0.2413],\n",
      "        [ 0.0561],\n",
      "        [ 0.1356]], grad_fn=<AddmmBackward0>), tensor([[ 0.0390],\n",
      "        [-0.0840],\n",
      "        [-0.0423],\n",
      "        [ 0.0577],\n",
      "        [-0.0470],\n",
      "        [ 0.0725],\n",
      "        [ 0.3174],\n",
      "        [-0.0272]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9989e-01, 1.1274e-04],\n",
      "        [9.9957e-01, 4.3084e-04],\n",
      "        [9.9983e-01, 1.7382e-04],\n",
      "        [2.3271e-04, 9.9977e-01],\n",
      "        [9.9991e-01, 9.3034e-05],\n",
      "        [9.9992e-01, 8.4935e-05],\n",
      "        [1.1689e-03, 9.9883e-01],\n",
      "        [9.9632e-01, 3.6810e-03]], grad_fn=<SoftmaxBackward0>), tensor([[2.3930e-04, 9.9976e-01],\n",
      "        [1.3820e-03, 9.9862e-01],\n",
      "        [3.6579e-04, 9.9963e-01],\n",
      "        [9.9950e-01, 4.9687e-04],\n",
      "        [8.0923e-04, 9.9919e-01],\n",
      "        [5.2658e-04, 9.9947e-01],\n",
      "        [9.9963e-01, 3.7440e-04],\n",
      "        [2.1489e-02, 9.7851e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9945e-01, 5.4734e-04],\n",
      "        [9.9958e-01, 4.2396e-04],\n",
      "        [9.9905e-01, 9.4704e-04],\n",
      "        [1.4851e-04, 9.9985e-01],\n",
      "        [9.9947e-01, 5.3418e-04],\n",
      "        [9.9743e-01, 2.5672e-03],\n",
      "        [1.2780e-03, 9.9872e-01],\n",
      "        [9.9668e-01, 3.3158e-03]], grad_fn=<SoftmaxBackward0>), tensor([[2.2114e-03, 9.9779e-01],\n",
      "        [5.0545e-03, 9.9495e-01],\n",
      "        [1.1522e-03, 9.9885e-01],\n",
      "        [9.9952e-01, 4.7907e-04],\n",
      "        [7.1583e-04, 9.9928e-01],\n",
      "        [1.8359e-03, 9.9816e-01],\n",
      "        [9.9978e-01, 2.1921e-04],\n",
      "        [1.6352e-03, 9.9836e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9969e-01, 3.1100e-04],\n",
      "        [9.9966e-01, 3.3788e-04],\n",
      "        [9.9839e-01, 1.6057e-03],\n",
      "        [5.3361e-04, 9.9947e-01],\n",
      "        [9.9937e-01, 6.3104e-04],\n",
      "        [9.9950e-01, 4.9586e-04],\n",
      "        [4.9172e-04, 9.9951e-01],\n",
      "        [9.9416e-01, 5.8390e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 8, 6, 7, 5],\n",
      "        [5, 8, 3, 0, 6],\n",
      "        [3, 4, 2, 7, 0],\n",
      "        [4, 6, 5, 0, 6],\n",
      "        [7, 1, 8, 5, 3],\n",
      "        [5, 0, 7, 0, 6],\n",
      "        [0, 6, 0, 0, 6],\n",
      "        [8, 6, 4, 0, 6]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.6055],\n",
      "        [ 0.4348],\n",
      "        [ 0.1963],\n",
      "        [ 0.1359],\n",
      "        [ 0.6495],\n",
      "        [ 0.6779],\n",
      "        [-0.1625],\n",
      "        [-0.0792]], grad_fn=<AddmmBackward0>), tensor([[ 0.2499],\n",
      "        [ 0.2665],\n",
      "        [ 0.5057],\n",
      "        [-0.2027],\n",
      "        [-0.0420],\n",
      "        [ 0.0382],\n",
      "        [ 0.3992],\n",
      "        [-0.0982]], grad_fn=<AddmmBackward0>), tensor([[-0.0623],\n",
      "        [-0.1733],\n",
      "        [-0.2231],\n",
      "        [ 0.0926],\n",
      "        [ 0.3026],\n",
      "        [ 0.2529],\n",
      "        [ 0.0320],\n",
      "        [ 0.3530]], grad_fn=<AddmmBackward0>), tensor([[ 0.2595],\n",
      "        [ 0.4492],\n",
      "        [ 0.2106],\n",
      "        [-0.0989],\n",
      "        [-0.0464],\n",
      "        [-0.1034],\n",
      "        [ 0.2444],\n",
      "        [-0.0924]], grad_fn=<AddmmBackward0>), tensor([[-0.0063],\n",
      "        [-0.1723],\n",
      "        [ 0.0764],\n",
      "        [ 0.1333],\n",
      "        [ 0.2084],\n",
      "        [ 0.0425],\n",
      "        [-0.0807],\n",
      "        [ 0.0273]], grad_fn=<AddmmBackward0>), tensor([[ 0.2906],\n",
      "        [ 0.2745],\n",
      "        [ 0.2547],\n",
      "        [-0.2440],\n",
      "        [-0.1303],\n",
      "        [-0.0797],\n",
      "        [ 0.2461],\n",
      "        [-0.0023]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0269],\n",
      "        [ 0.2309],\n",
      "        [ 0.0790],\n",
      "        [-0.1452],\n",
      "        [ 0.1560],\n",
      "        [ 0.5672],\n",
      "        [ 0.0689],\n",
      "        [ 0.1262]], grad_fn=<AddmmBackward0>), tensor([[ 0.1621],\n",
      "        [ 0.3726],\n",
      "        [ 0.1967],\n",
      "        [ 0.0126],\n",
      "        [ 0.1701],\n",
      "        [ 0.1249],\n",
      "        [ 0.3302],\n",
      "        [-0.0765]], grad_fn=<AddmmBackward0>), tensor([[ 0.2548],\n",
      "        [ 0.1898],\n",
      "        [ 0.2436],\n",
      "        [-0.0514],\n",
      "        [ 0.4452],\n",
      "        [ 0.0537],\n",
      "        [-0.0017],\n",
      "        [ 0.1804]], grad_fn=<AddmmBackward0>), tensor([[ 0.4272],\n",
      "        [ 0.2772],\n",
      "        [ 0.2881],\n",
      "        [ 0.0627],\n",
      "        [ 0.2459],\n",
      "        [-0.1191],\n",
      "        [-0.0722],\n",
      "        [ 0.0100]], grad_fn=<AddmmBackward0>), tensor([[ 0.2554],\n",
      "        [ 0.2262],\n",
      "        [ 0.1473],\n",
      "        [ 0.1429],\n",
      "        [ 0.2808],\n",
      "        [ 0.0185],\n",
      "        [-0.0430],\n",
      "        [ 0.1276]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9866e-01, 1.3434e-03],\n",
      "        [9.9964e-01, 3.6106e-04],\n",
      "        [9.9752e-01, 2.4759e-03],\n",
      "        [1.8182e-02, 9.8182e-01],\n",
      "        [1.8961e-03, 9.9810e-01],\n",
      "        [5.0308e-04, 9.9950e-01],\n",
      "        [9.9708e-01, 2.9186e-03],\n",
      "        [4.2975e-04, 9.9957e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.9572e-04, 9.9960e-01],\n",
      "        [1.3437e-04, 9.9987e-01],\n",
      "        [1.0976e-02, 9.8902e-01],\n",
      "        [9.6473e-01, 3.5272e-02],\n",
      "        [9.9861e-01, 1.3863e-03],\n",
      "        [9.9927e-01, 7.2712e-04],\n",
      "        [1.9858e-04, 9.9980e-01],\n",
      "        [9.9495e-01, 5.0544e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.9763e-01, 2.3727e-03],\n",
      "        [9.9923e-01, 7.7330e-04],\n",
      "        [9.9699e-01, 3.0062e-03],\n",
      "        [7.8560e-03, 9.9214e-01],\n",
      "        [3.1318e-04, 9.9969e-01],\n",
      "        [1.8353e-03, 9.9816e-01],\n",
      "        [9.9526e-01, 4.7422e-03],\n",
      "        [2.3562e-03, 9.9764e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.4359e-04, 9.9956e-01],\n",
      "        [6.1447e-04, 9.9939e-01],\n",
      "        [6.4825e-03, 9.9352e-01],\n",
      "        [9.8685e-01, 1.3155e-02],\n",
      "        [9.9940e-01, 6.0255e-04],\n",
      "        [9.9770e-01, 2.3033e-03],\n",
      "        [3.1697e-03, 9.9683e-01],\n",
      "        [9.9041e-01, 9.5930e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.9932e-01, 6.7672e-04],\n",
      "        [9.9762e-01, 2.3763e-03],\n",
      "        [9.9735e-01, 2.6534e-03],\n",
      "        [2.9846e-04, 9.9970e-01],\n",
      "        [9.9594e-04, 9.9900e-01],\n",
      "        [4.4394e-03, 9.9556e-01],\n",
      "        [9.9275e-01, 7.2476e-03],\n",
      "        [6.8963e-03, 9.9310e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 1, 7, 6, 2],\n",
      "        [2, 8, 6, 0, 6],\n",
      "        [8, 1, 5, 2, 0],\n",
      "        [6, 8, 2, 1, 5],\n",
      "        [1, 6, 2, 0, 6],\n",
      "        [8, 6, 2, 3, 0],\n",
      "        [7, 0, 6, 2, 0],\n",
      "        [7, 6, 0, 8, 6]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3566],\n",
      "        [ 0.3608],\n",
      "        [ 0.2176],\n",
      "        [ 0.3630],\n",
      "        [-0.1920],\n",
      "        [ 0.3201],\n",
      "        [ 0.4700],\n",
      "        [ 0.0718]], grad_fn=<AddmmBackward0>), tensor([[-0.0879],\n",
      "        [-0.2193],\n",
      "        [ 0.1430],\n",
      "        [-0.3671],\n",
      "        [ 0.6554],\n",
      "        [-0.1680],\n",
      "        [-0.2066],\n",
      "        [ 0.1452]], grad_fn=<AddmmBackward0>), tensor([[0.3074],\n",
      "        [0.0479],\n",
      "        [0.0506],\n",
      "        [0.2319],\n",
      "        [0.2438],\n",
      "        [0.2039],\n",
      "        [0.0100],\n",
      "        [0.2437]], grad_fn=<AddmmBackward0>), tensor([[-0.0306],\n",
      "        [-0.0853],\n",
      "        [ 0.2334],\n",
      "        [-0.2392],\n",
      "        [ 0.2414],\n",
      "        [-0.1613],\n",
      "        [-0.0751],\n",
      "        [-0.0687]], grad_fn=<AddmmBackward0>), tensor([[ 0.1944],\n",
      "        [-0.0723],\n",
      "        [-0.1221],\n",
      "        [ 0.2200],\n",
      "        [-0.0484],\n",
      "        [ 0.2838],\n",
      "        [ 0.0460],\n",
      "        [ 0.0327]], grad_fn=<AddmmBackward0>), tensor([[-0.0221],\n",
      "        [-0.0660],\n",
      "        [-0.0505],\n",
      "        [-0.2856],\n",
      "        [ 0.1583],\n",
      "        [-0.1888],\n",
      "        [-0.0869],\n",
      "        [-0.0685]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2638],\n",
      "        [ 0.0299],\n",
      "        [ 0.2342],\n",
      "        [-0.1011],\n",
      "        [ 0.1115],\n",
      "        [ 0.0702],\n",
      "        [ 0.1648],\n",
      "        [ 0.4102]], grad_fn=<AddmmBackward0>), tensor([[-0.0297],\n",
      "        [ 0.0175],\n",
      "        [ 0.2601],\n",
      "        [-0.0699],\n",
      "        [ 0.3946],\n",
      "        [-0.0231],\n",
      "        [-0.0588],\n",
      "        [ 0.2809]], grad_fn=<AddmmBackward0>), tensor([[0.2390],\n",
      "        [0.2044],\n",
      "        [0.0517],\n",
      "        [0.0131],\n",
      "        [0.2468],\n",
      "        [0.2129],\n",
      "        [0.0823],\n",
      "        [0.2754]], grad_fn=<AddmmBackward0>), tensor([[ 0.1033],\n",
      "        [ 0.0324],\n",
      "        [ 0.2635],\n",
      "        [-0.1280],\n",
      "        [ 0.4469],\n",
      "        [ 0.0956],\n",
      "        [ 0.1001],\n",
      "        [ 0.0852]], grad_fn=<AddmmBackward0>), tensor([[ 0.3909],\n",
      "        [ 0.0925],\n",
      "        [-0.0482],\n",
      "        [-0.0528],\n",
      "        [ 0.2077],\n",
      "        [ 0.0983],\n",
      "        [-0.0440],\n",
      "        [ 0.1749]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.1987e-04, 9.9988e-01],\n",
      "        [2.2679e-04, 9.9977e-01],\n",
      "        [9.9955e-01, 4.4575e-04],\n",
      "        [1.3125e-04, 9.9987e-01],\n",
      "        [9.9893e-01, 1.0740e-03],\n",
      "        [1.1941e-04, 9.9988e-01],\n",
      "        [1.8875e-04, 9.9981e-01],\n",
      "        [4.0899e-04, 9.9959e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9984e-01, 1.6217e-04],\n",
      "        [9.9966e-01, 3.3752e-04],\n",
      "        [8.2048e-04, 9.9918e-01],\n",
      "        [9.9980e-01, 2.0216e-04],\n",
      "        [2.5133e-04, 9.9975e-01],\n",
      "        [9.9962e-01, 3.7811e-04],\n",
      "        [9.9948e-01, 5.1813e-04],\n",
      "        [9.9554e-01, 4.4553e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.0645e-04, 9.9989e-01],\n",
      "        [5.9076e-04, 9.9941e-01],\n",
      "        [9.9859e-01, 1.4086e-03],\n",
      "        [4.4876e-04, 9.9955e-01],\n",
      "        [9.9927e-01, 7.2731e-04],\n",
      "        [8.2639e-04, 9.9917e-01],\n",
      "        [8.3908e-04, 9.9916e-01],\n",
      "        [2.6987e-03, 9.9730e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9974e-01, 2.6314e-04],\n",
      "        [9.9595e-01, 4.0462e-03],\n",
      "        [8.3259e-04, 9.9917e-01],\n",
      "        [9.9975e-01, 2.4939e-04],\n",
      "        [5.3983e-04, 9.9946e-01],\n",
      "        [9.9815e-01, 1.8548e-03],\n",
      "        [9.9872e-01, 1.2774e-03],\n",
      "        [9.9497e-01, 5.0349e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.0775e-03, 9.9892e-01],\n",
      "        [1.0073e-02, 9.8993e-01],\n",
      "        [9.9778e-01, 2.2203e-03],\n",
      "        [5.2387e-05, 9.9995e-01],\n",
      "        [9.9936e-01, 6.3536e-04],\n",
      "        [6.1019e-03, 9.9390e-01],\n",
      "        [1.1258e-02, 9.8874e-01],\n",
      "        [5.4483e-02, 9.4552e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 4, 0, 6, 5],\n",
      "        [7, 1, 4, 8, 2],\n",
      "        [1, 4, 8, 5, 6],\n",
      "        [4, 7, 6, 0, 5],\n",
      "        [1, 4, 3, 0, 2],\n",
      "        [7, 8, 4, 2, 0],\n",
      "        [6, 4, 1, 0, 5],\n",
      "        [0, 6, 2, 3, 4]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 1.5915e-01],\n",
      "        [-5.6148e-01],\n",
      "        [-1.6028e-04],\n",
      "        [-1.9709e-01],\n",
      "        [ 1.4705e-01],\n",
      "        [ 2.8587e-01],\n",
      "        [ 3.4206e-01],\n",
      "        [ 3.4206e-01]], grad_fn=<AddmmBackward0>), tensor([[ 0.4069],\n",
      "        [ 0.3675],\n",
      "        [ 0.2988],\n",
      "        [ 0.0163],\n",
      "        [-0.1538],\n",
      "        [-0.0837],\n",
      "        [-0.3491],\n",
      "        [-0.3902]], grad_fn=<AddmmBackward0>), tensor([[-0.1861],\n",
      "        [ 0.1107],\n",
      "        [-0.2090],\n",
      "        [-0.0699],\n",
      "        [-0.0994],\n",
      "        [ 0.4191],\n",
      "        [ 0.0704],\n",
      "        [ 0.3244]], grad_fn=<AddmmBackward0>), tensor([[ 0.0386],\n",
      "        [ 0.1322],\n",
      "        [ 0.2273],\n",
      "        [ 0.2223],\n",
      "        [-0.1190],\n",
      "        [-0.3856],\n",
      "        [-0.2068],\n",
      "        [-0.2719]], grad_fn=<AddmmBackward0>), tensor([[-0.0281],\n",
      "        [-0.0297],\n",
      "        [-0.2689],\n",
      "        [-0.0398],\n",
      "        [-0.0247],\n",
      "        [ 0.3943],\n",
      "        [ 0.0495],\n",
      "        [ 0.3368]], grad_fn=<AddmmBackward0>), tensor([[ 0.3023],\n",
      "        [ 0.2857],\n",
      "        [ 0.2744],\n",
      "        [ 0.1430],\n",
      "        [-0.0995],\n",
      "        [-0.2738],\n",
      "        [-0.1743],\n",
      "        [-0.2470]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.3687],\n",
      "        [-0.0206],\n",
      "        [-0.0872],\n",
      "        [ 0.3231],\n",
      "        [ 0.1799],\n",
      "        [ 0.0699],\n",
      "        [ 0.0836],\n",
      "        [ 0.0654]], grad_fn=<AddmmBackward0>), tensor([[ 0.5497],\n",
      "        [ 0.0443],\n",
      "        [ 0.1861],\n",
      "        [ 0.2867],\n",
      "        [ 0.1033],\n",
      "        [-0.0273],\n",
      "        [ 0.0699],\n",
      "        [ 0.0462]], grad_fn=<AddmmBackward0>), tensor([[0.0600],\n",
      "        [0.3169],\n",
      "        [0.1096],\n",
      "        [0.3300],\n",
      "        [0.0563],\n",
      "        [0.2985],\n",
      "        [0.1064],\n",
      "        [0.1758]], grad_fn=<AddmmBackward0>), tensor([[ 0.1705],\n",
      "        [ 0.2330],\n",
      "        [ 0.0529],\n",
      "        [ 0.1212],\n",
      "        [ 0.1561],\n",
      "        [ 0.1057],\n",
      "        [-0.0862],\n",
      "        [ 0.0790]], grad_fn=<AddmmBackward0>), tensor([[0.0070],\n",
      "        [0.2190],\n",
      "        [0.0388],\n",
      "        [0.0454],\n",
      "        [0.4424],\n",
      "        [0.2830],\n",
      "        [0.1181],\n",
      "        [0.2589]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9767e-01, 2.3291e-03],\n",
      "        [9.9982e-01, 1.8004e-04],\n",
      "        [9.9990e-01, 1.0450e-04],\n",
      "        [9.9982e-01, 1.8021e-04],\n",
      "        [1.1589e-04, 9.9988e-01],\n",
      "        [8.2379e-05, 9.9992e-01],\n",
      "        [5.8780e-06, 9.9999e-01],\n",
      "        [1.8320e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.3548e-03, 9.9665e-01],\n",
      "        [2.1140e-04, 9.9979e-01],\n",
      "        [4.6086e-05, 9.9995e-01],\n",
      "        [3.1128e-05, 9.9997e-01],\n",
      "        [9.9978e-01, 2.1560e-04],\n",
      "        [9.9926e-01, 7.4240e-04],\n",
      "        [9.9994e-01, 6.1763e-05],\n",
      "        [9.9992e-01, 8.0171e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.7751e-01, 2.2487e-02],\n",
      "        [9.9961e-01, 3.9051e-04],\n",
      "        [9.9993e-01, 7.2281e-05],\n",
      "        [9.9926e-01, 7.3993e-04],\n",
      "        [5.2436e-03, 9.9476e-01],\n",
      "        [9.1814e-05, 9.9991e-01],\n",
      "        [5.0027e-05, 9.9995e-01],\n",
      "        [1.8582e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.8418e-03, 9.9416e-01],\n",
      "        [1.3770e-04, 9.9986e-01],\n",
      "        [6.5303e-05, 9.9993e-01],\n",
      "        [6.4144e-04, 9.9936e-01],\n",
      "        [9.9754e-01, 2.4593e-03],\n",
      "        [9.9935e-01, 6.5231e-04],\n",
      "        [9.9959e-01, 4.1339e-04],\n",
      "        [9.9995e-01, 4.6762e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9924e-01, 7.6415e-04],\n",
      "        [9.9974e-01, 2.5997e-04],\n",
      "        [9.9971e-01, 2.8712e-04],\n",
      "        [9.9946e-01, 5.3518e-04],\n",
      "        [1.2499e-03, 9.9875e-01],\n",
      "        [4.3953e-04, 9.9956e-01],\n",
      "        [3.2807e-05, 9.9997e-01],\n",
      "        [7.4972e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 5, 8, 7, 4],\n",
      "        [3, 4, 2, 8, 0],\n",
      "        [8, 0, 1, 8, 1],\n",
      "        [1, 2, 4, 3, 7],\n",
      "        [2, 0, 1, 8, 1],\n",
      "        [3, 0, 1, 8, 1],\n",
      "        [3, 5, 7, 0, 1],\n",
      "        [1, 0, 2, 8, 3]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.3633],\n",
      "        [ 0.2293],\n",
      "        [ 0.8446],\n",
      "        [ 0.2583],\n",
      "        [ 0.3181],\n",
      "        [ 0.4314],\n",
      "        [ 0.3522],\n",
      "        [-0.3604]], grad_fn=<AddmmBackward0>), tensor([[-0.1295],\n",
      "        [-0.0377],\n",
      "        [-0.1476],\n",
      "        [ 0.0479],\n",
      "        [-0.0367],\n",
      "        [ 0.1233],\n",
      "        [ 0.0994],\n",
      "        [ 0.3107]], grad_fn=<AddmmBackward0>), tensor([[ 0.4302],\n",
      "        [ 0.0733],\n",
      "        [ 0.1190],\n",
      "        [ 0.1081],\n",
      "        [ 0.1218],\n",
      "        [ 0.1017],\n",
      "        [ 0.2300],\n",
      "        [-0.2846]], grad_fn=<AddmmBackward0>), tensor([[-0.2864],\n",
      "        [-0.1739],\n",
      "        [-0.0075],\n",
      "        [-0.0817],\n",
      "        [ 0.0874],\n",
      "        [-0.0591],\n",
      "        [ 0.0600],\n",
      "        [ 0.1595]], grad_fn=<AddmmBackward0>), tensor([[ 0.5131],\n",
      "        [ 0.1879],\n",
      "        [ 0.0376],\n",
      "        [ 0.2910],\n",
      "        [ 0.1047],\n",
      "        [ 0.0255],\n",
      "        [ 0.0628],\n",
      "        [-0.3428]], grad_fn=<AddmmBackward0>), tensor([[-0.1296],\n",
      "        [-0.0877],\n",
      "        [ 0.0008],\n",
      "        [ 0.0598],\n",
      "        [-0.0074],\n",
      "        [-0.0390],\n",
      "        [ 0.0418],\n",
      "        [ 0.3087]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0624],\n",
      "        [-0.1564],\n",
      "        [ 0.3236],\n",
      "        [-0.0825],\n",
      "        [ 0.5125],\n",
      "        [ 0.6042],\n",
      "        [ 0.1490],\n",
      "        [-0.1650]], grad_fn=<AddmmBackward0>), tensor([[ 0.0787],\n",
      "        [ 0.0223],\n",
      "        [ 0.0504],\n",
      "        [-0.0401],\n",
      "        [ 0.1652],\n",
      "        [ 0.0967],\n",
      "        [ 0.1717],\n",
      "        [-0.0839]], grad_fn=<AddmmBackward0>), tensor([[0.2865],\n",
      "        [0.1357],\n",
      "        [0.0744],\n",
      "        [0.2034],\n",
      "        [0.1582],\n",
      "        [0.1534],\n",
      "        [0.4034],\n",
      "        [0.0234]], grad_fn=<AddmmBackward0>), tensor([[0.1948],\n",
      "        [0.1739],\n",
      "        [0.0258],\n",
      "        [0.1058],\n",
      "        [0.0257],\n",
      "        [0.1740],\n",
      "        [0.0496],\n",
      "        [0.1019]], grad_fn=<AddmmBackward0>), tensor([[0.5878],\n",
      "        [0.0420],\n",
      "        [0.0298],\n",
      "        [0.2639],\n",
      "        [0.0948],\n",
      "        [0.0227],\n",
      "        [0.1468],\n",
      "        [0.0663]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.2977e-04, 9.9987e-01],\n",
      "        [2.0351e-04, 9.9980e-01],\n",
      "        [1.8961e-03, 9.9810e-01],\n",
      "        [1.9726e-04, 9.9980e-01],\n",
      "        [3.0441e-03, 9.9696e-01],\n",
      "        [1.1208e-04, 9.9989e-01],\n",
      "        [2.1632e-02, 9.7837e-01],\n",
      "        [1.0000e+00, 3.7339e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 1.1178e-05],\n",
      "        [9.9983e-01, 1.6759e-04],\n",
      "        [9.9672e-01, 3.2792e-03],\n",
      "        [9.9838e-01, 1.6170e-03],\n",
      "        [9.9839e-01, 1.6133e-03],\n",
      "        [9.9947e-01, 5.2835e-04],\n",
      "        [9.9941e-01, 5.8586e-04],\n",
      "        [1.0185e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.5106e-06, 9.9999e-01],\n",
      "        [2.9657e-04, 9.9970e-01],\n",
      "        [7.4578e-04, 9.9925e-01],\n",
      "        [6.0774e-04, 9.9939e-01],\n",
      "        [7.8418e-03, 9.9216e-01],\n",
      "        [2.6550e-04, 9.9973e-01],\n",
      "        [2.1514e-04, 9.9978e-01],\n",
      "        [9.9999e-01, 1.4760e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 1.0659e-05],\n",
      "        [9.9993e-01, 7.3687e-05],\n",
      "        [9.9187e-01, 8.1302e-03],\n",
      "        [9.9922e-01, 7.8206e-04],\n",
      "        [9.9657e-01, 3.4329e-03],\n",
      "        [9.9765e-01, 2.3518e-03],\n",
      "        [9.9647e-01, 3.5271e-03],\n",
      "        [1.6687e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.2792e-05, 9.9997e-01],\n",
      "        [2.1233e-03, 9.9788e-01],\n",
      "        [1.5957e-03, 9.9840e-01],\n",
      "        [2.7260e-04, 9.9973e-01],\n",
      "        [2.7529e-03, 9.9725e-01],\n",
      "        [1.1113e-03, 9.9889e-01],\n",
      "        [1.2954e-03, 9.9870e-01],\n",
      "        [9.9999e-01, 6.1442e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 8, 5, 0, 4],\n",
      "        [4, 0, 0, 6, 4],\n",
      "        [6, 5, 4, 1, 3],\n",
      "        [8, 3, 4, 7, 0],\n",
      "        [7, 1, 3, 5, 8],\n",
      "        [1, 4, 8, 5, 7],\n",
      "        [7, 0, 4, 6, 4],\n",
      "        [7, 5, 8, 0, 4]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3395],\n",
      "        [-0.3344],\n",
      "        [-0.5398],\n",
      "        [ 0.3942],\n",
      "        [ 0.7247],\n",
      "        [-0.5656],\n",
      "        [ 0.5245],\n",
      "        [-0.2499]], grad_fn=<AddmmBackward0>), tensor([[-0.2476],\n",
      "        [-0.0345],\n",
      "        [ 0.2613],\n",
      "        [-0.3718],\n",
      "        [-0.1850],\n",
      "        [ 0.4812],\n",
      "        [ 0.0730],\n",
      "        [ 0.2005]], grad_fn=<AddmmBackward0>), tensor([[ 0.3971],\n",
      "        [-0.1077],\n",
      "        [-0.3955],\n",
      "        [ 0.5468],\n",
      "        [ 0.4440],\n",
      "        [-0.3326],\n",
      "        [ 0.1182],\n",
      "        [-0.1693]], grad_fn=<AddmmBackward0>), tensor([[ 0.0696],\n",
      "        [ 0.0175],\n",
      "        [ 0.1897],\n",
      "        [-0.2630],\n",
      "        [ 0.1011],\n",
      "        [ 0.2634],\n",
      "        [-0.1269],\n",
      "        [ 0.2708]], grad_fn=<AddmmBackward0>), tensor([[ 0.0619],\n",
      "        [-0.1563],\n",
      "        [-0.0868],\n",
      "        [ 0.5562],\n",
      "        [ 0.2915],\n",
      "        [-0.1373],\n",
      "        [ 0.2779],\n",
      "        [-0.0865]], grad_fn=<AddmmBackward0>), tensor([[-0.1363],\n",
      "        [ 0.0354],\n",
      "        [ 0.3836],\n",
      "        [ 0.0066],\n",
      "        [-0.0107],\n",
      "        [ 0.3630],\n",
      "        [-0.1317],\n",
      "        [-0.0204]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2954],\n",
      "        [ 0.1928],\n",
      "        [-0.1255],\n",
      "        [-0.0844],\n",
      "        [ 0.2223],\n",
      "        [-0.1973],\n",
      "        [ 0.4114],\n",
      "        [ 0.2014]], grad_fn=<AddmmBackward0>), tensor([[ 0.1916],\n",
      "        [ 0.2238],\n",
      "        [-0.0807],\n",
      "        [-0.1068],\n",
      "        [ 0.1377],\n",
      "        [ 0.0691],\n",
      "        [ 0.0751],\n",
      "        [ 0.2870]], grad_fn=<AddmmBackward0>), tensor([[ 0.3252],\n",
      "        [ 0.0464],\n",
      "        [-0.0145],\n",
      "        [ 0.1367],\n",
      "        [ 0.1521],\n",
      "        [ 0.0170],\n",
      "        [ 0.0436],\n",
      "        [ 0.2443]], grad_fn=<AddmmBackward0>), tensor([[ 0.0203],\n",
      "        [ 0.0629],\n",
      "        [-0.0909],\n",
      "        [ 0.1886],\n",
      "        [ 0.1868],\n",
      "        [ 0.0533],\n",
      "        [ 0.0460],\n",
      "        [ 0.1366]], grad_fn=<AddmmBackward0>), tensor([[ 0.0302],\n",
      "        [-0.0116],\n",
      "        [ 0.0191],\n",
      "        [ 0.2664],\n",
      "        [ 0.4557],\n",
      "        [ 0.1711],\n",
      "        [-0.0351],\n",
      "        [ 0.0592]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[2.1072e-04, 9.9979e-01],\n",
      "        [9.9991e-01, 8.9475e-05],\n",
      "        [9.9999e-01, 9.1746e-06],\n",
      "        [5.0807e-06, 9.9999e-01],\n",
      "        [2.9200e-04, 9.9971e-01],\n",
      "        [9.9994e-01, 6.0417e-05],\n",
      "        [1.8928e-05, 9.9998e-01],\n",
      "        [9.9981e-01, 1.9143e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.6982e-05],\n",
      "        [2.7296e-03, 9.9727e-01],\n",
      "        [1.6295e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 2.1659e-06],\n",
      "        [9.9980e-01, 2.0441e-04],\n",
      "        [4.3818e-05, 9.9996e-01],\n",
      "        [9.9980e-01, 2.0351e-04],\n",
      "        [6.3562e-03, 9.9364e-01]], grad_fn=<SoftmaxBackward0>), tensor([[7.2872e-05, 9.9993e-01],\n",
      "        [9.9900e-01, 1.0045e-03],\n",
      "        [9.9987e-01, 1.2750e-04],\n",
      "        [1.1031e-04, 9.9989e-01],\n",
      "        [1.9223e-04, 9.9981e-01],\n",
      "        [9.9999e-01, 1.2627e-05],\n",
      "        [5.3908e-04, 9.9946e-01],\n",
      "        [9.9989e-01, 1.0891e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9968e-01, 3.1990e-04],\n",
      "        [2.9844e-03, 9.9702e-01],\n",
      "        [1.4158e-04, 9.9986e-01],\n",
      "        [9.9999e-01, 7.5012e-06],\n",
      "        [9.9998e-01, 2.4571e-05],\n",
      "        [9.2278e-06, 9.9999e-01],\n",
      "        [9.9992e-01, 7.8775e-05],\n",
      "        [3.6043e-02, 9.6396e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.9560e-04, 9.9930e-01],\n",
      "        [9.9957e-01, 4.3134e-04],\n",
      "        [1.0000e+00, 3.4420e-06],\n",
      "        [3.1079e-04, 9.9969e-01],\n",
      "        [6.5391e-05, 9.9993e-01],\n",
      "        [9.9995e-01, 4.5698e-05],\n",
      "        [1.5536e-04, 9.9984e-01],\n",
      "        [9.9947e-01, 5.2941e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0400, 0.2000, 0.0000, 0.0000, 0.4000, 0.0400, 0.1600, 0.1200, 0.0400]), tensor([0.0400, 0.2000, 0.0000, 0.0000, 0.4000, 0.0400, 0.1600, 0.1200, 0.0400]), 0.20066046888510194, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 0\n",
      "Player 0 prediction: (tensor([0.0000, 0.0800, 0.0400, 0.0400, 0.0000, 0.0800, 0.3200, 0.2000, 0.2400]), tensor([0.0000, 0.0800, 0.0400, 0.0400, 0.0000, 0.0800, 0.3200, 0.2000, 0.2400]), 0.20879934124576932, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 2\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.1600, 0.0000, 0.2400, 0.0000, 0.1600, 0.0000, 0.0800, 0.3600]), tensor([0.0000, 0.1600, 0.0000, 0.2400, 0.0000, 0.1600, 0.0000, 0.0800, 0.3600]), 0.2487237282957022, tensor(8))\n",
      "action: 8\n",
      "Player 1 random action: 7\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.4400, 0.0000, 0.3200, 0.0000, 0.2400, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.4400, 0.0000, 0.3200, 0.0000, 0.2400, 0.0000, 0.0000, 0.0000]), 0.2836819388545476, tensor(1))\n",
      "action: 1\n",
      "Player 1 random action: 5\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), 0.4047379151273232, tensor(3))\n",
      "action: 3\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 0, 4, 6, 0],\n",
      "        [3, 7, 0, 6, 0],\n",
      "        [4, 1, 3, 7, 0],\n",
      "        [5, 0, 4, 6, 0],\n",
      "        [1, 2, 0, 0, 0],\n",
      "        [1, 7, 0, 6, 0],\n",
      "        [2, 7, 0, 8, 3],\n",
      "        [4, 8, 2, 1, 6]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.9242],\n",
      "        [-0.3246],\n",
      "        [ 0.0367],\n",
      "        [ 0.1127],\n",
      "        [ 0.7461],\n",
      "        [-0.3886],\n",
      "        [ 0.3249],\n",
      "        [ 0.3249]], grad_fn=<AddmmBackward0>), tensor([[-0.0732],\n",
      "        [ 0.7129],\n",
      "        [ 0.1668],\n",
      "        [ 0.3470],\n",
      "        [-0.0078],\n",
      "        [ 0.6542],\n",
      "        [-0.2670],\n",
      "        [-0.3796]], grad_fn=<AddmmBackward0>), tensor([[ 0.0808],\n",
      "        [ 0.2444],\n",
      "        [-0.1010],\n",
      "        [-0.0500],\n",
      "        [ 0.3174],\n",
      "        [ 0.0550],\n",
      "        [ 0.4576],\n",
      "        [ 0.4542]], grad_fn=<AddmmBackward0>), tensor([[-0.1939],\n",
      "        [ 0.2024],\n",
      "        [ 0.4450],\n",
      "        [ 0.0360],\n",
      "        [ 0.0445],\n",
      "        [ 0.1990],\n",
      "        [-0.1809],\n",
      "        [-0.1552]], grad_fn=<AddmmBackward0>), tensor([[ 0.2336],\n",
      "        [ 0.0812],\n",
      "        [ 0.0348],\n",
      "        [-0.0634],\n",
      "        [ 0.0080],\n",
      "        [ 0.0579],\n",
      "        [ 0.2675],\n",
      "        [ 0.4058]], grad_fn=<AddmmBackward0>), tensor([[-0.0990],\n",
      "        [ 0.0745],\n",
      "        [ 0.2903],\n",
      "        [-0.0030],\n",
      "        [-0.1074],\n",
      "        [ 0.0513],\n",
      "        [ 0.1142],\n",
      "        [-0.1247]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.5551],\n",
      "        [ 0.0452],\n",
      "        [ 0.1481],\n",
      "        [ 0.3388],\n",
      "        [ 0.1408],\n",
      "        [ 0.0651],\n",
      "        [-0.0343],\n",
      "        [-0.0066]], grad_fn=<AddmmBackward0>), tensor([[0.1326],\n",
      "        [0.1196],\n",
      "        [0.0309],\n",
      "        [0.0525],\n",
      "        [0.4290],\n",
      "        [0.2378],\n",
      "        [0.0574],\n",
      "        [0.0477]], grad_fn=<AddmmBackward0>), tensor([[ 0.0904],\n",
      "        [ 0.1316],\n",
      "        [ 0.0393],\n",
      "        [ 0.0680],\n",
      "        [ 0.3499],\n",
      "        [ 0.0687],\n",
      "        [-0.0403],\n",
      "        [ 0.2136]], grad_fn=<AddmmBackward0>), tensor([[0.1041],\n",
      "        [0.0331],\n",
      "        [0.2225],\n",
      "        [0.1485],\n",
      "        [0.0437],\n",
      "        [0.0307],\n",
      "        [0.1051],\n",
      "        [0.0752]], grad_fn=<AddmmBackward0>), tensor([[-0.0408],\n",
      "        [-0.0152],\n",
      "        [ 0.2358],\n",
      "        [ 0.0100],\n",
      "        [-0.0699],\n",
      "        [-0.0187],\n",
      "        [ 0.0063],\n",
      "        [ 0.2181]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[6.9429e-06, 9.9999e-01],\n",
      "        [9.9996e-01, 4.3546e-05],\n",
      "        [9.9993e-01, 7.0888e-05],\n",
      "        [9.9734e-01, 2.6586e-03],\n",
      "        [6.7532e-05, 9.9993e-01],\n",
      "        [9.9990e-01, 1.0245e-04],\n",
      "        [1.4268e-05, 9.9999e-01],\n",
      "        [9.6921e-05, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9994e-01, 6.1805e-05],\n",
      "        [4.1554e-04, 9.9958e-01],\n",
      "        [7.4300e-05, 9.9993e-01],\n",
      "        [9.7489e-02, 9.0251e-01],\n",
      "        [9.9996e-01, 4.4352e-05],\n",
      "        [2.7174e-05, 9.9997e-01],\n",
      "        [9.9995e-01, 5.3183e-05],\n",
      "        [9.9998e-01, 2.3582e-05]], grad_fn=<SoftmaxBackward0>), tensor([[2.9215e-04, 9.9971e-01],\n",
      "        [9.9853e-01, 1.4721e-03],\n",
      "        [9.9998e-01, 1.5383e-05],\n",
      "        [9.9547e-01, 4.5307e-03],\n",
      "        [4.2209e-04, 9.9958e-01],\n",
      "        [9.9983e-01, 1.7010e-04],\n",
      "        [2.4075e-04, 9.9976e-01],\n",
      "        [3.0360e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9861e-01, 1.3906e-03],\n",
      "        [5.7225e-03, 9.9428e-01],\n",
      "        [9.2568e-05, 9.9991e-01],\n",
      "        [5.6051e-04, 9.9944e-01],\n",
      "        [9.8432e-01, 1.5682e-02],\n",
      "        [2.4794e-04, 9.9975e-01],\n",
      "        [9.9990e-01, 9.5572e-05],\n",
      "        [9.9997e-01, 2.5176e-05]], grad_fn=<SoftmaxBackward0>), tensor([[3.1177e-02, 9.6882e-01],\n",
      "        [9.4685e-01, 5.3146e-02],\n",
      "        [9.9994e-01, 6.1094e-05],\n",
      "        [9.9597e-01, 4.0269e-03],\n",
      "        [3.1226e-01, 6.8774e-01],\n",
      "        [9.9028e-01, 9.7218e-03],\n",
      "        [3.9953e-04, 9.9960e-01],\n",
      "        [1.5891e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "Player 0 win percentage vs random: 68.0 and average score: 0.54\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 2\n",
      "Player 1 prediction: (tensor([0.0800, 0.0800, 0.0000, 0.0400, 0.2400, 0.1600, 0.1200, 0.2400, 0.0400]), tensor([0.0800, 0.0800, 0.0000, 0.0400, 0.2400, 0.1600, 0.1200, 0.2400, 0.0400]), -0.32541766863029736, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 1\n",
      "Player 1 prediction: (tensor([0.2400, 0.0000, 0.0000, 0.0400, 0.0000, 0.1600, 0.1600, 0.2000, 0.2000]), tensor([0.2400, 0.0000, 0.0000, 0.0400, 0.0000, 0.1600, 0.1600, 0.2000, 0.2000]), -0.007860636338591576, tensor(0))\n",
      "action: 0\n",
      "Player 0 random action: 6\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.1600, 0.0000, 0.1600, 0.4400]), tensor([0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.1600, 0.0000, 0.1600, 0.4400]), -0.1427762548510845, tensor(8))\n",
      "action: 8\n",
      "1200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 6, 4, 2, 7],\n",
      "        [2, 5, 0, 6, 0],\n",
      "        [8, 1, 6, 0, 2],\n",
      "        [4, 1, 0, 0, 2],\n",
      "        [6, 0, 1, 5, 2],\n",
      "        [7, 0, 1, 5, 2],\n",
      "        [7, 2, 0, 5, 2],\n",
      "        [4, 8, 5, 3, 0]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.4594],\n",
      "        [-0.4029],\n",
      "        [ 0.5477],\n",
      "        [ 0.4449],\n",
      "        [ 0.1967],\n",
      "        [ 0.3673],\n",
      "        [ 0.3485],\n",
      "        [-0.1429]], grad_fn=<AddmmBackward0>), tensor([[ 0.0046],\n",
      "        [ 0.4552],\n",
      "        [-0.6081],\n",
      "        [-0.3443],\n",
      "        [ 0.3028],\n",
      "        [ 0.0176],\n",
      "        [ 0.1112],\n",
      "        [ 0.2271]], grad_fn=<AddmmBackward0>), tensor([[ 0.1145],\n",
      "        [-0.1253],\n",
      "        [ 0.6133],\n",
      "        [ 0.6321],\n",
      "        [-0.1912],\n",
      "        [ 0.0428],\n",
      "        [ 0.1674],\n",
      "        [-0.2876]], grad_fn=<AddmmBackward0>), tensor([[-0.3119],\n",
      "        [ 0.0719],\n",
      "        [-0.2397],\n",
      "        [-0.2929],\n",
      "        [ 0.1898],\n",
      "        [-0.0907],\n",
      "        [-0.0327],\n",
      "        [ 0.3736]], grad_fn=<AddmmBackward0>), tensor([[ 0.2048],\n",
      "        [-0.2220],\n",
      "        [ 0.1625],\n",
      "        [ 0.0559],\n",
      "        [-0.1400],\n",
      "        [ 0.3583],\n",
      "        [ 0.4248],\n",
      "        [-0.1383]], grad_fn=<AddmmBackward0>), tensor([[ 0.0925],\n",
      "        [ 0.1075],\n",
      "        [-0.1623],\n",
      "        [-0.1721],\n",
      "        [ 0.2791],\n",
      "        [-0.2126],\n",
      "        [-0.1461],\n",
      "        [-0.1038]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0183],\n",
      "        [ 0.1393],\n",
      "        [ 0.2755],\n",
      "        [ 0.4300],\n",
      "        [ 0.3178],\n",
      "        [ 0.4798],\n",
      "        [ 0.1831],\n",
      "        [-0.1113]], grad_fn=<AddmmBackward0>), tensor([[-0.0177],\n",
      "        [ 0.2156],\n",
      "        [ 0.2428],\n",
      "        [ 0.2177],\n",
      "        [ 0.1786],\n",
      "        [ 0.0530],\n",
      "        [ 0.1774],\n",
      "        [ 0.0827]], grad_fn=<AddmmBackward0>), tensor([[ 0.0549],\n",
      "        [ 0.1068],\n",
      "        [ 0.6241],\n",
      "        [ 0.4107],\n",
      "        [ 0.1025],\n",
      "        [ 0.0635],\n",
      "        [ 0.3134],\n",
      "        [-0.0884]], grad_fn=<AddmmBackward0>), tensor([[ 0.0257],\n",
      "        [ 0.4568],\n",
      "        [-0.0111],\n",
      "        [-0.0893],\n",
      "        [ 0.1595],\n",
      "        [ 0.0641],\n",
      "        [ 0.1890],\n",
      "        [ 0.0705]], grad_fn=<AddmmBackward0>), tensor([[ 0.1822],\n",
      "        [-0.0400],\n",
      "        [ 0.1965],\n",
      "        [-0.0004],\n",
      "        [ 0.1637],\n",
      "        [ 0.1793],\n",
      "        [ 0.2577],\n",
      "        [ 0.1301]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0001e-04, 9.9990e-01],\n",
      "        [9.9999e-01, 8.3791e-06],\n",
      "        [2.5388e-05, 9.9997e-01],\n",
      "        [1.7367e-03, 9.9826e-01],\n",
      "        [9.9997e-01, 3.0327e-05],\n",
      "        [2.9296e-06, 1.0000e+00],\n",
      "        [3.1477e-05, 9.9997e-01],\n",
      "        [9.9999e-01, 9.8132e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 2.8836e-05],\n",
      "        [2.1376e-04, 9.9979e-01],\n",
      "        [9.9999e-01, 8.0848e-06],\n",
      "        [9.9996e-01, 4.4750e-05],\n",
      "        [1.3044e-03, 9.9870e-01],\n",
      "        [9.9998e-01, 2.0970e-05],\n",
      "        [9.9997e-01, 3.1968e-05],\n",
      "        [1.8440e-04, 9.9982e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.1963e-05, 9.9994e-01],\n",
      "        [9.9990e-01, 9.6193e-05],\n",
      "        [2.9531e-05, 9.9997e-01],\n",
      "        [3.7802e-05, 9.9996e-01],\n",
      "        [9.9980e-01, 1.9620e-04],\n",
      "        [6.7020e-05, 9.9993e-01],\n",
      "        [1.8616e-04, 9.9981e-01],\n",
      "        [9.9997e-01, 2.7990e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.7180e-05],\n",
      "        [1.0211e-03, 9.9898e-01],\n",
      "        [9.9999e-01, 9.5438e-06],\n",
      "        [9.9985e-01, 1.5283e-04],\n",
      "        [2.8149e-04, 9.9972e-01],\n",
      "        [9.9999e-01, 1.1227e-05],\n",
      "        [9.9999e-01, 1.1856e-05],\n",
      "        [2.3973e-03, 9.9760e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.4011e-05, 9.9998e-01],\n",
      "        [9.9995e-01, 5.2171e-05],\n",
      "        [7.0281e-04, 9.9930e-01],\n",
      "        [2.7911e-03, 9.9721e-01],\n",
      "        [9.9996e-01, 4.2858e-05],\n",
      "        [7.7277e-05, 9.9992e-01],\n",
      "        [1.1094e-04, 9.9989e-01],\n",
      "        [9.9980e-01, 1.9980e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 8, 0, 2, 3],\n",
      "        [3, 4, 0, 5, 1],\n",
      "        [5, 0, 7, 3, 8],\n",
      "        [4, 0, 2, 5, 1],\n",
      "        [4, 1, 2, 0, 1],\n",
      "        [8, 3, 4, 1, 2],\n",
      "        [6, 2, 8, 0, 1],\n",
      "        [4, 8, 0, 1, 6]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.6869],\n",
      "        [ 0.0275],\n",
      "        [ 0.3628],\n",
      "        [ 0.3719],\n",
      "        [ 0.8393],\n",
      "        [-0.5062],\n",
      "        [ 0.0522],\n",
      "        [ 0.3628]], grad_fn=<AddmmBackward0>), tensor([[ 0.6748],\n",
      "        [ 0.5645],\n",
      "        [-0.2603],\n",
      "        [-0.0572],\n",
      "        [-0.0893],\n",
      "        [ 0.3497],\n",
      "        [-0.3120],\n",
      "        [-0.6145]], grad_fn=<AddmmBackward0>), tensor([[-0.4302],\n",
      "        [-0.1459],\n",
      "        [ 0.0129],\n",
      "        [ 0.1414],\n",
      "        [ 0.6631],\n",
      "        [-0.3126],\n",
      "        [ 0.2926],\n",
      "        [ 0.3197]], grad_fn=<AddmmBackward0>), tensor([[ 0.0641],\n",
      "        [ 0.0391],\n",
      "        [-0.2091],\n",
      "        [-0.0465],\n",
      "        [ 0.0317],\n",
      "        [ 0.1217],\n",
      "        [-0.0671],\n",
      "        [-0.3526]], grad_fn=<AddmmBackward0>), tensor([[-0.2153],\n",
      "        [-0.1830],\n",
      "        [ 0.2796],\n",
      "        [ 0.1167],\n",
      "        [-0.0441],\n",
      "        [-0.2068],\n",
      "        [ 0.0814],\n",
      "        [ 0.2704]], grad_fn=<AddmmBackward0>), tensor([[ 0.1074],\n",
      "        [ 0.2468],\n",
      "        [-0.1623],\n",
      "        [-0.0133],\n",
      "        [-0.0199],\n",
      "        [ 0.1160],\n",
      "        [ 0.0722],\n",
      "        [-0.2408]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0584],\n",
      "        [ 0.2296],\n",
      "        [ 0.0541],\n",
      "        [ 0.4471],\n",
      "        [ 0.4093],\n",
      "        [ 0.0405],\n",
      "        [ 0.0592],\n",
      "        [ 0.0311]], grad_fn=<AddmmBackward0>), tensor([[ 0.1816],\n",
      "        [ 0.6011],\n",
      "        [-0.0336],\n",
      "        [ 0.3369],\n",
      "        [ 0.3045],\n",
      "        [ 0.0202],\n",
      "        [ 0.0326],\n",
      "        [ 0.0111]], grad_fn=<AddmmBackward0>), tensor([[ 0.0462],\n",
      "        [-0.0581],\n",
      "        [-0.0045],\n",
      "        [ 0.3064],\n",
      "        [ 0.6111],\n",
      "        [ 0.1746],\n",
      "        [ 0.2370],\n",
      "        [-0.0093]], grad_fn=<AddmmBackward0>), tensor([[ 0.1593],\n",
      "        [-0.0102],\n",
      "        [-0.0087],\n",
      "        [ 0.0795],\n",
      "        [ 0.0225],\n",
      "        [ 0.1867],\n",
      "        [ 0.0943],\n",
      "        [ 0.0011]], grad_fn=<AddmmBackward0>), tensor([[0.0535],\n",
      "        [0.1098],\n",
      "        [0.0755],\n",
      "        [0.1632],\n",
      "        [0.0539],\n",
      "        [0.1424],\n",
      "        [0.2603],\n",
      "        [0.2674]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0000e+00, 4.7692e-06],\n",
      "        [9.9997e-01, 2.6818e-05],\n",
      "        [1.5151e-05, 9.9998e-01],\n",
      "        [1.8345e-04, 9.9982e-01],\n",
      "        [4.6950e-04, 9.9953e-01],\n",
      "        [1.0000e+00, 2.4297e-06],\n",
      "        [9.3418e-05, 9.9991e-01],\n",
      "        [2.0617e-04, 9.9979e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.2594e-04, 9.9987e-01],\n",
      "        [7.5032e-04, 9.9925e-01],\n",
      "        [9.9995e-01, 4.6412e-05],\n",
      "        [9.9985e-01, 1.4876e-04],\n",
      "        [9.9980e-01, 1.9825e-04],\n",
      "        [3.7508e-04, 9.9962e-01],\n",
      "        [9.9998e-01, 1.8766e-05],\n",
      "        [1.0000e+00, 2.4765e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 4.0524e-06],\n",
      "        [9.9970e-01, 3.0226e-04],\n",
      "        [1.4757e-03, 9.9852e-01],\n",
      "        [1.1120e-03, 9.9889e-01],\n",
      "        [2.0571e-04, 9.9979e-01],\n",
      "        [9.9997e-01, 2.9204e-05],\n",
      "        [8.1447e-04, 9.9919e-01],\n",
      "        [1.4266e-03, 9.9857e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.0906e-03, 9.9391e-01],\n",
      "        [3.0006e-03, 9.9700e-01],\n",
      "        [9.9993e-01, 6.8632e-05],\n",
      "        [9.9968e-01, 3.2010e-04],\n",
      "        [9.9876e-01, 1.2442e-03],\n",
      "        [5.2032e-03, 9.9480e-01],\n",
      "        [9.9999e-01, 1.4327e-05],\n",
      "        [9.9999e-01, 1.0229e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 9.9833e-06],\n",
      "        [9.9992e-01, 8.3355e-05],\n",
      "        [9.5304e-04, 9.9905e-01],\n",
      "        [9.8148e-05, 9.9990e-01],\n",
      "        [9.3213e-03, 9.9068e-01],\n",
      "        [9.9998e-01, 1.9564e-05],\n",
      "        [2.9082e-02, 9.7092e-01],\n",
      "        [1.3943e-04, 9.9986e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 28.000000000000004 and average score: -0.28\n",
      "Results vs random: {'player_0_score': 0.54, 'player_0_win%': 0.68, 'player_1_score': -0.28, 'player_1_win%': 0.28, 'score': 0.13}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0400, 0.0400, 0.0000, 0.0000, 0.8000, 0.0000, 0.0400, 0.0400, 0.0400]), tensor([0.0400, 0.0400, 0.0000, 0.0000, 0.8000, 0.0000, 0.0400, 0.0400, 0.0400]), 0.36552847047837883, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 5\n",
      "Player 0 prediction: (tensor([0.2400, 0.0400, 0.0400, 0.0400, 0.0000, 0.0000, 0.2000, 0.0400, 0.4000]), tensor([0.2400, 0.0400, 0.0400, 0.0400, 0.0000, 0.0000, 0.2000, 0.0400, 0.4000]), 0.3826911865841024, tensor(8))\n",
      "action: 8\n",
      "Player 1 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.1600, 0.2400, 0.0800, 0.0000, 0.0000, 0.2000, 0.3200, 0.0000]), tensor([0.0000, 0.1600, 0.2400, 0.0800, 0.0000, 0.0000, 0.2000, 0.3200, 0.0000]), 0.38860837284189004, tensor(7))\n",
      "action: 7\n",
      "Player 1 tictactoe_expert action: 6\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.1600, 0.2000, 0.6400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.1600, 0.2000, 0.6400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 0.3682347752440434, tensor(3))\n",
      "action: 3\n",
      "Player 1 tictactoe_expert action: 1\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.]), 0.4984356408508924, tensor(2))\n",
      "action: 2\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 7, 5, 2, 0],\n",
      "        [2, 6, 8, 0, 1],\n",
      "        [6, 1, 0, 8, 4],\n",
      "        [4, 1, 8, 6, 7],\n",
      "        [1, 0, 2, 7, 8],\n",
      "        [6, 0, 0, 7, 8],\n",
      "        [6, 0, 0, 7, 8],\n",
      "        [4, 0, 5, 2, 7]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.1554],\n",
      "        [-0.4034],\n",
      "        [ 0.3131],\n",
      "        [-0.1649],\n",
      "        [ 0.1574],\n",
      "        [-0.3479],\n",
      "        [-0.2234],\n",
      "        [ 0.1663]], grad_fn=<AddmmBackward0>), tensor([[ 0.0796],\n",
      "        [ 0.3183],\n",
      "        [-0.3205],\n",
      "        [-0.1737],\n",
      "        [ 0.0066],\n",
      "        [ 0.6962],\n",
      "        [ 0.4767],\n",
      "        [-0.1575]], grad_fn=<AddmmBackward0>), tensor([[ 0.2659],\n",
      "        [-0.3318],\n",
      "        [ 0.4734],\n",
      "        [-0.1134],\n",
      "        [-0.0284],\n",
      "        [-0.0140],\n",
      "        [-0.0305],\n",
      "        [ 0.1849]], grad_fn=<AddmmBackward0>), tensor([[-0.1314],\n",
      "        [ 0.2056],\n",
      "        [-0.4427],\n",
      "        [ 0.1772],\n",
      "        [-0.1326],\n",
      "        [-0.0306],\n",
      "        [-0.0669],\n",
      "        [-0.1276]], grad_fn=<AddmmBackward0>), tensor([[ 0.3190],\n",
      "        [-0.1867],\n",
      "        [ 0.3503],\n",
      "        [-0.0250],\n",
      "        [ 0.0285],\n",
      "        [-0.0282],\n",
      "        [-0.1109],\n",
      "        [ 0.2876]], grad_fn=<AddmmBackward0>), tensor([[-0.1926],\n",
      "        [ 0.3860],\n",
      "        [-0.1795],\n",
      "        [ 0.2463],\n",
      "        [-0.0523],\n",
      "        [-0.0978],\n",
      "        [-0.1566],\n",
      "        [-0.0158]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0890],\n",
      "        [-0.0564],\n",
      "        [ 0.0820],\n",
      "        [ 0.0534],\n",
      "        [ 0.4683],\n",
      "        [ 0.3684],\n",
      "        [ 0.3458],\n",
      "        [ 0.0221]], grad_fn=<AddmmBackward0>), tensor([[ 0.0206],\n",
      "        [ 0.1161],\n",
      "        [-0.0217],\n",
      "        [-0.0662],\n",
      "        [ 0.0832],\n",
      "        [ 0.6804],\n",
      "        [ 0.5594],\n",
      "        [ 0.1054]], grad_fn=<AddmmBackward0>), tensor([[0.1728],\n",
      "        [0.0359],\n",
      "        [0.1512],\n",
      "        [0.1064],\n",
      "        [0.1760],\n",
      "        [0.1175],\n",
      "        [0.1079],\n",
      "        [0.1258]], grad_fn=<AddmmBackward0>), tensor([[0.2008],\n",
      "        [0.2926],\n",
      "        [0.1054],\n",
      "        [0.2068],\n",
      "        [0.0539],\n",
      "        [0.1114],\n",
      "        [0.0516],\n",
      "        [0.2656]], grad_fn=<AddmmBackward0>), tensor([[0.3632],\n",
      "        [0.1416],\n",
      "        [0.2655],\n",
      "        [0.2410],\n",
      "        [0.1167],\n",
      "        [0.0946],\n",
      "        [0.0772],\n",
      "        [0.4337]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[6.5355e-04, 9.9935e-01],\n",
      "        [9.9999e-01, 5.0169e-06],\n",
      "        [4.9760e-06, 9.9999e-01],\n",
      "        [9.9994e-01, 5.6067e-05],\n",
      "        [5.8615e-05, 9.9994e-01],\n",
      "        [9.9846e-01, 1.5383e-03],\n",
      "        [9.9298e-01, 7.0150e-03],\n",
      "        [3.7216e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9986e-01, 1.3679e-04],\n",
      "        [5.4605e-05, 9.9995e-01],\n",
      "        [9.9999e-01, 1.0233e-05],\n",
      "        [4.6649e-05, 9.9995e-01],\n",
      "        [9.9968e-01, 3.1572e-04],\n",
      "        [7.9415e-04, 9.9921e-01],\n",
      "        [2.4796e-03, 9.9752e-01],\n",
      "        [9.9994e-01, 6.4073e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.2564e-03, 9.9874e-01],\n",
      "        [9.9997e-01, 2.5431e-05],\n",
      "        [2.3762e-05, 9.9998e-01],\n",
      "        [9.9999e-01, 1.2388e-05],\n",
      "        [1.8708e-03, 9.9813e-01],\n",
      "        [9.8670e-01, 1.3296e-02],\n",
      "        [9.8036e-01, 1.9644e-02],\n",
      "        [1.3183e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 4.4346e-06],\n",
      "        [4.2006e-05, 9.9996e-01],\n",
      "        [9.9994e-01, 5.8881e-05],\n",
      "        [2.7365e-05, 9.9997e-01],\n",
      "        [9.5721e-01, 4.2794e-02],\n",
      "        [6.4294e-03, 9.9357e-01],\n",
      "        [5.3820e-03, 9.9462e-01],\n",
      "        [9.9997e-01, 2.9113e-05]], grad_fn=<SoftmaxBackward0>), tensor([[2.2875e-04, 9.9977e-01],\n",
      "        [9.9998e-01, 1.7637e-05],\n",
      "        [1.9089e-05, 9.9998e-01],\n",
      "        [9.9958e-01, 4.1540e-04],\n",
      "        [1.0769e-02, 9.8923e-01],\n",
      "        [7.5346e-01, 2.4654e-01],\n",
      "        [8.7796e-01, 1.2204e-01],\n",
      "        [2.3686e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 14.000000000000002 and average score: -0.42\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.1200, 0.0800, 0.0800, 0.2400, 0.2800, 0.0800, 0.0800, 0.0400, 0.0000]), tensor([0.1200, 0.0800, 0.0800, 0.2400, 0.2800, 0.0800, 0.0800, 0.0400, 0.0000]), -0.3097494919426166, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 5\n",
      "Player 1 prediction: (tensor([0.3600, 0.1600, 0.1200, 0.0800, 0.0000, 0.0000, 0.1200, 0.1600, 0.0000]), tensor([0.3600, 0.1600, 0.1200, 0.0800, 0.0000, 0.0000, 0.1200, 0.1600, 0.0000]), -0.07331452518701553, tensor(0))\n",
      "action: 0\n",
      "Player 0 tictactoe_expert action: 2\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 2, 0, 1, 6],\n",
      "        [0, 0, 3, 1, 6],\n",
      "        [4, 6, 3, 7, 2],\n",
      "        [1, 7, 4, 2, 6],\n",
      "        [7, 6, 8, 5, 3],\n",
      "        [8, 6, 7, 5, 0],\n",
      "        [2, 6, 4, 1, 3],\n",
      "        [2, 6, 8, 7, 5]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[-0.2045],\n",
      "        [ 0.5561],\n",
      "        [ 0.0519],\n",
      "        [ 0.0809],\n",
      "        [ 0.3368],\n",
      "        [-0.2961],\n",
      "        [-0.2916],\n",
      "        [-0.3876]], grad_fn=<AddmmBackward0>), tensor([[ 0.0500],\n",
      "        [-0.0363],\n",
      "        [-0.2216],\n",
      "        [ 0.1159],\n",
      "        [-0.1470],\n",
      "        [ 0.4084],\n",
      "        [-0.0074],\n",
      "        [ 0.1462]], grad_fn=<AddmmBackward0>), tensor([[-0.1238],\n",
      "        [ 0.0132],\n",
      "        [ 0.0741],\n",
      "        [-0.0320],\n",
      "        [ 0.2128],\n",
      "        [ 0.0735],\n",
      "        [-0.0819],\n",
      "        [ 0.0466]], grad_fn=<AddmmBackward0>), tensor([[-0.1499],\n",
      "        [ 0.0727],\n",
      "        [ 0.1807],\n",
      "        [-0.2755],\n",
      "        [ 0.1202],\n",
      "        [ 0.4165],\n",
      "        [-0.1866],\n",
      "        [ 0.1305]], grad_fn=<AddmmBackward0>), tensor([[0.0976],\n",
      "        [0.0989],\n",
      "        [0.1846],\n",
      "        [0.1275],\n",
      "        [0.0592],\n",
      "        [0.0289],\n",
      "        [0.0400],\n",
      "        [0.3055]], grad_fn=<AddmmBackward0>), tensor([[-0.1194],\n",
      "        [-0.0392],\n",
      "        [ 0.1005],\n",
      "        [-0.0938],\n",
      "        [ 0.0920],\n",
      "        [-0.1607],\n",
      "        [ 0.0467],\n",
      "        [ 0.0939]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0671],\n",
      "        [ 0.2792],\n",
      "        [ 0.1367],\n",
      "        [-0.0450],\n",
      "        [ 0.2087],\n",
      "        [ 0.2628],\n",
      "        [ 0.0218],\n",
      "        [ 0.2741]], grad_fn=<AddmmBackward0>), tensor([[ 0.0903],\n",
      "        [ 0.0333],\n",
      "        [ 0.0745],\n",
      "        [-0.0586],\n",
      "        [ 0.1304],\n",
      "        [ 0.5028],\n",
      "        [ 0.0336],\n",
      "        [ 0.3409]], grad_fn=<AddmmBackward0>), tensor([[ 0.0189],\n",
      "        [-0.0101],\n",
      "        [ 0.1687],\n",
      "        [-0.0420],\n",
      "        [ 0.4246],\n",
      "        [ 0.2613],\n",
      "        [ 0.0589],\n",
      "        [ 0.2527]], grad_fn=<AddmmBackward0>), tensor([[0.1505],\n",
      "        [0.0024],\n",
      "        [0.2490],\n",
      "        [0.0147],\n",
      "        [0.1651],\n",
      "        [0.4600],\n",
      "        [0.1043],\n",
      "        [0.3031]], grad_fn=<AddmmBackward0>), tensor([[ 0.2340],\n",
      "        [-0.0539],\n",
      "        [ 0.3356],\n",
      "        [ 0.1113],\n",
      "        [ 0.3095],\n",
      "        [-0.0892],\n",
      "        [ 0.1562],\n",
      "        [ 0.3025]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.4126e-04, 9.9986e-01],\n",
      "        [3.4772e-06, 1.0000e+00],\n",
      "        [9.9861e-01, 1.3866e-03],\n",
      "        [1.0035e-04, 9.9990e-01],\n",
      "        [1.2507e-05, 9.9999e-01],\n",
      "        [9.9998e-01, 1.6042e-05],\n",
      "        [9.9999e-01, 8.7065e-06],\n",
      "        [4.9793e-04, 9.9950e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9926e-01, 7.4025e-04],\n",
      "        [9.9969e-01, 3.1009e-04],\n",
      "        [1.1378e-04, 9.9989e-01],\n",
      "        [9.9855e-01, 1.4538e-03],\n",
      "        [9.9989e-01, 1.1131e-04],\n",
      "        [1.4695e-05, 9.9999e-01],\n",
      "        [1.8920e-04, 9.9981e-01],\n",
      "        [9.9755e-01, 2.4464e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.0504e-02, 9.8950e-01],\n",
      "        [3.1415e-03, 9.9686e-01],\n",
      "        [9.9989e-01, 1.1021e-04],\n",
      "        [2.2944e-05, 9.9998e-01],\n",
      "        [1.7549e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 4.8712e-06],\n",
      "        [9.9931e-01, 6.8551e-04],\n",
      "        [1.8261e-03, 9.9817e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9962e-01, 3.8216e-04],\n",
      "        [9.9948e-01, 5.1783e-04],\n",
      "        [1.4899e-05, 9.9999e-01],\n",
      "        [9.9977e-01, 2.3420e-04],\n",
      "        [9.9983e-01, 1.6672e-04],\n",
      "        [1.9040e-06, 1.0000e+00],\n",
      "        [8.9347e-04, 9.9911e-01],\n",
      "        [9.9704e-01, 2.9633e-03]], grad_fn=<SoftmaxBackward0>), tensor([[2.9944e-04, 9.9970e-01],\n",
      "        [1.3692e-04, 9.9986e-01],\n",
      "        [9.9964e-01, 3.5878e-04],\n",
      "        [1.1747e-04, 9.9988e-01],\n",
      "        [8.8038e-06, 9.9999e-01],\n",
      "        [9.9591e-01, 4.0941e-03],\n",
      "        [9.9985e-01, 1.4920e-04],\n",
      "        [6.5064e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 2.0 and average score: -0.82\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.42, 'player_0_win%': 0.14, 'player_1_score': -0.82, 'player_1_win%': 0.02, 'score': -0.62}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 5, 2, 0, 4],\n",
      "        [7, 0, 0, 0, 4],\n",
      "        [4, 0, 5, 2, 0],\n",
      "        [6, 3, 7, 4, 8],\n",
      "        [5, 8, 4, 0, 6],\n",
      "        [6, 2, 0, 3, 8],\n",
      "        [0, 0, 4, 0, 4],\n",
      "        [6, 1, 7, 5, 0]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[0.0895],\n",
      "        [0.6562],\n",
      "        [0.3642],\n",
      "        [0.2135],\n",
      "        [0.1407],\n",
      "        [0.2135],\n",
      "        [0.4996],\n",
      "        [0.2566]], grad_fn=<AddmmBackward0>), tensor([[ 0.3631],\n",
      "        [-0.0207],\n",
      "        [-0.3373],\n",
      "        [-0.1763],\n",
      "        [ 0.3820],\n",
      "        [-0.1763],\n",
      "        [ 0.0251],\n",
      "        [-0.0617]], grad_fn=<AddmmBackward0>), tensor([[ 0.1966],\n",
      "        [ 0.0783],\n",
      "        [ 0.1254],\n",
      "        [ 0.3229],\n",
      "        [-0.1154],\n",
      "        [ 0.1858],\n",
      "        [-0.1111],\n",
      "        [ 0.3510]], grad_fn=<AddmmBackward0>), tensor([[ 0.0644],\n",
      "        [-0.0096],\n",
      "        [ 0.0138],\n",
      "        [-0.0429],\n",
      "        [ 0.1208],\n",
      "        [-0.1122],\n",
      "        [-0.1658],\n",
      "        [ 0.1380]], grad_fn=<AddmmBackward0>), tensor([[ 0.0037],\n",
      "        [-0.0420],\n",
      "        [-0.0973],\n",
      "        [-0.1470],\n",
      "        [-0.0126],\n",
      "        [ 0.2862],\n",
      "        [-0.0058],\n",
      "        [ 0.1364]], grad_fn=<AddmmBackward0>), tensor([[-0.1552],\n",
      "        [-0.1260],\n",
      "        [-0.0528],\n",
      "        [ 0.1452],\n",
      "        [ 0.0437],\n",
      "        [-0.0469],\n",
      "        [-0.2366],\n",
      "        [-0.0114]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2089],\n",
      "        [ 0.4391],\n",
      "        [ 0.0033],\n",
      "        [-0.1200],\n",
      "        [ 0.0300],\n",
      "        [-0.1200],\n",
      "        [ 0.4564],\n",
      "        [ 0.3159]], grad_fn=<AddmmBackward0>), tensor([[ 0.3289],\n",
      "        [ 0.2325],\n",
      "        [-0.1261],\n",
      "        [-0.0429],\n",
      "        [ 0.1468],\n",
      "        [-0.0819],\n",
      "        [ 0.0422],\n",
      "        [ 0.2654]], grad_fn=<AddmmBackward0>), tensor([[ 0.4843],\n",
      "        [ 0.0328],\n",
      "        [ 0.0382],\n",
      "        [ 0.0472],\n",
      "        [ 0.2271],\n",
      "        [-0.0186],\n",
      "        [ 0.0117],\n",
      "        [ 0.4362]], grad_fn=<AddmmBackward0>), tensor([[ 0.1037],\n",
      "        [-0.0506],\n",
      "        [ 0.1986],\n",
      "        [ 0.0016],\n",
      "        [ 0.2640],\n",
      "        [ 0.0906],\n",
      "        [-0.1005],\n",
      "        [ 0.3238]], grad_fn=<AddmmBackward0>), tensor([[ 0.0789],\n",
      "        [ 0.0829],\n",
      "        [-0.0107],\n",
      "        [ 0.0978],\n",
      "        [ 0.3865],\n",
      "        [ 0.2476],\n",
      "        [-0.0722],\n",
      "        [ 0.0832]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.5580e-04, 9.9984e-01],\n",
      "        [1.5955e-05, 9.9998e-01],\n",
      "        [9.9914e-01, 8.6153e-04],\n",
      "        [9.0490e-06, 9.9999e-01],\n",
      "        [3.2322e-05, 9.9997e-01],\n",
      "        [9.0490e-06, 9.9999e-01],\n",
      "        [2.4719e-06, 1.0000e+00],\n",
      "        [9.9913e-01, 8.6656e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9940e-01, 5.9972e-04],\n",
      "        [9.9931e-01, 6.8559e-04],\n",
      "        [1.3601e-04, 9.9986e-01],\n",
      "        [1.0000e+00, 1.0589e-06],\n",
      "        [9.9993e-01, 6.6372e-05],\n",
      "        [1.0000e+00, 1.6154e-06],\n",
      "        [9.9934e-01, 6.5700e-04],\n",
      "        [6.4235e-04, 9.9936e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.7509e-05, 9.9994e-01],\n",
      "        [1.1638e-04, 9.9988e-01],\n",
      "        [9.9923e-01, 7.6886e-04],\n",
      "        [6.9536e-05, 9.9993e-01],\n",
      "        [1.2553e-04, 9.9987e-01],\n",
      "        [5.0291e-05, 9.9995e-01],\n",
      "        [4.7872e-05, 9.9995e-01],\n",
      "        [9.9982e-01, 1.8153e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.8089e-01, 1.9106e-02],\n",
      "        [9.3590e-01, 6.4098e-02],\n",
      "        [8.4594e-06, 9.9999e-01],\n",
      "        [9.9991e-01, 9.0206e-05],\n",
      "        [9.9954e-01, 4.6043e-04],\n",
      "        [1.0000e+00, 1.7003e-06],\n",
      "        [9.9423e-01, 5.7675e-03],\n",
      "        [4.3986e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9968e-05, 9.9990e-01],\n",
      "        [2.8862e-03, 9.9711e-01],\n",
      "        [9.9899e-01, 1.0133e-03],\n",
      "        [1.3513e-05, 9.9999e-01],\n",
      "        [4.8894e-06, 1.0000e+00],\n",
      "        [3.8833e-05, 9.9996e-01],\n",
      "        [2.1898e-04, 9.9978e-01],\n",
      "        [9.9866e-01, 1.3359e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 0, 0, 1, 1],\n",
      "        [0, 0, 3, 1, 1],\n",
      "        [2, 4, 0, 6, 5],\n",
      "        [6, 2, 0, 1, 1],\n",
      "        [4, 1, 0, 2, 5],\n",
      "        [6, 8, 7, 5, 3],\n",
      "        [1, 6, 8, 3, 0],\n",
      "        [3, 7, 5, 0, 1]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.0768],\n",
      "        [ 0.5506],\n",
      "        [ 0.2348],\n",
      "        [-0.0818],\n",
      "        [ 0.2746],\n",
      "        [ 0.4608],\n",
      "        [-0.1371],\n",
      "        [ 0.1974]], grad_fn=<AddmmBackward0>), tensor([[-0.0667],\n",
      "        [-0.1387],\n",
      "        [-0.2599],\n",
      "        [ 0.5468],\n",
      "        [-0.3237],\n",
      "        [-0.3486],\n",
      "        [ 0.3890],\n",
      "        [-0.1528]], grad_fn=<AddmmBackward0>), tensor([[ 0.1336],\n",
      "        [ 0.0673],\n",
      "        [ 0.1779],\n",
      "        [ 0.1039],\n",
      "        [ 0.5006],\n",
      "        [ 0.3190],\n",
      "        [-0.0828],\n",
      "        [ 0.3643]], grad_fn=<AddmmBackward0>), tensor([[-0.0136],\n",
      "        [ 0.0437],\n",
      "        [-0.2527],\n",
      "        [ 0.0529],\n",
      "        [-0.1961],\n",
      "        [-0.0311],\n",
      "        [ 0.2616],\n",
      "        [ 0.0868]], grad_fn=<AddmmBackward0>), tensor([[ 0.0930],\n",
      "        [ 0.1097],\n",
      "        [ 0.2374],\n",
      "        [ 0.1316],\n",
      "        [ 0.2846],\n",
      "        [ 0.2897],\n",
      "        [ 0.2299],\n",
      "        [-0.0125]], grad_fn=<AddmmBackward0>), tensor([[ 3.9574e-02],\n",
      "        [ 1.4275e-01],\n",
      "        [-1.0714e-04],\n",
      "        [ 3.9821e-02],\n",
      "        [ 1.0711e-02],\n",
      "        [ 1.6490e-01],\n",
      "        [ 3.9045e-02],\n",
      "        [ 1.2809e-01]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.3576],\n",
      "        [ 0.3280],\n",
      "        [-0.0320],\n",
      "        [ 0.2954],\n",
      "        [-0.0930],\n",
      "        [-0.0335],\n",
      "        [ 0.1006],\n",
      "        [ 0.2500]], grad_fn=<AddmmBackward0>), tensor([[ 0.1902],\n",
      "        [-0.0902],\n",
      "        [ 0.0015],\n",
      "        [ 0.6129],\n",
      "        [ 0.0804],\n",
      "        [ 0.0236],\n",
      "        [ 0.3273],\n",
      "        [ 0.2974]], grad_fn=<AddmmBackward0>), tensor([[-0.0107],\n",
      "        [ 0.0197],\n",
      "        [-0.0380],\n",
      "        [ 0.0120],\n",
      "        [ 0.0795],\n",
      "        [ 0.1353],\n",
      "        [ 0.3263],\n",
      "        [ 0.4319]], grad_fn=<AddmmBackward0>), tensor([[ 0.0304],\n",
      "        [ 0.0774],\n",
      "        [ 0.0051],\n",
      "        [-0.0099],\n",
      "        [ 0.2269],\n",
      "        [ 0.0878],\n",
      "        [ 0.3822],\n",
      "        [-0.0362]], grad_fn=<AddmmBackward0>), tensor([[ 0.0248],\n",
      "        [-0.0402],\n",
      "        [ 0.0903],\n",
      "        [-0.0277],\n",
      "        [ 0.3027],\n",
      "        [ 0.3159],\n",
      "        [ 0.0357],\n",
      "        [-0.0670]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9929e-01, 7.0628e-04],\n",
      "        [8.6624e-04, 9.9913e-01],\n",
      "        [1.0732e-05, 9.9999e-01],\n",
      "        [1.0000e+00, 3.1220e-06],\n",
      "        [2.5605e-04, 9.9974e-01],\n",
      "        [7.4919e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 4.5559e-06],\n",
      "        [1.0053e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.7466e-03, 9.9725e-01],\n",
      "        [9.9931e-01, 6.9307e-04],\n",
      "        [9.9992e-01, 8.4647e-05],\n",
      "        [1.2142e-05, 9.9999e-01],\n",
      "        [9.9952e-01, 4.7706e-04],\n",
      "        [9.9997e-01, 3.4099e-05],\n",
      "        [1.7872e-04, 9.9982e-01],\n",
      "        [9.9999e-01, 7.9788e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9211e-01, 7.8917e-03],\n",
      "        [4.8926e-04, 9.9951e-01],\n",
      "        [2.0788e-05, 9.9998e-01],\n",
      "        [9.9985e-01, 1.4579e-04],\n",
      "        [1.4774e-03, 9.9852e-01],\n",
      "        [9.1511e-05, 9.9991e-01],\n",
      "        [9.9967e-01, 3.2670e-04],\n",
      "        [1.1573e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.1077e-03, 9.9589e-01],\n",
      "        [9.9982e-01, 1.8006e-04],\n",
      "        [9.9999e-01, 6.5132e-06],\n",
      "        [1.7378e-04, 9.9983e-01],\n",
      "        [9.9897e-01, 1.0289e-03],\n",
      "        [9.9972e-01, 2.7566e-04],\n",
      "        [1.0824e-05, 9.9999e-01],\n",
      "        [9.9892e-01, 1.0771e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.8559e-01, 1.4412e-02],\n",
      "        [1.4886e-03, 9.9851e-01],\n",
      "        [1.4163e-05, 9.9999e-01],\n",
      "        [9.9887e-01, 1.1343e-03],\n",
      "        [4.4147e-03, 9.9559e-01],\n",
      "        [3.0116e-05, 9.9997e-01],\n",
      "        [9.9943e-01, 5.6680e-04],\n",
      "        [7.7404e-03, 9.9226e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 0, 6, 1, 2],\n",
      "        [6, 8, 0, 1, 7],\n",
      "        [0, 0, 2, 5, 4],\n",
      "        [7, 1, 6, 0, 4],\n",
      "        [4, 0, 6, 1, 0],\n",
      "        [8, 2, 1, 3, 0],\n",
      "        [4, 0, 0, 5, 4],\n",
      "        [0, 3, 7, 0, 4]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1820],\n",
      "        [-0.4524],\n",
      "        [ 0.1728],\n",
      "        [ 0.5951],\n",
      "        [ 0.3008],\n",
      "        [-0.1298],\n",
      "        [ 0.1381],\n",
      "        [ 0.5194]], grad_fn=<AddmmBackward0>), tensor([[-0.0261],\n",
      "        [ 0.5347],\n",
      "        [-0.0643],\n",
      "        [-0.3798],\n",
      "        [-0.2690],\n",
      "        [ 0.2571],\n",
      "        [ 0.2513],\n",
      "        [-0.1242]], grad_fn=<AddmmBackward0>), tensor([[-0.2529],\n",
      "        [-0.2078],\n",
      "        [ 0.0771],\n",
      "        [ 0.3701],\n",
      "        [ 0.2335],\n",
      "        [-0.0397],\n",
      "        [ 0.1139],\n",
      "        [ 0.5627]], grad_fn=<AddmmBackward0>), tensor([[-0.1861],\n",
      "        [ 0.3701],\n",
      "        [-0.0025],\n",
      "        [-0.1732],\n",
      "        [ 0.0491],\n",
      "        [ 0.2444],\n",
      "        [ 0.0204],\n",
      "        [ 0.0200]], grad_fn=<AddmmBackward0>), tensor([[ 0.0103],\n",
      "        [ 0.0102],\n",
      "        [ 0.1337],\n",
      "        [ 0.0274],\n",
      "        [ 0.2960],\n",
      "        [-0.0345],\n",
      "        [ 0.0093],\n",
      "        [ 0.0006]], grad_fn=<AddmmBackward0>), tensor([[-0.0021],\n",
      "        [ 0.3722],\n",
      "        [-0.0588],\n",
      "        [-0.0563],\n",
      "        [-0.0611],\n",
      "        [-0.0281],\n",
      "        [ 0.0875],\n",
      "        [-0.0759]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0534],\n",
      "        [-0.0579],\n",
      "        [ 0.1354],\n",
      "        [ 0.1682],\n",
      "        [ 0.0619],\n",
      "        [ 0.0444],\n",
      "        [ 0.2296],\n",
      "        [ 0.4011]], grad_fn=<AddmmBackward0>), tensor([[-0.0105],\n",
      "        [ 0.3233],\n",
      "        [ 0.0049],\n",
      "        [ 0.1426],\n",
      "        [ 0.0213],\n",
      "        [ 0.2557],\n",
      "        [ 0.1712],\n",
      "        [ 0.2551]], grad_fn=<AddmmBackward0>), tensor([[-0.0118],\n",
      "        [ 0.0927],\n",
      "        [-0.0417],\n",
      "        [ 0.4084],\n",
      "        [ 0.0904],\n",
      "        [ 0.1200],\n",
      "        [-0.0734],\n",
      "        [ 0.4999]], grad_fn=<AddmmBackward0>), tensor([[-0.0469],\n",
      "        [ 0.3122],\n",
      "        [-0.0166],\n",
      "        [-0.0016],\n",
      "        [ 0.0869],\n",
      "        [ 0.3216],\n",
      "        [-0.0182],\n",
      "        [-0.0568]], grad_fn=<AddmmBackward0>), tensor([[ 0.1572],\n",
      "        [ 0.3423],\n",
      "        [-0.0239],\n",
      "        [ 0.0299],\n",
      "        [ 0.0005],\n",
      "        [-0.0538],\n",
      "        [-0.0865],\n",
      "        [-0.0833]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.8420e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 2.8256e-06],\n",
      "        [3.2853e-06, 1.0000e+00],\n",
      "        [1.0066e-05, 9.9999e-01],\n",
      "        [1.0403e-04, 9.9990e-01],\n",
      "        [9.9998e-01, 1.6302e-05],\n",
      "        [1.0000e+00, 3.3773e-06],\n",
      "        [8.2034e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9884e-01, 1.1560e-03],\n",
      "        [2.7535e-06, 1.0000e+00],\n",
      "        [9.9788e-01, 2.1231e-03],\n",
      "        [9.9999e-01, 1.3299e-05],\n",
      "        [9.9996e-01, 3.5903e-05],\n",
      "        [1.0807e-04, 9.9989e-01],\n",
      "        [6.9556e-05, 9.9993e-01],\n",
      "        [9.9999e-01, 1.3318e-05]], grad_fn=<SoftmaxBackward0>), tensor([[5.6952e-03, 9.9430e-01],\n",
      "        [9.9998e-01, 2.1738e-05],\n",
      "        [3.2146e-03, 9.9679e-01],\n",
      "        [2.9108e-05, 9.9997e-01],\n",
      "        [2.7363e-04, 9.9973e-01],\n",
      "        [9.9999e-01, 1.4466e-05],\n",
      "        [9.8725e-01, 1.2745e-02],\n",
      "        [4.8934e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.8962e-01, 1.0380e-02],\n",
      "        [6.8619e-06, 9.9999e-01],\n",
      "        [9.9867e-01, 1.3272e-03],\n",
      "        [9.9894e-01, 1.0563e-03],\n",
      "        [9.9998e-01, 2.3351e-05],\n",
      "        [2.1708e-05, 9.9998e-01],\n",
      "        [1.6846e-03, 9.9832e-01],\n",
      "        [9.9386e-01, 6.1432e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.4419e-02, 9.8558e-01],\n",
      "        [9.9996e-01, 3.6774e-05],\n",
      "        [3.2374e-04, 9.9968e-01],\n",
      "        [2.3289e-03, 9.9767e-01],\n",
      "        [8.3053e-04, 9.9917e-01],\n",
      "        [9.9868e-01, 1.3232e-03],\n",
      "        [9.9782e-01, 2.1765e-03],\n",
      "        [5.2329e-04, 9.9948e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 6, 2, 8, 7],\n",
      "        [1, 8, 7, 4, 5],\n",
      "        [5, 3, 1, 4, 7],\n",
      "        [2, 0, 0, 6, 0],\n",
      "        [8, 6, 4, 2, 1],\n",
      "        [8, 2, 0, 6, 0],\n",
      "        [5, 0, 1, 2, 0],\n",
      "        [3, 6, 4, 1, 8]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.1638],\n",
      "        [ 0.3232],\n",
      "        [ 0.2421],\n",
      "        [ 0.5373],\n",
      "        [ 0.1638],\n",
      "        [-0.2501],\n",
      "        [-0.4478],\n",
      "        [ 0.1638]], grad_fn=<AddmmBackward0>), tensor([[-0.4030],\n",
      "        [-0.1129],\n",
      "        [-0.0607],\n",
      "        [ 0.2209],\n",
      "        [-0.2338],\n",
      "        [ 0.5234],\n",
      "        [ 0.4662],\n",
      "        [-0.0259]], grad_fn=<AddmmBackward0>), tensor([[ 0.1650],\n",
      "        [ 0.3392],\n",
      "        [ 0.4052],\n",
      "        [ 0.1051],\n",
      "        [ 0.1794],\n",
      "        [ 0.0580],\n",
      "        [-0.1248],\n",
      "        [ 0.0350]], grad_fn=<AddmmBackward0>), tensor([[-0.1298],\n",
      "        [ 0.3372],\n",
      "        [ 0.0704],\n",
      "        [ 0.0163],\n",
      "        [-0.2675],\n",
      "        [ 0.1662],\n",
      "        [ 0.4824],\n",
      "        [-0.1674]], grad_fn=<AddmmBackward0>), tensor([[ 0.2406],\n",
      "        [-0.0046],\n",
      "        [ 0.0872],\n",
      "        [ 0.0506],\n",
      "        [ 0.1212],\n",
      "        [ 0.1173],\n",
      "        [-0.0413],\n",
      "        [ 0.2248]], grad_fn=<AddmmBackward0>), tensor([[ 0.0975],\n",
      "        [ 0.2744],\n",
      "        [ 0.2579],\n",
      "        [ 0.0138],\n",
      "        [ 0.0634],\n",
      "        [ 0.0638],\n",
      "        [ 0.1376],\n",
      "        [-0.0904]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0620],\n",
      "        [ 0.1793],\n",
      "        [ 0.0919],\n",
      "        [ 0.8445],\n",
      "        [ 0.0448],\n",
      "        [ 0.2149],\n",
      "        [-0.0217],\n",
      "        [ 0.0704]], grad_fn=<AddmmBackward0>), tensor([[-0.0526],\n",
      "        [ 0.1250],\n",
      "        [ 0.1142],\n",
      "        [ 0.0126],\n",
      "        [ 0.0709],\n",
      "        [ 0.5592],\n",
      "        [ 0.1843],\n",
      "        [ 0.0466]], grad_fn=<AddmmBackward0>), tensor([[ 0.0059],\n",
      "        [ 0.2052],\n",
      "        [ 0.1476],\n",
      "        [-0.0759],\n",
      "        [ 0.1609],\n",
      "        [ 0.1954],\n",
      "        [ 0.1313],\n",
      "        [ 0.1496]], grad_fn=<AddmmBackward0>), tensor([[-2.4988e-02],\n",
      "        [ 2.9508e-01],\n",
      "        [ 2.5228e-01],\n",
      "        [-4.9003e-02],\n",
      "        [ 2.0185e-02],\n",
      "        [ 2.9263e-01],\n",
      "        [ 4.0681e-01],\n",
      "        [ 7.8278e-06]], grad_fn=<AddmmBackward0>), tensor([[ 0.1883],\n",
      "        [ 0.1612],\n",
      "        [ 0.2446],\n",
      "        [-0.0261],\n",
      "        [ 0.0752],\n",
      "        [ 0.0216],\n",
      "        [ 0.0027],\n",
      "        [ 0.1464]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.4858e-06, 9.9999e-01],\n",
      "        [2.7148e-05, 9.9997e-01],\n",
      "        [1.1302e-05, 9.9999e-01],\n",
      "        [3.6463e-05, 9.9996e-01],\n",
      "        [5.1745e-06, 9.9999e-01],\n",
      "        [9.9995e-01, 4.8645e-05],\n",
      "        [9.9994e-01, 5.9894e-05],\n",
      "        [4.3897e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9995e-01, 4.5976e-05],\n",
      "        [9.9999e-01, 1.0813e-05],\n",
      "        [9.9998e-01, 2.0932e-05],\n",
      "        [9.9624e-01, 3.7598e-03],\n",
      "        [9.9997e-01, 3.0915e-05],\n",
      "        [1.1657e-04, 9.9988e-01],\n",
      "        [2.2346e-04, 9.9978e-01],\n",
      "        [9.9998e-01, 1.7015e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.5762e-05, 9.9998e-01],\n",
      "        [3.4343e-05, 9.9997e-01],\n",
      "        [1.3190e-05, 9.9999e-01],\n",
      "        [3.1459e-01, 6.8541e-01],\n",
      "        [1.0214e-05, 9.9999e-01],\n",
      "        [9.9330e-01, 6.7012e-03],\n",
      "        [9.9993e-01, 6.7759e-05],\n",
      "        [8.5097e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 1.2213e-05],\n",
      "        [9.9999e-01, 7.7232e-06],\n",
      "        [9.9989e-01, 1.0525e-04],\n",
      "        [9.3584e-01, 6.4160e-02],\n",
      "        [9.9999e-01, 9.7154e-06],\n",
      "        [1.4612e-02, 9.8539e-01],\n",
      "        [9.0092e-05, 9.9991e-01],\n",
      "        [9.9999e-01, 7.3821e-06]], grad_fn=<SoftmaxBackward0>), tensor([[2.2473e-05, 9.9998e-01],\n",
      "        [7.2062e-06, 9.9999e-01],\n",
      "        [5.9827e-06, 9.9999e-01],\n",
      "        [7.9479e-01, 2.0521e-01],\n",
      "        [1.6744e-05, 9.9998e-01],\n",
      "        [5.3735e-01, 4.6265e-01],\n",
      "        [9.9748e-01, 2.5212e-03],\n",
      "        [8.4496e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 4, 8, 2, 0],\n",
      "        [4, 0, 4, 2, 6],\n",
      "        [1, 3, 6, 4, 8],\n",
      "        [0, 8, 2, 3, 5],\n",
      "        [8, 1, 5, 0, 6],\n",
      "        [1, 0, 4, 2, 6],\n",
      "        [7, 8, 4, 2, 6],\n",
      "        [8, 2, 4, 1, 0]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2289],\n",
      "        [ 0.1739],\n",
      "        [ 0.3107],\n",
      "        [ 0.1694],\n",
      "        [ 0.3521],\n",
      "        [ 0.2936],\n",
      "        [ 0.2289],\n",
      "        [-0.3930]], grad_fn=<AddmmBackward0>), tensor([[-0.1225],\n",
      "        [-0.0848],\n",
      "        [-0.2119],\n",
      "        [-0.2292],\n",
      "        [-0.1968],\n",
      "        [-0.0536],\n",
      "        [-0.1191],\n",
      "        [ 0.1279]], grad_fn=<AddmmBackward0>), tensor([[-0.0028],\n",
      "        [ 0.0005],\n",
      "        [ 0.1111],\n",
      "        [ 0.0979],\n",
      "        [ 0.1498],\n",
      "        [ 0.0377],\n",
      "        [-0.0777],\n",
      "        [-0.3346]], grad_fn=<AddmmBackward0>), tensor([[-0.2333],\n",
      "        [-0.0658],\n",
      "        [-0.2604],\n",
      "        [ 0.0347],\n",
      "        [-0.1383],\n",
      "        [-0.0889],\n",
      "        [-0.2892],\n",
      "        [-0.0106]], grad_fn=<AddmmBackward0>), tensor([[ 0.0589],\n",
      "        [ 0.0342],\n",
      "        [ 0.0747],\n",
      "        [ 0.0814],\n",
      "        [-0.0201],\n",
      "        [ 0.0230],\n",
      "        [ 0.0428],\n",
      "        [-0.1998]], grad_fn=<AddmmBackward0>), tensor([[-0.2193],\n",
      "        [-0.0253],\n",
      "        [-0.0781],\n",
      "        [-0.0642],\n",
      "        [-0.0926],\n",
      "        [-0.0893],\n",
      "        [-0.0930],\n",
      "        [-0.0885]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.0333],\n",
      "        [0.6020],\n",
      "        [0.0832],\n",
      "        [0.1983],\n",
      "        [0.5496],\n",
      "        [0.2368],\n",
      "        [0.0972],\n",
      "        [0.0302]], grad_fn=<AddmmBackward0>), tensor([[ 0.0223],\n",
      "        [ 0.0951],\n",
      "        [-0.0160],\n",
      "        [ 0.3032],\n",
      "        [ 0.2397],\n",
      "        [ 0.1242],\n",
      "        [ 0.0710],\n",
      "        [-0.0316]], grad_fn=<AddmmBackward0>), tensor([[ 0.0456],\n",
      "        [ 0.0079],\n",
      "        [ 0.1189],\n",
      "        [ 0.6170],\n",
      "        [ 0.2615],\n",
      "        [ 0.2067],\n",
      "        [ 0.0139],\n",
      "        [-0.0160]], grad_fn=<AddmmBackward0>), tensor([[0.0242],\n",
      "        [0.1077],\n",
      "        [0.1015],\n",
      "        [0.3954],\n",
      "        [0.0301],\n",
      "        [0.0429],\n",
      "        [0.0294],\n",
      "        [0.2519]], grad_fn=<AddmmBackward0>), tensor([[0.3427],\n",
      "        [0.0620],\n",
      "        [0.3466],\n",
      "        [0.3565],\n",
      "        [0.0324],\n",
      "        [0.2368],\n",
      "        [0.2948],\n",
      "        [0.0811]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.2350e-06, 1.0000e+00],\n",
      "        [8.3242e-03, 9.9168e-01],\n",
      "        [2.0623e-03, 9.9794e-01],\n",
      "        [1.6662e-05, 9.9998e-01],\n",
      "        [9.7446e-04, 9.9903e-01],\n",
      "        [6.8943e-05, 9.9993e-01],\n",
      "        [2.9876e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 1.6226e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.5415e-06],\n",
      "        [8.5454e-01, 1.4546e-01],\n",
      "        [9.9995e-01, 5.5009e-05],\n",
      "        [9.9999e-01, 6.1061e-06],\n",
      "        [9.9999e-01, 5.3142e-06],\n",
      "        [9.9996e-01, 4.0193e-05],\n",
      "        [9.9999e-01, 5.9461e-06],\n",
      "        [1.2134e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.2237e-06, 1.0000e+00],\n",
      "        [1.3957e-01, 8.6043e-01],\n",
      "        [3.9917e-04, 9.9960e-01],\n",
      "        [4.8742e-05, 9.9995e-01],\n",
      "        [1.3515e-05, 9.9999e-01],\n",
      "        [4.0910e-04, 9.9959e-01],\n",
      "        [3.2247e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 3.0138e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 3.3497e-06],\n",
      "        [7.4840e-01, 2.5160e-01],\n",
      "        [9.9999e-01, 1.2686e-05],\n",
      "        [9.9973e-01, 2.7215e-04],\n",
      "        [9.9956e-01, 4.3952e-04],\n",
      "        [9.9974e-01, 2.6281e-04],\n",
      "        [9.9999e-01, 8.8631e-06],\n",
      "        [1.3472e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.0595e-06, 1.0000e+00],\n",
      "        [2.8788e-01, 7.1212e-01],\n",
      "        [5.5609e-05, 9.9994e-01],\n",
      "        [9.0193e-05, 9.9991e-01],\n",
      "        [2.1015e-02, 9.7899e-01],\n",
      "        [7.8214e-05, 9.9992e-01],\n",
      "        [8.8062e-06, 9.9999e-01],\n",
      "        [9.9997e-01, 3.0900e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0400, 0.1200, 0.0000, 0.0000, 0.5200, 0.0000, 0.0400, 0.0800, 0.2000]), tensor([0.0400, 0.1200, 0.0000, 0.0000, 0.5200, 0.0000, 0.0400, 0.0800, 0.2000]), 0.37341404845938087, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.0800, 0.0800, 0.1600, 0.0000, 0.2400, 0.2000, 0.0800, 0.0000]), tensor([0.1600, 0.0800, 0.0800, 0.1600, 0.0000, 0.2400, 0.2000, 0.0800, 0.0000]), 0.2333895628555463, tensor(5))\n",
      "action: 5\n",
      "Player 1 random action: 0\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.3200, 0.2800, 0.0800, 0.0000, 0.0000, 0.2400, 0.0800, 0.0000]), tensor([0.0000, 0.3200, 0.2800, 0.0800, 0.0000, 0.0000, 0.2400, 0.0800, 0.0000]), 0.21563582924696115, tensor(1))\n",
      "action: 1\n",
      "Player 1 random action: 7\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.2000, 0.3200, 0.0000, 0.0000, 0.4800, 0.0000, 0.0000]), tensor([0.0000, 0.0000, 0.2000, 0.3200, 0.0000, 0.0000, 0.4800, 0.0000, 0.0000]), 0.18927191828305906, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.]), 0.5400146724512944, tensor(2))\n",
      "action: 2\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: 0.38\n",
      "Test score {'score': 0.38, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 5, 2, 3, 0],\n",
      "        [8, 7, 6, 3, 0],\n",
      "        [3, 8, 0, 7, 1],\n",
      "        [6, 2, 4, 7, 0],\n",
      "        [5, 0, 0, 7, 1],\n",
      "        [5, 0, 4, 7, 1],\n",
      "        [5, 2, 6, 1, 0],\n",
      "        [0, 7, 6, 4, 8]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.2415],\n",
      "        [ 0.0082],\n",
      "        [ 0.1209],\n",
      "        [-0.1566],\n",
      "        [ 0.4928],\n",
      "        [ 0.2967],\n",
      "        [ 0.2034],\n",
      "        [-0.1991]], grad_fn=<AddmmBackward0>), tensor([[-0.1211],\n",
      "        [-0.1020],\n",
      "        [ 0.1168],\n",
      "        [ 0.1957],\n",
      "        [-0.1567],\n",
      "        [ 0.0988],\n",
      "        [-0.4239],\n",
      "        [ 0.1091]], grad_fn=<AddmmBackward0>), tensor([[ 0.0623],\n",
      "        [ 0.2493],\n",
      "        [ 0.1371],\n",
      "        [-0.0241],\n",
      "        [ 0.1515],\n",
      "        [-0.0441],\n",
      "        [ 0.3874],\n",
      "        [-0.0076]], grad_fn=<AddmmBackward0>), tensor([[ 0.0836],\n",
      "        [-0.0025],\n",
      "        [ 0.0306],\n",
      "        [ 0.0957],\n",
      "        [-0.0095],\n",
      "        [-0.0385],\n",
      "        [-0.2549],\n",
      "        [-0.0824]], grad_fn=<AddmmBackward0>), tensor([[ 0.0994],\n",
      "        [ 0.3555],\n",
      "        [-0.0199],\n",
      "        [ 0.1457],\n",
      "        [-0.0240],\n",
      "        [-0.0856],\n",
      "        [ 0.4341],\n",
      "        [-0.0108]], grad_fn=<AddmmBackward0>), tensor([[ 0.1628],\n",
      "        [-0.0437],\n",
      "        [-0.0577],\n",
      "        [ 0.0070],\n",
      "        [-0.0538],\n",
      "        [-0.0614],\n",
      "        [-0.2052],\n",
      "        [ 0.1077]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.2156],\n",
      "        [0.0857],\n",
      "        [0.2245],\n",
      "        [0.1023],\n",
      "        [0.3178],\n",
      "        [0.5514],\n",
      "        [0.0227],\n",
      "        [0.0396]], grad_fn=<AddmmBackward0>), tensor([[-0.0085],\n",
      "        [ 0.2155],\n",
      "        [ 0.5090],\n",
      "        [ 0.1119],\n",
      "        [ 0.3669],\n",
      "        [ 0.0804],\n",
      "        [-0.0186],\n",
      "        [ 0.0126]], grad_fn=<AddmmBackward0>), tensor([[ 0.3234],\n",
      "        [ 0.4358],\n",
      "        [ 0.0820],\n",
      "        [ 0.2983],\n",
      "        [ 0.1298],\n",
      "        [-0.0015],\n",
      "        [ 0.1664],\n",
      "        [ 0.0351]], grad_fn=<AddmmBackward0>), tensor([[0.1886],\n",
      "        [0.3021],\n",
      "        [0.0175],\n",
      "        [0.2858],\n",
      "        [0.0860],\n",
      "        [0.0087],\n",
      "        [0.1877],\n",
      "        [0.2452]], grad_fn=<AddmmBackward0>), tensor([[0.4226],\n",
      "        [0.6054],\n",
      "        [0.0404],\n",
      "        [0.3601],\n",
      "        [0.1063],\n",
      "        [0.0813],\n",
      "        [0.3893],\n",
      "        [0.1191]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9999e-01, 7.3759e-06],\n",
      "        [5.4208e-06, 9.9999e-01],\n",
      "        [1.8495e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 3.0390e-06],\n",
      "        [4.0076e-05, 9.9996e-01],\n",
      "        [9.9998e-01, 2.3897e-05],\n",
      "        [2.3297e-06, 1.0000e+00],\n",
      "        [9.9998e-01, 1.7807e-05]], grad_fn=<SoftmaxBackward0>), tensor([[5.3937e-06, 9.9999e-01],\n",
      "        [9.9997e-01, 3.0707e-05],\n",
      "        [9.9998e-01, 2.0763e-05],\n",
      "        [6.2169e-06, 9.9999e-01],\n",
      "        [9.9969e-01, 3.0682e-04],\n",
      "        [2.1149e-03, 9.9789e-01],\n",
      "        [9.9999e-01, 1.3886e-05],\n",
      "        [2.1097e-04, 9.9979e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.5607e-06],\n",
      "        [5.3719e-05, 9.9995e-01],\n",
      "        [2.6938e-03, 9.9731e-01],\n",
      "        [9.9999e-01, 6.6423e-06],\n",
      "        [1.5383e-02, 9.8462e-01],\n",
      "        [9.9123e-01, 8.7712e-03],\n",
      "        [4.1143e-06, 1.0000e+00],\n",
      "        [9.9998e-01, 1.7265e-05]], grad_fn=<SoftmaxBackward0>), tensor([[4.3000e-06, 1.0000e+00],\n",
      "        [9.9541e-01, 4.5871e-03],\n",
      "        [9.0200e-01, 9.8003e-02],\n",
      "        [6.0490e-06, 9.9999e-01],\n",
      "        [8.6458e-01, 1.3542e-01],\n",
      "        [4.2639e-03, 9.9574e-01],\n",
      "        [9.9999e-01, 1.4164e-05],\n",
      "        [1.6728e-04, 9.9983e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9995e-01, 4.6016e-05],\n",
      "        [1.5445e-04, 9.9985e-01],\n",
      "        [5.0526e-01, 4.9474e-01],\n",
      "        [9.9979e-01, 2.0784e-04],\n",
      "        [5.0846e-01, 4.9154e-01],\n",
      "        [9.9981e-01, 1.8990e-04],\n",
      "        [2.2907e-06, 1.0000e+00],\n",
      "        [9.9993e-01, 7.0141e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 70.0 and average score: 0.58\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0800, 0.0800, 0.0000, 0.0800, 0.0800, 0.2800, 0.2000, 0.0800, 0.1200]), tensor([0.0800, 0.0800, 0.0000, 0.0800, 0.0800, 0.2800, 0.2000, 0.0800, 0.1200]), -0.401047241085997, tensor(5))\n",
      "action: 5\n",
      "Player 0 random action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2400, 0.0400, 0.0000, 0.1600, 0.3200, 0.0000, 0.1200, 0.1200, 0.0000]), tensor([0.2400, 0.0400, 0.0000, 0.1600, 0.3200, 0.0000, 0.1200, 0.1200, 0.0000]), -0.32894479239789337, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 0\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.2400, 0.0000, 0.1200, 0.0000, 0.0000, 0.4000, 0.2400, 0.0000]), tensor([0.0000, 0.2400, 0.0000, 0.1200, 0.0000, 0.0000, 0.4000, 0.2400, 0.0000]), -0.28747354132624775, tensor(6))\n",
      "action: 6\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6400, 0.0000]), tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6400, 0.0000]), -0.2352208269473452, tensor(7))\n",
      "action: 7\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 8, 3, 1, 0],\n",
      "        [5, 6, 4, 0, 3],\n",
      "        [1, 4, 2, 5, 0],\n",
      "        [7, 8, 6, 1, 0],\n",
      "        [5, 7, 2, 0, 3],\n",
      "        [4, 8, 3, 0, 3],\n",
      "        [1, 2, 5, 0, 3],\n",
      "        [4, 5, 0, 8, 0]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.7341],\n",
      "        [-0.0998],\n",
      "        [-0.3498],\n",
      "        [-0.5706],\n",
      "        [ 0.3279],\n",
      "        [-0.5666],\n",
      "        [ 0.0556],\n",
      "        [ 0.3355]], grad_fn=<AddmmBackward0>), tensor([[ 0.4083],\n",
      "        [ 0.5467],\n",
      "        [ 0.6979],\n",
      "        [ 0.3339],\n",
      "        [-0.1091],\n",
      "        [ 0.1860],\n",
      "        [-0.1350],\n",
      "        [-0.4448]], grad_fn=<AddmmBackward0>), tensor([[-0.3811],\n",
      "        [-0.1726],\n",
      "        [-0.1015],\n",
      "        [-0.3245],\n",
      "        [ 0.3864],\n",
      "        [-0.1211],\n",
      "        [ 0.1462],\n",
      "        [ 0.2755]], grad_fn=<AddmmBackward0>), tensor([[ 0.4022],\n",
      "        [ 0.4257],\n",
      "        [ 0.2960],\n",
      "        [ 0.1682],\n",
      "        [-0.1224],\n",
      "        [ 0.3697],\n",
      "        [-0.0166],\n",
      "        [-0.4162]], grad_fn=<AddmmBackward0>), tensor([[-0.1434],\n",
      "        [-0.0655],\n",
      "        [-0.0708],\n",
      "        [ 0.0294],\n",
      "        [-0.1321],\n",
      "        [-0.1145],\n",
      "        [-0.0573],\n",
      "        [ 0.3879]], grad_fn=<AddmmBackward0>), tensor([[ 0.2835],\n",
      "        [ 0.4990],\n",
      "        [-0.0154],\n",
      "        [ 0.0212],\n",
      "        [-0.1239],\n",
      "        [ 0.2241],\n",
      "        [-0.0960],\n",
      "        [-0.1956]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.1490],\n",
      "        [-0.0231],\n",
      "        [ 0.0268],\n",
      "        [-0.0829],\n",
      "        [ 0.3117],\n",
      "        [-0.0208],\n",
      "        [ 0.1849],\n",
      "        [-0.1234]], grad_fn=<AddmmBackward0>), tensor([[ 0.0402],\n",
      "        [ 0.0995],\n",
      "        [ 0.5812],\n",
      "        [ 0.2396],\n",
      "        [ 0.2160],\n",
      "        [ 0.3456],\n",
      "        [ 0.1297],\n",
      "        [-0.0801]], grad_fn=<AddmmBackward0>), tensor([[-0.1660],\n",
      "        [ 0.0171],\n",
      "        [ 0.2803],\n",
      "        [ 0.1205],\n",
      "        [ 0.4834],\n",
      "        [ 0.1321],\n",
      "        [ 0.2602],\n",
      "        [ 0.0685]], grad_fn=<AddmmBackward0>), tensor([[ 0.0438],\n",
      "        [ 0.2445],\n",
      "        [ 0.4404],\n",
      "        [ 0.3976],\n",
      "        [-0.0769],\n",
      "        [ 0.1464],\n",
      "        [-0.0593],\n",
      "        [ 0.0711]], grad_fn=<AddmmBackward0>), tensor([[ 0.0093],\n",
      "        [ 0.0493],\n",
      "        [ 0.0237],\n",
      "        [ 0.0176],\n",
      "        [ 0.0410],\n",
      "        [ 0.0847],\n",
      "        [ 0.1304],\n",
      "        [-0.0056]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0000e+00, 8.5595e-07],\n",
      "        [1.0000e+00, 3.8311e-07],\n",
      "        [9.9999e-01, 8.6279e-06],\n",
      "        [9.9998e-01, 1.7020e-05],\n",
      "        [1.3445e-03, 9.9866e-01],\n",
      "        [1.0000e+00, 3.2632e-06],\n",
      "        [4.8710e-03, 9.9513e-01],\n",
      "        [1.3301e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.6362e-05, 9.9996e-01],\n",
      "        [2.4254e-04, 9.9976e-01],\n",
      "        [5.8606e-04, 9.9941e-01],\n",
      "        [4.8550e-04, 9.9951e-01],\n",
      "        [9.9953e-01, 4.6549e-04],\n",
      "        [6.5649e-04, 9.9934e-01],\n",
      "        [9.9999e-01, 1.1125e-05],\n",
      "        [1.0000e+00, 2.0385e-07]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 2.5555e-07],\n",
      "        [1.0000e+00, 7.6705e-07],\n",
      "        [9.9999e-01, 8.5313e-06],\n",
      "        [9.9986e-01, 1.3796e-04],\n",
      "        [1.4606e-03, 9.9854e-01],\n",
      "        [1.0000e+00, 3.8837e-06],\n",
      "        [7.9208e-02, 9.2079e-01],\n",
      "        [9.8414e-05, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.9853e-04, 9.9970e-01],\n",
      "        [8.7382e-04, 9.9913e-01],\n",
      "        [5.4886e-04, 9.9945e-01],\n",
      "        [1.1008e-03, 9.9890e-01],\n",
      "        [9.9950e-01, 5.0387e-04],\n",
      "        [2.6842e-02, 9.7316e-01],\n",
      "        [9.9948e-01, 5.2288e-04],\n",
      "        [1.0000e+00, 6.4493e-07]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 5.4663e-07],\n",
      "        [1.0000e+00, 1.2972e-06],\n",
      "        [9.9995e-01, 4.8321e-05],\n",
      "        [9.9964e-01, 3.5816e-04],\n",
      "        [4.4557e-01, 5.5443e-01],\n",
      "        [9.9999e-01, 6.5772e-06],\n",
      "        [8.7488e-01, 1.2512e-01],\n",
      "        [3.7889e-04, 9.9962e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "Player 1 win percentage vs random: 30.0 and average score: -0.24\n",
      "Results vs random: {'player_0_score': 0.58, 'player_0_win%': 0.7, 'player_1_score': -0.24, 'player_1_win%': 0.3, 'score': 0.16999999999999998}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0400, 0.0000, 0.6400, 0.0000, 0.0000, 0.0400, 0.1600]), tensor([0.0800, 0.0400, 0.0400, 0.0000, 0.6400, 0.0000, 0.0000, 0.0400, 0.1600]), 0.3955766148865223, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 8\n",
      "Player 0 prediction: (tensor([0.1200, 0.1600, 0.2000, 0.0400, 0.0000, 0.2000, 0.1200, 0.1600, 0.0000]), tensor([0.1200, 0.1600, 0.2000, 0.0400, 0.0000, 0.2000, 0.1200, 0.1600, 0.0000]), 0.43442577961832285, tensor(2))\n",
      "action: 2\n",
      "Player 1 tictactoe_expert action: 6\n",
      "Player 0 prediction: (tensor([0.6400, 0.1200, 0.0000, 0.0400, 0.0000, 0.0800, 0.0000, 0.1200, 0.0000]), tensor([0.6400, 0.1200, 0.0000, 0.0400, 0.0000, 0.0800, 0.0000, 0.1200, 0.0000]), 0.4961684046743008, tensor(0))\n",
      "action: 0\n",
      "Player 1 tictactoe_expert action: 7\n",
      "2300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 8, 4, 1, 5],\n",
      "        [1, 0, 3, 2, 4],\n",
      "        [7, 6, 5, 3, 0],\n",
      "        [3, 0, 4, 4, 6],\n",
      "        [7, 4, 6, 8, 2],\n",
      "        [6, 2, 0, 4, 6],\n",
      "        [2, 0, 4, 4, 6],\n",
      "        [6, 2, 8, 0, 0]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.1667],\n",
      "        [ 0.4133],\n",
      "        [ 0.1428],\n",
      "        [ 0.4625],\n",
      "        [ 0.4740],\n",
      "        [-0.5214],\n",
      "        [ 0.6174],\n",
      "        [ 0.1017]], grad_fn=<AddmmBackward0>), tensor([[ 0.1697],\n",
      "        [ 0.1083],\n",
      "        [-0.3284],\n",
      "        [ 0.0161],\n",
      "        [-0.1926],\n",
      "        [ 0.4094],\n",
      "        [ 0.0585],\n",
      "        [-0.0431]], grad_fn=<AddmmBackward0>), tensor([[-0.0058],\n",
      "        [-0.0795],\n",
      "        [ 0.1517],\n",
      "        [ 0.0436],\n",
      "        [ 0.4065],\n",
      "        [-0.0172],\n",
      "        [ 0.0373],\n",
      "        [ 0.1877]], grad_fn=<AddmmBackward0>), tensor([[ 0.0868],\n",
      "        [ 0.1354],\n",
      "        [-0.1502],\n",
      "        [-0.1144],\n",
      "        [-0.1200],\n",
      "        [ 0.0813],\n",
      "        [-0.0628],\n",
      "        [ 0.1075]], grad_fn=<AddmmBackward0>), tensor([[ 0.2210],\n",
      "        [ 0.1168],\n",
      "        [ 0.4426],\n",
      "        [ 0.0740],\n",
      "        [ 0.2776],\n",
      "        [-0.1424],\n",
      "        [ 0.1156],\n",
      "        [ 0.1343]], grad_fn=<AddmmBackward0>), tensor([[ 0.3526],\n",
      "        [-0.0455],\n",
      "        [-0.1087],\n",
      "        [-0.1721],\n",
      "        [ 0.1351],\n",
      "        [ 0.1965],\n",
      "        [-0.1449],\n",
      "        [ 0.0234]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0021],\n",
      "        [-0.1164],\n",
      "        [ 0.0311],\n",
      "        [ 0.3914],\n",
      "        [-0.0671],\n",
      "        [ 0.1055],\n",
      "        [ 0.6336],\n",
      "        [ 0.0462]], grad_fn=<AddmmBackward0>), tensor([[ 0.2064],\n",
      "        [ 0.0400],\n",
      "        [ 0.0092],\n",
      "        [-0.0711],\n",
      "        [ 0.0623],\n",
      "        [ 0.5573],\n",
      "        [ 0.0423],\n",
      "        [ 0.1672]], grad_fn=<AddmmBackward0>), tensor([[ 0.2111],\n",
      "        [-0.0105],\n",
      "        [ 0.0019],\n",
      "        [-0.0142],\n",
      "        [ 0.1413],\n",
      "        [ 0.0543],\n",
      "        [ 0.0311],\n",
      "        [ 0.3373]], grad_fn=<AddmmBackward0>), tensor([[ 0.2585],\n",
      "        [ 0.0569],\n",
      "        [ 0.0694],\n",
      "        [-0.1371],\n",
      "        [ 0.1765],\n",
      "        [ 0.0634],\n",
      "        [-0.1174],\n",
      "        [ 0.2008]], grad_fn=<AddmmBackward0>), tensor([[ 0.3599],\n",
      "        [ 0.3030],\n",
      "        [-0.0242],\n",
      "        [-0.1024],\n",
      "        [ 0.5448],\n",
      "        [ 0.0078],\n",
      "        [-0.1019],\n",
      "        [-0.0188]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0000e+00, 2.7090e-06],\n",
      "        [8.3509e-06, 9.9999e-01],\n",
      "        [1.6828e-06, 1.0000e+00],\n",
      "        [7.1566e-05, 9.9993e-01],\n",
      "        [3.2641e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 2.2113e-06],\n",
      "        [1.6904e-05, 9.9998e-01],\n",
      "        [9.9992e-01, 7.9094e-05]], grad_fn=<SoftmaxBackward0>), tensor([[3.8879e-05, 9.9996e-01],\n",
      "        [9.9999e-01, 7.9529e-06],\n",
      "        [9.9998e-01, 1.9283e-05],\n",
      "        [9.9052e-01, 9.4784e-03],\n",
      "        [1.0000e+00, 8.6478e-07],\n",
      "        [6.6803e-06, 9.9999e-01],\n",
      "        [9.9945e-01, 5.4685e-04],\n",
      "        [3.9856e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 6.3343e-06],\n",
      "        [2.0262e-04, 9.9980e-01],\n",
      "        [1.6531e-05, 9.9998e-01],\n",
      "        [3.2244e-03, 9.9678e-01],\n",
      "        [9.1592e-06, 9.9999e-01],\n",
      "        [9.9996e-01, 4.4031e-05],\n",
      "        [7.3628e-04, 9.9926e-01],\n",
      "        [9.9983e-01, 1.6894e-04]], grad_fn=<SoftmaxBackward0>), tensor([[1.4686e-05, 9.9999e-01],\n",
      "        [9.9996e-01, 4.1161e-05],\n",
      "        [1.0000e+00, 3.2811e-06],\n",
      "        [9.9966e-01, 3.4451e-04],\n",
      "        [9.9999e-01, 7.3834e-06],\n",
      "        [3.0658e-04, 9.9969e-01],\n",
      "        [9.9957e-01, 4.2990e-04],\n",
      "        [1.2975e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9995e-01, 4.9115e-05],\n",
      "        [1.9809e-04, 9.9980e-01],\n",
      "        [6.1750e-05, 9.9994e-01],\n",
      "        [2.2093e-04, 9.9978e-01],\n",
      "        [4.2637e-05, 9.9996e-01],\n",
      "        [9.9989e-01, 1.0712e-04],\n",
      "        [3.7598e-04, 9.9962e-01],\n",
      "        [9.9442e-01, 5.5812e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 1, 4, 0, 8],\n",
      "        [5, 4, 6, 3, 0],\n",
      "        [4, 7, 8, 6, 1],\n",
      "        [0, 1, 3, 6, 2],\n",
      "        [4, 0, 2, 3, 6],\n",
      "        [6, 8, 0, 7, 1],\n",
      "        [4, 5, 0, 6, 2],\n",
      "        [0, 7, 6, 1, 0]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1670],\n",
      "        [-0.3193],\n",
      "        [ 0.0808],\n",
      "        [-0.4412],\n",
      "        [ 0.5516],\n",
      "        [-0.2021],\n",
      "        [-0.0811],\n",
      "        [-0.3232]], grad_fn=<AddmmBackward0>), tensor([[ 0.2823],\n",
      "        [ 0.3504],\n",
      "        [-0.0716],\n",
      "        [ 0.3545],\n",
      "        [-0.3385],\n",
      "        [ 0.1649],\n",
      "        [-0.1562],\n",
      "        [ 0.4247]], grad_fn=<AddmmBackward0>), tensor([[ 0.3348],\n",
      "        [-0.3017],\n",
      "        [ 0.1948],\n",
      "        [-0.2234],\n",
      "        [ 0.3295],\n",
      "        [ 0.0540],\n",
      "        [ 0.1607],\n",
      "        [ 0.0315]], grad_fn=<AddmmBackward0>), tensor([[ 0.1504],\n",
      "        [ 0.1421],\n",
      "        [-0.0434],\n",
      "        [ 0.2949],\n",
      "        [-0.1257],\n",
      "        [ 0.0268],\n",
      "        [-0.2069],\n",
      "        [ 0.1187]], grad_fn=<AddmmBackward0>), tensor([[ 0.0019],\n",
      "        [-0.0331],\n",
      "        [ 0.0370],\n",
      "        [-0.1900],\n",
      "        [ 0.1664],\n",
      "        [ 0.1675],\n",
      "        [ 0.0246],\n",
      "        [ 0.1109]], grad_fn=<AddmmBackward0>), tensor([[-0.1265],\n",
      "        [ 0.0037],\n",
      "        [ 0.1057],\n",
      "        [ 0.0987],\n",
      "        [-0.0052],\n",
      "        [ 0.1618],\n",
      "        [-0.0147],\n",
      "        [-0.0960]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2556],\n",
      "        [ 0.1167],\n",
      "        [ 0.0859],\n",
      "        [ 0.0629],\n",
      "        [ 0.1658],\n",
      "        [ 0.1026],\n",
      "        [-0.0212],\n",
      "        [ 0.0325]], grad_fn=<AddmmBackward0>), tensor([[ 0.2842],\n",
      "        [ 0.1714],\n",
      "        [ 0.0226],\n",
      "        [ 0.1172],\n",
      "        [ 0.0148],\n",
      "        [ 0.2948],\n",
      "        [-0.0297],\n",
      "        [ 0.1867]], grad_fn=<AddmmBackward0>), tensor([[ 0.5610],\n",
      "        [ 0.0061],\n",
      "        [ 0.1565],\n",
      "        [ 0.0017],\n",
      "        [ 0.1524],\n",
      "        [ 0.1111],\n",
      "        [-0.0091],\n",
      "        [ 0.1814]], grad_fn=<AddmmBackward0>), tensor([[-0.0298],\n",
      "        [ 0.1199],\n",
      "        [ 0.1756],\n",
      "        [ 0.1662],\n",
      "        [ 0.0943],\n",
      "        [ 0.2317],\n",
      "        [ 0.1182],\n",
      "        [ 0.2626]], grad_fn=<AddmmBackward0>), tensor([[-0.0194],\n",
      "        [ 0.0785],\n",
      "        [ 0.2601],\n",
      "        [ 0.0724],\n",
      "        [ 0.3553],\n",
      "        [ 0.3258],\n",
      "        [ 0.1231],\n",
      "        [-0.0443]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9997e-01, 2.6365e-05],\n",
      "        [1.0000e+00, 6.6098e-07],\n",
      "        [4.0295e-05, 9.9996e-01],\n",
      "        [1.0000e+00, 4.6044e-06],\n",
      "        [2.9656e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 4.3780e-07],\n",
      "        [1.0000e+00, 4.1894e-07],\n",
      "        [1.0000e+00, 2.9796e-06]], grad_fn=<SoftmaxBackward0>), tensor([[3.9491e-04, 9.9961e-01],\n",
      "        [8.4646e-05, 9.9992e-01],\n",
      "        [9.9996e-01, 4.3959e-05],\n",
      "        [1.4900e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 9.8287e-07],\n",
      "        [9.7055e-05, 9.9990e-01],\n",
      "        [5.6321e-05, 9.9994e-01],\n",
      "        [3.2216e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.4746e-05],\n",
      "        [9.9999e-01, 6.9882e-06],\n",
      "        [3.8993e-04, 9.9961e-01],\n",
      "        [1.0000e+00, 4.4950e-07],\n",
      "        [7.7530e-05, 9.9992e-01],\n",
      "        [9.9989e-01, 1.1234e-04],\n",
      "        [9.9995e-01, 4.8634e-05],\n",
      "        [9.9991e-01, 9.1299e-05]], grad_fn=<SoftmaxBackward0>), tensor([[6.6694e-02, 9.3331e-01],\n",
      "        [1.6417e-03, 9.9836e-01],\n",
      "        [9.9925e-01, 7.4664e-04],\n",
      "        [1.4406e-05, 9.9999e-01],\n",
      "        [9.9999e-01, 1.0105e-05],\n",
      "        [6.7073e-04, 9.9933e-01],\n",
      "        [1.6698e-04, 9.9983e-01],\n",
      "        [2.5925e-03, 9.9741e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9846e-01, 1.5420e-03],\n",
      "        [9.9843e-01, 1.5731e-03],\n",
      "        [3.3028e-04, 9.9967e-01],\n",
      "        [1.0000e+00, 2.3925e-06],\n",
      "        [7.7942e-05, 9.9992e-01],\n",
      "        [9.9490e-01, 5.1013e-03],\n",
      "        [9.9977e-01, 2.2965e-04],\n",
      "        [9.9977e-01, 2.2987e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 12.0 and average score: -0.48\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0800, 0.1600, 0.1200, 0.0400, 0.0000, 0.4000, 0.1200, 0.0800]), tensor([0.0000, 0.0800, 0.1600, 0.1200, 0.0400, 0.0000, 0.4000, 0.1200, 0.0800]), -0.18057371905216804, tensor(6))\n",
      "action: 6\n",
      "Player 0 tictactoe_expert action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.1600, 0.0400, 0.4000, 0.0800, 0.0000, 0.0400, 0.2800]), tensor([0.0000, 0.0000, 0.1600, 0.0400, 0.4000, 0.0800, 0.0000, 0.0400, 0.2800]), -0.11290692179822005, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 2\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 3, 1, 5, 4],\n",
      "        [1, 3, 2, 0, 7],\n",
      "        [4, 0, 7, 5, 7],\n",
      "        [6, 2, 3, 1, 0],\n",
      "        [6, 7, 4, 0, 8],\n",
      "        [6, 0, 1, 0, 7],\n",
      "        [4, 0, 2, 1, 8],\n",
      "        [1, 0, 3, 5, 2]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2461],\n",
      "        [ 0.2184],\n",
      "        [ 0.5539],\n",
      "        [-0.0902],\n",
      "        [ 0.2461],\n",
      "        [ 0.3452],\n",
      "        [ 0.2461],\n",
      "        [-0.3453]], grad_fn=<AddmmBackward0>), tensor([[-0.1412],\n",
      "        [-0.0795],\n",
      "        [-0.1256],\n",
      "        [ 0.1440],\n",
      "        [-0.2010],\n",
      "        [-0.0655],\n",
      "        [-0.3304],\n",
      "        [ 0.4061]], grad_fn=<AddmmBackward0>), tensor([[ 0.2769],\n",
      "        [ 0.1662],\n",
      "        [ 0.0723],\n",
      "        [ 0.0145],\n",
      "        [ 0.3924],\n",
      "        [ 0.1681],\n",
      "        [ 0.2723],\n",
      "        [-0.3358]], grad_fn=<AddmmBackward0>), tensor([[-0.1530],\n",
      "        [ 0.0207],\n",
      "        [ 0.0474],\n",
      "        [ 0.1998],\n",
      "        [-0.2773],\n",
      "        [ 0.0448],\n",
      "        [-0.3013],\n",
      "        [ 0.4179]], grad_fn=<AddmmBackward0>), tensor([[ 0.2554],\n",
      "        [-0.0463],\n",
      "        [ 0.0909],\n",
      "        [-0.0043],\n",
      "        [ 0.1879],\n",
      "        [-0.0148],\n",
      "        [ 0.3271],\n",
      "        [-0.2255]], grad_fn=<AddmmBackward0>), tensor([[-0.2209],\n",
      "        [ 0.0285],\n",
      "        [ 0.0299],\n",
      "        [-0.0609],\n",
      "        [ 0.0279],\n",
      "        [ 0.0104],\n",
      "        [-0.1331],\n",
      "        [ 0.2733]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.0576],\n",
      "        [0.1163],\n",
      "        [0.3781],\n",
      "        [0.0899],\n",
      "        [0.0061],\n",
      "        [0.3681],\n",
      "        [0.0061],\n",
      "        [0.0880]], grad_fn=<AddmmBackward0>), tensor([[ 0.0212],\n",
      "        [ 0.1475],\n",
      "        [ 0.1402],\n",
      "        [ 0.2639],\n",
      "        [ 0.0900],\n",
      "        [ 0.1691],\n",
      "        [-0.0073],\n",
      "        [ 0.1074]], grad_fn=<AddmmBackward0>), tensor([[-0.0604],\n",
      "        [ 0.4196],\n",
      "        [ 0.2970],\n",
      "        [ 0.1696],\n",
      "        [ 0.0778],\n",
      "        [ 0.3994],\n",
      "        [-0.0490],\n",
      "        [ 0.0255]], grad_fn=<AddmmBackward0>), tensor([[-0.0420],\n",
      "        [-0.0260],\n",
      "        [ 0.2949],\n",
      "        [ 0.3721],\n",
      "        [ 0.0343],\n",
      "        [-0.0094],\n",
      "        [ 0.0856],\n",
      "        [ 0.1562]], grad_fn=<AddmmBackward0>), tensor([[ 0.0170],\n",
      "        [-0.0118],\n",
      "        [ 0.1770],\n",
      "        [-0.0103],\n",
      "        [ 0.1959],\n",
      "        [-0.0254],\n",
      "        [ 0.2919],\n",
      "        [ 0.1561]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.9411e-06, 1.0000e+00],\n",
      "        [2.3879e-04, 9.9976e-01],\n",
      "        [1.5507e-04, 9.9984e-01],\n",
      "        [9.9999e-01, 5.1738e-06],\n",
      "        [8.3053e-07, 1.0000e+00],\n",
      "        [1.7235e-05, 9.9998e-01],\n",
      "        [2.0089e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 1.3978e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.8348e-05],\n",
      "        [9.9999e-01, 5.5339e-06],\n",
      "        [9.9999e-01, 9.9069e-06],\n",
      "        [9.4925e-05, 9.9991e-01],\n",
      "        [9.9999e-01, 8.9951e-06],\n",
      "        [9.9998e-01, 1.8730e-05],\n",
      "        [9.9999e-01, 1.4037e-05],\n",
      "        [1.2749e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.1641e-06, 9.9999e-01],\n",
      "        [2.4947e-03, 9.9751e-01],\n",
      "        [2.1956e-04, 9.9978e-01],\n",
      "        [9.9999e-01, 1.3343e-05],\n",
      "        [5.8550e-06, 9.9999e-01],\n",
      "        [6.2839e-04, 9.9937e-01],\n",
      "        [1.2077e-05, 9.9999e-01],\n",
      "        [1.0000e+00, 3.7170e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.4387e-05],\n",
      "        [9.8454e-01, 1.5463e-02],\n",
      "        [9.9907e-01, 9.3296e-04],\n",
      "        [2.0058e-04, 9.9980e-01],\n",
      "        [9.9999e-01, 1.3605e-05],\n",
      "        [9.9493e-01, 5.0664e-03],\n",
      "        [1.0000e+00, 1.7924e-06],\n",
      "        [3.0194e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[7.4763e-05, 9.9993e-01],\n",
      "        [2.7799e-02, 9.7220e-01],\n",
      "        [6.2037e-04, 9.9938e-01],\n",
      "        [9.9902e-01, 9.7871e-04],\n",
      "        [5.4604e-05, 9.9995e-01],\n",
      "        [3.1326e-01, 6.8674e-01],\n",
      "        [2.7545e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 1.3431e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 0.0 and average score: -0.92\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.48, 'player_0_win%': 0.12, 'player_1_score': -0.92, 'player_1_win%': 0.0, 'score': -0.7}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 2, 0, 3, 1],\n",
      "        [2, 0, 8, 3, 1],\n",
      "        [7, 2, 4, 0, 1],\n",
      "        [0, 6, 1, 2, 0],\n",
      "        [6, 1, 0, 3, 0],\n",
      "        [2, 8, 4, 0, 1],\n",
      "        [6, 5, 3, 4, 8],\n",
      "        [2, 7, 0, 3, 1]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.4404],\n",
      "        [ 0.4703],\n",
      "        [-0.0928],\n",
      "        [-0.2375],\n",
      "        [-0.3281],\n",
      "        [ 0.4468],\n",
      "        [ 0.2127],\n",
      "        [ 0.3294]], grad_fn=<AddmmBackward0>), tensor([[-0.0478],\n",
      "        [ 0.0406],\n",
      "        [ 0.5843],\n",
      "        [ 0.3247],\n",
      "        [ 0.5225],\n",
      "        [-0.2876],\n",
      "        [-0.0789],\n",
      "        [ 0.5380]], grad_fn=<AddmmBackward0>), tensor([[ 0.5246],\n",
      "        [ 0.0395],\n",
      "        [-0.0304],\n",
      "        [-0.0221],\n",
      "        [ 0.1004],\n",
      "        [ 0.3610],\n",
      "        [ 0.4264],\n",
      "        [ 0.0942]], grad_fn=<AddmmBackward0>), tensor([[ 0.1106],\n",
      "        [-0.0036],\n",
      "        [ 0.4336],\n",
      "        [ 0.3079],\n",
      "        [ 0.3208],\n",
      "        [-0.1562],\n",
      "        [ 0.0695],\n",
      "        [ 0.0079]], grad_fn=<AddmmBackward0>), tensor([[ 0.2414],\n",
      "        [ 0.0548],\n",
      "        [ 0.0898],\n",
      "        [-0.0182],\n",
      "        [ 0.0637],\n",
      "        [ 0.2770],\n",
      "        [ 0.3129],\n",
      "        [ 0.0801]], grad_fn=<AddmmBackward0>), tensor([[0.1100],\n",
      "        [0.2068],\n",
      "        [0.3204],\n",
      "        [0.0040],\n",
      "        [0.0492],\n",
      "        [0.0772],\n",
      "        [0.1075],\n",
      "        [0.2072]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.4354],\n",
      "        [ 0.3764],\n",
      "        [-0.0170],\n",
      "        [ 0.0108],\n",
      "        [ 0.0933],\n",
      "        [-0.0672],\n",
      "        [ 0.1397],\n",
      "        [ 0.3455]], grad_fn=<AddmmBackward0>), tensor([[ 0.3564],\n",
      "        [-0.0529],\n",
      "        [ 0.2925],\n",
      "        [ 0.2628],\n",
      "        [ 0.2899],\n",
      "        [-0.0668],\n",
      "        [ 0.0802],\n",
      "        [ 0.4531]], grad_fn=<AddmmBackward0>), tensor([[ 0.2377],\n",
      "        [-0.0702],\n",
      "        [ 0.2630],\n",
      "        [ 0.2264],\n",
      "        [ 0.2796],\n",
      "        [ 0.1550],\n",
      "        [ 0.0638],\n",
      "        [ 0.0126]], grad_fn=<AddmmBackward0>), tensor([[ 1.2655e-01],\n",
      "        [-1.3187e-01],\n",
      "        [ 6.0566e-01],\n",
      "        [ 6.4045e-01],\n",
      "        [ 4.2863e-01],\n",
      "        [ 7.8480e-03],\n",
      "        [ 5.2965e-02],\n",
      "        [ 9.5954e-06]], grad_fn=<AddmmBackward0>), tensor([[0.2324],\n",
      "        [0.0042],\n",
      "        [0.2326],\n",
      "        [0.0188],\n",
      "        [0.0370],\n",
      "        [0.3609],\n",
      "        [0.3088],\n",
      "        [0.0442]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.8697e-06, 9.9999e-01],\n",
      "        [9.3340e-06, 9.9999e-01],\n",
      "        [9.9993e-01, 6.9717e-05],\n",
      "        [9.9997e-01, 3.4751e-05],\n",
      "        [1.0000e+00, 1.1190e-06],\n",
      "        [8.0329e-06, 9.9999e-01],\n",
      "        [3.7085e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 1.3898e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 6.8974e-06],\n",
      "        [8.1959e-01, 1.8041e-01],\n",
      "        [9.4176e-05, 9.9991e-01],\n",
      "        [3.2821e-05, 9.9997e-01],\n",
      "        [3.5577e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 1.4772e-06],\n",
      "        [9.9999e-01, 6.9374e-06],\n",
      "        [1.7368e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[4.4300e-05, 9.9996e-01],\n",
      "        [3.6448e-02, 9.6355e-01],\n",
      "        [1.0000e+00, 4.0977e-07],\n",
      "        [9.9985e-01, 1.4702e-04],\n",
      "        [9.9984e-01, 1.5688e-04],\n",
      "        [9.9114e-06, 9.9999e-01],\n",
      "        [1.0427e-05, 9.9999e-01],\n",
      "        [9.9198e-01, 8.0179e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.9281e-01, 7.1927e-03],\n",
      "        [7.3294e-02, 9.2671e-01],\n",
      "        [1.1702e-06, 1.0000e+00],\n",
      "        [5.2267e-05, 9.9995e-01],\n",
      "        [8.1659e-06, 9.9999e-01],\n",
      "        [9.9997e-01, 2.9382e-05],\n",
      "        [1.0000e+00, 1.4804e-06],\n",
      "        [1.6396e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.3731e-05, 9.9998e-01],\n",
      "        [4.9965e-02, 9.5004e-01],\n",
      "        [9.9996e-01, 4.4992e-05],\n",
      "        [9.7951e-01, 2.0489e-02],\n",
      "        [8.2760e-01, 1.7240e-01],\n",
      "        [6.0901e-06, 9.9999e-01],\n",
      "        [7.8791e-07, 1.0000e+00],\n",
      "        [8.2630e-01, 1.7370e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 7, 3, 4, 2],\n",
      "        [6, 1, 4, 0, 1],\n",
      "        [0, 5, 1, 3, 0],\n",
      "        [6, 3, 8, 1, 0],\n",
      "        [5, 1, 3, 8, 7],\n",
      "        [8, 0, 3, 8, 1],\n",
      "        [4, 2, 0, 8, 1],\n",
      "        [1, 8, 0, 8, 1]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2972],\n",
      "        [ 0.0876],\n",
      "        [-0.1956],\n",
      "        [ 0.1503],\n",
      "        [ 0.3201],\n",
      "        [ 0.6045],\n",
      "        [-0.1884],\n",
      "        [-0.5751]], grad_fn=<AddmmBackward0>), tensor([[-0.2119],\n",
      "        [-0.3260],\n",
      "        [ 0.2710],\n",
      "        [-0.1739],\n",
      "        [-0.1888],\n",
      "        [-0.2741],\n",
      "        [ 0.0617],\n",
      "        [ 0.5199]], grad_fn=<AddmmBackward0>), tensor([[ 0.2295],\n",
      "        [ 0.4570],\n",
      "        [ 0.0282],\n",
      "        [ 0.3178],\n",
      "        [ 0.3655],\n",
      "        [ 0.1124],\n",
      "        [-0.1515],\n",
      "        [-0.3426]], grad_fn=<AddmmBackward0>), tensor([[-0.0556],\n",
      "        [-0.1637],\n",
      "        [ 0.2218],\n",
      "        [-0.0838],\n",
      "        [-0.0721],\n",
      "        [-0.1927],\n",
      "        [-0.1019],\n",
      "        [ 0.1357]], grad_fn=<AddmmBackward0>), tensor([[ 0.1652],\n",
      "        [-0.0945],\n",
      "        [-0.0298],\n",
      "        [ 0.4123],\n",
      "        [ 0.2948],\n",
      "        [ 0.1000],\n",
      "        [-0.1350],\n",
      "        [-0.2280]], grad_fn=<AddmmBackward0>), tensor([[ 0.0164],\n",
      "        [-0.1467],\n",
      "        [-0.0370],\n",
      "        [-0.0736],\n",
      "        [ 0.0035],\n",
      "        [-0.1953],\n",
      "        [ 0.1138],\n",
      "        [ 0.1743]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.0168],\n",
      "        [0.0854],\n",
      "        [0.0931],\n",
      "        [0.1408],\n",
      "        [0.0401],\n",
      "        [0.4552],\n",
      "        [0.1120],\n",
      "        [0.0157]], grad_fn=<AddmmBackward0>), tensor([[-0.0790],\n",
      "        [-0.0542],\n",
      "        [ 0.2327],\n",
      "        [-0.0172],\n",
      "        [ 0.0522],\n",
      "        [-0.0915],\n",
      "        [ 0.3043],\n",
      "        [ 0.5748]], grad_fn=<AddmmBackward0>), tensor([[-0.1115],\n",
      "        [ 0.5770],\n",
      "        [ 0.3042],\n",
      "        [ 0.5898],\n",
      "        [ 0.3969],\n",
      "        [-0.0617],\n",
      "        [-0.0339],\n",
      "        [-0.0598]], grad_fn=<AddmmBackward0>), tensor([[-0.0412],\n",
      "        [-0.0321],\n",
      "        [ 0.4666],\n",
      "        [ 0.3890],\n",
      "        [ 0.3244],\n",
      "        [-0.0735],\n",
      "        [-0.0031],\n",
      "        [-0.0566]], grad_fn=<AddmmBackward0>), tensor([[ 0.1981],\n",
      "        [ 0.0995],\n",
      "        [-0.0193],\n",
      "        [ 0.1910],\n",
      "        [ 0.8092],\n",
      "        [ 0.1091],\n",
      "        [ 0.0816],\n",
      "        [ 0.0311]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[4.6907e-07, 1.0000e+00],\n",
      "        [6.0832e-05, 9.9994e-01],\n",
      "        [9.9999e-01, 8.2669e-06],\n",
      "        [5.1415e-05, 9.9995e-01],\n",
      "        [1.4814e-04, 9.9985e-01],\n",
      "        [4.1900e-04, 9.9958e-01],\n",
      "        [9.9998e-01, 2.4555e-05],\n",
      "        [9.9999e-01, 6.7008e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 9.5974e-07],\n",
      "        [9.9998e-01, 1.6556e-05],\n",
      "        [4.8956e-04, 9.9951e-01],\n",
      "        [9.9996e-01, 3.7807e-05],\n",
      "        [9.9998e-01, 2.3824e-05],\n",
      "        [9.9975e-01, 2.4590e-04],\n",
      "        [1.6273e-04, 9.9984e-01],\n",
      "        [4.6382e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.1984e-06, 1.0000e+00],\n",
      "        [4.7242e-05, 9.9995e-01],\n",
      "        [9.9988e-01, 1.1590e-04],\n",
      "        [7.1489e-05, 9.9993e-01],\n",
      "        [2.0290e-05, 9.9998e-01],\n",
      "        [9.3725e-05, 9.9991e-01],\n",
      "        [9.9624e-01, 3.7589e-03],\n",
      "        [9.9992e-01, 8.2155e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 5.4375e-07],\n",
      "        [9.9983e-01, 1.6534e-04],\n",
      "        [1.0197e-04, 9.9990e-01],\n",
      "        [9.9996e-01, 3.6514e-05],\n",
      "        [9.9999e-01, 6.3302e-06],\n",
      "        [9.9999e-01, 1.0466e-05],\n",
      "        [1.9917e-03, 9.9801e-01],\n",
      "        [4.7088e-04, 9.9953e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.1815e-06, 1.0000e+00],\n",
      "        [1.0950e-04, 9.9989e-01],\n",
      "        [9.4554e-01, 5.4461e-02],\n",
      "        [3.8676e-03, 9.9613e-01],\n",
      "        [4.7926e-06, 1.0000e+00],\n",
      "        [2.1205e-06, 1.0000e+00],\n",
      "        [9.9885e-01, 1.1491e-03],\n",
      "        [9.9994e-01, 6.0804e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 4, 2, 6, 7],\n",
      "        [1, 7, 8, 3, 2],\n",
      "        [0, 1, 2, 7, 4],\n",
      "        [4, 6, 3, 5, 0],\n",
      "        [3, 5, 6, 0, 4],\n",
      "        [2, 0, 5, 1, 4],\n",
      "        [4, 6, 3, 1, 0],\n",
      "        [1, 2, 4, 6, 8]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.0501],\n",
      "        [ 0.0432],\n",
      "        [ 0.1646],\n",
      "        [ 0.1646],\n",
      "        [-0.4412],\n",
      "        [ 0.4294],\n",
      "        [ 0.1646],\n",
      "        [ 0.2736]], grad_fn=<AddmmBackward0>), tensor([[ 0.0549],\n",
      "        [ 0.3709],\n",
      "        [-0.1842],\n",
      "        [-0.4956],\n",
      "        [ 0.5558],\n",
      "        [-0.0045],\n",
      "        [-0.4956],\n",
      "        [-0.0920]], grad_fn=<AddmmBackward0>), tensor([[-0.0297],\n",
      "        [-0.2201],\n",
      "        [ 0.4590],\n",
      "        [ 0.3878],\n",
      "        [-0.0603],\n",
      "        [ 0.0354],\n",
      "        [ 0.3878],\n",
      "        [ 0.2373]], grad_fn=<AddmmBackward0>), tensor([[ 0.1338],\n",
      "        [ 0.4849],\n",
      "        [-0.1227],\n",
      "        [-0.3022],\n",
      "        [ 0.4086],\n",
      "        [ 0.0033],\n",
      "        [-0.3022],\n",
      "        [ 0.0837]], grad_fn=<AddmmBackward0>), tensor([[ 0.2498],\n",
      "        [-0.0732],\n",
      "        [ 0.5229],\n",
      "        [ 0.4851],\n",
      "        [ 0.0065],\n",
      "        [ 0.0580],\n",
      "        [ 0.3861],\n",
      "        [ 0.2914]], grad_fn=<AddmmBackward0>), tensor([[ 0.2098],\n",
      "        [ 0.3858],\n",
      "        [-0.1414],\n",
      "        [-0.2121],\n",
      "        [ 0.1729],\n",
      "        [-0.0989],\n",
      "        [-0.3438],\n",
      "        [ 0.0581]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0563],\n",
      "        [ 0.0057],\n",
      "        [-0.0540],\n",
      "        [-0.0581],\n",
      "        [ 0.1548],\n",
      "        [ 0.2866],\n",
      "        [-0.0581],\n",
      "        [-0.0622]], grad_fn=<AddmmBackward0>), tensor([[0.0984],\n",
      "        [0.0652],\n",
      "        [0.0687],\n",
      "        [0.0967],\n",
      "        [0.2324],\n",
      "        [0.0093],\n",
      "        [0.0967],\n",
      "        [0.0067]], grad_fn=<AddmmBackward0>), tensor([[ 0.1305],\n",
      "        [ 0.0265],\n",
      "        [ 0.1254],\n",
      "        [-0.0492],\n",
      "        [ 0.2157],\n",
      "        [ 0.0211],\n",
      "        [-0.0492],\n",
      "        [ 0.0960]], grad_fn=<AddmmBackward0>), tensor([[ 0.3111],\n",
      "        [ 0.1563],\n",
      "        [-0.0234],\n",
      "        [-0.0105],\n",
      "        [-0.0003],\n",
      "        [ 0.0597],\n",
      "        [ 0.0276],\n",
      "        [ 0.1597]], grad_fn=<AddmmBackward0>), tensor([[ 0.2778],\n",
      "        [ 0.0177],\n",
      "        [ 0.1813],\n",
      "        [ 0.1011],\n",
      "        [-0.0690],\n",
      "        [-0.0311],\n",
      "        [ 0.1796],\n",
      "        [ 0.3485]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[6.2580e-06, 9.9999e-01],\n",
      "        [1.1068e-02, 9.8893e-01],\n",
      "        [1.3561e-06, 1.0000e+00],\n",
      "        [4.3066e-07, 1.0000e+00],\n",
      "        [9.9999e-01, 8.3191e-06],\n",
      "        [1.1452e-02, 9.8855e-01],\n",
      "        [4.3066e-07, 1.0000e+00],\n",
      "        [7.5715e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.8555e-05],\n",
      "        [9.9885e-01, 1.1478e-03],\n",
      "        [9.9999e-01, 6.9626e-06],\n",
      "        [1.0000e+00, 2.9574e-06],\n",
      "        [3.1836e-05, 9.9997e-01],\n",
      "        [9.8826e-01, 1.1744e-02],\n",
      "        [1.0000e+00, 2.9574e-06],\n",
      "        [9.9999e-01, 6.3625e-06]], grad_fn=<SoftmaxBackward0>), tensor([[2.2168e-05, 9.9998e-01],\n",
      "        [3.8919e-03, 9.9611e-01],\n",
      "        [1.0003e-05, 9.9999e-01],\n",
      "        [1.2571e-05, 9.9999e-01],\n",
      "        [1.0000e+00, 1.7619e-06],\n",
      "        [3.7867e-01, 6.2133e-01],\n",
      "        [1.2571e-05, 9.9999e-01],\n",
      "        [1.1785e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.4769e-06],\n",
      "        [9.9975e-01, 2.4846e-04],\n",
      "        [1.0000e+00, 4.8760e-06],\n",
      "        [1.0000e+00, 4.8879e-06],\n",
      "        [2.8718e-04, 9.9971e-01],\n",
      "        [9.9477e-01, 5.2343e-03],\n",
      "        [1.0000e+00, 3.1142e-06],\n",
      "        [9.9999e-01, 1.2953e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.6136e-06, 9.9999e-01],\n",
      "        [7.7208e-05, 9.9992e-01],\n",
      "        [9.7062e-06, 9.9999e-01],\n",
      "        [9.2528e-06, 9.9999e-01],\n",
      "        [9.9988e-01, 1.2395e-04],\n",
      "        [6.3769e-02, 9.3623e-01],\n",
      "        [1.2138e-05, 9.9999e-01],\n",
      "        [2.8847e-04, 9.9971e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 0, 6, 3, 2],\n",
      "        [1, 5, 6, 0, 0],\n",
      "        [8, 6, 4, 0, 2],\n",
      "        [2, 4, 1, 7, 0],\n",
      "        [3, 2, 4, 1, 0],\n",
      "        [4, 7, 2, 5, 8],\n",
      "        [8, 0, 6, 3, 2],\n",
      "        [0, 3, 6, 7, 4]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.2588],\n",
      "        [-0.5395],\n",
      "        [ 0.3153],\n",
      "        [-0.3114],\n",
      "        [-0.2350],\n",
      "        [-0.3114],\n",
      "        [ 0.5467],\n",
      "        [ 0.2807]], grad_fn=<AddmmBackward0>), tensor([[ 0.2351],\n",
      "        [ 0.4782],\n",
      "        [ 0.1758],\n",
      "        [ 0.3108],\n",
      "        [ 0.2794],\n",
      "        [ 0.3380],\n",
      "        [-0.0727],\n",
      "        [-0.2324]], grad_fn=<AddmmBackward0>), tensor([[ 0.0138],\n",
      "        [-0.3431],\n",
      "        [ 0.3658],\n",
      "        [-0.2264],\n",
      "        [-0.1799],\n",
      "        [-0.2637],\n",
      "        [ 0.2218],\n",
      "        [ 0.2774]], grad_fn=<AddmmBackward0>), tensor([[ 0.0267],\n",
      "        [ 0.6164],\n",
      "        [ 0.1774],\n",
      "        [ 0.2806],\n",
      "        [ 0.2977],\n",
      "        [ 0.2147],\n",
      "        [ 0.0055],\n",
      "        [-0.3070]], grad_fn=<AddmmBackward0>), tensor([[ 0.1334],\n",
      "        [-0.0280],\n",
      "        [ 0.1133],\n",
      "        [-0.0741],\n",
      "        [ 0.0131],\n",
      "        [-0.0581],\n",
      "        [ 0.1000],\n",
      "        [ 0.3814]], grad_fn=<AddmmBackward0>), tensor([[ 0.0673],\n",
      "        [ 0.1238],\n",
      "        [ 0.1118],\n",
      "        [ 0.3059],\n",
      "        [-0.0082],\n",
      "        [ 0.3758],\n",
      "        [-0.0219],\n",
      "        [-0.2878]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.5039],\n",
      "        [ 0.0812],\n",
      "        [ 0.1904],\n",
      "        [ 0.0083],\n",
      "        [-0.0549],\n",
      "        [ 0.0028],\n",
      "        [ 0.3363],\n",
      "        [ 0.0507]], grad_fn=<AddmmBackward0>), tensor([[0.0962],\n",
      "        [0.0501],\n",
      "        [0.2264],\n",
      "        [0.0383],\n",
      "        [0.2353],\n",
      "        [0.0468],\n",
      "        [0.1654],\n",
      "        [0.0798]], grad_fn=<AddmmBackward0>), tensor([[0.0273],\n",
      "        [0.0659],\n",
      "        [0.5295],\n",
      "        [0.0803],\n",
      "        [0.2018],\n",
      "        [0.0788],\n",
      "        [0.0353],\n",
      "        [0.0941]], grad_fn=<AddmmBackward0>), tensor([[0.0716],\n",
      "        [0.2856],\n",
      "        [0.0978],\n",
      "        [0.2101],\n",
      "        [0.3715],\n",
      "        [0.0045],\n",
      "        [0.1314],\n",
      "        [0.0516]], grad_fn=<AddmmBackward0>), tensor([[0.0361],\n",
      "        [0.1104],\n",
      "        [0.0228],\n",
      "        [0.2018],\n",
      "        [0.0611],\n",
      "        [0.1808],\n",
      "        [0.1477],\n",
      "        [0.2928]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.5201e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 2.0663e-06],\n",
      "        [2.1804e-04, 9.9978e-01],\n",
      "        [9.9999e-01, 4.9843e-06],\n",
      "        [9.9983e-01, 1.6956e-04],\n",
      "        [9.9999e-01, 1.0087e-05],\n",
      "        [7.4066e-05, 9.9993e-01],\n",
      "        [3.6203e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.7435e-01, 2.5649e-02],\n",
      "        [1.5162e-06, 1.0000e+00],\n",
      "        [9.9995e-01, 4.5137e-05],\n",
      "        [2.9954e-06, 1.0000e+00],\n",
      "        [1.0204e-04, 9.9990e-01],\n",
      "        [1.5028e-06, 1.0000e+00],\n",
      "        [9.9955e-01, 4.5018e-04],\n",
      "        [9.9999e-01, 5.7584e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.2171e-03, 9.9078e-01],\n",
      "        [9.9998e-01, 1.6171e-05],\n",
      "        [1.9895e-04, 9.9980e-01],\n",
      "        [9.9999e-01, 1.0223e-05],\n",
      "        [9.9999e-01, 7.9015e-06],\n",
      "        [1.0000e+00, 4.6494e-06],\n",
      "        [1.8541e-03, 9.9815e-01],\n",
      "        [7.7608e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.7454e-01, 2.5463e-02],\n",
      "        [4.1672e-06, 1.0000e+00],\n",
      "        [9.8251e-01, 1.7493e-02],\n",
      "        [2.0424e-06, 1.0000e+00],\n",
      "        [6.9259e-05, 9.9993e-01],\n",
      "        [1.1960e-06, 1.0000e+00],\n",
      "        [9.9995e-01, 5.1113e-05],\n",
      "        [9.9999e-01, 5.3851e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0803e-02, 9.8920e-01],\n",
      "        [9.9647e-01, 3.5335e-03],\n",
      "        [4.6916e-03, 9.9531e-01],\n",
      "        [9.9998e-01, 1.7676e-05],\n",
      "        [8.8913e-01, 1.1087e-01],\n",
      "        [1.0000e+00, 9.6966e-07],\n",
      "        [4.9748e-05, 9.9995e-01],\n",
      "        [6.3876e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: 0.35\n",
      "Test score {'score': 0.35, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 0, 7, 1, 3],\n",
      "        [8, 4, 0, 3, 5],\n",
      "        [4, 2, 3, 0, 0],\n",
      "        [6, 0, 8, 3, 5],\n",
      "        [2, 3, 0, 3, 5],\n",
      "        [8, 0, 8, 3, 5],\n",
      "        [7, 0, 1, 6, 2],\n",
      "        [1, 8, 4, 0, 5]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.5264],\n",
      "        [-0.1931],\n",
      "        [ 0.2614],\n",
      "        [-0.0827],\n",
      "        [ 0.1484],\n",
      "        [ 0.7811],\n",
      "        [-0.6761],\n",
      "        [ 0.4109]], grad_fn=<AddmmBackward0>), tensor([[-0.4957],\n",
      "        [ 0.3460],\n",
      "        [-0.5096],\n",
      "        [ 0.3969],\n",
      "        [ 0.2798],\n",
      "        [-0.2382],\n",
      "        [ 0.6517],\n",
      "        [-0.1695]], grad_fn=<AddmmBackward0>), tensor([[ 0.3255],\n",
      "        [ 0.0274],\n",
      "        [ 0.2796],\n",
      "        [-0.0216],\n",
      "        [ 0.3867],\n",
      "        [ 0.1853],\n",
      "        [-0.6359],\n",
      "        [ 0.1799]], grad_fn=<AddmmBackward0>), tensor([[-0.2848],\n",
      "        [ 0.1090],\n",
      "        [-0.1645],\n",
      "        [ 0.1011],\n",
      "        [ 0.0455],\n",
      "        [-0.0975],\n",
      "        [ 0.8830],\n",
      "        [-0.3429]], grad_fn=<AddmmBackward0>), tensor([[ 0.5465],\n",
      "        [ 0.0173],\n",
      "        [ 0.3759],\n",
      "        [-0.0237],\n",
      "        [ 0.1792],\n",
      "        [ 0.2829],\n",
      "        [-0.6154],\n",
      "        [ 0.3706]], grad_fn=<AddmmBackward0>), tensor([[-0.0338],\n",
      "        [-0.0102],\n",
      "        [-0.0346],\n",
      "        [ 0.1452],\n",
      "        [-0.1337],\n",
      "        [-0.1966],\n",
      "        [ 0.6340],\n",
      "        [-0.3365]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0625],\n",
      "        [ 0.0586],\n",
      "        [-0.0380],\n",
      "        [ 0.4162],\n",
      "        [ 0.5941],\n",
      "        [ 0.5659],\n",
      "        [-0.0770],\n",
      "        [-0.0230]], grad_fn=<AddmmBackward0>), tensor([[ 1.0464e-02],\n",
      "        [ 6.8674e-01],\n",
      "        [-6.1884e-04],\n",
      "        [ 9.9594e-02],\n",
      "        [ 4.1199e-01],\n",
      "        [ 7.1854e-03],\n",
      "        [-1.0993e-02],\n",
      "        [-1.4203e-01]], grad_fn=<AddmmBackward0>), tensor([[ 0.1647],\n",
      "        [ 0.0576],\n",
      "        [ 0.1627],\n",
      "        [-0.0383],\n",
      "        [ 0.1400],\n",
      "        [ 0.1151],\n",
      "        [ 0.1270],\n",
      "        [-0.0465]], grad_fn=<AddmmBackward0>), tensor([[ 0.1531],\n",
      "        [ 0.0674],\n",
      "        [ 0.1214],\n",
      "        [ 0.0352],\n",
      "        [-0.0250],\n",
      "        [-0.0136],\n",
      "        [ 0.1621],\n",
      "        [-0.0299]], grad_fn=<AddmmBackward0>), tensor([[ 0.4168],\n",
      "        [-0.0376],\n",
      "        [-0.0263],\n",
      "        [ 0.0338],\n",
      "        [ 0.1036],\n",
      "        [ 0.1681],\n",
      "        [ 0.1164],\n",
      "        [ 0.0513]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[2.8367e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 1.1111e-05],\n",
      "        [9.2135e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 5.2750e-06],\n",
      "        [1.2577e-04, 9.9987e-01],\n",
      "        [8.5344e-06, 9.9999e-01],\n",
      "        [9.9996e-01, 3.7364e-05],\n",
      "        [6.1725e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.5413e-05],\n",
      "        [1.7157e-04, 9.9983e-01],\n",
      "        [9.9999e-01, 5.2284e-06],\n",
      "        [2.5039e-04, 9.9975e-01],\n",
      "        [9.9996e-01, 4.0895e-05],\n",
      "        [9.9995e-01, 5.1434e-05],\n",
      "        [1.9265e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 6.3481e-07]], grad_fn=<SoftmaxBackward0>), tensor([[1.9367e-06, 1.0000e+00],\n",
      "        [9.9910e-01, 8.9880e-04],\n",
      "        [1.2987e-05, 9.9999e-01],\n",
      "        [9.9998e-01, 2.2861e-05],\n",
      "        [2.3266e-03, 9.9767e-01],\n",
      "        [3.9501e-04, 9.9961e-01],\n",
      "        [1.0000e+00, 3.8815e-06],\n",
      "        [4.1203e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 7.8689e-06],\n",
      "        [8.8768e-03, 9.9112e-01],\n",
      "        [1.0000e+00, 1.0485e-06],\n",
      "        [1.8118e-05, 9.9998e-01],\n",
      "        [9.9871e-01, 1.2859e-03],\n",
      "        [1.0000e+00, 2.1152e-06],\n",
      "        [1.9889e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 6.6893e-06]], grad_fn=<SoftmaxBackward0>), tensor([[4.1912e-06, 1.0000e+00],\n",
      "        [9.9961e-01, 3.9224e-04],\n",
      "        [1.0991e-03, 9.9890e-01],\n",
      "        [9.9995e-01, 4.5112e-05],\n",
      "        [7.8926e-04, 9.9921e-01],\n",
      "        [3.8578e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 2.6942e-06],\n",
      "        [1.2188e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0000, 0.0000, 0.0000, 0.7200, 0.0000, 0.0000, 0.0400, 0.1600]), tensor([0.0800, 0.0000, 0.0000, 0.0000, 0.7200, 0.0000, 0.0000, 0.0400, 0.1600]), 0.41336053824768615, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.3600, 0.0400, 0.0400, 0.0000, 0.0000, 0.1600, 0.1600, 0.1200, 0.1200]), tensor([0.3600, 0.0400, 0.0400, 0.0000, 0.0000, 0.1600, 0.1600, 0.1200, 0.1200]), 0.36735492729796815, tensor(0))\n",
      "action: 0\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.2000, 0.3200, 0.0000, 0.0000, 0.0800, 0.2800, 0.1200, 0.0000]), tensor([0.0000, 0.2000, 0.3200, 0.0000, 0.0000, 0.0800, 0.2800, 0.1200, 0.0000]), 0.41786694440704125, tensor(2))\n",
      "action: 2\n",
      "Player 1 random action: 6\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.2800, 0.0000, 0.3600, 0.0000]), tensor([0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.2800, 0.0000, 0.3600, 0.0000]), 0.5919868082094651, tensor(1))\n",
      "action: 1\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 1, 3, 6, 7],\n",
      "        [3, 2, 0, 1, 7],\n",
      "        [1, 2, 7, 0, 7],\n",
      "        [2, 7, 8, 1, 0],\n",
      "        [2, 4, 6, 0, 0],\n",
      "        [1, 0, 0, 1, 7],\n",
      "        [2, 8, 5, 4, 1],\n",
      "        [7, 8, 4, 6, 2]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1932],\n",
      "        [ 0.3521],\n",
      "        [-0.0912],\n",
      "        [-0.0964],\n",
      "        [ 0.1249],\n",
      "        [ 0.3389],\n",
      "        [-0.0939],\n",
      "        [-0.2553]], grad_fn=<AddmmBackward0>), tensor([[-0.1887],\n",
      "        [ 0.1294],\n",
      "        [ 0.2809],\n",
      "        [ 0.2990],\n",
      "        [ 0.1835],\n",
      "        [ 0.3992],\n",
      "        [ 0.2554],\n",
      "        [ 0.3141]], grad_fn=<AddmmBackward0>), tensor([[ 0.3118],\n",
      "        [ 0.3192],\n",
      "        [ 0.1045],\n",
      "        [ 0.2237],\n",
      "        [ 0.2451],\n",
      "        [ 0.2288],\n",
      "        [ 0.0385],\n",
      "        [-0.2359]], grad_fn=<AddmmBackward0>), tensor([[-0.1454],\n",
      "        [ 0.1809],\n",
      "        [ 0.3869],\n",
      "        [ 0.2549],\n",
      "        [ 0.1582],\n",
      "        [ 0.0828],\n",
      "        [ 0.3812],\n",
      "        [ 0.1914]], grad_fn=<AddmmBackward0>), tensor([[ 0.3089],\n",
      "        [ 0.0984],\n",
      "        [ 0.0776],\n",
      "        [ 0.2957],\n",
      "        [ 0.2752],\n",
      "        [-0.0133],\n",
      "        [-0.0267],\n",
      "        [-0.0529]], grad_fn=<AddmmBackward0>), tensor([[0.2408],\n",
      "        [0.0718],\n",
      "        [0.1023],\n",
      "        [0.0911],\n",
      "        [0.1855],\n",
      "        [0.0134],\n",
      "        [0.3965],\n",
      "        [0.3931]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0606],\n",
      "        [ 0.4522],\n",
      "        [ 0.1068],\n",
      "        [ 0.0560],\n",
      "        [-0.0958],\n",
      "        [ 0.8039],\n",
      "        [ 0.0389],\n",
      "        [ 0.0635]], grad_fn=<AddmmBackward0>), tensor([[0.0466],\n",
      "        [0.5691],\n",
      "        [0.2936],\n",
      "        [0.1475],\n",
      "        [0.0158],\n",
      "        [0.1013],\n",
      "        [0.0196],\n",
      "        [0.1115]], grad_fn=<AddmmBackward0>), tensor([[-0.0320],\n",
      "        [ 0.1426],\n",
      "        [ 0.3259],\n",
      "        [ 0.3017],\n",
      "        [ 0.1109],\n",
      "        [ 0.0439],\n",
      "        [ 0.0300],\n",
      "        [ 0.0606]], grad_fn=<AddmmBackward0>), tensor([[0.0638],\n",
      "        [0.3235],\n",
      "        [0.0332],\n",
      "        [0.4957],\n",
      "        [0.1749],\n",
      "        [0.0960],\n",
      "        [0.1951],\n",
      "        [0.1880]], grad_fn=<AddmmBackward0>), tensor([[0.0631],\n",
      "        [0.2019],\n",
      "        [0.0638],\n",
      "        [0.0743],\n",
      "        [0.0937],\n",
      "        [0.1447],\n",
      "        [0.2217],\n",
      "        [0.3412]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[5.6658e-07, 1.0000e+00],\n",
      "        [3.5649e-05, 9.9996e-01],\n",
      "        [9.9995e-01, 5.4874e-05],\n",
      "        [9.9999e-01, 6.3076e-06],\n",
      "        [6.5126e-05, 9.9993e-01],\n",
      "        [2.4686e-01, 7.5314e-01],\n",
      "        [9.9999e-01, 9.1471e-06],\n",
      "        [9.9999e-01, 1.2774e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 3.8993e-06],\n",
      "        [9.9952e-01, 4.7905e-04],\n",
      "        [2.4476e-05, 9.9998e-01],\n",
      "        [9.1881e-05, 9.9991e-01],\n",
      "        [1.0000e+00, 2.5709e-06],\n",
      "        [7.7294e-01, 2.2706e-01],\n",
      "        [4.2026e-05, 9.9996e-01],\n",
      "        [4.8698e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[7.7693e-06, 9.9999e-01],\n",
      "        [1.4960e-02, 9.8504e-01],\n",
      "        [9.9978e-01, 2.1943e-04],\n",
      "        [9.8918e-01, 1.0819e-02],\n",
      "        [6.6835e-05, 9.9993e-01],\n",
      "        [8.9061e-01, 1.0939e-01],\n",
      "        [1.0000e+00, 2.1445e-06],\n",
      "        [1.0000e+00, 3.4333e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 5.2901e-07],\n",
      "        [2.1136e-01, 7.8864e-01],\n",
      "        [1.2868e-02, 9.8713e-01],\n",
      "        [1.7302e-03, 9.9827e-01],\n",
      "        [9.9995e-01, 4.7441e-05],\n",
      "        [3.4933e-01, 6.5067e-01],\n",
      "        [4.9380e-06, 1.0000e+00],\n",
      "        [1.7446e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.1209e-06, 9.9999e-01],\n",
      "        [2.7433e-01, 7.2567e-01],\n",
      "        [9.8431e-01, 1.5686e-02],\n",
      "        [9.3443e-01, 6.5574e-02],\n",
      "        [1.2221e-01, 8.7779e-01],\n",
      "        [7.8076e-01, 2.1924e-01],\n",
      "        [9.9999e-01, 1.3051e-05],\n",
      "        [9.9998e-01, 2.2555e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 82.0 and average score: 0.68\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.0800, 0.0400, 0.0000, 0.4000, 0.0000, 0.0800, 0.1200, 0.0800]), tensor([0.2000, 0.0800, 0.0400, 0.0000, 0.4000, 0.0000, 0.0800, 0.1200, 0.0800]), -0.0056177366238373974, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2800, 0.1600, 0.0000, 0.0000, 0.0000, 0.1600, 0.1200, 0.0800, 0.2000]), tensor([0.2800, 0.1600, 0.0000, 0.0000, 0.0000, 0.1600, 0.1200, 0.0800, 0.2000]), -0.06852512903368244, tensor(0))\n",
      "action: 0\n",
      "Player 0 random action: 5\n",
      "Player 1 prediction: (tensor([0.0000, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.4400, 0.0800, 0.2400]), tensor([0.0000, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.4400, 0.0800, 0.2400]), -0.003297485697727937, tensor(6))\n",
      "action: 6\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2400, 0.7600]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2400, 0.7600]), 0.08947655716194557, tensor(8))\n",
      "action: 8\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 0, 5, 4, 3],\n",
      "        [2, 0, 5, 4, 3],\n",
      "        [2, 7, 3, 5, 0],\n",
      "        [7, 0, 5, 4, 3],\n",
      "        [7, 4, 5, 6, 0],\n",
      "        [3, 7, 0, 4, 3],\n",
      "        [1, 3, 8, 2, 6],\n",
      "        [4, 0, 6, 5, 2]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.2386],\n",
      "        [ 0.4166],\n",
      "        [-0.3029],\n",
      "        [ 0.4754],\n",
      "        [ 0.2475],\n",
      "        [-0.1614],\n",
      "        [ 0.1881],\n",
      "        [ 0.2778]], grad_fn=<AddmmBackward0>), tensor([[ 0.0330],\n",
      "        [ 0.0831],\n",
      "        [ 0.3789],\n",
      "        [-0.1280],\n",
      "        [-0.0871],\n",
      "        [ 0.3782],\n",
      "        [-0.3224],\n",
      "        [-0.4396]], grad_fn=<AddmmBackward0>), tensor([[ 0.0115],\n",
      "        [ 0.0736],\n",
      "        [ 0.1789],\n",
      "        [ 0.0762],\n",
      "        [ 0.0281],\n",
      "        [-0.0034],\n",
      "        [ 0.4554],\n",
      "        [ 0.3948]], grad_fn=<AddmmBackward0>), tensor([[-0.0333],\n",
      "        [-0.0780],\n",
      "        [ 0.3819],\n",
      "        [-0.0276],\n",
      "        [-0.1751],\n",
      "        [ 0.0104],\n",
      "        [-0.2513],\n",
      "        [-0.4273]], grad_fn=<AddmmBackward0>), tensor([[-0.0713],\n",
      "        [-0.0172],\n",
      "        [ 0.0060],\n",
      "        [ 0.0986],\n",
      "        [ 0.1607],\n",
      "        [-0.1543],\n",
      "        [ 0.4165],\n",
      "        [ 0.4915]], grad_fn=<AddmmBackward0>), tensor([[ 0.0824],\n",
      "        [ 0.0462],\n",
      "        [-0.0017],\n",
      "        [ 0.1031],\n",
      "        [-0.0539],\n",
      "        [ 0.2714],\n",
      "        [ 0.0277],\n",
      "        [-0.3322]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.3804],\n",
      "        [ 0.4927],\n",
      "        [ 0.1683],\n",
      "        [ 0.3756],\n",
      "        [-0.1272],\n",
      "        [ 0.3044],\n",
      "        [-0.0080],\n",
      "        [-0.1026]], grad_fn=<AddmmBackward0>), tensor([[-0.0567],\n",
      "        [ 0.0291],\n",
      "        [ 0.5092],\n",
      "        [ 0.0157],\n",
      "        [-0.0092],\n",
      "        [ 0.4949],\n",
      "        [ 0.0600],\n",
      "        [ 0.0044]], grad_fn=<AddmmBackward0>), tensor([[-0.0374],\n",
      "        [-0.0348],\n",
      "        [ 0.3067],\n",
      "        [-0.0912],\n",
      "        [-0.0621],\n",
      "        [-0.0091],\n",
      "        [ 0.2388],\n",
      "        [-0.0598]], grad_fn=<AddmmBackward0>), tensor([[ 0.0505],\n",
      "        [ 0.0358],\n",
      "        [ 0.4539],\n",
      "        [ 0.0217],\n",
      "        [ 0.1439],\n",
      "        [-0.0599],\n",
      "        [ 0.1771],\n",
      "        [ 0.0228]], grad_fn=<AddmmBackward0>), tensor([[ 0.0317],\n",
      "        [ 0.1970],\n",
      "        [-0.1016],\n",
      "        [ 0.1312],\n",
      "        [-0.1291],\n",
      "        [-0.1022],\n",
      "        [ 0.3470],\n",
      "        [ 0.0457]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[4.7333e-05, 9.9995e-01],\n",
      "        [5.8540e-05, 9.9994e-01],\n",
      "        [9.9999e-01, 1.0826e-05],\n",
      "        [2.1615e-05, 9.9998e-01],\n",
      "        [3.8710e-04, 9.9961e-01],\n",
      "        [1.0000e+00, 4.8541e-06],\n",
      "        [4.9488e-06, 9.9999e-01],\n",
      "        [9.0149e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 3.1027e-05],\n",
      "        [9.9997e-01, 2.9573e-05],\n",
      "        [2.1519e-05, 9.9998e-01],\n",
      "        [9.9998e-01, 1.6661e-05],\n",
      "        [9.9999e-01, 5.0910e-06],\n",
      "        [2.4278e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 2.2993e-06],\n",
      "        [9.9999e-01, 5.6798e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.5544e-03, 9.9845e-01],\n",
      "        [5.3897e-04, 9.9946e-01],\n",
      "        [1.0000e+00, 2.2242e-06],\n",
      "        [1.2501e-04, 9.9987e-01],\n",
      "        [1.0577e-04, 9.9989e-01],\n",
      "        [9.9998e-01, 2.3538e-05],\n",
      "        [4.0932e-05, 9.9996e-01],\n",
      "        [1.7396e-04, 9.9983e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9975e-01, 2.5160e-04],\n",
      "        [9.9993e-01, 6.9362e-05],\n",
      "        [4.2007e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 7.4108e-06],\n",
      "        [1.0000e+00, 2.9784e-06],\n",
      "        [4.2953e-03, 9.9570e-01],\n",
      "        [1.0000e+00, 3.1199e-07],\n",
      "        [9.9995e-01, 4.7334e-05]], grad_fn=<SoftmaxBackward0>), tensor([[3.2562e-04, 9.9967e-01],\n",
      "        [2.2303e-05, 9.9998e-01],\n",
      "        [9.9993e-01, 7.3517e-05],\n",
      "        [3.2620e-06, 1.0000e+00],\n",
      "        [5.7539e-03, 9.9425e-01],\n",
      "        [9.9997e-01, 3.4170e-05],\n",
      "        [7.2804e-05, 9.9993e-01],\n",
      "        [4.2071e-04, 9.9958e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 36.0 and average score: -0.14\n",
      "Results vs random: {'player_0_score': 0.68, 'player_0_win%': 0.82, 'player_1_score': -0.14, 'player_1_win%': 0.36, 'score': 0.27}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.8000, 0.1600, 0.0000, 0.0400, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.8000, 0.1600, 0.0000, 0.0400, 0.0000]), 0.3493435584868376, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0800, 0.3200, 0.0400, 0.0000, 0.1200, 0.2800, 0.0400, 0.1200]), tensor([0.0000, 0.0800, 0.3200, 0.0400, 0.0000, 0.1200, 0.2800, 0.0400, 0.1200]), 0.22219234987950096, tensor(2))\n",
      "action: 2\n",
      "Player 1 tictactoe_expert action: 6\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.2800, 0.0000, 0.2000, 0.0000, 0.1200, 0.0000, 0.1200, 0.2800]), tensor([0.0000, 0.2800, 0.0000, 0.2000, 0.0000, 0.1200, 0.0000, 0.1200, 0.2800]), 0.19806628960829514, tensor(1))\n",
      "action: 1\n",
      "Player 1 tictactoe_expert action: 3\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 3, 2, 4, 5],\n",
      "        [8, 3, 5, 0, 4],\n",
      "        [0, 0, 5, 3, 4],\n",
      "        [4, 8, 5, 0, 4],\n",
      "        [2, 0, 4, 3, 5],\n",
      "        [8, 0, 2, 5, 7],\n",
      "        [3, 5, 0, 3, 4],\n",
      "        [5, 4, 0, 3, 4]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.1673],\n",
      "        [ 0.0705],\n",
      "        [ 0.8896],\n",
      "        [ 0.0493],\n",
      "        [-0.2017],\n",
      "        [ 0.3854],\n",
      "        [ 0.2660],\n",
      "        [-0.2988]], grad_fn=<AddmmBackward0>), tensor([[ 0.1812],\n",
      "        [ 0.2892],\n",
      "        [-0.0084],\n",
      "        [ 0.1821],\n",
      "        [ 0.2996],\n",
      "        [-0.1464],\n",
      "        [ 0.3417],\n",
      "        [ 0.3129]], grad_fn=<AddmmBackward0>), tensor([[ 0.1056],\n",
      "        [ 0.2195],\n",
      "        [-0.0050],\n",
      "        [ 0.0740],\n",
      "        [-0.0905],\n",
      "        [ 0.1743],\n",
      "        [ 0.0895],\n",
      "        [-0.1182]], grad_fn=<AddmmBackward0>), tensor([[ 0.1580],\n",
      "        [ 0.3787],\n",
      "        [-0.0199],\n",
      "        [ 0.2723],\n",
      "        [ 0.2255],\n",
      "        [-0.0102],\n",
      "        [-0.0243],\n",
      "        [-0.0196]], grad_fn=<AddmmBackward0>), tensor([[ 0.2825],\n",
      "        [ 0.0543],\n",
      "        [ 0.1025],\n",
      "        [ 0.0103],\n",
      "        [ 0.0473],\n",
      "        [ 0.1067],\n",
      "        [ 0.0614],\n",
      "        [-0.0264]], grad_fn=<AddmmBackward0>), tensor([[ 0.3400],\n",
      "        [ 0.0617],\n",
      "        [-0.1379],\n",
      "        [ 0.0551],\n",
      "        [ 0.1558],\n",
      "        [ 0.1780],\n",
      "        [-0.1424],\n",
      "        [-0.0371]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.0082],\n",
      "        [0.1683],\n",
      "        [0.5854],\n",
      "        [0.2094],\n",
      "        [0.0179],\n",
      "        [0.0477],\n",
      "        [0.4401],\n",
      "        [0.0465]], grad_fn=<AddmmBackward0>), tensor([[0.0066],\n",
      "        [0.2330],\n",
      "        [0.0356],\n",
      "        [0.3619],\n",
      "        [0.1783],\n",
      "        [0.0325],\n",
      "        [0.3400],\n",
      "        [0.3574]], grad_fn=<AddmmBackward0>), tensor([[ 0.1748],\n",
      "        [ 0.3661],\n",
      "        [-0.0077],\n",
      "        [ 0.2965],\n",
      "        [ 0.0033],\n",
      "        [ 0.1563],\n",
      "        [-0.0031],\n",
      "        [ 0.0461]], grad_fn=<AddmmBackward0>), tensor([[ 0.3415],\n",
      "        [ 0.3712],\n",
      "        [-0.0138],\n",
      "        [ 0.1065],\n",
      "        [ 0.2446],\n",
      "        [ 0.1653],\n",
      "        [ 0.0173],\n",
      "        [ 0.0664]], grad_fn=<AddmmBackward0>), tensor([[0.4165],\n",
      "        [0.0579],\n",
      "        [0.1593],\n",
      "        [0.0241],\n",
      "        [0.2234],\n",
      "        [0.3054],\n",
      "        [0.0020],\n",
      "        [0.0276]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9999e-01, 9.1022e-06],\n",
      "        [9.9996e-01, 3.5867e-05],\n",
      "        [3.5398e-05, 9.9996e-01],\n",
      "        [9.9995e-01, 4.6939e-05],\n",
      "        [1.0000e+00, 1.2633e-06],\n",
      "        [8.6122e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 1.4868e-06],\n",
      "        [9.9999e-01, 6.2326e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.3342e-05, 9.9991e-01],\n",
      "        [3.3669e-05, 9.9997e-01],\n",
      "        [9.9986e-01, 1.4332e-04],\n",
      "        [3.7269e-04, 9.9963e-01],\n",
      "        [3.8040e-05, 9.9996e-01],\n",
      "        [9.9998e-01, 2.2205e-05],\n",
      "        [7.1076e-06, 9.9999e-01],\n",
      "        [2.9627e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 6.6186e-07],\n",
      "        [1.0000e+00, 1.3649e-06],\n",
      "        [1.3409e-01, 8.6591e-01],\n",
      "        [1.0000e+00, 2.4148e-06],\n",
      "        [1.0000e+00, 3.4399e-06],\n",
      "        [3.0075e-04, 9.9970e-01],\n",
      "        [9.9962e-01, 3.8040e-04],\n",
      "        [9.9979e-01, 2.0897e-04]], grad_fn=<SoftmaxBackward0>), tensor([[7.1281e-05, 9.9993e-01],\n",
      "        [1.5187e-06, 1.0000e+00],\n",
      "        [9.9994e-01, 5.6091e-05],\n",
      "        [1.0943e-03, 9.9891e-01],\n",
      "        [3.3997e-05, 9.9997e-01],\n",
      "        [1.0000e+00, 2.9295e-06],\n",
      "        [9.3259e-01, 6.7415e-02],\n",
      "        [4.0026e-01, 5.9974e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 2.3008e-07],\n",
      "        [1.0000e+00, 5.6579e-07],\n",
      "        [2.7078e-03, 9.9729e-01],\n",
      "        [9.9989e-01, 1.1273e-04],\n",
      "        [1.0000e+00, 3.6814e-07],\n",
      "        [1.2078e-04, 9.9988e-01],\n",
      "        [9.9934e-01, 6.5694e-04],\n",
      "        [9.8694e-01, 1.3065e-02]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 10.0 and average score: -0.62\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 7\n",
      "Player 1 prediction: (tensor([0.1600, 0.0400, 0.0400, 0.0800, 0.4400, 0.0000, 0.1600, 0.0000, 0.0800]), tensor([0.1600, 0.0400, 0.0400, 0.0800, 0.4400, 0.0000, 0.1600, 0.0000, 0.0800]), 0.084967930156451, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.1200, 0.1200, 0.0800, 0.0000, 0.0000, 0.0800, 0.4000, 0.0000, 0.2000]), tensor([0.1200, 0.1200, 0.0800, 0.0000, 0.0000, 0.0800, 0.4000, 0.0000, 0.2000]), 0.1433296732317943, tensor(6))\n",
      "action: 6\n",
      "Player 0 tictactoe_expert action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3200, 0.1200, 0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.4400]), tensor([0.3200, 0.1200, 0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.4400]), 0.1797118210591949, tensor(8))\n",
      "action: 8\n",
      "Player 0 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.5200, 0.0000, 0.0000, 0.0000, 0.4800, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.5200, 0.0000, 0.0000, 0.0000, 0.4800, 0.0000, 0.0000, 0.0000]), -0.2226534395550306, tensor(1))\n",
      "action: 1\n",
      "Player 0 tictactoe_expert action: 5\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 0, 6, 1, 8],\n",
      "        [8, 1, 3, 5, 7],\n",
      "        [7, 2, 8, 0, 8],\n",
      "        [3, 7, 0, 1, 8],\n",
      "        [4, 6, 0, 2, 8],\n",
      "        [0, 2, 3, 0, 8],\n",
      "        [2, 1, 7, 5, 0],\n",
      "        [8, 6, 1, 7, 2]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.6989],\n",
      "        [ 0.1715],\n",
      "        [ 0.5047],\n",
      "        [ 0.3930],\n",
      "        [ 0.2385],\n",
      "        [-0.3926],\n",
      "        [-0.2405],\n",
      "        [ 0.1872]], grad_fn=<AddmmBackward0>), tensor([[-0.2002],\n",
      "        [-0.0148],\n",
      "        [-0.2605],\n",
      "        [ 0.0721],\n",
      "        [-0.3850],\n",
      "        [ 0.3029],\n",
      "        [ 0.2744],\n",
      "        [-0.1972]], grad_fn=<AddmmBackward0>), tensor([[ 0.1929],\n",
      "        [ 0.1986],\n",
      "        [ 0.3258],\n",
      "        [ 0.2677],\n",
      "        [ 0.4042],\n",
      "        [-0.2511],\n",
      "        [ 0.0253],\n",
      "        [ 0.2681]], grad_fn=<AddmmBackward0>), tensor([[-0.1897],\n",
      "        [-0.0132],\n",
      "        [ 0.0388],\n",
      "        [ 0.0244],\n",
      "        [-0.4793],\n",
      "        [ 0.4048],\n",
      "        [ 0.2848],\n",
      "        [-0.0516]], grad_fn=<AddmmBackward0>), tensor([[ 0.2348],\n",
      "        [ 0.2866],\n",
      "        [-0.0287],\n",
      "        [ 0.0553],\n",
      "        [ 0.5683],\n",
      "        [-0.1670],\n",
      "        [ 0.0042],\n",
      "        [ 0.3518]], grad_fn=<AddmmBackward0>), tensor([[-0.1510],\n",
      "        [-0.0506],\n",
      "        [ 0.0676],\n",
      "        [ 0.0520],\n",
      "        [-0.0365],\n",
      "        [ 0.1742],\n",
      "        [-0.0087],\n",
      "        [ 0.1936]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.4718],\n",
      "        [ 0.1582],\n",
      "        [ 0.0546],\n",
      "        [ 0.3543],\n",
      "        [-0.1371],\n",
      "        [ 0.0215],\n",
      "        [ 0.2815],\n",
      "        [-0.0104]], grad_fn=<AddmmBackward0>), tensor([[-0.0092],\n",
      "        [ 0.2368],\n",
      "        [ 0.0973],\n",
      "        [ 0.3627],\n",
      "        [-0.0585],\n",
      "        [ 0.0960],\n",
      "        [ 0.2652],\n",
      "        [ 0.0387]], grad_fn=<AddmmBackward0>), tensor([[-0.0225],\n",
      "        [ 0.2813],\n",
      "        [ 0.5508],\n",
      "        [ 0.0150],\n",
      "        [ 0.0696],\n",
      "        [ 0.1027],\n",
      "        [ 0.3250],\n",
      "        [ 0.0718]], grad_fn=<AddmmBackward0>), tensor([[-0.0170],\n",
      "        [ 0.1800],\n",
      "        [-0.0309],\n",
      "        [ 0.0491],\n",
      "        [ 0.0451],\n",
      "        [-0.0200],\n",
      "        [ 0.3961],\n",
      "        [ 0.0833]], grad_fn=<AddmmBackward0>), tensor([[ 0.1618],\n",
      "        [ 0.4470],\n",
      "        [ 0.0166],\n",
      "        [ 0.0863],\n",
      "        [ 0.6247],\n",
      "        [ 0.0811],\n",
      "        [-0.0288],\n",
      "        [ 0.5391]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.5960e-04, 9.9984e-01],\n",
      "        [2.4150e-05, 9.9998e-01],\n",
      "        [6.2047e-06, 9.9999e-01],\n",
      "        [3.4251e-04, 9.9966e-01],\n",
      "        [6.4827e-05, 9.9994e-01],\n",
      "        [1.0000e+00, 3.7915e-06],\n",
      "        [1.0000e+00, 1.3758e-06],\n",
      "        [2.2525e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.8452e-01, 1.5482e-02],\n",
      "        [9.9998e-01, 1.5439e-05],\n",
      "        [9.9988e-01, 1.2345e-04],\n",
      "        [9.9998e-01, 1.9150e-05],\n",
      "        [1.0000e+00, 4.3859e-06],\n",
      "        [6.7160e-06, 9.9999e-01],\n",
      "        [7.1878e-06, 9.9999e-01],\n",
      "        [9.9992e-01, 8.3005e-05]], grad_fn=<SoftmaxBackward0>), tensor([[4.5348e-02, 9.5465e-01],\n",
      "        [2.8795e-05, 9.9997e-01],\n",
      "        [2.0636e-05, 9.9998e-01],\n",
      "        [1.1781e-01, 8.8219e-01],\n",
      "        [1.0494e-05, 9.9999e-01],\n",
      "        [9.9999e-01, 6.1401e-06],\n",
      "        [1.0000e+00, 4.3942e-06],\n",
      "        [1.2137e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.8741e-01, 8.1259e-01],\n",
      "        [1.0000e+00, 8.4367e-07],\n",
      "        [9.9790e-01, 2.1008e-03],\n",
      "        [8.4228e-01, 1.5772e-01],\n",
      "        [1.0000e+00, 3.9935e-06],\n",
      "        [1.2821e-04, 9.9987e-01],\n",
      "        [3.8093e-06, 1.0000e+00],\n",
      "        [9.9992e-01, 8.0428e-05]], grad_fn=<SoftmaxBackward0>), tensor([[7.7644e-01, 2.2356e-01],\n",
      "        [2.0884e-06, 1.0000e+00],\n",
      "        [9.5567e-02, 9.0443e-01],\n",
      "        [9.5461e-01, 4.5387e-02],\n",
      "        [7.1632e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 7.5618e-06],\n",
      "        [9.9858e-01, 1.4155e-03],\n",
      "        [2.1718e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 0.0 and average score: -0.92\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.62, 'player_0_win%': 0.1, 'player_1_score': -0.92, 'player_1_win%': 0.0, 'score': -0.77}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 2, 0, 4, 0],\n",
      "        [6, 7, 0, 3, 2],\n",
      "        [5, 2, 4, 3, 1],\n",
      "        [6, 4, 1, 8, 0],\n",
      "        [2, 3, 5, 0, 0],\n",
      "        [4, 2, 5, 7, 1],\n",
      "        [2, 0, 7, 4, 0],\n",
      "        [6, 8, 2, 1, 4]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[-0.4684],\n",
      "        [ 0.4850],\n",
      "        [ 0.3615],\n",
      "        [-0.4157],\n",
      "        [ 0.3227],\n",
      "        [-0.3492],\n",
      "        [ 0.2030],\n",
      "        [ 0.3615]], grad_fn=<AddmmBackward0>), tensor([[ 0.5705],\n",
      "        [-0.2901],\n",
      "        [-0.3029],\n",
      "        [ 0.4436],\n",
      "        [ 0.0080],\n",
      "        [ 0.2469],\n",
      "        [-0.0649],\n",
      "        [-0.3484]], grad_fn=<AddmmBackward0>), tensor([[-0.1630],\n",
      "        [ 0.6827],\n",
      "        [ 0.4579],\n",
      "        [-0.2654],\n",
      "        [ 0.2514],\n",
      "        [-0.2083],\n",
      "        [ 0.0593],\n",
      "        [ 0.3616]], grad_fn=<AddmmBackward0>), tensor([[ 0.0143],\n",
      "        [-0.0143],\n",
      "        [-0.5274],\n",
      "        [ 0.3591],\n",
      "        [ 0.0156],\n",
      "        [ 0.3891],\n",
      "        [ 0.0148],\n",
      "        [-0.4081]], grad_fn=<AddmmBackward0>), tensor([[-0.0680],\n",
      "        [ 0.2722],\n",
      "        [ 0.7117],\n",
      "        [-0.0985],\n",
      "        [ 0.0076],\n",
      "        [-0.1091],\n",
      "        [-0.0325],\n",
      "        [ 0.5289]], grad_fn=<AddmmBackward0>), tensor([[-0.0209],\n",
      "        [ 0.0939],\n",
      "        [-0.4094],\n",
      "        [ 0.0430],\n",
      "        [-0.0034],\n",
      "        [ 0.5613],\n",
      "        [-0.0055],\n",
      "        [-0.3095]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1089],\n",
      "        [ 0.2708],\n",
      "        [ 0.0217],\n",
      "        [ 0.0043],\n",
      "        [ 0.8348],\n",
      "        [ 0.0276],\n",
      "        [ 0.2021],\n",
      "        [-0.0032]], grad_fn=<AddmmBackward0>), tensor([[ 0.6901],\n",
      "        [ 0.1608],\n",
      "        [ 0.0010],\n",
      "        [ 0.2393],\n",
      "        [ 0.3565],\n",
      "        [-0.0151],\n",
      "        [-0.0406],\n",
      "        [-0.0573]], grad_fn=<AddmmBackward0>), tensor([[-0.0500],\n",
      "        [ 0.4115],\n",
      "        [ 0.0326],\n",
      "        [ 0.0461],\n",
      "        [ 0.6023],\n",
      "        [-0.0652],\n",
      "        [ 0.0655],\n",
      "        [-0.0486]], grad_fn=<AddmmBackward0>), tensor([[ 0.0158],\n",
      "        [ 0.3407],\n",
      "        [-0.0547],\n",
      "        [ 0.3708],\n",
      "        [-0.0523],\n",
      "        [ 0.1250],\n",
      "        [ 0.0480],\n",
      "        [-0.0013]], grad_fn=<AddmmBackward0>), tensor([[-0.0713],\n",
      "        [ 0.5293],\n",
      "        [ 0.1679],\n",
      "        [-0.0030],\n",
      "        [-0.0568],\n",
      "        [ 0.1291],\n",
      "        [-0.0297],\n",
      "        [ 0.4339]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9993e-01, 6.5325e-05],\n",
      "        [2.3348e-07, 1.0000e+00],\n",
      "        [3.8639e-07, 1.0000e+00],\n",
      "        [9.9987e-01, 1.2996e-04],\n",
      "        [1.6507e-04, 9.9983e-01],\n",
      "        [9.9967e-01, 3.3236e-04],\n",
      "        [1.5473e-06, 1.0000e+00],\n",
      "        [1.3717e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[1.1992e-04, 9.9988e-01],\n",
      "        [9.9993e-01, 7.2437e-05],\n",
      "        [1.0000e+00, 2.1455e-06],\n",
      "        [2.7299e-05, 9.9997e-01],\n",
      "        [9.9873e-01, 1.2651e-03],\n",
      "        [8.8880e-06, 9.9999e-01],\n",
      "        [9.9979e-01, 2.0631e-04],\n",
      "        [9.9996e-01, 4.3196e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9802e-01, 1.9822e-03],\n",
      "        [2.5293e-04, 9.9975e-01],\n",
      "        [1.3389e-06, 1.0000e+00],\n",
      "        [9.9971e-01, 2.9244e-04],\n",
      "        [4.1979e-04, 9.9958e-01],\n",
      "        [9.9948e-01, 5.2250e-04],\n",
      "        [7.5939e-03, 9.9241e-01],\n",
      "        [2.9089e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.3532e-02, 9.3647e-01],\n",
      "        [7.5773e-01, 2.4227e-01],\n",
      "        [9.9942e-01, 5.8309e-04],\n",
      "        [3.7302e-04, 9.9963e-01],\n",
      "        [9.9208e-01, 7.9184e-03],\n",
      "        [1.1376e-05, 9.9999e-01],\n",
      "        [5.2951e-01, 4.7049e-01],\n",
      "        [9.9999e-01, 7.7300e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.4956e-01, 5.0440e-02],\n",
      "        [2.3873e-01, 7.6127e-01],\n",
      "        [4.8457e-05, 9.9995e-01],\n",
      "        [9.9727e-01, 2.7312e-03],\n",
      "        [8.6466e-01, 1.3534e-01],\n",
      "        [9.9986e-01, 1.3552e-04],\n",
      "        [7.8811e-01, 2.1189e-01],\n",
      "        [3.2515e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 4, 0, 3, 2],\n",
      "        [0, 7, 8, 2, 1],\n",
      "        [2, 0, 8, 6, 3],\n",
      "        [5, 0, 0, 3, 2],\n",
      "        [4, 1, 0, 5, 6],\n",
      "        [4, 2, 5, 7, 0],\n",
      "        [7, 0, 0, 3, 2],\n",
      "        [8, 5, 3, 2, 6]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[-0.2055],\n",
      "        [-0.1429],\n",
      "        [ 0.3689],\n",
      "        [ 0.5120],\n",
      "        [ 0.2722],\n",
      "        [-0.1959],\n",
      "        [ 0.0094],\n",
      "        [ 0.1304]], grad_fn=<AddmmBackward0>), tensor([[ 0.2348],\n",
      "        [ 0.1159],\n",
      "        [-0.2358],\n",
      "        [-0.1136],\n",
      "        [-0.3298],\n",
      "        [ 0.2198],\n",
      "        [ 0.3598],\n",
      "        [-0.1596]], grad_fn=<AddmmBackward0>), tensor([[-0.0574],\n",
      "        [ 0.1880],\n",
      "        [ 0.1049],\n",
      "        [ 0.0097],\n",
      "        [ 0.4711],\n",
      "        [ 0.0940],\n",
      "        [ 0.0011],\n",
      "        [ 0.1833]], grad_fn=<AddmmBackward0>), tensor([[ 0.0407],\n",
      "        [-0.0246],\n",
      "        [ 0.1929],\n",
      "        [ 0.0014],\n",
      "        [-0.3468],\n",
      "        [ 0.3304],\n",
      "        [ 0.0183],\n",
      "        [-0.1662]], grad_fn=<AddmmBackward0>), tensor([[ 0.0012],\n",
      "        [ 0.1131],\n",
      "        [ 0.0802],\n",
      "        [-0.0468],\n",
      "        [ 0.5201],\n",
      "        [-0.0071],\n",
      "        [-0.0259],\n",
      "        [ 0.1676]], grad_fn=<AddmmBackward0>), tensor([[ 7.0678e-02],\n",
      "        [ 1.8274e-01],\n",
      "        [ 1.6578e-01],\n",
      "        [-6.1062e-02],\n",
      "        [-2.3125e-01],\n",
      "        [-1.9044e-03],\n",
      "        [-3.9633e-02],\n",
      "        [ 4.0505e-05]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1951],\n",
      "        [-0.0332],\n",
      "        [-0.0562],\n",
      "        [ 0.6161],\n",
      "        [-0.0453],\n",
      "        [ 0.2758],\n",
      "        [ 0.3054],\n",
      "        [ 0.0334]], grad_fn=<AddmmBackward0>), tensor([[ 0.5474],\n",
      "        [ 0.0052],\n",
      "        [ 0.0165],\n",
      "        [-0.0342],\n",
      "        [-0.1038],\n",
      "        [ 0.3478],\n",
      "        [ 0.0170],\n",
      "        [-0.0764]], grad_fn=<AddmmBackward0>), tensor([[ 0.0757],\n",
      "        [ 0.0076],\n",
      "        [ 0.1200],\n",
      "        [-0.0878],\n",
      "        [ 0.0621],\n",
      "        [ 0.3470],\n",
      "        [-0.0964],\n",
      "        [ 0.1960]], grad_fn=<AddmmBackward0>), tensor([[-0.0123],\n",
      "        [ 0.0690],\n",
      "        [ 0.3580],\n",
      "        [-0.0852],\n",
      "        [-0.0263],\n",
      "        [ 0.5704],\n",
      "        [-0.0596],\n",
      "        [ 0.1226]], grad_fn=<AddmmBackward0>), tensor([[ 0.0542],\n",
      "        [ 0.1120],\n",
      "        [ 0.2875],\n",
      "        [-0.0713],\n",
      "        [ 0.1271],\n",
      "        [-0.0781],\n",
      "        [-0.0807],\n",
      "        [ 0.3486]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9989e-01, 1.0943e-04],\n",
      "        [1.0000e+00, 4.3233e-06],\n",
      "        [9.6924e-01, 3.0761e-02],\n",
      "        [7.5270e-06, 9.9999e-01],\n",
      "        [4.4393e-07, 1.0000e+00],\n",
      "        [9.9997e-01, 2.6340e-05],\n",
      "        [9.9999e-01, 1.4456e-05],\n",
      "        [6.4140e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.4422e-04, 9.9986e-01],\n",
      "        [4.2094e-05, 9.9996e-01],\n",
      "        [3.2167e-03, 9.9678e-01],\n",
      "        [9.9999e-01, 1.4089e-05],\n",
      "        [1.0000e+00, 4.7816e-06],\n",
      "        [3.7948e-05, 9.9996e-01],\n",
      "        [3.3772e-03, 9.9662e-01],\n",
      "        [9.9999e-01, 9.0670e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9903e-01, 9.6731e-04],\n",
      "        [1.0000e+00, 4.0179e-06],\n",
      "        [9.9985e-01, 1.5040e-04],\n",
      "        [1.4116e-01, 8.5884e-01],\n",
      "        [2.8465e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 1.0450e-05],\n",
      "        [9.9897e-01, 1.0296e-03],\n",
      "        [1.5707e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.0806e-04, 9.9979e-01],\n",
      "        [7.6345e-05, 9.9992e-01],\n",
      "        [8.0274e-04, 9.9920e-01],\n",
      "        [9.9058e-01, 9.4186e-03],\n",
      "        [1.0000e+00, 3.5103e-06],\n",
      "        [2.6798e-06, 1.0000e+00],\n",
      "        [6.1065e-02, 9.3894e-01],\n",
      "        [9.9999e-01, 8.5254e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9988e-01, 1.1721e-04],\n",
      "        [1.0000e+00, 1.3645e-06],\n",
      "        [9.9994e-01, 6.0281e-05],\n",
      "        [1.9061e-02, 9.8094e-01],\n",
      "        [2.6237e-05, 9.9997e-01],\n",
      "        [9.9975e-01, 2.5069e-04],\n",
      "        [9.5261e-01, 4.7388e-02],\n",
      "        [4.1662e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 0, 2, 4, 3],\n",
      "        [2, 0, 2, 4, 3],\n",
      "        [3, 8, 2, 6, 7],\n",
      "        [0, 7, 5, 6, 2],\n",
      "        [0, 0, 2, 4, 3],\n",
      "        [8, 0, 2, 4, 3],\n",
      "        [3, 0, 2, 4, 3],\n",
      "        [7, 1, 8, 6, 2]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.3663],\n",
      "        [ 0.4469],\n",
      "        [ 0.0568],\n",
      "        [ 0.2381],\n",
      "        [ 0.0336],\n",
      "        [-0.1998],\n",
      "        [ 0.3790],\n",
      "        [-0.2289]], grad_fn=<AddmmBackward0>), tensor([[ 0.2962],\n",
      "        [ 0.0917],\n",
      "        [ 0.3631],\n",
      "        [-0.1304],\n",
      "        [-0.0403],\n",
      "        [ 0.3652],\n",
      "        [ 0.0450],\n",
      "        [ 0.5236]], grad_fn=<AddmmBackward0>), tensor([[ 0.1355],\n",
      "        [ 0.1579],\n",
      "        [-0.0026],\n",
      "        [ 0.4353],\n",
      "        [ 0.1534],\n",
      "        [ 0.0313],\n",
      "        [ 0.1993],\n",
      "        [-0.3226]], grad_fn=<AddmmBackward0>), tensor([[0.1562],\n",
      "        [0.0519],\n",
      "        [0.3274],\n",
      "        [0.0163],\n",
      "        [0.0233],\n",
      "        [0.1849],\n",
      "        [0.2120],\n",
      "        [0.4382]], grad_fn=<AddmmBackward0>), tensor([[ 0.0836],\n",
      "        [ 0.1181],\n",
      "        [ 0.0558],\n",
      "        [ 0.3182],\n",
      "        [ 0.1402],\n",
      "        [-0.0462],\n",
      "        [ 0.2717],\n",
      "        [-0.3348]], grad_fn=<AddmmBackward0>), tensor([[0.2600],\n",
      "        [0.1122],\n",
      "        [0.3559],\n",
      "        [0.1165],\n",
      "        [0.0787],\n",
      "        [0.3569],\n",
      "        [0.1813],\n",
      "        [0.4317]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.3842],\n",
      "        [ 0.3815],\n",
      "        [ 0.0228],\n",
      "        [-0.0391],\n",
      "        [ 0.0842],\n",
      "        [ 0.0884],\n",
      "        [ 0.1870],\n",
      "        [-0.0851]], grad_fn=<AddmmBackward0>), tensor([[ 0.2130],\n",
      "        [-0.0372],\n",
      "        [ 0.3361],\n",
      "        [-0.0500],\n",
      "        [ 0.0407],\n",
      "        [ 0.0400],\n",
      "        [ 0.0099],\n",
      "        [-0.0614]], grad_fn=<AddmmBackward0>), tensor([[ 0.0086],\n",
      "        [-0.0512],\n",
      "        [ 0.1326],\n",
      "        [ 0.2405],\n",
      "        [-0.0754],\n",
      "        [-0.0217],\n",
      "        [ 0.0497],\n",
      "        [-0.0212]], grad_fn=<AddmmBackward0>), tensor([[ 0.0086],\n",
      "        [-0.0777],\n",
      "        [ 0.4341],\n",
      "        [ 0.1504],\n",
      "        [-0.0240],\n",
      "        [ 0.0111],\n",
      "        [ 0.1312],\n",
      "        [ 0.1372]], grad_fn=<AddmmBackward0>), tensor([[-0.0101],\n",
      "        [ 0.0373],\n",
      "        [ 0.1327],\n",
      "        [ 0.2886],\n",
      "        [ 0.0584],\n",
      "        [ 0.0379],\n",
      "        [ 0.1092],\n",
      "        [ 0.0036]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0000e+00, 8.0419e-07],\n",
      "        [1.1645e-05, 9.9999e-01],\n",
      "        [9.9997e-01, 3.2539e-05],\n",
      "        [1.2885e-06, 1.0000e+00],\n",
      "        [3.3178e-05, 9.9997e-01],\n",
      "        [9.9999e-01, 7.4700e-06],\n",
      "        [3.5878e-06, 1.0000e+00],\n",
      "        [9.9997e-01, 2.8995e-05]], grad_fn=<SoftmaxBackward0>), tensor([[3.4802e-05, 9.9997e-01],\n",
      "        [9.9998e-01, 1.9674e-05],\n",
      "        [6.9306e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 5.8163e-06],\n",
      "        [9.9998e-01, 1.5401e-05],\n",
      "        [4.5396e-04, 9.9955e-01],\n",
      "        [9.9999e-01, 1.3672e-05],\n",
      "        [8.5068e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.4875e-06],\n",
      "        [2.8911e-03, 9.9711e-01],\n",
      "        [1.0000e+00, 4.4165e-06],\n",
      "        [1.6411e-06, 1.0000e+00],\n",
      "        [2.2446e-04, 9.9978e-01],\n",
      "        [1.0000e+00, 3.3044e-06],\n",
      "        [6.7586e-05, 9.9993e-01],\n",
      "        [9.9999e-01, 8.3469e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.2874e-05, 9.9999e-01],\n",
      "        [9.9997e-01, 2.5282e-05],\n",
      "        [8.2364e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 4.2099e-07],\n",
      "        [9.9993e-01, 6.8172e-05],\n",
      "        [6.6514e-06, 9.9999e-01],\n",
      "        [9.9970e-01, 2.9562e-04],\n",
      "        [1.2406e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 7.2795e-06],\n",
      "        [4.8450e-05, 9.9995e-01],\n",
      "        [1.0000e+00, 4.4446e-06],\n",
      "        [1.8199e-06, 1.0000e+00],\n",
      "        [1.4262e-05, 9.9999e-01],\n",
      "        [9.9998e-01, 2.0587e-05],\n",
      "        [3.7166e-05, 9.9996e-01],\n",
      "        [9.9999e-01, 6.6096e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 6, 2, 3, 0],\n",
      "        [8, 0, 3, 6, 3],\n",
      "        [6, 5, 8, 1, 4],\n",
      "        [0, 3, 8, 4, 0],\n",
      "        [1, 3, 0, 6, 3],\n",
      "        [6, 0, 3, 6, 3],\n",
      "        [8, 1, 0, 0, 3],\n",
      "        [7, 5, 0, 2, 6]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[-0.0567],\n",
      "        [-0.0049],\n",
      "        [ 0.1847],\n",
      "        [ 0.3414],\n",
      "        [-0.0236],\n",
      "        [ 0.3257],\n",
      "        [ 0.2888],\n",
      "        [-0.5429]], grad_fn=<AddmmBackward0>), tensor([[ 0.0648],\n",
      "        [-0.0887],\n",
      "        [-0.3776],\n",
      "        [-0.3562],\n",
      "        [ 0.5433],\n",
      "        [ 0.0704],\n",
      "        [-0.2565],\n",
      "        [ 0.4339]], grad_fn=<AddmmBackward0>), tensor([[-0.0659],\n",
      "        [ 0.1452],\n",
      "        [ 0.3173],\n",
      "        [ 0.3841],\n",
      "        [-0.0720],\n",
      "        [-0.0028],\n",
      "        [ 0.3001],\n",
      "        [-0.2919]], grad_fn=<AddmmBackward0>), tensor([[ 0.1858],\n",
      "        [ 0.0878],\n",
      "        [-0.4103],\n",
      "        [-0.2151],\n",
      "        [-0.0473],\n",
      "        [-0.0341],\n",
      "        [-0.2185],\n",
      "        [ 0.5250]], grad_fn=<AddmmBackward0>), tensor([[ 0.2375],\n",
      "        [ 0.1679],\n",
      "        [ 0.4389],\n",
      "        [ 0.3680],\n",
      "        [-0.1216],\n",
      "        [-0.0185],\n",
      "        [ 0.0667],\n",
      "        [ 0.0546]], grad_fn=<AddmmBackward0>), tensor([[-0.1096],\n",
      "        [-0.0578],\n",
      "        [-0.4263],\n",
      "        [-0.2030],\n",
      "        [ 0.1711],\n",
      "        [ 0.0184],\n",
      "        [-0.2399],\n",
      "        [ 0.4261]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0260],\n",
      "        [ 0.1274],\n",
      "        [ 0.0679],\n",
      "        [ 0.0654],\n",
      "        [ 0.2460],\n",
      "        [ 0.4703],\n",
      "        [-0.0086],\n",
      "        [ 0.0353]], grad_fn=<AddmmBackward0>), tensor([[ 0.1100],\n",
      "        [ 0.0559],\n",
      "        [ 0.0363],\n",
      "        [ 0.0753],\n",
      "        [ 0.4408],\n",
      "        [-0.0036],\n",
      "        [ 0.0347],\n",
      "        [ 0.0901]], grad_fn=<AddmmBackward0>), tensor([[ 0.0161],\n",
      "        [ 0.2838],\n",
      "        [-0.0125],\n",
      "        [ 0.1366],\n",
      "        [ 0.0098],\n",
      "        [-0.0272],\n",
      "        [ 0.1791],\n",
      "        [ 0.1278]], grad_fn=<AddmmBackward0>), tensor([[ 0.2200],\n",
      "        [ 0.1617],\n",
      "        [-0.0642],\n",
      "        [ 0.0652],\n",
      "        [-0.0746],\n",
      "        [ 0.0104],\n",
      "        [ 0.0439],\n",
      "        [ 0.2552]], grad_fn=<AddmmBackward0>), tensor([[-0.0982],\n",
      "        [ 0.1927],\n",
      "        [ 0.2396],\n",
      "        [-0.0079],\n",
      "        [-0.0142],\n",
      "        [-0.0212],\n",
      "        [-0.0453],\n",
      "        [ 0.2909]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[8.8320e-05, 9.9991e-01],\n",
      "        [2.2627e-06, 1.0000e+00],\n",
      "        [2.4445e-06, 1.0000e+00],\n",
      "        [1.5489e-04, 9.9985e-01],\n",
      "        [1.0000e+00, 8.5634e-07],\n",
      "        [5.3785e-04, 9.9946e-01],\n",
      "        [1.5052e-05, 9.9998e-01],\n",
      "        [9.9998e-01, 1.8874e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.7221e-05],\n",
      "        [9.9999e-01, 6.4079e-06],\n",
      "        [9.9997e-01, 2.6726e-05],\n",
      "        [9.9999e-01, 7.1842e-06],\n",
      "        [1.9214e-06, 1.0000e+00],\n",
      "        [9.9908e-01, 9.2061e-04],\n",
      "        [9.9997e-01, 3.3409e-05],\n",
      "        [6.9335e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[7.1547e-05, 9.9993e-01],\n",
      "        [2.7178e-05, 9.9997e-01],\n",
      "        [9.4470e-06, 9.9999e-01],\n",
      "        [1.1514e-05, 9.9999e-01],\n",
      "        [9.9907e-01, 9.2653e-04],\n",
      "        [3.5915e-01, 6.4085e-01],\n",
      "        [1.9775e-05, 9.9998e-01],\n",
      "        [9.9999e-01, 5.2187e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 3.8345e-07],\n",
      "        [9.9999e-01, 1.1941e-05],\n",
      "        [9.9997e-01, 2.7457e-05],\n",
      "        [1.0000e+00, 2.5626e-06],\n",
      "        [2.5444e-02, 9.7456e-01],\n",
      "        [9.9710e-01, 2.8973e-03],\n",
      "        [9.9982e-01, 1.7582e-04],\n",
      "        [9.9789e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.6540e-03, 9.9835e-01],\n",
      "        [1.9927e-05, 9.9998e-01],\n",
      "        [2.4035e-06, 1.0000e+00],\n",
      "        [2.9945e-05, 9.9997e-01],\n",
      "        [9.9983e-01, 1.6566e-04],\n",
      "        [5.6806e-01, 4.3194e-01],\n",
      "        [2.5505e-04, 9.9974e-01],\n",
      "        [1.0000e+00, 3.4316e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: 0.3\n",
      "Test score {'score': 0.3, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 2, 6, 3, 0],\n",
      "        [8, 7, 0, 2, 0],\n",
      "        [8, 0, 6, 7, 5],\n",
      "        [3, 1, 4, 0, 6],\n",
      "        [6, 1, 5, 4, 2],\n",
      "        [7, 0, 6, 7, 5],\n",
      "        [5, 2, 1, 7, 3],\n",
      "        [3, 7, 8, 4, 0]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.1534],\n",
      "        [-0.0091],\n",
      "        [ 0.5130],\n",
      "        [-0.3329],\n",
      "        [ 0.3293],\n",
      "        [ 0.5245],\n",
      "        [ 0.4225],\n",
      "        [-0.1390]], grad_fn=<AddmmBackward0>), tensor([[ 0.1878],\n",
      "        [ 0.0077],\n",
      "        [ 0.2442],\n",
      "        [ 0.3434],\n",
      "        [-0.2668],\n",
      "        [ 0.1754],\n",
      "        [-0.0989],\n",
      "        [ 0.2357]], grad_fn=<AddmmBackward0>), tensor([[ 0.0463],\n",
      "        [ 0.2042],\n",
      "        [ 0.0742],\n",
      "        [-0.2144],\n",
      "        [ 0.3458],\n",
      "        [ 0.0080],\n",
      "        [ 0.3320],\n",
      "        [ 0.0582]], grad_fn=<AddmmBackward0>), tensor([[ 0.1801],\n",
      "        [ 0.0541],\n",
      "        [-0.0146],\n",
      "        [ 0.1954],\n",
      "        [-0.1224],\n",
      "        [-0.0390],\n",
      "        [ 0.0898],\n",
      "        [ 0.1870]], grad_fn=<AddmmBackward0>), tensor([[-0.0159],\n",
      "        [ 0.2385],\n",
      "        [-0.0133],\n",
      "        [-0.0857],\n",
      "        [ 0.3635],\n",
      "        [ 0.0668],\n",
      "        [ 0.3751],\n",
      "        [ 0.1899]], grad_fn=<AddmmBackward0>), tensor([[ 0.2180],\n",
      "        [ 0.0091],\n",
      "        [ 0.0841],\n",
      "        [ 0.2716],\n",
      "        [ 0.1221],\n",
      "        [ 0.0211],\n",
      "        [-0.0337],\n",
      "        [ 0.0640]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0668],\n",
      "        [-0.2538],\n",
      "        [ 0.5187],\n",
      "        [-0.0407],\n",
      "        [ 0.0248],\n",
      "        [ 0.4109],\n",
      "        [-0.0258],\n",
      "        [-0.0709]], grad_fn=<AddmmBackward0>), tensor([[ 0.0894],\n",
      "        [ 0.0904],\n",
      "        [-0.0467],\n",
      "        [-0.1204],\n",
      "        [-0.0780],\n",
      "        [-0.0708],\n",
      "        [ 0.1718],\n",
      "        [-0.0501]], grad_fn=<AddmmBackward0>), tensor([[ 0.1272],\n",
      "        [ 0.1775],\n",
      "        [-0.0834],\n",
      "        [-0.1174],\n",
      "        [ 0.0422],\n",
      "        [-0.0643],\n",
      "        [ 0.2410],\n",
      "        [-0.0510]], grad_fn=<AddmmBackward0>), tensor([[ 0.0971],\n",
      "        [ 0.2084],\n",
      "        [-0.0639],\n",
      "        [ 0.1228],\n",
      "        [ 0.1146],\n",
      "        [-0.1033],\n",
      "        [ 0.2322],\n",
      "        [ 0.1309]], grad_fn=<AddmmBackward0>), tensor([[ 0.1178],\n",
      "        [-0.0315],\n",
      "        [-0.0486],\n",
      "        [ 0.0633],\n",
      "        [ 0.3978],\n",
      "        [-0.1096],\n",
      "        [ 0.4438],\n",
      "        [-0.0604]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9999e-01, 6.7428e-06],\n",
      "        [1.2428e-05, 9.9999e-01],\n",
      "        [3.2921e-05, 9.9997e-01],\n",
      "        [9.9999e-01, 5.4334e-06],\n",
      "        [1.2827e-05, 9.9999e-01],\n",
      "        [7.6202e-06, 9.9999e-01],\n",
      "        [1.1687e-05, 9.9999e-01],\n",
      "        [9.9999e-01, 5.5406e-06]], grad_fn=<SoftmaxBackward0>), tensor([[5.1594e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 1.3968e-05],\n",
      "        [9.9666e-01, 3.3353e-03],\n",
      "        [9.6759e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 3.7084e-06],\n",
      "        [9.9765e-01, 2.3475e-03],\n",
      "        [9.9999e-01, 1.0442e-05],\n",
      "        [2.1387e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 2.2163e-06],\n",
      "        [1.9726e-06, 1.0000e+00],\n",
      "        [2.7817e-01, 7.2183e-01],\n",
      "        [9.9999e-01, 6.1357e-06],\n",
      "        [1.8359e-06, 1.0000e+00],\n",
      "        [2.7738e-02, 9.7226e-01],\n",
      "        [4.5049e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 8.9699e-06]], grad_fn=<SoftmaxBackward0>), tensor([[3.2468e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 5.3083e-07],\n",
      "        [9.0526e-01, 9.4741e-02],\n",
      "        [1.7716e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 3.0398e-06],\n",
      "        [8.9653e-01, 1.0347e-01],\n",
      "        [1.0000e+00, 1.8016e-06],\n",
      "        [1.4651e-04, 9.9985e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 7.1457e-07],\n",
      "        [3.1093e-04, 9.9969e-01],\n",
      "        [8.2321e-01, 1.7679e-01],\n",
      "        [1.0000e+00, 8.6961e-07],\n",
      "        [4.4668e-06, 1.0000e+00],\n",
      "        [3.9257e-01, 6.0743e-01],\n",
      "        [4.5872e-07, 1.0000e+00],\n",
      "        [9.9089e-01, 9.1074e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 7, 3, 0, 5],\n",
      "        [2, 8, 0, 4, 1],\n",
      "        [4, 1, 6, 8, 5],\n",
      "        [0, 2, 4, 0, 0],\n",
      "        [5, 0, 6, 4, 0],\n",
      "        [0, 3, 7, 0, 0],\n",
      "        [4, 0, 5, 8, 2],\n",
      "        [8, 6, 0, 4, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.3756],\n",
      "        [-0.1916],\n",
      "        [ 0.3263],\n",
      "        [ 0.5077],\n",
      "        [ 0.3582],\n",
      "        [ 0.1189],\n",
      "        [ 0.3263],\n",
      "        [-0.1792]], grad_fn=<AddmmBackward0>), tensor([[ 0.4006],\n",
      "        [ 0.2121],\n",
      "        [-0.3305],\n",
      "        [-0.2032],\n",
      "        [ 0.3535],\n",
      "        [ 0.1202],\n",
      "        [-0.3305],\n",
      "        [ 0.2927]], grad_fn=<AddmmBackward0>), tensor([[-0.2883],\n",
      "        [-0.0829],\n",
      "        [ 0.4886],\n",
      "        [ 0.2914],\n",
      "        [ 0.0817],\n",
      "        [ 0.0407],\n",
      "        [ 0.2974],\n",
      "        [ 0.0523]], grad_fn=<AddmmBackward0>), tensor([[ 0.3432],\n",
      "        [ 0.0116],\n",
      "        [-0.2948],\n",
      "        [-0.0289],\n",
      "        [-0.0343],\n",
      "        [ 0.1725],\n",
      "        [-0.2501],\n",
      "        [ 0.0631]], grad_fn=<AddmmBackward0>), tensor([[-0.1979],\n",
      "        [-0.0294],\n",
      "        [ 0.3427],\n",
      "        [ 0.0286],\n",
      "        [-0.1309],\n",
      "        [ 0.0458],\n",
      "        [ 0.3224],\n",
      "        [-0.0668]], grad_fn=<AddmmBackward0>), tensor([[ 0.2455],\n",
      "        [ 0.1632],\n",
      "        [-0.1343],\n",
      "        [-0.0040],\n",
      "        [-0.0022],\n",
      "        [ 0.0006],\n",
      "        [-0.0833],\n",
      "        [-0.0234]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0053],\n",
      "        [ 0.0623],\n",
      "        [-0.0062],\n",
      "        [ 0.0922],\n",
      "        [ 0.4112],\n",
      "        [ 0.1284],\n",
      "        [-0.0062],\n",
      "        [ 0.0882]], grad_fn=<AddmmBackward0>), tensor([[-0.0346],\n",
      "        [ 0.0122],\n",
      "        [-0.0387],\n",
      "        [-0.0597],\n",
      "        [ 0.3082],\n",
      "        [ 0.2071],\n",
      "        [ 0.0376],\n",
      "        [ 0.4362]], grad_fn=<AddmmBackward0>), tensor([[-0.0686],\n",
      "        [-0.0421],\n",
      "        [ 0.0489],\n",
      "        [ 0.2629],\n",
      "        [ 0.0654],\n",
      "        [ 0.3881],\n",
      "        [ 0.0453],\n",
      "        [-0.0631]], grad_fn=<AddmmBackward0>), tensor([[ 0.0937],\n",
      "        [ 0.2432],\n",
      "        [-0.0731],\n",
      "        [-0.0301],\n",
      "        [ 0.1407],\n",
      "        [-0.0264],\n",
      "        [ 0.0340],\n",
      "        [ 0.0821]], grad_fn=<AddmmBackward0>), tensor([[ 0.0677],\n",
      "        [ 0.1992],\n",
      "        [ 0.1624],\n",
      "        [-0.0581],\n",
      "        [ 0.0027],\n",
      "        [-0.0433],\n",
      "        [ 0.1748],\n",
      "        [-0.0447]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 1.5835e-05],\n",
      "        [9.9999e-01, 1.2947e-05],\n",
      "        [1.8324e-06, 1.0000e+00],\n",
      "        [1.2210e-05, 9.9999e-01],\n",
      "        [9.9989e-01, 1.0716e-04],\n",
      "        [9.9983e-01, 1.6833e-04],\n",
      "        [1.8324e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 3.2148e-06]], grad_fn=<SoftmaxBackward0>), tensor([[3.5903e-05, 9.9996e-01],\n",
      "        [9.5454e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 5.6280e-06],\n",
      "        [9.9998e-01, 1.6152e-05],\n",
      "        [3.2917e-05, 9.9997e-01],\n",
      "        [6.4387e-05, 9.9994e-01],\n",
      "        [9.9998e-01, 2.1079e-05],\n",
      "        [1.5309e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 9.0786e-07],\n",
      "        [9.9969e-01, 3.0854e-04],\n",
      "        [1.2941e-06, 1.0000e+00],\n",
      "        [1.6398e-05, 9.9998e-01],\n",
      "        [9.6158e-01, 3.8418e-02],\n",
      "        [9.9964e-01, 3.6369e-04],\n",
      "        [2.9250e-05, 9.9997e-01],\n",
      "        [9.9888e-01, 1.1200e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.3157e-06, 9.9999e-01],\n",
      "        [2.8447e-05, 9.9997e-01],\n",
      "        [9.9999e-01, 8.2430e-06],\n",
      "        [9.9942e-01, 5.7511e-04],\n",
      "        [5.4475e-05, 9.9995e-01],\n",
      "        [5.8350e-03, 9.9417e-01],\n",
      "        [9.9999e-01, 8.6008e-06],\n",
      "        [4.4985e-03, 9.9550e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 6.7580e-07],\n",
      "        [9.9988e-01, 1.2273e-04],\n",
      "        [7.0404e-06, 9.9999e-01],\n",
      "        [7.7157e-02, 9.2284e-01],\n",
      "        [9.9882e-01, 1.1764e-03],\n",
      "        [8.9942e-01, 1.0058e-01],\n",
      "        [4.2085e-05, 9.9996e-01],\n",
      "        [9.8940e-01, 1.0600e-02]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0400, 0.0400, 0.0000, 0.0000, 0.6000, 0.0000, 0.1200, 0.2000, 0.0000]), tensor([0.0400, 0.0400, 0.0000, 0.0000, 0.6000, 0.0000, 0.1200, 0.2000, 0.0000]), 0.31707615677553874, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 5\n",
      "Player 0 prediction: (tensor([0.1200, 0.0400, 0.0800, 0.0400, 0.0000, 0.0000, 0.0800, 0.0800, 0.5600]), tensor([0.1200, 0.0400, 0.0800, 0.0400, 0.0000, 0.0000, 0.0800, 0.0800, 0.5600]), 0.45683122146874666, tensor(8))\n",
      "action: 8\n",
      "Player 1 random action: 0\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0800, 0.3200, 0.0800, 0.0000, 0.0000, 0.3600, 0.1600, 0.0000]), tensor([0.0000, 0.0800, 0.3200, 0.0800, 0.0000, 0.0000, 0.3600, 0.1600, 0.0000]), 0.28764255667248595, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 3\n",
      "Player 0 prediction: (tensor([0.0000, 0.4400, 0.3600, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000]), tensor([0.0000, 0.4400, 0.3600, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000]), 0.367149357110835, tensor(1))\n",
      "action: 1\n",
      "Player 1 random action: 2\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), 0.20280561617647225, tensor(7))\n",
      "action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 0, 1, 3, 4],\n",
      "        [1, 5, 3, 6, 7],\n",
      "        [8, 0, 4, 4, 7],\n",
      "        [8, 4, 2, 6, 1],\n",
      "        [6, 0, 5, 3, 8],\n",
      "        [0, 1, 3, 0, 7],\n",
      "        [6, 4, 2, 8, 0],\n",
      "        [2, 1, 3, 7, 0]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2370],\n",
      "        [-0.0048],\n",
      "        [ 0.5790],\n",
      "        [ 0.1829],\n",
      "        [-0.4835],\n",
      "        [ 0.1557],\n",
      "        [-0.1428],\n",
      "        [ 0.0102]], grad_fn=<AddmmBackward0>), tensor([[ 0.2522],\n",
      "        [ 0.0380],\n",
      "        [ 0.1153],\n",
      "        [-0.2161],\n",
      "        [ 0.3219],\n",
      "        [ 0.0491],\n",
      "        [ 0.2163],\n",
      "        [ 0.1882]], grad_fn=<AddmmBackward0>), tensor([[-0.1300],\n",
      "        [ 0.0664],\n",
      "        [ 0.0847],\n",
      "        [-0.1712],\n",
      "        [-0.4618],\n",
      "        [ 0.1797],\n",
      "        [-0.2857],\n",
      "        [ 0.1337]], grad_fn=<AddmmBackward0>), tensor([[ 0.2272],\n",
      "        [ 0.3182],\n",
      "        [-0.0277],\n",
      "        [ 0.0471],\n",
      "        [ 0.3676],\n",
      "        [ 0.0338],\n",
      "        [ 0.2224],\n",
      "        [ 0.3642]], grad_fn=<AddmmBackward0>), tensor([[ 0.0126],\n",
      "        [ 0.2985],\n",
      "        [ 0.0435],\n",
      "        [-0.1482],\n",
      "        [-0.1828],\n",
      "        [-0.0779],\n",
      "        [-0.0200],\n",
      "        [ 0.0330]], grad_fn=<AddmmBackward0>), tensor([[ 0.1791],\n",
      "        [ 0.1987],\n",
      "        [ 0.0609],\n",
      "        [ 0.1844],\n",
      "        [ 0.2101],\n",
      "        [-0.0282],\n",
      "        [ 0.0145],\n",
      "        [-0.0161]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1416],\n",
      "        [ 0.2759],\n",
      "        [ 0.7333],\n",
      "        [-0.0475],\n",
      "        [-0.0043],\n",
      "        [ 0.1972],\n",
      "        [ 0.0219],\n",
      "        [ 0.3550]], grad_fn=<AddmmBackward0>), tensor([[-0.0163],\n",
      "        [ 0.1575],\n",
      "        [-0.0711],\n",
      "        [-0.0479],\n",
      "        [-0.0282],\n",
      "        [ 0.4092],\n",
      "        [-0.0574],\n",
      "        [ 0.3042]], grad_fn=<AddmmBackward0>), tensor([[ 0.0518],\n",
      "        [ 0.1496],\n",
      "        [ 0.1210],\n",
      "        [-0.0290],\n",
      "        [ 0.0728],\n",
      "        [ 0.2805],\n",
      "        [-0.0129],\n",
      "        [ 0.5847]], grad_fn=<AddmmBackward0>), tensor([[ 0.1669],\n",
      "        [ 0.5560],\n",
      "        [ 0.0081],\n",
      "        [-0.0408],\n",
      "        [ 0.2102],\n",
      "        [-0.1092],\n",
      "        [ 0.2150],\n",
      "        [ 0.4740]], grad_fn=<AddmmBackward0>), tensor([[ 0.4492],\n",
      "        [ 0.3771],\n",
      "        [ 0.0117],\n",
      "        [ 0.1107],\n",
      "        [ 0.2829],\n",
      "        [-0.0721],\n",
      "        [-0.0162],\n",
      "        [-0.1002]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0000e+00, 4.5078e-06],\n",
      "        [5.2343e-06, 9.9999e-01],\n",
      "        [1.3508e-04, 9.9986e-01],\n",
      "        [2.4956e-06, 1.0000e+00],\n",
      "        [9.9997e-01, 2.7954e-05],\n",
      "        [4.0042e-04, 9.9960e-01],\n",
      "        [9.9998e-01, 1.8442e-05],\n",
      "        [9.9985e-01, 1.4591e-04]], grad_fn=<SoftmaxBackward0>), tensor([[5.8769e-04, 9.9941e-01],\n",
      "        [9.9992e-01, 8.3797e-05],\n",
      "        [9.9940e-01, 6.0039e-04],\n",
      "        [9.9998e-01, 1.6151e-05],\n",
      "        [5.1785e-06, 9.9999e-01],\n",
      "        [9.9320e-01, 6.8033e-03],\n",
      "        [9.7695e-06, 9.9999e-01],\n",
      "        [2.8434e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9968e-01, 3.1690e-04],\n",
      "        [7.2714e-05, 9.9993e-01],\n",
      "        [4.1637e-04, 9.9958e-01],\n",
      "        [4.0788e-05, 9.9996e-01],\n",
      "        [9.9999e-01, 6.7760e-06],\n",
      "        [2.7210e-04, 9.9973e-01],\n",
      "        [9.9999e-01, 9.3060e-06],\n",
      "        [9.9999e-01, 6.1063e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.7488e-03, 9.9825e-01],\n",
      "        [9.9999e-01, 1.2614e-05],\n",
      "        [9.9923e-01, 7.7052e-04],\n",
      "        [9.9995e-01, 4.6759e-05],\n",
      "        [1.1186e-05, 9.9999e-01],\n",
      "        [9.9363e-01, 6.3657e-03],\n",
      "        [1.8515e-05, 9.9998e-01],\n",
      "        [2.7029e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 3.4182e-05],\n",
      "        [2.6599e-06, 1.0000e+00],\n",
      "        [1.6932e-05, 9.9998e-01],\n",
      "        [1.1879e-04, 9.9988e-01],\n",
      "        [9.9999e-01, 6.8362e-06],\n",
      "        [8.4838e-02, 9.1516e-01],\n",
      "        [9.9975e-01, 2.5485e-04],\n",
      "        [9.9945e-01, 5.4696e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 80.0 and average score: 0.64\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0400, 0.0400, 0.0400, 0.0000, 0.5600, 0.1200, 0.1200, 0.0800, 0.0000]), tensor([0.0400, 0.0400, 0.0400, 0.0000, 0.5600, 0.1200, 0.1200, 0.0800, 0.0000]), 0.1982776947462788, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 0\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.1200, 0.1600, 0.0000, 0.0000, 0.2800, 0.2400, 0.0800, 0.1200]), tensor([0.0000, 0.1200, 0.1600, 0.0000, 0.0000, 0.2800, 0.2400, 0.0800, 0.1200]), 0.04245430367210737, tensor(5))\n",
      "action: 5\n",
      "Player 0 random action: 7\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0800, 0.1600, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 0.6000]), tensor([0.0000, 0.0800, 0.1600, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 0.6000]), 0.20731449857927287, tensor(8))\n",
      "action: 8\n",
      "Player 0 random action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.7600, 0.0000, 0.0000]), tensor([0.0000, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.7600, 0.0000, 0.0000]), 0.099981947730367, tensor(6))\n",
      "action: 6\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 0, 0, 1, 4],\n",
      "        [0, 2, 1, 7, 4],\n",
      "        [8, 1, 5, 2, 0],\n",
      "        [5, 0, 4, 1, 4],\n",
      "        [0, 7, 8, 5, 1],\n",
      "        [3, 0, 4, 1, 4],\n",
      "        [7, 0, 4, 1, 4],\n",
      "        [2, 8, 7, 0, 0]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.4633],\n",
      "        [ 0.2710],\n",
      "        [ 0.0743],\n",
      "        [ 0.4393],\n",
      "        [-0.1166],\n",
      "        [ 0.4902],\n",
      "        [ 0.1691],\n",
      "        [-0.3068]], grad_fn=<AddmmBackward0>), tensor([[ 0.0595],\n",
      "        [-0.3000],\n",
      "        [-0.0119],\n",
      "        [-0.0458],\n",
      "        [ 0.2558],\n",
      "        [ 0.0114],\n",
      "        [ 0.1580],\n",
      "        [ 0.4064]], grad_fn=<AddmmBackward0>), tensor([[ 0.2436],\n",
      "        [ 0.3776],\n",
      "        [ 0.2268],\n",
      "        [ 0.0080],\n",
      "        [-0.0477],\n",
      "        [-0.0364],\n",
      "        [ 0.0708],\n",
      "        [-0.1183]], grad_fn=<AddmmBackward0>), tensor([[ 0.0318],\n",
      "        [-0.0912],\n",
      "        [ 0.0438],\n",
      "        [-0.1002],\n",
      "        [ 0.2605],\n",
      "        [-0.0706],\n",
      "        [ 0.0696],\n",
      "        [ 0.2983]], grad_fn=<AddmmBackward0>), tensor([[ 0.1710],\n",
      "        [ 0.3384],\n",
      "        [ 0.3541],\n",
      "        [ 0.0524],\n",
      "        [ 0.0194],\n",
      "        [-0.0291],\n",
      "        [ 0.0433],\n",
      "        [-0.1177]], grad_fn=<AddmmBackward0>), tensor([[-0.1188],\n",
      "        [ 0.0824],\n",
      "        [-0.0249],\n",
      "        [-0.1918],\n",
      "        [ 0.3567],\n",
      "        [-0.1775],\n",
      "        [ 0.0099],\n",
      "        [ 0.1085]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0075],\n",
      "        [-0.0638],\n",
      "        [ 0.1689],\n",
      "        [ 0.5683],\n",
      "        [-0.0095],\n",
      "        [ 0.6249],\n",
      "        [ 0.2866],\n",
      "        [-0.0222]], grad_fn=<AddmmBackward0>), tensor([[ 0.0453],\n",
      "        [-0.0600],\n",
      "        [ 0.1656],\n",
      "        [-0.0422],\n",
      "        [ 0.0241],\n",
      "        [ 0.0170],\n",
      "        [ 0.0793],\n",
      "        [ 0.1162]], grad_fn=<AddmmBackward0>), tensor([[-0.0436],\n",
      "        [ 0.1944],\n",
      "        [ 0.1817],\n",
      "        [-0.0664],\n",
      "        [ 0.2464],\n",
      "        [-0.0943],\n",
      "        [-0.0108],\n",
      "        [ 0.0292]], grad_fn=<AddmmBackward0>), tensor([[-0.0710],\n",
      "        [ 0.0224],\n",
      "        [ 0.5784],\n",
      "        [-0.0505],\n",
      "        [ 0.1868],\n",
      "        [-0.0607],\n",
      "        [ 0.1011],\n",
      "        [ 0.2119]], grad_fn=<AddmmBackward0>), tensor([[-0.0014],\n",
      "        [ 0.5809],\n",
      "        [ 0.0534],\n",
      "        [-0.1089],\n",
      "        [ 0.3947],\n",
      "        [-0.1065],\n",
      "        [ 0.0523],\n",
      "        [ 0.0220]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[5.6781e-06, 9.9999e-01],\n",
      "        [5.7338e-05, 9.9994e-01],\n",
      "        [9.0044e-05, 9.9991e-01],\n",
      "        [2.4666e-03, 9.9753e-01],\n",
      "        [1.0000e+00, 1.7735e-06],\n",
      "        [4.0620e-04, 9.9959e-01],\n",
      "        [1.6858e-04, 9.9983e-01],\n",
      "        [9.9999e-01, 7.8255e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 3.0016e-06],\n",
      "        [1.0000e+00, 4.5277e-07],\n",
      "        [9.9998e-01, 2.4205e-05],\n",
      "        [9.9965e-01, 3.5158e-04],\n",
      "        [6.6582e-05, 9.9993e-01],\n",
      "        [9.9969e-01, 3.1141e-04],\n",
      "        [9.9968e-01, 3.2326e-04],\n",
      "        [3.1283e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.3169e-05, 9.9996e-01],\n",
      "        [1.0013e-05, 9.9999e-01],\n",
      "        [4.9891e-05, 9.9995e-01],\n",
      "        [4.2925e-03, 9.9571e-01],\n",
      "        [9.9999e-01, 8.7517e-06],\n",
      "        [1.0918e-03, 9.9891e-01],\n",
      "        [1.5219e-02, 9.8478e-01],\n",
      "        [1.0000e+00, 7.3067e-07]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.7616e-05],\n",
      "        [1.0000e+00, 1.7681e-07],\n",
      "        [1.0000e+00, 4.4857e-06],\n",
      "        [9.9762e-01, 2.3814e-03],\n",
      "        [8.4380e-04, 9.9916e-01],\n",
      "        [9.9943e-01, 5.6933e-04],\n",
      "        [9.6667e-01, 3.3332e-02],\n",
      "        [6.2675e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>), tensor([[8.4344e-07, 1.0000e+00],\n",
      "        [4.4306e-05, 9.9996e-01],\n",
      "        [6.2683e-03, 9.9373e-01],\n",
      "        [1.3918e-04, 9.9986e-01],\n",
      "        [9.9998e-01, 1.8430e-05],\n",
      "        [2.8798e-05, 9.9997e-01],\n",
      "        [3.7234e-01, 6.2766e-01],\n",
      "        [9.9998e-01, 2.2027e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 52.0 and average score: 0.08\n",
      "Results vs random: {'player_0_score': 0.64, 'player_0_win%': 0.8, 'player_1_score': 0.08, 'player_1_win%': 0.52, 'score': 0.36}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1200, 0.1200, 0.0000, 0.0000, 0.5600, 0.0400, 0.1200, 0.0400, 0.0000]), tensor([0.1200, 0.1200, 0.0000, 0.0000, 0.5600, 0.0400, 0.1200, 0.0400, 0.0000]), 0.4548410202663105, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 7\n",
      "Player 0 prediction: (tensor([0.2000, 0.0800, 0.0800, 0.0400, 0.0000, 0.0400, 0.3200, 0.0000, 0.2400]), tensor([0.2000, 0.0800, 0.0800, 0.0400, 0.0000, 0.0400, 0.3200, 0.0000, 0.2400]), 0.415052828593896, tensor(6))\n",
      "action: 6\n",
      "Player 1 tictactoe_expert action: 2\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.1200, 0.0000, 0.0800, 0.0000, 0.0800, 0.0000, 0.0000, 0.5600]), tensor([0.1600, 0.1200, 0.0000, 0.0800, 0.0000, 0.0800, 0.0000, 0.0000, 0.5600]), 0.681596563412593, tensor(8))\n",
      "action: 8\n",
      "Player 1 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.4000, 0.0000, 0.2000, 0.0000, 0.4000, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.4000, 0.0000, 0.2000, 0.0000, 0.4000, 0.0000, 0.0000, 0.0000]), 0.4805016702470871, tensor(1))\n",
      "action: 1\n",
      "Player 1 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), 0.24004220174482235, tensor(5))\n",
      "action: 5\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 2, 1, 8, 0],\n",
      "        [7, 2, 1, 3, 4],\n",
      "        [7, 5, 8, 2, 0],\n",
      "        [2, 0, 6, 0, 4],\n",
      "        [4, 8, 0, 2, 5],\n",
      "        [0, 2, 5, 8, 6],\n",
      "        [8, 7, 0, 1, 0],\n",
      "        [3, 2, 5, 6, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2092],\n",
      "        [-0.2828],\n",
      "        [-0.1756],\n",
      "        [ 0.1185],\n",
      "        [-0.2828],\n",
      "        [-0.3094],\n",
      "        [-0.0790],\n",
      "        [-0.3094]], grad_fn=<AddmmBackward0>), tensor([[0.2432],\n",
      "        [0.2219],\n",
      "        [0.4665],\n",
      "        [0.1324],\n",
      "        [0.0661],\n",
      "        [0.1115],\n",
      "        [0.1606],\n",
      "        [0.3536]], grad_fn=<AddmmBackward0>), tensor([[-0.0520],\n",
      "        [-0.2945],\n",
      "        [-0.1878],\n",
      "        [ 0.0495],\n",
      "        [-0.0698],\n",
      "        [-0.3003],\n",
      "        [ 0.0289],\n",
      "        [-0.2264]], grad_fn=<AddmmBackward0>), tensor([[0.3764],\n",
      "        [0.3550],\n",
      "        [0.3724],\n",
      "        [0.2096],\n",
      "        [0.0246],\n",
      "        [0.1140],\n",
      "        [0.1103],\n",
      "        [0.2767]], grad_fn=<AddmmBackward0>), tensor([[ 0.0148],\n",
      "        [-0.3007],\n",
      "        [ 0.0546],\n",
      "        [-0.0510],\n",
      "        [-0.0637],\n",
      "        [-0.1143],\n",
      "        [ 0.0250],\n",
      "        [-0.1622]], grad_fn=<AddmmBackward0>), tensor([[-0.0218],\n",
      "        [ 0.4100],\n",
      "        [ 0.1225],\n",
      "        [ 0.0132],\n",
      "        [ 0.1055],\n",
      "        [ 0.0567],\n",
      "        [-0.0366],\n",
      "        [ 0.0932]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2043],\n",
      "        [-0.0063],\n",
      "        [ 0.0592],\n",
      "        [ 0.1474],\n",
      "        [-0.0221],\n",
      "        [-0.0422],\n",
      "        [-0.0307],\n",
      "        [-0.0554]], grad_fn=<AddmmBackward0>), tensor([[ 0.4415],\n",
      "        [-0.0703],\n",
      "        [ 0.0018],\n",
      "        [ 0.2148],\n",
      "        [-0.1139],\n",
      "        [-0.0554],\n",
      "        [ 0.0667],\n",
      "        [ 0.1162]], grad_fn=<AddmmBackward0>), tensor([[ 0.4688],\n",
      "        [-0.0133],\n",
      "        [-0.0436],\n",
      "        [ 0.3090],\n",
      "        [-0.0786],\n",
      "        [-0.0608],\n",
      "        [ 0.0102],\n",
      "        [ 0.0178]], grad_fn=<AddmmBackward0>), tensor([[ 0.5349],\n",
      "        [ 0.0429],\n",
      "        [ 0.4848],\n",
      "        [-0.0696],\n",
      "        [-0.0189],\n",
      "        [ 0.0673],\n",
      "        [ 0.3107],\n",
      "        [ 0.2356]], grad_fn=<AddmmBackward0>), tensor([[-0.0990],\n",
      "        [ 0.1641],\n",
      "        [ 0.1534],\n",
      "        [ 0.0187],\n",
      "        [-0.0498],\n",
      "        [-0.0210],\n",
      "        [-0.0180],\n",
      "        [ 0.0062]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 1.5081e-05],\n",
      "        [9.9999e-01, 7.2781e-06],\n",
      "        [9.9859e-01, 1.4107e-03],\n",
      "        [9.9997e-01, 2.8016e-05],\n",
      "        [9.9998e-01, 1.5280e-05],\n",
      "        [9.9999e-01, 7.5814e-06],\n",
      "        [1.0000e+00, 3.9837e-06],\n",
      "        [9.9999e-01, 8.0405e-06]], grad_fn=<SoftmaxBackward0>), tensor([[3.4609e-05, 9.9997e-01],\n",
      "        [1.2510e-05, 9.9999e-01],\n",
      "        [6.6913e-05, 9.9993e-01],\n",
      "        [6.8961e-05, 9.9993e-01],\n",
      "        [2.0021e-06, 1.0000e+00],\n",
      "        [4.6433e-05, 9.9995e-01],\n",
      "        [2.5961e-05, 9.9997e-01],\n",
      "        [1.7618e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.9589e-06],\n",
      "        [9.9985e-01, 1.5298e-04],\n",
      "        [9.9697e-01, 3.0342e-03],\n",
      "        [9.9919e-01, 8.1044e-04],\n",
      "        [9.9999e-01, 1.0237e-05],\n",
      "        [9.9999e-01, 9.8270e-06],\n",
      "        [9.9997e-01, 3.0005e-05],\n",
      "        [9.9994e-01, 5.9126e-05]], grad_fn=<SoftmaxBackward0>), tensor([[3.1697e-06, 1.0000e+00],\n",
      "        [7.1792e-06, 9.9999e-01],\n",
      "        [6.4207e-04, 9.9936e-01],\n",
      "        [3.0701e-02, 9.6930e-01],\n",
      "        [1.5730e-05, 9.9998e-01],\n",
      "        [1.0547e-04, 9.9989e-01],\n",
      "        [2.8407e-04, 9.9972e-01],\n",
      "        [5.6188e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9557e-01, 4.4272e-03],\n",
      "        [1.0000e+00, 1.1246e-06],\n",
      "        [9.9994e-01, 5.5500e-05],\n",
      "        [9.9335e-01, 6.6543e-03],\n",
      "        [1.0000e+00, 2.3121e-06],\n",
      "        [9.9998e-01, 2.1584e-05],\n",
      "        [9.9765e-01, 2.3544e-03],\n",
      "        [9.9998e-01, 2.0935e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 4.0 and average score: -0.78\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.0400, 0.2000, 0.1200, 0.3600, 0.0800, 0.0000, 0.0000, 0.0000]), tensor([0.2000, 0.0400, 0.2000, 0.1200, 0.3600, 0.0800, 0.0000, 0.0000, 0.0000]), -0.23981958591880706, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.0400, 0.2000, 0.0000, 0.0000, 0.2400, 0.2400, 0.0800, 0.0000]), tensor([0.2000, 0.0400, 0.2000, 0.0000, 0.0000, 0.2400, 0.2400, 0.0800, 0.0000]), 0.11240842730666582, tensor(5))\n",
      "action: 5\n",
      "Player 0 tictactoe_expert action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2400, 0.3200, 0.0000, 0.0000, 0.0000, 0.0000, 0.2400, 0.2000, 0.0000]), tensor([0.2400, 0.3200, 0.0000, 0.0000, 0.0000, 0.0000, 0.2400, 0.2000, 0.0000]), -0.020399344368622854, tensor(1))\n",
      "action: 1\n",
      "Player 0 tictactoe_expert action: 7\n",
      "Player 1 prediction: (tensor([0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8000, 0.0000, 0.0000]), tensor([0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8000, 0.0000, 0.0000]), 0.10365058238116595, tensor(6))\n",
      "action: 6\n",
      "Player 0 tictactoe_expert action: 0\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 1, 8, 5, 0],\n",
      "        [1, 2, 6, 0, 0],\n",
      "        [7, 6, 4, 3, 0],\n",
      "        [8, 6, 0, 3, 4],\n",
      "        [8, 5, 0, 1, 3],\n",
      "        [2, 0, 0, 3, 4],\n",
      "        [4, 2, 0, 6, 1],\n",
      "        [8, 2, 0, 3, 4]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2394],\n",
      "        [-0.1974],\n",
      "        [ 0.2651],\n",
      "        [-0.0608],\n",
      "        [-0.1689],\n",
      "        [-0.1457],\n",
      "        [ 0.2651],\n",
      "        [ 0.4318]], grad_fn=<AddmmBackward0>), tensor([[ 0.1575],\n",
      "        [ 0.4272],\n",
      "        [-0.2448],\n",
      "        [ 0.3504],\n",
      "        [ 0.3181],\n",
      "        [ 0.3472],\n",
      "        [-0.2867],\n",
      "        [ 0.1945]], grad_fn=<AddmmBackward0>), tensor([[-0.1522],\n",
      "        [ 0.0037],\n",
      "        [ 0.0923],\n",
      "        [ 0.1609],\n",
      "        [-0.0265],\n",
      "        [-0.0915],\n",
      "        [ 0.3102],\n",
      "        [ 0.3074]], grad_fn=<AddmmBackward0>), tensor([[ 0.1851],\n",
      "        [ 0.1664],\n",
      "        [-0.2990],\n",
      "        [ 0.0757],\n",
      "        [ 0.1923],\n",
      "        [-0.0545],\n",
      "        [-0.2626],\n",
      "        [ 0.0821]], grad_fn=<AddmmBackward0>), tensor([[-0.1155],\n",
      "        [ 0.0363],\n",
      "        [ 0.5019],\n",
      "        [ 0.0388],\n",
      "        [-0.0151],\n",
      "        [-0.0099],\n",
      "        [ 0.3134],\n",
      "        [ 0.1244]], grad_fn=<AddmmBackward0>), tensor([[ 0.1170],\n",
      "        [-0.0155],\n",
      "        [-0.3454],\n",
      "        [ 0.0986],\n",
      "        [ 0.3160],\n",
      "        [ 0.0185],\n",
      "        [-0.1342],\n",
      "        [ 0.1344]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.0348],\n",
      "        [0.0571],\n",
      "        [0.0590],\n",
      "        [0.0698],\n",
      "        [0.0201],\n",
      "        [0.2192],\n",
      "        [0.0421],\n",
      "        [0.4279]], grad_fn=<AddmmBackward0>), tensor([[ 0.0048],\n",
      "        [ 0.3787],\n",
      "        [ 0.0324],\n",
      "        [ 0.5656],\n",
      "        [ 0.0569],\n",
      "        [-0.0352],\n",
      "        [ 0.0021],\n",
      "        [ 0.5140]], grad_fn=<AddmmBackward0>), tensor([[ 0.0026],\n",
      "        [ 0.1516],\n",
      "        [ 0.1209],\n",
      "        [ 0.0633],\n",
      "        [ 0.1868],\n",
      "        [-0.0260],\n",
      "        [ 0.1129],\n",
      "        [ 0.0553]], grad_fn=<AddmmBackward0>), tensor([[ 0.0044],\n",
      "        [ 0.4794],\n",
      "        [ 0.0254],\n",
      "        [ 0.0082],\n",
      "        [ 0.2778],\n",
      "        [-0.0749],\n",
      "        [ 0.0260],\n",
      "        [-0.0280]], grad_fn=<AddmmBackward0>), tensor([[ 0.0328],\n",
      "        [-0.0118],\n",
      "        [ 0.0654],\n",
      "        [ 0.0172],\n",
      "        [ 0.4350],\n",
      "        [-0.0644],\n",
      "        [ 0.2079],\n",
      "        [ 0.1452]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9999e-01, 5.8819e-06],\n",
      "        [9.9997e-01, 3.2841e-05],\n",
      "        [3.4041e-06, 1.0000e+00],\n",
      "        [9.9997e-01, 3.1048e-05],\n",
      "        [9.9999e-01, 7.5379e-06],\n",
      "        [9.9992e-01, 8.3727e-05],\n",
      "        [5.4866e-07, 1.0000e+00],\n",
      "        [1.6800e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.9774e-06, 9.9999e-01],\n",
      "        [3.3862e-05, 9.9997e-01],\n",
      "        [9.9995e-01, 4.5763e-05],\n",
      "        [4.0140e-05, 9.9996e-01],\n",
      "        [3.7970e-05, 9.9996e-01],\n",
      "        [2.1380e-02, 9.7862e-01],\n",
      "        [9.9999e-01, 7.3525e-06],\n",
      "        [9.9999e-01, 1.1535e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.7628e-06],\n",
      "        [9.9989e-01, 1.1194e-04],\n",
      "        [6.1738e-06, 9.9999e-01],\n",
      "        [9.9699e-01, 3.0140e-03],\n",
      "        [9.9999e-01, 5.1200e-06],\n",
      "        [9.9039e-01, 9.6110e-03],\n",
      "        [1.1748e-05, 9.9999e-01],\n",
      "        [8.4131e-03, 9.9159e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.6410e-06, 1.0000e+00],\n",
      "        [1.1511e-04, 9.9988e-01],\n",
      "        [1.0000e+00, 1.6441e-06],\n",
      "        [1.5063e-01, 8.4937e-01],\n",
      "        [1.0576e-05, 9.9999e-01],\n",
      "        [2.7766e-01, 7.2234e-01],\n",
      "        [9.9999e-01, 7.1092e-06],\n",
      "        [9.8671e-01, 1.3293e-02]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 9.0919e-06],\n",
      "        [9.8985e-01, 1.0154e-02],\n",
      "        [6.1324e-06, 9.9999e-01],\n",
      "        [9.9961e-01, 3.9416e-04],\n",
      "        [1.0000e+00, 1.1961e-06],\n",
      "        [9.9758e-01, 2.4241e-03],\n",
      "        [4.4350e-06, 1.0000e+00],\n",
      "        [7.7840e-01, 2.2160e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 4.0 and average score: -0.82\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.78, 'player_0_win%': 0.04, 'player_1_score': -0.82, 'player_1_win%': 0.04, 'score': -0.8}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 3, 7, 0, 8],\n",
      "        [0, 3, 4, 1, 8],\n",
      "        [8, 2, 1, 5, 0],\n",
      "        [4, 8, 0, 2, 1],\n",
      "        [7, 2, 0, 6, 6],\n",
      "        [8, 4, 1, 7, 6],\n",
      "        [8, 2, 0, 6, 6],\n",
      "        [6, 2, 8, 5, 0]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.6712],\n",
      "        [ 0.1224],\n",
      "        [ 0.0206],\n",
      "        [ 0.0280],\n",
      "        [-0.0410],\n",
      "        [ 0.6712],\n",
      "        [-0.5173],\n",
      "        [-0.5419]], grad_fn=<AddmmBackward0>), tensor([[-0.6493],\n",
      "        [ 0.2292],\n",
      "        [ 0.3649],\n",
      "        [-0.0365],\n",
      "        [ 0.3425],\n",
      "        [-0.4093],\n",
      "        [ 0.6468],\n",
      "        [ 0.7452]], grad_fn=<AddmmBackward0>), tensor([[ 0.7344],\n",
      "        [ 0.2206],\n",
      "        [-0.0850],\n",
      "        [ 0.1175],\n",
      "        [ 0.0821],\n",
      "        [ 0.4011],\n",
      "        [-0.3258],\n",
      "        [-0.5188]], grad_fn=<AddmmBackward0>), tensor([[-0.6397],\n",
      "        [ 0.1846],\n",
      "        [ 0.3770],\n",
      "        [-0.0142],\n",
      "        [ 0.1841],\n",
      "        [-0.3704],\n",
      "        [ 0.4829],\n",
      "        [ 0.6352]], grad_fn=<AddmmBackward0>), tensor([[ 0.6489],\n",
      "        [ 0.2543],\n",
      "        [-0.0905],\n",
      "        [ 0.1949],\n",
      "        [-0.0293],\n",
      "        [ 0.6677],\n",
      "        [-0.1950],\n",
      "        [-0.3651]], grad_fn=<AddmmBackward0>), tensor([[-0.2661],\n",
      "        [ 0.2781],\n",
      "        [ 0.0547],\n",
      "        [ 0.2799],\n",
      "        [ 0.1115],\n",
      "        [-0.6458],\n",
      "        [ 0.4624],\n",
      "        [ 0.6014]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0316],\n",
      "        [ 0.0279],\n",
      "        [-0.0887],\n",
      "        [ 0.0706],\n",
      "        [ 0.1555],\n",
      "        [-0.0127],\n",
      "        [ 0.0979],\n",
      "        [ 0.0462]], grad_fn=<AddmmBackward0>), tensor([[-0.0908],\n",
      "        [ 0.1775],\n",
      "        [ 0.2584],\n",
      "        [ 0.1167],\n",
      "        [ 0.7293],\n",
      "        [-0.0599],\n",
      "        [ 0.2423],\n",
      "        [ 0.0426]], grad_fn=<AddmmBackward0>), tensor([[-0.0563],\n",
      "        [ 0.1335],\n",
      "        [ 0.2941],\n",
      "        [ 0.0448],\n",
      "        [ 0.0552],\n",
      "        [ 0.0176],\n",
      "        [ 0.1588],\n",
      "        [ 0.0053]], grad_fn=<AddmmBackward0>), tensor([[-0.0669],\n",
      "        [ 0.5380],\n",
      "        [ 0.5121],\n",
      "        [ 0.2655],\n",
      "        [ 0.0686],\n",
      "        [ 0.0347],\n",
      "        [ 0.2163],\n",
      "        [ 0.1877]], grad_fn=<AddmmBackward0>), tensor([[ 0.2816],\n",
      "        [ 0.4899],\n",
      "        [ 0.0290],\n",
      "        [ 0.3100],\n",
      "        [-0.0151],\n",
      "        [ 0.1854],\n",
      "        [ 0.0307],\n",
      "        [ 0.2020]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.2652e-06, 1.0000e+00],\n",
      "        [9.9983e-01, 1.7085e-04],\n",
      "        [9.9748e-01, 2.5185e-03],\n",
      "        [9.9967e-01, 3.2871e-04],\n",
      "        [9.9974e-01, 2.6317e-04],\n",
      "        [7.1704e-06, 9.9999e-01],\n",
      "        [9.9983e-01, 1.6997e-04],\n",
      "        [9.9987e-01, 1.2862e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9977e-01, 2.2902e-04],\n",
      "        [1.5984e-04, 9.9984e-01],\n",
      "        [7.7061e-05, 9.9992e-01],\n",
      "        [6.6076e-05, 9.9993e-01],\n",
      "        [1.2531e-04, 9.9987e-01],\n",
      "        [9.9988e-01, 1.2470e-04],\n",
      "        [7.6711e-05, 9.9992e-01],\n",
      "        [1.6897e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.3936e-06, 1.0000e+00],\n",
      "        [9.9848e-01, 1.5233e-03],\n",
      "        [9.9970e-01, 3.0065e-04],\n",
      "        [9.9948e-01, 5.1833e-04],\n",
      "        [9.9819e-01, 1.8135e-03],\n",
      "        [2.4107e-06, 1.0000e+00],\n",
      "        [9.9995e-01, 5.4459e-05],\n",
      "        [9.9972e-01, 2.7804e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9986e-01, 1.3932e-04],\n",
      "        [2.3064e-04, 9.9977e-01],\n",
      "        [4.8829e-05, 9.9995e-01],\n",
      "        [1.3335e-04, 9.9987e-01],\n",
      "        [3.5608e-03, 9.9644e-01],\n",
      "        [9.9996e-01, 4.4713e-05],\n",
      "        [3.2963e-04, 9.9967e-01],\n",
      "        [2.7133e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[7.1694e-06, 9.9999e-01],\n",
      "        [9.9815e-01, 1.8528e-03],\n",
      "        [9.9650e-01, 3.4980e-03],\n",
      "        [9.9968e-01, 3.2356e-04],\n",
      "        [9.9803e-01, 1.9721e-03],\n",
      "        [6.2158e-06, 9.9999e-01],\n",
      "        [9.9988e-01, 1.2078e-04],\n",
      "        [9.9998e-01, 2.4219e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 6, 0, 3, 5],\n",
      "        [2, 8, 1, 0, 0],\n",
      "        [4, 8, 1, 0, 6],\n",
      "        [2, 8, 0, 0, 0],\n",
      "        [3, 1, 0, 5, 0],\n",
      "        [3, 5, 0, 5, 0],\n",
      "        [7, 5, 8, 0, 0],\n",
      "        [8, 4, 6, 0, 7]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[-0.3523],\n",
      "        [-0.1349],\n",
      "        [ 0.3276],\n",
      "        [ 0.1788],\n",
      "        [ 0.4060],\n",
      "        [-0.1125],\n",
      "        [-0.1714],\n",
      "        [ 0.3276]], grad_fn=<AddmmBackward0>), tensor([[ 0.5401],\n",
      "        [ 0.3121],\n",
      "        [-0.4137],\n",
      "        [-0.0258],\n",
      "        [ 0.0916],\n",
      "        [ 0.3827],\n",
      "        [ 0.4393],\n",
      "        [-0.2345]], grad_fn=<AddmmBackward0>), tensor([[-0.3564],\n",
      "        [ 0.1724],\n",
      "        [ 0.4597],\n",
      "        [ 0.1885],\n",
      "        [ 0.3048],\n",
      "        [-0.1151],\n",
      "        [-0.1112],\n",
      "        [ 0.1685]], grad_fn=<AddmmBackward0>), tensor([[ 0.4802],\n",
      "        [ 0.3836],\n",
      "        [-0.2362],\n",
      "        [ 0.0807],\n",
      "        [ 0.0708],\n",
      "        [ 0.2074],\n",
      "        [ 0.3947],\n",
      "        [-0.0845]], grad_fn=<AddmmBackward0>), tensor([[-0.1401],\n",
      "        [ 0.0449],\n",
      "        [ 0.3026],\n",
      "        [ 0.0226],\n",
      "        [ 0.1046],\n",
      "        [ 0.0289],\n",
      "        [ 0.0988],\n",
      "        [ 0.1987]], grad_fn=<AddmmBackward0>), tensor([[ 0.5327],\n",
      "        [ 0.0241],\n",
      "        [-0.0204],\n",
      "        [-0.0098],\n",
      "        [ 0.0279],\n",
      "        [ 0.0496],\n",
      "        [ 0.0768],\n",
      "        [ 0.0637]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0460],\n",
      "        [ 0.3567],\n",
      "        [ 0.0296],\n",
      "        [ 0.0840],\n",
      "        [ 0.6526],\n",
      "        [-0.0172],\n",
      "        [ 0.2567],\n",
      "        [ 0.0219]], grad_fn=<AddmmBackward0>), tensor([[-0.0007],\n",
      "        [ 0.5912],\n",
      "        [-0.0107],\n",
      "        [ 0.5009],\n",
      "        [ 0.5151],\n",
      "        [ 0.1205],\n",
      "        [ 0.1396],\n",
      "        [ 0.0470]], grad_fn=<AddmmBackward0>), tensor([[ 1.3569e-01],\n",
      "        [ 5.8353e-01],\n",
      "        [ 3.9823e-02],\n",
      "        [ 3.4945e-01],\n",
      "        [-4.7025e-04],\n",
      "        [ 5.5333e-02],\n",
      "        [ 2.1841e-01],\n",
      "        [ 2.2956e-02]], grad_fn=<AddmmBackward0>), tensor([[ 0.1570],\n",
      "        [ 0.0652],\n",
      "        [ 0.0406],\n",
      "        [ 0.0690],\n",
      "        [-0.0031],\n",
      "        [ 0.2002],\n",
      "        [ 0.4323],\n",
      "        [ 0.1435]], grad_fn=<AddmmBackward0>), tensor([[ 0.1805],\n",
      "        [-0.0010],\n",
      "        [ 0.1321],\n",
      "        [-0.0147],\n",
      "        [-0.0085],\n",
      "        [ 0.0181],\n",
      "        [ 0.0482],\n",
      "        [ 0.2153]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9997e-01, 2.9628e-05],\n",
      "        [9.9982e-01, 1.8209e-04],\n",
      "        [3.8605e-06, 1.0000e+00],\n",
      "        [2.8866e-05, 9.9997e-01],\n",
      "        [9.5711e-04, 9.9904e-01],\n",
      "        [9.9996e-01, 3.6095e-05],\n",
      "        [9.9998e-01, 2.1678e-05],\n",
      "        [1.0561e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.3826e-05, 9.9999e-01],\n",
      "        [1.3039e-04, 9.9987e-01],\n",
      "        [9.9982e-01, 1.8344e-04],\n",
      "        [9.9964e-01, 3.6258e-04],\n",
      "        [9.9971e-01, 2.8894e-04],\n",
      "        [1.1091e-04, 9.9989e-01],\n",
      "        [2.3951e-04, 9.9976e-01],\n",
      "        [9.9991e-01, 8.6606e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 1.4757e-05],\n",
      "        [9.9985e-01, 1.5469e-04],\n",
      "        [7.5612e-06, 9.9999e-01],\n",
      "        [6.7271e-04, 9.9933e-01],\n",
      "        [9.7704e-02, 9.0230e-01],\n",
      "        [9.9954e-01, 4.6416e-04],\n",
      "        [9.9972e-01, 2.8432e-04],\n",
      "        [2.6667e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[7.7839e-05, 9.9992e-01],\n",
      "        [3.0701e-02, 9.6930e-01],\n",
      "        [9.9999e-01, 8.4152e-06],\n",
      "        [9.9639e-01, 3.6082e-03],\n",
      "        [9.5185e-01, 4.8152e-02],\n",
      "        [1.8004e-04, 9.9982e-01],\n",
      "        [1.9179e-02, 9.8082e-01],\n",
      "        [1.0000e+00, 4.2787e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 4.8253e-06],\n",
      "        [9.6939e-01, 3.0608e-02],\n",
      "        [1.7178e-05, 9.9998e-01],\n",
      "        [8.5613e-02, 9.1439e-01],\n",
      "        [4.5260e-01, 5.4740e-01],\n",
      "        [9.9936e-01, 6.3629e-04],\n",
      "        [9.9936e-01, 6.3691e-04],\n",
      "        [1.6356e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 4, 1, 0, 2],\n",
      "        [7, 0, 0, 7, 3],\n",
      "        [8, 7, 2, 5, 1],\n",
      "        [1, 8, 0, 2, 0],\n",
      "        [2, 1, 4, 7, 3],\n",
      "        [8, 2, 1, 6, 0],\n",
      "        [2, 3, 8, 0, 3],\n",
      "        [8, 2, 0, 7, 3]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[-0.1684],\n",
      "        [ 0.1646],\n",
      "        [ 0.2785],\n",
      "        [-0.1863],\n",
      "        [-0.2834],\n",
      "        [ 0.1649],\n",
      "        [-0.2683],\n",
      "        [ 0.0947]], grad_fn=<AddmmBackward0>), tensor([[ 0.4007],\n",
      "        [ 0.4125],\n",
      "        [ 0.1162],\n",
      "        [ 0.6281],\n",
      "        [ 0.3859],\n",
      "        [ 0.0085],\n",
      "        [ 0.3788],\n",
      "        [-0.0497]], grad_fn=<AddmmBackward0>), tensor([[-0.3764],\n",
      "        [ 0.1411],\n",
      "        [ 0.3410],\n",
      "        [-0.1049],\n",
      "        [-0.1453],\n",
      "        [ 0.1940],\n",
      "        [-0.0152],\n",
      "        [ 0.2318]], grad_fn=<AddmmBackward0>), tensor([[ 0.6837],\n",
      "        [ 0.1099],\n",
      "        [ 0.1668],\n",
      "        [ 0.2494],\n",
      "        [ 0.1858],\n",
      "        [ 0.0549],\n",
      "        [ 0.2221],\n",
      "        [-0.0343]], grad_fn=<AddmmBackward0>), tensor([[-0.4250],\n",
      "        [ 0.0941],\n",
      "        [ 0.3013],\n",
      "        [ 0.0903],\n",
      "        [ 0.0820],\n",
      "        [ 0.1576],\n",
      "        [ 0.0385],\n",
      "        [ 0.2982]], grad_fn=<AddmmBackward0>), tensor([[ 0.5472],\n",
      "        [ 0.0444],\n",
      "        [ 0.0849],\n",
      "        [ 0.0179],\n",
      "        [ 0.2062],\n",
      "        [-0.0433],\n",
      "        [ 0.1400],\n",
      "        [ 0.1529]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1177],\n",
      "        [ 0.4155],\n",
      "        [ 0.1237],\n",
      "        [-0.1003],\n",
      "        [ 0.0413],\n",
      "        [ 0.0281],\n",
      "        [ 0.1267],\n",
      "        [ 0.1226]], grad_fn=<AddmmBackward0>), tensor([[-0.0981],\n",
      "        [ 0.4857],\n",
      "        [ 0.1993],\n",
      "        [ 0.0401],\n",
      "        [ 0.0847],\n",
      "        [ 0.0759],\n",
      "        [ 0.3590],\n",
      "        [ 0.0126]], grad_fn=<AddmmBackward0>), tensor([[-0.0711],\n",
      "        [ 0.0507],\n",
      "        [ 0.3320],\n",
      "        [ 0.1096],\n",
      "        [ 0.0175],\n",
      "        [ 0.2598],\n",
      "        [ 0.2242],\n",
      "        [ 0.1552]], grad_fn=<AddmmBackward0>), tensor([[0.0489],\n",
      "        [0.0286],\n",
      "        [0.5502],\n",
      "        [0.4989],\n",
      "        [0.1935],\n",
      "        [0.1504],\n",
      "        [0.5474],\n",
      "        [0.1089]], grad_fn=<AddmmBackward0>), tensor([[0.2373],\n",
      "        [0.0350],\n",
      "        [0.7012],\n",
      "        [0.0317],\n",
      "        [0.0476],\n",
      "        [0.0353],\n",
      "        [0.2685],\n",
      "        [0.3939]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 2.3757e-05],\n",
      "        [2.2183e-04, 9.9978e-01],\n",
      "        [9.1405e-05, 9.9991e-01],\n",
      "        [9.9997e-01, 3.2998e-05],\n",
      "        [9.9999e-01, 1.2430e-05],\n",
      "        [1.4449e-04, 9.9986e-01],\n",
      "        [9.9999e-01, 1.4078e-05],\n",
      "        [1.7812e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.2218e-05, 9.9999e-01],\n",
      "        [9.9956e-01, 4.3685e-04],\n",
      "        [9.9973e-01, 2.7459e-04],\n",
      "        [8.2948e-05, 9.9992e-01],\n",
      "        [9.5477e-06, 9.9999e-01],\n",
      "        [9.9989e-01, 1.1071e-04],\n",
      "        [1.9631e-05, 9.9998e-01],\n",
      "        [9.9997e-01, 3.3990e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 2.5757e-05],\n",
      "        [4.7079e-01, 5.2921e-01],\n",
      "        [1.8733e-04, 9.9981e-01],\n",
      "        [9.9998e-01, 2.2139e-05],\n",
      "        [9.9998e-01, 1.8189e-05],\n",
      "        [2.0267e-05, 9.9998e-01],\n",
      "        [9.9999e-01, 6.5009e-06],\n",
      "        [7.2212e-04, 9.9928e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.7385e-04, 9.9983e-01],\n",
      "        [9.2267e-01, 7.7331e-02],\n",
      "        [9.9992e-01, 7.7689e-05],\n",
      "        [6.1286e-05, 9.9994e-01],\n",
      "        [4.0255e-05, 9.9996e-01],\n",
      "        [9.9994e-01, 6.3423e-05],\n",
      "        [2.1922e-04, 9.9978e-01],\n",
      "        [9.9994e-01, 6.2501e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9996e-01, 4.3815e-05],\n",
      "        [9.7145e-03, 9.9029e-01],\n",
      "        [1.2873e-05, 9.9999e-01],\n",
      "        [9.9956e-01, 4.3579e-04],\n",
      "        [9.9998e-01, 1.7625e-05],\n",
      "        [1.2209e-01, 8.7791e-01],\n",
      "        [9.9987e-01, 1.2975e-04],\n",
      "        [1.2658e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 0, 3, 5, 6],\n",
      "        [1, 0, 7, 5, 2],\n",
      "        [1, 8, 0, 6, 0],\n",
      "        [6, 8, 1, 0, 4],\n",
      "        [2, 0, 0, 1, 4],\n",
      "        [8, 2, 1, 7, 3],\n",
      "        [2, 0, 2, 1, 4],\n",
      "        [4, 5, 0, 6, 2]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.4191],\n",
      "        [-0.1424],\n",
      "        [-0.2005],\n",
      "        [-0.0371],\n",
      "        [-0.1114],\n",
      "        [ 0.4773],\n",
      "        [ 0.2678],\n",
      "        [ 0.4662]], grad_fn=<AddmmBackward0>), tensor([[-0.2452],\n",
      "        [ 0.5575],\n",
      "        [ 0.5783],\n",
      "        [ 0.3380],\n",
      "        [ 0.3475],\n",
      "        [-0.0297],\n",
      "        [ 0.0089],\n",
      "        [-0.3720]], grad_fn=<AddmmBackward0>), tensor([[ 0.3927],\n",
      "        [-0.3660],\n",
      "        [-0.1775],\n",
      "        [ 0.1577],\n",
      "        [ 0.0574],\n",
      "        [ 0.3085],\n",
      "        [ 0.0518],\n",
      "        [ 0.6527]], grad_fn=<AddmmBackward0>), tensor([[-0.0181],\n",
      "        [ 0.6394],\n",
      "        [ 0.3950],\n",
      "        [ 0.3126],\n",
      "        [ 0.0731],\n",
      "        [ 0.0909],\n",
      "        [-0.0188],\n",
      "        [-0.4305]], grad_fn=<AddmmBackward0>), tensor([[ 0.3257],\n",
      "        [-0.1613],\n",
      "        [-0.0307],\n",
      "        [ 0.0874],\n",
      "        [ 0.0971],\n",
      "        [ 0.3448],\n",
      "        [ 0.0885],\n",
      "        [ 0.4810]], grad_fn=<AddmmBackward0>), tensor([[ 0.0423],\n",
      "        [ 0.6704],\n",
      "        [ 0.0471],\n",
      "        [ 0.1224],\n",
      "        [ 0.1229],\n",
      "        [-0.0152],\n",
      "        [-0.1629],\n",
      "        [-0.2953]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0005],\n",
      "        [-0.0701],\n",
      "        [-0.1150],\n",
      "        [ 0.2320],\n",
      "        [ 0.1483],\n",
      "        [ 0.2884],\n",
      "        [ 0.2094],\n",
      "        [-0.0263]], grad_fn=<AddmmBackward0>), tensor([[ 0.0478],\n",
      "        [ 0.1729],\n",
      "        [ 0.4138],\n",
      "        [ 0.2890],\n",
      "        [ 0.3181],\n",
      "        [ 0.1974],\n",
      "        [-0.0118],\n",
      "        [-0.0446]], grad_fn=<AddmmBackward0>), tensor([[ 0.1161],\n",
      "        [-0.0222],\n",
      "        [ 0.1187],\n",
      "        [ 0.6279],\n",
      "        [ 0.0177],\n",
      "        [ 0.2437],\n",
      "        [ 0.0029],\n",
      "        [ 0.0293]], grad_fn=<AddmmBackward0>), tensor([[0.1209],\n",
      "        [0.0192],\n",
      "        [0.4906],\n",
      "        [0.0286],\n",
      "        [0.0944],\n",
      "        [0.3307],\n",
      "        [0.0116],\n",
      "        [0.0341]], grad_fn=<AddmmBackward0>), tensor([[0.5226],\n",
      "        [0.0362],\n",
      "        [0.0214],\n",
      "        [0.0161],\n",
      "        [0.0397],\n",
      "        [0.5227],\n",
      "        [0.0632],\n",
      "        [0.3438]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[2.5646e-05, 9.9997e-01],\n",
      "        [9.9994e-01, 5.9619e-05],\n",
      "        [9.9994e-01, 6.3663e-05],\n",
      "        [9.9988e-01, 1.1913e-04],\n",
      "        [9.9993e-01, 7.4435e-05],\n",
      "        [1.2878e-04, 9.9987e-01],\n",
      "        [3.7375e-05, 9.9996e-01],\n",
      "        [3.2405e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 5.8575e-06],\n",
      "        [1.4073e-05, 9.9999e-01],\n",
      "        [3.9231e-05, 9.9996e-01],\n",
      "        [5.0140e-05, 9.9995e-01],\n",
      "        [5.2035e-03, 9.9480e-01],\n",
      "        [9.9992e-01, 8.0483e-05],\n",
      "        [9.9972e-01, 2.7764e-04],\n",
      "        [9.9997e-01, 2.9815e-05]], grad_fn=<SoftmaxBackward0>), tensor([[5.0272e-05, 9.9995e-01],\n",
      "        [9.9936e-01, 6.4053e-04],\n",
      "        [9.9999e-01, 1.4576e-05],\n",
      "        [9.9994e-01, 6.0094e-05],\n",
      "        [9.9422e-01, 5.7802e-03],\n",
      "        [2.6160e-04, 9.9974e-01],\n",
      "        [1.3197e-03, 9.9868e-01],\n",
      "        [1.1962e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 1.2978e-05],\n",
      "        [8.3629e-06, 9.9999e-01],\n",
      "        [5.6209e-05, 9.9994e-01],\n",
      "        [1.1795e-01, 8.8205e-01],\n",
      "        [2.3465e-03, 9.9765e-01],\n",
      "        [9.9987e-01, 1.3087e-04],\n",
      "        [9.9998e-01, 1.5037e-05],\n",
      "        [9.9989e-01, 1.1214e-04]], grad_fn=<SoftmaxBackward0>), tensor([[2.3406e-05, 9.9998e-01],\n",
      "        [9.9986e-01, 1.4439e-04],\n",
      "        [9.9965e-01, 3.5144e-04],\n",
      "        [9.9029e-01, 9.7105e-03],\n",
      "        [9.9980e-01, 2.0260e-04],\n",
      "        [5.1543e-05, 9.9995e-01],\n",
      "        [4.2895e-05, 9.9996e-01],\n",
      "        [5.7948e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: 0.23\n",
      "Test score {'score': 0.23, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "4900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 2, 0, 0, 4],\n",
      "        [2, 5, 3, 7, 4],\n",
      "        [7, 0, 4, 8, 3],\n",
      "        [0, 8, 5, 3, 1],\n",
      "        [6, 1, 8, 0, 4],\n",
      "        [0, 7, 6, 5, 2],\n",
      "        [4, 8, 6, 1, 2],\n",
      "        [8, 6, 3, 1, 0]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2169],\n",
      "        [ 0.0328],\n",
      "        [ 0.1541],\n",
      "        [-0.4657],\n",
      "        [ 0.2514],\n",
      "        [-0.4657],\n",
      "        [ 0.1541],\n",
      "        [-0.2386]], grad_fn=<AddmmBackward0>), tensor([[ 0.3091],\n",
      "        [-0.0728],\n",
      "        [-0.2547],\n",
      "        [ 0.0623],\n",
      "        [-0.0291],\n",
      "        [ 0.0623],\n",
      "        [-0.4132],\n",
      "        [ 0.1334]], grad_fn=<AddmmBackward0>), tensor([[-0.2686],\n",
      "        [ 0.1537],\n",
      "        [-0.1079],\n",
      "        [-0.1044],\n",
      "        [ 0.2291],\n",
      "        [-0.2545],\n",
      "        [ 0.3558],\n",
      "        [-0.1093]], grad_fn=<AddmmBackward0>), tensor([[ 0.1220],\n",
      "        [-0.1311],\n",
      "        [-0.2682],\n",
      "        [ 0.1897],\n",
      "        [-0.1030],\n",
      "        [ 0.0270],\n",
      "        [-0.4268],\n",
      "        [ 0.1079]], grad_fn=<AddmmBackward0>), tensor([[-0.1285],\n",
      "        [ 0.1777],\n",
      "        [ 0.1795],\n",
      "        [-0.0779],\n",
      "        [-0.0600],\n",
      "        [-0.1107],\n",
      "        [ 0.4111],\n",
      "        [-0.0531]], grad_fn=<AddmmBackward0>), tensor([[-0.1264],\n",
      "        [-0.3368],\n",
      "        [-0.0550],\n",
      "        [ 0.2904],\n",
      "        [-0.1125],\n",
      "        [ 0.0470],\n",
      "        [-0.2099],\n",
      "        [-0.1049]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1711],\n",
      "        [-0.0255],\n",
      "        [ 0.0478],\n",
      "        [-0.0120],\n",
      "        [ 0.5625],\n",
      "        [-0.0120],\n",
      "        [ 0.0498],\n",
      "        [-0.0974]], grad_fn=<AddmmBackward0>), tensor([[ 0.2151],\n",
      "        [ 0.1299],\n",
      "        [-0.0247],\n",
      "        [ 0.0235],\n",
      "        [ 0.5824],\n",
      "        [-0.1179],\n",
      "        [-0.0225],\n",
      "        [ 0.2054]], grad_fn=<AddmmBackward0>), tensor([[ 0.0436],\n",
      "        [ 0.1442],\n",
      "        [-0.0205],\n",
      "        [-0.0340],\n",
      "        [ 0.6000],\n",
      "        [ 0.0481],\n",
      "        [-0.0467],\n",
      "        [ 0.0739]], grad_fn=<AddmmBackward0>), tensor([[-0.0156],\n",
      "        [ 0.1270],\n",
      "        [ 0.0528],\n",
      "        [ 0.2631],\n",
      "        [-0.0069],\n",
      "        [ 0.0506],\n",
      "        [-0.0519],\n",
      "        [ 0.3085]], grad_fn=<AddmmBackward0>), tensor([[-0.0615],\n",
      "        [ 0.1779],\n",
      "        [ 0.0992],\n",
      "        [-0.0099],\n",
      "        [ 0.1155],\n",
      "        [ 0.2615],\n",
      "        [ 0.2298],\n",
      "        [ 0.0241]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9996e-01, 4.1778e-05],\n",
      "        [9.9999e-01, 1.3899e-05],\n",
      "        [2.2197e-05, 9.9998e-01],\n",
      "        [9.9995e-01, 5.4485e-05],\n",
      "        [4.3372e-05, 9.9996e-01],\n",
      "        [9.9995e-01, 5.4485e-05],\n",
      "        [7.0917e-06, 9.9999e-01],\n",
      "        [9.9977e-01, 2.3432e-04]], grad_fn=<SoftmaxBackward0>), tensor([[4.9998e-05, 9.9995e-01],\n",
      "        [2.7193e-04, 9.9973e-01],\n",
      "        [9.9984e-01, 1.6244e-04],\n",
      "        [6.4223e-06, 9.9999e-01],\n",
      "        [9.9993e-01, 6.5544e-05],\n",
      "        [2.0989e-05, 9.9998e-01],\n",
      "        [9.9991e-01, 9.0180e-05],\n",
      "        [7.7761e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9990e-01, 1.0340e-04],\n",
      "        [9.9991e-01, 8.6287e-05],\n",
      "        [4.9186e-05, 9.9995e-01],\n",
      "        [9.9987e-01, 1.3022e-04],\n",
      "        [2.7436e-05, 9.9997e-01],\n",
      "        [9.9976e-01, 2.3928e-04],\n",
      "        [6.4302e-06, 9.9999e-01],\n",
      "        [9.9998e-01, 2.0950e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.1685e-02, 9.8831e-01],\n",
      "        [5.0502e-04, 9.9949e-01],\n",
      "        [9.9988e-01, 1.2342e-04],\n",
      "        [5.8447e-05, 9.9994e-01],\n",
      "        [9.9971e-01, 2.8701e-04],\n",
      "        [4.6712e-05, 9.9995e-01],\n",
      "        [9.9989e-01, 1.1326e-04],\n",
      "        [8.3871e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.5233e-01, 4.7673e-02],\n",
      "        [9.9997e-01, 3.2263e-05],\n",
      "        [2.6664e-05, 9.9997e-01],\n",
      "        [9.9981e-01, 1.9093e-04],\n",
      "        [3.7208e-04, 9.9963e-01],\n",
      "        [9.9981e-01, 1.9130e-04],\n",
      "        [2.2368e-05, 9.9998e-01],\n",
      "        [9.9972e-01, 2.7801e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 4, 0, 7, 1],\n",
      "        [8, 0, 5, 6, 7],\n",
      "        [2, 4, 6, 8, 1],\n",
      "        [6, 2, 4, 8, 0],\n",
      "        [7, 8, 6, 1, 4],\n",
      "        [5, 0, 8, 5, 6],\n",
      "        [2, 8, 3, 6, 1],\n",
      "        [3, 8, 4, 2, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2018],\n",
      "        [-0.2404],\n",
      "        [-0.1020],\n",
      "        [ 0.3170],\n",
      "        [ 0.3170],\n",
      "        [ 0.5369],\n",
      "        [ 0.1349],\n",
      "        [-0.0855]], grad_fn=<AddmmBackward0>), tensor([[ 0.3764],\n",
      "        [ 0.3128],\n",
      "        [ 0.0307],\n",
      "        [-0.1730],\n",
      "        [-0.1983],\n",
      "        [ 0.2516],\n",
      "        [-0.0829],\n",
      "        [ 0.3477]], grad_fn=<AddmmBackward0>), tensor([[-0.2911],\n",
      "        [-0.0802],\n",
      "        [ 0.0711],\n",
      "        [ 0.3390],\n",
      "        [ 0.0767],\n",
      "        [ 0.2043],\n",
      "        [ 0.2588],\n",
      "        [ 0.0670]], grad_fn=<AddmmBackward0>), tensor([[ 0.1832],\n",
      "        [ 0.1512],\n",
      "        [ 0.2358],\n",
      "        [-0.2202],\n",
      "        [-0.0243],\n",
      "        [ 0.1551],\n",
      "        [ 0.1186],\n",
      "        [ 0.2002]], grad_fn=<AddmmBackward0>), tensor([[ 0.0306],\n",
      "        [-0.0717],\n",
      "        [ 0.1625],\n",
      "        [ 0.3056],\n",
      "        [ 0.1421],\n",
      "        [ 0.0950],\n",
      "        [ 0.1983],\n",
      "        [ 0.0798]], grad_fn=<AddmmBackward0>), tensor([[ 0.1822],\n",
      "        [ 0.1471],\n",
      "        [ 0.2903],\n",
      "        [-0.0355],\n",
      "        [-0.2106],\n",
      "        [ 0.0306],\n",
      "        [ 0.1368],\n",
      "        [-0.0072]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0453],\n",
      "        [ 0.0526],\n",
      "        [ 0.0088],\n",
      "        [ 0.0269],\n",
      "        [ 0.1700],\n",
      "        [ 0.4430],\n",
      "        [-0.0513],\n",
      "        [ 0.0395]], grad_fn=<AddmmBackward0>), tensor([[-0.0899],\n",
      "        [ 0.0280],\n",
      "        [ 0.1092],\n",
      "        [-0.0305],\n",
      "        [ 0.0222],\n",
      "        [ 0.0468],\n",
      "        [ 0.0299],\n",
      "        [ 0.0732]], grad_fn=<AddmmBackward0>), tensor([[ 0.0668],\n",
      "        [ 0.0430],\n",
      "        [ 0.0711],\n",
      "        [-0.0677],\n",
      "        [ 0.0988],\n",
      "        [ 0.0612],\n",
      "        [ 0.1179],\n",
      "        [ 0.2685]], grad_fn=<AddmmBackward0>), tensor([[0.2325],\n",
      "        [0.0548],\n",
      "        [0.2350],\n",
      "        [0.0040],\n",
      "        [0.0178],\n",
      "        [0.0326],\n",
      "        [0.2067],\n",
      "        [0.3172]], grad_fn=<AddmmBackward0>), tensor([[0.1055],\n",
      "        [0.0511],\n",
      "        [0.4384],\n",
      "        [0.1764],\n",
      "        [0.1664],\n",
      "        [0.0514],\n",
      "        [0.2883],\n",
      "        [0.0134]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9999e-01, 1.3130e-05],\n",
      "        [9.9998e-01, 1.5537e-05],\n",
      "        [9.9997e-01, 3.1719e-05],\n",
      "        [1.3008e-05, 9.9999e-01],\n",
      "        [3.1036e-05, 9.9997e-01],\n",
      "        [2.1375e-04, 9.9979e-01],\n",
      "        [3.2270e-04, 9.9968e-01],\n",
      "        [9.9996e-01, 4.4796e-05]], grad_fn=<SoftmaxBackward0>), tensor([[7.7998e-06, 9.9999e-01],\n",
      "        [6.7906e-04, 9.9932e-01],\n",
      "        [6.2561e-04, 9.9937e-01],\n",
      "        [9.9999e-01, 1.4129e-05],\n",
      "        [9.9996e-01, 3.8121e-05],\n",
      "        [9.9489e-01, 5.1140e-03],\n",
      "        [9.9990e-01, 9.6534e-05],\n",
      "        [2.5373e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9981e-01, 1.8771e-04],\n",
      "        [9.9997e-01, 3.0276e-05],\n",
      "        [9.9970e-01, 3.0293e-04],\n",
      "        [9.2468e-06, 9.9999e-01],\n",
      "        [6.8105e-05, 9.9993e-01],\n",
      "        [6.2851e-02, 9.3715e-01],\n",
      "        [8.6336e-04, 9.9914e-01],\n",
      "        [9.9992e-01, 7.9701e-05]], grad_fn=<SoftmaxBackward0>), tensor([[6.9811e-04, 9.9930e-01],\n",
      "        [2.6982e-04, 9.9973e-01],\n",
      "        [6.0125e-04, 9.9940e-01],\n",
      "        [9.9998e-01, 1.9955e-05],\n",
      "        [9.9999e-01, 1.0751e-05],\n",
      "        [8.1969e-01, 1.8031e-01],\n",
      "        [9.9952e-01, 4.8045e-04],\n",
      "        [1.5274e-04, 9.9985e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9878e-01, 1.2235e-03],\n",
      "        [9.9984e-01, 1.5586e-04],\n",
      "        [9.9989e-01, 1.1014e-04],\n",
      "        [1.1510e-04, 9.9988e-01],\n",
      "        [7.8977e-06, 9.9999e-01],\n",
      "        [4.1324e-02, 9.5868e-01],\n",
      "        [6.1428e-04, 9.9939e-01],\n",
      "        [9.8772e-01, 1.2277e-02]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.0400, 0.0000, 0.0000, 0.4400, 0.0000, 0.1200, 0.2400, 0.0000]), tensor([0.1600, 0.0400, 0.0000, 0.0000, 0.4400, 0.0000, 0.1200, 0.2400, 0.0000]), 0.29331690015701145, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 1\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.4000, 0.0000, 0.0400, 0.0400, 0.0000, 0.0400, 0.0800, 0.2400, 0.1600]), tensor([0.4000, 0.0000, 0.0400, 0.0400, 0.0000, 0.0400, 0.0800, 0.2400, 0.1600]), 0.4192710816860199, tensor(0))\n",
      "action: 0\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.2400, 0.0400, 0.0000, 0.4000, 0.2000, 0.1200, 0.0000]), tensor([0.0000, 0.0000, 0.2400, 0.0400, 0.0000, 0.4000, 0.2000, 0.1200, 0.0000]), 0.45482996141967863, tensor(5))\n",
      "action: 5\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.6800, 0.1200, 0.0000]), tensor([0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.6800, 0.1200, 0.0000]), 0.5823950352003942, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 2\n",
      "Player 0 prediction: (tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), 0.4933195042495544, tensor(7))\n",
      "action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 6, 7, 0, 2],\n",
      "        [1, 5, 0, 7, 3],\n",
      "        [6, 3, 5, 2, 1],\n",
      "        [4, 8, 7, 0, 1],\n",
      "        [8, 5, 0, 1, 3],\n",
      "        [4, 6, 8, 1, 2],\n",
      "        [7, 2, 0, 4, 5],\n",
      "        [7, 0, 5, 4, 5]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.4200],\n",
      "        [ 0.3718],\n",
      "        [-0.1639],\n",
      "        [-0.3175],\n",
      "        [-0.2291],\n",
      "        [ 0.2206],\n",
      "        [-0.2897],\n",
      "        [ 0.1560]], grad_fn=<AddmmBackward0>), tensor([[-0.2128],\n",
      "        [-0.0653],\n",
      "        [ 0.0561],\n",
      "        [ 0.0471],\n",
      "        [ 0.1244],\n",
      "        [-0.4497],\n",
      "        [ 0.2890],\n",
      "        [ 0.1132]], grad_fn=<AddmmBackward0>), tensor([[ 0.1754],\n",
      "        [ 0.1726],\n",
      "        [-0.1190],\n",
      "        [-0.1530],\n",
      "        [-0.2277],\n",
      "        [ 0.2644],\n",
      "        [-0.1447],\n",
      "        [-0.0367]], grad_fn=<AddmmBackward0>), tensor([[-0.0395],\n",
      "        [-0.1115],\n",
      "        [ 0.1349],\n",
      "        [ 0.1972],\n",
      "        [-0.0129],\n",
      "        [-0.2512],\n",
      "        [-0.0891],\n",
      "        [-0.0836]], grad_fn=<AddmmBackward0>), tensor([[-0.1150],\n",
      "        [ 0.1038],\n",
      "        [-0.1336],\n",
      "        [-0.2401],\n",
      "        [-0.0506],\n",
      "        [ 0.2605],\n",
      "        [-0.1571],\n",
      "        [-0.0784]], grad_fn=<AddmmBackward0>), tensor([[ 0.0728],\n",
      "        [-0.1710],\n",
      "        [ 0.2418],\n",
      "        [ 0.2557],\n",
      "        [ 0.1906],\n",
      "        [-0.0930],\n",
      "        [-0.0748],\n",
      "        [-0.0792]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.0541],\n",
      "        [0.0572],\n",
      "        [0.0278],\n",
      "        [0.0460],\n",
      "        [0.0547],\n",
      "        [0.0600],\n",
      "        [0.1720],\n",
      "        [0.6664]], grad_fn=<AddmmBackward0>), tensor([[0.0047],\n",
      "        [0.3548],\n",
      "        [0.0641],\n",
      "        [0.0344],\n",
      "        [0.0329],\n",
      "        [0.0657],\n",
      "        [0.4962],\n",
      "        [0.0534]], grad_fn=<AddmmBackward0>), tensor([[0.2037],\n",
      "        [0.3373],\n",
      "        [0.0353],\n",
      "        [0.0858],\n",
      "        [0.1484],\n",
      "        [0.1241],\n",
      "        [0.0325],\n",
      "        [0.0102]], grad_fn=<AddmmBackward0>), tensor([[ 0.0955],\n",
      "        [ 0.2754],\n",
      "        [ 0.3753],\n",
      "        [ 0.1528],\n",
      "        [ 0.5281],\n",
      "        [-0.0155],\n",
      "        [-0.0317],\n",
      "        [ 0.0576]], grad_fn=<AddmmBackward0>), tensor([[ 0.0991],\n",
      "        [ 0.4366],\n",
      "        [ 0.4134],\n",
      "        [ 0.2458],\n",
      "        [ 0.5778],\n",
      "        [ 0.2995],\n",
      "        [-0.0110],\n",
      "        [ 0.0147]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0240e-04, 9.9990e-01],\n",
      "        [4.3712e-05, 9.9996e-01],\n",
      "        [9.9997e-01, 2.7183e-05],\n",
      "        [9.9997e-01, 2.5078e-05],\n",
      "        [9.9999e-01, 1.0045e-05],\n",
      "        [1.7017e-06, 1.0000e+00],\n",
      "        [9.9922e-01, 7.7763e-04],\n",
      "        [3.4858e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9991e-01, 9.1585e-05],\n",
      "        [9.9999e-01, 1.1737e-05],\n",
      "        [2.2360e-05, 9.9998e-01],\n",
      "        [3.7459e-06, 1.0000e+00],\n",
      "        [2.7235e-05, 9.9997e-01],\n",
      "        [9.9990e-01, 1.0347e-04],\n",
      "        [2.2140e-04, 9.9978e-01],\n",
      "        [9.9268e-01, 7.3229e-03]], grad_fn=<SoftmaxBackward0>), tensor([[5.5609e-05, 9.9994e-01],\n",
      "        [3.8899e-04, 9.9961e-01],\n",
      "        [9.9991e-01, 8.9180e-05],\n",
      "        [9.9999e-01, 8.4709e-06],\n",
      "        [9.9985e-01, 1.5448e-04],\n",
      "        [1.9501e-06, 1.0000e+00],\n",
      "        [9.9337e-01, 6.6265e-03],\n",
      "        [2.7112e-02, 9.7289e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9984e-01, 1.5535e-04],\n",
      "        [9.9950e-01, 5.0185e-04],\n",
      "        [6.0736e-06, 9.9999e-01],\n",
      "        [2.5211e-05, 9.9997e-01],\n",
      "        [5.5065e-05, 9.9994e-01],\n",
      "        [9.9992e-01, 7.9045e-05],\n",
      "        [9.3246e-03, 9.9068e-01],\n",
      "        [9.1689e-01, 8.3111e-02]], grad_fn=<SoftmaxBackward0>), tensor([[1.2385e-04, 9.9988e-01],\n",
      "        [7.5296e-05, 9.9992e-01],\n",
      "        [9.9999e-01, 9.1554e-06],\n",
      "        [9.9984e-01, 1.5846e-04],\n",
      "        [9.9995e-01, 5.2228e-05],\n",
      "        [1.1224e-05, 9.9999e-01],\n",
      "        [7.3005e-01, 2.6995e-01],\n",
      "        [2.6375e-03, 9.9736e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 92.0 and average score: 0.9\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.0000, 0.0400, 0.0000, 0.3600, 0.0800, 0.1600, 0.0800, 0.0800]), tensor([0.2000, 0.0000, 0.0400, 0.0000, 0.3600, 0.0800, 0.1600, 0.0800, 0.0800]), -0.14698638064930072, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.0000, 0.1200, 0.0000, 0.0000, 0.1600, 0.2800, 0.1200, 0.1200]), tensor([0.2000, 0.0000, 0.1200, 0.0000, 0.0000, 0.1600, 0.2800, 0.1200, 0.1200]), 0.042408113152934954, tensor(6))\n",
      "action: 6\n",
      "Player 0 random action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.5600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.0000, 0.1600, 0.2400]), tensor([0.5600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.0000, 0.1600, 0.2400]), 0.07003511796490504, tensor(0))\n",
      "action: 0\n",
      "Player 0 random action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.0000, 0.4000, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.0000, 0.4000, 0.0000]), -0.24163772631436586, tensor(5))\n",
      "action: 5\n",
      "Player 0 random action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 4, 7, 0, 0],\n",
      "        [5, 7, 0, 6, 4],\n",
      "        [1, 0, 3, 6, 4],\n",
      "        [8, 3, 5, 7, 4],\n",
      "        [0, 2, 5, 8, 1],\n",
      "        [0, 2, 3, 0, 4],\n",
      "        [4, 6, 0, 7, 3],\n",
      "        [2, 7, 0, 3, 5]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.4216],\n",
      "        [ 0.3350],\n",
      "        [ 0.3160],\n",
      "        [ 0.0843],\n",
      "        [ 0.4138],\n",
      "        [ 0.1037],\n",
      "        [ 0.2676],\n",
      "        [-0.1104]], grad_fn=<AddmmBackward0>), tensor([[-0.1468],\n",
      "        [ 0.1159],\n",
      "        [ 0.2965],\n",
      "        [ 0.1299],\n",
      "        [-0.2894],\n",
      "        [ 0.2163],\n",
      "        [-0.2423],\n",
      "        [ 0.1820]], grad_fn=<AddmmBackward0>), tensor([[0.2339],\n",
      "        [0.4071],\n",
      "        [0.0874],\n",
      "        [0.0898],\n",
      "        [0.3402],\n",
      "        [0.0574],\n",
      "        [0.3526],\n",
      "        [0.0861]], grad_fn=<AddmmBackward0>), tensor([[ 0.2178],\n",
      "        [ 0.0988],\n",
      "        [ 0.0521],\n",
      "        [ 0.0364],\n",
      "        [-0.1620],\n",
      "        [ 0.3170],\n",
      "        [-0.2568],\n",
      "        [ 0.2053]], grad_fn=<AddmmBackward0>), tensor([[ 0.1753],\n",
      "        [ 0.1191],\n",
      "        [ 0.0392],\n",
      "        [ 0.2871],\n",
      "        [ 0.3005],\n",
      "        [-0.0044],\n",
      "        [ 0.4632],\n",
      "        [ 0.1079]], grad_fn=<AddmmBackward0>), tensor([[ 0.0421],\n",
      "        [ 0.0159],\n",
      "        [ 0.0217],\n",
      "        [-0.1402],\n",
      "        [ 0.0673],\n",
      "        [ 0.0182],\n",
      "        [-0.1201],\n",
      "        [ 0.2102]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0076],\n",
      "        [ 0.5314],\n",
      "        [ 0.4749],\n",
      "        [ 0.0095],\n",
      "        [ 0.0331],\n",
      "        [ 0.1744],\n",
      "        [ 0.0494],\n",
      "        [ 0.0401]], grad_fn=<AddmmBackward0>), tensor([[ 0.1991],\n",
      "        [ 0.7082],\n",
      "        [ 0.0337],\n",
      "        [-0.0960],\n",
      "        [-0.0377],\n",
      "        [ 0.4815],\n",
      "        [ 0.0945],\n",
      "        [ 0.0231]], grad_fn=<AddmmBackward0>), tensor([[0.1980],\n",
      "        [0.0537],\n",
      "        [0.0239],\n",
      "        [0.0482],\n",
      "        [0.0363],\n",
      "        [0.5855],\n",
      "        [0.0986],\n",
      "        [0.2076]], grad_fn=<AddmmBackward0>), tensor([[ 0.2698],\n",
      "        [ 0.1300],\n",
      "        [ 0.0424],\n",
      "        [-0.1014],\n",
      "        [ 0.0922],\n",
      "        [ 0.0534],\n",
      "        [ 0.1179],\n",
      "        [ 0.1345]], grad_fn=<AddmmBackward0>), tensor([[0.0460],\n",
      "        [0.0647],\n",
      "        [0.1428],\n",
      "        [0.0504],\n",
      "        [0.1821],\n",
      "        [0.0602],\n",
      "        [0.1613],\n",
      "        [0.2323]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[4.1593e-05, 9.9996e-01],\n",
      "        [2.9755e-05, 9.9997e-01],\n",
      "        [9.9988e-01, 1.1862e-04],\n",
      "        [9.9999e-01, 1.3642e-05],\n",
      "        [2.9289e-05, 9.9997e-01],\n",
      "        [9.9982e-01, 1.7774e-04],\n",
      "        [1.8718e-06, 1.0000e+00],\n",
      "        [9.9993e-01, 7.1663e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 3.2782e-06],\n",
      "        [9.9995e-01, 4.5058e-05],\n",
      "        [4.8858e-01, 5.1142e-01],\n",
      "        [1.5979e-04, 9.9984e-01],\n",
      "        [9.9995e-01, 5.0807e-05],\n",
      "        [4.2594e-05, 9.9996e-01],\n",
      "        [9.9994e-01, 5.9157e-05],\n",
      "        [2.8291e-04, 9.9972e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.2908e-04, 9.9987e-01],\n",
      "        [3.2466e-01, 6.7534e-01],\n",
      "        [3.1271e-01, 6.8729e-01],\n",
      "        [9.9967e-01, 3.3180e-04],\n",
      "        [1.7384e-05, 9.9998e-01],\n",
      "        [9.9994e-01, 5.8956e-05],\n",
      "        [2.6461e-05, 9.9997e-01],\n",
      "        [9.9955e-01, 4.5259e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9995e-01, 5.0782e-05],\n",
      "        [9.6286e-01, 3.7145e-02],\n",
      "        [4.6759e-01, 5.3241e-01],\n",
      "        [2.7833e-03, 9.9722e-01],\n",
      "        [9.9995e-01, 4.8945e-05],\n",
      "        [4.3368e-02, 9.5663e-01],\n",
      "        [9.9995e-01, 4.8824e-05],\n",
      "        [1.1352e-03, 9.9886e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.7856e-02, 9.4214e-01],\n",
      "        [2.8194e-01, 7.1806e-01],\n",
      "        [4.1836e-02, 9.5816e-01],\n",
      "        [9.9490e-01, 5.1019e-03],\n",
      "        [2.3093e-05, 9.9998e-01],\n",
      "        [6.3281e-01, 3.6719e-01],\n",
      "        [6.0482e-06, 9.9999e-01],\n",
      "        [9.9966e-01, 3.4043e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 64.0 and average score: 0.32\n",
      "Results vs random: {'player_0_score': 0.9, 'player_0_win%': 0.92, 'player_1_score': 0.32, 'player_1_win%': 0.64, 'score': 0.61}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.0800, 0.0000, 0.0400, 0.5600, 0.0400, 0.0000, 0.1200, 0.0000]), tensor([0.1600, 0.0800, 0.0000, 0.0400, 0.5600, 0.0400, 0.0000, 0.1200, 0.0000]), 0.2889335583179043, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 2\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.3200, 0.0800, 0.0000, 0.0400, 0.0000, 0.0800, 0.0400, 0.3200, 0.1200]), tensor([0.3200, 0.0800, 0.0000, 0.0400, 0.0000, 0.0800, 0.0400, 0.3200, 0.1200]), 0.20479652400200182, tensor(0))\n",
      "action: 0\n",
      "Player 1 tictactoe_expert action: 8\n",
      "Player 0 prediction: (tensor([0.0000, 0.2800, 0.0000, 0.0800, 0.0000, 0.2000, 0.2400, 0.2000, 0.0000]), tensor([0.0000, 0.2800, 0.0000, 0.0800, 0.0000, 0.2000, 0.2400, 0.2000, 0.0000]), 0.3042952987150504, tensor(1))\n",
      "action: 1\n",
      "Player 1 tictactoe_expert action: 5\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 2, 4, 1, 5],\n",
      "        [5, 6, 7, 4, 0],\n",
      "        [0, 2, 5, 1, 3],\n",
      "        [4, 8, 0, 6, 2],\n",
      "        [1, 8, 2, 0, 4],\n",
      "        [1, 3, 8, 0, 0],\n",
      "        [3, 2, 0, 1, 4],\n",
      "        [6, 2, 4, 5, 7]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.2146],\n",
      "        [ 0.2850],\n",
      "        [ 0.2822],\n",
      "        [ 0.2850],\n",
      "        [-0.2928],\n",
      "        [ 0.4183],\n",
      "        [ 0.3212],\n",
      "        [-0.0030]], grad_fn=<AddmmBackward0>), tensor([[-0.2746],\n",
      "        [-0.1113],\n",
      "        [-0.0300],\n",
      "        [-0.1819],\n",
      "        [ 0.2690],\n",
      "        [-0.2241],\n",
      "        [ 0.0474],\n",
      "        [ 0.2686]], grad_fn=<AddmmBackward0>), tensor([[ 0.3752],\n",
      "        [ 0.2029],\n",
      "        [ 0.2728],\n",
      "        [ 0.3300],\n",
      "        [ 0.0014],\n",
      "        [ 0.3603],\n",
      "        [ 0.1161],\n",
      "        [-0.0965]], grad_fn=<AddmmBackward0>), tensor([[-0.0566],\n",
      "        [ 0.0417],\n",
      "        [-0.0938],\n",
      "        [-0.0708],\n",
      "        [ 0.1714],\n",
      "        [-0.0226],\n",
      "        [ 0.0145],\n",
      "        [ 0.2519]], grad_fn=<AddmmBackward0>), tensor([[ 0.3183],\n",
      "        [-0.1155],\n",
      "        [ 0.3048],\n",
      "        [ 0.1793],\n",
      "        [ 0.0256],\n",
      "        [ 0.1892],\n",
      "        [ 0.0913],\n",
      "        [ 0.1308]], grad_fn=<AddmmBackward0>), tensor([[-0.0971],\n",
      "        [ 0.0246],\n",
      "        [ 0.1340],\n",
      "        [ 0.0414],\n",
      "        [ 0.0467],\n",
      "        [-0.0147],\n",
      "        [-0.0507],\n",
      "        [ 0.2713]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0286],\n",
      "        [ 0.0402],\n",
      "        [ 0.0733],\n",
      "        [ 0.0429],\n",
      "        [ 0.0672],\n",
      "        [-0.0253],\n",
      "        [ 0.0684],\n",
      "        [ 0.1075]], grad_fn=<AddmmBackward0>), tensor([[ 0.0492],\n",
      "        [ 0.0295],\n",
      "        [ 0.0039],\n",
      "        [ 0.0059],\n",
      "        [ 0.2691],\n",
      "        [-0.0360],\n",
      "        [ 0.3388],\n",
      "        [-0.0452]], grad_fn=<AddmmBackward0>), tensor([[ 0.2199],\n",
      "        [ 0.0218],\n",
      "        [ 0.0681],\n",
      "        [ 0.1354],\n",
      "        [ 0.5655],\n",
      "        [ 0.2070],\n",
      "        [-0.0295],\n",
      "        [ 0.4182]], grad_fn=<AddmmBackward0>), tensor([[0.1235],\n",
      "        [0.0458],\n",
      "        [0.1934],\n",
      "        [0.0312],\n",
      "        [0.0260],\n",
      "        [0.1157],\n",
      "        [0.0887],\n",
      "        [0.7052]], grad_fn=<AddmmBackward0>), tensor([[0.1650],\n",
      "        [0.0707],\n",
      "        [0.5731],\n",
      "        [0.1431],\n",
      "        [0.0314],\n",
      "        [0.0437],\n",
      "        [0.0617],\n",
      "        [0.4826]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.0178e-06, 1.0000e+00],\n",
      "        [3.5414e-06, 1.0000e+00],\n",
      "        [4.6073e-06, 1.0000e+00],\n",
      "        [1.5419e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 1.0238e-05],\n",
      "        [4.6559e-06, 1.0000e+00],\n",
      "        [3.4538e-05, 9.9997e-01],\n",
      "        [9.9994e-01, 5.5417e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 3.1603e-05],\n",
      "        [9.9997e-01, 3.3818e-05],\n",
      "        [9.9989e-01, 1.1150e-04],\n",
      "        [9.9995e-01, 5.2751e-05],\n",
      "        [3.2403e-06, 1.0000e+00],\n",
      "        [9.9998e-01, 2.1399e-05],\n",
      "        [9.9992e-01, 7.8157e-05],\n",
      "        [4.1635e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[6.0862e-06, 9.9999e-01],\n",
      "        [2.8859e-05, 9.9997e-01],\n",
      "        [1.5111e-05, 9.9998e-01],\n",
      "        [6.8362e-06, 9.9999e-01],\n",
      "        [9.9980e-01, 1.9837e-04],\n",
      "        [4.6168e-06, 1.0000e+00],\n",
      "        [3.1785e-03, 9.9682e-01],\n",
      "        [9.9997e-01, 3.4920e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9971e-01, 2.8685e-04],\n",
      "        [9.9978e-01, 2.1553e-04],\n",
      "        [9.9991e-01, 8.9814e-05],\n",
      "        [9.9996e-01, 3.6785e-05],\n",
      "        [4.8911e-03, 9.9511e-01],\n",
      "        [9.9998e-01, 2.4808e-05],\n",
      "        [9.1115e-01, 8.8848e-02],\n",
      "        [1.4934e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[3.4321e-05, 9.9997e-01],\n",
      "        [1.6042e-05, 9.9998e-01],\n",
      "        [2.4464e-05, 9.9998e-01],\n",
      "        [9.7642e-07, 1.0000e+00],\n",
      "        [8.4376e-01, 1.5624e-01],\n",
      "        [1.2996e-03, 9.9870e-01],\n",
      "        [5.3424e-03, 9.9466e-01],\n",
      "        [9.9987e-01, 1.3137e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 18.0 and average score: -0.58\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 6\n",
      "Player 1 prediction: (tensor([0.2400, 0.0000, 0.1600, 0.0400, 0.3600, 0.0400, 0.0000, 0.0800, 0.0800]), tensor([0.2400, 0.0000, 0.1600, 0.0400, 0.3600, 0.0400, 0.0000, 0.0800, 0.0800]), -0.23512469196262267, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 7\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.1600, 0.0800, 0.0400, 0.0000, 0.3200, 0.0000, 0.0000, 0.2000]), tensor([0.2000, 0.1600, 0.0800, 0.0400, 0.0000, 0.3200, 0.0000, 0.0000, 0.2000]), 0.21518252352969006, tensor(5))\n",
      "action: 5\n",
      "Player 0 tictactoe_expert action: 8\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 3, 7, 4, 0],\n",
      "        [0, 0, 2, 4, 7],\n",
      "        [2, 4, 8, 3, 0],\n",
      "        [6, 5, 2, 0, 0],\n",
      "        [1, 3, 2, 0, 7],\n",
      "        [6, 4, 0, 2, 3],\n",
      "        [4, 0, 6, 1, 2],\n",
      "        [4, 8, 7, 6, 2]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[-0.2184],\n",
      "        [ 0.5763],\n",
      "        [ 0.4924],\n",
      "        [-0.5254],\n",
      "        [-0.3867],\n",
      "        [ 0.3483],\n",
      "        [ 0.3483],\n",
      "        [-0.3511]], grad_fn=<AddmmBackward0>), tensor([[ 0.0932],\n",
      "        [ 0.1047],\n",
      "        [-0.3857],\n",
      "        [ 0.4838],\n",
      "        [ 0.3911],\n",
      "        [-0.3119],\n",
      "        [-0.4847],\n",
      "        [ 0.1354]], grad_fn=<AddmmBackward0>), tensor([[-0.1072],\n",
      "        [-0.0028],\n",
      "        [ 0.3795],\n",
      "        [-0.4781],\n",
      "        [-0.3293],\n",
      "        [ 0.2928],\n",
      "        [ 0.2653],\n",
      "        [-0.1403]], grad_fn=<AddmmBackward0>), tensor([[ 0.2933],\n",
      "        [-0.0591],\n",
      "        [-0.0007],\n",
      "        [ 0.5292],\n",
      "        [ 0.2699],\n",
      "        [-0.3697],\n",
      "        [-0.2937],\n",
      "        [ 0.2557]], grad_fn=<AddmmBackward0>), tensor([[-0.1002],\n",
      "        [-0.0555],\n",
      "        [ 0.3619],\n",
      "        [-0.2274],\n",
      "        [-0.1449],\n",
      "        [ 0.2886],\n",
      "        [ 0.3096],\n",
      "        [-0.1501]], grad_fn=<AddmmBackward0>), tensor([[ 0.0046],\n",
      "        [ 0.0959],\n",
      "        [-0.0729],\n",
      "        [ 0.1231],\n",
      "        [ 0.0620],\n",
      "        [-0.1048],\n",
      "        [-0.3088],\n",
      "        [ 0.3536]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1124],\n",
      "        [ 1.0127],\n",
      "        [ 0.0767],\n",
      "        [ 0.1365],\n",
      "        [ 0.0252],\n",
      "        [ 0.0149],\n",
      "        [-0.0072],\n",
      "        [ 0.1097]], grad_fn=<AddmmBackward0>), tensor([[ 0.0643],\n",
      "        [ 0.0011],\n",
      "        [ 0.2469],\n",
      "        [-0.0186],\n",
      "        [ 0.0244],\n",
      "        [ 0.0340],\n",
      "        [ 0.0534],\n",
      "        [-0.0727]], grad_fn=<AddmmBackward0>), tensor([[0.1322],\n",
      "        [0.0382],\n",
      "        [0.8264],\n",
      "        [0.1460],\n",
      "        [0.4068],\n",
      "        [0.0513],\n",
      "        [0.0157],\n",
      "        [0.0867]], grad_fn=<AddmmBackward0>), tensor([[ 0.4639],\n",
      "        [-0.0113],\n",
      "        [ 0.6468],\n",
      "        [ 0.0668],\n",
      "        [-0.0547],\n",
      "        [-0.0056],\n",
      "        [ 0.1756],\n",
      "        [ 0.2080]], grad_fn=<AddmmBackward0>), tensor([[ 0.0419],\n",
      "        [ 0.0601],\n",
      "        [-0.0043],\n",
      "        [ 0.0132],\n",
      "        [ 0.0502],\n",
      "        [ 0.3719],\n",
      "        [ 0.4655],\n",
      "        [ 0.1748]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9999e-01, 8.3937e-06],\n",
      "        [1.3935e-04, 9.9986e-01],\n",
      "        [1.0973e-04, 9.9989e-01],\n",
      "        [9.9999e-01, 1.1941e-05],\n",
      "        [9.9999e-01, 6.1296e-06],\n",
      "        [5.6892e-07, 1.0000e+00],\n",
      "        [2.9060e-06, 1.0000e+00],\n",
      "        [9.9998e-01, 2.3384e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.9163e-05, 9.9998e-01],\n",
      "        [9.8880e-01, 1.1199e-02],\n",
      "        [9.9997e-01, 3.2786e-05],\n",
      "        [1.0190e-05, 9.9999e-01],\n",
      "        [2.8153e-05, 9.9997e-01],\n",
      "        [1.0000e+00, 4.1116e-06],\n",
      "        [9.9998e-01, 1.5706e-05],\n",
      "        [4.4793e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.0368e-05],\n",
      "        [1.2156e-02, 9.8784e-01],\n",
      "        [1.5616e-05, 9.9998e-01],\n",
      "        [9.9972e-01, 2.7896e-04],\n",
      "        [9.9992e-01, 7.7694e-05],\n",
      "        [4.0543e-06, 1.0000e+00],\n",
      "        [7.5398e-06, 9.9999e-01],\n",
      "        [9.9987e-01, 1.2538e-04]], grad_fn=<SoftmaxBackward0>), tensor([[2.6039e-05, 9.9997e-01],\n",
      "        [9.9676e-01, 3.2408e-03],\n",
      "        [9.9996e-01, 4.4706e-05],\n",
      "        [1.0338e-04, 9.9990e-01],\n",
      "        [1.1305e-03, 9.9887e-01],\n",
      "        [9.9993e-01, 6.7035e-05],\n",
      "        [9.9986e-01, 1.3968e-04],\n",
      "        [5.2856e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.6596e-01, 3.4040e-02],\n",
      "        [1.8707e-04, 9.9981e-01],\n",
      "        [1.1380e-02, 9.8862e-01],\n",
      "        [9.9982e-01, 1.8161e-04],\n",
      "        [5.7220e-01, 4.2780e-01],\n",
      "        [1.0607e-05, 9.9999e-01],\n",
      "        [2.2074e-06, 1.0000e+00],\n",
      "        [9.9984e-01, 1.6412e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 2.0 and average score: -0.82\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.58, 'player_0_win%': 0.18, 'player_1_score': -0.82, 'player_1_win%': 0.02, 'score': -0.7}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 1, 0, 0, 7],\n",
      "        [7, 0, 3, 8, 7],\n",
      "        [2, 1, 5, 7, 0],\n",
      "        [4, 2, 8, 7, 1],\n",
      "        [0, 2, 4, 7, 8],\n",
      "        [1, 4, 0, 6, 2],\n",
      "        [6, 7, 0, 8, 7],\n",
      "        [3, 0, 3, 8, 7]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.5681],\n",
      "        [ 0.0692],\n",
      "        [-0.1816],\n",
      "        [ 0.0827],\n",
      "        [-0.2989],\n",
      "        [-0.2377],\n",
      "        [-0.1191],\n",
      "        [ 0.1892]], grad_fn=<AddmmBackward0>), tensor([[-0.4925],\n",
      "        [-0.0332],\n",
      "        [ 0.3429],\n",
      "        [-0.2983],\n",
      "        [ 0.1149],\n",
      "        [ 0.4387],\n",
      "        [ 0.3196],\n",
      "        [-0.0719]], grad_fn=<AddmmBackward0>), tensor([[ 0.6487],\n",
      "        [ 0.0721],\n",
      "        [-0.0222],\n",
      "        [ 0.3910],\n",
      "        [-0.0181],\n",
      "        [-0.5994],\n",
      "        [ 0.2732],\n",
      "        [ 0.0495]], grad_fn=<AddmmBackward0>), tensor([[-0.2853],\n",
      "        [ 0.0632],\n",
      "        [ 0.3808],\n",
      "        [ 0.0696],\n",
      "        [-0.0554],\n",
      "        [ 0.5809],\n",
      "        [ 0.0099],\n",
      "        [ 0.0217]], grad_fn=<AddmmBackward0>), tensor([[ 0.0721],\n",
      "        [ 0.1783],\n",
      "        [ 0.1158],\n",
      "        [ 0.4459],\n",
      "        [ 0.3254],\n",
      "        [-0.4532],\n",
      "        [ 0.0491],\n",
      "        [ 0.0873]], grad_fn=<AddmmBackward0>), tensor([[-0.0040],\n",
      "        [ 0.0592],\n",
      "        [ 0.0835],\n",
      "        [ 0.0279],\n",
      "        [ 0.2268],\n",
      "        [ 0.6349],\n",
      "        [ 0.0794],\n",
      "        [ 0.0654]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 3.7382e-02],\n",
      "        [ 4.4607e-01],\n",
      "        [ 9.0003e-02],\n",
      "        [-1.1407e-02],\n",
      "        [-4.1338e-02],\n",
      "        [-2.6490e-04],\n",
      "        [ 1.7826e-01],\n",
      "        [ 4.8064e-01]], grad_fn=<AddmmBackward0>), tensor([[ 0.1336],\n",
      "        [ 0.0908],\n",
      "        [-0.1130],\n",
      "        [-0.0500],\n",
      "        [-0.0169],\n",
      "        [ 0.0423],\n",
      "        [ 0.5029],\n",
      "        [-0.0257]], grad_fn=<AddmmBackward0>), tensor([[ 0.4298],\n",
      "        [ 0.0235],\n",
      "        [ 0.4959],\n",
      "        [ 0.1331],\n",
      "        [-0.0610],\n",
      "        [-0.0222],\n",
      "        [-0.0382],\n",
      "        [ 0.0105]], grad_fn=<AddmmBackward0>), tensor([[-0.0231],\n",
      "        [ 0.1483],\n",
      "        [ 0.7151],\n",
      "        [ 0.1673],\n",
      "        [ 0.0681],\n",
      "        [-0.0188],\n",
      "        [-0.0113],\n",
      "        [-0.0240]], grad_fn=<AddmmBackward0>), tensor([[ 0.0138],\n",
      "        [ 0.3013],\n",
      "        [-0.0188],\n",
      "        [ 0.3927],\n",
      "        [ 0.1325],\n",
      "        [ 0.0701],\n",
      "        [ 0.0009],\n",
      "        [ 0.1385]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.3812e-05, 9.9997e-01],\n",
      "        [1.0357e-04, 9.9990e-01],\n",
      "        [9.9997e-01, 2.6000e-05],\n",
      "        [1.9924e-05, 9.9998e-01],\n",
      "        [9.9999e-01, 1.1006e-05],\n",
      "        [9.9999e-01, 5.5780e-06],\n",
      "        [9.9998e-01, 2.0712e-05],\n",
      "        [1.2987e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9985e-01, 1.5269e-04],\n",
      "        [9.9916e-01, 8.4224e-04],\n",
      "        [5.7979e-05, 9.9994e-01],\n",
      "        [9.9989e-01, 1.1489e-04],\n",
      "        [1.5564e-05, 9.9998e-01],\n",
      "        [4.6365e-06, 1.0000e+00],\n",
      "        [1.1919e-05, 9.9999e-01],\n",
      "        [9.9230e-01, 7.7028e-03]], grad_fn=<SoftmaxBackward0>), tensor([[2.1109e-05, 9.9998e-01],\n",
      "        [2.5595e-05, 9.9997e-01],\n",
      "        [9.9998e-01, 2.2612e-05],\n",
      "        [1.5496e-05, 9.9998e-01],\n",
      "        [9.9995e-01, 5.0103e-05],\n",
      "        [9.9986e-01, 1.4129e-04],\n",
      "        [9.9794e-01, 2.0623e-03],\n",
      "        [2.3345e-03, 9.9767e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9881e-01, 1.1902e-03],\n",
      "        [9.9998e-01, 1.8471e-05],\n",
      "        [1.1424e-05, 9.9999e-01],\n",
      "        [9.9978e-01, 2.1935e-04],\n",
      "        [2.4198e-05, 9.9998e-01],\n",
      "        [1.9572e-05, 9.9998e-01],\n",
      "        [4.4345e-03, 9.9557e-01],\n",
      "        [9.8196e-01, 1.8037e-02]], grad_fn=<SoftmaxBackward0>), tensor([[7.0090e-04, 9.9930e-01],\n",
      "        [4.4864e-06, 1.0000e+00],\n",
      "        [9.8041e-01, 1.9594e-02],\n",
      "        [1.4309e-05, 9.9999e-01],\n",
      "        [9.9992e-01, 8.2523e-05],\n",
      "        [9.9995e-01, 5.1794e-05],\n",
      "        [8.3001e-01, 1.6999e-01],\n",
      "        [1.9476e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 6, 2, 0, 1],\n",
      "        [5, 3, 7, 1, 4],\n",
      "        [3, 6, 7, 2, 0],\n",
      "        [6, 0, 0, 8, 1],\n",
      "        [6, 4, 2, 0, 8],\n",
      "        [5, 6, 8, 2, 1],\n",
      "        [8, 3, 5, 0, 1],\n",
      "        [3, 7, 0, 8, 1]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2975],\n",
      "        [ 0.2189],\n",
      "        [-0.5140],\n",
      "        [ 0.1965],\n",
      "        [-0.3931],\n",
      "        [ 0.2886],\n",
      "        [ 0.0683],\n",
      "        [ 0.0730]], grad_fn=<AddmmBackward0>), tensor([[ 0.3443],\n",
      "        [-0.2414],\n",
      "        [ 0.5865],\n",
      "        [ 0.1897],\n",
      "        [ 0.2988],\n",
      "        [-0.4070],\n",
      "        [ 0.0740],\n",
      "        [ 0.3360]], grad_fn=<AddmmBackward0>), tensor([[ 0.1314],\n",
      "        [ 0.3178],\n",
      "        [-0.4287],\n",
      "        [ 0.1096],\n",
      "        [-0.4490],\n",
      "        [ 0.1878],\n",
      "        [ 0.2332],\n",
      "        [ 0.2327]], grad_fn=<AddmmBackward0>), tensor([[ 0.0891],\n",
      "        [-0.1768],\n",
      "        [ 0.6575],\n",
      "        [-0.0142],\n",
      "        [ 0.3156],\n",
      "        [-0.1105],\n",
      "        [ 0.0212],\n",
      "        [-0.0322]], grad_fn=<AddmmBackward0>), tensor([[-0.0841],\n",
      "        [ 0.4601],\n",
      "        [ 0.0446],\n",
      "        [-0.0088],\n",
      "        [-0.4595],\n",
      "        [ 0.2327],\n",
      "        [-0.0967],\n",
      "        [ 0.0454]], grad_fn=<AddmmBackward0>), tensor([[ 0.0835],\n",
      "        [-0.3848],\n",
      "        [ 0.0401],\n",
      "        [-0.0921],\n",
      "        [ 0.3440],\n",
      "        [ 0.0804],\n",
      "        [-0.0763],\n",
      "        [-0.0511]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2483],\n",
      "        [-0.0325],\n",
      "        [-0.0036],\n",
      "        [ 0.2842],\n",
      "        [-0.0050],\n",
      "        [-0.0222],\n",
      "        [ 0.2521],\n",
      "        [ 0.1356]], grad_fn=<AddmmBackward0>), tensor([[ 0.5390],\n",
      "        [ 0.0145],\n",
      "        [-0.0071],\n",
      "        [ 0.5215],\n",
      "        [-0.0061],\n",
      "        [ 0.0225],\n",
      "        [ 0.4244],\n",
      "        [ 0.7586]], grad_fn=<AddmmBackward0>), tensor([[ 0.6858],\n",
      "        [-0.0983],\n",
      "        [ 0.1066],\n",
      "        [-0.0043],\n",
      "        [ 0.0019],\n",
      "        [ 0.1424],\n",
      "        [ 0.4992],\n",
      "        [ 0.0399]], grad_fn=<AddmmBackward0>), tensor([[ 0.0357],\n",
      "        [-0.1172],\n",
      "        [ 0.7008],\n",
      "        [ 0.0564],\n",
      "        [-0.0023],\n",
      "        [ 0.1080],\n",
      "        [ 0.0522],\n",
      "        [ 0.1252]], grad_fn=<AddmmBackward0>), tensor([[ 0.0208],\n",
      "        [ 0.0795],\n",
      "        [ 0.0874],\n",
      "        [-0.0310],\n",
      "        [ 0.0903],\n",
      "        [ 0.1225],\n",
      "        [ 0.0668],\n",
      "        [ 0.0632]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.0706e-04, 9.9969e-01],\n",
      "        [7.7344e-06, 9.9999e-01],\n",
      "        [9.9984e-01, 1.5952e-04],\n",
      "        [9.9994e-01, 6.3320e-05],\n",
      "        [9.9996e-01, 3.9526e-05],\n",
      "        [1.1543e-05, 9.9999e-01],\n",
      "        [1.7172e-04, 9.9983e-01],\n",
      "        [9.9993e-01, 6.8459e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9613e-01, 3.8740e-03],\n",
      "        [9.9989e-01, 1.0688e-04],\n",
      "        [1.1802e-05, 9.9999e-01],\n",
      "        [1.3975e-03, 9.9860e-01],\n",
      "        [6.8951e-06, 9.9999e-01],\n",
      "        [9.9963e-01, 3.7331e-04],\n",
      "        [9.9958e-01, 4.1695e-04],\n",
      "        [1.0459e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.5176e-03, 9.9848e-01],\n",
      "        [1.3318e-05, 9.9999e-01],\n",
      "        [9.9998e-01, 1.9202e-05],\n",
      "        [9.7112e-01, 2.8876e-02],\n",
      "        [9.9993e-01, 6.5141e-05],\n",
      "        [7.8205e-05, 9.9992e-01],\n",
      "        [1.9133e-04, 9.9981e-01],\n",
      "        [9.9809e-01, 1.9061e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.8992e-01, 1.0081e-02],\n",
      "        [9.9999e-01, 6.0998e-06],\n",
      "        [2.9632e-05, 9.9997e-01],\n",
      "        [6.2100e-01, 3.7900e-01],\n",
      "        [8.1025e-05, 9.9992e-01],\n",
      "        [9.9997e-01, 3.0022e-05],\n",
      "        [9.9461e-01, 5.3889e-03],\n",
      "        [2.0259e-02, 9.7974e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.0028e-02, 9.0997e-01],\n",
      "        [5.0221e-06, 9.9999e-01],\n",
      "        [9.9953e-01, 4.6799e-04],\n",
      "        [3.5590e-01, 6.4410e-01],\n",
      "        [9.9998e-01, 2.2806e-05],\n",
      "        [1.6832e-04, 9.9983e-01],\n",
      "        [5.0557e-03, 9.9494e-01],\n",
      "        [8.2363e-01, 1.7637e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 5, 3, 0, 8],\n",
      "        [4, 1, 0, 3, 5],\n",
      "        [6, 8, 2, 0, 7],\n",
      "        [2, 1, 8, 7, 5],\n",
      "        [6, 0, 2, 1, 7],\n",
      "        [2, 6, 0, 8, 0],\n",
      "        [6, 1, 0, 1, 7],\n",
      "        [3, 0, 2, 1, 7]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.0499],\n",
      "        [ 0.1554],\n",
      "        [ 0.1355],\n",
      "        [-0.2446],\n",
      "        [ 0.3958],\n",
      "        [ 0.5589],\n",
      "        [-0.1320],\n",
      "        [ 0.3289]], grad_fn=<AddmmBackward0>), tensor([[-0.0318],\n",
      "        [-0.2219],\n",
      "        [ 0.2702],\n",
      "        [ 0.0716],\n",
      "        [ 0.1233],\n",
      "        [-0.3764],\n",
      "        [ 0.3044],\n",
      "        [ 0.1988]], grad_fn=<AddmmBackward0>), tensor([[ 0.2027],\n",
      "        [ 0.2495],\n",
      "        [ 0.0988],\n",
      "        [-0.0966],\n",
      "        [ 0.0971],\n",
      "        [ 0.3871],\n",
      "        [ 0.0815],\n",
      "        [ 0.1082]], grad_fn=<AddmmBackward0>), tensor([[ 0.1524],\n",
      "        [-0.2944],\n",
      "        [ 0.3233],\n",
      "        [ 0.1664],\n",
      "        [ 0.0021],\n",
      "        [-0.3341],\n",
      "        [ 0.0156],\n",
      "        [-0.0324]], grad_fn=<AddmmBackward0>), tensor([[ 0.1601],\n",
      "        [ 0.3856],\n",
      "        [ 0.0026],\n",
      "        [ 0.0520],\n",
      "        [ 0.0425],\n",
      "        [ 0.3074],\n",
      "        [-0.0719],\n",
      "        [ 0.1414]], grad_fn=<AddmmBackward0>), tensor([[ 0.5124],\n",
      "        [-0.0107],\n",
      "        [ 0.0610],\n",
      "        [ 0.0622],\n",
      "        [ 0.0059],\n",
      "        [-0.0620],\n",
      "        [ 0.1020],\n",
      "        [-0.0052]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0067],\n",
      "        [ 0.0627],\n",
      "        [ 0.0014],\n",
      "        [ 0.0270],\n",
      "        [ 0.9448],\n",
      "        [-0.0241],\n",
      "        [ 0.3549],\n",
      "        [ 0.9738]], grad_fn=<AddmmBackward0>), tensor([[ 0.0348],\n",
      "        [-0.0293],\n",
      "        [ 0.1607],\n",
      "        [ 0.1447],\n",
      "        [ 0.0474],\n",
      "        [ 0.0590],\n",
      "        [ 0.8833],\n",
      "        [ 0.0965]], grad_fn=<AddmmBackward0>), tensor([[ 0.2327],\n",
      "        [ 0.2430],\n",
      "        [ 0.8000],\n",
      "        [ 0.1415],\n",
      "        [ 0.0954],\n",
      "        [ 0.1298],\n",
      "        [-0.0053],\n",
      "        [ 0.1554]], grad_fn=<AddmmBackward0>), tensor([[0.1369],\n",
      "        [0.0662],\n",
      "        [0.0731],\n",
      "        [0.6528],\n",
      "        [0.0443],\n",
      "        [0.0991],\n",
      "        [0.0582],\n",
      "        [0.0695]], grad_fn=<AddmmBackward0>), tensor([[0.4083],\n",
      "        [0.2048],\n",
      "        [0.0456],\n",
      "        [0.2711],\n",
      "        [0.0222],\n",
      "        [0.0905],\n",
      "        [0.0879],\n",
      "        [0.1897]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 2.0995e-05],\n",
      "        [3.0359e-05, 9.9997e-01],\n",
      "        [9.9985e-01, 1.5172e-04],\n",
      "        [9.9990e-01, 9.8267e-05],\n",
      "        [1.3090e-04, 9.9987e-01],\n",
      "        [5.0795e-05, 9.9995e-01],\n",
      "        [9.9992e-01, 8.3632e-05],\n",
      "        [4.6125e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.5271e-04, 9.9975e-01],\n",
      "        [9.9994e-01, 5.6018e-05],\n",
      "        [1.3835e-04, 9.9986e-01],\n",
      "        [5.9023e-05, 9.9994e-01],\n",
      "        [9.9870e-01, 1.2980e-03],\n",
      "        [9.9996e-01, 4.2853e-05],\n",
      "        [2.4160e-04, 9.9976e-01],\n",
      "        [9.9911e-01, 8.8552e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.3967e-05],\n",
      "        [9.7473e-05, 9.9990e-01],\n",
      "        [9.9997e-01, 2.8682e-05],\n",
      "        [9.9998e-01, 1.8354e-05],\n",
      "        [3.2738e-02, 9.6726e-01],\n",
      "        [1.4103e-04, 9.9986e-01],\n",
      "        [9.9919e-01, 8.0587e-04],\n",
      "        [1.3803e-02, 9.8620e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.2472e-05, 9.9996e-01],\n",
      "        [9.9995e-01, 4.5492e-05],\n",
      "        [9.2674e-03, 9.9073e-01],\n",
      "        [7.0566e-05, 9.9993e-01],\n",
      "        [9.9984e-01, 1.6411e-04],\n",
      "        [9.9993e-01, 6.9753e-05],\n",
      "        [1.2322e-03, 9.9877e-01],\n",
      "        [9.9989e-01, 1.1102e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 2.9412e-05],\n",
      "        [6.0722e-05, 9.9994e-01],\n",
      "        [9.9878e-01, 1.2165e-03],\n",
      "        [9.9990e-01, 1.0229e-04],\n",
      "        [3.7779e-03, 9.9622e-01],\n",
      "        [3.2892e-03, 9.9671e-01],\n",
      "        [9.9994e-01, 6.2428e-05],\n",
      "        [8.8165e-04, 9.9912e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 8, 2, 4, 0],\n",
      "        [2, 0, 8, 0, 8],\n",
      "        [2, 6, 7, 0, 8],\n",
      "        [2, 8, 4, 0, 8],\n",
      "        [2, 7, 1, 8, 5],\n",
      "        [5, 8, 4, 2, 1],\n",
      "        [7, 5, 2, 8, 0],\n",
      "        [7, 0, 8, 0, 8]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.3872],\n",
      "        [-0.1112],\n",
      "        [ 0.4843],\n",
      "        [ 0.0896],\n",
      "        [ 0.1252],\n",
      "        [ 0.0354],\n",
      "        [-0.4804],\n",
      "        [ 0.2717]], grad_fn=<AddmmBackward0>), tensor([[ 0.4795],\n",
      "        [ 0.3543],\n",
      "        [-0.1503],\n",
      "        [-0.1953],\n",
      "        [-0.3861],\n",
      "        [ 0.0861],\n",
      "        [ 0.4992],\n",
      "        [ 0.1423]], grad_fn=<AddmmBackward0>), tensor([[-0.1900],\n",
      "        [-0.0730],\n",
      "        [ 0.4848],\n",
      "        [ 0.3005],\n",
      "        [ 0.3734],\n",
      "        [ 0.0411],\n",
      "        [-0.3737],\n",
      "        [ 0.0080]], grad_fn=<AddmmBackward0>), tensor([[ 0.5115],\n",
      "        [ 0.0970],\n",
      "        [-0.0662],\n",
      "        [-0.1530],\n",
      "        [-0.0461],\n",
      "        [-0.2663],\n",
      "        [ 0.5069],\n",
      "        [-0.0425]], grad_fn=<AddmmBackward0>), tensor([[-0.0438],\n",
      "        [-0.0772],\n",
      "        [-0.0245],\n",
      "        [ 0.0128],\n",
      "        [ 0.2217],\n",
      "        [ 0.2830],\n",
      "        [-0.1150],\n",
      "        [-0.0903]], grad_fn=<AddmmBackward0>), tensor([[ 0.1038],\n",
      "        [-0.0420],\n",
      "        [-0.0827],\n",
      "        [-0.0383],\n",
      "        [ 0.0772],\n",
      "        [ 0.0892],\n",
      "        [ 0.4493],\n",
      "        [-0.1277]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0156],\n",
      "        [ 0.2125],\n",
      "        [ 0.2224],\n",
      "        [-0.0025],\n",
      "        [ 0.0253],\n",
      "        [-0.0455],\n",
      "        [-0.0073],\n",
      "        [ 1.0096]], grad_fn=<AddmmBackward0>), tensor([[ 0.2730],\n",
      "        [ 0.1302],\n",
      "        [ 0.0985],\n",
      "        [ 0.0387],\n",
      "        [ 0.0424],\n",
      "        [ 0.1596],\n",
      "        [-0.0018],\n",
      "        [ 0.0728]], grad_fn=<AddmmBackward0>), tensor([[-0.0281],\n",
      "        [ 0.0412],\n",
      "        [ 0.1902],\n",
      "        [ 0.5526],\n",
      "        [ 0.5961],\n",
      "        [ 0.0474],\n",
      "        [ 0.0579],\n",
      "        [ 0.0700]], grad_fn=<AddmmBackward0>), tensor([[1.0062],\n",
      "        [0.0756],\n",
      "        [0.0422],\n",
      "        [0.0621],\n",
      "        [0.2542],\n",
      "        [0.0248],\n",
      "        [0.6090],\n",
      "        [0.0591]], grad_fn=<AddmmBackward0>), tensor([[0.0672],\n",
      "        [0.0169],\n",
      "        [0.0747],\n",
      "        [0.0200],\n",
      "        [0.2650],\n",
      "        [0.1890],\n",
      "        [0.2912],\n",
      "        [0.0740]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9990e-01, 9.8950e-05],\n",
      "        [9.9995e-01, 4.8437e-05],\n",
      "        [1.0138e-04, 9.9990e-01],\n",
      "        [7.9954e-05, 9.9992e-01],\n",
      "        [9.7670e-06, 9.9999e-01],\n",
      "        [5.6813e-05, 9.9994e-01],\n",
      "        [9.9986e-01, 1.4027e-04],\n",
      "        [5.0083e-04, 9.9950e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.0785e-05, 9.9996e-01],\n",
      "        [6.0358e-03, 9.9396e-01],\n",
      "        [9.9995e-01, 4.8356e-05],\n",
      "        [9.9999e-01, 8.2453e-06],\n",
      "        [9.9997e-01, 2.9374e-05],\n",
      "        [9.9979e-01, 2.0992e-04],\n",
      "        [1.0772e-04, 9.9989e-01],\n",
      "        [9.9865e-01, 1.3549e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 6.5517e-06],\n",
      "        [9.9991e-01, 8.8210e-05],\n",
      "        [2.0967e-05, 9.9998e-01],\n",
      "        [5.9568e-05, 9.9994e-01],\n",
      "        [5.7391e-05, 9.9994e-01],\n",
      "        [5.8722e-05, 9.9994e-01],\n",
      "        [9.9988e-01, 1.2117e-04],\n",
      "        [2.1395e-02, 9.7860e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.4022e-04, 9.9986e-01],\n",
      "        [1.0553e-01, 8.9447e-01],\n",
      "        [9.9985e-01, 1.4765e-04],\n",
      "        [9.9990e-01, 9.7656e-05],\n",
      "        [1.0000e+00, 3.8599e-06],\n",
      "        [9.9999e-01, 5.1554e-06],\n",
      "        [5.1890e-04, 9.9948e-01],\n",
      "        [9.7949e-01, 2.0509e-02]], grad_fn=<SoftmaxBackward0>), tensor([[9.9989e-01, 1.1007e-04],\n",
      "        [9.9972e-01, 2.8297e-04],\n",
      "        [8.9447e-03, 9.9106e-01],\n",
      "        [6.9344e-04, 9.9931e-01],\n",
      "        [1.1590e-05, 9.9999e-01],\n",
      "        [9.3354e-06, 9.9999e-01],\n",
      "        [9.9994e-01, 5.5844e-05],\n",
      "        [1.1205e-01, 8.8795e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: -0.1\n",
      "Test score {'score': -0.1, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "5900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 5, 6, 0, 4],\n",
      "        [0, 0, 6, 8, 4],\n",
      "        [6, 8, 0, 8, 4],\n",
      "        [4, 2, 8, 1, 0],\n",
      "        [4, 2, 0, 8, 1],\n",
      "        [1, 7, 5, 0, 4],\n",
      "        [1, 8, 0, 8, 4],\n",
      "        [7, 5, 4, 0, 8]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.4427],\n",
      "        [ 0.7287],\n",
      "        [-0.2827],\n",
      "        [ 0.2473],\n",
      "        [ 0.4741],\n",
      "        [ 0.3333],\n",
      "        [-0.4168],\n",
      "        [ 0.4930]], grad_fn=<AddmmBackward0>), tensor([[ 0.0326],\n",
      "        [ 0.0479],\n",
      "        [ 0.3780],\n",
      "        [-0.2867],\n",
      "        [-0.4427],\n",
      "        [-0.0282],\n",
      "        [ 0.4478],\n",
      "        [-0.2883]], grad_fn=<AddmmBackward0>), tensor([[ 0.3376],\n",
      "        [-0.0012],\n",
      "        [ 0.1415],\n",
      "        [ 0.2654],\n",
      "        [ 0.3000],\n",
      "        [ 0.2786],\n",
      "        [-0.0113],\n",
      "        [ 0.3968]], grad_fn=<AddmmBackward0>), tensor([[ 0.0730],\n",
      "        [ 0.0157],\n",
      "        [ 0.0463],\n",
      "        [ 0.1321],\n",
      "        [-0.5423],\n",
      "        [-0.0833],\n",
      "        [-0.0164],\n",
      "        [-0.3510]], grad_fn=<AddmmBackward0>), tensor([[ 0.0032],\n",
      "        [ 0.0009],\n",
      "        [ 0.0906],\n",
      "        [ 0.1426],\n",
      "        [ 0.3375],\n",
      "        [-0.0400],\n",
      "        [ 0.0272],\n",
      "        [ 0.2658]], grad_fn=<AddmmBackward0>), tensor([[-0.0065],\n",
      "        [ 0.1168],\n",
      "        [ 0.0704],\n",
      "        [-0.0284],\n",
      "        [-0.2615],\n",
      "        [-0.0212],\n",
      "        [-0.0248],\n",
      "        [ 0.1016]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2918],\n",
      "        [ 1.1595],\n",
      "        [ 0.2582],\n",
      "        [ 0.0021],\n",
      "        [-0.0147],\n",
      "        [-0.0729],\n",
      "        [ 0.0478],\n",
      "        [ 0.1443]], grad_fn=<AddmmBackward0>), tensor([[ 0.6034],\n",
      "        [ 0.0352],\n",
      "        [ 0.5396],\n",
      "        [ 0.0112],\n",
      "        [-0.0415],\n",
      "        [ 0.0820],\n",
      "        [ 0.6143],\n",
      "        [ 0.0145]], grad_fn=<AddmmBackward0>), tensor([[ 0.9023],\n",
      "        [ 0.0071],\n",
      "        [ 0.0135],\n",
      "        [ 0.1770],\n",
      "        [-0.0760],\n",
      "        [ 0.2488],\n",
      "        [-0.0009],\n",
      "        [ 0.2445]], grad_fn=<AddmmBackward0>), tensor([[-0.0152],\n",
      "        [-0.0375],\n",
      "        [-0.0580],\n",
      "        [ 0.5592],\n",
      "        [-0.0207],\n",
      "        [-0.0174],\n",
      "        [-0.0439],\n",
      "        [ 0.0787]], grad_fn=<AddmmBackward0>), tensor([[ 0.0039],\n",
      "        [-0.0232],\n",
      "        [ 0.0369],\n",
      "        [-0.0141],\n",
      "        [ 0.1559],\n",
      "        [-0.0107],\n",
      "        [ 0.0102],\n",
      "        [ 0.3093]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[4.6525e-04, 9.9953e-01],\n",
      "        [7.8835e-04, 9.9921e-01],\n",
      "        [9.9996e-01, 4.4436e-05],\n",
      "        [8.3020e-06, 9.9999e-01],\n",
      "        [3.2699e-06, 1.0000e+00],\n",
      "        [3.7408e-05, 9.9996e-01],\n",
      "        [9.9998e-01, 2.1550e-05],\n",
      "        [3.0996e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9992e-01, 7.6438e-05],\n",
      "        [9.8965e-01, 1.0347e-02],\n",
      "        [1.7607e-04, 9.9982e-01],\n",
      "        [1.0000e+00, 3.4004e-06],\n",
      "        [9.9999e-01, 8.3506e-06],\n",
      "        [9.9991e-01, 8.8762e-05],\n",
      "        [1.0268e-04, 9.9990e-01],\n",
      "        [9.9999e-01, 1.0071e-05]], grad_fn=<SoftmaxBackward0>), tensor([[6.3694e-05, 9.9994e-01],\n",
      "        [1.1198e-01, 8.8802e-01],\n",
      "        [9.9637e-01, 3.6300e-03],\n",
      "        [7.5548e-05, 9.9992e-01],\n",
      "        [1.1637e-05, 9.9999e-01],\n",
      "        [9.4188e-05, 9.9991e-01],\n",
      "        [9.9888e-01, 1.1163e-03],\n",
      "        [6.7280e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9833e-01, 1.6652e-03],\n",
      "        [9.9872e-01, 1.2789e-03],\n",
      "        [2.7891e-01, 7.2109e-01],\n",
      "        [9.9998e-01, 1.6494e-05],\n",
      "        [1.0000e+00, 4.7153e-07],\n",
      "        [9.9954e-01, 4.6312e-04],\n",
      "        [2.0029e-01, 7.9971e-01],\n",
      "        [9.9997e-01, 3.0431e-05]], grad_fn=<SoftmaxBackward0>), tensor([[2.2680e-02, 9.7732e-01],\n",
      "        [4.1146e-02, 9.5885e-01],\n",
      "        [9.8862e-01, 1.1384e-02],\n",
      "        [1.6124e-01, 8.3876e-01],\n",
      "        [1.8201e-06, 1.0000e+00],\n",
      "        [1.5459e-02, 9.8454e-01],\n",
      "        [9.9854e-01, 1.4625e-03],\n",
      "        [1.6695e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 7, in <module>\n",
      "    from search.search_factories import create_mcts\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/search_factories.py\", line 1, in <module>\n",
      "    from search.algorithms import GumbelSequentialHalving, SearchAlgorithm, UCTSearch\n",
      "ImportError: cannot import name 'GumbelSequentialHalving' from 'search.algorithms' (/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 0, 0, 8, 7],\n",
      "        [0, 5, 6, 2, 4],\n",
      "        [5, 6, 8, 2, 0],\n",
      "        [6, 5, 1, 0, 3],\n",
      "        [8, 0, 2, 1, 0],\n",
      "        [2, 1, 4, 0, 7],\n",
      "        [2, 3, 8, 0, 7],\n",
      "        [0, 1, 7, 5, 3]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.6342],\n",
      "        [ 0.2478],\n",
      "        [ 0.2537],\n",
      "        [-0.2916],\n",
      "        [ 0.0449],\n",
      "        [ 0.2218],\n",
      "        [-0.4199],\n",
      "        [-0.0854]], grad_fn=<AddmmBackward0>), tensor([[ 0.2441],\n",
      "        [-0.3806],\n",
      "        [ 0.1308],\n",
      "        [ 0.3233],\n",
      "        [ 0.0119],\n",
      "        [-0.3364],\n",
      "        [ 0.3473],\n",
      "        [ 0.1701]], grad_fn=<AddmmBackward0>), tensor([[ 0.0207],\n",
      "        [ 0.2168],\n",
      "        [ 0.1630],\n",
      "        [-0.0889],\n",
      "        [-0.0324],\n",
      "        [ 0.4101],\n",
      "        [-0.1630],\n",
      "        [-0.0786]], grad_fn=<AddmmBackward0>), tensor([[-0.0307],\n",
      "        [-0.3066],\n",
      "        [ 0.0774],\n",
      "        [ 0.2953],\n",
      "        [ 0.2663],\n",
      "        [-0.3478],\n",
      "        [ 0.1396],\n",
      "        [ 0.2255]], grad_fn=<AddmmBackward0>), tensor([[-0.0860],\n",
      "        [ 0.3100],\n",
      "        [ 0.2564],\n",
      "        [ 0.0932],\n",
      "        [ 0.0194],\n",
      "        [ 0.3113],\n",
      "        [-0.1145],\n",
      "        [ 0.0793]], grad_fn=<AddmmBackward0>), tensor([[-0.0104],\n",
      "        [-0.3187],\n",
      "        [-0.0046],\n",
      "        [ 0.1836],\n",
      "        [ 0.0175],\n",
      "        [-0.0168],\n",
      "        [ 0.0440],\n",
      "        [ 0.2538]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.8476],\n",
      "        [ 0.0021],\n",
      "        [ 0.0937],\n",
      "        [-0.0722],\n",
      "        [-0.1585],\n",
      "        [ 0.0177],\n",
      "        [ 0.0073],\n",
      "        [-0.1165]], grad_fn=<AddmmBackward0>), tensor([[-0.0479],\n",
      "        [ 0.0148],\n",
      "        [ 0.3571],\n",
      "        [ 0.0889],\n",
      "        [ 0.0206],\n",
      "        [-0.0555],\n",
      "        [ 0.4206],\n",
      "        [-0.0237]], grad_fn=<AddmmBackward0>), tensor([[-0.0305],\n",
      "        [-0.1332],\n",
      "        [ 0.3829],\n",
      "        [ 0.1077],\n",
      "        [ 0.0217],\n",
      "        [ 0.1132],\n",
      "        [ 0.1084],\n",
      "        [ 0.0645]], grad_fn=<AddmmBackward0>), tensor([[ 0.0033],\n",
      "        [-0.0356],\n",
      "        [ 0.4613],\n",
      "        [ 0.3355],\n",
      "        [ 0.3533],\n",
      "        [ 0.0557],\n",
      "        [-0.0598],\n",
      "        [ 0.2442]], grad_fn=<AddmmBackward0>), tensor([[ 0.0259],\n",
      "        [ 0.0338],\n",
      "        [-0.0146],\n",
      "        [ 0.1269],\n",
      "        [-0.0427],\n",
      "        [ 0.1272],\n",
      "        [-0.0458],\n",
      "        [ 0.1285]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9994e-01, 6.1517e-05],\n",
      "        [9.5621e-05, 9.9990e-01],\n",
      "        [2.0575e-04, 9.9979e-01],\n",
      "        [9.9996e-01, 3.6352e-05],\n",
      "        [9.9981e-01, 1.8827e-04],\n",
      "        [9.9808e-05, 9.9990e-01],\n",
      "        [9.9997e-01, 3.2375e-05],\n",
      "        [9.9999e-01, 8.4126e-06]], grad_fn=<SoftmaxBackward0>), tensor([[8.3596e-03, 9.9164e-01],\n",
      "        [9.9996e-01, 3.9878e-05],\n",
      "        [9.9993e-01, 7.3824e-05],\n",
      "        [5.0764e-04, 9.9949e-01],\n",
      "        [1.5967e-03, 9.9840e-01],\n",
      "        [9.9996e-01, 4.3720e-05],\n",
      "        [6.1960e-04, 9.9938e-01],\n",
      "        [3.7016e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9462e-01, 5.3785e-03],\n",
      "        [2.0041e-04, 9.9980e-01],\n",
      "        [1.0948e-04, 9.9989e-01],\n",
      "        [9.9994e-01, 5.7215e-05],\n",
      "        [9.9996e-01, 3.7627e-05],\n",
      "        [7.8452e-05, 9.9992e-01],\n",
      "        [9.9972e-01, 2.8099e-04],\n",
      "        [9.9995e-01, 5.0200e-05]], grad_fn=<SoftmaxBackward0>), tensor([[8.3495e-02, 9.1651e-01],\n",
      "        [9.9999e-01, 6.1544e-06],\n",
      "        [9.9996e-01, 3.8855e-05],\n",
      "        [1.3990e-03, 9.9860e-01],\n",
      "        [7.2642e-05, 9.9993e-01],\n",
      "        [9.9998e-01, 2.3915e-05],\n",
      "        [5.0429e-02, 9.4957e-01],\n",
      "        [8.6630e-05, 9.9991e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.7762e-01, 2.2378e-02],\n",
      "        [1.4656e-04, 9.9985e-01],\n",
      "        [1.8916e-02, 9.8108e-01],\n",
      "        [9.9709e-01, 2.9068e-03],\n",
      "        [9.9998e-01, 2.1161e-05],\n",
      "        [7.2698e-05, 9.9993e-01],\n",
      "        [9.8314e-01, 1.6858e-02],\n",
      "        [9.9998e-01, 2.4106e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 5, 0, 2, 1],\n",
      "        [0, 5, 2, 1, 0],\n",
      "        [4, 1, 0, 7, 2],\n",
      "        [2, 4, 1, 6, 8],\n",
      "        [4, 2, 7, 8, 5],\n",
      "        [1, 2, 8, 0, 1],\n",
      "        [3, 5, 2, 1, 0],\n",
      "        [3, 7, 0, 2, 1]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1281],\n",
      "        [-0.5011],\n",
      "        [ 0.1671],\n",
      "        [ 0.1671],\n",
      "        [-0.1446],\n",
      "        [ 0.4772],\n",
      "        [-0.1444],\n",
      "        [-0.3488]], grad_fn=<AddmmBackward0>), tensor([[ 0.0090],\n",
      "        [ 0.2234],\n",
      "        [-0.3608],\n",
      "        [-0.3591],\n",
      "        [-0.1553],\n",
      "        [-0.1552],\n",
      "        [ 0.2731],\n",
      "        [ 0.1088]], grad_fn=<AddmmBackward0>), tensor([[ 0.1284],\n",
      "        [-0.0005],\n",
      "        [ 0.2892],\n",
      "        [ 0.0134],\n",
      "        [ 0.0283],\n",
      "        [ 0.1425],\n",
      "        [-0.0163],\n",
      "        [ 0.0065]], grad_fn=<AddmmBackward0>), tensor([[-0.0110],\n",
      "        [ 0.1185],\n",
      "        [-0.5128],\n",
      "        [-0.0601],\n",
      "        [-0.1809],\n",
      "        [ 0.0177],\n",
      "        [ 0.1323],\n",
      "        [-0.1487]], grad_fn=<AddmmBackward0>), tensor([[-0.1197],\n",
      "        [ 0.0134],\n",
      "        [ 0.5920],\n",
      "        [-0.0227],\n",
      "        [ 0.0157],\n",
      "        [-0.0605],\n",
      "        [-0.0482],\n",
      "        [-0.1087]], grad_fn=<AddmmBackward0>), tensor([[-0.0609],\n",
      "        [ 0.0030],\n",
      "        [-0.4232],\n",
      "        [-0.1054],\n",
      "        [-0.1135],\n",
      "        [-0.1251],\n",
      "        [-0.0040],\n",
      "        [-0.0630]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.7436],\n",
      "        [ 0.0095],\n",
      "        [-0.0344],\n",
      "        [-0.0605],\n",
      "        [-0.0788],\n",
      "        [ 0.0929],\n",
      "        [ 0.0427],\n",
      "        [-0.0808]], grad_fn=<AddmmBackward0>), tensor([[ 0.5107],\n",
      "        [ 0.0487],\n",
      "        [-0.0230],\n",
      "        [-0.0687],\n",
      "        [ 0.0217],\n",
      "        [ 0.1619],\n",
      "        [ 0.0507],\n",
      "        [ 0.6810]], grad_fn=<AddmmBackward0>), tensor([[ 0.5105],\n",
      "        [ 0.0388],\n",
      "        [-0.0521],\n",
      "        [ 0.0825],\n",
      "        [-0.0040],\n",
      "        [ 0.0507],\n",
      "        [ 0.0244],\n",
      "        [ 0.0108]], grad_fn=<AddmmBackward0>), tensor([[0.1678],\n",
      "        [0.1096],\n",
      "        [0.0007],\n",
      "        [0.0154],\n",
      "        [0.0523],\n",
      "        [0.0168],\n",
      "        [0.0943],\n",
      "        [0.0214]], grad_fn=<AddmmBackward0>), tensor([[ 0.1169],\n",
      "        [-0.0015],\n",
      "        [ 0.2949],\n",
      "        [ 0.1012],\n",
      "        [ 0.1287],\n",
      "        [-0.0420],\n",
      "        [-0.0024],\n",
      "        [ 0.0620]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.0437e-05, 9.9991e-01],\n",
      "        [9.9991e-01, 9.4863e-05],\n",
      "        [9.9293e-06, 9.9999e-01],\n",
      "        [5.5220e-06, 9.9999e-01],\n",
      "        [9.9995e-01, 5.3594e-05],\n",
      "        [1.3334e-04, 9.9987e-01],\n",
      "        [9.9985e-01, 1.4788e-04],\n",
      "        [9.9981e-01, 1.9354e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9989e-01, 1.1298e-04],\n",
      "        [1.7999e-03, 9.9820e-01],\n",
      "        [9.9995e-01, 4.7391e-05],\n",
      "        [9.9997e-01, 3.3081e-05],\n",
      "        [3.6144e-05, 9.9996e-01],\n",
      "        [9.9976e-01, 2.4146e-04],\n",
      "        [2.2098e-03, 9.9779e-01],\n",
      "        [1.6273e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.9677e-05, 9.9995e-01],\n",
      "        [9.9990e-01, 9.9379e-05],\n",
      "        [4.0879e-05, 9.9996e-01],\n",
      "        [1.7362e-05, 9.9998e-01],\n",
      "        [9.9986e-01, 1.4262e-04],\n",
      "        [3.7787e-04, 9.9962e-01],\n",
      "        [9.9983e-01, 1.7408e-04],\n",
      "        [9.9802e-01, 1.9820e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.9929e-01, 7.1216e-04],\n",
      "        [1.4624e-04, 9.9985e-01],\n",
      "        [9.9983e-01, 1.6545e-04],\n",
      "        [9.9942e-01, 5.8363e-04],\n",
      "        [5.2499e-06, 9.9999e-01],\n",
      "        [9.9735e-01, 2.6502e-03],\n",
      "        [1.9200e-04, 9.9981e-01],\n",
      "        [3.3500e-02, 9.6650e-01]], grad_fn=<SoftmaxBackward0>), tensor([[7.3820e-05, 9.9993e-01],\n",
      "        [9.9987e-01, 1.3475e-04],\n",
      "        [4.0838e-05, 9.9996e-01],\n",
      "        [3.4580e-05, 9.9997e-01],\n",
      "        [9.9997e-01, 2.5000e-05],\n",
      "        [4.4465e-03, 9.9555e-01],\n",
      "        [9.9987e-01, 1.2978e-04],\n",
      "        [9.7379e-01, 2.6211e-02]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 4, 5, 2, 7],\n",
      "        [0, 5, 0, 5, 8],\n",
      "        [0, 8, 7, 0, 8],\n",
      "        [1, 8, 0, 5, 8],\n",
      "        [6, 8, 2, 0, 0],\n",
      "        [8, 1, 3, 0, 5],\n",
      "        [7, 2, 5, 0, 8],\n",
      "        [5, 4, 2, 7, 0]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.0836],\n",
      "        [-0.1556],\n",
      "        [ 0.3009],\n",
      "        [ 0.4111],\n",
      "        [-0.6056],\n",
      "        [ 0.4474],\n",
      "        [-0.6056],\n",
      "        [ 0.0836]], grad_fn=<AddmmBackward0>), tensor([[-0.2953],\n",
      "        [ 0.2204],\n",
      "        [-0.1668],\n",
      "        [ 0.0210],\n",
      "        [ 0.5023],\n",
      "        [-0.3995],\n",
      "        [ 0.7286],\n",
      "        [-0.1189]], grad_fn=<AddmmBackward0>), tensor([[ 0.0213],\n",
      "        [-0.0347],\n",
      "        [ 0.3114],\n",
      "        [ 0.1255],\n",
      "        [-0.3592],\n",
      "        [ 0.4046],\n",
      "        [-0.4934],\n",
      "        [-0.2882]], grad_fn=<AddmmBackward0>), tensor([[-5.1286e-02],\n",
      "        [-3.3766e-02],\n",
      "        [ 1.1606e-01],\n",
      "        [-8.4117e-05],\n",
      "        [ 3.5063e-01],\n",
      "        [-7.5494e-02],\n",
      "        [ 7.1132e-01],\n",
      "        [ 1.9550e-01]], grad_fn=<AddmmBackward0>), tensor([[ 0.0135],\n",
      "        [-0.0396],\n",
      "        [-0.0044],\n",
      "        [ 0.0889],\n",
      "        [-0.0230],\n",
      "        [ 0.2174],\n",
      "        [-0.4980],\n",
      "        [-0.1358]], grad_fn=<AddmmBackward0>), tensor([[-0.0478],\n",
      "        [-0.0176],\n",
      "        [ 0.0046],\n",
      "        [ 0.0647],\n",
      "        [ 0.0197],\n",
      "        [ 0.1539],\n",
      "        [ 0.6966],\n",
      "        [ 0.2122]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0465],\n",
      "        [ 0.3275],\n",
      "        [ 0.4548],\n",
      "        [ 0.0712],\n",
      "        [ 0.0623],\n",
      "        [-0.0577],\n",
      "        [ 0.0625],\n",
      "        [ 0.1255]], grad_fn=<AddmmBackward0>), tensor([[ 0.0653],\n",
      "        [ 0.8001],\n",
      "        [ 0.0334],\n",
      "        [ 0.4960],\n",
      "        [-0.0079],\n",
      "        [ 0.0886],\n",
      "        [ 0.0348],\n",
      "        [ 0.0986]], grad_fn=<AddmmBackward0>), tensor([[0.0944],\n",
      "        [0.0400],\n",
      "        [0.5506],\n",
      "        [0.0283],\n",
      "        [0.0630],\n",
      "        [0.1212],\n",
      "        [0.0456],\n",
      "        [0.1539]], grad_fn=<AddmmBackward0>), tensor([[0.0990],\n",
      "        [0.0536],\n",
      "        [0.0758],\n",
      "        [0.2800],\n",
      "        [0.6351],\n",
      "        [0.0253],\n",
      "        [0.0612],\n",
      "        [0.0046]], grad_fn=<AddmmBackward0>), tensor([[0.0749],\n",
      "        [0.0557],\n",
      "        [0.0738],\n",
      "        [0.1070],\n",
      "        [0.0393],\n",
      "        [0.8675],\n",
      "        [0.0610],\n",
      "        [0.1775]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[2.3645e-05, 9.9998e-01],\n",
      "        [9.9993e-01, 7.1731e-05],\n",
      "        [5.9514e-04, 9.9940e-01],\n",
      "        [9.9977e-01, 2.2855e-04],\n",
      "        [9.9998e-01, 2.2874e-05],\n",
      "        [3.8969e-05, 9.9996e-01],\n",
      "        [9.9998e-01, 1.8044e-05],\n",
      "        [2.0964e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 3.3396e-05],\n",
      "        [8.1312e-05, 9.9992e-01],\n",
      "        [9.9982e-01, 1.7977e-04],\n",
      "        [2.5733e-04, 9.9974e-01],\n",
      "        [2.4099e-05, 9.9998e-01],\n",
      "        [9.9995e-01, 5.1858e-05],\n",
      "        [2.7089e-05, 9.9997e-01],\n",
      "        [9.9998e-01, 1.8181e-05]], grad_fn=<SoftmaxBackward0>), tensor([[5.8056e-05, 9.9994e-01],\n",
      "        [9.9887e-01, 1.1289e-03],\n",
      "        [1.0021e-04, 9.9990e-01],\n",
      "        [9.9374e-01, 6.2556e-03],\n",
      "        [9.9999e-01, 1.3702e-05],\n",
      "        [8.3761e-05, 9.9992e-01],\n",
      "        [9.9998e-01, 1.8930e-05],\n",
      "        [4.6041e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 6.2569e-06],\n",
      "        [1.0572e-02, 9.8943e-01],\n",
      "        [9.9241e-01, 7.5893e-03],\n",
      "        [1.7765e-02, 9.8224e-01],\n",
      "        [9.9243e-05, 9.9990e-01],\n",
      "        [9.9996e-01, 3.8675e-05],\n",
      "        [1.2039e-04, 9.9988e-01],\n",
      "        [9.9995e-01, 4.9606e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.5596e-04, 9.9984e-01],\n",
      "        [9.9757e-01, 2.4326e-03],\n",
      "        [1.1728e-01, 8.8272e-01],\n",
      "        [9.9214e-01, 7.8588e-03],\n",
      "        [9.9920e-01, 8.0407e-04],\n",
      "        [6.1981e-05, 9.9994e-01],\n",
      "        [9.9999e-01, 1.2095e-05],\n",
      "        [3.5373e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 5, 6, 7, 0],\n",
      "        [1, 2, 0, 4, 4],\n",
      "        [7, 0, 8, 4, 4],\n",
      "        [8, 1, 5, 0, 0],\n",
      "        [5, 3, 6, 0, 2],\n",
      "        [1, 2, 0, 4, 4],\n",
      "        [8, 6, 1, 3, 0],\n",
      "        [2, 3, 7, 0, 4]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2685],\n",
      "        [-0.4657],\n",
      "        [ 0.1442],\n",
      "        [ 0.2149],\n",
      "        [ 0.1671],\n",
      "        [ 0.2024],\n",
      "        [-0.1967],\n",
      "        [ 0.4852]], grad_fn=<AddmmBackward0>), tensor([[ 0.0406],\n",
      "        [ 0.9440],\n",
      "        [ 0.2918],\n",
      "        [ 0.1014],\n",
      "        [-0.3484],\n",
      "        [-0.0771],\n",
      "        [ 0.2580],\n",
      "        [-0.3253]], grad_fn=<AddmmBackward0>), tensor([[ 0.1797],\n",
      "        [-0.5600],\n",
      "        [-0.0862],\n",
      "        [ 0.0335],\n",
      "        [ 0.1280],\n",
      "        [ 0.2951],\n",
      "        [-0.1744],\n",
      "        [ 0.2270]], grad_fn=<AddmmBackward0>), tensor([[ 0.0697],\n",
      "        [ 0.3508],\n",
      "        [-0.0461],\n",
      "        [ 0.0408],\n",
      "        [-0.2268],\n",
      "        [-0.0717],\n",
      "        [ 0.0523],\n",
      "        [-0.2841]], grad_fn=<AddmmBackward0>), tensor([[ 0.0309],\n",
      "        [-0.5508],\n",
      "        [-0.0171],\n",
      "        [ 0.0148],\n",
      "        [ 0.1513],\n",
      "        [ 0.0194],\n",
      "        [ 0.0443],\n",
      "        [ 0.0936]], grad_fn=<AddmmBackward0>), tensor([[ 0.0419],\n",
      "        [ 0.2534],\n",
      "        [-0.0297],\n",
      "        [-0.0313],\n",
      "        [ 0.2618],\n",
      "        [-0.3413],\n",
      "        [ 0.0331],\n",
      "        [-0.3510]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0634],\n",
      "        [-0.0601],\n",
      "        [ 1.0390],\n",
      "        [-0.0305],\n",
      "        [-0.0714],\n",
      "        [ 0.0545],\n",
      "        [-0.0119],\n",
      "        [ 0.0719]], grad_fn=<AddmmBackward0>), tensor([[ 0.1045],\n",
      "        [ 0.1938],\n",
      "        [-0.0061],\n",
      "        [ 0.2372],\n",
      "        [-0.0627],\n",
      "        [-0.0318],\n",
      "        [ 0.0523],\n",
      "        [ 0.0661]], grad_fn=<AddmmBackward0>), tensor([[ 0.0403],\n",
      "        [ 0.0659],\n",
      "        [ 0.0050],\n",
      "        [ 0.1771],\n",
      "        [ 0.0324],\n",
      "        [-0.0143],\n",
      "        [ 0.0006],\n",
      "        [-0.0678]], grad_fn=<AddmmBackward0>), tensor([[ 1.7322e-01],\n",
      "        [-2.5956e-02],\n",
      "        [ 1.9991e-04],\n",
      "        [ 6.0666e-01],\n",
      "        [ 2.5104e-01],\n",
      "        [-6.0698e-02],\n",
      "        [ 7.0744e-01],\n",
      "        [-2.8073e-02]], grad_fn=<AddmmBackward0>), tensor([[ 0.1048],\n",
      "        [-0.0297],\n",
      "        [-0.0375],\n",
      "        [-0.0298],\n",
      "        [ 0.8676],\n",
      "        [-0.0318],\n",
      "        [-0.0202],\n",
      "        [-0.0231]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9996e-01, 3.8645e-05],\n",
      "        [9.9994e-01, 5.9808e-05],\n",
      "        [9.9956e-01, 4.4433e-04],\n",
      "        [9.9993e-01, 7.4933e-05],\n",
      "        [9.9998e-01, 2.1305e-05],\n",
      "        [2.4888e-05, 9.9998e-01],\n",
      "        [9.9996e-01, 3.6006e-05],\n",
      "        [1.8976e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.2007e-05, 9.9995e-01],\n",
      "        [1.4608e-04, 9.9985e-01],\n",
      "        [6.6707e-02, 9.3329e-01],\n",
      "        [9.0502e-05, 9.9991e-01],\n",
      "        [6.1870e-05, 9.9994e-01],\n",
      "        [9.9998e-01, 1.6158e-05],\n",
      "        [6.1828e-05, 9.9994e-01],\n",
      "        [9.9997e-01, 3.4556e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9979e-01, 2.0985e-04],\n",
      "        [9.9985e-01, 1.5265e-04],\n",
      "        [9.8096e-01, 1.9041e-02],\n",
      "        [9.9997e-01, 2.7083e-05],\n",
      "        [9.9994e-01, 5.5987e-05],\n",
      "        [5.0187e-04, 9.9950e-01],\n",
      "        [9.9998e-01, 2.3882e-05],\n",
      "        [1.8967e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.3843e-06, 9.9999e-01],\n",
      "        [7.0764e-04, 9.9929e-01],\n",
      "        [2.4259e-02, 9.7574e-01],\n",
      "        [5.1346e-05, 9.9995e-01],\n",
      "        [1.0739e-05, 9.9999e-01],\n",
      "        [9.9941e-01, 5.8622e-04],\n",
      "        [1.5586e-04, 9.9984e-01],\n",
      "        [9.9980e-01, 2.0169e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.0022e-05],\n",
      "        [9.9992e-01, 8.0700e-05],\n",
      "        [4.8489e-01, 5.1511e-01],\n",
      "        [9.9940e-01, 5.9551e-04],\n",
      "        [1.0000e+00, 1.6935e-06],\n",
      "        [8.0027e-05, 9.9992e-01],\n",
      "        [9.9910e-01, 9.0378e-04],\n",
      "        [7.0260e-04, 9.9930e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 3, 2, 5, 6],\n",
      "        [3, 0, 8, 2, 4],\n",
      "        [1, 4, 6, 2, 3],\n",
      "        [5, 6, 1, 2, 4],\n",
      "        [7, 8, 2, 1, 5],\n",
      "        [8, 0, 8, 2, 4],\n",
      "        [4, 6, 7, 0, 4],\n",
      "        [1, 0, 8, 2, 4]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3154],\n",
      "        [ 0.0278],\n",
      "        [ 0.0995],\n",
      "        [ 0.4461],\n",
      "        [-0.1311],\n",
      "        [ 0.2391],\n",
      "        [ 0.3338],\n",
      "        [ 0.4516]], grad_fn=<AddmmBackward0>), tensor([[-0.2375],\n",
      "        [ 0.2308],\n",
      "        [-0.0319],\n",
      "        [-0.0392],\n",
      "        [ 0.2204],\n",
      "        [ 0.2858],\n",
      "        [-0.0891],\n",
      "        [ 0.2808]], grad_fn=<AddmmBackward0>), tensor([[ 0.3812],\n",
      "        [-0.0805],\n",
      "        [ 0.1380],\n",
      "        [ 0.1889],\n",
      "        [ 0.0153],\n",
      "        [-0.0081],\n",
      "        [ 0.2819],\n",
      "        [ 0.1176]], grad_fn=<AddmmBackward0>), tensor([[-0.2651],\n",
      "        [ 0.2658],\n",
      "        [ 0.0466],\n",
      "        [ 0.0610],\n",
      "        [ 0.1833],\n",
      "        [ 0.0944],\n",
      "        [ 0.1375],\n",
      "        [ 0.1055]], grad_fn=<AddmmBackward0>), tensor([[ 0.4410],\n",
      "        [-0.0047],\n",
      "        [ 0.2595],\n",
      "        [ 0.1661],\n",
      "        [ 0.1534],\n",
      "        [-0.0252],\n",
      "        [ 0.0775],\n",
      "        [ 0.0356]], grad_fn=<AddmmBackward0>), tensor([[-0.0547],\n",
      "        [ 0.0841],\n",
      "        [ 0.0980],\n",
      "        [ 0.0081],\n",
      "        [ 0.2354],\n",
      "        [ 0.0986],\n",
      "        [ 0.0332],\n",
      "        [ 0.1008]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0314],\n",
      "        [ 0.4516],\n",
      "        [-0.1154],\n",
      "        [ 0.0373],\n",
      "        [-0.1255],\n",
      "        [ 0.6270],\n",
      "        [ 0.0313],\n",
      "        [ 0.9132]], grad_fn=<AddmmBackward0>), tensor([[-0.0267],\n",
      "        [-0.0604],\n",
      "        [ 0.0193],\n",
      "        [-0.0190],\n",
      "        [ 0.0156],\n",
      "        [ 0.0057],\n",
      "        [ 0.0046],\n",
      "        [ 0.0493]], grad_fn=<AddmmBackward0>), tensor([[-0.0453],\n",
      "        [-0.0711],\n",
      "        [ 0.0335],\n",
      "        [ 0.1153],\n",
      "        [-0.0780],\n",
      "        [-0.0628],\n",
      "        [ 0.3455],\n",
      "        [-0.0233]], grad_fn=<AddmmBackward0>), tensor([[0.0507],\n",
      "        [0.1662],\n",
      "        [0.1049],\n",
      "        [0.2710],\n",
      "        [0.1036],\n",
      "        [0.0099],\n",
      "        [0.1289],\n",
      "        [0.0116]], grad_fn=<AddmmBackward0>), tensor([[ 0.2113],\n",
      "        [ 0.0982],\n",
      "        [-0.0368],\n",
      "        [ 0.1267],\n",
      "        [ 0.2981],\n",
      "        [ 0.0294],\n",
      "        [ 0.0184],\n",
      "        [ 0.0211]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[8.6937e-06, 9.9999e-01],\n",
      "        [9.9998e-01, 1.9047e-05],\n",
      "        [2.6306e-05, 9.9997e-01],\n",
      "        [7.1206e-05, 9.9993e-01],\n",
      "        [9.9997e-01, 2.6679e-05],\n",
      "        [1.0000e+00, 3.7666e-06],\n",
      "        [1.7109e-05, 9.9998e-01],\n",
      "        [5.4703e-04, 9.9945e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 7.8385e-06],\n",
      "        [3.5602e-01, 6.4398e-01],\n",
      "        [9.9999e-01, 1.1012e-05],\n",
      "        [1.0000e+00, 1.8887e-06],\n",
      "        [7.4928e-05, 9.9993e-01],\n",
      "        [1.4094e-02, 9.8591e-01],\n",
      "        [9.9999e-01, 7.6478e-06],\n",
      "        [9.9925e-01, 7.5503e-04]], grad_fn=<SoftmaxBackward0>), tensor([[3.8207e-05, 9.9996e-01],\n",
      "        [9.9849e-01, 1.5088e-03],\n",
      "        [6.8901e-05, 9.9993e-01],\n",
      "        [2.5321e-05, 9.9997e-01],\n",
      "        [9.9997e-01, 2.9282e-05],\n",
      "        [9.9125e-01, 8.7476e-03],\n",
      "        [1.4387e-05, 9.9999e-01],\n",
      "        [4.2697e-01, 5.7303e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 1.3838e-06],\n",
      "        [6.5130e-02, 9.3487e-01],\n",
      "        [9.9999e-01, 1.3905e-05],\n",
      "        [1.0000e+00, 3.3493e-07],\n",
      "        [7.5165e-04, 9.9925e-01],\n",
      "        [3.2825e-02, 9.6718e-01],\n",
      "        [9.9995e-01, 4.8804e-05],\n",
      "        [9.9649e-01, 3.5094e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.4549e-04, 9.9985e-01],\n",
      "        [9.9935e-01, 6.5363e-04],\n",
      "        [1.3410e-04, 9.9987e-01],\n",
      "        [4.8048e-05, 9.9995e-01],\n",
      "        [9.9998e-01, 1.8155e-05],\n",
      "        [9.9574e-01, 4.2643e-03],\n",
      "        [1.7229e-03, 9.9828e-01],\n",
      "        [2.1372e-02, 9.7863e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 7, 6, 1, 0],\n",
      "        [2, 0, 0, 6, 5],\n",
      "        [2, 5, 0, 8, 7],\n",
      "        [4, 3, 5, 2, 8],\n",
      "        [4, 1, 0, 6, 8],\n",
      "        [0, 6, 4, 5, 8],\n",
      "        [6, 7, 1, 0, 5],\n",
      "        [4, 5, 8, 0, 2]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2513],\n",
      "        [ 0.8824],\n",
      "        [ 0.2412],\n",
      "        [-0.0269],\n",
      "        [ 0.2513],\n",
      "        [ 0.2513],\n",
      "        [ 0.2137],\n",
      "        [-0.0298]], grad_fn=<AddmmBackward0>), tensor([[-0.4480],\n",
      "        [-0.0476],\n",
      "        [-0.1356],\n",
      "        [-0.2644],\n",
      "        [-0.4480],\n",
      "        [-0.4174],\n",
      "        [-0.2739],\n",
      "        [ 0.0509]], grad_fn=<AddmmBackward0>), tensor([[ 0.4177],\n",
      "        [-0.0371],\n",
      "        [ 0.2788],\n",
      "        [ 0.2656],\n",
      "        [ 0.3673],\n",
      "        [ 0.3503],\n",
      "        [ 0.3577],\n",
      "        [ 0.1498]], grad_fn=<AddmmBackward0>), tensor([[-0.4077],\n",
      "        [-0.0785],\n",
      "        [-0.0686],\n",
      "        [-0.1022],\n",
      "        [-0.5876],\n",
      "        [-0.4800],\n",
      "        [-0.1282],\n",
      "        [ 0.0089]], grad_fn=<AddmmBackward0>), tensor([[ 0.4796],\n",
      "        [-0.0579],\n",
      "        [ 0.0612],\n",
      "        [ 0.0843],\n",
      "        [ 0.4441],\n",
      "        [ 0.3367],\n",
      "        [-0.0941],\n",
      "        [-0.0732]], grad_fn=<AddmmBackward0>), tensor([[-0.6175],\n",
      "        [-0.0137],\n",
      "        [-0.0854],\n",
      "        [-0.0617],\n",
      "        [ 0.1007],\n",
      "        [ 0.0075],\n",
      "        [-0.1225],\n",
      "        [ 0.3042]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0123],\n",
      "        [ 1.0631],\n",
      "        [-0.0338],\n",
      "        [-0.0603],\n",
      "        [ 0.0123],\n",
      "        [ 0.0276],\n",
      "        [-0.0403],\n",
      "        [ 0.0365]], grad_fn=<AddmmBackward0>), tensor([[-0.0257],\n",
      "        [ 0.0032],\n",
      "        [ 0.2369],\n",
      "        [ 0.0493],\n",
      "        [-0.0320],\n",
      "        [-0.0378],\n",
      "        [ 0.1421],\n",
      "        [ 0.0022]], grad_fn=<AddmmBackward0>), tensor([[ 5.1709e-02],\n",
      "        [ 3.8229e-05],\n",
      "        [ 3.3782e-01],\n",
      "        [ 4.9427e-02],\n",
      "        [-6.4294e-03],\n",
      "        [ 4.0659e-02],\n",
      "        [ 1.4460e-01],\n",
      "        [-8.0067e-03]], grad_fn=<AddmmBackward0>), tensor([[-0.0372],\n",
      "        [ 0.0437],\n",
      "        [-0.0957],\n",
      "        [ 0.0200],\n",
      "        [ 0.0180],\n",
      "        [ 0.0456],\n",
      "        [-0.0429],\n",
      "        [ 0.0044]], grad_fn=<AddmmBackward0>), tensor([[ 0.0909],\n",
      "        [ 0.0369],\n",
      "        [ 0.1034],\n",
      "        [ 0.0809],\n",
      "        [ 0.5416],\n",
      "        [ 0.5306],\n",
      "        [-0.0382],\n",
      "        [ 0.1559]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.5171e-06, 9.9999e-01],\n",
      "        [1.9081e-03, 9.9809e-01],\n",
      "        [6.8630e-04, 9.9931e-01],\n",
      "        [1.5517e-04, 9.9984e-01],\n",
      "        [9.5171e-06, 9.9999e-01],\n",
      "        [1.7353e-05, 9.9998e-01],\n",
      "        [6.1489e-03, 9.9385e-01],\n",
      "        [1.0000e+00, 2.9669e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 9.8727e-06],\n",
      "        [9.9641e-01, 3.5940e-03],\n",
      "        [9.9997e-01, 3.1064e-05],\n",
      "        [9.9999e-01, 6.5867e-06],\n",
      "        [9.9999e-01, 8.1667e-06],\n",
      "        [9.9999e-01, 6.7529e-06],\n",
      "        [9.9993e-01, 6.8549e-05],\n",
      "        [9.6744e-05, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.7048e-05, 9.9994e-01],\n",
      "        [7.4126e-01, 2.5874e-01],\n",
      "        [1.0503e-03, 9.9895e-01],\n",
      "        [1.3179e-04, 9.9987e-01],\n",
      "        [4.3800e-05, 9.9996e-01],\n",
      "        [2.3723e-05, 9.9998e-01],\n",
      "        [4.0056e-04, 9.9960e-01],\n",
      "        [9.9999e-01, 7.3725e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9991e-01, 9.0195e-05],\n",
      "        [9.6724e-01, 3.2762e-02],\n",
      "        [9.9988e-01, 1.2273e-04],\n",
      "        [1.0000e+00, 1.7586e-06],\n",
      "        [9.9991e-01, 8.8362e-05],\n",
      "        [9.9999e-01, 5.4924e-06],\n",
      "        [9.9914e-01, 8.6003e-04],\n",
      "        [8.5653e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.6593e-04, 9.9973e-01],\n",
      "        [5.5696e-01, 4.4304e-01],\n",
      "        [1.7751e-04, 9.9982e-01],\n",
      "        [5.2017e-05, 9.9995e-01],\n",
      "        [7.5240e-05, 9.9992e-01],\n",
      "        [4.2703e-06, 1.0000e+00],\n",
      "        [3.7810e-02, 9.6219e-01],\n",
      "        [9.9999e-01, 5.5418e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 2, 7, 1, 6],\n",
      "        [2, 0, 1, 3, 5],\n",
      "        [6, 1, 0, 4, 8],\n",
      "        [8, 0, 6, 1, 2],\n",
      "        [8, 0, 7, 6, 6],\n",
      "        [2, 8, 0, 7, 3],\n",
      "        [5, 4, 0, 6, 6],\n",
      "        [7, 6, 1, 4, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3373],\n",
      "        [ 0.3503],\n",
      "        [ 0.3373],\n",
      "        [-0.2693],\n",
      "        [ 0.4467],\n",
      "        [ 0.0629],\n",
      "        [ 0.1017],\n",
      "        [-0.0778]], grad_fn=<AddmmBackward0>), tensor([[-0.3008],\n",
      "        [-0.1361],\n",
      "        [-0.1401],\n",
      "        [ 0.3760],\n",
      "        [ 0.2216],\n",
      "        [ 0.1261],\n",
      "        [ 0.4322],\n",
      "        [ 0.5639]], grad_fn=<AddmmBackward0>), tensor([[ 0.3780],\n",
      "        [ 0.2786],\n",
      "        [ 0.5353],\n",
      "        [-0.2203],\n",
      "        [ 0.2984],\n",
      "        [ 0.2676],\n",
      "        [ 0.0347],\n",
      "        [-0.1474]], grad_fn=<AddmmBackward0>), tensor([[-0.0215],\n",
      "        [ 0.1192],\n",
      "        [-0.3040],\n",
      "        [ 0.3553],\n",
      "        [ 0.1755],\n",
      "        [ 0.0783],\n",
      "        [ 0.0622],\n",
      "        [ 0.6547]], grad_fn=<AddmmBackward0>), tensor([[ 0.4524],\n",
      "        [ 0.2694],\n",
      "        [ 0.5970],\n",
      "        [-0.0121],\n",
      "        [ 0.0302],\n",
      "        [ 0.3305],\n",
      "        [-0.0021],\n",
      "        [-0.2127]], grad_fn=<AddmmBackward0>), tensor([[-0.0883],\n",
      "        [ 0.1498],\n",
      "        [ 0.1883],\n",
      "        [ 0.3418],\n",
      "        [ 0.0536],\n",
      "        [ 0.1459],\n",
      "        [ 0.0144],\n",
      "        [ 0.4271]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0264],\n",
      "        [-0.1026],\n",
      "        [ 0.0169],\n",
      "        [ 0.0549],\n",
      "        [ 1.0612],\n",
      "        [ 0.0866],\n",
      "        [-0.0841],\n",
      "        [ 0.0576]], grad_fn=<AddmmBackward0>), tensor([[-0.0296],\n",
      "        [ 0.0787],\n",
      "        [-0.0271],\n",
      "        [-0.0094],\n",
      "        [ 0.5030],\n",
      "        [ 0.0119],\n",
      "        [ 0.7482],\n",
      "        [ 0.1347]], grad_fn=<AddmmBackward0>), tensor([[-0.0062],\n",
      "        [ 0.4407],\n",
      "        [ 0.0084],\n",
      "        [-0.0651],\n",
      "        [ 0.2768],\n",
      "        [ 0.1380],\n",
      "        [ 0.0292],\n",
      "        [ 0.0362]], grad_fn=<AddmmBackward0>), tensor([[-0.0218],\n",
      "        [ 0.1262],\n",
      "        [ 0.0083],\n",
      "        [ 0.1273],\n",
      "        [-0.0061],\n",
      "        [ 0.0568],\n",
      "        [-0.0752],\n",
      "        [ 0.2368]], grad_fn=<AddmmBackward0>), tensor([[ 0.0963],\n",
      "        [ 0.6940],\n",
      "        [ 0.1561],\n",
      "        [ 0.0976],\n",
      "        [-0.0355],\n",
      "        [ 0.4697],\n",
      "        [-0.0733],\n",
      "        [ 0.1028]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.0140e-06, 1.0000e+00],\n",
      "        [1.0809e-05, 9.9999e-01],\n",
      "        [3.7109e-06, 1.0000e+00],\n",
      "        [9.9998e-01, 1.5752e-05],\n",
      "        [1.3757e-04, 9.9986e-01],\n",
      "        [7.0774e-06, 9.9999e-01],\n",
      "        [9.9975e-01, 2.5403e-04],\n",
      "        [9.9999e-01, 9.0911e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.1787e-05],\n",
      "        [9.9995e-01, 4.9129e-05],\n",
      "        [9.9997e-01, 2.9885e-05],\n",
      "        [8.6033e-06, 9.9999e-01],\n",
      "        [9.7905e-01, 2.0945e-02],\n",
      "        [9.9967e-01, 3.2927e-04],\n",
      "        [3.6051e-05, 9.9996e-01],\n",
      "        [5.5366e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.4091e-06, 1.0000e+00],\n",
      "        [5.0222e-06, 9.9999e-01],\n",
      "        [1.8964e-05, 9.9998e-01],\n",
      "        [9.9994e-01, 5.7985e-05],\n",
      "        [2.6574e-03, 9.9734e-01],\n",
      "        [1.7259e-05, 9.9998e-01],\n",
      "        [9.9474e-01, 5.2645e-03],\n",
      "        [9.9984e-01, 1.5943e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 2.6291e-05],\n",
      "        [9.9985e-01, 1.4662e-04],\n",
      "        [9.9983e-01, 1.6631e-04],\n",
      "        [7.4007e-06, 9.9999e-01],\n",
      "        [8.9821e-01, 1.0179e-01],\n",
      "        [9.9985e-01, 1.4941e-04],\n",
      "        [1.0955e-02, 9.8905e-01],\n",
      "        [2.6174e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[7.7157e-06, 9.9999e-01],\n",
      "        [2.4162e-05, 9.9998e-01],\n",
      "        [1.6705e-05, 9.9998e-01],\n",
      "        [9.9989e-01, 1.0915e-04],\n",
      "        [3.1618e-04, 9.9968e-01],\n",
      "        [4.3816e-05, 9.9996e-01],\n",
      "        [9.6846e-01, 3.1535e-02],\n",
      "        [9.9690e-01, 3.1019e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 5, 0, 0, 8],\n",
      "        [6, 1, 4, 8, 0],\n",
      "        [7, 6, 3, 2, 8],\n",
      "        [0, 6, 3, 7, 0],\n",
      "        [1, 5, 8, 6, 0],\n",
      "        [0, 2, 6, 1, 3],\n",
      "        [3, 7, 5, 6, 2],\n",
      "        [7, 0, 1, 0, 8]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2102],\n",
      "        [-0.0657],\n",
      "        [ 0.2655],\n",
      "        [ 0.4036],\n",
      "        [ 0.0635],\n",
      "        [-0.4928],\n",
      "        [-0.0397],\n",
      "        [ 0.3767]], grad_fn=<AddmmBackward0>), tensor([[ 0.0190],\n",
      "        [ 0.3036],\n",
      "        [-0.0582],\n",
      "        [-0.1686],\n",
      "        [ 0.2004],\n",
      "        [ 0.4143],\n",
      "        [ 0.2201],\n",
      "        [ 0.2959]], grad_fn=<AddmmBackward0>), tensor([[ 0.0880],\n",
      "        [ 0.1560],\n",
      "        [-0.2119],\n",
      "        [ 0.1872],\n",
      "        [ 0.1411],\n",
      "        [-0.1743],\n",
      "        [-0.0907],\n",
      "        [ 0.0048]], grad_fn=<AddmmBackward0>), tensor([[ 0.0468],\n",
      "        [-0.0057],\n",
      "        [ 0.2890],\n",
      "        [ 0.2259],\n",
      "        [ 0.2043],\n",
      "        [ 0.3108],\n",
      "        [ 0.3685],\n",
      "        [ 0.0636]], grad_fn=<AddmmBackward0>), tensor([[ 4.6184e-02],\n",
      "        [ 2.3643e-01],\n",
      "        [-4.1291e-02],\n",
      "        [ 1.9390e-01],\n",
      "        [ 1.0786e-01],\n",
      "        [ 2.2291e-01],\n",
      "        [-1.4569e-01],\n",
      "        [ 1.5432e-04]], grad_fn=<AddmmBackward0>), tensor([[-0.0494],\n",
      "        [-0.0233],\n",
      "        [ 0.1899],\n",
      "        [ 0.0016],\n",
      "        [ 0.3101],\n",
      "        [ 0.3633],\n",
      "        [ 0.4040],\n",
      "        [-0.0831]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2784],\n",
      "        [ 0.0554],\n",
      "        [ 0.0227],\n",
      "        [ 0.2912],\n",
      "        [-0.0068],\n",
      "        [ 0.0582],\n",
      "        [ 0.0156],\n",
      "        [ 0.9450]], grad_fn=<AddmmBackward0>), tensor([[ 0.0961],\n",
      "        [ 0.1338],\n",
      "        [ 0.0695],\n",
      "        [ 0.1312],\n",
      "        [-0.1085],\n",
      "        [ 0.0890],\n",
      "        [-0.0202],\n",
      "        [ 0.0412]], grad_fn=<AddmmBackward0>), tensor([[0.0407],\n",
      "        [0.0631],\n",
      "        [0.0614],\n",
      "        [0.6115],\n",
      "        [0.0560],\n",
      "        [0.0095],\n",
      "        [0.0866],\n",
      "        [0.0955]], grad_fn=<AddmmBackward0>), tensor([[-0.0386],\n",
      "        [ 0.1449],\n",
      "        [ 0.1502],\n",
      "        [ 1.0088],\n",
      "        [-0.0630],\n",
      "        [ 0.3912],\n",
      "        [ 0.2094],\n",
      "        [ 0.0108]], grad_fn=<AddmmBackward0>), tensor([[-0.0182],\n",
      "        [-0.0531],\n",
      "        [ 0.1633],\n",
      "        [ 0.1069],\n",
      "        [ 0.6417],\n",
      "        [ 0.6525],\n",
      "        [ 0.2259],\n",
      "        [-0.0385]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9464e-01, 5.3601e-03],\n",
      "        [9.9994e-01, 6.2711e-05],\n",
      "        [2.0182e-05, 9.9998e-01],\n",
      "        [6.0019e-05, 9.9994e-01],\n",
      "        [6.0906e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 8.1706e-06],\n",
      "        [9.6498e-05, 9.9990e-01],\n",
      "        [1.0246e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.2353e-04, 9.9958e-01],\n",
      "        [2.9022e-05, 9.9997e-01],\n",
      "        [9.9989e-01, 1.0901e-04],\n",
      "        [9.9945e-01, 5.5132e-04],\n",
      "        [9.9977e-01, 2.2711e-04],\n",
      "        [8.9520e-06, 9.9999e-01],\n",
      "        [9.9995e-01, 4.7131e-05],\n",
      "        [9.9360e-01, 6.4002e-03]], grad_fn=<SoftmaxBackward0>), tensor([[9.4021e-01, 5.9793e-02],\n",
      "        [9.9974e-01, 2.5861e-04],\n",
      "        [3.2146e-05, 9.9997e-01],\n",
      "        [1.1730e-04, 9.9988e-01],\n",
      "        [8.2489e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 6.0663e-06],\n",
      "        [9.7563e-05, 9.9990e-01],\n",
      "        [1.1598e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.4412e-01, 8.5588e-01],\n",
      "        [7.8923e-05, 9.9992e-01],\n",
      "        [9.9992e-01, 7.8907e-05],\n",
      "        [9.9973e-01, 2.6684e-04],\n",
      "        [9.9992e-01, 7.6226e-05],\n",
      "        [2.3619e-05, 9.9998e-01],\n",
      "        [9.9993e-01, 6.7675e-05],\n",
      "        [9.9020e-01, 9.7995e-03]], grad_fn=<SoftmaxBackward0>), tensor([[4.8628e-01, 5.1372e-01],\n",
      "        [9.9793e-01, 2.0661e-03],\n",
      "        [5.8849e-05, 9.9994e-01],\n",
      "        [5.6934e-04, 9.9943e-01],\n",
      "        [9.8374e-06, 9.9999e-01],\n",
      "        [9.9999e-01, 1.3874e-05],\n",
      "        [2.1796e-05, 9.9998e-01],\n",
      "        [1.8829e-03, 9.9812e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 1, 0, 1, 3],\n",
      "        [2, 7, 5, 0, 3],\n",
      "        [2, 4, 8, 1, 6],\n",
      "        [6, 0, 8, 0, 3],\n",
      "        [2, 1, 0, 1, 3],\n",
      "        [2, 4, 1, 6, 8],\n",
      "        [1, 0, 7, 1, 3],\n",
      "        [0, 6, 0, 1, 3]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.4158],\n",
      "        [-0.2923],\n",
      "        [-0.3147],\n",
      "        [ 0.3844],\n",
      "        [ 0.1012],\n",
      "        [-0.3147],\n",
      "        [ 0.1517],\n",
      "        [-0.3657]], grad_fn=<AddmmBackward0>), tensor([[ 0.1415],\n",
      "        [ 0.1869],\n",
      "        [ 0.2901],\n",
      "        [-0.5831],\n",
      "        [-0.0540],\n",
      "        [ 0.2901],\n",
      "        [ 0.3460],\n",
      "        [ 0.1611]], grad_fn=<AddmmBackward0>), tensor([[ 0.0294],\n",
      "        [ 0.1096],\n",
      "        [-0.3644],\n",
      "        [ 0.1716],\n",
      "        [ 0.1961],\n",
      "        [-0.3644],\n",
      "        [ 0.0330],\n",
      "        [ 0.0199]], grad_fn=<AddmmBackward0>), tensor([[-0.1267],\n",
      "        [ 0.0267],\n",
      "        [ 0.2018],\n",
      "        [ 0.0937],\n",
      "        [-0.0264],\n",
      "        [ 0.4266],\n",
      "        [ 0.0201],\n",
      "        [-0.0549]], grad_fn=<AddmmBackward0>), tensor([[-0.0247],\n",
      "        [-0.0266],\n",
      "        [-0.0139],\n",
      "        [-0.0336],\n",
      "        [-0.0071],\n",
      "        [-0.2961],\n",
      "        [-0.0018],\n",
      "        [-0.0802]], grad_fn=<AddmmBackward0>), tensor([[-0.0126],\n",
      "        [-0.0284],\n",
      "        [-0.0083],\n",
      "        [ 0.0359],\n",
      "        [ 0.0328],\n",
      "        [ 0.1135],\n",
      "        [-0.0029],\n",
      "        [ 0.0361]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0219],\n",
      "        [ 0.1136],\n",
      "        [ 0.0095],\n",
      "        [-0.1899],\n",
      "        [ 0.0119],\n",
      "        [ 0.0095],\n",
      "        [ 0.7948],\n",
      "        [ 0.0379]], grad_fn=<AddmmBackward0>), tensor([[0.1373],\n",
      "        [0.7613],\n",
      "        [0.0083],\n",
      "        [0.0571],\n",
      "        [0.7994],\n",
      "        [0.0083],\n",
      "        [0.0263],\n",
      "        [0.5448]], grad_fn=<AddmmBackward0>), tensor([[-0.0577],\n",
      "        [ 0.8803],\n",
      "        [ 0.0162],\n",
      "        [ 0.8751],\n",
      "        [-0.0370],\n",
      "        [-0.0410],\n",
      "        [ 0.0072],\n",
      "        [-0.0342]], grad_fn=<AddmmBackward0>), tensor([[ 0.0563],\n",
      "        [ 0.0297],\n",
      "        [ 0.2180],\n",
      "        [ 0.0604],\n",
      "        [ 0.0282],\n",
      "        [ 0.1720],\n",
      "        [-0.0064],\n",
      "        [-0.0362]], grad_fn=<AddmmBackward0>), tensor([[-0.1600],\n",
      "        [ 0.0168],\n",
      "        [ 0.4489],\n",
      "        [ 0.0190],\n",
      "        [ 0.0815],\n",
      "        [-0.0025],\n",
      "        [-0.0377],\n",
      "        [ 0.0032]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 2.4503e-05],\n",
      "        [9.9960e-01, 3.9932e-04],\n",
      "        [9.9990e-01, 1.0485e-04],\n",
      "        [7.0121e-05, 9.9993e-01],\n",
      "        [9.9977e-01, 2.3144e-04],\n",
      "        [9.9990e-01, 1.0485e-04],\n",
      "        [9.9992e-01, 8.2145e-05],\n",
      "        [9.9965e-01, 3.5284e-04]], grad_fn=<SoftmaxBackward0>), tensor([[1.2045e-04, 9.9988e-01],\n",
      "        [2.4237e-05, 9.9998e-01],\n",
      "        [5.7624e-05, 9.9994e-01],\n",
      "        [9.9701e-01, 2.9860e-03],\n",
      "        [4.5887e-05, 9.9995e-01],\n",
      "        [5.7624e-05, 9.9994e-01],\n",
      "        [1.5528e-03, 9.9845e-01],\n",
      "        [3.0771e-03, 9.9692e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.7981e-01, 2.0193e-02],\n",
      "        [9.9943e-01, 5.6801e-04],\n",
      "        [9.9997e-01, 3.1723e-05],\n",
      "        [1.1810e-03, 9.9882e-01],\n",
      "        [9.1515e-01, 8.4850e-02],\n",
      "        [9.9988e-01, 1.1895e-04],\n",
      "        [3.0752e-01, 6.9248e-01],\n",
      "        [7.0031e-01, 2.9969e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.2704e-04, 9.9987e-01],\n",
      "        [6.8694e-04, 9.9931e-01],\n",
      "        [9.1179e-06, 9.9999e-01],\n",
      "        [9.8000e-01, 2.0004e-02],\n",
      "        [3.7522e-04, 9.9962e-01],\n",
      "        [3.9870e-05, 9.9996e-01],\n",
      "        [2.6009e-02, 9.7399e-01],\n",
      "        [5.6440e-02, 9.4356e-01]], grad_fn=<SoftmaxBackward0>), tensor([[8.7042e-01, 1.2958e-01],\n",
      "        [7.6841e-01, 2.3159e-01],\n",
      "        [9.9984e-01, 1.5952e-04],\n",
      "        [1.1628e-04, 9.9988e-01],\n",
      "        [8.5291e-01, 1.4709e-01],\n",
      "        [9.9985e-01, 1.5074e-04],\n",
      "        [2.8608e-02, 9.7139e-01],\n",
      "        [3.6113e-02, 9.6389e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "6900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 1, 0, 5, 2],\n",
      "        [0, 7, 0, 3, 0],\n",
      "        [4, 1, 8, 6, 0],\n",
      "        [5, 6, 0, 3, 0],\n",
      "        [2, 6, 8, 1, 5],\n",
      "        [0, 7, 3, 0, 0],\n",
      "        [1, 2, 8, 4, 5],\n",
      "        [4, 1, 0, 6, 5]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.0076],\n",
      "        [-0.4660],\n",
      "        [ 0.1880],\n",
      "        [ 0.2594],\n",
      "        [ 0.1168],\n",
      "        [ 0.1356],\n",
      "        [ 0.2744],\n",
      "        [ 0.1880]], grad_fn=<AddmmBackward0>), tensor([[-0.2889],\n",
      "        [ 0.4813],\n",
      "        [-0.2356],\n",
      "        [ 0.5043],\n",
      "        [-0.1918],\n",
      "        [ 0.2441],\n",
      "        [-0.2772],\n",
      "        [-0.2356]], grad_fn=<AddmmBackward0>), tensor([[ 0.2886],\n",
      "        [ 0.1248],\n",
      "        [ 0.4281],\n",
      "        [-0.0481],\n",
      "        [ 0.3275],\n",
      "        [ 0.2054],\n",
      "        [ 0.2804],\n",
      "        [ 0.4281]], grad_fn=<AddmmBackward0>), tensor([[ 0.0526],\n",
      "        [ 0.0365],\n",
      "        [-0.2294],\n",
      "        [ 0.0644],\n",
      "        [ 0.0737],\n",
      "        [ 0.3305],\n",
      "        [ 0.0083],\n",
      "        [-0.5073]], grad_fn=<AddmmBackward0>), tensor([[ 0.4286],\n",
      "        [ 0.0110],\n",
      "        [ 0.3206],\n",
      "        [-0.0018],\n",
      "        [ 0.1630],\n",
      "        [-0.0536],\n",
      "        [ 0.1744],\n",
      "        [ 0.5567]], grad_fn=<AddmmBackward0>), tensor([[-1.4213e-02],\n",
      "        [-3.9451e-06],\n",
      "        [-4.9071e-03],\n",
      "        [ 4.7373e-02],\n",
      "        [ 1.4654e-01],\n",
      "        [ 8.3977e-03],\n",
      "        [ 2.2184e-01],\n",
      "        [-2.8514e-01]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0871],\n",
      "        [-0.0540],\n",
      "        [ 0.0349],\n",
      "        [ 0.0304],\n",
      "        [-0.0315],\n",
      "        [ 0.0793],\n",
      "        [-0.0040],\n",
      "        [ 0.0349]], grad_fn=<AddmmBackward0>), tensor([[ 0.0298],\n",
      "        [ 0.7395],\n",
      "        [ 0.0196],\n",
      "        [ 0.9021],\n",
      "        [-0.0246],\n",
      "        [ 0.7167],\n",
      "        [-0.1766],\n",
      "        [ 0.0196]], grad_fn=<AddmmBackward0>), tensor([[ 0.2045],\n",
      "        [ 0.0078],\n",
      "        [ 0.0226],\n",
      "        [ 0.0206],\n",
      "        [ 0.0961],\n",
      "        [ 0.9534],\n",
      "        [ 0.0758],\n",
      "        [-0.0188]], grad_fn=<AddmmBackward0>), tensor([[ 0.2308],\n",
      "        [ 0.0234],\n",
      "        [ 0.0341],\n",
      "        [ 0.0565],\n",
      "        [ 0.0210],\n",
      "        [-0.0068],\n",
      "        [-0.0942],\n",
      "        [ 0.0205]], grad_fn=<AddmmBackward0>), tensor([[ 0.5655],\n",
      "        [ 0.0276],\n",
      "        [ 0.6831],\n",
      "        [ 0.0125],\n",
      "        [ 0.8135],\n",
      "        [ 0.0056],\n",
      "        [ 0.1781],\n",
      "        [-0.0792]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[4.1182e-04, 9.9959e-01],\n",
      "        [9.9971e-01, 2.9205e-04],\n",
      "        [1.7910e-05, 9.9998e-01],\n",
      "        [9.9998e-01, 1.8210e-05],\n",
      "        [1.0308e-04, 9.9990e-01],\n",
      "        [9.9996e-01, 4.1065e-05],\n",
      "        [1.8863e-05, 9.9998e-01],\n",
      "        [1.7910e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9848e-01, 1.5159e-03],\n",
      "        [2.5154e-05, 9.9997e-01],\n",
      "        [9.9993e-01, 6.9744e-05],\n",
      "        [1.4322e-04, 9.9986e-01],\n",
      "        [9.9983e-01, 1.7146e-04],\n",
      "        [2.3635e-05, 9.9998e-01],\n",
      "        [9.9991e-01, 9.2638e-05],\n",
      "        [9.9993e-01, 6.9744e-05]], grad_fn=<SoftmaxBackward0>), tensor([[4.1785e-04, 9.9958e-01],\n",
      "        [9.7552e-01, 2.4479e-02],\n",
      "        [1.0155e-04, 9.9990e-01],\n",
      "        [9.9070e-01, 9.2999e-03],\n",
      "        [2.7216e-04, 9.9973e-01],\n",
      "        [9.9995e-01, 4.5904e-05],\n",
      "        [1.8148e-05, 9.9998e-01],\n",
      "        [3.5886e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9716e-01, 2.8440e-03],\n",
      "        [2.8685e-04, 9.9971e-01],\n",
      "        [9.9997e-01, 3.0121e-05],\n",
      "        [1.2694e-04, 9.9987e-01],\n",
      "        [9.9852e-01, 1.4774e-03],\n",
      "        [4.3239e-04, 9.9957e-01],\n",
      "        [9.9965e-01, 3.5034e-04],\n",
      "        [9.9931e-01, 6.8769e-04]], grad_fn=<SoftmaxBackward0>), tensor([[7.8684e-06, 9.9999e-01],\n",
      "        [9.9343e-01, 6.5689e-03],\n",
      "        [2.6225e-05, 9.9997e-01],\n",
      "        [9.9640e-01, 3.6040e-03],\n",
      "        [1.3787e-04, 9.9986e-01],\n",
      "        [7.8686e-01, 2.1314e-01],\n",
      "        [2.2284e-05, 9.9998e-01],\n",
      "        [2.9299e-04, 9.9971e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 7, in <module>\n",
      "    from search.search_factories import create_mcts\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/search_factories.py\", line 1, in <module>\n",
      "    from search.algorithms import GumbelSequentialHalving, SearchAlgorithm, UCTSearch\n",
      "ImportError: cannot import name 'GumbelSequentialHalving' from 'search.algorithms' (/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 6, 1, 3, 4],\n",
      "        [4, 6, 8, 2, 5],\n",
      "        [4, 8, 6, 2, 1],\n",
      "        [5, 6, 1, 7, 0],\n",
      "        [0, 4, 2, 5, 6],\n",
      "        [2, 3, 0, 8, 5],\n",
      "        [0, 2, 0, 2, 8],\n",
      "        [2, 1, 5, 4, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.4494],\n",
      "        [ 0.3019],\n",
      "        [ 0.0030],\n",
      "        [ 0.1663],\n",
      "        [ 0.3019],\n",
      "        [-1.0709],\n",
      "        [-0.4212],\n",
      "        [ 0.3019]], grad_fn=<AddmmBackward0>), tensor([[-0.3071],\n",
      "        [-0.4413],\n",
      "        [-0.1316],\n",
      "        [-0.2528],\n",
      "        [-0.4627],\n",
      "        [ 0.4748],\n",
      "        [ 0.3765],\n",
      "        [-0.3052]], grad_fn=<AddmmBackward0>), tensor([[ 0.3589],\n",
      "        [ 0.4159],\n",
      "        [ 0.1580],\n",
      "        [ 0.3567],\n",
      "        [ 0.1598],\n",
      "        [-0.3398],\n",
      "        [-0.1099],\n",
      "        [ 0.3247]], grad_fn=<AddmmBackward0>), tensor([[-0.3564],\n",
      "        [-0.4321],\n",
      "        [-0.1071],\n",
      "        [ 0.0045],\n",
      "        [-0.3023],\n",
      "        [ 0.0937],\n",
      "        [ 0.0264],\n",
      "        [-0.2815]], grad_fn=<AddmmBackward0>), tensor([[ 0.5049],\n",
      "        [ 0.3168],\n",
      "        [ 0.3038],\n",
      "        [ 0.3073],\n",
      "        [ 0.1082],\n",
      "        [-0.1427],\n",
      "        [-0.1200],\n",
      "        [ 0.4779]], grad_fn=<AddmmBackward0>), tensor([[-0.5137],\n",
      "        [-0.1691],\n",
      "        [ 0.0961],\n",
      "        [-0.0344],\n",
      "        [ 0.0119],\n",
      "        [ 0.0966],\n",
      "        [ 0.0405],\n",
      "        [-0.3737]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.1185],\n",
      "        [ 0.0194],\n",
      "        [-0.0476],\n",
      "        [-0.0096],\n",
      "        [ 0.0547],\n",
      "        [-0.0019],\n",
      "        [-0.0236],\n",
      "        [ 0.0273]], grad_fn=<AddmmBackward0>), tensor([[-0.0196],\n",
      "        [ 0.0345],\n",
      "        [-0.0829],\n",
      "        [ 0.3614],\n",
      "        [-0.0124],\n",
      "        [ 0.3084],\n",
      "        [ 0.6604],\n",
      "        [ 0.1160]], grad_fn=<AddmmBackward0>), tensor([[0.1089],\n",
      "        [0.0739],\n",
      "        [0.3654],\n",
      "        [0.7462],\n",
      "        [0.0081],\n",
      "        [0.0296],\n",
      "        [0.0060],\n",
      "        [0.0653]], grad_fn=<AddmmBackward0>), tensor([[ 0.1019],\n",
      "        [-0.0009],\n",
      "        [ 0.2147],\n",
      "        [ 0.8270],\n",
      "        [-0.0353],\n",
      "        [ 0.0526],\n",
      "        [-0.0479],\n",
      "        [ 0.0255]], grad_fn=<AddmmBackward0>), tensor([[0.1125],\n",
      "        [0.1813],\n",
      "        [0.8670],\n",
      "        [0.0099],\n",
      "        [0.4789],\n",
      "        [0.0038],\n",
      "        [0.0052],\n",
      "        [0.0616]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[8.1464e-05, 9.9992e-01],\n",
      "        [2.2905e-05, 9.9998e-01],\n",
      "        [3.4650e-05, 9.9997e-01],\n",
      "        [1.9671e-03, 9.9803e-01],\n",
      "        [4.6305e-05, 9.9995e-01],\n",
      "        [9.9996e-01, 4.3739e-05],\n",
      "        [9.9998e-01, 1.9149e-05],\n",
      "        [4.8763e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 3.1056e-05],\n",
      "        [9.9995e-01, 4.5199e-05],\n",
      "        [9.9999e-01, 1.1725e-05],\n",
      "        [9.9996e-01, 3.6503e-05],\n",
      "        [9.9993e-01, 7.2558e-05],\n",
      "        [1.1122e-04, 9.9989e-01],\n",
      "        [1.4345e-04, 9.9986e-01],\n",
      "        [9.9997e-01, 2.9887e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.6951e-05, 9.9998e-01],\n",
      "        [9.2113e-05, 9.9991e-01],\n",
      "        [2.4389e-05, 9.9998e-01],\n",
      "        [2.9404e-04, 9.9971e-01],\n",
      "        [7.8735e-05, 9.9992e-01],\n",
      "        [9.9985e-01, 1.4812e-04],\n",
      "        [9.9793e-01, 2.0721e-03],\n",
      "        [2.3494e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9972e-01, 2.7816e-04],\n",
      "        [9.9999e-01, 1.1029e-05],\n",
      "        [9.9993e-01, 6.6478e-05],\n",
      "        [9.9473e-01, 5.2733e-03],\n",
      "        [9.9997e-01, 2.8214e-05],\n",
      "        [1.2866e-03, 9.9871e-01],\n",
      "        [4.4355e-02, 9.5565e-01],\n",
      "        [9.9999e-01, 9.6896e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.2023e-04, 9.9988e-01],\n",
      "        [7.9184e-05, 9.9992e-01],\n",
      "        [2.1446e-05, 9.9998e-01],\n",
      "        [5.7257e-02, 9.4274e-01],\n",
      "        [1.0062e-04, 9.9990e-01],\n",
      "        [9.9973e-01, 2.7414e-04],\n",
      "        [9.9878e-01, 1.2176e-03],\n",
      "        [4.5623e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "7100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 0, 5, 4, 5],\n",
      "        [7, 0, 0, 4, 5],\n",
      "        [1, 6, 3, 8, 0],\n",
      "        [6, 2, 0, 5, 0],\n",
      "        [8, 2, 0, 4, 7],\n",
      "        [3, 7, 6, 0, 5],\n",
      "        [4, 0, 6, 3, 1],\n",
      "        [1, 0, 5, 4, 5]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3186],\n",
      "        [ 0.4934],\n",
      "        [-0.0524],\n",
      "        [-0.6542],\n",
      "        [ 0.2652],\n",
      "        [ 0.4177],\n",
      "        [ 0.3117],\n",
      "        [ 0.0342]], grad_fn=<AddmmBackward0>), tensor([[ 0.3710],\n",
      "        [-0.1019],\n",
      "        [ 0.5109],\n",
      "        [ 0.6810],\n",
      "        [ 0.0389],\n",
      "        [-0.2615],\n",
      "        [-0.4644],\n",
      "        [ 0.6486]], grad_fn=<AddmmBackward0>), tensor([[-0.0979],\n",
      "        [ 0.4279],\n",
      "        [-0.3080],\n",
      "        [-0.2871],\n",
      "        [ 0.2559],\n",
      "        [ 0.4599],\n",
      "        [ 0.3098],\n",
      "        [-0.0618]], grad_fn=<AddmmBackward0>), tensor([[ 0.1134],\n",
      "        [-0.0813],\n",
      "        [ 0.5088],\n",
      "        [ 0.5352],\n",
      "        [ 0.1579],\n",
      "        [ 0.0874],\n",
      "        [-0.5406],\n",
      "        [ 0.0149]], grad_fn=<AddmmBackward0>), tensor([[-0.2773],\n",
      "        [ 0.0071],\n",
      "        [-0.2134],\n",
      "        [-0.0262],\n",
      "        [ 0.1336],\n",
      "        [ 0.1347],\n",
      "        [ 0.7432],\n",
      "        [-0.1490]], grad_fn=<AddmmBackward0>), tensor([[ 0.1742],\n",
      "        [-0.0951],\n",
      "        [ 0.5703],\n",
      "        [ 0.0265],\n",
      "        [ 0.5237],\n",
      "        [ 0.0146],\n",
      "        [-0.4158],\n",
      "        [ 0.0035]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.3799],\n",
      "        [0.2476],\n",
      "        [0.0343],\n",
      "        [0.0229],\n",
      "        [0.0297],\n",
      "        [0.1592],\n",
      "        [0.0300],\n",
      "        [1.0254]], grad_fn=<AddmmBackward0>), tensor([[ 0.0228],\n",
      "        [ 0.3955],\n",
      "        [ 0.1420],\n",
      "        [ 0.5045],\n",
      "        [-0.0832],\n",
      "        [ 0.1371],\n",
      "        [ 0.0127],\n",
      "        [ 0.0133]], grad_fn=<AddmmBackward0>), tensor([[-0.0039],\n",
      "        [-0.0283],\n",
      "        [ 0.0591],\n",
      "        [ 0.1872],\n",
      "        [ 0.0282],\n",
      "        [ 1.1056],\n",
      "        [ 0.0263],\n",
      "        [-0.0242]], grad_fn=<AddmmBackward0>), tensor([[ 0.0309],\n",
      "        [-0.0137],\n",
      "        [ 0.1599],\n",
      "        [ 0.5459],\n",
      "        [-0.0495],\n",
      "        [-0.0448],\n",
      "        [ 0.0355],\n",
      "        [ 0.0196]], grad_fn=<AddmmBackward0>), tensor([[-0.0257],\n",
      "        [-0.0090],\n",
      "        [ 0.2373],\n",
      "        [ 0.0253],\n",
      "        [ 0.2960],\n",
      "        [ 0.0659],\n",
      "        [-0.0668],\n",
      "        [ 0.0086]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9992e-01, 8.3848e-05],\n",
      "        [1.7511e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 1.7800e-06],\n",
      "        [9.9999e-01, 6.9916e-06],\n",
      "        [2.0331e-03, 9.9797e-01],\n",
      "        [1.3020e-03, 9.9870e-01],\n",
      "        [2.6641e-05, 9.9997e-01],\n",
      "        [9.9999e-01, 1.0427e-05]], grad_fn=<SoftmaxBackward0>), tensor([[8.0672e-04, 9.9919e-01],\n",
      "        [9.9988e-01, 1.2003e-04],\n",
      "        [2.9499e-05, 9.9997e-01],\n",
      "        [1.9473e-05, 9.9998e-01],\n",
      "        [9.9916e-01, 8.4066e-04],\n",
      "        [9.9997e-01, 2.9935e-05],\n",
      "        [9.9999e-01, 9.9249e-06],\n",
      "        [2.5301e-03, 9.9747e-01]], grad_fn=<SoftmaxBackward0>), tensor([[8.7154e-01, 1.2846e-01],\n",
      "        [5.5944e-03, 9.9441e-01],\n",
      "        [9.9999e-01, 6.0591e-06],\n",
      "        [9.9999e-01, 1.1557e-05],\n",
      "        [2.3195e-04, 9.9977e-01],\n",
      "        [1.4732e-03, 9.9853e-01],\n",
      "        [1.9799e-04, 9.9980e-01],\n",
      "        [9.8160e-01, 1.8397e-02]], grad_fn=<SoftmaxBackward0>), tensor([[8.8048e-02, 9.1195e-01],\n",
      "        [9.9153e-01, 8.4737e-03],\n",
      "        [2.7121e-04, 9.9973e-01],\n",
      "        [9.1307e-05, 9.9991e-01],\n",
      "        [9.9999e-01, 1.2086e-05],\n",
      "        [9.9914e-01, 8.5898e-04],\n",
      "        [1.0000e+00, 3.3753e-06],\n",
      "        [4.3762e-02, 9.5624e-01]], grad_fn=<SoftmaxBackward0>), tensor([[8.8140e-01, 1.1860e-01],\n",
      "        [4.7004e-03, 9.9530e-01],\n",
      "        [9.9996e-01, 4.3330e-05],\n",
      "        [9.9961e-01, 3.8649e-04],\n",
      "        [3.3459e-05, 9.9997e-01],\n",
      "        [4.5140e-03, 9.9549e-01],\n",
      "        [2.2072e-04, 9.9978e-01],\n",
      "        [8.4451e-01, 1.5549e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "7200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 0, 7, 6, 0],\n",
      "        [7, 4, 3, 2, 5],\n",
      "        [4, 3, 7, 0, 0],\n",
      "        [4, 5, 8, 1, 2],\n",
      "        [4, 0, 7, 6, 0],\n",
      "        [7, 5, 2, 4, 3],\n",
      "        [1, 0, 7, 6, 0],\n",
      "        [3, 5, 0, 6, 0]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.6399],\n",
      "        [-0.4186],\n",
      "        [-0.2112],\n",
      "        [-0.3051],\n",
      "        [ 0.4239],\n",
      "        [-0.4186],\n",
      "        [ 0.5968],\n",
      "        [-0.1805]], grad_fn=<AddmmBackward0>), tensor([[-0.1165],\n",
      "        [ 0.3747],\n",
      "        [ 0.1364],\n",
      "        [-0.0918],\n",
      "        [-0.0948],\n",
      "        [ 0.3747],\n",
      "        [-0.1765],\n",
      "        [ 0.2500]], grad_fn=<AddmmBackward0>), tensor([[ 0.0710],\n",
      "        [-0.5642],\n",
      "        [-0.0991],\n",
      "        [ 0.3422],\n",
      "        [ 0.1960],\n",
      "        [-0.2162],\n",
      "        [ 0.0268],\n",
      "        [-0.0811]], grad_fn=<AddmmBackward0>), tensor([[ 0.0065],\n",
      "        [ 0.7092],\n",
      "        [ 0.0790],\n",
      "        [ 0.1254],\n",
      "        [-0.0288],\n",
      "        [ 0.3232],\n",
      "        [-0.0192],\n",
      "        [-0.0624]], grad_fn=<AddmmBackward0>), tensor([[-0.0176],\n",
      "        [-0.3660],\n",
      "        [-0.0389],\n",
      "        [ 0.3366],\n",
      "        [-0.0097],\n",
      "        [-0.2864],\n",
      "        [-0.0212],\n",
      "        [-0.0343]], grad_fn=<AddmmBackward0>), tensor([[ 0.0328],\n",
      "        [ 0.4728],\n",
      "        [-0.0821],\n",
      "        [ 0.1418],\n",
      "        [-0.0531],\n",
      "        [ 0.6384],\n",
      "        [-0.0522],\n",
      "        [-0.0531]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[1.0866],\n",
      "        [0.0400],\n",
      "        [0.2070],\n",
      "        [0.0480],\n",
      "        [0.1563],\n",
      "        [0.0400],\n",
      "        [0.2560],\n",
      "        [0.0957]], grad_fn=<AddmmBackward0>), tensor([[0.0117],\n",
      "        [0.0243],\n",
      "        [0.1140],\n",
      "        [0.0144],\n",
      "        [0.0290],\n",
      "        [0.0044],\n",
      "        [0.0658],\n",
      "        [0.2338]], grad_fn=<AddmmBackward0>), tensor([[-0.0289],\n",
      "        [ 0.0202],\n",
      "        [ 0.8672],\n",
      "        [ 0.2635],\n",
      "        [ 0.0390],\n",
      "        [ 0.0138],\n",
      "        [ 0.0406],\n",
      "        [-0.0560]], grad_fn=<AddmmBackward0>), tensor([[-0.0562],\n",
      "        [-0.0102],\n",
      "        [ 0.0186],\n",
      "        [ 0.0029],\n",
      "        [ 0.0462],\n",
      "        [ 0.0127],\n",
      "        [ 0.0443],\n",
      "        [ 0.0455]], grad_fn=<AddmmBackward0>), tensor([[4.0124e-02],\n",
      "        [2.3803e-01],\n",
      "        [8.7766e-03],\n",
      "        [6.6347e-01],\n",
      "        [5.9241e-04],\n",
      "        [3.0582e-02],\n",
      "        [1.0759e-02],\n",
      "        [8.2663e-03]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.4733e-05, 9.9997e-01],\n",
      "        [9.9994e-01, 5.9582e-05],\n",
      "        [9.9996e-01, 3.8119e-05],\n",
      "        [1.0000e+00, 3.9623e-06],\n",
      "        [5.1512e-04, 9.9948e-01],\n",
      "        [9.9994e-01, 5.9582e-05],\n",
      "        [1.8866e-05, 9.9998e-01],\n",
      "        [9.9995e-01, 5.2787e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9993e-01, 7.2891e-05],\n",
      "        [4.1758e-05, 9.9996e-01],\n",
      "        [2.9660e-05, 9.9997e-01],\n",
      "        [6.4336e-05, 9.9994e-01],\n",
      "        [9.9897e-01, 1.0302e-03],\n",
      "        [3.2266e-05, 9.9997e-01],\n",
      "        [9.9906e-01, 9.4260e-04],\n",
      "        [1.3898e-04, 9.9986e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.2834e-05, 9.9997e-01],\n",
      "        [9.9991e-01, 8.9881e-05],\n",
      "        [9.9989e-01, 1.0879e-04],\n",
      "        [1.0000e+00, 2.8283e-06],\n",
      "        [3.2457e-04, 9.9968e-01],\n",
      "        [9.9998e-01, 1.8048e-05],\n",
      "        [2.3217e-03, 9.9768e-01],\n",
      "        [9.7935e-01, 2.0651e-02]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 8.9611e-06],\n",
      "        [2.3157e-05, 9.9998e-01],\n",
      "        [8.3723e-03, 9.9163e-01],\n",
      "        [7.4521e-06, 9.9999e-01],\n",
      "        [9.9823e-01, 1.7728e-03],\n",
      "        [5.5613e-05, 9.9994e-01],\n",
      "        [9.9362e-01, 6.3762e-03],\n",
      "        [7.6144e-03, 9.9239e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.2536e-03, 9.9875e-01],\n",
      "        [9.9999e-01, 1.3890e-05],\n",
      "        [9.6353e-01, 3.6466e-02],\n",
      "        [9.9997e-01, 3.1571e-05],\n",
      "        [2.2432e-01, 7.7568e-01],\n",
      "        [9.9996e-01, 3.6519e-05],\n",
      "        [4.8679e-01, 5.1321e-01],\n",
      "        [9.9468e-01, 5.3212e-03]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1107, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 150, in run\n",
      "    state,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 299, in _run_single_simulation\n",
      "    return (\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/search_nodes.py\", line 205, in expand\n",
      "    self._populate_children(allowed_actions, policy)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/search_nodes.py\", line 209, in _populate_children\n",
      "    allowed_policy = {a: policy[a] for a in allowed_actions}\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/search_nodes.py\", line 209, in <dictcomp>\n",
      "    allowed_policy = {a: policy[a] for a in allowed_actions}\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1107, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 150, in run\n",
      "    state,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 276, in _run_single_simulation\n",
      "    self,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1166, in recurrent_inference\n",
      "    self.world_model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero_world_model.py\", line 266, in recurrent_inference\n",
      "    reward, next_hidden_state, to_play, reward_hidden = self.dynamics(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero_world_model.py\", line 163, in forward\n",
      "    next_hidden_state = self._fuse_and_process(hidden_state, action)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero_world_model.py\", line 102, in _fuse_and_process\n",
      "    S = F.relu(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 1686, in relu\n",
      "    def relu(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1107, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1107, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 150, in run\n",
      "    state,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 150, in run\n",
      "    state,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 276, in _run_single_simulation\n",
      "    self,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/algorithms.py\", line 276, in _run_single_simulation\n",
      "    self,\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1171, in recurrent_inference\n",
      "    value, policy = self.prediction(next_hidden_state)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1171, in recurrent_inference\n",
      "    value, policy = self.prediction(next_hidden_state)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 959, in forward\n",
      "    return self.head(S)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 959, in forward\n",
      "    return self.head(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 937, in forward\n",
      "    return self.critic(S), self.actor(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/critic.py\", line 30, in forward\n",
      "    x = self.net(inputs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 126, in forward\n",
      "    x = self.conv_layers(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 937, in forward\n",
      "    return self.critic(S), self.actor(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/critic.py\", line 30, in forward\n",
      "    x = self.net(inputs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/conv.py\", line 94, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 126, in forward\n",
      "    x = self.conv_layers(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/conv.py\", line 94, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     81\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 83\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:337\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_replay_buffer_size:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[1;32m    327\u001b[0m         (\n\u001b[1;32m    328\u001b[0m             value_loss,\n\u001b[1;32m    329\u001b[0m             policy_loss,\n\u001b[1;32m    330\u001b[0m             reward_loss,\n\u001b[1;32m    331\u001b[0m             to_play_loss,\n\u001b[1;32m    332\u001b[0m             cons_loss,\n\u001b[1;32m    333\u001b[0m             q_loss,\n\u001b[1;32m    334\u001b[0m             sigma_loss,\n\u001b[1;32m    335\u001b[0m             vqvae_commitment_cost,\n\u001b[1;32m    336\u001b[0m             loss,\n\u001b[0;32m--> 337\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:938\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m loss_mean \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 938\u001b[0m \u001b[43mloss_mean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    940\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 10,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"stochastic\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": False,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"refactored_search_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa549b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from muzero.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"dynamics_residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.5,\n",
    "    # \"min_replay_buffer_size\": 1000,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"stochastic_fixed_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5693ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": True,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"efficient_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8494974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 2,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"consistency_loss_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf70d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"reanalyze_noise\": True,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"injection_frac\": 0.25,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"unplugged_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_16_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_8_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "    # \"norm_type\": \"none\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"modular_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -3, -1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-8, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 4, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 3 + 1e-8, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\n",
    "            \"min_replay_buffer_size\", np.log(5000), np.log(5000) + 1e-8, 1000\n",
    "        )\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 2 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 2, 3, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 7, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIGHTLY WIDER IMPROVED SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-10, 2)),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 2, 5, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 2, 8, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL STANDARD SPACE (no picking num filters etc), should be compatible with initial\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8.01, 8.02, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0.91, 0.92, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=200,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
