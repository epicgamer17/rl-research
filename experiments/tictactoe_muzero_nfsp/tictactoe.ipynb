{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    # \"learning_rate\": hp.qloguniform(\n",
    "    #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "    # ),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\n",
    "        \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "    ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    # \"minibatch_size\": scope.int(\n",
    "    #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "    # ),\n",
    "    # \"min_replay_buffer_size\": scope.int(\n",
    "    #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    # ),\n",
    "    # \"replay_buffer_size\": scope.int(\n",
    "    #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "    # ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 3, 5, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 12 trials to 36 (+24) trials\n",
      "Params:  {'action_function': <function action_as_plane at 0x3005dc550>, 'actor_dense_layer_widths': (), 'clip_low_prob': 0.0, 'clipnorm': 2, 'conv_layers': (), 'critic_dense_layer_widths': (), 'dense_layer_widths': (), 'kernel_initializer': 'he_normal', 'known_bounds': (-1, 1), 'learning_rate': 0.0001, 'lr_ratio': inf, 'min_replay_buffer_size': 4000, 'minibatch_size': 8, 'multi_process': {'multi_process': True, 'num_workers': 2}, 'n_step': 9, 'noisy_sigma': 0.0, 'num_simulations': 25, 'optimizer': {'adam_epsilon': 0.01, 'optimizer': 'adam'}, 'output_filters': 24, 'pb_c_base': 19652, 'pb_c_init': 1.25, 'per_alpha': 0.0, 'per_beta': 0.0, 'per_beta_final': 0.0, 'per_epsilon': 0.0001, 'policy_loss_function': <utils.utils.CategoricalCrossentropyLoss object at 0x3139f1810>, 'replay_buffer_size': 10000, 'residual_filters': 8, 'residual_stacks': 1, 'reward_dense_layer_widths': (), 'reward_loss_function': <utils.utils.MSELoss object at 0x3139f16c0>, 'root_dirichlet_alpha': 2.0, 'root_exploration_fraction': 0.25, 'temperature_updates': (0,), 'temperature_with_training_steps': False, 'temperatures': (1.0, 0.1), 'training_steps': 22000, 'unroll_steps': 5, 'value_loss_factor': 1.0, 'value_loss_function': <utils.utils.MSELoss object at 0x3139f1570>, 'weight_decay': 0.0001}\n",
      "Making environments\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 22000\n",
      "Using         adam_epsilon                  : 0.01\n",
      "Using default momentum                      : 0.9\n",
      "Using         learning_rate                 : 0.0001\n",
      "Using         clipnorm                      : 2\n",
      "Using         optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using         weight_decay                  : 0.0001\n",
      "Using default loss_function                 : <class 'utils.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : he_normal\n",
      "Using         minibatch_size                : 8\n",
      "Using         replay_buffer_size            : 10000\n",
      "Using         min_replay_buffer_size        : 4000\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : (-1, 1)\n",
      "Using         residual_layers               : [(8, 3, 1)]\n",
      "Using         conv_layers                   : ()\n",
      "Using         dense_layer_widths            : ()\n",
      "Using default representation_residual_layers: [(8, 3, 1)]\n",
      "Using default representation_conv_layers    : ()\n",
      "Using default representation_dense_layer_widths: []\n",
      "Using default dynamics_residual_layers      : [(8, 3, 1)]\n",
      "Using default dynamics_conv_layers          : ()\n",
      "Using default dynamics_dense_layer_widths   : []\n",
      "Using         reward_conv_layers            : [(24, 1, 1)]\n",
      "Using         reward_dense_layer_widths     : ()\n",
      "Using         critic_conv_layers            : [(24, 1, 1)]\n",
      "Using         critic_dense_layer_widths     : ()\n",
      "Using         actor_conv_layers             : [(24, 1, 1)]\n",
      "Using         actor_dense_layer_widths      : ()\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using         weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 2.0\n",
      "Using         root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 25\n",
      "Using         temperatures                  : (1.0, 0.1)\n",
      "Using         temperature_updates           : (0,)\n",
      "Using         temperature_with_training_steps: False\n",
      "Using         clip_low_prob                 : 0.0\n",
      "Using         pb_c_base                     : 19652\n",
      "Using         pb_c_init                     : 1.25\n",
      "Using         value_loss_function           : <utils.utils.MSELoss object at 0x3139f1570>\n",
      "Using         reward_loss_function          : <utils.utils.MSELoss object at 0x3139f16c0>\n",
      "Using         policy_loss_function          : <utils.utils.CategoricalCrossentropyLoss object at 0x3139f1810>\n",
      "Using         action_function               : <function action_as_plane at 0x3005dc550>\n",
      "Using         n_step                        : 9\n",
      "Using default discount_factor               : 1.0\n",
      "Using         unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 0.0001\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : None\n",
      "Using         multi_process                 : True\n",
      "Using         num_workers                   : 2\n",
      "Using         lr_ratio                      : inf\n",
      "Using device: cpu\n",
      "making test env\n",
      "Test env with record video\n",
      "env render mode rgb_array\n",
      "petting zoo env\n",
      "Test env: RecordVideo<ChannelLastToFirstWrapper<TwoPlayerPlayerPlaneWrapper<FrameStackWrapper<ActionMaskInInfoWrapper<tictactoe_v3>>>>>\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (9, 3, 3)\n",
      "Observation dtype: int8\n",
      "num_actions:  9\n",
      "Test agents: [<agents.random.RandomAgent object at 0x1061bcf40>, <agents.tictactoe_expert.TicTacToeBestAgent object at 0x32a17bc70>]\n",
      "9\n",
      "Hidden state shape: (8, 8, 3, 3)\n",
      "Action function output shape: torch.Size([1, 3, 3])\n",
      "torch.Size([8, 9, 3, 3])\n",
      "dynamics input shape torch.Size([8, 9, 3, 3])\n",
      "9\n",
      "8\n",
      "Layer weights:\n",
      "representation.residual_layers.residual_layers.0.conv1.weight:\n",
      "tensor([[[[-8.1016e-02, -1.0580e-01,  1.0379e-01],\n",
      "          [-9.2861e-02, -4.2601e-02, -6.1814e-03],\n",
      "          [ 2.8931e-03,  9.7934e-03,  5.3630e-02]],\n",
      "\n",
      "         [[-3.7978e-02, -4.4098e-02,  4.3206e-02],\n",
      "          [-3.7973e-02,  7.1924e-02, -9.9531e-02],\n",
      "          [ 1.1728e-02, -7.8084e-02, -4.3482e-02]],\n",
      "\n",
      "         [[-3.3744e-03,  9.8822e-02, -7.5422e-02],\n",
      "          [ 3.1621e-02,  4.5901e-02, -2.7778e-02],\n",
      "          [-2.1392e-02,  3.3330e-02,  3.9976e-02]],\n",
      "\n",
      "         [[-1.0277e-02, -7.9062e-02, -6.9278e-04],\n",
      "          [-3.1136e-02, -8.9955e-02, -1.9893e-03],\n",
      "          [ 7.1778e-02, -2.2741e-02, -8.3637e-02]],\n",
      "\n",
      "         [[-3.5435e-02,  5.6903e-02,  5.8703e-02],\n",
      "          [ 9.3256e-02,  7.1600e-02, -7.7332e-02],\n",
      "          [ 2.1491e-03, -1.0439e-01, -9.2520e-02]],\n",
      "\n",
      "         [[ 2.4693e-02, -9.3737e-02,  1.5397e-03],\n",
      "          [-4.2574e-02,  3.1689e-02,  5.6507e-02],\n",
      "          [ 8.3826e-02,  6.8554e-02,  8.5074e-02]],\n",
      "\n",
      "         [[ 4.1960e-02,  1.0842e-01, -6.5821e-02],\n",
      "          [ 5.2798e-02, -6.1904e-02, -5.7241e-02],\n",
      "          [ 1.3242e-02, -7.7196e-02,  5.7969e-03]],\n",
      "\n",
      "         [[ 3.7157e-02, -5.7743e-02, -7.9472e-02],\n",
      "          [-6.6138e-02, -6.3582e-02,  1.0114e-01],\n",
      "          [ 7.8069e-02,  9.0226e-02, -8.8028e-02]],\n",
      "\n",
      "         [[ 2.6767e-02,  7.1407e-02,  6.6960e-02],\n",
      "          [ 7.1110e-02,  3.3567e-03,  5.5029e-02],\n",
      "          [-9.4545e-02,  4.9484e-02, -3.8651e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.9898e-02,  1.0480e-01, -9.5661e-02],\n",
      "          [ 5.1490e-02,  7.6083e-02,  8.7099e-02],\n",
      "          [ 2.5178e-02, -4.8659e-03, -7.4191e-02]],\n",
      "\n",
      "         [[ 5.0282e-02,  9.7644e-02, -9.7864e-02],\n",
      "          [-5.5348e-02,  6.8243e-02, -7.3238e-02],\n",
      "          [-2.2436e-02, -2.3374e-02, -1.0457e-01]],\n",
      "\n",
      "         [[-2.6252e-02,  1.4381e-02,  1.7176e-02],\n",
      "          [ 1.0615e-01, -8.2329e-03,  1.0806e-01],\n",
      "          [ 7.3212e-02, -2.8282e-02, -1.5597e-02]],\n",
      "\n",
      "         [[-9.3123e-02,  4.8826e-02, -9.6794e-02],\n",
      "          [-9.3820e-02, -4.6375e-02,  7.7909e-02],\n",
      "          [-5.7567e-02, -2.9274e-02, -4.9445e-02]],\n",
      "\n",
      "         [[ 2.4096e-02,  1.3432e-02, -1.1126e-02],\n",
      "          [-9.0896e-02,  6.0640e-02,  7.2138e-02],\n",
      "          [ 1.0098e-01,  6.6861e-02,  1.0352e-01]],\n",
      "\n",
      "         [[ 9.4540e-02,  5.4655e-02,  9.3070e-02],\n",
      "          [ 5.7268e-02,  8.0750e-02, -1.1011e-02],\n",
      "          [-3.9635e-02, -3.2914e-02,  1.2861e-02]],\n",
      "\n",
      "         [[-9.6125e-02, -1.0304e-01,  1.1848e-02],\n",
      "          [ 7.1900e-02, -1.6658e-02,  5.5747e-02],\n",
      "          [-1.0055e-01,  1.0653e-01, -1.3718e-02]],\n",
      "\n",
      "         [[ 3.2577e-02, -9.1140e-02,  3.9673e-02],\n",
      "          [ 4.4457e-02, -9.4359e-02, -9.1893e-02],\n",
      "          [-6.0142e-02, -6.2097e-02, -6.0662e-03]],\n",
      "\n",
      "         [[ 1.9835e-03, -7.7667e-02, -8.0718e-02],\n",
      "          [-6.8971e-02, -1.1032e-02,  3.5325e-02],\n",
      "          [-1.6978e-02,  8.1444e-02,  6.5146e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.2717e-02, -2.1980e-02, -1.2453e-02],\n",
      "          [-5.9275e-02,  2.6282e-02,  4.4201e-02],\n",
      "          [ 9.6059e-02, -9.8071e-03,  2.1999e-02]],\n",
      "\n",
      "         [[ 7.9079e-02,  1.9641e-02,  3.7253e-02],\n",
      "          [ 1.7118e-02,  1.0967e-04, -1.0155e-01],\n",
      "          [-2.9001e-03,  5.7181e-02, -5.2570e-02]],\n",
      "\n",
      "         [[ 9.1041e-02,  6.4858e-02,  9.5555e-03],\n",
      "          [-6.5360e-02,  3.0205e-02, -9.6209e-02],\n",
      "          [-1.5813e-02,  2.6975e-02, -6.0731e-04]],\n",
      "\n",
      "         [[ 9.4481e-02, -8.0843e-02,  1.9346e-02],\n",
      "          [-7.5449e-02,  2.8816e-02,  2.0196e-02],\n",
      "          [-2.1353e-02, -3.0873e-02, -1.0936e-01]],\n",
      "\n",
      "         [[ 4.1252e-02, -9.2008e-02,  1.0579e-01],\n",
      "          [-6.4231e-02,  3.7775e-02,  9.9979e-02],\n",
      "          [ 1.0917e-01, -6.9691e-02, -2.6770e-02]],\n",
      "\n",
      "         [[-5.1821e-03,  4.9155e-02, -1.2547e-02],\n",
      "          [-3.2832e-02,  4.9952e-02, -9.7550e-02],\n",
      "          [ 2.8677e-02,  8.0368e-02,  9.8076e-02]],\n",
      "\n",
      "         [[-5.1136e-02, -7.4820e-02, -3.3919e-02],\n",
      "          [-1.0584e-01,  9.2679e-02, -7.7622e-02],\n",
      "          [ 1.1033e-01,  1.3766e-02, -1.0856e-01]],\n",
      "\n",
      "         [[-2.8037e-02,  4.8356e-02, -8.9941e-02],\n",
      "          [ 4.8253e-02, -3.3900e-02,  9.5491e-02],\n",
      "          [-4.7951e-02,  7.7594e-02,  9.5209e-02]],\n",
      "\n",
      "         [[ 5.2460e-03, -9.3753e-03, -6.9856e-02],\n",
      "          [ 8.6716e-02,  9.5751e-02,  7.8904e-02],\n",
      "          [ 7.8155e-02, -3.1213e-02,  5.3859e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0257e-01, -2.6051e-02, -1.3249e-02],\n",
      "          [ 1.1039e-01,  1.6759e-02, -5.5134e-03],\n",
      "          [-6.8469e-02, -3.6126e-02,  5.0154e-02]],\n",
      "\n",
      "         [[-2.7028e-02, -5.1556e-02,  5.0153e-02],\n",
      "          [-2.8121e-03,  7.1204e-02, -7.1828e-02],\n",
      "          [ 5.5134e-02,  7.4229e-03, -1.0044e-02]],\n",
      "\n",
      "         [[ 3.3056e-02,  1.2971e-02,  1.6300e-02],\n",
      "          [-8.9779e-02,  9.4894e-02,  9.2200e-02],\n",
      "          [ 8.8320e-02, -8.6552e-02, -9.9312e-02]],\n",
      "\n",
      "         [[-3.1653e-02,  7.8379e-02,  2.9974e-02],\n",
      "          [-2.0045e-02, -6.3299e-02,  6.3285e-02],\n",
      "          [ 1.5714e-02, -5.8797e-02, -3.0572e-02]],\n",
      "\n",
      "         [[-2.1782e-02, -7.5065e-03, -1.9777e-02],\n",
      "          [-7.0828e-02,  1.0725e-01,  4.7568e-02],\n",
      "          [-8.7328e-03,  6.4820e-02,  9.5013e-02]],\n",
      "\n",
      "         [[ 1.0486e-01, -7.6497e-02, -5.8459e-02],\n",
      "          [ 2.4848e-02,  8.8870e-03,  2.0578e-02],\n",
      "          [ 7.8960e-02,  1.0398e-01, -8.2383e-02]],\n",
      "\n",
      "         [[-2.9078e-02, -2.5057e-02,  5.6547e-02],\n",
      "          [ 5.4660e-02, -5.8869e-02, -9.2888e-02],\n",
      "          [ 6.1320e-02, -6.0724e-02, -8.5643e-02]],\n",
      "\n",
      "         [[ 5.5647e-03, -9.9393e-02,  1.0948e-01],\n",
      "          [ 9.3904e-02,  1.0788e-02,  4.0817e-02],\n",
      "          [-3.4547e-03, -3.4798e-02,  1.0188e-01]],\n",
      "\n",
      "         [[-4.1137e-02,  6.4195e-02, -4.4686e-02],\n",
      "          [ 1.2508e-02,  7.9270e-02,  9.0027e-02],\n",
      "          [ 7.1802e-03, -6.0781e-02, -9.7543e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7478e-02,  6.6497e-03,  3.4477e-02],\n",
      "          [ 1.3729e-02,  1.5714e-02,  7.7910e-02],\n",
      "          [ 5.1856e-02,  1.9116e-02, -3.3774e-02]],\n",
      "\n",
      "         [[-6.3169e-03,  2.0551e-02, -8.4874e-02],\n",
      "          [ 6.5110e-02, -2.0085e-02,  9.4601e-02],\n",
      "          [-9.0708e-02, -5.9966e-02, -4.4109e-02]],\n",
      "\n",
      "         [[-1.0060e-01,  8.8867e-02, -8.1829e-02],\n",
      "          [-1.2913e-02,  9.4587e-02, -8.2157e-02],\n",
      "          [-1.8707e-02,  9.5107e-02,  9.0995e-02]],\n",
      "\n",
      "         [[ 6.9823e-02, -7.0558e-02, -3.5358e-02],\n",
      "          [ 7.5144e-02,  6.6286e-02, -9.1740e-02],\n",
      "          [ 8.0760e-02,  9.1935e-02,  5.3084e-02]],\n",
      "\n",
      "         [[ 1.3174e-03, -9.7144e-03, -6.2331e-02],\n",
      "          [ 4.9405e-02,  2.7360e-02, -9.8015e-02],\n",
      "          [ 7.5987e-02,  8.0551e-02, -4.3245e-02]],\n",
      "\n",
      "         [[ 1.4547e-02, -8.6290e-02, -1.1832e-02],\n",
      "          [ 4.8210e-02,  5.3815e-03,  7.5941e-02],\n",
      "          [-6.1219e-02, -1.1628e-02, -8.1391e-02]],\n",
      "\n",
      "         [[-8.9502e-02,  5.5998e-02, -5.7180e-02],\n",
      "          [-7.5403e-02, -3.0577e-02, -4.3026e-02],\n",
      "          [ 5.6020e-02,  7.1880e-02,  1.0887e-01]],\n",
      "\n",
      "         [[ 1.0451e-01,  2.2905e-02,  7.5189e-03],\n",
      "          [-1.0872e-01,  3.7072e-02, -3.4591e-02],\n",
      "          [ 4.6091e-03, -5.9298e-02, -1.1024e-01]],\n",
      "\n",
      "         [[ 5.2567e-02, -9.8077e-02, -8.3998e-02],\n",
      "          [-9.0744e-02,  2.6119e-02, -2.0169e-02],\n",
      "          [-3.0592e-02, -8.4423e-03,  2.2133e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.6147e-02,  4.2840e-02, -3.8834e-02],\n",
      "          [ 9.5304e-02,  6.5459e-02, -1.4636e-02],\n",
      "          [-8.5130e-02, -9.1813e-02,  9.0531e-03]],\n",
      "\n",
      "         [[-1.6802e-02,  1.0169e-01,  1.0740e-01],\n",
      "          [ 1.0501e-01,  9.8372e-02, -3.0080e-02],\n",
      "          [-9.0719e-02, -7.0503e-02,  6.9145e-02]],\n",
      "\n",
      "         [[-9.0380e-02, -9.8841e-02,  1.4403e-02],\n",
      "          [-1.9004e-02, -8.9298e-02, -1.3273e-02],\n",
      "          [-9.0238e-02, -8.8148e-02, -7.4182e-02]],\n",
      "\n",
      "         [[-9.2665e-02, -2.2574e-02, -8.3932e-02],\n",
      "          [-4.1733e-02,  4.4897e-02,  6.8089e-02],\n",
      "          [ 1.0332e-01,  9.4879e-02, -1.0766e-01]],\n",
      "\n",
      "         [[-6.9934e-02, -6.1717e-02, -7.7355e-02],\n",
      "          [-1.1311e-02,  1.2852e-02,  2.2499e-02],\n",
      "          [ 8.9265e-02,  3.3736e-02, -4.8351e-02]],\n",
      "\n",
      "         [[-3.0566e-02,  2.4830e-02, -2.7941e-02],\n",
      "          [ 1.4377e-02,  8.7906e-02,  5.7391e-02],\n",
      "          [ 3.3667e-03,  2.3902e-02,  3.8026e-02]],\n",
      "\n",
      "         [[ 5.9186e-02, -8.1087e-02, -3.4978e-02],\n",
      "          [-2.0871e-02,  1.9786e-02,  2.1357e-03],\n",
      "          [-3.7858e-03, -2.5002e-02, -3.0698e-03]],\n",
      "\n",
      "         [[ 7.6203e-02,  6.2937e-02,  3.1411e-02],\n",
      "          [-4.1751e-02, -6.7014e-02,  1.8374e-03],\n",
      "          [ 4.6731e-02,  4.1666e-02,  1.0518e-01]],\n",
      "\n",
      "         [[-1.0476e-01,  1.1141e-02,  4.1359e-03],\n",
      "          [ 7.2653e-02,  1.8819e-02, -1.0134e-01],\n",
      "          [ 2.0862e-02,  8.4730e-03, -3.7636e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.8214e-02, -2.4215e-02, -1.7346e-02],\n",
      "          [ 2.6586e-02, -1.3336e-02, -7.3313e-02],\n",
      "          [-3.0323e-02, -8.5701e-02,  7.2131e-02]],\n",
      "\n",
      "         [[ 5.3392e-02, -7.8444e-02,  1.6421e-02],\n",
      "          [-5.1362e-02,  3.4383e-02, -4.5542e-02],\n",
      "          [ 2.6345e-02,  9.9647e-02,  6.8190e-02]],\n",
      "\n",
      "         [[-4.3985e-02,  3.1176e-02,  1.0772e-01],\n",
      "          [ 8.1271e-03,  2.6587e-02, -8.3641e-02],\n",
      "          [-3.3366e-02,  7.0047e-02,  5.7161e-02]],\n",
      "\n",
      "         [[-8.2665e-02,  2.4814e-03,  9.4469e-02],\n",
      "          [ 7.0616e-02,  5.6700e-02,  9.0324e-02],\n",
      "          [-2.7899e-02, -6.8900e-03,  5.3502e-02]],\n",
      "\n",
      "         [[ 7.4102e-02, -4.1501e-02, -1.9571e-02],\n",
      "          [ 3.5173e-02,  6.5341e-02, -1.0544e-01],\n",
      "          [-2.0847e-03, -1.0303e-01, -4.2391e-02]],\n",
      "\n",
      "         [[ 1.0992e-01,  7.4676e-02,  4.5355e-02],\n",
      "          [-3.3726e-02, -8.7844e-02,  4.4993e-02],\n",
      "          [-1.0153e-01, -4.4286e-02, -9.0387e-02]],\n",
      "\n",
      "         [[ 7.1159e-02, -8.1681e-03,  9.5964e-02],\n",
      "          [ 4.2977e-02,  1.7451e-02, -2.9378e-02],\n",
      "          [ 2.6218e-02,  8.0652e-02,  4.8764e-02]],\n",
      "\n",
      "         [[ 1.0729e-01,  4.8812e-02, -3.5814e-02],\n",
      "          [-9.0701e-02,  7.5851e-02,  2.0398e-03],\n",
      "          [-6.8889e-02, -7.3730e-02, -3.5749e-03]],\n",
      "\n",
      "         [[ 8.4445e-02,  6.4301e-02, -6.5938e-02],\n",
      "          [-3.0023e-02,  3.0804e-02, -6.8112e-02],\n",
      "          [-4.1790e-02, -1.3986e-02, -8.6079e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2112e-02,  1.0960e-01, -3.2849e-03],\n",
      "          [-1.2873e-02, -4.8723e-03,  1.0879e-02],\n",
      "          [ 9.4398e-02,  5.4496e-02, -8.4396e-02]],\n",
      "\n",
      "         [[-4.4033e-02,  4.2012e-02,  4.4874e-02],\n",
      "          [ 2.4941e-02,  7.2732e-02,  4.9361e-04],\n",
      "          [ 1.1033e-01,  9.7874e-02,  1.0168e-01]],\n",
      "\n",
      "         [[ 1.0988e-01, -3.3488e-02,  6.5862e-02],\n",
      "          [ 3.0817e-02, -1.0427e-01, -8.0643e-02],\n",
      "          [ 1.8964e-02,  1.2854e-02,  2.4189e-02]],\n",
      "\n",
      "         [[ 2.0513e-02, -1.6197e-03, -6.6818e-02],\n",
      "          [ 5.5430e-02, -5.9526e-02, -6.9686e-02],\n",
      "          [ 5.4729e-02,  8.0527e-02,  6.7440e-02]],\n",
      "\n",
      "         [[-4.0412e-04, -3.1632e-02, -4.3477e-02],\n",
      "          [-3.1634e-02, -1.0085e-01, -1.0369e-01],\n",
      "          [ 1.3239e-02, -5.7777e-02, -4.2513e-02]],\n",
      "\n",
      "         [[-1.0802e-01, -8.2594e-02, -1.0512e-01],\n",
      "          [ 3.2928e-02, -2.3891e-02,  2.4545e-02],\n",
      "          [ 4.3337e-02,  5.5815e-02, -9.7533e-02]],\n",
      "\n",
      "         [[-3.9795e-03,  4.9320e-02,  4.4925e-02],\n",
      "          [-7.7649e-02,  2.5039e-02, -9.5749e-02],\n",
      "          [ 4.1047e-03, -3.5346e-02,  1.6042e-02]],\n",
      "\n",
      "         [[ 2.1544e-02,  1.0552e-01, -4.8037e-02],\n",
      "          [ 5.8986e-02,  1.1062e-02, -2.3970e-02],\n",
      "          [ 7.7652e-02, -9.5053e-02, -6.7870e-02]],\n",
      "\n",
      "         [[-8.6018e-02, -4.0554e-02, -8.8735e-02],\n",
      "          [-3.7792e-02, -1.0967e-01,  6.0558e-02],\n",
      "          [-4.0060e-03,  8.4993e-02,  8.2210e-02]]]])\n",
      "Shape: torch.Size([8, 9, 3, 3]), std: 0.0637, mean: 0.0013\n",
      "\n",
      "representation.residual_layers.residual_layers.0.bn1.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "representation.residual_layers.residual_layers.0.bn1.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "representation.residual_layers.residual_layers.0.conv2.weight:\n",
      "tensor([[[[-0.1123, -0.0117, -0.0503],\n",
      "          [-0.0281, -0.0182,  0.1102],\n",
      "          [ 0.0403,  0.1119, -0.0527]],\n",
      "\n",
      "         [[ 0.0362, -0.0346, -0.0427],\n",
      "          [ 0.0951, -0.0598, -0.1166],\n",
      "          [-0.0123,  0.0754,  0.1024]],\n",
      "\n",
      "         [[-0.0694, -0.0591, -0.1123],\n",
      "          [-0.0278,  0.0440, -0.0016],\n",
      "          [ 0.0072, -0.0740, -0.0478]],\n",
      "\n",
      "         [[ 0.0119, -0.0258,  0.0176],\n",
      "          [ 0.0406,  0.0423, -0.0757],\n",
      "          [ 0.0586,  0.0607,  0.0635]],\n",
      "\n",
      "         [[ 0.1040, -0.0727,  0.0687],\n",
      "          [ 0.0319, -0.0452, -0.1005],\n",
      "          [-0.0339,  0.0637, -0.0858]],\n",
      "\n",
      "         [[-0.0596,  0.0902,  0.0963],\n",
      "          [ 0.0514, -0.0336,  0.0021],\n",
      "          [ 0.0218,  0.0681, -0.0624]],\n",
      "\n",
      "         [[-0.0069, -0.0812, -0.0738],\n",
      "          [-0.1137,  0.1013, -0.0377],\n",
      "          [-0.1038, -0.0419,  0.0730]],\n",
      "\n",
      "         [[ 0.0003, -0.0198,  0.0152],\n",
      "          [ 0.0525,  0.0105, -0.0021],\n",
      "          [-0.0774,  0.0543, -0.0293]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134,  0.0289,  0.0110],\n",
      "          [-0.0575,  0.1011,  0.1160],\n",
      "          [ 0.1131,  0.0463, -0.1074]],\n",
      "\n",
      "         [[-0.0723, -0.0727, -0.0662],\n",
      "          [-0.1148, -0.0717, -0.0231],\n",
      "          [-0.0548, -0.0822, -0.0214]],\n",
      "\n",
      "         [[ 0.1042,  0.1050, -0.0287],\n",
      "          [-0.0368, -0.0255, -0.0385],\n",
      "          [ 0.0192, -0.0006,  0.0552]],\n",
      "\n",
      "         [[-0.1104, -0.1119, -0.0954],\n",
      "          [ 0.0710, -0.0071, -0.0882],\n",
      "          [ 0.0585, -0.0551, -0.1002]],\n",
      "\n",
      "         [[-0.0470, -0.0463,  0.0443],\n",
      "          [-0.0424, -0.0724, -0.0392],\n",
      "          [ 0.1167,  0.0332,  0.0698]],\n",
      "\n",
      "         [[ 0.0762, -0.1098,  0.0851],\n",
      "          [-0.0593, -0.0895,  0.1028],\n",
      "          [-0.0394,  0.0977, -0.0087]],\n",
      "\n",
      "         [[ 0.0238, -0.0360, -0.0399],\n",
      "          [ 0.0052, -0.0736,  0.0629],\n",
      "          [ 0.0278, -0.0787,  0.0796]],\n",
      "\n",
      "         [[-0.0600,  0.0345, -0.0965],\n",
      "          [ 0.0487,  0.0341,  0.1115],\n",
      "          [-0.0120,  0.0049, -0.0405]]],\n",
      "\n",
      "\n",
      "        [[[-0.0499, -0.1103,  0.0987],\n",
      "          [ 0.0316,  0.0478,  0.1048],\n",
      "          [ 0.0497,  0.0779, -0.0067]],\n",
      "\n",
      "         [[ 0.0458, -0.0245, -0.0046],\n",
      "          [ 0.0592,  0.0214, -0.0374],\n",
      "          [-0.0471,  0.0794, -0.0272]],\n",
      "\n",
      "         [[ 0.1018,  0.1075, -0.0205],\n",
      "          [-0.0307,  0.0013,  0.0134],\n",
      "          [ 0.0702, -0.0374,  0.0812]],\n",
      "\n",
      "         [[-0.0048, -0.0661, -0.0770],\n",
      "          [-0.0735,  0.0983,  0.0217],\n",
      "          [ 0.0816,  0.0989,  0.0233]],\n",
      "\n",
      "         [[ 0.0220,  0.0818, -0.1125],\n",
      "          [ 0.0447,  0.0204,  0.0842],\n",
      "          [-0.0621, -0.1027,  0.0232]],\n",
      "\n",
      "         [[ 0.0196,  0.1156,  0.1026],\n",
      "          [-0.0721,  0.0522, -0.1167],\n",
      "          [-0.0011,  0.0055, -0.0479]],\n",
      "\n",
      "         [[ 0.0526, -0.0036,  0.0899],\n",
      "          [-0.0580,  0.0572,  0.0017],\n",
      "          [-0.0979, -0.0558, -0.1004]],\n",
      "\n",
      "         [[-0.1085,  0.1048, -0.0199],\n",
      "          [ 0.1021, -0.0435,  0.0451],\n",
      "          [ 0.0096, -0.1069, -0.0466]]],\n",
      "\n",
      "\n",
      "        [[[-0.0966, -0.0728, -0.0502],\n",
      "          [-0.0702,  0.0075,  0.0017],\n",
      "          [-0.0495,  0.0630, -0.0271]],\n",
      "\n",
      "         [[ 0.0287, -0.0414, -0.0232],\n",
      "          [-0.0515,  0.0088, -0.0210],\n",
      "          [ 0.0778, -0.0244,  0.0680]],\n",
      "\n",
      "         [[-0.0386, -0.0268,  0.0980],\n",
      "          [-0.0980,  0.1012, -0.0570],\n",
      "          [-0.0408,  0.1125,  0.0707]],\n",
      "\n",
      "         [[ 0.0569, -0.0831,  0.0531],\n",
      "          [ 0.0325,  0.0460, -0.0080],\n",
      "          [-0.0175,  0.1063,  0.1096]],\n",
      "\n",
      "         [[-0.0409,  0.0677,  0.0129],\n",
      "          [ 0.1155, -0.0464, -0.0916],\n",
      "          [ 0.0589,  0.0236, -0.0669]],\n",
      "\n",
      "         [[ 0.1133, -0.0428,  0.0553],\n",
      "          [ 0.0503,  0.1028,  0.0604],\n",
      "          [ 0.0909, -0.0531, -0.0023]],\n",
      "\n",
      "         [[-0.0759,  0.0779,  0.0023],\n",
      "          [-0.0281,  0.1068,  0.0838],\n",
      "          [-0.0999, -0.0415,  0.0843]],\n",
      "\n",
      "         [[ 0.0189,  0.0637, -0.0505],\n",
      "          [-0.1163,  0.1094, -0.0646],\n",
      "          [ 0.0542, -0.0463,  0.1162]]],\n",
      "\n",
      "\n",
      "        [[[-0.1060,  0.0520,  0.0859],\n",
      "          [ 0.0495,  0.0435, -0.1119],\n",
      "          [ 0.0431,  0.0462, -0.0768]],\n",
      "\n",
      "         [[ 0.0730, -0.0979, -0.0146],\n",
      "          [ 0.0048, -0.0734, -0.0187],\n",
      "          [-0.0591, -0.0099, -0.0710]],\n",
      "\n",
      "         [[ 0.0872,  0.0079,  0.1081],\n",
      "          [-0.0955, -0.1080,  0.0302],\n",
      "          [ 0.0312, -0.0045,  0.0485]],\n",
      "\n",
      "         [[-0.0477, -0.0939, -0.0376],\n",
      "          [ 0.0591,  0.0513, -0.0528],\n",
      "          [ 0.0134, -0.0120, -0.0827]],\n",
      "\n",
      "         [[ 0.1087,  0.0756, -0.0059],\n",
      "          [-0.0060, -0.0854,  0.0961],\n",
      "          [-0.0009, -0.0056,  0.1070]],\n",
      "\n",
      "         [[-0.0092,  0.0657,  0.0319],\n",
      "          [-0.0959,  0.0432, -0.0478],\n",
      "          [ 0.0541, -0.0251, -0.0271]],\n",
      "\n",
      "         [[ 0.1057, -0.0216,  0.0033],\n",
      "          [ 0.0147, -0.0563, -0.0043],\n",
      "          [-0.1094, -0.0646, -0.0503]],\n",
      "\n",
      "         [[ 0.0577,  0.0447,  0.1178],\n",
      "          [-0.0947, -0.0518, -0.0524],\n",
      "          [ 0.1170, -0.0547,  0.0608]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0379, -0.0027, -0.0684],\n",
      "          [ 0.0439, -0.0400,  0.1109],\n",
      "          [-0.1171, -0.0059, -0.0412]],\n",
      "\n",
      "         [[-0.0814, -0.0125,  0.0670],\n",
      "          [-0.0519,  0.1118, -0.0899],\n",
      "          [ 0.0944, -0.0023, -0.0933]],\n",
      "\n",
      "         [[ 0.1046, -0.0208, -0.0618],\n",
      "          [ 0.0903,  0.0945,  0.0542],\n",
      "          [-0.0122,  0.1133,  0.1074]],\n",
      "\n",
      "         [[-0.0968, -0.0903,  0.0728],\n",
      "          [-0.0139, -0.0678,  0.0202],\n",
      "          [ 0.1060, -0.0550,  0.0516]],\n",
      "\n",
      "         [[-0.0736, -0.0160, -0.1078],\n",
      "          [-0.0610, -0.0953,  0.0451],\n",
      "          [-0.1052,  0.0565,  0.0983]],\n",
      "\n",
      "         [[-0.0188,  0.1059, -0.0702],\n",
      "          [ 0.0800,  0.0367, -0.0186],\n",
      "          [ 0.0869, -0.1073, -0.0769]],\n",
      "\n",
      "         [[-0.0820,  0.0746, -0.0572],\n",
      "          [-0.0116, -0.0610,  0.0526],\n",
      "          [ 0.0571, -0.0427,  0.0691]],\n",
      "\n",
      "         [[ 0.1156, -0.0340,  0.1021],\n",
      "          [-0.0731,  0.0014, -0.0932],\n",
      "          [-0.0331,  0.0266,  0.0879]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0531, -0.0634, -0.0289],\n",
      "          [-0.1027,  0.0968,  0.1057],\n",
      "          [ 0.0144, -0.0971,  0.0499]],\n",
      "\n",
      "         [[ 0.1174, -0.0597,  0.1157],\n",
      "          [-0.0959,  0.0656, -0.0543],\n",
      "          [-0.0890,  0.0655, -0.0498]],\n",
      "\n",
      "         [[ 0.0910,  0.0230,  0.1175],\n",
      "          [-0.0126, -0.1152, -0.0309],\n",
      "          [-0.0459, -0.1032,  0.0513]],\n",
      "\n",
      "         [[ 0.0046, -0.0034,  0.0250],\n",
      "          [ 0.0730,  0.0560,  0.1081],\n",
      "          [ 0.0809, -0.1027, -0.0129]],\n",
      "\n",
      "         [[ 0.0670,  0.0979, -0.1097],\n",
      "          [-0.1045, -0.0741, -0.0905],\n",
      "          [-0.0336, -0.0067, -0.0343]],\n",
      "\n",
      "         [[ 0.0171, -0.0740, -0.0162],\n",
      "          [-0.0617, -0.0008,  0.0562],\n",
      "          [-0.0979, -0.1146, -0.0585]],\n",
      "\n",
      "         [[ 0.0587,  0.0246,  0.0793],\n",
      "          [ 0.1128,  0.1168, -0.0454],\n",
      "          [-0.0063,  0.0338,  0.0560]],\n",
      "\n",
      "         [[ 0.0901,  0.0349,  0.1166],\n",
      "          [-0.0287, -0.0903,  0.0188],\n",
      "          [-0.0806, -0.0700,  0.0014]]],\n",
      "\n",
      "\n",
      "        [[[-0.0200, -0.0007,  0.0056],\n",
      "          [ 0.1086,  0.0667,  0.0425],\n",
      "          [-0.1153, -0.0947, -0.1001]],\n",
      "\n",
      "         [[-0.0870,  0.0293,  0.0250],\n",
      "          [-0.0012, -0.0116, -0.0057],\n",
      "          [ 0.0153,  0.0809, -0.1173]],\n",
      "\n",
      "         [[ 0.0589,  0.0114,  0.0562],\n",
      "          [ 0.0999,  0.1092, -0.1063],\n",
      "          [ 0.0106,  0.0477, -0.0051]],\n",
      "\n",
      "         [[-0.1018,  0.1069, -0.0557],\n",
      "          [ 0.0544,  0.0515,  0.1032],\n",
      "          [ 0.0776, -0.0546,  0.0140]],\n",
      "\n",
      "         [[-0.1062,  0.0780, -0.0769],\n",
      "          [-0.1091,  0.0580, -0.0819],\n",
      "          [-0.0009,  0.0349,  0.0021]],\n",
      "\n",
      "         [[ 0.0593,  0.0468,  0.0790],\n",
      "          [-0.0292, -0.0644, -0.0834],\n",
      "          [ 0.0970,  0.0994,  0.0484]],\n",
      "\n",
      "         [[ 0.0006,  0.0133,  0.0284],\n",
      "          [-0.0670, -0.1008,  0.0053],\n",
      "          [ 0.0905,  0.0627,  0.0464]],\n",
      "\n",
      "         [[-0.0213, -0.0285, -0.0297],\n",
      "          [ 0.0763,  0.0601, -0.0849],\n",
      "          [ 0.0787,  0.1115, -0.0931]]]])\n",
      "Shape: torch.Size([8, 8, 3, 3]), std: 0.0686, mean: 0.0018\n",
      "\n",
      "representation.residual_layers.residual_layers.0.bn2.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "representation.residual_layers.residual_layers.0.bn2.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "representation.residual_layers.residual_layers.0.downsample.0.weight:\n",
      "tensor([[[[ 5.2159e-02, -3.5819e-02, -9.0624e-02],\n",
      "          [ 3.2434e-02,  1.0675e-01,  1.3184e-02],\n",
      "          [-9.4592e-02, -5.0204e-02, -5.0333e-02]],\n",
      "\n",
      "         [[-9.8942e-02, -4.4169e-02,  4.0935e-02],\n",
      "          [ 4.9677e-02, -9.5611e-02, -4.4966e-02],\n",
      "          [-1.3399e-03,  6.4063e-02, -9.6362e-02]],\n",
      "\n",
      "         [[-9.5189e-02,  1.1033e-01, -3.3132e-02],\n",
      "          [ 4.5683e-02,  7.1269e-02, -4.8529e-02],\n",
      "          [ 5.5476e-02, -6.6923e-02, -9.6911e-02]],\n",
      "\n",
      "         [[-3.7009e-02, -7.0724e-02,  5.6307e-02],\n",
      "          [ 6.0570e-02, -2.8096e-02, -3.6123e-02],\n",
      "          [ 8.6332e-02, -9.2643e-02,  6.2771e-02]],\n",
      "\n",
      "         [[-4.0672e-02, -6.5910e-02, -2.6438e-02],\n",
      "          [-5.9039e-02,  8.4521e-03, -7.2094e-02],\n",
      "          [-3.2265e-02, -6.0990e-02,  6.8433e-02]],\n",
      "\n",
      "         [[ 9.3543e-03, -1.0709e-01, -1.4850e-02],\n",
      "          [-1.0664e-01,  5.0803e-02,  9.1449e-02],\n",
      "          [-9.2463e-02, -2.4871e-02, -4.1184e-02]],\n",
      "\n",
      "         [[ 5.5899e-02, -2.6148e-02, -7.0405e-02],\n",
      "          [ 8.6781e-02,  8.6004e-02,  5.4862e-02],\n",
      "          [ 1.4315e-02, -1.2485e-02,  9.7768e-02]],\n",
      "\n",
      "         [[ 3.1167e-02,  5.5641e-02,  8.9961e-02],\n",
      "          [ 9.2217e-02,  1.0477e-01,  3.3538e-02],\n",
      "          [-7.4624e-02,  4.0563e-02, -1.5593e-02]],\n",
      "\n",
      "         [[-8.2886e-02, -4.2371e-02, -4.6072e-02],\n",
      "          [-7.6525e-02, -1.4549e-02,  3.7764e-02],\n",
      "          [-6.2568e-02, -3.6287e-03,  1.0152e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0411e-01,  6.6644e-02, -9.3836e-02],\n",
      "          [ 9.3578e-02,  2.6075e-02, -5.1862e-02],\n",
      "          [ 2.4331e-02, -6.8792e-02,  7.0962e-02]],\n",
      "\n",
      "         [[ 5.9092e-02,  2.2421e-02, -8.6987e-02],\n",
      "          [-2.6492e-02, -7.1816e-02,  4.5122e-02],\n",
      "          [ 8.1163e-02, -1.0646e-01, -1.0107e-01]],\n",
      "\n",
      "         [[-6.5371e-02,  8.3913e-03, -6.9618e-02],\n",
      "          [ 9.1678e-02, -9.8802e-02,  4.5033e-02],\n",
      "          [-3.7000e-02,  7.2715e-02, -1.0392e-01]],\n",
      "\n",
      "         [[ 8.8497e-02, -7.7446e-05,  3.2464e-02],\n",
      "          [ 6.9383e-02,  3.3202e-02, -4.6687e-02],\n",
      "          [-1.8600e-03, -8.7902e-02, -4.6024e-02]],\n",
      "\n",
      "         [[ 6.6727e-02, -4.7602e-02, -1.0824e-01],\n",
      "          [ 9.8089e-02, -2.7955e-02,  4.0657e-02],\n",
      "          [ 6.5817e-02,  1.0163e-01,  4.3178e-02]],\n",
      "\n",
      "         [[-5.6969e-02,  7.1652e-02,  4.5510e-03],\n",
      "          [ 9.3319e-02, -8.8294e-02, -2.6567e-02],\n",
      "          [ 7.5980e-02, -7.0275e-02, -1.0134e-01]],\n",
      "\n",
      "         [[-7.7826e-02,  8.7409e-02, -3.7459e-02],\n",
      "          [ 5.3100e-02,  7.9503e-02,  1.0584e-01],\n",
      "          [ 1.7389e-02,  3.4389e-02, -9.3650e-02]],\n",
      "\n",
      "         [[-1.2453e-02,  2.0354e-03, -1.1027e-01],\n",
      "          [ 9.8199e-03, -6.3822e-02,  1.0625e-02],\n",
      "          [-5.3995e-03, -8.4887e-02,  3.3530e-02]],\n",
      "\n",
      "         [[ 4.8976e-02, -9.4392e-02,  1.0281e-02],\n",
      "          [-5.4704e-02, -5.8183e-02,  6.9643e-02],\n",
      "          [ 8.9182e-03,  1.0667e-01,  8.1495e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3365e-02,  6.3738e-03,  9.3089e-02],\n",
      "          [ 2.8064e-02, -1.3021e-02, -4.5457e-02],\n",
      "          [ 8.1923e-02,  2.7806e-02,  1.7666e-02]],\n",
      "\n",
      "         [[-1.0351e-01, -4.9606e-02, -7.8236e-03],\n",
      "          [ 1.0232e-01, -7.1858e-02, -2.0381e-02],\n",
      "          [ 1.5560e-02, -4.2249e-02, -2.3285e-02]],\n",
      "\n",
      "         [[-3.1925e-02,  2.1247e-02,  5.1591e-02],\n",
      "          [ 7.0156e-02,  3.8664e-02,  4.3388e-03],\n",
      "          [-7.7628e-02,  9.3363e-02,  2.9014e-02]],\n",
      "\n",
      "         [[ 6.2443e-02,  9.7917e-02,  7.2344e-02],\n",
      "          [-9.2936e-02,  6.5307e-02, -4.4234e-02],\n",
      "          [ 1.0688e-01, -4.7840e-02,  8.4417e-02]],\n",
      "\n",
      "         [[-1.3796e-02,  1.0724e-01, -5.0110e-02],\n",
      "          [ 1.1418e-02, -1.0411e-03, -9.8428e-02],\n",
      "          [-9.0144e-02,  4.7846e-02, -1.1354e-02]],\n",
      "\n",
      "         [[-3.5728e-03, -1.0960e-01, -5.4442e-02],\n",
      "          [-1.0584e-02, -4.2078e-03, -8.8784e-02],\n",
      "          [ 2.1117e-02, -6.8010e-02,  3.9249e-02]],\n",
      "\n",
      "         [[-1.8490e-02,  4.8701e-02, -7.9671e-02],\n",
      "          [ 7.3888e-02,  4.7038e-02, -6.1775e-02],\n",
      "          [-5.6970e-02, -2.6623e-02, -8.5273e-02]],\n",
      "\n",
      "         [[-5.5021e-02, -4.9146e-02,  1.0928e-01],\n",
      "          [ 7.6526e-02,  6.4835e-02,  3.9775e-02],\n",
      "          [-4.1107e-02, -6.8882e-04,  7.0244e-02]],\n",
      "\n",
      "         [[-7.0698e-02, -9.5369e-02,  1.3669e-02],\n",
      "          [-3.3050e-02,  8.5722e-02, -3.8128e-02],\n",
      "          [-1.9017e-02,  6.8112e-02,  6.2137e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6356e-02,  7.2759e-02,  3.0134e-02],\n",
      "          [-5.5577e-02,  9.0601e-03, -8.4978e-02],\n",
      "          [-1.3155e-02, -4.0174e-02,  8.7302e-02]],\n",
      "\n",
      "         [[ 6.8628e-03,  3.4723e-02, -4.2203e-02],\n",
      "          [ 1.0716e-01, -1.0433e-01,  1.0869e-01],\n",
      "          [-5.6210e-02,  1.5836e-02,  9.0055e-02]],\n",
      "\n",
      "         [[ 9.7412e-02,  7.8008e-02,  2.4106e-02],\n",
      "          [-3.4343e-02, -1.0267e-01,  4.7365e-02],\n",
      "          [-9.4778e-02,  4.1958e-02, -8.3160e-02]],\n",
      "\n",
      "         [[ 6.8778e-02,  5.2539e-02,  9.9390e-02],\n",
      "          [-8.6746e-02,  6.3595e-02,  5.4313e-02],\n",
      "          [ 5.9963e-02, -6.9256e-02,  2.6390e-02]],\n",
      "\n",
      "         [[ 2.4532e-02,  3.9698e-02, -2.7369e-02],\n",
      "          [ 9.4154e-02, -3.5925e-02, -9.4224e-02],\n",
      "          [ 2.1568e-02,  6.9776e-02,  3.7697e-02]],\n",
      "\n",
      "         [[-8.0285e-02,  7.0571e-02, -1.0443e-01],\n",
      "          [ 6.4950e-02,  4.3657e-02,  5.4105e-02],\n",
      "          [ 5.7105e-02,  1.2479e-02, -7.1325e-02]],\n",
      "\n",
      "         [[-9.5383e-02,  5.3311e-02, -5.4111e-02],\n",
      "          [ 5.0788e-02, -2.0293e-02, -2.6034e-03],\n",
      "          [ 3.2871e-03,  1.0037e-02,  1.1668e-02]],\n",
      "\n",
      "         [[ 3.2192e-02, -8.2036e-02,  5.0330e-02],\n",
      "          [ 5.5517e-02,  8.0203e-02, -1.7426e-02],\n",
      "          [-8.5431e-02, -3.7571e-02,  1.0889e-01]],\n",
      "\n",
      "         [[-9.2507e-02,  1.2948e-02, -8.1460e-02],\n",
      "          [-6.1761e-02,  9.7514e-02, -8.4777e-02],\n",
      "          [-4.1940e-02,  4.4227e-02,  7.3614e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.3916e-02,  5.9052e-02, -2.2158e-03],\n",
      "          [ 3.8098e-02,  2.6136e-02, -5.1897e-02],\n",
      "          [ 2.4653e-02,  5.1594e-02, -4.1442e-02]],\n",
      "\n",
      "         [[ 2.6198e-02, -4.0801e-02, -7.1177e-02],\n",
      "          [-5.1665e-02, -1.1030e-02, -4.6281e-02],\n",
      "          [ 7.8045e-02, -9.7851e-02, -6.0147e-02]],\n",
      "\n",
      "         [[-8.7319e-02, -9.5385e-02,  8.2863e-02],\n",
      "          [ 1.0475e-01, -2.2579e-02, -1.5036e-02],\n",
      "          [-4.6435e-03,  4.8739e-02, -1.0738e-01]],\n",
      "\n",
      "         [[ 1.1083e-01, -5.6266e-02,  1.0262e-01],\n",
      "          [ 5.6228e-02, -1.9402e-02,  9.5653e-02],\n",
      "          [ 1.0333e-01, -2.4236e-02, -6.7351e-02]],\n",
      "\n",
      "         [[ 2.2598e-02, -5.5769e-02, -7.3680e-02],\n",
      "          [-4.7808e-02, -2.0055e-02, -7.5082e-03],\n",
      "          [ 5.0728e-02,  7.7900e-02, -4.0221e-02]],\n",
      "\n",
      "         [[ 8.3005e-02,  1.8284e-03,  9.8249e-02],\n",
      "          [ 1.2918e-02, -4.6361e-02,  6.1349e-03],\n",
      "          [ 3.5680e-02, -2.8714e-02,  1.0709e-01]],\n",
      "\n",
      "         [[ 9.8021e-02, -1.0965e-01, -3.6659e-02],\n",
      "          [-6.6418e-02,  1.0316e-01, -2.0426e-02],\n",
      "          [ 9.3138e-02,  7.3945e-02,  4.4732e-02]],\n",
      "\n",
      "         [[-8.4202e-02, -5.1742e-02, -2.6519e-02],\n",
      "          [-9.7314e-02,  1.0927e-01, -7.4529e-02],\n",
      "          [-3.7822e-02,  4.1309e-02, -6.2672e-02]],\n",
      "\n",
      "         [[ 1.6204e-02, -5.0959e-02,  9.8266e-02],\n",
      "          [ 1.8477e-02,  2.2256e-02,  5.5385e-02],\n",
      "          [ 3.8841e-02,  5.7720e-03,  6.0208e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.7909e-02,  1.5016e-02,  3.2399e-02],\n",
      "          [-1.0365e-01, -5.3757e-02, -9.5962e-02],\n",
      "          [ 7.2787e-02, -9.6771e-02, -1.0615e-01]],\n",
      "\n",
      "         [[ 9.5440e-02,  5.4999e-02, -1.9376e-02],\n",
      "          [-2.6321e-04,  4.2767e-02,  9.7248e-02],\n",
      "          [-7.0880e-02,  9.9853e-02,  4.6395e-02]],\n",
      "\n",
      "         [[ 9.2865e-03, -5.2232e-02, -6.3805e-02],\n",
      "          [-1.1582e-02, -1.0982e-01, -4.8649e-02],\n",
      "          [-1.0375e-01,  5.6517e-02, -7.6744e-04]],\n",
      "\n",
      "         [[-6.0398e-03, -5.1119e-02,  4.5375e-02],\n",
      "          [ 6.6025e-02,  4.4816e-02,  2.4883e-02],\n",
      "          [ 2.3199e-02, -9.9836e-02, -1.8306e-02]],\n",
      "\n",
      "         [[-1.8253e-02, -2.3640e-02,  5.6264e-02],\n",
      "          [ 2.0336e-02,  9.5466e-02, -8.5591e-02],\n",
      "          [-6.7941e-02,  1.9984e-02,  4.3396e-02]],\n",
      "\n",
      "         [[ 1.0284e-01, -2.9591e-02, -8.7670e-02],\n",
      "          [-3.4120e-02,  5.3614e-02, -1.0447e-01],\n",
      "          [-1.4717e-03,  2.7530e-02, -6.8561e-03]],\n",
      "\n",
      "         [[-6.7733e-02, -1.6912e-02,  6.6504e-02],\n",
      "          [ 5.8751e-02, -6.9938e-02,  7.1476e-02],\n",
      "          [ 6.4988e-02, -2.8070e-02,  5.5369e-02]],\n",
      "\n",
      "         [[ 9.4452e-02,  7.4351e-02,  3.7800e-02],\n",
      "          [ 1.0296e-01, -8.4105e-02,  3.1672e-02],\n",
      "          [-3.7537e-02, -7.5775e-02, -4.8415e-02]],\n",
      "\n",
      "         [[-2.4681e-02, -5.7774e-02, -6.8013e-02],\n",
      "          [ 7.8958e-03, -6.6925e-02,  1.2475e-02],\n",
      "          [-2.0182e-02, -5.5459e-02, -1.0648e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5372e-02, -3.3264e-02,  5.7517e-02],\n",
      "          [-2.4732e-03,  1.0087e-01, -9.0934e-03],\n",
      "          [ 8.2742e-02, -3.6141e-02, -4.2068e-02]],\n",
      "\n",
      "         [[-6.9541e-02, -9.7730e-02, -5.1089e-02],\n",
      "          [-5.1400e-02, -2.8723e-03,  8.7182e-02],\n",
      "          [-1.5751e-02,  6.0405e-03, -1.0750e-01]],\n",
      "\n",
      "         [[-7.0730e-02, -1.0678e-01, -6.1561e-02],\n",
      "          [ 3.5328e-02,  4.0319e-02, -5.6167e-02],\n",
      "          [-1.7578e-02,  8.4359e-03,  6.0231e-02]],\n",
      "\n",
      "         [[ 1.8049e-02, -1.9148e-02, -8.6756e-03],\n",
      "          [-8.0909e-02,  6.8927e-02,  8.7596e-02],\n",
      "          [ 2.6008e-02, -8.8748e-03,  5.1200e-02]],\n",
      "\n",
      "         [[ 2.4573e-02,  7.9944e-02, -5.4344e-02],\n",
      "          [-6.3006e-02, -1.6540e-02, -5.3126e-02],\n",
      "          [ 1.0519e-01, -5.2411e-02,  4.6904e-02]],\n",
      "\n",
      "         [[ 1.0593e-01,  2.2582e-02, -5.5432e-02],\n",
      "          [ 8.5600e-02, -8.4807e-02,  1.2281e-02],\n",
      "          [ 9.1715e-02, -2.7687e-02, -5.1536e-02]],\n",
      "\n",
      "         [[ 3.5046e-02, -6.0870e-02,  8.6134e-02],\n",
      "          [ 6.2456e-02, -8.7382e-02,  9.7246e-02],\n",
      "          [ 7.5844e-02,  1.0372e-01,  9.3542e-02]],\n",
      "\n",
      "         [[-2.3425e-02, -2.8784e-02, -7.2408e-02],\n",
      "          [ 5.2897e-02, -7.4078e-02, -6.9428e-02],\n",
      "          [ 4.3353e-02,  7.9155e-02,  8.8465e-03]],\n",
      "\n",
      "         [[-1.8668e-02,  4.6170e-02,  1.9544e-02],\n",
      "          [ 4.2008e-03,  9.9926e-02,  5.6872e-02],\n",
      "          [-7.1927e-02, -2.6806e-02,  2.2380e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9670e-02, -9.3071e-03,  3.5050e-02],\n",
      "          [ 6.3801e-02, -4.6672e-02, -1.1520e-02],\n",
      "          [-2.1187e-02,  6.2731e-02, -3.8404e-02]],\n",
      "\n",
      "         [[ 3.4699e-02, -9.3635e-03,  1.0412e-01],\n",
      "          [ 1.4925e-02, -9.1181e-02, -6.6256e-02],\n",
      "          [ 7.6155e-02, -1.1300e-02, -5.7788e-02]],\n",
      "\n",
      "         [[ 6.6263e-02, -7.5469e-02,  3.2046e-02],\n",
      "          [-5.5779e-02, -5.5128e-02, -1.5537e-02],\n",
      "          [ 3.8263e-02, -4.3548e-02, -1.0402e-01]],\n",
      "\n",
      "         [[-4.8704e-02, -1.5570e-02,  1.0104e-01],\n",
      "          [ 8.2078e-02, -7.4454e-02,  8.1067e-02],\n",
      "          [-7.4924e-02, -8.5529e-02, -2.6835e-02]],\n",
      "\n",
      "         [[ 5.5346e-03, -5.7050e-02,  4.5998e-02],\n",
      "          [ 8.2109e-02,  8.2499e-02,  9.8500e-03],\n",
      "          [-2.7308e-02, -9.2377e-02,  9.9266e-02]],\n",
      "\n",
      "         [[ 3.9754e-02,  2.8580e-02, -5.6353e-02],\n",
      "          [-7.3560e-02,  5.1943e-02,  7.2246e-02],\n",
      "          [ 5.5349e-02, -3.3039e-02,  1.9706e-02]],\n",
      "\n",
      "         [[ 4.7379e-02, -2.8180e-03,  7.2585e-02],\n",
      "          [ 6.8467e-02,  1.6327e-02, -6.7580e-03],\n",
      "          [ 8.5103e-02, -5.7855e-02,  1.0264e-01]],\n",
      "\n",
      "         [[-1.0157e-01,  4.5453e-03,  9.0667e-02],\n",
      "          [ 6.2315e-02, -3.5735e-02, -1.4351e-02],\n",
      "          [-1.9356e-02,  1.0793e-01, -5.2910e-02]],\n",
      "\n",
      "         [[ 3.6204e-02, -5.4549e-02,  8.5535e-02],\n",
      "          [ 1.0809e-01,  7.4517e-02,  7.4179e-02],\n",
      "          [ 2.7945e-02, -2.0730e-02,  4.1371e-03]]]])\n",
      "Shape: torch.Size([8, 9, 3, 3]), std: 0.0632, mean: 0.0021\n",
      "\n",
      "representation.residual_layers.residual_layers.0.downsample.1.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "representation.residual_layers.residual_layers.0.downsample.1.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.conv1.weight:\n",
      "tensor([[[[ 0.0593, -0.0452,  0.1014],\n",
      "          [ 0.0092, -0.1073,  0.0198],\n",
      "          [-0.0935, -0.0422,  0.0600]],\n",
      "\n",
      "         [[-0.0444, -0.0512, -0.0663],\n",
      "          [ 0.0406,  0.0414, -0.0248],\n",
      "          [-0.0419,  0.1030, -0.0157]],\n",
      "\n",
      "         [[ 0.0269, -0.0902,  0.0910],\n",
      "          [ 0.0906,  0.0754, -0.1080],\n",
      "          [ 0.0311, -0.0647, -0.0843]],\n",
      "\n",
      "         [[ 0.0963,  0.0665, -0.0396],\n",
      "          [-0.0545, -0.0600, -0.0197],\n",
      "          [-0.0827,  0.1019, -0.0262]],\n",
      "\n",
      "         [[ 0.0972,  0.0620,  0.0007],\n",
      "          [ 0.0405, -0.0750, -0.0475],\n",
      "          [-0.0079, -0.0063, -0.0159]],\n",
      "\n",
      "         [[-0.0879, -0.0823, -0.1086],\n",
      "          [ 0.0217, -0.0716, -0.0558],\n",
      "          [ 0.0746,  0.0157,  0.0188]],\n",
      "\n",
      "         [[-0.0869, -0.0267,  0.0975],\n",
      "          [-0.0185,  0.1036, -0.0123],\n",
      "          [-0.0895,  0.0503,  0.0423]],\n",
      "\n",
      "         [[-0.0440, -0.0976,  0.0764],\n",
      "          [-0.0575, -0.0778, -0.0857],\n",
      "          [-0.0747, -0.0722, -0.0823]],\n",
      "\n",
      "         [[-0.0481,  0.0802,  0.0181],\n",
      "          [ 0.0626,  0.0834, -0.0239],\n",
      "          [ 0.0374,  0.1046, -0.0715]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0739, -0.0356, -0.0899],\n",
      "          [-0.0177, -0.0790,  0.0499],\n",
      "          [-0.0871, -0.1095,  0.0799]],\n",
      "\n",
      "         [[ 0.0341,  0.0187,  0.0570],\n",
      "          [ 0.1031,  0.0070,  0.0014],\n",
      "          [ 0.1016, -0.0074,  0.0570]],\n",
      "\n",
      "         [[ 0.0559,  0.0704, -0.0933],\n",
      "          [ 0.1026, -0.0123,  0.0051],\n",
      "          [ 0.0592,  0.0136, -0.0655]],\n",
      "\n",
      "         [[-0.0424, -0.0254,  0.0366],\n",
      "          [ 0.0055, -0.0117,  0.0725],\n",
      "          [-0.0895,  0.0573,  0.0465]],\n",
      "\n",
      "         [[ 0.0327,  0.0692, -0.0195],\n",
      "          [-0.0867,  0.0060, -0.1040],\n",
      "          [ 0.0234, -0.0532, -0.0054]],\n",
      "\n",
      "         [[-0.0136,  0.0994, -0.0119],\n",
      "          [-0.0316,  0.0040, -0.0733],\n",
      "          [ 0.1040,  0.0893,  0.1078]],\n",
      "\n",
      "         [[ 0.0796, -0.0694,  0.1076],\n",
      "          [-0.0798,  0.0465, -0.0262],\n",
      "          [ 0.0738,  0.0209, -0.0870]],\n",
      "\n",
      "         [[-0.0937,  0.0778, -0.0112],\n",
      "          [ 0.0857, -0.0051, -0.0746],\n",
      "          [-0.1044, -0.0408, -0.1053]],\n",
      "\n",
      "         [[-0.0896,  0.0918, -0.0119],\n",
      "          [-0.0706, -0.0553,  0.0320],\n",
      "          [ 0.0453, -0.0145,  0.0131]]],\n",
      "\n",
      "\n",
      "        [[[-0.0704, -0.0307,  0.0032],\n",
      "          [ 0.0243, -0.0561,  0.0820],\n",
      "          [ 0.0011,  0.0238,  0.0850]],\n",
      "\n",
      "         [[ 0.0555, -0.0936,  0.0502],\n",
      "          [ 0.0581, -0.0152,  0.0582],\n",
      "          [-0.0881, -0.0305,  0.0619]],\n",
      "\n",
      "         [[-0.0485,  0.0609,  0.0888],\n",
      "          [ 0.0292, -0.0314, -0.0359],\n",
      "          [-0.0104,  0.0058,  0.0249]],\n",
      "\n",
      "         [[-0.0018,  0.0490,  0.0995],\n",
      "          [ 0.0425, -0.0909,  0.0157],\n",
      "          [-0.0568,  0.0670,  0.0300]],\n",
      "\n",
      "         [[-0.0426, -0.0412,  0.0936],\n",
      "          [-0.0210, -0.0958, -0.0415],\n",
      "          [-0.0696,  0.0608,  0.0166]],\n",
      "\n",
      "         [[ 0.1085, -0.0174,  0.0636],\n",
      "          [ 0.0071,  0.0892, -0.1023],\n",
      "          [ 0.0228,  0.0608, -0.0092]],\n",
      "\n",
      "         [[-0.0883, -0.0495, -0.0147],\n",
      "          [ 0.0149, -0.0835,  0.0203],\n",
      "          [-0.0312,  0.0812,  0.1004]],\n",
      "\n",
      "         [[ 0.0235,  0.0072, -0.0435],\n",
      "          [ 0.0352, -0.0418, -0.0454],\n",
      "          [ 0.0966, -0.0322, -0.0054]],\n",
      "\n",
      "         [[-0.0626,  0.0081, -0.0158],\n",
      "          [-0.1090,  0.1078,  0.0627],\n",
      "          [ 0.0839, -0.0045,  0.0523]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0952,  0.0406,  0.0281],\n",
      "          [ 0.0388, -0.0575,  0.0141],\n",
      "          [-0.0208,  0.0683, -0.0947]],\n",
      "\n",
      "         [[-0.0411, -0.1026,  0.0670],\n",
      "          [-0.0328, -0.0036, -0.0267],\n",
      "          [ 0.0786, -0.0861, -0.0245]],\n",
      "\n",
      "         [[ 0.0419,  0.0491,  0.1082],\n",
      "          [-0.0441,  0.0297,  0.0803],\n",
      "          [-0.1062,  0.1087,  0.0370]],\n",
      "\n",
      "         [[-0.1109, -0.0506, -0.0446],\n",
      "          [ 0.0060, -0.0678, -0.0014],\n",
      "          [-0.0022,  0.0654,  0.0925]],\n",
      "\n",
      "         [[ 0.0320,  0.0876,  0.0869],\n",
      "          [ 0.0317, -0.0017,  0.0322],\n",
      "          [-0.0538,  0.0261,  0.0626]],\n",
      "\n",
      "         [[ 0.0262,  0.0608, -0.0914],\n",
      "          [-0.0789, -0.0508, -0.0364],\n",
      "          [ 0.0712, -0.0481,  0.0479]],\n",
      "\n",
      "         [[ 0.0419, -0.0515, -0.0071],\n",
      "          [-0.0750,  0.0244, -0.0130],\n",
      "          [ 0.0798,  0.0568, -0.0406]],\n",
      "\n",
      "         [[-0.0186,  0.1008, -0.0247],\n",
      "          [ 0.0365,  0.1019, -0.0938],\n",
      "          [ 0.0285,  0.0960, -0.0997]],\n",
      "\n",
      "         [[ 0.0080,  0.0748,  0.0726],\n",
      "          [ 0.0023,  0.0255, -0.0044],\n",
      "          [ 0.0545,  0.0135,  0.0545]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0233, -0.0348,  0.0663],\n",
      "          [ 0.0758, -0.0771, -0.0021],\n",
      "          [-0.1000,  0.0287, -0.0653]],\n",
      "\n",
      "         [[ 0.0530,  0.0979,  0.1078],\n",
      "          [ 0.0898,  0.0164, -0.0711],\n",
      "          [ 0.0993, -0.0596, -0.1073]],\n",
      "\n",
      "         [[-0.0323,  0.1068,  0.0156],\n",
      "          [ 0.0658, -0.0547, -0.1048],\n",
      "          [ 0.0183,  0.0193,  0.0274]],\n",
      "\n",
      "         [[-0.0263,  0.0899,  0.0269],\n",
      "          [-0.0506,  0.1093, -0.0353],\n",
      "          [-0.0568,  0.0967, -0.0790]],\n",
      "\n",
      "         [[-0.0179, -0.0336, -0.0623],\n",
      "          [-0.0275,  0.0984,  0.0521],\n",
      "          [-0.0335, -0.0544,  0.1046]],\n",
      "\n",
      "         [[-0.0763, -0.0914, -0.0013],\n",
      "          [ 0.0926,  0.0854, -0.0828],\n",
      "          [ 0.0488, -0.0772, -0.0956]],\n",
      "\n",
      "         [[ 0.0200, -0.0631, -0.0197],\n",
      "          [-0.0404,  0.0247, -0.0799],\n",
      "          [ 0.0221,  0.0985,  0.0712]],\n",
      "\n",
      "         [[-0.0145,  0.0455, -0.0867],\n",
      "          [-0.0910, -0.0592, -0.0338],\n",
      "          [ 0.0419, -0.0842, -0.0559]],\n",
      "\n",
      "         [[-0.0228, -0.0547,  0.0696],\n",
      "          [ 0.0559, -0.1048,  0.0092],\n",
      "          [ 0.0022, -0.1057, -0.0610]]],\n",
      "\n",
      "\n",
      "        [[[-0.0463,  0.0266,  0.0572],\n",
      "          [ 0.0615,  0.0360, -0.0791],\n",
      "          [ 0.0005, -0.0524,  0.0597]],\n",
      "\n",
      "         [[-0.0958, -0.0213, -0.0093],\n",
      "          [ 0.0511, -0.0465,  0.0411],\n",
      "          [-0.0749,  0.0218,  0.0575]],\n",
      "\n",
      "         [[-0.0083, -0.0568, -0.1079],\n",
      "          [-0.0531,  0.0546,  0.0741],\n",
      "          [-0.0286,  0.0569,  0.0155]],\n",
      "\n",
      "         [[ 0.0739,  0.0656,  0.1025],\n",
      "          [ 0.0233, -0.0673,  0.0203],\n",
      "          [-0.0898, -0.0700, -0.0894]],\n",
      "\n",
      "         [[-0.0707, -0.0137, -0.0013],\n",
      "          [ 0.0719, -0.0011,  0.0154],\n",
      "          [-0.0222, -0.0980,  0.0380]],\n",
      "\n",
      "         [[-0.0095,  0.0691,  0.1039],\n",
      "          [ 0.1023, -0.0281,  0.0414],\n",
      "          [ 0.0588, -0.0353, -0.0998]],\n",
      "\n",
      "         [[-0.0389,  0.0184, -0.0487],\n",
      "          [-0.0218,  0.0597, -0.0542],\n",
      "          [ 0.1095, -0.0895,  0.0254]],\n",
      "\n",
      "         [[-0.0098, -0.1074, -0.0616],\n",
      "          [ 0.0655,  0.0121, -0.0610],\n",
      "          [-0.0147,  0.0698,  0.0305]],\n",
      "\n",
      "         [[-0.0199,  0.0878, -0.0131],\n",
      "          [-0.0825, -0.0304,  0.0020],\n",
      "          [-0.1089,  0.0692,  0.0801]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0062, -0.1043, -0.0657],\n",
      "          [ 0.0336,  0.0974, -0.0124],\n",
      "          [ 0.0096,  0.0581,  0.0201]],\n",
      "\n",
      "         [[-0.0307,  0.0471, -0.0420],\n",
      "          [ 0.1107,  0.0317,  0.0937],\n",
      "          [ 0.0472,  0.0580, -0.0379]],\n",
      "\n",
      "         [[ 0.0429, -0.0764,  0.0327],\n",
      "          [ 0.1010, -0.0819, -0.0314],\n",
      "          [-0.0327, -0.0834,  0.0011]],\n",
      "\n",
      "         [[ 0.0357,  0.0415,  0.0071],\n",
      "          [-0.0666,  0.0813, -0.0572],\n",
      "          [ 0.0938, -0.0766,  0.0380]],\n",
      "\n",
      "         [[-0.0800,  0.0736, -0.0413],\n",
      "          [ 0.0066,  0.1050, -0.0852],\n",
      "          [ 0.0616,  0.0648, -0.0206]],\n",
      "\n",
      "         [[-0.0944,  0.1088, -0.0870],\n",
      "          [-0.0630, -0.0236, -0.0648],\n",
      "          [-0.0958, -0.0201,  0.0301]],\n",
      "\n",
      "         [[ 0.0929,  0.0101, -0.0639],\n",
      "          [ 0.0603,  0.0015, -0.1033],\n",
      "          [ 0.0970, -0.0613, -0.0135]],\n",
      "\n",
      "         [[ 0.0287, -0.0842,  0.0974],\n",
      "          [-0.0034, -0.1049, -0.0635],\n",
      "          [-0.1104,  0.0819,  0.0180]],\n",
      "\n",
      "         [[ 0.0049, -0.0897,  0.0818],\n",
      "          [ 0.0588,  0.0144, -0.0433],\n",
      "          [-0.0391,  0.0193, -0.0885]]],\n",
      "\n",
      "\n",
      "        [[[-0.0774,  0.0587,  0.0720],\n",
      "          [-0.0434,  0.0371, -0.0331],\n",
      "          [ 0.0048, -0.0880,  0.0178]],\n",
      "\n",
      "         [[ 0.0198, -0.1003, -0.0073],\n",
      "          [-0.0258, -0.0079,  0.0174],\n",
      "          [ 0.0170, -0.0738,  0.0274]],\n",
      "\n",
      "         [[-0.0147, -0.0326,  0.0304],\n",
      "          [ 0.0958,  0.0712, -0.1097],\n",
      "          [-0.0220,  0.0572,  0.0959]],\n",
      "\n",
      "         [[ 0.0719,  0.0320, -0.1108],\n",
      "          [ 0.0330,  0.1074, -0.0036],\n",
      "          [-0.0442,  0.0209,  0.0518]],\n",
      "\n",
      "         [[-0.0660, -0.0261, -0.0272],\n",
      "          [-0.0076,  0.0811,  0.0765],\n",
      "          [-0.0758,  0.0742, -0.0945]],\n",
      "\n",
      "         [[ 0.0461,  0.0827, -0.0969],\n",
      "          [-0.0127, -0.0947, -0.0135],\n",
      "          [-0.0791,  0.0378,  0.0468]],\n",
      "\n",
      "         [[-0.0878,  0.0508, -0.0932],\n",
      "          [ 0.0985, -0.0261, -0.0498],\n",
      "          [ 0.0403,  0.0721, -0.0801]],\n",
      "\n",
      "         [[-0.0279, -0.0757, -0.0512],\n",
      "          [-0.0119, -0.0010, -0.1040],\n",
      "          [-0.0949, -0.0977,  0.0716]],\n",
      "\n",
      "         [[ 0.1038,  0.0282, -0.0862],\n",
      "          [ 0.0440, -0.0217, -0.0889],\n",
      "          [-0.0951, -0.0268,  0.0805]]]])\n",
      "Shape: torch.Size([8, 9, 3, 3]), std: 0.0633, mean: -0.0004\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.bn1.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.bn1.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.conv2.weight:\n",
      "tensor([[[[ 6.9264e-02,  3.2499e-02,  6.7252e-02],\n",
      "          [ 9.1286e-02,  4.1726e-02, -4.4741e-03],\n",
      "          [-8.6483e-02,  9.0532e-02, -6.6119e-02]],\n",
      "\n",
      "         [[-2.9169e-02,  8.2996e-03, -6.8562e-02],\n",
      "          [ 4.6010e-02, -3.7000e-02,  1.8474e-02],\n",
      "          [-9.1260e-03,  1.1508e-01,  1.0631e-01]],\n",
      "\n",
      "         [[-1.8961e-02,  9.9075e-02, -4.6915e-02],\n",
      "          [-9.4604e-02,  1.1143e-01,  4.9921e-02],\n",
      "          [-6.2306e-02,  3.5456e-02,  2.4740e-02]],\n",
      "\n",
      "         [[ 2.3194e-02,  1.1638e-01, -6.2392e-02],\n",
      "          [-1.0109e-01,  7.5922e-02,  4.0759e-02],\n",
      "          [-2.9457e-02,  3.6505e-02, -6.3776e-02]],\n",
      "\n",
      "         [[-5.0044e-02, -1.1760e-01, -1.1285e-01],\n",
      "          [ 9.6632e-02,  9.0234e-02, -5.7632e-02],\n",
      "          [ 3.5569e-02,  7.9385e-02,  1.1393e-01]],\n",
      "\n",
      "         [[-9.6915e-02, -3.0724e-02, -2.3529e-02],\n",
      "          [ 6.7883e-02, -5.2125e-02,  9.8178e-02],\n",
      "          [ 3.0191e-02,  2.8552e-02, -7.9540e-02]],\n",
      "\n",
      "         [[-8.4496e-02, -8.4410e-02,  6.3461e-02],\n",
      "          [ 9.4021e-02, -5.6123e-02, -2.9020e-02],\n",
      "          [ 3.7385e-02,  8.2938e-02, -2.3652e-02]],\n",
      "\n",
      "         [[ 1.0181e-01,  1.1461e-01,  5.5868e-02],\n",
      "          [-1.8332e-02, -5.0171e-02, -2.7182e-03],\n",
      "          [ 4.4445e-03, -1.7536e-02, -1.1126e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.7946e-02, -9.1187e-02, -1.9557e-02],\n",
      "          [-2.4800e-02, -1.5429e-02, -1.0784e-01],\n",
      "          [-5.0369e-02,  5.6164e-02, -3.8255e-02]],\n",
      "\n",
      "         [[ 3.6995e-02, -1.1507e-01,  3.4228e-02],\n",
      "          [-2.7226e-03, -7.3286e-04, -4.8605e-02],\n",
      "          [-9.9667e-02,  3.1194e-02, -9.9468e-02]],\n",
      "\n",
      "         [[ 9.6006e-02,  8.9146e-02,  1.0354e-01],\n",
      "          [ 7.0803e-02,  3.0192e-02,  1.7077e-02],\n",
      "          [ 2.8557e-02, -4.5477e-02,  6.7695e-02]],\n",
      "\n",
      "         [[ 1.1293e-01, -8.7528e-02, -9.3358e-02],\n",
      "          [-1.7644e-02,  1.0749e-01, -1.1378e-01],\n",
      "          [-1.0588e-01, -7.1562e-02, -1.1739e-01]],\n",
      "\n",
      "         [[ 1.0540e-01,  4.3065e-03,  8.0134e-02],\n",
      "          [ 8.2489e-02,  9.7745e-02, -2.8284e-02],\n",
      "          [-8.5085e-02,  1.4017e-02, -1.0477e-01]],\n",
      "\n",
      "         [[-1.0748e-01, -1.1470e-01,  2.8640e-02],\n",
      "          [ 2.7430e-02, -9.4667e-02,  7.4911e-02],\n",
      "          [-5.4492e-02, -5.0327e-02,  6.9435e-02]],\n",
      "\n",
      "         [[ 5.9413e-02,  2.0551e-02, -8.2951e-02],\n",
      "          [ 2.6692e-03, -5.3602e-02, -2.4971e-02],\n",
      "          [ 5.6912e-02, -9.6100e-03, -9.6911e-02]],\n",
      "\n",
      "         [[ 1.6000e-02,  3.8155e-02,  1.6678e-02],\n",
      "          [ 6.3919e-02,  3.7523e-02,  7.0348e-02],\n",
      "          [ 3.4320e-02, -3.8073e-02,  1.6016e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.4052e-02, -5.3872e-02, -3.3774e-02],\n",
      "          [ 8.3609e-02, -8.8879e-02, -1.1006e-02],\n",
      "          [ 9.5053e-02, -1.1784e-01, -1.6297e-02]],\n",
      "\n",
      "         [[-6.1045e-02, -7.7936e-02,  9.3219e-02],\n",
      "          [-5.2010e-03, -2.5642e-03,  8.1160e-02],\n",
      "          [ 1.1735e-01, -1.1104e-01,  6.7509e-02]],\n",
      "\n",
      "         [[ 5.2872e-02, -4.5446e-02,  5.0211e-03],\n",
      "          [ 1.0595e-01,  1.0139e-01,  9.0020e-02],\n",
      "          [-8.3432e-02, -1.1469e-01,  3.0416e-02]],\n",
      "\n",
      "         [[ 4.7732e-03, -1.4196e-02,  1.1617e-01],\n",
      "          [-2.2373e-02, -2.8263e-02,  9.0919e-02],\n",
      "          [-2.9978e-02,  1.0896e-01,  5.3621e-02]],\n",
      "\n",
      "         [[-9.5573e-02,  6.3913e-02,  1.0237e-01],\n",
      "          [ 1.1287e-01, -1.0210e-01,  3.7079e-02],\n",
      "          [ 2.6057e-02,  9.2807e-02,  9.3821e-02]],\n",
      "\n",
      "         [[-1.0541e-01, -6.8703e-02,  1.1308e-01],\n",
      "          [ 6.3289e-02, -9.3359e-02,  7.1042e-02],\n",
      "          [ 9.9437e-02, -8.0047e-02,  5.5637e-02]],\n",
      "\n",
      "         [[ 1.1001e-01, -1.0205e-01, -4.1055e-02],\n",
      "          [ 1.1721e-01,  2.4287e-02, -3.3124e-02],\n",
      "          [ 9.3739e-02, -5.1779e-02,  5.5297e-02]],\n",
      "\n",
      "         [[-9.3847e-02,  1.4937e-02, -6.9743e-02],\n",
      "          [-8.0117e-02, -9.0793e-02,  2.9397e-02],\n",
      "          [-6.4045e-02, -9.6865e-02,  4.6718e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0891e-01, -6.5759e-02,  3.2359e-02],\n",
      "          [ 8.8059e-02, -3.7532e-02,  1.1410e-01],\n",
      "          [-7.0913e-02,  5.6230e-02,  4.4124e-02]],\n",
      "\n",
      "         [[-1.0327e-02, -1.6915e-02, -6.4407e-02],\n",
      "          [ 1.1060e-01,  9.3819e-02, -3.0247e-02],\n",
      "          [-1.0431e-01,  1.1038e-01, -6.5461e-02]],\n",
      "\n",
      "         [[ 9.2635e-02,  5.2863e-02, -1.0049e-01],\n",
      "          [ 5.5471e-02, -6.9940e-02, -1.3780e-02],\n",
      "          [-1.0780e-01,  6.9275e-02, -5.1115e-02]],\n",
      "\n",
      "         [[-8.8745e-02,  8.9054e-03, -1.0213e-01],\n",
      "          [-4.4756e-02,  9.4430e-02,  8.2050e-02],\n",
      "          [ 1.1207e-01, -8.1068e-02, -8.3054e-02]],\n",
      "\n",
      "         [[ 9.3633e-02, -6.9543e-02,  7.5269e-02],\n",
      "          [ 1.0802e-02, -5.2510e-02,  7.8016e-02],\n",
      "          [-1.6818e-02,  5.7383e-03,  9.7192e-02]],\n",
      "\n",
      "         [[ 8.7381e-02,  4.4689e-02, -5.3940e-02],\n",
      "          [-5.9735e-02, -1.1122e-01,  1.6620e-02],\n",
      "          [-7.0426e-02,  9.6991e-02, -9.9890e-02]],\n",
      "\n",
      "         [[ 4.7701e-02, -5.5103e-02,  6.0381e-02],\n",
      "          [-1.1493e-01,  9.4787e-04,  7.6309e-02],\n",
      "          [ 5.7272e-02, -2.4133e-02,  8.0491e-02]],\n",
      "\n",
      "         [[-8.2412e-02, -7.7376e-02,  1.1628e-01],\n",
      "          [ 9.3575e-02, -1.0099e-01, -1.8239e-02],\n",
      "          [ 2.9587e-02, -1.1382e-01,  5.1542e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.9527e-02,  5.7769e-02, -4.6369e-02],\n",
      "          [-8.2521e-02, -1.2574e-02,  1.5206e-02],\n",
      "          [ 7.9470e-02, -3.6326e-02, -3.3808e-02]],\n",
      "\n",
      "         [[ 2.2479e-02, -3.9092e-02,  4.7399e-02],\n",
      "          [ 1.1342e-01,  1.0342e-01, -6.8050e-02],\n",
      "          [ 4.4030e-02,  3.4115e-02, -8.4793e-02]],\n",
      "\n",
      "         [[ 1.1300e-01, -8.8058e-02,  2.5456e-02],\n",
      "          [-2.7017e-02,  7.8358e-02,  2.6026e-02],\n",
      "          [ 1.1013e-01, -8.6700e-02, -8.0989e-02]],\n",
      "\n",
      "         [[ 6.9729e-02,  9.2863e-02,  8.9739e-02],\n",
      "          [ 1.0092e-02,  5.5168e-02,  9.5743e-02],\n",
      "          [-1.0276e-01,  3.6421e-03,  2.7581e-02]],\n",
      "\n",
      "         [[ 1.0785e-01, -4.9836e-02, -9.1191e-02],\n",
      "          [ 1.0327e-01, -1.1192e-01, -1.1590e-02],\n",
      "          [-4.3974e-02, -3.4875e-02, -8.8909e-03]],\n",
      "\n",
      "         [[ 3.5096e-02, -3.9988e-02,  7.5402e-02],\n",
      "          [-2.6208e-02,  5.5238e-02,  6.0724e-02],\n",
      "          [ 3.1585e-02,  8.8685e-02,  4.8061e-02]],\n",
      "\n",
      "         [[-6.1220e-02,  1.3091e-02,  2.8764e-02],\n",
      "          [ 7.3845e-02,  9.3956e-02,  4.9856e-02],\n",
      "          [ 5.9378e-02, -4.4426e-02, -9.3029e-02]],\n",
      "\n",
      "         [[ 1.0001e-01,  9.7722e-02,  5.7865e-02],\n",
      "          [ 1.7515e-02,  2.4611e-02,  8.3390e-02],\n",
      "          [-6.5395e-02,  4.3132e-02, -9.1278e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.7327e-02, -7.1407e-02,  9.3448e-02],\n",
      "          [-6.8994e-02, -9.9370e-02, -1.1544e-01],\n",
      "          [ 1.0990e-01,  8.8595e-02, -3.0049e-02]],\n",
      "\n",
      "         [[-9.3103e-02,  5.4425e-02,  4.8435e-02],\n",
      "          [-3.8592e-02, -4.8643e-02, -5.8932e-02],\n",
      "          [-9.9216e-02,  1.2520e-02, -9.3168e-02]],\n",
      "\n",
      "         [[-3.7168e-02,  8.1366e-02, -3.7723e-03],\n",
      "          [ 1.9087e-02, -1.0519e-01,  2.8437e-02],\n",
      "          [-1.0342e-01,  1.1001e-01,  1.1205e-01]],\n",
      "\n",
      "         [[ 8.7075e-02, -1.2190e-02,  2.1309e-02],\n",
      "          [ 1.8469e-02, -3.1944e-02,  3.8197e-02],\n",
      "          [ 9.3139e-03,  1.0809e-01,  8.5931e-02]],\n",
      "\n",
      "         [[ 4.8675e-02, -1.0085e-01, -8.2464e-03],\n",
      "          [-2.9946e-02, -5.8414e-02, -5.7576e-02],\n",
      "          [-7.0132e-03,  7.6160e-03, -3.3101e-02]],\n",
      "\n",
      "         [[-6.1023e-02,  7.9527e-02, -4.5469e-03],\n",
      "          [-2.9333e-03,  2.9466e-02, -5.7604e-02],\n",
      "          [-5.5730e-02, -6.8989e-02,  9.6045e-03]],\n",
      "\n",
      "         [[ 7.3858e-02,  6.6527e-02, -4.7379e-02],\n",
      "          [ 8.1299e-02,  1.5055e-02, -3.4565e-04],\n",
      "          [-6.8052e-02, -4.7560e-02, -7.9947e-02]],\n",
      "\n",
      "         [[ 5.3668e-02,  8.9719e-02,  2.6354e-02],\n",
      "          [ 5.7503e-02,  6.2735e-02,  6.1032e-02],\n",
      "          [-6.2414e-03, -1.8450e-02,  5.9849e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.6305e-02,  7.2192e-02,  8.8489e-02],\n",
      "          [-3.0419e-02, -8.9739e-02, -6.3547e-02],\n",
      "          [-1.0108e-01,  5.7601e-02, -3.7245e-02]],\n",
      "\n",
      "         [[ 9.1692e-02,  4.8616e-02,  1.1785e-01],\n",
      "          [ 1.2256e-03,  1.0681e-01,  1.6255e-02],\n",
      "          [ 1.1774e-01, -9.9868e-02, -1.0376e-01]],\n",
      "\n",
      "         [[-7.8997e-02, -4.4286e-02,  9.6502e-02],\n",
      "          [ 4.6505e-02,  2.8336e-02,  7.3688e-02],\n",
      "          [ 6.1320e-02, -6.0608e-02, -7.1828e-02]],\n",
      "\n",
      "         [[ 7.8808e-02,  7.0482e-02,  9.4874e-02],\n",
      "          [ 1.0065e-01,  2.5301e-02, -5.8426e-02],\n",
      "          [ 5.0322e-02, -7.0974e-02, -1.0858e-01]],\n",
      "\n",
      "         [[-8.2567e-02, -1.1300e-01,  7.4102e-02],\n",
      "          [ 1.1677e-01, -3.8551e-02, -9.6069e-02],\n",
      "          [-1.0891e-01,  8.9889e-02,  2.3154e-02]],\n",
      "\n",
      "         [[ 8.6593e-02,  8.7908e-02,  3.6772e-02],\n",
      "          [-4.7224e-02,  5.6749e-03,  8.3313e-02],\n",
      "          [ 1.0728e-01, -8.6588e-03,  4.7690e-02]],\n",
      "\n",
      "         [[-1.8807e-02, -1.0175e-01, -3.5067e-03],\n",
      "          [ 4.3224e-02,  1.1605e-01, -3.3443e-03],\n",
      "          [-1.0084e-01, -1.0026e-01,  1.1108e-01]],\n",
      "\n",
      "         [[-5.8509e-02,  1.0097e-02,  5.4359e-02],\n",
      "          [-6.0802e-03, -7.6867e-02,  3.5175e-02],\n",
      "          [-3.4012e-03, -1.5684e-02,  6.7859e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3042e-02, -1.1211e-01,  1.1333e-01],\n",
      "          [-1.1504e-01, -1.0912e-01,  8.6036e-02],\n",
      "          [-1.9512e-02,  6.0019e-02, -2.7406e-03]],\n",
      "\n",
      "         [[ 6.3395e-02, -5.0817e-03, -6.4733e-02],\n",
      "          [-9.7181e-02,  1.0283e-01,  4.0916e-03],\n",
      "          [ 2.0476e-03,  7.2850e-02,  9.3159e-02]],\n",
      "\n",
      "         [[-9.5527e-02,  1.8904e-03,  1.4261e-02],\n",
      "          [-5.4404e-03,  1.8944e-02,  6.1946e-02],\n",
      "          [-1.0872e-02, -1.3108e-04,  5.6428e-02]],\n",
      "\n",
      "         [[-1.5244e-02,  2.3232e-02, -9.0856e-02],\n",
      "          [ 2.9486e-02,  9.2509e-02,  3.9861e-02],\n",
      "          [ 9.4940e-02,  3.9982e-02,  4.2013e-02]],\n",
      "\n",
      "         [[ 1.0783e-01,  6.9878e-02,  1.0441e-01],\n",
      "          [-2.4318e-02,  4.6355e-02, -8.1399e-02],\n",
      "          [-9.0973e-02,  8.7043e-02,  9.6534e-02]],\n",
      "\n",
      "         [[-8.1526e-04, -6.6933e-02, -5.4481e-02],\n",
      "          [-5.7002e-02, -1.0670e-01, -9.4038e-02],\n",
      "          [-2.3260e-03,  6.2762e-02,  8.0336e-02]],\n",
      "\n",
      "         [[-9.4848e-02, -7.0601e-02,  2.0925e-02],\n",
      "          [ 3.3365e-02, -2.9410e-02,  1.6295e-02],\n",
      "          [ 5.6836e-02, -4.6686e-02, -8.4312e-02]],\n",
      "\n",
      "         [[-3.7114e-02,  1.0181e-02, -9.8035e-02],\n",
      "          [-1.9727e-02, -2.3592e-02,  2.5710e-02],\n",
      "          [ 7.2198e-04, -2.4523e-02, -5.9919e-05]]]])\n",
      "Shape: torch.Size([8, 8, 3, 3]), std: 0.0699, mean: 0.0050\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.bn2.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.bn2.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.downsample.0.weight:\n",
      "tensor([[[[ 0.0637,  0.0172, -0.0339],\n",
      "          [ 0.0291, -0.0114, -0.0014],\n",
      "          [ 0.0762,  0.0033, -0.0167]],\n",
      "\n",
      "         [[ 0.0816,  0.0041,  0.0861],\n",
      "          [-0.0533, -0.0770,  0.0561],\n",
      "          [-0.0787,  0.0418,  0.0120]],\n",
      "\n",
      "         [[ 0.1010,  0.0193,  0.0567],\n",
      "          [-0.1060,  0.0162, -0.0227],\n",
      "          [-0.0736,  0.0426,  0.0065]],\n",
      "\n",
      "         [[ 0.0419,  0.0956, -0.1098],\n",
      "          [ 0.0799, -0.0801,  0.0920],\n",
      "          [-0.0022,  0.0277,  0.0393]],\n",
      "\n",
      "         [[ 0.0847,  0.1102,  0.0055],\n",
      "          [ 0.0760, -0.0616, -0.0720],\n",
      "          [ 0.0054,  0.1007,  0.0372]],\n",
      "\n",
      "         [[-0.1065, -0.0784, -0.0123],\n",
      "          [-0.1027, -0.0140, -0.0780],\n",
      "          [-0.1083,  0.0389, -0.0741]],\n",
      "\n",
      "         [[-0.0541, -0.0585, -0.0302],\n",
      "          [ 0.0161, -0.0682,  0.1009],\n",
      "          [ 0.0192, -0.0309,  0.0986]],\n",
      "\n",
      "         [[ 0.0752, -0.1046,  0.0726],\n",
      "          [ 0.0275,  0.1021,  0.0033],\n",
      "          [ 0.0321, -0.0225,  0.0163]],\n",
      "\n",
      "         [[-0.0799,  0.0207, -0.0988],\n",
      "          [-0.0286, -0.0633, -0.0646],\n",
      "          [-0.0464,  0.0263, -0.0901]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0061,  0.0966, -0.0297],\n",
      "          [-0.0534,  0.0800, -0.0575],\n",
      "          [-0.0128,  0.1048, -0.0088]],\n",
      "\n",
      "         [[ 0.0532, -0.0221, -0.1026],\n",
      "          [-0.0697, -0.0660, -0.0252],\n",
      "          [ 0.0090, -0.0448,  0.0454]],\n",
      "\n",
      "         [[-0.0139, -0.0114,  0.0852],\n",
      "          [-0.0507,  0.0778, -0.0320],\n",
      "          [ 0.0109,  0.0535,  0.0525]],\n",
      "\n",
      "         [[ 0.0151, -0.0083,  0.0838],\n",
      "          [-0.1106,  0.0917,  0.0966],\n",
      "          [-0.0496, -0.0826, -0.0188]],\n",
      "\n",
      "         [[ 0.0775, -0.0894, -0.1010],\n",
      "          [ 0.0711,  0.0228, -0.0430],\n",
      "          [ 0.0960,  0.0248,  0.0495]],\n",
      "\n",
      "         [[ 0.0565,  0.0237,  0.1008],\n",
      "          [-0.0058,  0.0884, -0.0814],\n",
      "          [-0.0771,  0.0721, -0.0974]],\n",
      "\n",
      "         [[ 0.0313,  0.0564, -0.0578],\n",
      "          [ 0.0146, -0.0547, -0.0303],\n",
      "          [ 0.1022,  0.0803, -0.0973]],\n",
      "\n",
      "         [[-0.0448, -0.0152,  0.0457],\n",
      "          [-0.0340,  0.0710, -0.0022],\n",
      "          [-0.0761, -0.0013, -0.0494]],\n",
      "\n",
      "         [[ 0.0826, -0.0215,  0.0596],\n",
      "          [-0.0403,  0.0766,  0.0645],\n",
      "          [-0.0577,  0.0954,  0.0092]]],\n",
      "\n",
      "\n",
      "        [[[-0.0440,  0.1070,  0.0449],\n",
      "          [ 0.0792, -0.0452, -0.0026],\n",
      "          [ 0.0085, -0.0925,  0.0047]],\n",
      "\n",
      "         [[ 0.0692,  0.0525, -0.0448],\n",
      "          [ 0.1078, -0.0870, -0.0358],\n",
      "          [ 0.0102, -0.0155,  0.0833]],\n",
      "\n",
      "         [[-0.0550, -0.0642,  0.0803],\n",
      "          [ 0.0272,  0.0027, -0.0029],\n",
      "          [ 0.0835, -0.0532,  0.0673]],\n",
      "\n",
      "         [[ 0.0268, -0.0193,  0.0027],\n",
      "          [-0.0680,  0.1092, -0.1050],\n",
      "          [ 0.0789,  0.0674,  0.0852]],\n",
      "\n",
      "         [[ 0.0679, -0.0950, -0.0188],\n",
      "          [-0.0845,  0.0639,  0.0281],\n",
      "          [ 0.0460,  0.0883,  0.0638]],\n",
      "\n",
      "         [[-0.0913,  0.0319, -0.0415],\n",
      "          [ 0.1095,  0.0071, -0.0527],\n",
      "          [-0.0459,  0.0934,  0.0468]],\n",
      "\n",
      "         [[ 0.0647, -0.1066, -0.0313],\n",
      "          [-0.0279, -0.0627,  0.0010],\n",
      "          [-0.0222, -0.1048, -0.0815]],\n",
      "\n",
      "         [[ 0.0877, -0.0532,  0.1028],\n",
      "          [-0.0168, -0.0954,  0.0608],\n",
      "          [-0.0267, -0.0923,  0.0172]],\n",
      "\n",
      "         [[ 0.0928, -0.0084,  0.0928],\n",
      "          [ 0.0339,  0.0501,  0.1061],\n",
      "          [ 0.0484,  0.0002, -0.0586]]],\n",
      "\n",
      "\n",
      "        [[[-0.0258,  0.1107,  0.0103],\n",
      "          [-0.0446, -0.0867, -0.0754],\n",
      "          [ 0.0826, -0.0870,  0.0023]],\n",
      "\n",
      "         [[-0.0495,  0.0444,  0.0847],\n",
      "          [ 0.0184, -0.0321, -0.0991],\n",
      "          [ 0.0367, -0.0966, -0.0069]],\n",
      "\n",
      "         [[ 0.0755,  0.0768, -0.0835],\n",
      "          [-0.1046,  0.0973, -0.0973],\n",
      "          [-0.0831,  0.0901,  0.0062]],\n",
      "\n",
      "         [[-0.1088,  0.0780,  0.0261],\n",
      "          [-0.0779,  0.0150, -0.0782],\n",
      "          [ 0.0652,  0.0770, -0.0755]],\n",
      "\n",
      "         [[-0.1004, -0.0341, -0.0387],\n",
      "          [-0.0159,  0.0784,  0.0029],\n",
      "          [-0.0129, -0.0788, -0.0855]],\n",
      "\n",
      "         [[ 0.0018, -0.0831, -0.0378],\n",
      "          [ 0.0358,  0.1083,  0.0412],\n",
      "          [ 0.0402, -0.0404,  0.0123]],\n",
      "\n",
      "         [[ 0.0732, -0.0092,  0.0126],\n",
      "          [-0.0030,  0.0781,  0.0564],\n",
      "          [ 0.0642,  0.0535,  0.0878]],\n",
      "\n",
      "         [[ 0.0285,  0.1027,  0.1097],\n",
      "          [ 0.0458,  0.0759, -0.0348],\n",
      "          [-0.0819, -0.0363,  0.0498]],\n",
      "\n",
      "         [[-0.0605,  0.1007, -0.0564],\n",
      "          [ 0.0187,  0.0690,  0.0173],\n",
      "          [-0.0420, -0.0822, -0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.0471, -0.0404,  0.1015],\n",
      "          [ 0.0709,  0.0387,  0.0163],\n",
      "          [ 0.1074, -0.0869, -0.0368]],\n",
      "\n",
      "         [[-0.0327, -0.0660, -0.0961],\n",
      "          [ 0.0914,  0.0044, -0.0075],\n",
      "          [ 0.0890, -0.0594,  0.0448]],\n",
      "\n",
      "         [[-0.0619,  0.0467, -0.0885],\n",
      "          [ 0.0068, -0.0872,  0.0952],\n",
      "          [-0.0103, -0.0683,  0.0714]],\n",
      "\n",
      "         [[-0.0580,  0.1079, -0.0822],\n",
      "          [ 0.0301, -0.1085, -0.0537],\n",
      "          [-0.0966,  0.0586,  0.0667]],\n",
      "\n",
      "         [[-0.0179, -0.0780,  0.0864],\n",
      "          [-0.0559,  0.0776, -0.0980],\n",
      "          [ 0.0149, -0.0297, -0.0913]],\n",
      "\n",
      "         [[ 0.0366,  0.0398,  0.0312],\n",
      "          [-0.0793, -0.0265,  0.0224],\n",
      "          [ 0.0685, -0.0759, -0.0542]],\n",
      "\n",
      "         [[ 0.0317,  0.1109,  0.0058],\n",
      "          [-0.0044,  0.0169, -0.0257],\n",
      "          [ 0.0260,  0.0289,  0.1024]],\n",
      "\n",
      "         [[ 0.0941, -0.0332, -0.0793],\n",
      "          [-0.0671, -0.0713,  0.0498],\n",
      "          [ 0.0042, -0.0045,  0.0377]],\n",
      "\n",
      "         [[ 0.0264,  0.0331, -0.1055],\n",
      "          [ 0.0696, -0.0663,  0.0722],\n",
      "          [ 0.0796, -0.0767, -0.0284]]],\n",
      "\n",
      "\n",
      "        [[[-0.0841, -0.0340, -0.0549],\n",
      "          [ 0.0493, -0.0293,  0.0631],\n",
      "          [ 0.0792, -0.0219,  0.1110]],\n",
      "\n",
      "         [[-0.0051, -0.0925,  0.0969],\n",
      "          [ 0.0043, -0.0108, -0.0951],\n",
      "          [-0.0246, -0.0548, -0.0277]],\n",
      "\n",
      "         [[-0.0567, -0.0831, -0.1054],\n",
      "          [ 0.0919,  0.0341, -0.1102],\n",
      "          [ 0.0387, -0.1100,  0.0237]],\n",
      "\n",
      "         [[ 0.0759, -0.1023, -0.0734],\n",
      "          [-0.0957, -0.0260, -0.0933],\n",
      "          [-0.0751,  0.0623, -0.0692]],\n",
      "\n",
      "         [[ 0.0653,  0.0282, -0.0385],\n",
      "          [ 0.0853,  0.0808, -0.0514],\n",
      "          [-0.0851,  0.1078,  0.0978]],\n",
      "\n",
      "         [[-0.0561,  0.0584, -0.0045],\n",
      "          [-0.0734,  0.0726,  0.0942],\n",
      "          [-0.0261,  0.0145, -0.1093]],\n",
      "\n",
      "         [[ 0.0652,  0.0928,  0.0687],\n",
      "          [ 0.0638, -0.0206, -0.0993],\n",
      "          [ 0.1086, -0.1006, -0.0870]],\n",
      "\n",
      "         [[ 0.0218, -0.0263,  0.0450],\n",
      "          [ 0.0970,  0.0286, -0.0956],\n",
      "          [-0.0500,  0.0315, -0.0747]],\n",
      "\n",
      "         [[ 0.1031, -0.0054,  0.0763],\n",
      "          [ 0.0646,  0.0679,  0.0187],\n",
      "          [ 0.0847,  0.0308, -0.0549]]],\n",
      "\n",
      "\n",
      "        [[[-0.0341, -0.0604, -0.0917],\n",
      "          [-0.0483, -0.0524,  0.0039],\n",
      "          [-0.0631,  0.0646, -0.0270]],\n",
      "\n",
      "         [[ 0.0062,  0.0433,  0.0728],\n",
      "          [-0.0132,  0.0334,  0.0844],\n",
      "          [ 0.1041,  0.0506,  0.0492]],\n",
      "\n",
      "         [[-0.0921,  0.0141,  0.0386],\n",
      "          [ 0.0843, -0.0377, -0.0705],\n",
      "          [ 0.0107, -0.0510,  0.0139]],\n",
      "\n",
      "         [[ 0.0060, -0.0093,  0.0528],\n",
      "          [ 0.0420, -0.1022,  0.0145],\n",
      "          [ 0.0039, -0.0436,  0.0806]],\n",
      "\n",
      "         [[-0.0024,  0.0236,  0.0809],\n",
      "          [ 0.0111,  0.0714,  0.1020],\n",
      "          [ 0.0719,  0.0726, -0.0911]],\n",
      "\n",
      "         [[ 0.0932, -0.0127, -0.0448],\n",
      "          [-0.0027,  0.0952, -0.0264],\n",
      "          [ 0.0729,  0.0265,  0.0621]],\n",
      "\n",
      "         [[-0.0203,  0.0824, -0.0219],\n",
      "          [-0.0277, -0.0451, -0.1067],\n",
      "          [-0.0990, -0.0987,  0.0600]],\n",
      "\n",
      "         [[-0.0042, -0.0540, -0.0780],\n",
      "          [-0.1005, -0.0105, -0.0369],\n",
      "          [ 0.0839,  0.0054, -0.0688]],\n",
      "\n",
      "         [[ 0.0355, -0.0487, -0.1028],\n",
      "          [-0.0562,  0.0474,  0.1107],\n",
      "          [ 0.1056,  0.0301,  0.1027]]],\n",
      "\n",
      "\n",
      "        [[[-0.0374,  0.0163, -0.0953],\n",
      "          [-0.0388,  0.0760,  0.0469],\n",
      "          [-0.1043,  0.1001,  0.0828]],\n",
      "\n",
      "         [[ 0.0215,  0.0044,  0.0146],\n",
      "          [ 0.0251,  0.0808, -0.0280],\n",
      "          [-0.0639, -0.0667,  0.1036]],\n",
      "\n",
      "         [[ 0.0431,  0.0580,  0.0048],\n",
      "          [-0.0803,  0.0459,  0.0801],\n",
      "          [-0.0790, -0.0268,  0.0510]],\n",
      "\n",
      "         [[ 0.0870, -0.0895,  0.0221],\n",
      "          [ 0.0440,  0.0981,  0.0565],\n",
      "          [-0.0380, -0.0099, -0.0262]],\n",
      "\n",
      "         [[-0.0146, -0.0670, -0.0200],\n",
      "          [-0.0760,  0.0373,  0.0430],\n",
      "          [ 0.0478, -0.0017, -0.1035]],\n",
      "\n",
      "         [[ 0.0093, -0.0935,  0.0964],\n",
      "          [ 0.0330,  0.0096, -0.0104],\n",
      "          [ 0.0276,  0.0198, -0.0850]],\n",
      "\n",
      "         [[-0.0532, -0.0775,  0.0013],\n",
      "          [-0.0807, -0.0621, -0.0067],\n",
      "          [-0.0774,  0.0999, -0.0784]],\n",
      "\n",
      "         [[-0.1009, -0.0798, -0.0063],\n",
      "          [-0.0238,  0.1082,  0.0786],\n",
      "          [-0.0847,  0.0481, -0.1043]],\n",
      "\n",
      "         [[-0.0271, -0.0923,  0.0900],\n",
      "          [ 0.0095, -0.0661, -0.0976],\n",
      "          [ 0.0375,  0.0911,  0.0325]]]])\n",
      "Shape: torch.Size([8, 9, 3, 3]), std: 0.0649, mean: 0.0016\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.downsample.1.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "dynamics.residual_layers.residual_layers.0.downsample.1.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "dynamics.reward_conv_layers.conv_layers.0.weight:\n",
      "tensor([[[[-0.1105]],\n",
      "\n",
      "         [[ 0.3517]],\n",
      "\n",
      "         [[-0.1688]],\n",
      "\n",
      "         [[-0.0891]],\n",
      "\n",
      "         [[ 0.2551]],\n",
      "\n",
      "         [[ 0.0129]],\n",
      "\n",
      "         [[ 0.3007]],\n",
      "\n",
      "         [[-0.2619]]],\n",
      "\n",
      "\n",
      "        [[[-0.1578]],\n",
      "\n",
      "         [[-0.0109]],\n",
      "\n",
      "         [[ 0.1968]],\n",
      "\n",
      "         [[ 0.1157]],\n",
      "\n",
      "         [[ 0.3413]],\n",
      "\n",
      "         [[-0.1492]],\n",
      "\n",
      "         [[ 0.1578]],\n",
      "\n",
      "         [[ 0.1365]]],\n",
      "\n",
      "\n",
      "        [[[-0.2074]],\n",
      "\n",
      "         [[-0.1254]],\n",
      "\n",
      "         [[-0.0223]],\n",
      "\n",
      "         [[-0.0406]],\n",
      "\n",
      "         [[ 0.1991]],\n",
      "\n",
      "         [[-0.2689]],\n",
      "\n",
      "         [[-0.1673]],\n",
      "\n",
      "         [[-0.2147]]],\n",
      "\n",
      "\n",
      "        [[[-0.0652]],\n",
      "\n",
      "         [[-0.2454]],\n",
      "\n",
      "         [[ 0.3215]],\n",
      "\n",
      "         [[-0.0175]],\n",
      "\n",
      "         [[ 0.2622]],\n",
      "\n",
      "         [[-0.3429]],\n",
      "\n",
      "         [[ 0.2677]],\n",
      "\n",
      "         [[ 0.1529]]],\n",
      "\n",
      "\n",
      "        [[[-0.0610]],\n",
      "\n",
      "         [[-0.3168]],\n",
      "\n",
      "         [[ 0.0427]],\n",
      "\n",
      "         [[-0.3316]],\n",
      "\n",
      "         [[ 0.1266]],\n",
      "\n",
      "         [[ 0.0436]],\n",
      "\n",
      "         [[ 0.1902]],\n",
      "\n",
      "         [[-0.3053]]],\n",
      "\n",
      "\n",
      "        [[[-0.0885]],\n",
      "\n",
      "         [[-0.0558]],\n",
      "\n",
      "         [[-0.2810]],\n",
      "\n",
      "         [[-0.0582]],\n",
      "\n",
      "         [[-0.3386]],\n",
      "\n",
      "         [[ 0.0881]],\n",
      "\n",
      "         [[-0.3426]],\n",
      "\n",
      "         [[-0.1236]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2272]],\n",
      "\n",
      "         [[ 0.2628]],\n",
      "\n",
      "         [[ 0.3052]],\n",
      "\n",
      "         [[ 0.2268]],\n",
      "\n",
      "         [[ 0.3200]],\n",
      "\n",
      "         [[ 0.3292]],\n",
      "\n",
      "         [[-0.2741]],\n",
      "\n",
      "         [[ 0.0554]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0523]],\n",
      "\n",
      "         [[ 0.2036]],\n",
      "\n",
      "         [[-0.1253]],\n",
      "\n",
      "         [[-0.3237]],\n",
      "\n",
      "         [[ 0.0739]],\n",
      "\n",
      "         [[ 0.3201]],\n",
      "\n",
      "         [[-0.3379]],\n",
      "\n",
      "         [[-0.2016]]],\n",
      "\n",
      "\n",
      "        [[[-0.1465]],\n",
      "\n",
      "         [[-0.1468]],\n",
      "\n",
      "         [[-0.3231]],\n",
      "\n",
      "         [[ 0.2136]],\n",
      "\n",
      "         [[ 0.0377]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[-0.2591]],\n",
      "\n",
      "         [[-0.0371]]],\n",
      "\n",
      "\n",
      "        [[[-0.2350]],\n",
      "\n",
      "         [[-0.2184]],\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[ 0.2466]],\n",
      "\n",
      "         [[ 0.2463]],\n",
      "\n",
      "         [[ 0.3275]],\n",
      "\n",
      "         [[ 0.3073]],\n",
      "\n",
      "         [[-0.2716]]],\n",
      "\n",
      "\n",
      "        [[[-0.2788]],\n",
      "\n",
      "         [[-0.2202]],\n",
      "\n",
      "         [[ 0.1166]],\n",
      "\n",
      "         [[-0.3499]],\n",
      "\n",
      "         [[-0.0833]],\n",
      "\n",
      "         [[-0.2750]],\n",
      "\n",
      "         [[-0.2168]],\n",
      "\n",
      "         [[-0.3113]]],\n",
      "\n",
      "\n",
      "        [[[-0.1361]],\n",
      "\n",
      "         [[ 0.2431]],\n",
      "\n",
      "         [[-0.0898]],\n",
      "\n",
      "         [[-0.0974]],\n",
      "\n",
      "         [[ 0.3187]],\n",
      "\n",
      "         [[-0.0420]],\n",
      "\n",
      "         [[-0.2750]],\n",
      "\n",
      "         [[-0.2892]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1343]],\n",
      "\n",
      "         [[ 0.2249]],\n",
      "\n",
      "         [[ 0.2249]],\n",
      "\n",
      "         [[-0.1543]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.1922]],\n",
      "\n",
      "         [[-0.0512]],\n",
      "\n",
      "         [[ 0.2880]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0167]],\n",
      "\n",
      "         [[ 0.2580]],\n",
      "\n",
      "         [[-0.1091]],\n",
      "\n",
      "         [[ 0.3120]],\n",
      "\n",
      "         [[-0.2802]],\n",
      "\n",
      "         [[ 0.2848]],\n",
      "\n",
      "         [[ 0.2576]],\n",
      "\n",
      "         [[ 0.1354]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2750]],\n",
      "\n",
      "         [[-0.2493]],\n",
      "\n",
      "         [[ 0.0348]],\n",
      "\n",
      "         [[-0.1257]],\n",
      "\n",
      "         [[ 0.1814]],\n",
      "\n",
      "         [[-0.1506]],\n",
      "\n",
      "         [[ 0.1061]],\n",
      "\n",
      "         [[ 0.1952]]],\n",
      "\n",
      "\n",
      "        [[[-0.3198]],\n",
      "\n",
      "         [[ 0.2092]],\n",
      "\n",
      "         [[ 0.0467]],\n",
      "\n",
      "         [[ 0.1239]],\n",
      "\n",
      "         [[ 0.1361]],\n",
      "\n",
      "         [[-0.2633]],\n",
      "\n",
      "         [[-0.3313]],\n",
      "\n",
      "         [[ 0.1896]]],\n",
      "\n",
      "\n",
      "        [[[-0.3363]],\n",
      "\n",
      "         [[ 0.2324]],\n",
      "\n",
      "         [[-0.2230]],\n",
      "\n",
      "         [[ 0.1875]],\n",
      "\n",
      "         [[ 0.1106]],\n",
      "\n",
      "         [[-0.3145]],\n",
      "\n",
      "         [[-0.2394]],\n",
      "\n",
      "         [[ 0.0989]]],\n",
      "\n",
      "\n",
      "        [[[-0.2930]],\n",
      "\n",
      "         [[ 0.2849]],\n",
      "\n",
      "         [[-0.0661]],\n",
      "\n",
      "         [[ 0.0702]],\n",
      "\n",
      "         [[ 0.1316]],\n",
      "\n",
      "         [[ 0.1707]],\n",
      "\n",
      "         [[ 0.3264]],\n",
      "\n",
      "         [[-0.0180]]],\n",
      "\n",
      "\n",
      "        [[[-0.2373]],\n",
      "\n",
      "         [[-0.1958]],\n",
      "\n",
      "         [[-0.3224]],\n",
      "\n",
      "         [[ 0.1874]],\n",
      "\n",
      "         [[ 0.0053]],\n",
      "\n",
      "         [[-0.1314]],\n",
      "\n",
      "         [[ 0.1786]],\n",
      "\n",
      "         [[-0.2890]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0516]],\n",
      "\n",
      "         [[-0.0576]],\n",
      "\n",
      "         [[ 0.3453]],\n",
      "\n",
      "         [[-0.3107]],\n",
      "\n",
      "         [[ 0.0730]],\n",
      "\n",
      "         [[ 0.3433]],\n",
      "\n",
      "         [[-0.2868]],\n",
      "\n",
      "         [[ 0.1602]]],\n",
      "\n",
      "\n",
      "        [[[-0.0266]],\n",
      "\n",
      "         [[-0.1936]],\n",
      "\n",
      "         [[ 0.1370]],\n",
      "\n",
      "         [[ 0.0553]],\n",
      "\n",
      "         [[ 0.1529]],\n",
      "\n",
      "         [[-0.0383]],\n",
      "\n",
      "         [[ 0.2027]],\n",
      "\n",
      "         [[ 0.1095]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0664]],\n",
      "\n",
      "         [[-0.1800]],\n",
      "\n",
      "         [[ 0.1515]],\n",
      "\n",
      "         [[ 0.1789]],\n",
      "\n",
      "         [[-0.2751]],\n",
      "\n",
      "         [[-0.3417]],\n",
      "\n",
      "         [[ 0.0986]],\n",
      "\n",
      "         [[-0.3415]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1608]],\n",
      "\n",
      "         [[ 0.2640]],\n",
      "\n",
      "         [[ 0.0149]],\n",
      "\n",
      "         [[-0.2321]],\n",
      "\n",
      "         [[ 0.3457]],\n",
      "\n",
      "         [[-0.2930]],\n",
      "\n",
      "         [[ 0.1012]],\n",
      "\n",
      "         [[ 0.1021]]],\n",
      "\n",
      "\n",
      "        [[[-0.2850]],\n",
      "\n",
      "         [[-0.1889]],\n",
      "\n",
      "         [[ 0.0480]],\n",
      "\n",
      "         [[ 0.2261]],\n",
      "\n",
      "         [[-0.2004]],\n",
      "\n",
      "         [[-0.2803]],\n",
      "\n",
      "         [[-0.1348]],\n",
      "\n",
      "         [[ 0.3428]]]])\n",
      "Shape: torch.Size([24, 8, 1, 1]), std: 0.2161, mean: -0.0071\n",
      "\n",
      "dynamics.reward_conv_layers.conv_layers.0.bias:\n",
      "tensor([ 0.2163,  0.0033, -0.1504, -0.1648,  0.3082, -0.1202,  0.2644,  0.0391,\n",
      "        -0.0279, -0.2449, -0.1504, -0.1768, -0.0787, -0.2790, -0.0608,  0.2634,\n",
      "         0.3169, -0.0033, -0.0393,  0.0872, -0.1830,  0.2587,  0.0945,  0.0894])\n",
      "Shape: torch.Size([24]), std: 0.1834, mean: 0.0109\n",
      "\n",
      "dynamics.reward.layer.weight:\n",
      "tensor([[-0.0218,  0.0203, -0.0114, -0.0612, -0.0005,  0.0614, -0.0120, -0.0077,\n",
      "          0.0098,  0.0328, -0.0181,  0.0016,  0.0123,  0.0535, -0.0177,  0.0497,\n",
      "          0.0331, -0.0381, -0.0638, -0.0550,  0.0198, -0.0494, -0.0182,  0.0005,\n",
      "          0.0091, -0.0283, -0.0464, -0.0668,  0.0039,  0.0075, -0.0275, -0.0176,\n",
      "          0.0473,  0.0047,  0.0531, -0.0666, -0.0075,  0.0565,  0.0354, -0.0269,\n",
      "         -0.0328, -0.0248,  0.0198, -0.0507, -0.0131,  0.0197,  0.0223, -0.0153,\n",
      "         -0.0087, -0.0130,  0.0398,  0.0375, -0.0638,  0.0121, -0.0577, -0.0176,\n",
      "         -0.0017, -0.0633,  0.0467,  0.0433, -0.0471,  0.0304,  0.0598, -0.0201,\n",
      "         -0.0278, -0.0558, -0.0354, -0.0191,  0.0538, -0.0346, -0.0639, -0.0406,\n",
      "          0.0610,  0.0199,  0.0534, -0.0501, -0.0650,  0.0283,  0.0518, -0.0526,\n",
      "         -0.0379, -0.0291, -0.0402, -0.0534, -0.0058, -0.0159,  0.0092, -0.0159,\n",
      "          0.0176, -0.0009,  0.0153,  0.0426,  0.0391, -0.0041,  0.0403,  0.0427,\n",
      "         -0.0116,  0.0147, -0.0238,  0.0306, -0.0215,  0.0433, -0.0147,  0.0125,\n",
      "          0.0143, -0.0510,  0.0116,  0.0469, -0.0548, -0.0034,  0.0447, -0.0480,\n",
      "         -0.0320,  0.0175, -0.0232,  0.0109,  0.0283,  0.0664, -0.0547,  0.0564,\n",
      "          0.0174, -0.0318, -0.0204,  0.0432, -0.0301, -0.0637, -0.0509,  0.0632,\n",
      "          0.0060, -0.0119,  0.0640, -0.0537, -0.0344, -0.0237,  0.0485,  0.0140,\n",
      "         -0.0441, -0.0045, -0.0494,  0.0158,  0.0486,  0.0674,  0.0262,  0.0566,\n",
      "          0.0626,  0.0536,  0.0248, -0.0396, -0.0301,  0.0378, -0.0063, -0.0410,\n",
      "         -0.0257,  0.0208,  0.0288, -0.0438,  0.0566,  0.0245, -0.0309,  0.0658,\n",
      "         -0.0545,  0.0643, -0.0477, -0.0621, -0.0066,  0.0007,  0.0081, -0.0322,\n",
      "          0.0089,  0.0486, -0.0175,  0.0167,  0.0080, -0.0012,  0.0677,  0.0190,\n",
      "         -0.0179, -0.0651, -0.0271, -0.0105, -0.0013,  0.0013, -0.0525,  0.0278,\n",
      "          0.0514,  0.0048, -0.0556,  0.0318, -0.0111, -0.0457,  0.0456,  0.0221,\n",
      "          0.0310, -0.0496, -0.0326,  0.0669, -0.0084,  0.0559,  0.0525, -0.0386,\n",
      "          0.0435, -0.0286,  0.0026, -0.0047, -0.0048, -0.0339, -0.0580,  0.0366,\n",
      "          0.0225,  0.0130,  0.0255, -0.0119, -0.0128, -0.0569,  0.0489,  0.0284]])\n",
      "Shape: torch.Size([1, 216]), std: 0.0379, mean: -0.0007\n",
      "\n",
      "dynamics.reward.layer.bias:\n",
      "tensor([0.0453])\n",
      "Shape: torch.Size([1]), std: nan, mean: 0.0453\n",
      "\n",
      "prediction.residual_layers.residual_layers.0.conv1.weight:\n",
      "tensor([[[[-5.4648e-02, -1.1546e-01,  7.1527e-03],\n",
      "          [ 1.3535e-02, -1.0861e-01, -3.4701e-02],\n",
      "          [-3.5255e-02, -1.1391e-01,  2.6562e-02]],\n",
      "\n",
      "         [[ 7.7063e-03, -1.1137e-02, -3.1434e-02],\n",
      "          [-2.9827e-02,  4.7775e-02, -6.9471e-02],\n",
      "          [-1.1306e-01,  3.1633e-02,  8.3705e-02]],\n",
      "\n",
      "         [[ 9.3647e-02, -5.7569e-02, -5.0319e-02],\n",
      "          [-1.1766e-01,  1.0536e-01,  1.0082e-01],\n",
      "          [-1.0865e-02, -7.1718e-02, -2.4978e-02]],\n",
      "\n",
      "         [[-1.0257e-01,  9.2830e-02,  8.9358e-02],\n",
      "          [ 7.1103e-02,  1.2507e-02, -8.8332e-02],\n",
      "          [-4.0411e-02,  6.5766e-02,  2.8779e-02]],\n",
      "\n",
      "         [[-4.7659e-02, -4.1723e-02,  6.0352e-03],\n",
      "          [ 9.7762e-03, -7.6286e-03, -5.7665e-03],\n",
      "          [-1.1992e-02,  7.5417e-02, -5.1462e-02]],\n",
      "\n",
      "         [[ 3.1165e-02, -5.3822e-02,  8.3725e-03],\n",
      "          [-8.7959e-02, -9.0829e-02, -7.0098e-02],\n",
      "          [ 8.5595e-02,  5.3450e-02,  2.1136e-02]],\n",
      "\n",
      "         [[ 7.4977e-02,  2.0292e-02, -3.2532e-02],\n",
      "          [ 5.0571e-03,  4.6472e-02,  7.3296e-02],\n",
      "          [ 1.1594e-01, -5.4588e-02,  9.1126e-02]],\n",
      "\n",
      "         [[-1.0438e-01, -2.4383e-02, -1.1424e-01],\n",
      "          [-1.1358e-01, -2.5810e-02,  1.0138e-02],\n",
      "          [-1.0604e-02,  1.0265e-01, -1.7725e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.8556e-02, -1.2586e-02,  4.5058e-02],\n",
      "          [ 1.4034e-02, -2.4906e-02,  5.5319e-02],\n",
      "          [-7.9627e-02, -2.2239e-03, -7.8185e-02]],\n",
      "\n",
      "         [[ 9.8994e-03,  1.9681e-02, -8.8845e-02],\n",
      "          [ 8.5718e-02, -4.0750e-02, -9.8184e-02],\n",
      "          [-4.8221e-03, -1.3782e-02, -7.4234e-02]],\n",
      "\n",
      "         [[-1.1204e-01, -2.7651e-02, -8.7343e-02],\n",
      "          [ 8.7297e-02, -9.0214e-02,  9.4863e-02],\n",
      "          [ 5.9501e-02,  5.1685e-02, -4.4924e-02]],\n",
      "\n",
      "         [[-8.8177e-02,  8.5403e-02, -5.0045e-02],\n",
      "          [ 1.3625e-02,  2.2247e-02, -7.8340e-02],\n",
      "          [-7.1672e-02,  1.0887e-01,  1.0504e-01]],\n",
      "\n",
      "         [[-9.8598e-02,  4.8124e-02,  7.0624e-02],\n",
      "          [-9.6794e-02, -7.3439e-02,  2.6761e-02],\n",
      "          [-1.1487e-01,  3.5032e-02,  1.0594e-01]],\n",
      "\n",
      "         [[ 1.0408e-02, -1.0853e-01, -3.5952e-02],\n",
      "          [ 2.6312e-02, -3.3054e-02, -3.6963e-02],\n",
      "          [ 8.2471e-02, -1.1461e-01, -5.7857e-02]],\n",
      "\n",
      "         [[-7.8439e-02, -8.7328e-02, -9.5381e-02],\n",
      "          [-1.7513e-02, -5.2952e-02, -1.1476e-02],\n",
      "          [-9.9657e-02,  8.3112e-02,  3.2034e-02]],\n",
      "\n",
      "         [[-4.1238e-02, -7.5059e-03,  7.5079e-02],\n",
      "          [-9.9673e-02, -2.3225e-02, -9.1296e-02],\n",
      "          [-1.0241e-01, -9.9375e-02,  9.0197e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2951e-02, -1.1031e-01,  7.1749e-02],\n",
      "          [-8.4895e-02,  4.8555e-02,  7.9552e-02],\n",
      "          [-8.5417e-02,  1.4674e-02,  3.8477e-03]],\n",
      "\n",
      "         [[ 1.0264e-01,  6.2909e-02,  1.0917e-01],\n",
      "          [ 6.8798e-02, -3.3018e-02,  1.0303e-01],\n",
      "          [ 7.9289e-02, -1.0939e-01,  1.3828e-02]],\n",
      "\n",
      "         [[ 8.7819e-03,  3.4372e-02, -9.2988e-02],\n",
      "          [ 8.3893e-02, -1.0974e-01, -7.1218e-02],\n",
      "          [-4.5534e-02, -1.0117e-01,  2.2805e-02]],\n",
      "\n",
      "         [[ 5.0808e-02,  2.6974e-02, -4.0206e-02],\n",
      "          [-1.0611e-01,  8.3755e-02, -8.9757e-03],\n",
      "          [ 9.4766e-02, -4.1167e-02, -8.3349e-02]],\n",
      "\n",
      "         [[-3.8026e-03,  8.5688e-02, -5.9031e-02],\n",
      "          [-5.1820e-02,  5.5103e-02,  2.6957e-02],\n",
      "          [ 2.7756e-02, -9.9422e-02,  1.0904e-01]],\n",
      "\n",
      "         [[-1.1553e-01, -1.1305e-01,  2.9787e-02],\n",
      "          [ 6.4515e-02,  8.9243e-02,  9.9420e-02],\n",
      "          [ 9.2535e-02,  8.4806e-03, -5.3533e-02]],\n",
      "\n",
      "         [[ 7.2324e-02, -1.9517e-02,  1.0273e-01],\n",
      "          [-4.7999e-02,  8.9535e-03, -1.0366e-01],\n",
      "          [-1.0804e-01,  7.4851e-02, -1.0803e-01]],\n",
      "\n",
      "         [[ 7.4033e-02, -7.3273e-02, -6.0592e-02],\n",
      "          [ 2.3113e-02,  5.2204e-02,  6.0127e-02],\n",
      "          [ 7.2536e-02, -7.1701e-02,  3.8280e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.4157e-02,  5.0650e-02,  7.3014e-03],\n",
      "          [-3.4235e-02, -8.3318e-02, -3.2763e-02],\n",
      "          [ 3.3039e-02, -9.2772e-02, -5.0450e-02]],\n",
      "\n",
      "         [[-1.0722e-01,  9.8971e-02,  9.0936e-02],\n",
      "          [ 5.2819e-02, -9.5007e-02,  9.9113e-02],\n",
      "          [ 6.1831e-02, -2.7186e-02,  1.0443e-01]],\n",
      "\n",
      "         [[-1.7563e-02, -1.1702e-01, -1.5025e-02],\n",
      "          [-9.3216e-02,  1.1715e-01,  6.7284e-02],\n",
      "          [-1.1561e-01, -8.2706e-02, -1.9680e-02]],\n",
      "\n",
      "         [[-8.6872e-02,  1.4837e-02,  1.1368e-01],\n",
      "          [ 3.0850e-02, -7.7468e-02, -3.5884e-03],\n",
      "          [-8.9660e-05, -9.0846e-02, -7.4523e-02]],\n",
      "\n",
      "         [[-6.9006e-03, -3.2579e-02, -5.7800e-02],\n",
      "          [-1.3415e-02,  1.2476e-02, -9.4971e-03],\n",
      "          [-5.3014e-02, -1.1761e-01,  2.8808e-02]],\n",
      "\n",
      "         [[ 6.4834e-02,  5.9317e-02,  1.0823e-02],\n",
      "          [-1.4275e-03,  5.4975e-02,  2.6325e-02],\n",
      "          [-1.1593e-02,  5.9168e-02,  1.0779e-01]],\n",
      "\n",
      "         [[ 4.1025e-02, -5.5056e-02, -2.2173e-02],\n",
      "          [ 1.1745e-01,  1.0820e-01, -2.3202e-02],\n",
      "          [ 9.0167e-02, -1.0159e-01,  1.1231e-01]],\n",
      "\n",
      "         [[-2.1613e-02,  1.0548e-02,  4.3279e-02],\n",
      "          [ 1.1214e-01, -4.5688e-02, -1.4235e-02],\n",
      "          [-8.8587e-02,  1.2938e-02,  5.1490e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.7890e-02,  4.7951e-02,  1.1024e-01],\n",
      "          [ 9.3437e-02,  3.8636e-02, -6.1845e-02],\n",
      "          [ 4.8114e-02,  1.1415e-01,  9.2565e-02]],\n",
      "\n",
      "         [[-1.1182e-01,  5.1276e-02,  5.1683e-02],\n",
      "          [-4.5852e-02, -6.0997e-02, -9.2922e-02],\n",
      "          [ 9.7290e-02, -6.1643e-04, -6.0039e-02]],\n",
      "\n",
      "         [[ 9.4608e-02,  8.5516e-02,  2.6021e-02],\n",
      "          [ 3.4652e-02, -1.4459e-03, -9.7873e-03],\n",
      "          [ 5.1034e-02,  7.2370e-02, -6.1985e-02]],\n",
      "\n",
      "         [[ 1.6442e-02,  7.6926e-02, -1.0526e-01],\n",
      "          [-6.3903e-02, -1.3277e-02, -1.9162e-02],\n",
      "          [-4.0381e-02, -8.4779e-02,  1.1203e-02]],\n",
      "\n",
      "         [[-4.7869e-02,  1.1621e-01,  1.0154e-01],\n",
      "          [-3.1298e-02, -2.0558e-02, -5.3976e-02],\n",
      "          [-3.4532e-02,  5.8137e-02,  7.6413e-02]],\n",
      "\n",
      "         [[ 3.6740e-02, -1.8978e-02,  4.5940e-02],\n",
      "          [-9.9752e-02,  2.4095e-02, -6.7263e-02],\n",
      "          [-1.1735e-01, -5.2934e-02, -4.2058e-02]],\n",
      "\n",
      "         [[-7.0119e-02,  5.1454e-02, -4.7573e-03],\n",
      "          [-2.1850e-02,  8.1423e-02,  2.9705e-02],\n",
      "          [-9.0877e-02,  3.0646e-02, -6.5380e-02]],\n",
      "\n",
      "         [[ 9.0519e-02, -4.7383e-04, -6.3685e-02],\n",
      "          [-5.2548e-02,  1.1064e-01, -1.1033e-01],\n",
      "          [ 3.7798e-02,  6.5605e-02, -8.5797e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.7232e-02, -3.8560e-02,  9.2004e-02],\n",
      "          [ 5.2630e-02,  4.6959e-02,  5.9451e-02],\n",
      "          [ 7.2200e-02, -3.9577e-02, -1.1266e-01]],\n",
      "\n",
      "         [[-8.3655e-02, -1.0661e-02,  2.9352e-02],\n",
      "          [ 3.1827e-02, -2.0631e-03, -1.0000e-02],\n",
      "          [ 1.1505e-01,  2.6976e-03, -7.6272e-02]],\n",
      "\n",
      "         [[ 4.1699e-02,  7.6851e-02,  1.9933e-02],\n",
      "          [ 9.4277e-02,  7.2387e-02,  4.8146e-02],\n",
      "          [-4.9742e-02,  9.1166e-02,  8.3007e-02]],\n",
      "\n",
      "         [[ 1.1683e-01, -1.7108e-03,  7.3469e-02],\n",
      "          [ 1.3231e-02,  1.1638e-01,  1.3672e-02],\n",
      "          [ 7.1660e-03, -6.6007e-02,  9.4352e-02]],\n",
      "\n",
      "         [[-4.9041e-02, -1.1510e-02, -5.5348e-02],\n",
      "          [ 4.5485e-02,  8.8901e-02,  4.8118e-02],\n",
      "          [-4.4623e-02, -7.5858e-02,  1.0816e-01]],\n",
      "\n",
      "         [[-2.5643e-02, -1.1464e-01,  4.9171e-02],\n",
      "          [-6.0231e-03, -1.4442e-02, -5.5482e-02],\n",
      "          [-3.3598e-02, -3.4886e-03,  4.3478e-02]],\n",
      "\n",
      "         [[-1.3883e-03, -9.9992e-02,  3.3550e-02],\n",
      "          [ 8.8889e-02,  4.3136e-02, -9.3462e-02],\n",
      "          [ 9.5223e-03,  1.2208e-02,  1.7063e-02]],\n",
      "\n",
      "         [[ 7.0774e-02,  1.0818e-01,  7.0032e-02],\n",
      "          [-4.4204e-02,  1.7879e-02,  8.5905e-02],\n",
      "          [ 5.9017e-02,  3.5852e-02,  5.6859e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1249e-02,  1.5924e-02, -9.2979e-02],\n",
      "          [ 9.2298e-02,  1.3856e-02, -9.1598e-02],\n",
      "          [ 9.6988e-02, -1.1489e-01,  1.1712e-01]],\n",
      "\n",
      "         [[ 6.1385e-03,  3.0227e-02,  1.0753e-01],\n",
      "          [ 8.2307e-02,  8.6227e-02,  4.7571e-02],\n",
      "          [-7.2019e-02, -7.8525e-02,  1.9362e-03]],\n",
      "\n",
      "         [[-7.4447e-03,  2.5744e-02, -1.0737e-01],\n",
      "          [ 1.3717e-02, -6.8469e-02, -7.3360e-02],\n",
      "          [ 7.4005e-03,  6.0458e-02, -9.2584e-02]],\n",
      "\n",
      "         [[ 2.4929e-02, -1.1328e-01,  9.5680e-02],\n",
      "          [-8.2583e-02, -7.2566e-02,  2.2131e-02],\n",
      "          [-6.0098e-02,  8.1785e-02, -2.9728e-02]],\n",
      "\n",
      "         [[-2.8528e-02, -1.0213e-01, -2.0278e-02],\n",
      "          [-1.1090e-02,  3.2012e-02, -1.0362e-01],\n",
      "          [ 3.9727e-02,  5.7322e-02, -6.4295e-02]],\n",
      "\n",
      "         [[ 5.4842e-02, -3.3402e-02,  8.6093e-02],\n",
      "          [ 8.7382e-03,  1.1661e-01, -7.4140e-02],\n",
      "          [ 1.1397e-01,  8.0473e-02, -6.7232e-02]],\n",
      "\n",
      "         [[ 3.6293e-02, -1.6097e-02,  1.0894e-01],\n",
      "          [ 5.5227e-02, -9.6389e-02, -7.9833e-02],\n",
      "          [-4.3505e-02,  2.2736e-02, -8.7301e-02]],\n",
      "\n",
      "         [[-3.1787e-03,  8.3420e-02, -1.1042e-01],\n",
      "          [-1.3965e-02, -5.4738e-02,  3.2986e-02],\n",
      "          [ 4.6999e-03, -3.7377e-02, -1.9454e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 6.1365e-02,  7.6872e-02,  8.7597e-02],\n",
      "          [ 1.0284e-01, -4.6986e-02, -3.9559e-02],\n",
      "          [-8.4601e-02, -7.4352e-02,  6.3656e-02]],\n",
      "\n",
      "         [[ 2.3824e-04, -4.4435e-02, -1.1113e-01],\n",
      "          [-4.0325e-02,  8.4532e-02, -2.0576e-02],\n",
      "          [-1.1340e-01,  5.5638e-02, -2.7011e-02]],\n",
      "\n",
      "         [[ 1.9410e-04, -2.3558e-02, -1.0632e-01],\n",
      "          [-3.4112e-02,  5.0085e-02,  9.1875e-02],\n",
      "          [ 2.9491e-02, -1.0421e-01,  1.8247e-02]],\n",
      "\n",
      "         [[-5.1331e-04,  9.6359e-02,  1.0970e-01],\n",
      "          [ 4.7527e-02,  2.3059e-03,  7.2348e-02],\n",
      "          [-8.5420e-03, -1.1103e-01,  1.0917e-01]],\n",
      "\n",
      "         [[-1.0234e-02,  9.1109e-03, -9.7063e-02],\n",
      "          [ 9.4144e-03, -4.1733e-02,  7.0485e-02],\n",
      "          [ 3.6083e-02, -7.4735e-02,  1.9831e-02]],\n",
      "\n",
      "         [[-7.9543e-02, -1.0879e-02, -6.9833e-02],\n",
      "          [ 2.0461e-02,  4.6622e-02,  4.7631e-03],\n",
      "          [ 4.5108e-02, -2.2955e-02,  9.6947e-02]],\n",
      "\n",
      "         [[ 6.0544e-02, -8.9982e-02,  8.2677e-02],\n",
      "          [ 1.1474e-02,  7.8625e-02, -4.3819e-02],\n",
      "          [ 4.3033e-02,  2.9048e-02, -1.2424e-03]],\n",
      "\n",
      "         [[-6.7266e-02, -3.2153e-02, -3.1213e-02],\n",
      "          [-1.1454e-01,  5.1631e-02, -1.3800e-02],\n",
      "          [-4.3694e-02,  2.3215e-02,  1.0518e-01]]]])\n",
      "Shape: torch.Size([8, 8, 3, 3]), std: 0.0678, mean: 0.0005\n",
      "\n",
      "prediction.residual_layers.residual_layers.0.bn1.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "prediction.residual_layers.residual_layers.0.bn1.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "prediction.residual_layers.residual_layers.0.conv2.weight:\n",
      "tensor([[[[ 0.0900,  0.0311,  0.0262],\n",
      "          [-0.0476,  0.0573, -0.0394],\n",
      "          [-0.0635, -0.0399, -0.0864]],\n",
      "\n",
      "         [[ 0.0141,  0.0427,  0.0272],\n",
      "          [ 0.0042,  0.0028,  0.0466],\n",
      "          [ 0.0941,  0.0726,  0.0524]],\n",
      "\n",
      "         [[ 0.0153, -0.0158, -0.0396],\n",
      "          [-0.0796,  0.0813, -0.0200],\n",
      "          [ 0.0935, -0.0125, -0.0820]],\n",
      "\n",
      "         [[-0.1110, -0.0506,  0.0135],\n",
      "          [-0.0866,  0.0259,  0.0360],\n",
      "          [-0.0771,  0.1105, -0.0473]],\n",
      "\n",
      "         [[ 0.1075, -0.0917,  0.0618],\n",
      "          [ 0.0052, -0.0641, -0.0185],\n",
      "          [ 0.1072,  0.0534,  0.0753]],\n",
      "\n",
      "         [[-0.0756,  0.0045, -0.0777],\n",
      "          [ 0.1137,  0.0092, -0.0055],\n",
      "          [ 0.0410, -0.0120, -0.0980]],\n",
      "\n",
      "         [[-0.0567,  0.0057,  0.0678],\n",
      "          [-0.0917,  0.0747, -0.0846],\n",
      "          [-0.0901, -0.0410, -0.1006]],\n",
      "\n",
      "         [[-0.0570, -0.0981,  0.0967],\n",
      "          [-0.0653, -0.1114, -0.1133],\n",
      "          [ 0.0324,  0.1090,  0.0759]]],\n",
      "\n",
      "\n",
      "        [[[-0.0701,  0.0785, -0.0049],\n",
      "          [ 0.0999, -0.0438,  0.0021],\n",
      "          [ 0.1160,  0.0631,  0.0280]],\n",
      "\n",
      "         [[ 0.0084, -0.0828,  0.0327],\n",
      "          [-0.0232, -0.0918, -0.0434],\n",
      "          [ 0.0351,  0.0262,  0.0301]],\n",
      "\n",
      "         [[ 0.0459, -0.0318, -0.1007],\n",
      "          [-0.0345, -0.0113, -0.1089],\n",
      "          [-0.0546,  0.1052, -0.1021]],\n",
      "\n",
      "         [[ 0.0513,  0.0468, -0.0131],\n",
      "          [-0.0345, -0.0393,  0.0096],\n",
      "          [-0.0760,  0.0306, -0.0462]],\n",
      "\n",
      "         [[ 0.1002, -0.1061,  0.0553],\n",
      "          [ 0.0987,  0.0533, -0.0032],\n",
      "          [-0.0826,  0.0235, -0.0888]],\n",
      "\n",
      "         [[-0.0575,  0.1049, -0.0279],\n",
      "          [ 0.0853, -0.0098,  0.0822],\n",
      "          [ 0.0660, -0.0992,  0.0880]],\n",
      "\n",
      "         [[-0.0642, -0.0388,  0.0161],\n",
      "          [ 0.0545, -0.1141, -0.0318],\n",
      "          [ 0.1011, -0.1009,  0.0484]],\n",
      "\n",
      "         [[ 0.0746, -0.0617,  0.0445],\n",
      "          [ 0.0780,  0.0895, -0.0709],\n",
      "          [ 0.0709, -0.0082, -0.0404]]],\n",
      "\n",
      "\n",
      "        [[[-0.0571, -0.0522,  0.0717],\n",
      "          [-0.0217, -0.0197,  0.0923],\n",
      "          [-0.0852,  0.0403, -0.0908]],\n",
      "\n",
      "         [[-0.0379,  0.0335,  0.0767],\n",
      "          [-0.0086,  0.0330,  0.0278],\n",
      "          [-0.0846, -0.0279, -0.0463]],\n",
      "\n",
      "         [[-0.0802, -0.0034,  0.1028],\n",
      "          [-0.0337, -0.0164,  0.1006],\n",
      "          [ 0.0678,  0.0427,  0.0610]],\n",
      "\n",
      "         [[ 0.0724, -0.0585, -0.1146],\n",
      "          [-0.0213,  0.0358, -0.0243],\n",
      "          [ 0.0052,  0.1031, -0.0682]],\n",
      "\n",
      "         [[ 0.0802, -0.0537, -0.0116],\n",
      "          [ 0.0033, -0.0611,  0.0114],\n",
      "          [-0.0859,  0.1150, -0.1103]],\n",
      "\n",
      "         [[-0.1153,  0.0270,  0.0717],\n",
      "          [-0.0040,  0.1081,  0.0689],\n",
      "          [ 0.0937,  0.0177, -0.0328]],\n",
      "\n",
      "         [[-0.0886,  0.0889, -0.0373],\n",
      "          [ 0.0876,  0.0626,  0.1066],\n",
      "          [-0.0203, -0.0559, -0.0721]],\n",
      "\n",
      "         [[-0.0671,  0.0932, -0.0851],\n",
      "          [-0.1154,  0.0728,  0.0272],\n",
      "          [ 0.0110, -0.1058,  0.1005]]],\n",
      "\n",
      "\n",
      "        [[[-0.0651, -0.0396, -0.1103],\n",
      "          [-0.0474,  0.0604, -0.0009],\n",
      "          [ 0.0609,  0.1003,  0.0863]],\n",
      "\n",
      "         [[ 0.0441,  0.0674, -0.0450],\n",
      "          [-0.0714, -0.0958, -0.0438],\n",
      "          [ 0.0338,  0.0024, -0.1076]],\n",
      "\n",
      "         [[-0.1062,  0.0316, -0.0288],\n",
      "          [ 0.0871,  0.0866, -0.1087],\n",
      "          [ 0.0495, -0.0200,  0.0305]],\n",
      "\n",
      "         [[ 0.0469,  0.0948, -0.0173],\n",
      "          [-0.0409, -0.1010,  0.1051],\n",
      "          [-0.0893,  0.0603,  0.0363]],\n",
      "\n",
      "         [[ 0.0949, -0.1025, -0.0894],\n",
      "          [ 0.0313, -0.0850, -0.0890],\n",
      "          [ 0.1157, -0.0309, -0.0251]],\n",
      "\n",
      "         [[-0.0526,  0.0117,  0.0701],\n",
      "          [ 0.0405, -0.0596,  0.0949],\n",
      "          [ 0.0324, -0.0826, -0.0102]],\n",
      "\n",
      "         [[-0.0108, -0.1104,  0.0070],\n",
      "          [ 0.0104, -0.1162, -0.1039],\n",
      "          [ 0.0408,  0.1025,  0.1068]],\n",
      "\n",
      "         [[ 0.0968, -0.1107,  0.0069],\n",
      "          [-0.0986, -0.0907,  0.1039],\n",
      "          [-0.0013,  0.0916, -0.0502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1126,  0.1144,  0.0220],\n",
      "          [-0.0168,  0.0463,  0.1119],\n",
      "          [-0.0943,  0.0167,  0.0240]],\n",
      "\n",
      "         [[-0.0287, -0.0200,  0.0158],\n",
      "          [-0.0919,  0.0309,  0.0865],\n",
      "          [ 0.0349,  0.0092,  0.0185]],\n",
      "\n",
      "         [[-0.0924,  0.0411, -0.0909],\n",
      "          [ 0.0030,  0.0699,  0.0046],\n",
      "          [-0.1096, -0.0592, -0.0796]],\n",
      "\n",
      "         [[ 0.1017, -0.0297, -0.0551],\n",
      "          [ 0.0973,  0.0401, -0.1026],\n",
      "          [-0.0115,  0.0221,  0.0284]],\n",
      "\n",
      "         [[-0.0833,  0.0262, -0.1062],\n",
      "          [ 0.0376, -0.0123,  0.0488],\n",
      "          [-0.0384,  0.1081,  0.0694]],\n",
      "\n",
      "         [[ 0.0555,  0.0655,  0.0811],\n",
      "          [-0.0136,  0.0436, -0.0391],\n",
      "          [ 0.0731,  0.0303, -0.1140]],\n",
      "\n",
      "         [[ 0.0295,  0.0077, -0.0749],\n",
      "          [-0.0852,  0.0403, -0.0057],\n",
      "          [-0.0755, -0.1001, -0.0439]],\n",
      "\n",
      "         [[-0.0837, -0.0935, -0.0187],\n",
      "          [ 0.0482, -0.1141,  0.1065],\n",
      "          [ 0.0886,  0.0602,  0.0806]]],\n",
      "\n",
      "\n",
      "        [[[-0.0513, -0.1065, -0.0401],\n",
      "          [-0.1096,  0.0291, -0.0378],\n",
      "          [-0.0984,  0.1132,  0.0515]],\n",
      "\n",
      "         [[ 0.0548,  0.0675, -0.0625],\n",
      "          [ 0.0212,  0.0633, -0.0032],\n",
      "          [ 0.0008,  0.1063, -0.0884]],\n",
      "\n",
      "         [[-0.0687, -0.0464, -0.1112],\n",
      "          [ 0.0073, -0.0757,  0.0099],\n",
      "          [-0.0609,  0.0225,  0.0047]],\n",
      "\n",
      "         [[-0.1053, -0.0033, -0.1132],\n",
      "          [-0.1149, -0.0368, -0.0370],\n",
      "          [-0.0040, -0.1025,  0.0328]],\n",
      "\n",
      "         [[ 0.0214,  0.1010,  0.0216],\n",
      "          [ 0.0565, -0.0656,  0.0710],\n",
      "          [-0.0871, -0.0064,  0.0551]],\n",
      "\n",
      "         [[-0.0041,  0.0484, -0.0473],\n",
      "          [-0.0630, -0.0424,  0.0795],\n",
      "          [-0.0999, -0.0687,  0.0986]],\n",
      "\n",
      "         [[-0.0581, -0.0528, -0.0544],\n",
      "          [ 0.0926, -0.0978,  0.0060],\n",
      "          [ 0.0755, -0.0480,  0.0928]],\n",
      "\n",
      "         [[-0.0572,  0.0663,  0.0695],\n",
      "          [ 0.0162,  0.0959, -0.0324],\n",
      "          [ 0.0602, -0.0692,  0.0744]]],\n",
      "\n",
      "\n",
      "        [[[-0.0285,  0.0188,  0.0239],\n",
      "          [-0.0573, -0.1042,  0.0038],\n",
      "          [ 0.0874, -0.0248,  0.0658]],\n",
      "\n",
      "         [[ 0.0244,  0.1028, -0.1072],\n",
      "          [ 0.0560,  0.0879, -0.0767],\n",
      "          [-0.0724, -0.0877, -0.0386]],\n",
      "\n",
      "         [[-0.0606, -0.0615, -0.0328],\n",
      "          [ 0.0787, -0.1161,  0.0329],\n",
      "          [-0.0747,  0.0988, -0.0306]],\n",
      "\n",
      "         [[-0.0992,  0.0561, -0.1097],\n",
      "          [-0.0322, -0.0248, -0.0803],\n",
      "          [ 0.0773,  0.0996,  0.0474]],\n",
      "\n",
      "         [[ 0.0404,  0.0288, -0.0282],\n",
      "          [-0.0427, -0.0159,  0.1012],\n",
      "          [ 0.0213,  0.0958, -0.0367]],\n",
      "\n",
      "         [[-0.0273,  0.0515, -0.0153],\n",
      "          [-0.1169, -0.0746,  0.0616],\n",
      "          [-0.0937, -0.1172, -0.0706]],\n",
      "\n",
      "         [[ 0.0860,  0.0443,  0.0128],\n",
      "          [ 0.1009, -0.0414, -0.0240],\n",
      "          [-0.0995, -0.0175,  0.1172]],\n",
      "\n",
      "         [[ 0.0712, -0.0056, -0.0015],\n",
      "          [-0.0610, -0.0873,  0.0413],\n",
      "          [ 0.0205, -0.0204,  0.0946]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0158, -0.0484, -0.1100],\n",
      "          [ 0.0933,  0.0495, -0.0013],\n",
      "          [-0.0665, -0.0979, -0.0571]],\n",
      "\n",
      "         [[ 0.0401,  0.1036,  0.0542],\n",
      "          [-0.0428,  0.1154,  0.0670],\n",
      "          [-0.1152, -0.1066, -0.0246]],\n",
      "\n",
      "         [[-0.0887, -0.1150,  0.0813],\n",
      "          [ 0.0814,  0.1038, -0.0958],\n",
      "          [ 0.1171,  0.0536,  0.0475]],\n",
      "\n",
      "         [[ 0.1040, -0.0337,  0.1142],\n",
      "          [-0.0899, -0.1128,  0.0365],\n",
      "          [-0.0461, -0.0003,  0.0687]],\n",
      "\n",
      "         [[-0.1148, -0.0447, -0.0065],\n",
      "          [-0.0760, -0.1035, -0.0375],\n",
      "          [ 0.0969,  0.0834, -0.1051]],\n",
      "\n",
      "         [[-0.0368, -0.0806, -0.0351],\n",
      "          [ 0.0470, -0.0125, -0.1134],\n",
      "          [ 0.0238,  0.1055,  0.0364]],\n",
      "\n",
      "         [[ 0.0025, -0.0234,  0.0306],\n",
      "          [ 0.0107,  0.0616,  0.1174],\n",
      "          [-0.0422, -0.0428,  0.0479]],\n",
      "\n",
      "         [[ 0.1005, -0.0937,  0.1121],\n",
      "          [-0.0859,  0.0375,  0.0325],\n",
      "          [-0.0449, -0.0618, -0.0445]]]])\n",
      "Shape: torch.Size([8, 8, 3, 3]), std: 0.0692, mean: -0.0023\n",
      "\n",
      "prediction.residual_layers.residual_layers.0.bn2.weight:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 1.0000\n",
      "\n",
      "prediction.residual_layers.residual_layers.0.bn2.bias:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([8]), std: 0.0000, mean: 0.0000\n",
      "\n",
      "prediction.critic.conv_layers.conv_layers.0.weight:\n",
      "tensor([[[[ 0.1116]],\n",
      "\n",
      "         [[-0.1228]],\n",
      "\n",
      "         [[-0.3188]],\n",
      "\n",
      "         [[-0.2621]],\n",
      "\n",
      "         [[-0.1269]],\n",
      "\n",
      "         [[ 0.1625]],\n",
      "\n",
      "         [[-0.2034]],\n",
      "\n",
      "         [[ 0.1819]]],\n",
      "\n",
      "\n",
      "        [[[-0.1710]],\n",
      "\n",
      "         [[-0.0256]],\n",
      "\n",
      "         [[ 0.3228]],\n",
      "\n",
      "         [[ 0.2763]],\n",
      "\n",
      "         [[-0.0661]],\n",
      "\n",
      "         [[ 0.2625]],\n",
      "\n",
      "         [[-0.2524]],\n",
      "\n",
      "         [[-0.2778]]],\n",
      "\n",
      "\n",
      "        [[[-0.0815]],\n",
      "\n",
      "         [[ 0.3016]],\n",
      "\n",
      "         [[ 0.2490]],\n",
      "\n",
      "         [[ 0.2044]],\n",
      "\n",
      "         [[ 0.1671]],\n",
      "\n",
      "         [[-0.0217]],\n",
      "\n",
      "         [[-0.1314]],\n",
      "\n",
      "         [[-0.2654]]],\n",
      "\n",
      "\n",
      "        [[[-0.1553]],\n",
      "\n",
      "         [[-0.1362]],\n",
      "\n",
      "         [[ 0.2422]],\n",
      "\n",
      "         [[-0.1122]],\n",
      "\n",
      "         [[ 0.2579]],\n",
      "\n",
      "         [[ 0.2061]],\n",
      "\n",
      "         [[-0.3399]],\n",
      "\n",
      "         [[ 0.1175]]],\n",
      "\n",
      "\n",
      "        [[[-0.0470]],\n",
      "\n",
      "         [[-0.0682]],\n",
      "\n",
      "         [[-0.2547]],\n",
      "\n",
      "         [[-0.2247]],\n",
      "\n",
      "         [[ 0.0094]],\n",
      "\n",
      "         [[-0.1415]],\n",
      "\n",
      "         [[-0.3532]],\n",
      "\n",
      "         [[-0.0627]]],\n",
      "\n",
      "\n",
      "        [[[-0.2223]],\n",
      "\n",
      "         [[-0.3460]],\n",
      "\n",
      "         [[ 0.0099]],\n",
      "\n",
      "         [[ 0.1568]],\n",
      "\n",
      "         [[-0.0882]],\n",
      "\n",
      "         [[ 0.0352]],\n",
      "\n",
      "         [[-0.0161]],\n",
      "\n",
      "         [[-0.0227]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2781]],\n",
      "\n",
      "         [[-0.1572]],\n",
      "\n",
      "         [[ 0.2410]],\n",
      "\n",
      "         [[-0.2574]],\n",
      "\n",
      "         [[-0.0318]],\n",
      "\n",
      "         [[-0.0773]],\n",
      "\n",
      "         [[ 0.2857]],\n",
      "\n",
      "         [[ 0.1091]]],\n",
      "\n",
      "\n",
      "        [[[-0.0478]],\n",
      "\n",
      "         [[-0.0215]],\n",
      "\n",
      "         [[ 0.0666]],\n",
      "\n",
      "         [[ 0.0954]],\n",
      "\n",
      "         [[-0.2362]],\n",
      "\n",
      "         [[-0.2942]],\n",
      "\n",
      "         [[ 0.1016]],\n",
      "\n",
      "         [[-0.2937]]],\n",
      "\n",
      "\n",
      "        [[[-0.1323]],\n",
      "\n",
      "         [[ 0.0582]],\n",
      "\n",
      "         [[-0.2153]],\n",
      "\n",
      "         [[-0.1514]],\n",
      "\n",
      "         [[-0.1611]],\n",
      "\n",
      "         [[-0.2156]],\n",
      "\n",
      "         [[ 0.1265]],\n",
      "\n",
      "         [[ 0.3445]]],\n",
      "\n",
      "\n",
      "        [[[-0.1747]],\n",
      "\n",
      "         [[-0.2578]],\n",
      "\n",
      "         [[-0.2825]],\n",
      "\n",
      "         [[-0.2191]],\n",
      "\n",
      "         [[-0.2856]],\n",
      "\n",
      "         [[-0.3054]],\n",
      "\n",
      "         [[-0.2884]],\n",
      "\n",
      "         [[-0.2497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3374]],\n",
      "\n",
      "         [[ 0.1725]],\n",
      "\n",
      "         [[ 0.0228]],\n",
      "\n",
      "         [[ 0.0146]],\n",
      "\n",
      "         [[ 0.2119]],\n",
      "\n",
      "         [[ 0.3090]],\n",
      "\n",
      "         [[ 0.0845]],\n",
      "\n",
      "         [[ 0.1806]]],\n",
      "\n",
      "\n",
      "        [[[-0.2713]],\n",
      "\n",
      "         [[-0.1292]],\n",
      "\n",
      "         [[-0.2563]],\n",
      "\n",
      "         [[-0.2014]],\n",
      "\n",
      "         [[-0.1152]],\n",
      "\n",
      "         [[ 0.3378]],\n",
      "\n",
      "         [[ 0.2022]],\n",
      "\n",
      "         [[ 0.3295]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2541]],\n",
      "\n",
      "         [[ 0.0805]],\n",
      "\n",
      "         [[-0.3151]],\n",
      "\n",
      "         [[ 0.1632]],\n",
      "\n",
      "         [[ 0.0327]],\n",
      "\n",
      "         [[ 0.2407]],\n",
      "\n",
      "         [[-0.2432]],\n",
      "\n",
      "         [[-0.1074]]],\n",
      "\n",
      "\n",
      "        [[[-0.3305]],\n",
      "\n",
      "         [[-0.2584]],\n",
      "\n",
      "         [[-0.1039]],\n",
      "\n",
      "         [[ 0.0654]],\n",
      "\n",
      "         [[-0.3503]],\n",
      "\n",
      "         [[ 0.0872]],\n",
      "\n",
      "         [[ 0.2770]],\n",
      "\n",
      "         [[ 0.1195]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976]],\n",
      "\n",
      "         [[ 0.3317]],\n",
      "\n",
      "         [[ 0.1324]],\n",
      "\n",
      "         [[-0.2170]],\n",
      "\n",
      "         [[-0.2994]],\n",
      "\n",
      "         [[-0.2327]],\n",
      "\n",
      "         [[ 0.1058]],\n",
      "\n",
      "         [[ 0.1328]]],\n",
      "\n",
      "\n",
      "        [[[-0.2185]],\n",
      "\n",
      "         [[ 0.2912]],\n",
      "\n",
      "         [[-0.0938]],\n",
      "\n",
      "         [[-0.1087]],\n",
      "\n",
      "         [[-0.1018]],\n",
      "\n",
      "         [[ 0.1365]],\n",
      "\n",
      "         [[ 0.1199]],\n",
      "\n",
      "         [[-0.1867]]],\n",
      "\n",
      "\n",
      "        [[[-0.0312]],\n",
      "\n",
      "         [[-0.1751]],\n",
      "\n",
      "         [[ 0.1049]],\n",
      "\n",
      "         [[-0.1897]],\n",
      "\n",
      "         [[ 0.0915]],\n",
      "\n",
      "         [[ 0.1837]],\n",
      "\n",
      "         [[-0.0314]],\n",
      "\n",
      "         [[ 0.3533]]],\n",
      "\n",
      "\n",
      "        [[[-0.2138]],\n",
      "\n",
      "         [[-0.2919]],\n",
      "\n",
      "         [[-0.2989]],\n",
      "\n",
      "         [[ 0.3536]],\n",
      "\n",
      "         [[ 0.1790]],\n",
      "\n",
      "         [[ 0.1266]],\n",
      "\n",
      "         [[ 0.0708]],\n",
      "\n",
      "         [[ 0.1943]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1891]],\n",
      "\n",
      "         [[-0.1592]],\n",
      "\n",
      "         [[ 0.1901]],\n",
      "\n",
      "         [[ 0.1917]],\n",
      "\n",
      "         [[-0.2221]],\n",
      "\n",
      "         [[-0.2442]],\n",
      "\n",
      "         [[ 0.1139]],\n",
      "\n",
      "         [[ 0.0715]]],\n",
      "\n",
      "\n",
      "        [[[-0.3127]],\n",
      "\n",
      "         [[-0.2503]],\n",
      "\n",
      "         [[-0.1962]],\n",
      "\n",
      "         [[ 0.1549]],\n",
      "\n",
      "         [[ 0.1549]],\n",
      "\n",
      "         [[-0.1290]],\n",
      "\n",
      "         [[ 0.1174]],\n",
      "\n",
      "         [[ 0.0554]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2063]],\n",
      "\n",
      "         [[-0.0588]],\n",
      "\n",
      "         [[ 0.3037]],\n",
      "\n",
      "         [[ 0.1512]],\n",
      "\n",
      "         [[ 0.2591]],\n",
      "\n",
      "         [[-0.2805]],\n",
      "\n",
      "         [[-0.2035]],\n",
      "\n",
      "         [[ 0.2798]]],\n",
      "\n",
      "\n",
      "        [[[-0.1582]],\n",
      "\n",
      "         [[ 0.1831]],\n",
      "\n",
      "         [[-0.0567]],\n",
      "\n",
      "         [[-0.2937]],\n",
      "\n",
      "         [[ 0.0629]],\n",
      "\n",
      "         [[ 0.0393]],\n",
      "\n",
      "         [[ 0.0353]],\n",
      "\n",
      "         [[-0.2122]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1669]],\n",
      "\n",
      "         [[ 0.2349]],\n",
      "\n",
      "         [[-0.3151]],\n",
      "\n",
      "         [[ 0.2438]],\n",
      "\n",
      "         [[-0.0365]],\n",
      "\n",
      "         [[ 0.1438]],\n",
      "\n",
      "         [[-0.1700]],\n",
      "\n",
      "         [[ 0.0653]]],\n",
      "\n",
      "\n",
      "        [[[-0.2834]],\n",
      "\n",
      "         [[-0.2434]],\n",
      "\n",
      "         [[-0.2738]],\n",
      "\n",
      "         [[ 0.3090]],\n",
      "\n",
      "         [[-0.1306]],\n",
      "\n",
      "         [[ 0.1057]],\n",
      "\n",
      "         [[ 0.2371]],\n",
      "\n",
      "         [[ 0.1642]]]])\n",
      "Shape: torch.Size([24, 8, 1, 1]), std: 0.2045, mean: -0.0148\n",
      "\n",
      "prediction.critic.conv_layers.conv_layers.0.bias:\n",
      "tensor([ 0.1277,  0.2449,  0.2939, -0.0969, -0.3250, -0.2636,  0.2869, -0.1508,\n",
      "        -0.0763,  0.1123,  0.1658,  0.2576,  0.2669, -0.1618,  0.0501, -0.2215,\n",
      "         0.2789,  0.1561,  0.3474, -0.3317, -0.0718, -0.3452,  0.1804, -0.2746])\n",
      "Shape: torch.Size([24]), std: 0.2350, mean: 0.0188\n",
      "\n",
      "prediction.critic.value.layer.weight:\n",
      "tensor([[-0.0416,  0.0222, -0.0217,  0.0035, -0.0516,  0.0226, -0.0203,  0.0662,\n",
      "          0.0386,  0.0476, -0.0276, -0.0668,  0.0620,  0.0039,  0.0502, -0.0667,\n",
      "          0.0104, -0.0342, -0.0070, -0.0560, -0.0175,  0.0106,  0.0307, -0.0117,\n",
      "         -0.0329,  0.0522, -0.0557,  0.0030,  0.0032,  0.0031, -0.0177, -0.0227,\n",
      "         -0.0105,  0.0277, -0.0154,  0.0559,  0.0178,  0.0395,  0.0092,  0.0292,\n",
      "          0.0253, -0.0019, -0.0327,  0.0259, -0.0176,  0.0167,  0.0477, -0.0081,\n",
      "          0.0306,  0.0625, -0.0280,  0.0500, -0.0104,  0.0109, -0.0025, -0.0610,\n",
      "         -0.0253, -0.0514, -0.0558,  0.0488,  0.0472, -0.0466,  0.0643,  0.0388,\n",
      "          0.0344, -0.0005,  0.0178,  0.0413, -0.0354,  0.0636, -0.0156, -0.0543,\n",
      "          0.0342, -0.0211, -0.0170,  0.0284,  0.0667, -0.0312,  0.0334, -0.0403,\n",
      "         -0.0250, -0.0453,  0.0425,  0.0203, -0.0449,  0.0538,  0.0432,  0.0006,\n",
      "         -0.0557,  0.0519,  0.0281,  0.0651,  0.0370,  0.0519,  0.0189,  0.0674,\n",
      "         -0.0045,  0.0645,  0.0651,  0.0151,  0.0583,  0.0004,  0.0360, -0.0030,\n",
      "         -0.0265, -0.0150,  0.0035,  0.0346, -0.0592,  0.0606,  0.0059, -0.0508,\n",
      "         -0.0524, -0.0353, -0.0038, -0.0411, -0.0304,  0.0290,  0.0675, -0.0369,\n",
      "          0.0260,  0.0261,  0.0579,  0.0664,  0.0486,  0.0278,  0.0004,  0.0009,\n",
      "         -0.0554, -0.0020, -0.0448,  0.0087, -0.0558,  0.0198,  0.0383, -0.0673,\n",
      "          0.0656,  0.0119, -0.0494, -0.0138, -0.0471,  0.0560, -0.0482, -0.0199,\n",
      "          0.0183, -0.0142, -0.0383,  0.0385,  0.0220,  0.0131,  0.0077, -0.0436,\n",
      "          0.0307, -0.0216, -0.0572,  0.0448, -0.0505, -0.0108,  0.0559,  0.0132,\n",
      "         -0.0260,  0.0283, -0.0236, -0.0335, -0.0054, -0.0115,  0.0545, -0.0274,\n",
      "         -0.0268, -0.0393, -0.0625,  0.0205, -0.0381,  0.0231,  0.0308, -0.0654,\n",
      "          0.0489, -0.0159,  0.0280,  0.0375,  0.0288,  0.0107, -0.0220,  0.0532,\n",
      "         -0.0037, -0.0196,  0.0270,  0.0553,  0.0645, -0.0357, -0.0189, -0.0159,\n",
      "         -0.0451,  0.0228,  0.0456,  0.0060, -0.0581,  0.0139,  0.0545,  0.0144,\n",
      "         -0.0160,  0.0422, -0.0510, -0.0265,  0.0612, -0.0637, -0.0436,  0.0509,\n",
      "          0.0612, -0.0269, -0.0347,  0.0398,  0.0023,  0.0274,  0.0463, -0.0288]])\n",
      "Shape: torch.Size([1, 216]), std: 0.0385, mean: 0.0038\n",
      "\n",
      "prediction.critic.value.layer.bias:\n",
      "tensor([0.0483])\n",
      "Shape: torch.Size([1]), std: nan, mean: 0.0483\n",
      "\n",
      "prediction.actor.conv_layers.conv_layers.0.weight:\n",
      "tensor([[[[-2.2119e-01]],\n",
      "\n",
      "         [[ 6.0282e-02]],\n",
      "\n",
      "         [[ 1.7843e-01]],\n",
      "\n",
      "         [[-2.1950e-01]],\n",
      "\n",
      "         [[ 1.7306e-01]],\n",
      "\n",
      "         [[-3.4821e-01]],\n",
      "\n",
      "         [[ 2.5445e-01]],\n",
      "\n",
      "         [[-6.2654e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.3085e-01]],\n",
      "\n",
      "         [[-2.3948e-01]],\n",
      "\n",
      "         [[-7.8525e-02]],\n",
      "\n",
      "         [[-1.9921e-01]],\n",
      "\n",
      "         [[ 2.5992e-01]],\n",
      "\n",
      "         [[-2.4426e-01]],\n",
      "\n",
      "         [[ 3.4654e-01]],\n",
      "\n",
      "         [[ 1.4801e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.4522e-01]],\n",
      "\n",
      "         [[-6.5029e-02]],\n",
      "\n",
      "         [[ 1.3132e-01]],\n",
      "\n",
      "         [[-7.5403e-02]],\n",
      "\n",
      "         [[ 1.7797e-01]],\n",
      "\n",
      "         [[-1.5931e-01]],\n",
      "\n",
      "         [[-3.3237e-01]],\n",
      "\n",
      "         [[ 3.1811e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6015e-01]],\n",
      "\n",
      "         [[-1.8790e-01]],\n",
      "\n",
      "         [[ 1.6648e-01]],\n",
      "\n",
      "         [[-2.9404e-01]],\n",
      "\n",
      "         [[ 3.0787e-01]],\n",
      "\n",
      "         [[-1.4940e-01]],\n",
      "\n",
      "         [[-3.2555e-01]],\n",
      "\n",
      "         [[-1.2771e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6901e-02]],\n",
      "\n",
      "         [[ 1.3223e-01]],\n",
      "\n",
      "         [[-1.1269e-01]],\n",
      "\n",
      "         [[-1.4026e-01]],\n",
      "\n",
      "         [[ 3.4859e-01]],\n",
      "\n",
      "         [[-2.0030e-01]],\n",
      "\n",
      "         [[ 2.5810e-01]],\n",
      "\n",
      "         [[-3.4364e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6595e-02]],\n",
      "\n",
      "         [[ 3.2801e-01]],\n",
      "\n",
      "         [[-2.8276e-04]],\n",
      "\n",
      "         [[ 3.3742e-01]],\n",
      "\n",
      "         [[ 1.3855e-01]],\n",
      "\n",
      "         [[-2.9637e-01]],\n",
      "\n",
      "         [[ 3.0203e-01]],\n",
      "\n",
      "         [[ 2.1930e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0110e-01]],\n",
      "\n",
      "         [[ 7.4090e-02]],\n",
      "\n",
      "         [[ 5.9870e-02]],\n",
      "\n",
      "         [[-4.5052e-02]],\n",
      "\n",
      "         [[-6.3438e-02]],\n",
      "\n",
      "         [[-2.0160e-01]],\n",
      "\n",
      "         [[-2.8696e-01]],\n",
      "\n",
      "         [[ 4.1815e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.8966e-01]],\n",
      "\n",
      "         [[-3.2311e-01]],\n",
      "\n",
      "         [[ 1.6565e-01]],\n",
      "\n",
      "         [[-1.4860e-01]],\n",
      "\n",
      "         [[-1.1743e-01]],\n",
      "\n",
      "         [[-3.0911e-01]],\n",
      "\n",
      "         [[ 1.4085e-01]],\n",
      "\n",
      "         [[-9.2262e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0645e-01]],\n",
      "\n",
      "         [[-2.8596e-01]],\n",
      "\n",
      "         [[-1.6592e-01]],\n",
      "\n",
      "         [[ 4.4273e-02]],\n",
      "\n",
      "         [[ 2.5610e-01]],\n",
      "\n",
      "         [[-1.4298e-01]],\n",
      "\n",
      "         [[ 8.4784e-02]],\n",
      "\n",
      "         [[ 2.2530e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7272e-03]],\n",
      "\n",
      "         [[-7.8514e-02]],\n",
      "\n",
      "         [[ 7.1102e-04]],\n",
      "\n",
      "         [[ 3.0393e-02]],\n",
      "\n",
      "         [[ 1.4760e-01]],\n",
      "\n",
      "         [[-1.1435e-02]],\n",
      "\n",
      "         [[-1.9289e-01]],\n",
      "\n",
      "         [[ 3.2870e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4646e-02]],\n",
      "\n",
      "         [[ 1.8089e-01]],\n",
      "\n",
      "         [[ 6.4504e-03]],\n",
      "\n",
      "         [[-1.5866e-01]],\n",
      "\n",
      "         [[-1.7379e-01]],\n",
      "\n",
      "         [[ 6.3240e-02]],\n",
      "\n",
      "         [[-2.4295e-01]],\n",
      "\n",
      "         [[-9.6454e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.4106e-02]],\n",
      "\n",
      "         [[-3.1491e-01]],\n",
      "\n",
      "         [[ 2.5773e-01]],\n",
      "\n",
      "         [[-3.4508e-02]],\n",
      "\n",
      "         [[ 1.9832e-01]],\n",
      "\n",
      "         [[ 1.3829e-01]],\n",
      "\n",
      "         [[-1.2235e-02]],\n",
      "\n",
      "         [[ 1.2607e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8556e-01]],\n",
      "\n",
      "         [[ 7.0476e-02]],\n",
      "\n",
      "         [[-3.0051e-01]],\n",
      "\n",
      "         [[-8.4320e-02]],\n",
      "\n",
      "         [[-2.9319e-01]],\n",
      "\n",
      "         [[-1.0682e-01]],\n",
      "\n",
      "         [[-4.7189e-02]],\n",
      "\n",
      "         [[ 1.9301e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6668e-02]],\n",
      "\n",
      "         [[-5.4313e-02]],\n",
      "\n",
      "         [[ 3.4565e-01]],\n",
      "\n",
      "         [[-1.1428e-01]],\n",
      "\n",
      "         [[ 1.6682e-01]],\n",
      "\n",
      "         [[ 3.3251e-01]],\n",
      "\n",
      "         [[ 3.1631e-01]],\n",
      "\n",
      "         [[ 1.6629e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2233e-01]],\n",
      "\n",
      "         [[ 2.7916e-01]],\n",
      "\n",
      "         [[ 2.5549e-02]],\n",
      "\n",
      "         [[ 3.5316e-01]],\n",
      "\n",
      "         [[-3.3096e-01]],\n",
      "\n",
      "         [[ 6.7704e-03]],\n",
      "\n",
      "         [[-1.6330e-01]],\n",
      "\n",
      "         [[ 4.7759e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.1909e-02]],\n",
      "\n",
      "         [[ 1.6141e-01]],\n",
      "\n",
      "         [[-2.6343e-01]],\n",
      "\n",
      "         [[ 2.5034e-01]],\n",
      "\n",
      "         [[ 3.1094e-01]],\n",
      "\n",
      "         [[-2.7748e-01]],\n",
      "\n",
      "         [[-3.1106e-01]],\n",
      "\n",
      "         [[-1.5675e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8045e-02]],\n",
      "\n",
      "         [[ 1.4070e-01]],\n",
      "\n",
      "         [[ 1.2513e-01]],\n",
      "\n",
      "         [[-5.3961e-02]],\n",
      "\n",
      "         [[-3.0879e-01]],\n",
      "\n",
      "         [[-3.5128e-01]],\n",
      "\n",
      "         [[ 3.1579e-01]],\n",
      "\n",
      "         [[ 2.4754e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.7212e-02]],\n",
      "\n",
      "         [[-1.5758e-01]],\n",
      "\n",
      "         [[ 1.5316e-02]],\n",
      "\n",
      "         [[ 1.9460e-01]],\n",
      "\n",
      "         [[ 2.5605e-01]],\n",
      "\n",
      "         [[-1.2696e-01]],\n",
      "\n",
      "         [[-8.0544e-02]],\n",
      "\n",
      "         [[ 2.1265e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1289e-01]],\n",
      "\n",
      "         [[ 4.5341e-02]],\n",
      "\n",
      "         [[-8.6734e-02]],\n",
      "\n",
      "         [[ 2.7423e-01]],\n",
      "\n",
      "         [[ 3.0991e-01]],\n",
      "\n",
      "         [[ 2.7417e-01]],\n",
      "\n",
      "         [[ 6.1739e-02]],\n",
      "\n",
      "         [[-1.5577e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.7625e-02]],\n",
      "\n",
      "         [[-2.7575e-02]],\n",
      "\n",
      "         [[-2.1757e-01]],\n",
      "\n",
      "         [[-8.0138e-02]],\n",
      "\n",
      "         [[-1.6963e-01]],\n",
      "\n",
      "         [[-1.8865e-01]],\n",
      "\n",
      "         [[ 7.1636e-02]],\n",
      "\n",
      "         [[-3.4120e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3440e-02]],\n",
      "\n",
      "         [[ 1.3483e-01]],\n",
      "\n",
      "         [[ 2.4226e-01]],\n",
      "\n",
      "         [[-2.5527e-02]],\n",
      "\n",
      "         [[-2.9831e-01]],\n",
      "\n",
      "         [[-2.1929e-01]],\n",
      "\n",
      "         [[-1.9911e-01]],\n",
      "\n",
      "         [[-1.8836e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.8274e-01]],\n",
      "\n",
      "         [[-2.0775e-01]],\n",
      "\n",
      "         [[-7.2767e-02]],\n",
      "\n",
      "         [[ 2.1384e-01]],\n",
      "\n",
      "         [[-2.6578e-01]],\n",
      "\n",
      "         [[ 1.2102e-01]],\n",
      "\n",
      "         [[-1.1239e-01]],\n",
      "\n",
      "         [[ 3.4834e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6472e-02]],\n",
      "\n",
      "         [[-1.7878e-01]],\n",
      "\n",
      "         [[ 2.4806e-01]],\n",
      "\n",
      "         [[-3.1899e-01]],\n",
      "\n",
      "         [[-1.2195e-01]],\n",
      "\n",
      "         [[-1.2793e-01]],\n",
      "\n",
      "         [[ 1.7027e-01]],\n",
      "\n",
      "         [[ 3.4687e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7466e-01]],\n",
      "\n",
      "         [[-2.5270e-01]],\n",
      "\n",
      "         [[-2.3122e-01]],\n",
      "\n",
      "         [[-1.3434e-01]],\n",
      "\n",
      "         [[-3.4523e-01]],\n",
      "\n",
      "         [[ 3.2079e-01]],\n",
      "\n",
      "         [[-3.2324e-01]],\n",
      "\n",
      "         [[ 3.5320e-01]]]])\n",
      "Shape: torch.Size([24, 8, 1, 1]), std: 0.2061, mean: -0.0068\n",
      "\n",
      "prediction.actor.conv_layers.conv_layers.0.bias:\n",
      "tensor([ 0.1109,  0.3486, -0.2909,  0.2681, -0.1267,  0.2546,  0.3064, -0.1505,\n",
      "         0.2689, -0.2283,  0.3244,  0.0313,  0.1870, -0.0923,  0.3377,  0.2060,\n",
      "        -0.0019,  0.0348, -0.0972, -0.0493, -0.3389,  0.2880,  0.1141,  0.1215])\n",
      "Shape: torch.Size([24]), std: 0.2104, mean: 0.0761\n",
      "\n",
      "prediction.actor.actions.layer.weight:\n",
      "tensor([[-0.0445,  0.0515, -0.0149,  ..., -0.0575,  0.0606,  0.0078],\n",
      "        [ 0.0393, -0.0106,  0.0678,  ..., -0.0068,  0.0341,  0.0463],\n",
      "        [-0.0348,  0.0301,  0.0554,  ...,  0.0066, -0.0506,  0.0275],\n",
      "        ...,\n",
      "        [-0.0066,  0.0463, -0.0118,  ..., -0.0307, -0.0248, -0.0352],\n",
      "        [-0.0328,  0.0595,  0.0570,  ...,  0.0299,  0.0035,  0.0211],\n",
      "        [ 0.0250, -0.0264, -0.0555,  ...,  0.0174, -0.0121,  0.0343]])\n",
      "Shape: torch.Size([9, 216]), std: 0.0393, mean: 0.0004\n",
      "\n",
      "prediction.actor.actions.layer.bias:\n",
      "tensor([-0.0228,  0.0395,  0.0339, -0.0004,  0.0322, -0.0556, -0.0155,  0.0415,\n",
      "        -0.0647])\n",
      "Shape: torch.Size([9]), std: 0.0411, mean: -0.0013\n",
      "\n",
      "Warning: for board games it is recommnded to have n_step >= game length\n",
      "Max size: 10000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Resuming training at step 1 / 22000\n",
      "Resuming training at step 1 / 22000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:103: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  f\"Shape: {param.shape}, std: {param.std():.4f}, mean: {param.mean():.4f}\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker 1] Starting self-play...\n",
      "[Worker 0] Starting self-play...\n",
      "Buffer size: 8\n",
      "Buffer size: 17\n",
      "Buffer size: 26\n",
      "Buffer size: 35\n",
      "Buffer size: 44\n",
      "Buffer size: 52\n",
      "Buffer size: 61\n",
      "Buffer size: 70\n",
      "Buffer size: 78\n",
      "Buffer size: 87\n",
      "Buffer size: 95\n",
      "Buffer size: 103\n",
      "Buffer size: 112\n",
      "Buffer size: 121\n",
      "Buffer size: 130\n",
      "Buffer size: 138\n",
      "Buffer size: 145\n",
      "Buffer size: 153\n",
      "Buffer size: 161\n",
      "Buffer size: 169\n",
      "Buffer size: 177\n",
      "Buffer size: 186\n",
      "Buffer size: 195\n",
      "Buffer size: 204\n",
      "Buffer size: 211\n",
      "Buffer size: 218\n",
      "Buffer size: 225\n",
      "Buffer size: 234\n",
      "Buffer size: 241\n",
      "Buffer size: 250\n",
      "Buffer size: 257\n",
      "Buffer size: 265\n",
      "Buffer size: 272\n",
      "Buffer size: 277\n",
      "Buffer size: 285\n",
      "Buffer size: 292\n",
      "Buffer size: 298\n",
      "Buffer size: 305\n",
      "Buffer size: 314\n",
      "Buffer size: 321\n",
      "Buffer size: 329\n",
      "Buffer size: 336\n",
      "Buffer size: 345\n",
      "Buffer size: 354\n",
      "Buffer size: 361\n",
      "Buffer size: 368\n",
      "Buffer size: 373\n",
      "Buffer size: 381\n",
      "Buffer size: 390\n",
      "Buffer size: 398\n",
      "Buffer size: 407\n",
      "Buffer size: 415\n",
      "Buffer size: 420\n",
      "Buffer size: 426\n",
      "Buffer size: 432\n",
      "Buffer size: 440\n",
      "Buffer size: 449\n",
      "Buffer size: 456\n",
      "Buffer size: 464\n",
      "Buffer size: 472\n",
      "Buffer size: 481\n",
      "Buffer size: 488\n",
      "Buffer size: 494\n",
      "Buffer size: 503\n",
      "Buffer size: 510\n",
      "Buffer size: 516\n",
      "Buffer size: 525\n",
      "Buffer size: 534\n",
      "Buffer size: 543\n",
      "Buffer size: 550\n",
      "Buffer size: 559\n",
      "Buffer size: 568\n",
      "Buffer size: 577\n",
      "Buffer size: 585\n",
      "Buffer size: 590\n",
      "Buffer size: 598\n",
      "Buffer size: 604\n",
      "Buffer size: 612\n",
      "Buffer size: 621\n",
      "Buffer size: 628\n",
      "Buffer size: 637\n",
      "Buffer size: 644\n",
      "Buffer size: 652\n",
      "Buffer size: 661\n",
      "Buffer size: 670\n",
      "Buffer size: 679\n",
      "Buffer size: 688\n",
      "Buffer size: 697\n",
      "Buffer size: 711\n",
      "Buffer size: 711\n",
      "Buffer size: 716\n",
      "Buffer size: 725\n",
      "Buffer size: 734\n",
      "Buffer size: 742\n",
      "Buffer size: 747\n",
      "Buffer size: 756\n",
      "Buffer size: 764\n",
      "Buffer size: 772\n",
      "Buffer size: 781\n",
      "Buffer size: 788\n",
      "Buffer size: 795\n",
      "Buffer size: 803\n",
      "Buffer size: 810\n",
      "Buffer size: 818\n",
      "Buffer size: 827\n",
      "Buffer size: 834\n",
      "Buffer size: 843\n",
      "Buffer size: 851\n",
      "Buffer size: 857\n",
      "Buffer size: 866\n",
      "Buffer size: 874\n",
      "Buffer size: 882\n",
      "Buffer size: 890\n",
      "Buffer size: 898\n",
      "Buffer size: 907\n",
      "Buffer size: 915\n",
      "Buffer size: 921\n",
      "Buffer size: 930\n",
      "Buffer size: 939\n",
      "Buffer size: 947\n",
      "Buffer size: 956\n",
      "Buffer size: 965\n",
      "Buffer size: 974\n",
      "Buffer size: 979\n",
      "Buffer size: 986\n",
      "Buffer size: 994\n",
      "unroll step 0\n",
      "predicted value tensor([0.3217], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0810, 0.1259, 0.0729, 0.1090, 0.0967, 0.1422, 0.0934, 0.1820, 0.0970],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.4400, 0.0000, 0.0000, 0.0000, 0.1600, 0.1200, 0.2400, 0.0400])\n",
      "sample losses tensor([0.1035], grad_fn=<MulBackward0>) tensor(0.) tensor(2.0108, grad_fn=<NegBackward0>)\n",
      "unroll step 1\n",
      "predicted value tensor([0.1906], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.3312], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1058, 0.1418, 0.0868, 0.0879, 0.0828, 0.1478, 0.0930, 0.1440, 0.1100],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.3200, 0.0000, 0.0000, 0.0000, 0.0000, 0.1200, 0.1600, 0.2800, 0.1200])\n",
      "sample losses tensor([0.0363], grad_fn=<MulBackward0>) tensor([0.1097], grad_fn=<PowBackward0>) tensor(2.1355, grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "predicted value tensor([0.0692], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.2612], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1357, 0.0931, 0.0884, 0.0988, 0.0714, 0.1243, 0.0941, 0.1782, 0.1160],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.1200, 0.1600, 0.1600, 0.1600, 0.2800])\n",
      "sample losses tensor([0.0048], grad_fn=<MulBackward0>) tensor([0.0682], grad_fn=<PowBackward0>) tensor(2.1854, grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "predicted value tensor([0.2671], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.0241], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0917, 0.0842, 0.0860, 0.1333, 0.1003, 0.1242, 0.1016, 0.1472, 0.1315],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.1600, 0.1600, 0.2000, 0.2000, 0.2800, 0.0000])\n",
      "sample losses tensor([0.0713], grad_fn=<MulBackward0>) tensor([0.0006], grad_fn=<PowBackward0>) tensor(2.1013, grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "predicted value tensor([0.2819], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.2199], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0947, 0.1102, 0.0962, 0.0939, 0.1023, 0.1375, 0.1127, 0.1499, 0.1025],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.2000, 0.2800, 0.2400, 0.2800, 0.0000, 0.0000])\n",
      "sample losses tensor([0.0794], grad_fn=<MulBackward0>) tensor([0.0484], grad_fn=<PowBackward0>) tensor(2.1987, grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "predicted value tensor([0.3290], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.1436], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1311, 0.1175, 0.0737, 0.1284, 0.0666, 0.0950, 0.1144, 0.1732, 0.1000],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.3600, 0.2800, 0.3600, 0.0000, 0.0000, 0.0000])\n",
      "sample losses tensor([0.1082], grad_fn=<MulBackward0>) tensor([0.0206], grad_fn=<PowBackward0>) tensor(2.3448, grad_fn=<NegBackward0>)\n",
      "Losses 3.9208349286127486 13.29139730334282 0.5628046383818628 17.775035858154297\n",
      "Training Step: 1\n",
      "Losses 3.903402585481672 13.433188408613205 0.4257703170160312 17.762359619140625\n",
      "Training Step: 2\n",
      "Losses 4.394953867784352 13.263445347547531 0.6690310612029862 18.327430725097656\n",
      "Training Step: 3\n",
      "Losses 3.7847404975055383 13.335856258869171 0.4742133964170421 17.594810485839844\n",
      "Training Step: 4\n",
      "Losses 3.2178352185583208 13.3824802339077 0.8715530633053277 17.47186851501465\n",
      "Training Step: 5\n",
      "Losses 5.335026166809257 13.322461038827896 0.5135777772593428 19.171062469482422\n",
      "Training Step: 6\n",
      "Losses 3.1971478618042966 13.398524463176727 0.4864419788315948 17.082115173339844\n",
      "Training Step: 7\n",
      "Losses 3.252745488192886 13.344439417123795 0.452148506666461 17.049333572387695\n",
      "Training Step: 8\n",
      "Losses 4.493272922682081 13.16600376367569 0.5126107182104533 18.171890258789062\n",
      "Training Step: 9\n",
      "Losses 3.8881878436077386 13.441212385892868 0.5843250944749343 17.913724899291992\n",
      "Training Step: 10\n",
      "Losses 4.329727572098818 13.35068878531456 0.7973229173117033 18.477739334106445\n",
      "Training Step: 11\n",
      "Losses 5.433336510322988 13.225212395191193 0.575313026328331 19.23386001586914\n",
      "Training Step: 12\n",
      "Losses 3.7495632955001383 13.385311305522919 0.5914405212195675 17.726320266723633\n",
      "Training Step: 13\n",
      "Losses 4.449336189194582 13.425571650266647 0.7973385830819097 18.672245025634766\n",
      "Training Step: 14\n",
      "Losses 5.1355051181672025 13.55927687883377 0.5154470935677864 19.210229873657227\n",
      "Training Step: 15\n",
      "Losses 4.187887286418118 13.210286512970924 0.43515392450717627 17.833328247070312\n",
      "Training Step: 16\n",
      "Losses 3.548524180718232 13.323421061038971 0.37630668644123944 17.248249053955078\n",
      "Training Step: 17\n",
      "Losses 3.9717611482919892 13.376404702663422 0.6094216830533696 17.95758819580078\n",
      "Training Step: 18\n",
      "Losses 5.132060920623189 13.32754734158516 0.5656721138784633 19.02528190612793\n",
      "Training Step: 19\n",
      "Losses 3.521116323105673 13.554816782474518 0.7294812096967007 17.80541229248047\n",
      "Training Step: 20\n",
      "Losses 5.460747266188264 13.42926424741745 0.20568879596862644 19.095703125\n",
      "Training Step: 21\n",
      "Losses 4.069468943640459 13.37318879365921 0.6056020045796231 18.048263549804688\n",
      "Training Step: 22\n",
      "Losses 2.4909205077892693 13.462819278240204 0.5400370000573389 16.4937744140625\n",
      "Training Step: 23\n",
      "Losses 5.211970549367834 13.202445536851883 0.3627492555635854 18.77716636657715\n",
      "Training Step: 24\n",
      "Losses 4.436457432806492 13.413003593683243 0.33216547706547317 18.181629180908203\n",
      "Training Step: 25\n",
      "Losses 2.774545119920731 13.55552726984024 0.28786980763847225 16.617942810058594\n",
      "Training Step: 26\n",
      "Losses 3.0814425438620674 13.344960659742355 0.7151573651673289 17.141563415527344\n",
      "Training Step: 27\n",
      "Losses 4.479692721732135 13.294872969388962 0.5495233920362922 18.32408905029297\n",
      "Training Step: 28\n",
      "Losses 3.406637906708056 13.191540241241455 0.5958430563696311 17.19402503967285\n",
      "Training Step: 29\n",
      "Losses 2.1061895634462555 13.290878981351852 0.5323059764274376 15.929375648498535\n",
      "Training Step: 30\n",
      "Losses 2.5171147257733537 13.341912865638733 0.4157267804416733 16.274751663208008\n",
      "Training Step: 31\n",
      "Losses 3.165257302727696 13.312219828367233 0.41179930659494346 16.88927459716797\n",
      "Training Step: 32\n",
      "Losses 4.274672797151879 13.318343341350555 0.39456541336039663 17.98758316040039\n",
      "Training Step: 33\n",
      "Losses 4.303948809629219 13.231507122516632 0.351439670781474 17.88689613342285\n",
      "Training Step: 34\n",
      "Losses 2.9345788205610006 13.346294581890106 0.474026528118884 16.754899978637695\n",
      "Training Step: 35\n",
      "Losses 3.5265196065520286 13.43457105755806 0.7877366787636078 17.748828887939453\n",
      "Training Step: 36\n",
      "Losses 3.3922449484216486 13.232565104961395 0.5716980763681931 17.19650650024414\n",
      "Training Step: 37\n",
      "Losses 3.488297167194105 13.400880694389343 0.5926052277936833 17.481782913208008\n",
      "Training Step: 38\n",
      "Losses 3.904979038752799 13.385304540395737 0.5510038243952451 17.84128761291504\n",
      "Training Step: 39\n",
      "Losses 2.0820481133177964 13.142092257738113 0.34436224824639794 15.568503379821777\n",
      "Training Step: 40\n",
      "Losses 2.3719670943028177 13.315391629934311 0.7648959008893144 16.452253341674805\n",
      "Training Step: 41\n",
      "Losses 4.136496463714494 13.387670487165451 0.6541753426425769 18.178342819213867\n",
      "Training Step: 42\n",
      "Losses 4.007630390085978 13.310931205749512 0.5596365650127382 17.878198623657227\n",
      "Training Step: 43\n",
      "Losses 5.157136508400072 13.30570051074028 0.6791864381498698 19.142024993896484\n",
      "Training Step: 44\n",
      "Losses 2.8158184479279953 13.461496263742447 0.5005347666083253 16.777851104736328\n",
      "Training Step: 45\n",
      "Losses 2.104357782109446 13.286000832915306 0.6841432625024026 16.07449722290039\n",
      "Training Step: 46\n",
      "Losses 3.2911299257611972 13.37195336818695 0.6744878981517104 17.337570190429688\n",
      "Training Step: 47\n",
      "Losses 3.7762717459490887 13.283337712287903 0.43634238760819244 17.495952606201172\n",
      "Training Step: 48\n",
      "Losses 2.863656116983833 13.334723815321922 0.8121076586903655 17.010486602783203\n",
      "Training Step: 49\n",
      "Losses 2.703649089230261 13.305468082427979 0.31874266463250933 16.327861785888672\n",
      "Training Step: 50\n",
      "Losses 2.6105881293069615 13.195226848125458 0.22619917319934757 16.032014846801758\n",
      "Training Step: 51\n",
      "Losses 2.8236354390296583 13.363019675016403 0.5103623644849904 16.697017669677734\n",
      "Training Step: 52\n",
      "Losses 3.3784657138789953 13.291060149669647 0.6206885818791079 17.29021453857422\n",
      "Training Step: 53\n",
      "Losses 2.011137192567503 13.296246647834778 0.2806635871912704 15.58804702758789\n",
      "Training Step: 54\n",
      "Losses 3.05236539942689 13.322839140892029 0.5330807294600568 16.90828514099121\n",
      "Training Step: 55\n",
      "Losses 3.4360588026343066 13.294996857643127 0.8280589854572001 17.559114456176758\n",
      "Training Step: 56\n",
      "Losses 4.415059038709842 13.292264938354492 0.5662470753595699 18.27357292175293\n",
      "Training Step: 57\n",
      "Losses 4.565465288865312 13.191115736961365 0.729253696198839 18.4858341217041\n",
      "Training Step: 58\n",
      "Losses 4.745985369724394 13.232209086418152 0.5454192391771358 18.52361488342285\n",
      "Training Step: 59\n",
      "Losses 5.5941342602309305 13.320061087608337 0.46225403672121956 19.37645149230957\n",
      "Training Step: 60\n",
      "Losses 3.357323279893535 13.18364953994751 0.400076156481191 16.941049575805664\n",
      "Training Step: 61\n",
      "Losses 4.461004643793785 13.327716320753098 0.4357149618998619 18.224437713623047\n",
      "Training Step: 62\n",
      "Losses 3.981472708263027 13.292837530374527 0.4008824092828718 17.675195693969727\n",
      "Training Step: 63\n",
      "Losses 2.914231742033735 13.191643297672272 0.46241609051503474 16.56829261779785\n",
      "Training Step: 64\n",
      "Losses 3.0484258306187257 13.386108785867691 0.5462802042438852 16.98081398010254\n",
      "Training Step: 65\n",
      "Losses 1.9882472481661182 13.269688069820404 0.37692864219934563 15.634866714477539\n",
      "Training Step: 66\n",
      "Losses 3.6242902564408723 13.284817606210709 0.7373145910511951 17.646421432495117\n",
      "Training Step: 67\n",
      "Losses 3.428023203165031 13.294684156775475 0.559198607669714 17.281906127929688\n",
      "Training Step: 68\n",
      "Losses 4.16223741092881 13.392147898674011 0.6873345002222777 18.241722106933594\n",
      "Training Step: 69\n",
      "Losses 4.431682074161586 13.399692863225937 0.5505773223142114 18.381954193115234\n",
      "Training Step: 70\n",
      "Losses 3.439954404229411 13.467218577861786 0.6108500989183501 17.518022537231445\n",
      "Training Step: 71\n",
      "Losses 3.6757347638417315 13.343845337629318 0.6414025964659231 17.660980224609375\n",
      "Training Step: 72\n",
      "Losses 3.6528461126210345 13.333743155002594 0.2640751645230921 17.250659942626953\n",
      "Training Step: 73\n",
      "Losses 5.043657162832915 13.396647095680237 0.42299179553629074 18.863300323486328\n",
      "Training Step: 74\n",
      "Losses 2.843337777058423 13.390284210443497 0.45531737354622237 16.688940048217773\n",
      "Training Step: 75\n",
      "Losses 5.183635716587787 13.339836031198502 0.3139222606241674 18.837392807006836\n",
      "Training Step: 76\n",
      "Losses 3.364981475715581 13.196685522794724 0.4669231144745254 17.028589248657227\n",
      "Training Step: 77\n",
      "Losses 3.3526389727485366 13.353635460138321 0.5361017049890506 17.24237823486328\n",
      "Training Step: 78\n",
      "Losses 4.235042611351673 13.263972163200378 0.3733482856614234 17.87236213684082\n",
      "Training Step: 79\n",
      "Losses 3.912759561679991 13.285186350345612 0.7924737248841893 17.990419387817383\n",
      "Training Step: 80\n",
      "Losses 4.311579267290654 13.32347771525383 0.7252107064050506 18.360273361206055\n",
      "Training Step: 81\n",
      "Losses 3.142292129810812 13.224469155073166 0.33561761513260535 16.702377319335938\n",
      "Training Step: 82\n",
      "Losses 2.6782311965926056 13.367958158254623 0.6070718467225129 16.65326499938965\n",
      "Training Step: 83\n",
      "Losses 4.43631091675752 13.232494682073593 0.5942427228134974 18.263050079345703\n",
      "Training Step: 84\n",
      "Losses 2.357168314421642 13.212622284889221 0.34894255954494113 15.918733596801758\n",
      "Training Step: 85\n",
      "Losses 3.0354744284668413 13.524478673934937 0.5904717559992605 17.150423049926758\n",
      "Training Step: 86\n",
      "Losses 3.237917680827252 13.258291155099869 0.33061714098317907 16.826826095581055\n",
      "Training Step: 87\n",
      "Losses 3.308683794681201 13.212674781680107 0.5756576837643479 17.097015380859375\n",
      "Training Step: 88\n",
      "Losses 2.5327780395073205 13.230358451604843 0.48524385107873513 16.248380661010742\n",
      "Training Step: 89\n",
      "Losses 1.8346407162170362 13.201352715492249 0.5258103627074888 15.561806678771973\n",
      "Training Step: 90\n",
      "Losses 2.7079605484482157 13.200276851654053 0.7695505722672351 16.67778778076172\n",
      "Training Step: 91\n",
      "Losses 4.239869372407156 13.242615014314651 0.40510067404462546 17.887582778930664\n",
      "Training Step: 92\n",
      "Losses 3.118361285782612 13.341372400522232 0.7291434661011067 17.188875198364258\n",
      "Training Step: 93\n",
      "Losses 3.2224782899356796 13.158716470003128 0.3148902802274165 16.696086883544922\n",
      "Training Step: 94\n",
      "Losses 4.096026754289472 13.315596491098404 0.4322561492957675 17.84387969970703\n",
      "Training Step: 95\n",
      "Losses 3.7943977303763745 13.35761484503746 0.3139581218965759 17.465970993041992\n",
      "Training Step: 96\n",
      "Losses 3.0717308599335524 13.271998405456543 0.7596966286364477 17.103424072265625\n",
      "Training Step: 97\n",
      "Losses 4.053495414617828 13.198543429374695 0.6767598538497168 17.928800582885742\n",
      "Training Step: 98\n",
      "Losses 4.550975804651898 13.241020530462265 0.43238837460160084 18.224388122558594\n",
      "Training Step: 99\n",
      "Losses 3.8369737814427936 13.244831293821335 0.34818701484010717 17.429990768432617\n",
      "Training Step: 100\n",
      "Saving Checkpoint\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting loss\n",
      "unroll step 0\n",
      "predicted value tensor([0.1694], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0872, 0.0987, 0.0903, 0.1391, 0.1189, 0.1149, 0.1053, 0.1393, 0.1063],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1600, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.2400, 0.1600])\n",
      "sample losses tensor([0.6898], grad_fn=<MulBackward0>) tensor(0.) tensor(2.2152, grad_fn=<NegBackward0>)\n",
      "unroll step 1\n",
      "predicted value tensor([0.0477], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.2962], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1185, 0.0880, 0.0948, 0.0843, 0.0890, 0.1096, 0.1297, 0.1709, 0.1152],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1200, 0.2000, 0.0000, 0.2800, 0.1200, 0.0000, 0.0000, 0.2000, 0.0800])\n",
      "sample losses tensor([1.0976], grad_fn=<MulBackward0>) tensor([0.0877], grad_fn=<PowBackward0>) tensor(2.2512, grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "predicted value tensor([-0.1146], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.0080], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1210, 0.0901, 0.1170, 0.1216, 0.0917, 0.1079, 0.1045, 0.1308, 0.1155],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.2800, 0.1600, 0.0800, 0.0000, 0.1200, 0.0000, 0.0000, 0.2000, 0.1600])\n",
      "sample losses tensor([1.2423], grad_fn=<MulBackward0>) tensor([6.3827e-05], grad_fn=<PowBackward0>) tensor(2.1871, grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "predicted value tensor([0.0038], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([-0.0455], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0880, 0.1172, 0.0918, 0.1230, 0.1174, 0.1318, 0.0911, 0.1201, 0.1197],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.2000, 0.1600, 0.0000, 0.2000, 0.0000, 0.0000, 0.2400, 0.2000])\n",
      "sample losses tensor([1.0077], grad_fn=<MulBackward0>) tensor([0.0021], grad_fn=<PowBackward0>) tensor(2.1726, grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "predicted value tensor([0.0316], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.0123], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1085, 0.1023, 0.0974, 0.0849, 0.1066, 0.0898, 0.1352, 0.1476, 0.1278],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.2400, 0.2400, 0.0000, 0.2800, 0.0000, 0.0000, 0.0000, 0.2400])\n",
      "sample losses tensor([0.9378], grad_fn=<MulBackward0>) tensor([0.0002], grad_fn=<PowBackward0>) tensor(2.2268, grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "predicted value tensor([0.0620], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.1655], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1207, 0.0965, 0.0987, 0.1143, 0.0992, 0.1130, 0.1168, 0.1254, 0.1155],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.4000, 0.2800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3200])\n",
      "sample losses tensor([1.1278], grad_fn=<MulBackward0>) tensor([0.0274], grad_fn=<PowBackward0>) tensor(2.2746, grad_fn=<NegBackward0>)\n",
      "Losses 3.17408671781377 13.27391603589058 0.5088047334666044 16.956809997558594\n",
      "Training Step: 101\n",
      "Losses 3.1625240364994625 13.174675688147545 0.6905661127457279 17.02776527404785\n",
      "Training Step: 102\n",
      "Losses 3.4440797616007046 13.270551443099976 0.3605210678497315 17.07515525817871\n",
      "Training Step: 103\n",
      "Losses 4.056113996106092 13.246699899435043 0.765452380568604 18.068265914916992\n",
      "Training Step: 104\n",
      "Losses 2.4341223301287904 13.339302629232407 0.9332023851238773 16.706628799438477\n",
      "Training Step: 105\n",
      "Losses 4.230777643435431 13.24946278333664 0.3459907188662328 17.826229095458984\n",
      "Training Step: 106\n",
      "Losses 3.5329710523478752 13.242109820246696 0.5181702870686422 17.29325294494629\n",
      "Training Step: 107\n",
      "Losses 3.1184672916415366 13.276827082037926 0.6838121980929657 17.07910919189453\n",
      "Training Step: 108\n",
      "Losses 4.1810597801959375 13.272288650274277 0.6507012548044457 18.104047775268555\n",
      "Training Step: 109\n",
      "Losses 4.357471530940529 13.183877348899841 0.3848412502567271 17.926191329956055\n",
      "Training Step: 110\n",
      "Losses 3.3103682147032956 13.244268983602524 0.7549255785978934 17.30956268310547\n",
      "Training Step: 111\n",
      "Losses 2.495257157528556 13.280852645635605 0.6048101041219205 16.38092041015625\n",
      "Training Step: 112\n",
      "Losses 4.7513158774527255 13.297194302082062 0.6339574012981757 18.682466506958008\n",
      "Training Step: 113\n",
      "Losses 4.3900798133522585 13.212162792682648 0.5858302664613859 18.188072204589844\n",
      "Training Step: 114\n",
      "Losses 2.8238038860642973 13.325342744588852 0.8372413269512435 16.986387252807617\n",
      "Training Step: 115\n",
      "Losses 3.3091260310755786 13.232324630022049 0.6711158068094392 17.212568283081055\n",
      "Training Step: 116\n",
      "Losses 3.5171966966317854 13.264543682336807 0.706279971085678 17.488021850585938\n",
      "Training Step: 117\n",
      "Losses 3.6678641243966794 13.312292575836182 0.5600260736864584 17.540180206298828\n",
      "Training Step: 118\n",
      "Losses 2.630026274697803 13.423441141843796 0.40144681689889694 16.454914093017578\n",
      "Training Step: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 707, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 643, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 392, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 629, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 809, in recurrent_inference\n",
      "    reward, hidden_state = self.dynamics(nn_input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 387, in forward\n",
      "    S.view(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 707, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 643, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 392, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 629, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 810, in recurrent_inference\n",
      "    value, policy = self.prediction(hidden_state)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 533, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 151, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo saved Trials! Starting from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarl_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Objective Function to optimize\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Hyperparameter's Search Space\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optimization algorithm (representative TPE)\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of optimization attempts\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Record the results\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# early_stop_fn=no_progress_loss(5, 1),\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_trials.p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_best_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n\u001b[1;32m     72\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m space_eval(search_space, best)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../hyperparameter_optimization/hyperopt.py:189\u001b[0m, in \u001b[0;36mmarl_objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_buffer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_replay_buffer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# score = run_training([params, env, name])\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mmarl_run_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    191\u001b[0m     status \u001b[38;5;241m=\u001b[39m STATUS_FAIL\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../hyperparameter_optimization/hyperopt.py:103\u001b[0m, in \u001b[0;36mmarl_run_training\u001b[0;34m(params, agent_name)\u001b[0m\n\u001b[1;32m    100\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtest_interval\n\u001b[1;32m    101\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtest_trials\n\u001b[0;32m--> 103\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39meval_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elo_evaulation(agent)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:287\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_replay_buffer_size:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[0;32m--> 287\u001b[0m         value_loss, policy_loss, reward_loss, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:605\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m         clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm)\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriorities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Convert tensors to float for return values\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    609\u001b[0m     val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size,\n\u001b[1;32m    610\u001b[0m     pol_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size,\n\u001b[1;32m    611\u001b[0m     rew_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size,\n\u001b[1;32m    612\u001b[0m     loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    613\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../replay_buffers/muzero_replay_buffer.py:180\u001b[0m, in \u001b[0;36mMuZeroReplayBuffer.update_priorities\u001b[0;34m(self, indices, priorities, ids)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_tree[index] \u001b[38;5;241m=\u001b[39m priority\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m priority\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority, priority\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../replay_buffers/segment_tree.py:111\u001b[0m, in \u001b[0;36mSegmentTree.__setitem__\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m    109\u001b[0m idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"best_agent_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=400,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
