{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7491063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 2,\n",
    "    \"stochastic\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.1,\n",
    "    \"reanalyze_noise\": False,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": False,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a659894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 20000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using default learning_rate                 : 0.001\n",
      "Using default clipnorm                      : 0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default loss_function                 : <class 'modules.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 8\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using default min_replay_buffer_size        : 8\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using default norm_type                     : none\n",
      "Using         world_model_cls               : <class 'modules.muzero_world_model.MuzeroWorldModel'>\n",
      "Using default norm_type                     : batch\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : [(24, 3, 1)]\n",
      "Using default conv_layers                   : []\n",
      "Using default dense_layer_widths            : []\n",
      "Using default representation_residual_layers: [(24, 3, 1)]\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: []\n",
      "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : []\n",
      "Using         reward_conv_layers            : [(16, 1, 1)]\n",
      "Using         reward_dense_layer_widths     : []\n",
      "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
      "Using         to_play_dense_layer_widths    : []\n",
      "Using         critic_conv_layers            : [(16, 1, 1)]\n",
      "Using         critic_dense_layer_widths     : []\n",
      "Using         actor_conv_layers             : [(16, 1, 1)]\n",
      "Using         actor_dense_layer_widths      : []\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using default to_play_loss_factor           : 1.0\n",
      "Using default weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.25\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 25\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using default temperature_updates           : [5]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default value_loss_function           : <modules.utils.MSELoss object at 0x3258247c0>\n",
      "Using default reward_loss_function          : <modules.utils.MSELoss object at 0x325824790>\n",
      "Using         policy_loss_function          : <modules.utils.CategoricalCrossentropyLoss object at 0x104f25a20>\n",
      "Using default to_play_loss_function         : <modules.utils.CategoricalCrossentropyLoss object at 0x3258247f0>\n",
      "Using         n_step                        : 10\n",
      "Using default discount_factor               : 1.0\n",
      "Using default unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_use_initial_max_priority  : True\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using         num_workers                   : 4\n",
      "Using default lr_ratio                      : inf\n",
      "Using         transfer_interval             : 1\n",
      "Using         reanalyze_ratio               : 0.0\n",
      "Using         reanalyze_method              : mcts\n",
      "Using default reanalyze_tau                 : 0.3\n",
      "Using         injection_frac                : 0.0\n",
      "Using         reanalyze_noise               : True\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         gumbel                        : True\n",
      "Using         gumbel_m                      : 8\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using         consistency_loss_factor       : 0.0\n",
      "Using         projector_output_dim          : 128\n",
      "Using         projector_hidden_dim          : 128\n",
      "Using         predictor_output_dim          : 128\n",
      "Using         predictor_hidden_dim          : 64\n",
      "Using default mask_absorbing                : False\n",
      "Using         value_prefix                  : False\n",
      "Using default lstm_horizon_len              : 5\n",
      "Using default lstm_hidden_size              : 64\n",
      "Using default q_estimation_method           : v_mix\n",
      "Using         stochastic                    : False\n",
      "Using default use_true_chance_codes         : False\n",
      "Using default num_chance                    : 32\n",
      "Using default sigma_loss                    : <modules.utils.CategoricalCrossentropyLoss object at 0x325824850>\n",
      "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
      "Using default afterstate_conv_layers        : []\n",
      "Using default afterstate_dense_layer_widths : []\n",
      "Using         chance_conv_layers            : [(16, 1, 1)]\n",
      "Using         chance_dense_layer_widths     : []\n",
      "Using default vqvae_commitment_cost_factor  : 1.0\n",
      "Using default action_embedding_dim          : 32\n",
      "Using default single_action_plane           : False\n",
      "[gumbel_test] Using device: cpu\n",
      "Observation dimensions: (9, 3, 3)\n",
      "Num actions: 9 (Discrete: True)\n",
      "Making test env...\n",
      "Test env configured for video recording.\n",
      "MARL Agent 'gumbel_test' initialized. Test agents: ['random', 'tictactoe_expert']\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "encoder input shape (8, 18, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "encoder input shape (8, 18, 3, 3)\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'cons_loss' with subkeys None\n",
      "Initializing stat 'q_loss' with subkeys None\n",
      "Initializing stat 'sigma_loss' with subkeys None\n",
      "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'num_codes' with subkeys None\n",
      "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 1] Starting self-play...\n",
      "[Worker 2] Starting self-play...\n",
      "[Worker 3] Starting self-play...\n",
      "0\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 3, 7, 5, 6],\n",
      "        [2, 3, 7, 5, 6],\n",
      "        [2, 3, 7, 5, 6],\n",
      "        [2, 3, 7, 5, 6],\n",
      "        [3, 7, 5, 6, 1],\n",
      "        [3, 7, 5, 6, 1],\n",
      "        [3, 7, 5, 6, 1],\n",
      "        [3, 7, 5, 6, 1]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[-0.4055],\n",
      "        [-0.4055],\n",
      "        [-0.4055],\n",
      "        [-0.4055],\n",
      "        [-0.2019],\n",
      "        [-0.2019],\n",
      "        [-0.2019],\n",
      "        [-0.2019]], grad_fn=<AddmmBackward0>), tensor([[-0.4018],\n",
      "        [-0.4018],\n",
      "        [-0.4018],\n",
      "        [-0.4018],\n",
      "        [-0.4338],\n",
      "        [-0.4338],\n",
      "        [-0.4338],\n",
      "        [-0.4338]], grad_fn=<AddmmBackward0>), tensor([[-0.4660],\n",
      "        [-0.4660],\n",
      "        [-0.4660],\n",
      "        [-0.4660],\n",
      "        [-0.0123],\n",
      "        [-0.0123],\n",
      "        [-0.0123],\n",
      "        [-0.0123]], grad_fn=<AddmmBackward0>), tensor([[-0.4938],\n",
      "        [-0.4938],\n",
      "        [-0.4938],\n",
      "        [-0.4938],\n",
      "        [-0.5737],\n",
      "        [-0.5737],\n",
      "        [-0.5737],\n",
      "        [-0.5737]], grad_fn=<AddmmBackward0>), tensor([[-0.2517],\n",
      "        [-0.2517],\n",
      "        [-0.2517],\n",
      "        [-0.2517],\n",
      "        [-0.2886],\n",
      "        [-0.2886],\n",
      "        [-0.2886],\n",
      "        [-0.2886]], grad_fn=<AddmmBackward0>), tensor([[-0.2094],\n",
      "        [-0.2094],\n",
      "        [-0.2094],\n",
      "        [-0.2094],\n",
      "        [-0.3802],\n",
      "        [-0.3802],\n",
      "        [-0.3802],\n",
      "        [-0.3802]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[0.2080],\n",
      "        [0.2080],\n",
      "        [0.2080],\n",
      "        [0.2080],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0112],\n",
      "        [0.0112]], grad_fn=<AddmmBackward0>), tensor([[0.2189],\n",
      "        [0.2189],\n",
      "        [0.2189],\n",
      "        [0.2189],\n",
      "        [0.1553],\n",
      "        [0.1553],\n",
      "        [0.1553],\n",
      "        [0.1553]], grad_fn=<AddmmBackward0>), tensor([[0.1283],\n",
      "        [0.1283],\n",
      "        [0.1283],\n",
      "        [0.1283],\n",
      "        [0.2615],\n",
      "        [0.2615],\n",
      "        [0.2615],\n",
      "        [0.2615]], grad_fn=<AddmmBackward0>), tensor([[ 0.0478],\n",
      "        [ 0.0478],\n",
      "        [ 0.0478],\n",
      "        [ 0.0478],\n",
      "        [-0.1492],\n",
      "        [-0.1492],\n",
      "        [-0.1492],\n",
      "        [-0.1492]], grad_fn=<AddmmBackward0>), tensor([[ 0.3121],\n",
      "        [ 0.3121],\n",
      "        [ 0.3121],\n",
      "        [ 0.3121],\n",
      "        [-0.1458],\n",
      "        [-0.1458],\n",
      "        [-0.1458],\n",
      "        [-0.1458]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[0.6232, 0.3768],\n",
      "        [0.6232, 0.3768],\n",
      "        [0.6232, 0.3768],\n",
      "        [0.6232, 0.3768],\n",
      "        [0.6413, 0.3587],\n",
      "        [0.6413, 0.3587],\n",
      "        [0.6413, 0.3587],\n",
      "        [0.6413, 0.3587]], grad_fn=<SoftmaxBackward0>), tensor([[0.7018, 0.2982],\n",
      "        [0.7018, 0.2982],\n",
      "        [0.7018, 0.2982],\n",
      "        [0.7018, 0.2982],\n",
      "        [0.6422, 0.3578],\n",
      "        [0.6422, 0.3578],\n",
      "        [0.6422, 0.3578],\n",
      "        [0.6422, 0.3578]], grad_fn=<SoftmaxBackward0>), tensor([[0.5251, 0.4749],\n",
      "        [0.5251, 0.4749],\n",
      "        [0.5251, 0.4749],\n",
      "        [0.5251, 0.4749],\n",
      "        [0.6990, 0.3010],\n",
      "        [0.6990, 0.3010],\n",
      "        [0.6990, 0.3010],\n",
      "        [0.6990, 0.3010]], grad_fn=<SoftmaxBackward0>), tensor([[0.5725, 0.4275],\n",
      "        [0.5725, 0.4275],\n",
      "        [0.5725, 0.4275],\n",
      "        [0.5725, 0.4275],\n",
      "        [0.6279, 0.3721],\n",
      "        [0.6279, 0.3721],\n",
      "        [0.6279, 0.3721],\n",
      "        [0.6279, 0.3721]], grad_fn=<SoftmaxBackward0>), tensor([[0.5864, 0.4136],\n",
      "        [0.5864, 0.4136],\n",
      "        [0.5864, 0.4136],\n",
      "        [0.5864, 0.4136],\n",
      "        [0.6855, 0.3145],\n",
      "        [0.6855, 0.3145],\n",
      "        [0.6855, 0.3145],\n",
      "        [0.6855, 0.3145]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True]]) tensor([[True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 4, 0, 7, 4],\n",
      "        [8, 0, 5, 7, 4],\n",
      "        [2, 0, 5, 7, 4],\n",
      "        [7, 6, 2, 4, 1],\n",
      "        [0, 5, 7, 3, 0],\n",
      "        [7, 3, 1, 4, 0],\n",
      "        [5, 0, 5, 7, 4],\n",
      "        [1, 0, 7, 3, 5]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[ 0.4112],\n",
      "        [ 1.1474],\n",
      "        [ 0.3258],\n",
      "        [-0.3817],\n",
      "        [ 0.7086],\n",
      "        [ 0.2453],\n",
      "        [ 0.9285],\n",
      "        [-0.9287]], grad_fn=<AddmmBackward0>), tensor([[-0.2490],\n",
      "        [ 0.3366],\n",
      "        [ 0.1387],\n",
      "        [ 0.0692],\n",
      "        [-0.1585],\n",
      "        [ 0.5961],\n",
      "        [-0.0785],\n",
      "        [ 0.8263]], grad_fn=<AddmmBackward0>), tensor([[ 4.1061e-01],\n",
      "        [ 1.5575e-01],\n",
      "        [ 7.4847e-02],\n",
      "        [ 8.5020e-04],\n",
      "        [ 6.4015e-01],\n",
      "        [ 6.1084e-04],\n",
      "        [ 2.5391e-01],\n",
      "        [-3.2498e-01]], grad_fn=<AddmmBackward0>), tensor([[-0.1158],\n",
      "        [ 0.1348],\n",
      "        [-0.0747],\n",
      "        [ 0.0018],\n",
      "        [-0.2672],\n",
      "        [ 0.5830],\n",
      "        [ 0.1669],\n",
      "        [ 0.4974]], grad_fn=<AddmmBackward0>), tensor([[ 0.4176],\n",
      "        [-0.1265],\n",
      "        [ 0.2214],\n",
      "        [ 0.1875],\n",
      "        [ 0.7457],\n",
      "        [-0.2058],\n",
      "        [ 0.1815],\n",
      "        [-0.0554]], grad_fn=<AddmmBackward0>), tensor([[-0.1777],\n",
      "        [ 0.3389],\n",
      "        [-0.1686],\n",
      "        [ 0.2303],\n",
      "        [-0.2018],\n",
      "        [ 0.4095],\n",
      "        [-0.0128],\n",
      "        [ 0.5848]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0209],\n",
      "        [ 0.2654],\n",
      "        [ 0.3517],\n",
      "        [ 0.1667],\n",
      "        [-0.0875],\n",
      "        [-0.0154],\n",
      "        [ 0.2234],\n",
      "        [ 0.0873]], grad_fn=<AddmmBackward0>), tensor([[0.1758],\n",
      "        [0.3654],\n",
      "        [0.1861],\n",
      "        [0.2232],\n",
      "        [0.0155],\n",
      "        [0.1464],\n",
      "        [0.2661],\n",
      "        [0.2005]], grad_fn=<AddmmBackward0>), tensor([[0.2446],\n",
      "        [0.1897],\n",
      "        [0.2338],\n",
      "        [0.2087],\n",
      "        [0.1789],\n",
      "        [0.2543],\n",
      "        [0.1676],\n",
      "        [0.2549]], grad_fn=<AddmmBackward0>), tensor([[0.2086],\n",
      "        [0.3358],\n",
      "        [0.0331],\n",
      "        [0.2570],\n",
      "        [0.0852],\n",
      "        [0.3850],\n",
      "        [0.1166],\n",
      "        [0.2925]], grad_fn=<AddmmBackward0>), tensor([[0.1748],\n",
      "        [0.4034],\n",
      "        [0.1807],\n",
      "        [0.2165],\n",
      "        [0.1923],\n",
      "        [0.1583],\n",
      "        [0.2570],\n",
      "        [0.1257]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[0.0039, 0.9961],\n",
      "        [0.9248, 0.0752],\n",
      "        [0.2023, 0.7977],\n",
      "        [0.0577, 0.9423],\n",
      "        [0.0164, 0.9836],\n",
      "        [0.9820, 0.0180],\n",
      "        [0.2304, 0.7696],\n",
      "        [0.9939, 0.0061]], grad_fn=<SoftmaxBackward0>), tensor([[0.9968, 0.0032],\n",
      "        [0.2963, 0.7037],\n",
      "        [0.9558, 0.0442],\n",
      "        [0.9383, 0.0617],\n",
      "        [0.9944, 0.0056],\n",
      "        [0.0013, 0.9987],\n",
      "        [0.7714, 0.2286],\n",
      "        [0.0037, 0.9963]], grad_fn=<SoftmaxBackward0>), tensor([[0.0097, 0.9903],\n",
      "        [0.9266, 0.0734],\n",
      "        [0.0156, 0.9844],\n",
      "        [0.2459, 0.7541],\n",
      "        [0.0029, 0.9971],\n",
      "        [0.9965, 0.0035],\n",
      "        [0.3950, 0.6050],\n",
      "        [0.9968, 0.0032]], grad_fn=<SoftmaxBackward0>), tensor([[0.9926, 0.0074],\n",
      "        [0.0115, 0.9885],\n",
      "        [0.9528, 0.0472],\n",
      "        [0.9190, 0.0810],\n",
      "        [0.9970, 0.0030],\n",
      "        [0.0056, 0.9944],\n",
      "        [0.4563, 0.5437],\n",
      "        [0.0080, 0.9920]], grad_fn=<SoftmaxBackward0>), tensor([[0.0038, 0.9962],\n",
      "        [0.9793, 0.0207],\n",
      "        [0.0062, 0.9938],\n",
      "        [0.0204, 0.9796],\n",
      "        [0.0027, 0.9973],\n",
      "        [0.9957, 0.0043],\n",
      "        [0.4598, 0.5402],\n",
      "        [0.9902, 0.0098]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 0, 2, 4, 8],\n",
      "        [1, 6, 8, 7, 5],\n",
      "        [7, 5, 8, 3, 6],\n",
      "        [8, 3, 5, 2, 7],\n",
      "        [3, 5, 0, 4, 8],\n",
      "        [7, 1, 3, 2, 6],\n",
      "        [7, 8, 1, 0, 8],\n",
      "        [2, 5, 4, 1, 8]])\n",
      "target value tensor([[ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.1423],\n",
      "        [-0.4316],\n",
      "        [-0.2990],\n",
      "        [-0.4910],\n",
      "        [-0.1692],\n",
      "        [-0.3722],\n",
      "        [ 0.0194],\n",
      "        [ 0.0827]], grad_fn=<AddmmBackward0>), tensor([[-0.3367],\n",
      "        [ 0.1202],\n",
      "        [-0.0687],\n",
      "        [ 0.1652],\n",
      "        [ 0.1523],\n",
      "        [ 0.2184],\n",
      "        [-0.2025],\n",
      "        [-0.2837]], grad_fn=<AddmmBackward0>), tensor([[ 0.1466],\n",
      "        [-0.2308],\n",
      "        [-0.0514],\n",
      "        [ 0.1026],\n",
      "        [ 0.2857],\n",
      "        [-0.0830],\n",
      "        [ 0.2415],\n",
      "        [ 0.3252]], grad_fn=<AddmmBackward0>), tensor([[-0.0681],\n",
      "        [ 0.0709],\n",
      "        [ 0.1541],\n",
      "        [ 0.1494],\n",
      "        [-0.0076],\n",
      "        [ 0.1378],\n",
      "        [ 0.0596],\n",
      "        [-0.1998]], grad_fn=<AddmmBackward0>), tensor([[-0.0105],\n",
      "        [ 0.1511],\n",
      "        [ 0.1012],\n",
      "        [ 0.0961],\n",
      "        [-0.0798],\n",
      "        [ 0.0699],\n",
      "        [-0.0354],\n",
      "        [ 0.1951]], grad_fn=<AddmmBackward0>), tensor([[ 0.0725],\n",
      "        [ 0.0670],\n",
      "        [ 0.0506],\n",
      "        [ 0.0599],\n",
      "        [ 0.1682],\n",
      "        [-0.0027],\n",
      "        [ 0.1043],\n",
      "        [ 0.0445]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2175],\n",
      "        [-0.0490],\n",
      "        [-0.0204],\n",
      "        [ 0.1458],\n",
      "        [ 0.2324],\n",
      "        [ 0.1588],\n",
      "        [ 0.2464],\n",
      "        [-0.1368]], grad_fn=<AddmmBackward0>), tensor([[-0.1159],\n",
      "        [ 0.1405],\n",
      "        [ 0.1056],\n",
      "        [ 0.1416],\n",
      "        [ 0.2360],\n",
      "        [ 0.0748],\n",
      "        [ 0.0826],\n",
      "        [-0.1418]], grad_fn=<AddmmBackward0>), tensor([[0.0023],\n",
      "        [0.0917],\n",
      "        [0.0314],\n",
      "        [0.2244],\n",
      "        [0.0473],\n",
      "        [0.2105],\n",
      "        [0.1163],\n",
      "        [0.1290]], grad_fn=<AddmmBackward0>), tensor([[ 0.0554],\n",
      "        [ 0.0523],\n",
      "        [-0.0190],\n",
      "        [ 0.0550],\n",
      "        [ 0.0218],\n",
      "        [ 0.1239],\n",
      "        [ 0.0166],\n",
      "        [ 0.0662]], grad_fn=<AddmmBackward0>), tensor([[-0.0068],\n",
      "        [ 0.1079],\n",
      "        [ 0.0474],\n",
      "        [ 0.0517],\n",
      "        [-0.0837],\n",
      "        [-0.0220],\n",
      "        [ 0.1480],\n",
      "        [ 0.1550]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[6.4958e-03, 9.9350e-01],\n",
      "        [9.9914e-01, 8.5740e-04],\n",
      "        [9.9603e-01, 3.9696e-03],\n",
      "        [9.9611e-01, 3.8935e-03],\n",
      "        [3.2674e-02, 9.6733e-01],\n",
      "        [9.9247e-01, 7.5256e-03],\n",
      "        [1.4333e-03, 9.9857e-01],\n",
      "        [1.1583e-03, 9.9884e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9842e-01, 1.5817e-03],\n",
      "        [9.2823e-04, 9.9907e-01],\n",
      "        [2.7821e-03, 9.9722e-01],\n",
      "        [2.1624e-03, 9.9784e-01],\n",
      "        [9.9777e-01, 2.2322e-03],\n",
      "        [1.3324e-02, 9.8668e-01],\n",
      "        [9.9918e-01, 8.2211e-04],\n",
      "        [9.9949e-01, 5.1264e-04]], grad_fn=<SoftmaxBackward0>), tensor([[0.0016, 0.9984],\n",
      "        [0.9983, 0.0017],\n",
      "        [0.9988, 0.0012],\n",
      "        [0.9978, 0.0022],\n",
      "        [0.0170, 0.9830],\n",
      "        [0.9969, 0.0031],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0011, 0.9989]], grad_fn=<SoftmaxBackward0>), tensor([[9.9693e-01, 3.0710e-03],\n",
      "        [3.2771e-03, 9.9672e-01],\n",
      "        [2.6412e-03, 9.9736e-01],\n",
      "        [2.9978e-03, 9.9700e-01],\n",
      "        [9.8945e-01, 1.0546e-02],\n",
      "        [6.5886e-03, 9.9341e-01],\n",
      "        [9.9847e-01, 1.5332e-03],\n",
      "        [9.9915e-01, 8.5056e-04]], grad_fn=<SoftmaxBackward0>), tensor([[0.0064, 0.9936],\n",
      "        [0.9981, 0.0019],\n",
      "        [0.9966, 0.0034],\n",
      "        [0.9971, 0.0029],\n",
      "        [0.0110, 0.9890],\n",
      "        [0.9979, 0.0021],\n",
      "        [0.0036, 0.9964],\n",
      "        [0.0017, 0.9983]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 4, 2, 0, 3],\n",
      "        [5, 2, 0, 4, 0],\n",
      "        [0, 0, 1, 4, 0],\n",
      "        [8, 0, 3, 6, 2],\n",
      "        [4, 7, 2, 5, 6],\n",
      "        [1, 7, 6, 8, 3],\n",
      "        [5, 6, 0, 4, 8],\n",
      "        [5, 2, 0, 4, 0]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3834],\n",
      "        [-0.2590],\n",
      "        [ 0.4356],\n",
      "        [ 0.3834],\n",
      "        [ 0.3834],\n",
      "        [-0.4086],\n",
      "        [ 0.7133],\n",
      "        [ 0.5355]], grad_fn=<AddmmBackward0>), tensor([[-0.0488],\n",
      "        [ 0.3888],\n",
      "        [-0.2054],\n",
      "        [-0.0488],\n",
      "        [-0.4280],\n",
      "        [ 0.2814],\n",
      "        [-0.2652],\n",
      "        [ 0.3324]], grad_fn=<AddmmBackward0>), tensor([[ 0.1097],\n",
      "        [-0.1734],\n",
      "        [ 0.2676],\n",
      "        [ 0.3118],\n",
      "        [ 0.2881],\n",
      "        [-0.3653],\n",
      "        [ 0.2954],\n",
      "        [ 0.0584]], grad_fn=<AddmmBackward0>), tensor([[ 0.0639],\n",
      "        [ 0.2303],\n",
      "        [-0.1549],\n",
      "        [-0.0570],\n",
      "        [-0.0689],\n",
      "        [ 0.2771],\n",
      "        [-0.2516],\n",
      "        [ 0.0315]], grad_fn=<AddmmBackward0>), tensor([[ 0.1913],\n",
      "        [-0.0344],\n",
      "        [ 0.0887],\n",
      "        [ 0.3569],\n",
      "        [ 0.3836],\n",
      "        [-0.2495],\n",
      "        [ 0.0810],\n",
      "        [-0.0351]], grad_fn=<AddmmBackward0>), tensor([[ 0.1045],\n",
      "        [ 0.1085],\n",
      "        [-0.0532],\n",
      "        [-0.0962],\n",
      "        [-0.1255],\n",
      "        [ 0.3626],\n",
      "        [ 0.0371],\n",
      "        [ 0.0487]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0032],\n",
      "        [ 0.3378],\n",
      "        [ 0.3149],\n",
      "        [ 0.0032],\n",
      "        [-0.0539],\n",
      "        [ 0.1225],\n",
      "        [ 0.2357],\n",
      "        [ 0.1081]], grad_fn=<AddmmBackward0>), tensor([[ 0.0358],\n",
      "        [ 0.2559],\n",
      "        [ 0.0005],\n",
      "        [-0.1198],\n",
      "        [-0.0460],\n",
      "        [-0.0068],\n",
      "        [-0.0122],\n",
      "        [ 0.1807]], grad_fn=<AddmmBackward0>), tensor([[ 0.2983],\n",
      "        [ 0.0320],\n",
      "        [-0.0679],\n",
      "        [-0.0377],\n",
      "        [ 0.1440],\n",
      "        [ 0.0701],\n",
      "        [ 0.0768],\n",
      "        [ 0.0775]], grad_fn=<AddmmBackward0>), tensor([[ 0.0614],\n",
      "        [ 0.1811],\n",
      "        [-0.0084],\n",
      "        [-0.0143],\n",
      "        [ 0.0910],\n",
      "        [ 0.1701],\n",
      "        [ 0.1079],\n",
      "        [ 0.0576]], grad_fn=<AddmmBackward0>), tensor([[0.1255],\n",
      "        [0.0560],\n",
      "        [0.1337],\n",
      "        [0.1131],\n",
      "        [0.2730],\n",
      "        [0.1419],\n",
      "        [0.2305],\n",
      "        [0.0556]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.0391e-03, 9.9896e-01],\n",
      "        [9.9961e-01, 3.8714e-04],\n",
      "        [5.8360e-03, 9.9416e-01],\n",
      "        [1.0391e-03, 9.9896e-01],\n",
      "        [6.3910e-04, 9.9936e-01],\n",
      "        [9.9947e-01, 5.3281e-04],\n",
      "        [1.6593e-02, 9.8341e-01],\n",
      "        [4.6855e-02, 9.5315e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9886e-01, 1.1394e-03],\n",
      "        [5.9086e-04, 9.9941e-01],\n",
      "        [9.9618e-01, 3.8152e-03],\n",
      "        [9.9888e-01, 1.1190e-03],\n",
      "        [9.9955e-01, 4.4508e-04],\n",
      "        [2.5059e-04, 9.9975e-01],\n",
      "        [9.6744e-01, 3.2556e-02],\n",
      "        [9.2373e-01, 7.6269e-02]], grad_fn=<SoftmaxBackward0>), tensor([[5.0229e-03, 9.9498e-01],\n",
      "        [9.9984e-01, 1.6125e-04],\n",
      "        [1.7077e-03, 9.9829e-01],\n",
      "        [8.2610e-04, 9.9917e-01],\n",
      "        [6.8062e-04, 9.9932e-01],\n",
      "        [9.9662e-01, 3.3786e-03],\n",
      "        [6.0089e-04, 9.9940e-01],\n",
      "        [7.4123e-02, 9.2588e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9789e-01, 2.1144e-03],\n",
      "        [1.2236e-03, 9.9878e-01],\n",
      "        [9.9758e-01, 2.4192e-03],\n",
      "        [9.9887e-01, 1.1324e-03],\n",
      "        [9.9924e-01, 7.5861e-04],\n",
      "        [4.7986e-04, 9.9952e-01],\n",
      "        [9.9477e-01, 5.2333e-03],\n",
      "        [5.6502e-01, 4.3498e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.2340e-03, 9.9677e-01],\n",
      "        [9.9913e-01, 8.7118e-04],\n",
      "        [1.3642e-03, 9.9864e-01],\n",
      "        [2.3078e-04, 9.9977e-01],\n",
      "        [7.0594e-04, 9.9929e-01],\n",
      "        [9.9957e-01, 4.2759e-04],\n",
      "        [4.4389e-03, 9.9556e-01],\n",
      "        [7.7348e-02, 9.2265e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 7, 3, 5, 6],\n",
      "        [1, 4, 5, 3, 7],\n",
      "        [7, 3, 2, 5, 6],\n",
      "        [4, 2, 1, 7, 3],\n",
      "        [5, 3, 0, 7, 7],\n",
      "        [2, 0, 7, 5, 3],\n",
      "        [6, 7, 3, 8, 0],\n",
      "        [2, 0, 1, 5, 4]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.1709],\n",
      "        [ 0.1709],\n",
      "        [-0.0689],\n",
      "        [-0.5266],\n",
      "        [ 0.2111],\n",
      "        [ 0.7562],\n",
      "        [ 0.1200],\n",
      "        [ 0.1709]], grad_fn=<AddmmBackward0>), tensor([[-0.3012],\n",
      "        [ 0.0282],\n",
      "        [ 0.1900],\n",
      "        [ 0.4111],\n",
      "        [ 0.4343],\n",
      "        [-0.2911],\n",
      "        [-0.2166],\n",
      "        [-0.3405]], grad_fn=<AddmmBackward0>), tensor([[ 0.0783],\n",
      "        [ 0.1914],\n",
      "        [ 0.0708],\n",
      "        [-0.0873],\n",
      "        [ 0.0725],\n",
      "        [ 0.1310],\n",
      "        [ 0.1635],\n",
      "        [ 0.0944]], grad_fn=<AddmmBackward0>), tensor([[-0.1604],\n",
      "        [-0.2257],\n",
      "        [ 0.0307],\n",
      "        [ 0.2240],\n",
      "        [ 0.0992],\n",
      "        [ 0.2594],\n",
      "        [-0.0269],\n",
      "        [-0.2432]], grad_fn=<AddmmBackward0>), tensor([[ 0.1389],\n",
      "        [ 0.3046],\n",
      "        [-0.0219],\n",
      "        [ 0.0709],\n",
      "        [ 0.1396],\n",
      "        [-0.0406],\n",
      "        [ 0.2710],\n",
      "        [ 0.1325]], grad_fn=<AddmmBackward0>), tensor([[-0.0214],\n",
      "        [-0.1954],\n",
      "        [ 0.2393],\n",
      "        [ 0.3766],\n",
      "        [ 0.0650],\n",
      "        [ 0.0435],\n",
      "        [ 0.1720],\n",
      "        [-0.3334]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.1476],\n",
      "        [-0.0954],\n",
      "        [-0.1010],\n",
      "        [ 0.0220],\n",
      "        [ 0.5783],\n",
      "        [ 0.1215],\n",
      "        [ 0.1886],\n",
      "        [-0.1061]], grad_fn=<AddmmBackward0>), tensor([[-0.0263],\n",
      "        [ 0.0142],\n",
      "        [-0.0130],\n",
      "        [ 0.1573],\n",
      "        [ 0.4197],\n",
      "        [ 0.1350],\n",
      "        [ 0.0546],\n",
      "        [-0.0114]], grad_fn=<AddmmBackward0>), tensor([[0.0560],\n",
      "        [0.0742],\n",
      "        [0.0970],\n",
      "        [0.2946],\n",
      "        [0.2073],\n",
      "        [0.1536],\n",
      "        [0.1890],\n",
      "        [0.1668]], grad_fn=<AddmmBackward0>), tensor([[0.3124],\n",
      "        [0.1817],\n",
      "        [0.0849],\n",
      "        [0.3557],\n",
      "        [0.1441],\n",
      "        [0.2881],\n",
      "        [0.2116],\n",
      "        [0.1304]], grad_fn=<AddmmBackward0>), tensor([[0.1021],\n",
      "        [0.2554],\n",
      "        [0.2569],\n",
      "        [0.3425],\n",
      "        [0.0523],\n",
      "        [0.2447],\n",
      "        [0.1123],\n",
      "        [0.2200]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.1980e-04, 9.9968e-01],\n",
      "        [6.5804e-04, 9.9934e-01],\n",
      "        [9.9988e-01, 1.1914e-04],\n",
      "        [9.9988e-01, 1.2443e-04],\n",
      "        [9.9838e-01, 1.6202e-03],\n",
      "        [3.2262e-03, 9.9677e-01],\n",
      "        [1.0963e-02, 9.8904e-01],\n",
      "        [4.5969e-04, 9.9954e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9827e-01, 1.7263e-03],\n",
      "        [9.9618e-01, 3.8166e-03],\n",
      "        [7.3337e-04, 9.9927e-01],\n",
      "        [1.2294e-03, 9.9877e-01],\n",
      "        [1.0862e-04, 9.9989e-01],\n",
      "        [9.3414e-01, 6.5861e-02],\n",
      "        [9.7925e-01, 2.0752e-02],\n",
      "        [9.9720e-01, 2.7965e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.4674e-03, 9.9853e-01],\n",
      "        [6.6017e-04, 9.9934e-01],\n",
      "        [9.9891e-01, 1.0885e-03],\n",
      "        [9.9946e-01, 5.3508e-04],\n",
      "        [9.9907e-01, 9.3147e-04],\n",
      "        [1.3297e-02, 9.8670e-01],\n",
      "        [1.0962e-03, 9.9890e-01],\n",
      "        [1.8855e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9775e-01, 2.2529e-03],\n",
      "        [9.9807e-01, 1.9251e-03],\n",
      "        [8.9388e-04, 9.9911e-01],\n",
      "        [7.8762e-03, 9.9212e-01],\n",
      "        [9.1854e-04, 9.9908e-01],\n",
      "        [9.8007e-01, 1.9928e-02],\n",
      "        [9.9752e-01, 2.4839e-03],\n",
      "        [9.9135e-01, 8.6487e-03]], grad_fn=<SoftmaxBackward0>), tensor([[6.8683e-04, 9.9931e-01],\n",
      "        [1.5313e-03, 9.9847e-01],\n",
      "        [9.9965e-01, 3.5009e-04],\n",
      "        [9.9562e-01, 4.3762e-03],\n",
      "        [9.9318e-01, 6.8162e-03],\n",
      "        [6.8863e-04, 9.9931e-01],\n",
      "        [1.3718e-03, 9.9863e-01],\n",
      "        [8.1524e-04, 9.9918e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 5, 7, 1, 6],\n",
      "        [2, 0, 4, 6, 7],\n",
      "        [6, 0, 5, 8, 0],\n",
      "        [1, 6, 0, 8, 0],\n",
      "        [7, 6, 0, 0, 0],\n",
      "        [6, 3, 1, 0, 0],\n",
      "        [3, 4, 2, 5, 8],\n",
      "        [4, 3, 6, 1, 5]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.4195],\n",
      "        [ 0.3425],\n",
      "        [ 0.4769],\n",
      "        [-0.3779],\n",
      "        [-0.4746],\n",
      "        [ 0.0961],\n",
      "        [-0.2663],\n",
      "        [ 0.4874]], grad_fn=<AddmmBackward0>), tensor([[ 0.0792],\n",
      "        [-0.1820],\n",
      "        [ 0.2159],\n",
      "        [ 0.6404],\n",
      "        [ 0.4175],\n",
      "        [ 0.2025],\n",
      "        [ 0.1238],\n",
      "        [-0.1902]], grad_fn=<AddmmBackward0>), tensor([[ 0.3976],\n",
      "        [ 0.2299],\n",
      "        [ 0.4271],\n",
      "        [-0.0291],\n",
      "        [ 0.2974],\n",
      "        [ 0.6045],\n",
      "        [-0.0896],\n",
      "        [ 0.2214]], grad_fn=<AddmmBackward0>), tensor([[ 0.0636],\n",
      "        [-0.0672],\n",
      "        [ 0.0617],\n",
      "        [ 0.2507],\n",
      "        [-0.0504],\n",
      "        [-0.0728],\n",
      "        [ 0.1941],\n",
      "        [ 0.1742]], grad_fn=<AddmmBackward0>), tensor([[ 0.2898],\n",
      "        [ 0.3900],\n",
      "        [ 0.3131],\n",
      "        [ 0.0640],\n",
      "        [-0.1408],\n",
      "        [ 0.2831],\n",
      "        [ 0.0552],\n",
      "        [ 0.4478]], grad_fn=<AddmmBackward0>), tensor([[-0.1034],\n",
      "        [ 0.2658],\n",
      "        [ 0.1210],\n",
      "        [ 0.2745],\n",
      "        [-0.1148],\n",
      "        [-0.0747],\n",
      "        [ 0.1726],\n",
      "        [ 0.0008]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1215],\n",
      "        [ 0.0461],\n",
      "        [ 0.5118],\n",
      "        [ 0.2505],\n",
      "        [ 0.3619],\n",
      "        [ 0.2981],\n",
      "        [-0.0229],\n",
      "        [ 0.1635]], grad_fn=<AddmmBackward0>), tensor([[ 0.1726],\n",
      "        [-0.1065],\n",
      "        [ 0.1520],\n",
      "        [ 0.4818],\n",
      "        [ 0.5577],\n",
      "        [ 0.2274],\n",
      "        [ 0.0946],\n",
      "        [-0.0555]], grad_fn=<AddmmBackward0>), tensor([[0.3087],\n",
      "        [0.1640],\n",
      "        [0.4312],\n",
      "        [0.0508],\n",
      "        [0.3688],\n",
      "        [0.3916],\n",
      "        [0.0336],\n",
      "        [0.2080]], grad_fn=<AddmmBackward0>), tensor([[ 0.2641],\n",
      "        [-0.0555],\n",
      "        [ 0.2572],\n",
      "        [-0.0275],\n",
      "        [ 0.1195],\n",
      "        [-0.0257],\n",
      "        [ 0.3486],\n",
      "        [ 0.0736]], grad_fn=<AddmmBackward0>), tensor([[ 0.4406],\n",
      "        [ 0.2584],\n",
      "        [ 0.1348],\n",
      "        [-0.1315],\n",
      "        [-0.0673],\n",
      "        [ 0.0709],\n",
      "        [ 0.2493],\n",
      "        [ 0.3619]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.3594e-03, 9.9864e-01],\n",
      "        [8.8707e-04, 9.9911e-01],\n",
      "        [1.0345e-03, 9.9897e-01],\n",
      "        [9.9994e-01, 5.9313e-05],\n",
      "        [3.6279e-01, 6.3721e-01],\n",
      "        [5.2507e-03, 9.9475e-01],\n",
      "        [9.9998e-01, 1.8638e-05],\n",
      "        [6.9357e-04, 9.9931e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9974e-01, 2.6415e-04],\n",
      "        [9.9804e-01, 1.9569e-03],\n",
      "        [9.9984e-01, 1.6066e-04],\n",
      "        [7.7965e-04, 9.9922e-01],\n",
      "        [9.8030e-01, 1.9702e-02],\n",
      "        [9.9946e-01, 5.3893e-04],\n",
      "        [1.8315e-05, 9.9998e-01],\n",
      "        [9.9757e-01, 2.4346e-03]], grad_fn=<SoftmaxBackward0>), tensor([[2.5485e-03, 9.9745e-01],\n",
      "        [9.1225e-05, 9.9991e-01],\n",
      "        [9.4488e-04, 9.9906e-01],\n",
      "        [9.9985e-01, 1.4922e-04],\n",
      "        [3.0653e-01, 6.9347e-01],\n",
      "        [6.2181e-04, 9.9938e-01],\n",
      "        [9.9993e-01, 7.3083e-05],\n",
      "        [6.5816e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9978e-01, 2.2102e-04],\n",
      "        [9.9448e-01, 5.5242e-03],\n",
      "        [9.9971e-01, 2.8624e-04],\n",
      "        [3.4142e-03, 9.9659e-01],\n",
      "        [5.6134e-01, 4.3866e-01],\n",
      "        [9.9993e-01, 7.2414e-05],\n",
      "        [7.1288e-04, 9.9929e-01],\n",
      "        [9.9984e-01, 1.5881e-04]], grad_fn=<SoftmaxBackward0>), tensor([[2.2145e-03, 9.9779e-01],\n",
      "        [3.9913e-03, 9.9601e-01],\n",
      "        [3.1391e-03, 9.9686e-01],\n",
      "        [9.9867e-01, 1.3269e-03],\n",
      "        [5.7795e-02, 9.4220e-01],\n",
      "        [3.2391e-04, 9.9968e-01],\n",
      "        [9.9976e-01, 2.4012e-04],\n",
      "        [7.9221e-04, 9.9921e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 0, 8, 7, 4],\n",
      "        [6, 8, 7, 2, 0],\n",
      "        [5, 7, 0, 6, 0],\n",
      "        [8, 0, 7, 6, 8],\n",
      "        [8, 5, 0, 4, 0],\n",
      "        [8, 0, 5, 1, 0],\n",
      "        [0, 2, 8, 1, 6],\n",
      "        [5, 7, 8, 6, 3]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.]])\n",
      "predicted values [tensor([[ 0.3012],\n",
      "        [-0.2459],\n",
      "        [ 0.5674],\n",
      "        [ 0.5577],\n",
      "        [-0.4378],\n",
      "        [-0.3850],\n",
      "        [-0.4149],\n",
      "        [ 0.6248]], grad_fn=<AddmmBackward0>), tensor([[-0.4351],\n",
      "        [ 0.4353],\n",
      "        [ 0.0734],\n",
      "        [-0.0781],\n",
      "        [ 0.1662],\n",
      "        [ 0.2954],\n",
      "        [ 0.0860],\n",
      "        [-0.2664]], grad_fn=<AddmmBackward0>), tensor([[ 0.1188],\n",
      "        [-0.0706],\n",
      "        [ 0.4992],\n",
      "        [ 0.1694],\n",
      "        [-0.3884],\n",
      "        [-0.4513],\n",
      "        [-0.1993],\n",
      "        [ 0.0582]], grad_fn=<AddmmBackward0>), tensor([[-0.2398],\n",
      "        [ 0.2353],\n",
      "        [ 0.0682],\n",
      "        [-0.0047],\n",
      "        [ 0.1500],\n",
      "        [ 0.1998],\n",
      "        [ 0.0496],\n",
      "        [-0.1352]], grad_fn=<AddmmBackward0>), tensor([[ 0.1129],\n",
      "        [-0.1197],\n",
      "        [ 0.1621],\n",
      "        [ 0.1375],\n",
      "        [-0.2980],\n",
      "        [-0.3239],\n",
      "        [-0.3737],\n",
      "        [ 0.2376]], grad_fn=<AddmmBackward0>), tensor([[-0.2570],\n",
      "        [ 0.1524],\n",
      "        [-0.0923],\n",
      "        [-0.1288],\n",
      "        [-0.0093],\n",
      "        [ 0.0383],\n",
      "        [ 0.3641],\n",
      "        [-0.0883]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0981],\n",
      "        [ 0.1432],\n",
      "        [ 0.2114],\n",
      "        [ 0.3306],\n",
      "        [ 0.0033],\n",
      "        [ 0.0376],\n",
      "        [ 0.0368],\n",
      "        [-0.0281]], grad_fn=<AddmmBackward0>), tensor([[-0.0389],\n",
      "        [ 0.1511],\n",
      "        [ 0.1336],\n",
      "        [ 0.0048],\n",
      "        [ 0.0303],\n",
      "        [ 0.1951],\n",
      "        [ 0.0853],\n",
      "        [-0.0649]], grad_fn=<AddmmBackward0>), tensor([[-0.0040],\n",
      "        [ 0.2090],\n",
      "        [ 0.2455],\n",
      "        [-0.0295],\n",
      "        [ 0.0058],\n",
      "        [ 0.1051],\n",
      "        [ 0.1032],\n",
      "        [ 0.1323]], grad_fn=<AddmmBackward0>), tensor([[-0.0299],\n",
      "        [ 0.2994],\n",
      "        [ 0.3649],\n",
      "        [ 0.0360],\n",
      "        [ 0.1613],\n",
      "        [-0.0408],\n",
      "        [-0.0071],\n",
      "        [ 0.0858]], grad_fn=<AddmmBackward0>), tensor([[ 0.1259],\n",
      "        [ 0.2494],\n",
      "        [ 0.1681],\n",
      "        [-0.0029],\n",
      "        [-0.0015],\n",
      "        [-0.0255],\n",
      "        [ 0.0439],\n",
      "        [ 0.0925]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.6314e-04, 9.9984e-01],\n",
      "        [9.9065e-01, 9.3500e-03],\n",
      "        [3.5341e-04, 9.9965e-01],\n",
      "        [1.2540e-04, 9.9987e-01],\n",
      "        [9.9973e-01, 2.6892e-04],\n",
      "        [9.9991e-01, 8.5772e-05],\n",
      "        [9.9988e-01, 1.2217e-04],\n",
      "        [1.3750e-04, 9.9986e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9972e-01, 2.7821e-04],\n",
      "        [8.2745e-04, 9.9917e-01],\n",
      "        [9.9929e-01, 7.1327e-04],\n",
      "        [9.9945e-01, 5.5021e-04],\n",
      "        [9.3879e-05, 9.9991e-01],\n",
      "        [6.5279e-05, 9.9993e-01],\n",
      "        [1.8869e-05, 9.9998e-01],\n",
      "        [9.9677e-01, 3.2295e-03]], grad_fn=<SoftmaxBackward0>), tensor([[1.0636e-05, 9.9999e-01],\n",
      "        [9.9844e-01, 1.5637e-03],\n",
      "        [4.0677e-04, 9.9959e-01],\n",
      "        [1.6600e-04, 9.9983e-01],\n",
      "        [9.9918e-01, 8.2228e-04],\n",
      "        [9.9890e-01, 1.0989e-03],\n",
      "        [9.9972e-01, 2.7973e-04],\n",
      "        [1.0079e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9947e-01, 5.3224e-04],\n",
      "        [1.4586e-04, 9.9985e-01],\n",
      "        [9.9910e-01, 8.9890e-04],\n",
      "        [9.9852e-01, 1.4765e-03],\n",
      "        [6.3348e-05, 9.9994e-01],\n",
      "        [1.2634e-04, 9.9987e-01],\n",
      "        [5.9549e-05, 9.9994e-01],\n",
      "        [9.9915e-01, 8.5227e-04]], grad_fn=<SoftmaxBackward0>), tensor([[1.3984e-04, 9.9986e-01],\n",
      "        [9.9995e-01, 4.5851e-05],\n",
      "        [1.6913e-04, 9.9983e-01],\n",
      "        [3.2426e-04, 9.9968e-01],\n",
      "        [9.9941e-01, 5.8577e-04],\n",
      "        [9.9936e-01, 6.4057e-04],\n",
      "        [9.9952e-01, 4.8155e-04],\n",
      "        [1.8630e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 0, 2, 8, 5],\n",
      "        [7, 2, 4, 6, 0],\n",
      "        [3, 2, 8, 5, 7],\n",
      "        [4, 2, 8, 6, 3],\n",
      "        [2, 4, 6, 8, 7],\n",
      "        [8, 4, 3, 5, 7],\n",
      "        [2, 3, 1, 0, 5],\n",
      "        [5, 6, 0, 2, 1]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.]])\n",
      "predicted values [tensor([[ 0.2626],\n",
      "        [ 0.3704],\n",
      "        [-0.3820],\n",
      "        [-0.3425],\n",
      "        [ 0.3704],\n",
      "        [-0.4406],\n",
      "        [ 0.8044],\n",
      "        [ 0.4317]], grad_fn=<AddmmBackward0>), tensor([[ 0.1886],\n",
      "        [-0.2772],\n",
      "        [ 0.1809],\n",
      "        [ 0.0751],\n",
      "        [-0.3236],\n",
      "        [ 0.4483],\n",
      "        [-0.3142],\n",
      "        [-0.2827]], grad_fn=<AddmmBackward0>), tensor([[ 0.0355],\n",
      "        [ 0.2935],\n",
      "        [-0.1111],\n",
      "        [-0.2157],\n",
      "        [ 0.0244],\n",
      "        [-0.4019],\n",
      "        [ 0.4033],\n",
      "        [ 0.2356]], grad_fn=<AddmmBackward0>), tensor([[-0.0522],\n",
      "        [-0.3179],\n",
      "        [ 0.2628],\n",
      "        [ 0.1743],\n",
      "        [-0.1688],\n",
      "        [ 0.2913],\n",
      "        [ 0.0994],\n",
      "        [-0.1481]], grad_fn=<AddmmBackward0>), tensor([[ 0.0561],\n",
      "        [ 0.0843],\n",
      "        [ 0.1304],\n",
      "        [-0.0326],\n",
      "        [ 0.2221],\n",
      "        [ 0.0266],\n",
      "        [ 0.0702],\n",
      "        [ 0.1926]], grad_fn=<AddmmBackward0>), tensor([[-0.1123],\n",
      "        [-0.1088],\n",
      "        [ 0.3647],\n",
      "        [ 0.2199],\n",
      "        [-0.1676],\n",
      "        [ 0.3450],\n",
      "        [ 0.0361],\n",
      "        [ 0.0283]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.4481],\n",
      "        [-0.0117],\n",
      "        [ 0.0855],\n",
      "        [-0.0831],\n",
      "        [-0.0371],\n",
      "        [-0.0465],\n",
      "        [ 0.4088],\n",
      "        [-0.0461]], grad_fn=<AddmmBackward0>), tensor([[ 0.0897],\n",
      "        [ 0.0241],\n",
      "        [ 0.1325],\n",
      "        [ 0.1504],\n",
      "        [-0.1032],\n",
      "        [ 0.1237],\n",
      "        [ 0.0913],\n",
      "        [ 0.0508]], grad_fn=<AddmmBackward0>), tensor([[0.1420],\n",
      "        [0.1527],\n",
      "        [0.1282],\n",
      "        [0.1289],\n",
      "        [0.1007],\n",
      "        [0.1140],\n",
      "        [0.3085],\n",
      "        [0.1162]], grad_fn=<AddmmBackward0>), tensor([[-0.0375],\n",
      "        [ 0.0559],\n",
      "        [ 0.2578],\n",
      "        [ 0.1280],\n",
      "        [ 0.0838],\n",
      "        [ 0.3124],\n",
      "        [-0.0178],\n",
      "        [ 0.0551]], grad_fn=<AddmmBackward0>), tensor([[-0.0435],\n",
      "        [ 0.2789],\n",
      "        [ 0.1862],\n",
      "        [ 0.0675],\n",
      "        [ 0.2300],\n",
      "        [ 0.2165],\n",
      "        [ 0.0872],\n",
      "        [ 0.1267]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.4442e-04, 9.9986e-01],\n",
      "        [6.5295e-05, 9.9993e-01],\n",
      "        [9.9973e-01, 2.6979e-04],\n",
      "        [9.9993e-01, 6.8819e-05],\n",
      "        [6.9182e-05, 9.9993e-01],\n",
      "        [9.9997e-01, 3.4615e-05],\n",
      "        [9.7054e-05, 9.9990e-01],\n",
      "        [2.0599e-04, 9.9979e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9833e-01, 1.6747e-03],\n",
      "        [9.9586e-01, 4.1428e-03],\n",
      "        [1.1306e-04, 9.9989e-01],\n",
      "        [1.0041e-05, 9.9999e-01],\n",
      "        [9.9916e-01, 8.4274e-04],\n",
      "        [3.3112e-05, 9.9997e-01],\n",
      "        [9.9833e-01, 1.6691e-03],\n",
      "        [9.9990e-01, 1.0227e-04]], grad_fn=<SoftmaxBackward0>), tensor([[4.0542e-05, 9.9996e-01],\n",
      "        [1.0018e-04, 9.9990e-01],\n",
      "        [9.9944e-01, 5.5869e-04],\n",
      "        [9.9966e-01, 3.4307e-04],\n",
      "        [7.9308e-05, 9.9992e-01],\n",
      "        [9.9927e-01, 7.3462e-04],\n",
      "        [2.7231e-03, 9.9728e-01],\n",
      "        [8.4175e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9379e-01, 6.2061e-03],\n",
      "        [9.9943e-01, 5.7395e-04],\n",
      "        [1.8670e-04, 9.9981e-01],\n",
      "        [2.6096e-04, 9.9974e-01],\n",
      "        [9.9983e-01, 1.6595e-04],\n",
      "        [4.5791e-04, 9.9954e-01],\n",
      "        [9.9874e-01, 1.2647e-03],\n",
      "        [9.9992e-01, 8.3978e-05]], grad_fn=<SoftmaxBackward0>), tensor([[7.7944e-04, 9.9922e-01],\n",
      "        [1.5104e-04, 9.9985e-01],\n",
      "        [9.9951e-01, 4.9126e-04],\n",
      "        [9.9876e-01, 1.2361e-03],\n",
      "        [8.5896e-05, 9.9991e-01],\n",
      "        [9.9937e-01, 6.2572e-04],\n",
      "        [1.7658e-04, 9.9982e-01],\n",
      "        [4.5941e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 1, 4, 0, 3],\n",
      "        [8, 2, 1, 3, 0],\n",
      "        [2, 4, 5, 8, 0],\n",
      "        [3, 2, 8, 1, 0],\n",
      "        [7, 1, 2, 0, 3],\n",
      "        [3, 8, 0, 2, 0],\n",
      "        [7, 0, 6, 6, 3],\n",
      "        [5, 4, 8, 0, 1]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.6145],\n",
      "        [ 0.3124],\n",
      "        [ 0.1153],\n",
      "        [ 0.0923],\n",
      "        [ 0.2647],\n",
      "        [-0.3822],\n",
      "        [ 0.1743],\n",
      "        [ 0.1747]], grad_fn=<AddmmBackward0>), tensor([[-0.1011],\n",
      "        [ 0.4007],\n",
      "        [-0.0420],\n",
      "        [-0.0699],\n",
      "        [ 0.0602],\n",
      "        [-0.0844],\n",
      "        [ 0.1648],\n",
      "        [-0.3542]], grad_fn=<AddmmBackward0>), tensor([[ 0.2692],\n",
      "        [ 0.1050],\n",
      "        [-0.0849],\n",
      "        [-0.0955],\n",
      "        [ 0.2428],\n",
      "        [-0.1857],\n",
      "        [ 0.0616],\n",
      "        [ 0.1032]], grad_fn=<AddmmBackward0>), tensor([[-0.0719],\n",
      "        [ 0.3312],\n",
      "        [-0.0007],\n",
      "        [-0.1126],\n",
      "        [ 0.1580],\n",
      "        [ 0.1008],\n",
      "        [ 0.0569],\n",
      "        [-0.1489]], grad_fn=<AddmmBackward0>), tensor([[ 0.1593],\n",
      "        [-0.1072],\n",
      "        [ 0.1369],\n",
      "        [-0.2264],\n",
      "        [-0.1545],\n",
      "        [ 0.0055],\n",
      "        [ 0.0049],\n",
      "        [ 0.1921]], grad_fn=<AddmmBackward0>), tensor([[ 0.0566],\n",
      "        [-0.1351],\n",
      "        [-0.0586],\n",
      "        [ 0.2428],\n",
      "        [ 0.0806],\n",
      "        [ 0.0208],\n",
      "        [-0.0561],\n",
      "        [ 0.0678]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0550],\n",
      "        [ 0.2700],\n",
      "        [ 0.1544],\n",
      "        [ 0.0931],\n",
      "        [ 0.1328],\n",
      "        [ 0.0675],\n",
      "        [ 0.4406],\n",
      "        [-0.0607]], grad_fn=<AddmmBackward0>), tensor([[ 0.1118],\n",
      "        [ 0.4101],\n",
      "        [ 0.0709],\n",
      "        [ 0.0979],\n",
      "        [ 0.1250],\n",
      "        [ 0.1316],\n",
      "        [ 0.0652],\n",
      "        [-0.0130]], grad_fn=<AddmmBackward0>), tensor([[0.3001],\n",
      "        [0.3169],\n",
      "        [0.2277],\n",
      "        [0.2129],\n",
      "        [0.4120],\n",
      "        [0.0083],\n",
      "        [0.0226],\n",
      "        [0.0863]], grad_fn=<AddmmBackward0>), tensor([[ 0.0685],\n",
      "        [ 0.4066],\n",
      "        [ 0.3317],\n",
      "        [ 0.1664],\n",
      "        [-0.0081],\n",
      "        [-0.0048],\n",
      "        [ 0.0250],\n",
      "        [ 0.0350]], grad_fn=<AddmmBackward0>), tensor([[ 0.0603],\n",
      "        [ 0.1096],\n",
      "        [ 0.2805],\n",
      "        [ 0.1641],\n",
      "        [-0.0681],\n",
      "        [ 0.0158],\n",
      "        [ 0.0020],\n",
      "        [ 0.1399]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.4016e-03, 9.9860e-01],\n",
      "        [9.9907e-01, 9.2997e-04],\n",
      "        [1.6994e-04, 9.9983e-01],\n",
      "        [9.9995e-01, 5.4753e-05],\n",
      "        [9.2318e-03, 9.9077e-01],\n",
      "        [9.9998e-01, 1.6195e-05],\n",
      "        [1.5319e-01, 8.4681e-01],\n",
      "        [1.0920e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9907e-01, 9.3156e-04],\n",
      "        [6.2268e-04, 9.9938e-01],\n",
      "        [9.9913e-01, 8.7240e-04],\n",
      "        [7.2405e-05, 9.9993e-01],\n",
      "        [9.9960e-01, 3.9600e-04],\n",
      "        [4.8128e-05, 9.9995e-01],\n",
      "        [9.8971e-01, 1.0290e-02],\n",
      "        [9.9964e-01, 3.6177e-04]], grad_fn=<SoftmaxBackward0>), tensor([[1.0307e-04, 9.9990e-01],\n",
      "        [9.9775e-01, 2.2541e-03],\n",
      "        [1.3282e-04, 9.9987e-01],\n",
      "        [9.9957e-01, 4.3097e-04],\n",
      "        [7.6775e-05, 9.9992e-01],\n",
      "        [9.9938e-01, 6.1569e-04],\n",
      "        [2.3059e-03, 9.9769e-01],\n",
      "        [1.7018e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9993e-01, 7.3714e-05],\n",
      "        [2.4935e-05, 9.9998e-01],\n",
      "        [9.9739e-01, 2.6073e-03],\n",
      "        [3.0354e-05, 9.9997e-01],\n",
      "        [9.9983e-01, 1.7309e-04],\n",
      "        [9.3249e-05, 9.9991e-01],\n",
      "        [9.8464e-01, 1.5364e-02],\n",
      "        [9.9988e-01, 1.2040e-04]], grad_fn=<SoftmaxBackward0>), tensor([[7.6897e-05, 9.9992e-01],\n",
      "        [9.9992e-01, 7.8924e-05],\n",
      "        [2.5229e-04, 9.9975e-01],\n",
      "        [9.9995e-01, 4.7792e-05],\n",
      "        [1.2696e-04, 9.9987e-01],\n",
      "        [9.9897e-01, 1.0316e-03],\n",
      "        [2.2935e-03, 9.9771e-01],\n",
      "        [6.2521e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 2, 3, 7, 8],\n",
      "        [2, 1, 7, 3, 6],\n",
      "        [2, 0, 3, 8, 5],\n",
      "        [1, 8, 7, 0, 5],\n",
      "        [5, 0, 3, 2, 6],\n",
      "        [2, 7, 0, 8, 5],\n",
      "        [7, 0, 3, 8, 5],\n",
      "        [8, 3, 2, 0, 5]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.3358],\n",
      "        [ 0.5087],\n",
      "        [-0.0009],\n",
      "        [ 0.0483],\n",
      "        [ 0.2434],\n",
      "        [ 0.1494],\n",
      "        [ 0.1025],\n",
      "        [ 0.1305]], grad_fn=<AddmmBackward0>), tensor([[ 0.4453],\n",
      "        [-0.2203],\n",
      "        [ 0.6660],\n",
      "        [-0.1320],\n",
      "        [-0.1434],\n",
      "        [ 0.3733],\n",
      "        [ 0.0007],\n",
      "        [-0.1638]], grad_fn=<AddmmBackward0>), tensor([[ 0.1064],\n",
      "        [ 0.4065],\n",
      "        [-0.1288],\n",
      "        [ 0.3118],\n",
      "        [ 0.1539],\n",
      "        [ 0.0145],\n",
      "        [ 0.2885],\n",
      "        [ 0.1710]], grad_fn=<AddmmBackward0>), tensor([[ 0.4564],\n",
      "        [-0.1483],\n",
      "        [ 0.2819],\n",
      "        [ 0.0828],\n",
      "        [-0.0429],\n",
      "        [ 0.1017],\n",
      "        [-0.0390],\n",
      "        [-0.0561]], grad_fn=<AddmmBackward0>), tensor([[ 0.0413],\n",
      "        [ 0.3407],\n",
      "        [-0.2268],\n",
      "        [ 0.2811],\n",
      "        [ 0.2901],\n",
      "        [-0.1148],\n",
      "        [ 0.1845],\n",
      "        [ 0.1121]], grad_fn=<AddmmBackward0>), tensor([[ 0.4013],\n",
      "        [ 0.1847],\n",
      "        [ 0.2138],\n",
      "        [-0.0995],\n",
      "        [ 0.1895],\n",
      "        [ 0.1482],\n",
      "        [ 0.1115],\n",
      "        [-0.0351]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0182],\n",
      "        [-0.0578],\n",
      "        [ 0.2534],\n",
      "        [ 0.0003],\n",
      "        [-0.0386],\n",
      "        [ 0.0150],\n",
      "        [ 0.1131],\n",
      "        [ 0.0146]], grad_fn=<AddmmBackward0>), tensor([[ 0.1907],\n",
      "        [ 0.0431],\n",
      "        [ 0.5104],\n",
      "        [ 0.1408],\n",
      "        [-0.1060],\n",
      "        [ 0.2396],\n",
      "        [-0.0336],\n",
      "        [ 0.1044]], grad_fn=<AddmmBackward0>), tensor([[ 0.1806],\n",
      "        [ 0.1450],\n",
      "        [ 0.1558],\n",
      "        [ 0.1678],\n",
      "        [-0.0007],\n",
      "        [-0.0257],\n",
      "        [ 0.0524],\n",
      "        [ 0.2423]], grad_fn=<AddmmBackward0>), tensor([[ 0.2370],\n",
      "        [ 0.1364],\n",
      "        [ 0.2366],\n",
      "        [ 0.0393],\n",
      "        [ 0.0748],\n",
      "        [-0.0396],\n",
      "        [ 0.0279],\n",
      "        [ 0.0834]], grad_fn=<AddmmBackward0>), tensor([[ 0.2613],\n",
      "        [ 0.3319],\n",
      "        [ 0.1011],\n",
      "        [ 0.0118],\n",
      "        [ 0.1638],\n",
      "        [-0.0437],\n",
      "        [ 0.1640],\n",
      "        [ 0.0354]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 1.6334e-05],\n",
      "        [4.4581e-04, 9.9955e-01],\n",
      "        [9.9996e-01, 3.6867e-05],\n",
      "        [5.4359e-04, 9.9946e-01],\n",
      "        [8.2704e-05, 9.9992e-01],\n",
      "        [9.9998e-01, 2.4741e-05],\n",
      "        [2.6738e-04, 9.9973e-01],\n",
      "        [5.9608e-04, 9.9940e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.7898e-05, 9.9996e-01],\n",
      "        [9.9984e-01, 1.5965e-04],\n",
      "        [3.4355e-05, 9.9997e-01],\n",
      "        [9.9987e-01, 1.3353e-04],\n",
      "        [9.9971e-01, 2.9135e-04],\n",
      "        [9.5187e-03, 9.9048e-01],\n",
      "        [9.9672e-01, 3.2752e-03],\n",
      "        [9.9980e-01, 2.0170e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 2.5717e-05],\n",
      "        [3.0452e-04, 9.9970e-01],\n",
      "        [9.9985e-01, 1.5482e-04],\n",
      "        [8.1739e-04, 9.9918e-01],\n",
      "        [7.9746e-06, 9.9999e-01],\n",
      "        [9.9212e-01, 7.8817e-03],\n",
      "        [6.6458e-03, 9.9335e-01],\n",
      "        [3.2584e-04, 9.9967e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.1502e-04, 9.9988e-01],\n",
      "        [9.9872e-01, 1.2769e-03],\n",
      "        [3.0873e-04, 9.9969e-01],\n",
      "        [9.9856e-01, 1.4376e-03],\n",
      "        [9.9977e-01, 2.2970e-04],\n",
      "        [1.0380e-02, 9.8962e-01],\n",
      "        [9.9287e-01, 7.1321e-03],\n",
      "        [9.9979e-01, 2.0877e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9996e-01, 4.1312e-05],\n",
      "        [1.7702e-04, 9.9982e-01],\n",
      "        [9.9969e-01, 3.1467e-04],\n",
      "        [3.8768e-04, 9.9961e-01],\n",
      "        [5.1914e-05, 9.9995e-01],\n",
      "        [8.6053e-01, 1.3947e-01],\n",
      "        [1.7833e-03, 9.9822e-01],\n",
      "        [1.8941e-03, 9.9811e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 0, 0, 2, 3],\n",
      "        [1, 7, 5, 6, 0],\n",
      "        [8, 0, 0, 2, 3],\n",
      "        [5, 8, 3, 0, 4],\n",
      "        [4, 3, 0, 0, 3],\n",
      "        [1, 4, 2, 6, 5],\n",
      "        [8, 0, 6, 1, 0],\n",
      "        [0, 7, 6, 2, 4]])\n",
      "target value tensor([[-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[-0.1289],\n",
      "        [-0.2786],\n",
      "        [ 0.7587],\n",
      "        [ 0.2320],\n",
      "        [-0.3716],\n",
      "        [ 0.4252],\n",
      "        [-0.1665],\n",
      "        [-0.4722]], grad_fn=<AddmmBackward0>), tensor([[ 0.5942],\n",
      "        [ 0.3963],\n",
      "        [ 0.0331],\n",
      "        [-0.0925],\n",
      "        [ 0.0676],\n",
      "        [-0.2235],\n",
      "        [ 0.4800],\n",
      "        [ 0.1790]], grad_fn=<AddmmBackward0>), tensor([[ 0.1241],\n",
      "        [-0.2613],\n",
      "        [ 0.1214],\n",
      "        [-0.0290],\n",
      "        [ 0.1211],\n",
      "        [ 0.4612],\n",
      "        [ 0.1336],\n",
      "        [-0.0977]], grad_fn=<AddmmBackward0>), tensor([[ 0.2046],\n",
      "        [ 0.1869],\n",
      "        [-0.1817],\n",
      "        [-0.1440],\n",
      "        [ 0.0889],\n",
      "        [ 0.0887],\n",
      "        [ 0.2766],\n",
      "        [ 0.3717]], grad_fn=<AddmmBackward0>), tensor([[-0.0582],\n",
      "        [-0.0980],\n",
      "        [ 0.0247],\n",
      "        [-0.0059],\n",
      "        [ 0.0233],\n",
      "        [ 0.3166],\n",
      "        [ 0.1820],\n",
      "        [ 0.1595]], grad_fn=<AddmmBackward0>), tensor([[ 0.1327],\n",
      "        [-0.0033],\n",
      "        [-0.1540],\n",
      "        [ 0.0157],\n",
      "        [ 0.0281],\n",
      "        [-0.1042],\n",
      "        [ 0.1849],\n",
      "        [ 0.0874]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0758],\n",
      "        [-0.1187],\n",
      "        [ 0.7104],\n",
      "        [ 0.0081],\n",
      "        [-0.0051],\n",
      "        [ 0.1778],\n",
      "        [-0.0227],\n",
      "        [-0.0726]], grad_fn=<AddmmBackward0>), tensor([[ 0.3982],\n",
      "        [ 0.1274],\n",
      "        [ 0.1119],\n",
      "        [ 0.1539],\n",
      "        [ 0.1292],\n",
      "        [ 0.2946],\n",
      "        [ 0.1003],\n",
      "        [-0.0257]], grad_fn=<AddmmBackward0>), tensor([[-0.0129],\n",
      "        [ 0.0625],\n",
      "        [-0.0398],\n",
      "        [ 0.1375],\n",
      "        [ 0.1472],\n",
      "        [ 0.5360],\n",
      "        [ 0.3206],\n",
      "        [-0.0062]], grad_fn=<AddmmBackward0>), tensor([[ 0.0387],\n",
      "        [ 0.3804],\n",
      "        [-0.1356],\n",
      "        [ 0.0624],\n",
      "        [ 0.0386],\n",
      "        [ 0.3590],\n",
      "        [ 0.2981],\n",
      "        [ 0.0280]], grad_fn=<AddmmBackward0>), tensor([[-0.0078],\n",
      "        [ 0.0568],\n",
      "        [-0.0568],\n",
      "        [ 0.2685],\n",
      "        [-0.0524],\n",
      "        [ 0.4022],\n",
      "        [ 0.0138],\n",
      "        [ 0.0996]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9987e-01, 1.2543e-04],\n",
      "        [9.9998e-01, 1.9633e-05],\n",
      "        [1.0007e-03, 9.9900e-01],\n",
      "        [9.8656e-05, 9.9990e-01],\n",
      "        [9.9997e-01, 2.8001e-05],\n",
      "        [2.3115e-04, 9.9977e-01],\n",
      "        [9.9987e-01, 1.2898e-04],\n",
      "        [9.9996e-01, 3.5679e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.3725e-05, 9.9999e-01],\n",
      "        [1.9082e-04, 9.9981e-01],\n",
      "        [9.9985e-01, 1.5258e-04],\n",
      "        [9.9774e-01, 2.2566e-03],\n",
      "        [1.5646e-04, 9.9984e-01],\n",
      "        [9.9999e-01, 8.6140e-06],\n",
      "        [3.3134e-04, 9.9967e-01],\n",
      "        [6.9707e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 2.4733e-05],\n",
      "        [9.9989e-01, 1.1185e-04],\n",
      "        [8.9587e-04, 9.9910e-01],\n",
      "        [1.4685e-04, 9.9985e-01],\n",
      "        [9.9991e-01, 8.6204e-05],\n",
      "        [1.6098e-05, 9.9998e-01],\n",
      "        [9.9987e-01, 1.2992e-04],\n",
      "        [9.9993e-01, 7.4872e-05]], grad_fn=<SoftmaxBackward0>), tensor([[6.4428e-05, 9.9994e-01],\n",
      "        [3.7363e-04, 9.9963e-01],\n",
      "        [9.9858e-01, 1.4231e-03],\n",
      "        [9.9849e-01, 1.5059e-03],\n",
      "        [2.8525e-03, 9.9715e-01],\n",
      "        [9.9999e-01, 1.4704e-05],\n",
      "        [1.3226e-04, 9.9987e-01],\n",
      "        [2.1434e-04, 9.9979e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9997e-01, 2.5525e-05],\n",
      "        [9.9976e-01, 2.4186e-04],\n",
      "        [2.8478e-03, 9.9715e-01],\n",
      "        [3.1403e-05, 9.9997e-01],\n",
      "        [9.9979e-01, 2.0711e-04],\n",
      "        [1.3483e-05, 9.9999e-01],\n",
      "        [9.9988e-01, 1.2065e-04],\n",
      "        [9.9992e-01, 7.9662e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0400, 0.0400, 0.1200, 0.0400, 0.1200, 0.0400, 0.3200, 0.0000, 0.2800]), tensor([1.5987e-06, 7.6333e-08, 3.2001e-01, 3.3416e-06, 4.3291e-03, 9.6136e-02,\n",
      "        1.0286e-02, 4.4174e-04, 5.6879e-01]), 0.3199116140604019, tensor(8))\n",
      "action: 8\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.3600, 0.0400, 0.1200, 0.0000, 0.0400, 0.3600, 0.0400, 0.0400, 0.0000]), tensor([8.3604e-01, 1.6253e-03, 2.0473e-04, 0.0000e+00, 6.8110e-06, 8.0621e-02,\n",
      "        8.1486e-02, 1.7416e-05, 0.0000e+00]), 0.2627273169704355, tensor(0))\n",
      "action: 0\n",
      "Player 1 random action: 5\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0800, 0.3600, 0.0000, 0.0800, 0.0000, 0.4000, 0.0800, 0.0000]), tensor([0.0000, 0.0035, 0.0597, 0.0000, 0.0024, 0.0000, 0.9280, 0.0063, 0.0000]), 0.397431689959306, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 2\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.4000, 0.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.2000, 0.0000]), tensor([0.0000e+00, 3.2276e-04, 0.0000e+00, 0.0000e+00, 9.9911e-01, 0.0000e+00,\n",
      "        0.0000e+00, 5.6783e-04, 0.0000e+00]), 0.2590516861528158, tensor(4))\n",
      "action: 4\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 8, 1, 0, 8],\n",
      "        [1, 8, 0, 2, 8],\n",
      "        [8, 7, 5, 1, 0],\n",
      "        [2, 6, 4, 1, 7],\n",
      "        [6, 0, 3, 7, 1],\n",
      "        [8, 6, 0, 2, 8],\n",
      "        [2, 7, 4, 5, 6],\n",
      "        [0, 6, 7, 1, 0]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.4128],\n",
      "        [ 0.1443],\n",
      "        [-0.0627],\n",
      "        [ 0.3585],\n",
      "        [-0.4291],\n",
      "        [-0.4475],\n",
      "        [-0.2647],\n",
      "        [ 0.0666]], grad_fn=<AddmmBackward0>), tensor([[ 0.2159],\n",
      "        [-0.1217],\n",
      "        [ 0.2156],\n",
      "        [ 0.2598],\n",
      "        [ 0.0749],\n",
      "        [ 0.3434],\n",
      "        [ 0.3557],\n",
      "        [ 0.1542]], grad_fn=<AddmmBackward0>), tensor([[ 0.3711],\n",
      "        [ 0.0764],\n",
      "        [ 0.0356],\n",
      "        [-0.0486],\n",
      "        [-0.0137],\n",
      "        [-0.0247],\n",
      "        [ 0.0830],\n",
      "        [ 0.0278]], grad_fn=<AddmmBackward0>), tensor([[ 0.3504],\n",
      "        [-0.2144],\n",
      "        [ 0.3295],\n",
      "        [ 0.3234],\n",
      "        [ 0.2150],\n",
      "        [ 0.2971],\n",
      "        [-0.1158],\n",
      "        [ 0.3544]], grad_fn=<AddmmBackward0>), tensor([[-0.1286],\n",
      "        [ 0.1851],\n",
      "        [ 0.0613],\n",
      "        [ 0.0871],\n",
      "        [ 0.0807],\n",
      "        [ 0.0645],\n",
      "        [ 0.0849],\n",
      "        [ 0.0246]], grad_fn=<AddmmBackward0>), tensor([[ 0.1396],\n",
      "        [-0.1662],\n",
      "        [ 0.1935],\n",
      "        [ 0.7133],\n",
      "        [ 0.3595],\n",
      "        [ 0.2531],\n",
      "        [-0.0947],\n",
      "        [ 0.2861]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.3013],\n",
      "        [ 0.1897],\n",
      "        [ 0.2949],\n",
      "        [ 0.0077],\n",
      "        [ 0.0276],\n",
      "        [ 0.1918],\n",
      "        [-0.1360],\n",
      "        [ 0.2354]], grad_fn=<AddmmBackward0>), tensor([[ 0.2988],\n",
      "        [ 0.1593],\n",
      "        [ 0.2570],\n",
      "        [ 0.1116],\n",
      "        [ 0.1146],\n",
      "        [ 0.5115],\n",
      "        [-0.0447],\n",
      "        [ 0.3323]], grad_fn=<AddmmBackward0>), tensor([[ 0.1729],\n",
      "        [ 0.0018],\n",
      "        [ 0.3245],\n",
      "        [ 0.1260],\n",
      "        [ 0.3353],\n",
      "        [ 0.1439],\n",
      "        [-0.0294],\n",
      "        [ 0.3348]], grad_fn=<AddmmBackward0>), tensor([[ 0.0556],\n",
      "        [-0.0250],\n",
      "        [ 0.4484],\n",
      "        [ 0.3203],\n",
      "        [ 0.2918],\n",
      "        [ 0.2177],\n",
      "        [ 0.1409],\n",
      "        [ 0.5041]], grad_fn=<AddmmBackward0>), tensor([[-0.0490],\n",
      "        [ 0.0689],\n",
      "        [ 0.0176],\n",
      "        [ 0.3036],\n",
      "        [ 0.2448],\n",
      "        [ 0.0354],\n",
      "        [ 0.3388],\n",
      "        [ 0.0115]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9976e-01, 2.3972e-04],\n",
      "        [2.7354e-05, 9.9997e-01],\n",
      "        [9.9983e-01, 1.6665e-04],\n",
      "        [9.9971e-01, 2.9167e-04],\n",
      "        [9.9988e-01, 1.2339e-04],\n",
      "        [9.9998e-01, 2.3664e-05],\n",
      "        [9.9996e-01, 3.6985e-05],\n",
      "        [9.9994e-01, 6.2117e-05]], grad_fn=<SoftmaxBackward0>), tensor([[5.8226e-03, 9.9418e-01],\n",
      "        [9.9999e-01, 5.7746e-06],\n",
      "        [2.5974e-03, 9.9740e-01],\n",
      "        [1.1469e-04, 9.9989e-01],\n",
      "        [1.0666e-03, 9.9893e-01],\n",
      "        [4.1617e-05, 9.9996e-01],\n",
      "        [7.7550e-05, 9.9992e-01],\n",
      "        [5.6806e-04, 9.9943e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9973e-01, 2.7170e-04],\n",
      "        [2.0741e-04, 9.9979e-01],\n",
      "        [9.9949e-01, 5.1294e-04],\n",
      "        [9.9992e-01, 7.5516e-05],\n",
      "        [9.9847e-01, 1.5314e-03],\n",
      "        [9.9991e-01, 9.3847e-05],\n",
      "        [9.9955e-01, 4.4840e-04],\n",
      "        [9.9999e-01, 8.6964e-06]], grad_fn=<SoftmaxBackward0>), tensor([[7.1597e-03, 9.9284e-01],\n",
      "        [9.9992e-01, 8.3023e-05],\n",
      "        [1.1952e-03, 9.9880e-01],\n",
      "        [4.0894e-06, 1.0000e+00],\n",
      "        [6.3177e-02, 9.3682e-01],\n",
      "        [2.3365e-04, 9.9977e-01],\n",
      "        [1.6859e-03, 9.9831e-01],\n",
      "        [3.3762e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>), tensor([[9.8547e-01, 1.4534e-02],\n",
      "        [2.5788e-04, 9.9974e-01],\n",
      "        [9.9992e-01, 8.2637e-05],\n",
      "        [1.0000e+00, 2.2774e-06],\n",
      "        [9.9496e-01, 5.0383e-03],\n",
      "        [9.9985e-01, 1.5234e-04],\n",
      "        [9.9448e-01, 5.5224e-03],\n",
      "        [9.9999e-01, 6.3719e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "Player 0 win percentage vs random: 74.0 and average score: 0.56\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 2\n",
      "Player 1 prediction: (tensor([0.0400, 0.0400, 0.0000, 0.2800, 0.3200, 0.0400, 0.1200, 0.0400, 0.1200]), tensor([1.9631e-04, 3.9339e-12, 0.0000e+00, 3.0069e-01, 6.5574e-01, 2.2419e-03,\n",
      "        1.1876e-03, 1.1793e-04, 3.9821e-02]), -0.20662854368296954, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 5\n",
      "Player 1 prediction: (tensor([0.3600, 0.0400, 0.0000, 0.0400, 0.0000, 0.0000, 0.1600, 0.0400, 0.3600]), tensor([6.9571e-02, 2.4504e-04, 0.0000e+00, 1.0415e-01, 0.0000e+00, 0.0000e+00,\n",
      "        3.8867e-03, 2.7973e-02, 7.9418e-01]), 0.06964195068352498, tensor(8))\n",
      "action: 8\n",
      "Player 0 random action: 1\n",
      "Player 1 prediction: (tensor([0.4000, 0.0000, 0.0000, 0.3600, 0.0000, 0.0000, 0.1200, 0.1200, 0.0000]), tensor([5.7375e-01, 0.0000e+00, 0.0000e+00, 4.2539e-01, 0.0000e+00, 0.0000e+00,\n",
      "        7.7694e-05, 7.8408e-04, 0.0000e+00]), -0.0660227183252573, tensor(0))\n",
      "action: 0\n",
      "1200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 6, 0, 8, 3],\n",
      "        [5, 4, 6, 3, 2],\n",
      "        [8, 4, 0, 5, 7],\n",
      "        [1, 3, 0, 8, 3],\n",
      "        [3, 4, 7, 1, 6],\n",
      "        [6, 2, 0, 0, 3],\n",
      "        [2, 6, 5, 1, 8],\n",
      "        [1, 4, 7, 6, 3]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.0993],\n",
      "        [ 0.2119],\n",
      "        [-0.2253],\n",
      "        [-0.2098],\n",
      "        [ 0.3054],\n",
      "        [ 0.2686],\n",
      "        [-0.4243],\n",
      "        [ 0.2822]], grad_fn=<AddmmBackward0>), tensor([[ 0.6232],\n",
      "        [-0.2555],\n",
      "        [ 0.2444],\n",
      "        [ 0.2460],\n",
      "        [-0.0225],\n",
      "        [ 0.2323],\n",
      "        [ 0.4626],\n",
      "        [-0.1509]], grad_fn=<AddmmBackward0>), tensor([[-0.0760],\n",
      "        [ 0.1159],\n",
      "        [-0.1746],\n",
      "        [-0.1057],\n",
      "        [ 0.0493],\n",
      "        [ 0.0606],\n",
      "        [-0.1623],\n",
      "        [ 0.0232]], grad_fn=<AddmmBackward0>), tensor([[ 0.0314],\n",
      "        [-0.2481],\n",
      "        [ 0.2641],\n",
      "        [ 0.0908],\n",
      "        [ 0.1825],\n",
      "        [ 0.2567],\n",
      "        [ 0.1634],\n",
      "        [ 0.0693]], grad_fn=<AddmmBackward0>), tensor([[-0.0213],\n",
      "        [ 0.1884],\n",
      "        [ 0.0343],\n",
      "        [ 0.1006],\n",
      "        [ 0.3294],\n",
      "        [ 0.0296],\n",
      "        [-0.1173],\n",
      "        [ 0.0736]], grad_fn=<AddmmBackward0>), tensor([[ 0.1770],\n",
      "        [-0.0718],\n",
      "        [ 0.1251],\n",
      "        [ 0.3433],\n",
      "        [ 0.2475],\n",
      "        [ 0.2782],\n",
      "        [ 0.1362],\n",
      "        [-0.1041]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1660],\n",
      "        [ 0.0166],\n",
      "        [ 0.0904],\n",
      "        [ 0.3788],\n",
      "        [ 0.1906],\n",
      "        [ 0.0252],\n",
      "        [-0.1342],\n",
      "        [ 0.0968]], grad_fn=<AddmmBackward0>), tensor([[ 0.6258],\n",
      "        [-0.0474],\n",
      "        [ 0.1319],\n",
      "        [ 0.5987],\n",
      "        [ 0.1101],\n",
      "        [ 0.1784],\n",
      "        [ 0.2017],\n",
      "        [ 0.0946]], grad_fn=<AddmmBackward0>), tensor([[ 0.0074],\n",
      "        [-0.0029],\n",
      "        [ 0.0314],\n",
      "        [ 0.0010],\n",
      "        [ 0.1043],\n",
      "        [ 0.2393],\n",
      "        [ 0.0665],\n",
      "        [ 0.1971]], grad_fn=<AddmmBackward0>), tensor([[0.0101],\n",
      "        [0.0941],\n",
      "        [0.2027],\n",
      "        [0.0421],\n",
      "        [0.1228],\n",
      "        [0.0330],\n",
      "        [0.1810],\n",
      "        [0.2629]], grad_fn=<AddmmBackward0>), tensor([[-0.0923],\n",
      "        [ 0.1680],\n",
      "        [ 0.0890],\n",
      "        [-0.0364],\n",
      "        [ 0.4920],\n",
      "        [ 0.0725],\n",
      "        [ 0.2883],\n",
      "        [ 0.3217]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9989e-01, 1.0857e-04],\n",
      "        [2.3220e-05, 9.9998e-01],\n",
      "        [9.9966e-01, 3.4253e-04],\n",
      "        [9.9773e-01, 2.2712e-03],\n",
      "        [4.3254e-05, 9.9996e-01],\n",
      "        [9.9992e-01, 8.1738e-05],\n",
      "        [9.9984e-01, 1.6116e-04],\n",
      "        [6.3438e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>), tensor([[7.6604e-06, 9.9999e-01],\n",
      "        [9.9972e-01, 2.7558e-04],\n",
      "        [4.3613e-05, 9.9996e-01],\n",
      "        [9.1408e-06, 9.9999e-01],\n",
      "        [9.9726e-01, 2.7425e-03],\n",
      "        [1.0524e-05, 9.9999e-01],\n",
      "        [4.4245e-05, 9.9996e-01],\n",
      "        [9.9997e-01, 3.4071e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9964e-01, 3.6136e-04],\n",
      "        [7.8487e-06, 9.9999e-01],\n",
      "        [9.9840e-01, 1.5957e-03],\n",
      "        [9.9965e-01, 3.5259e-04],\n",
      "        [3.0484e-05, 9.9997e-01],\n",
      "        [9.9948e-01, 5.2120e-04],\n",
      "        [9.9990e-01, 9.5102e-05],\n",
      "        [1.9868e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.4658e-05, 9.9991e-01],\n",
      "        [9.9968e-01, 3.1762e-04],\n",
      "        [2.9631e-05, 9.9997e-01],\n",
      "        [2.7717e-05, 9.9997e-01],\n",
      "        [9.9951e-01, 4.9430e-04],\n",
      "        [1.9804e-04, 9.9980e-01],\n",
      "        [1.4149e-04, 9.9986e-01],\n",
      "        [9.9998e-01, 2.1023e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9718e-01, 2.8177e-03],\n",
      "        [3.7456e-06, 1.0000e+00],\n",
      "        [9.9978e-01, 2.2257e-04],\n",
      "        [9.9954e-01, 4.6417e-04],\n",
      "        [6.2530e-05, 9.9994e-01],\n",
      "        [9.9254e-01, 7.4621e-03],\n",
      "        [9.9914e-01, 8.6428e-04],\n",
      "        [2.2507e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 4, 5, 0, 0],\n",
      "        [1, 0, 6, 1, 7],\n",
      "        [8, 5, 6, 7, 0],\n",
      "        [6, 7, 0, 0, 7],\n",
      "        [7, 0, 6, 1, 7],\n",
      "        [6, 5, 0, 8, 1],\n",
      "        [2, 6, 1, 5, 0],\n",
      "        [8, 0, 2, 5, 0]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.0142],\n",
      "        [ 0.6926],\n",
      "        [-0.1493],\n",
      "        [ 0.0751],\n",
      "        [ 0.2959],\n",
      "        [ 0.0896],\n",
      "        [ 0.0533],\n",
      "        [-0.1870]], grad_fn=<AddmmBackward0>), tensor([[ 0.2428],\n",
      "        [ 0.2184],\n",
      "        [ 0.1980],\n",
      "        [ 0.1927],\n",
      "        [-0.0891],\n",
      "        [-0.1149],\n",
      "        [-0.0804],\n",
      "        [ 0.6840]], grad_fn=<AddmmBackward0>), tensor([[0.0226],\n",
      "        [0.2178],\n",
      "        [0.2359],\n",
      "        [0.2188],\n",
      "        [0.0545],\n",
      "        [0.4025],\n",
      "        [0.1880],\n",
      "        [0.0418]], grad_fn=<AddmmBackward0>), tensor([[ 0.2707],\n",
      "        [ 0.0279],\n",
      "        [ 0.2129],\n",
      "        [ 0.3896],\n",
      "        [-0.1172],\n",
      "        [-0.0233],\n",
      "        [-0.1187],\n",
      "        [ 0.1846]], grad_fn=<AddmmBackward0>), tensor([[0.0327],\n",
      "        [0.1820],\n",
      "        [0.1441],\n",
      "        [0.1086],\n",
      "        [0.1360],\n",
      "        [0.3483],\n",
      "        [0.3637],\n",
      "        [0.1417]], grad_fn=<AddmmBackward0>), tensor([[ 0.1552],\n",
      "        [ 0.1486],\n",
      "        [ 0.0996],\n",
      "        [ 0.0951],\n",
      "        [-0.1157],\n",
      "        [ 0.2206],\n",
      "        [-0.0663],\n",
      "        [ 0.0103]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0597],\n",
      "        [ 0.4853],\n",
      "        [ 0.1337],\n",
      "        [-0.0435],\n",
      "        [ 0.0100],\n",
      "        [-0.0997],\n",
      "        [-0.0908],\n",
      "        [ 0.1376]], grad_fn=<AddmmBackward0>), tensor([[ 0.0314],\n",
      "        [ 0.0693],\n",
      "        [ 0.3249],\n",
      "        [ 0.0953],\n",
      "        [-0.0212],\n",
      "        [-0.0666],\n",
      "        [-0.1264],\n",
      "        [ 0.4569]], grad_fn=<AddmmBackward0>), tensor([[ 0.2887],\n",
      "        [ 0.0722],\n",
      "        [ 0.4089],\n",
      "        [ 0.1870],\n",
      "        [ 0.0459],\n",
      "        [ 0.0750],\n",
      "        [-0.0477],\n",
      "        [ 0.4414]], grad_fn=<AddmmBackward0>), tensor([[0.1495],\n",
      "        [0.0560],\n",
      "        [0.4375],\n",
      "        [0.0030],\n",
      "        [0.0066],\n",
      "        [0.1164],\n",
      "        [0.0381],\n",
      "        [0.5222]], grad_fn=<AddmmBackward0>), tensor([[0.0333],\n",
      "        [0.1192],\n",
      "        [0.0621],\n",
      "        [0.0328],\n",
      "        [0.1281],\n",
      "        [0.3453],\n",
      "        [0.0958],\n",
      "        [0.1103]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[1.5569e-02, 9.8443e-01],\n",
      "        [6.4494e-04, 9.9936e-01],\n",
      "        [9.9982e-01, 1.8445e-04],\n",
      "        [9.9993e-01, 7.3172e-05],\n",
      "        [2.8777e-04, 9.9971e-01],\n",
      "        [6.0031e-05, 9.9994e-01],\n",
      "        [2.3186e-05, 9.9998e-01],\n",
      "        [9.9980e-01, 1.9698e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9692e-01, 3.0758e-03],\n",
      "        [9.9801e-01, 1.9898e-03],\n",
      "        [4.6951e-06, 1.0000e+00],\n",
      "        [4.0231e-06, 1.0000e+00],\n",
      "        [9.9885e-01, 1.1511e-03],\n",
      "        [9.8327e-01, 1.6727e-02],\n",
      "        [9.9979e-01, 2.1475e-04],\n",
      "        [2.7171e-04, 9.9973e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.1378e-04, 9.9959e-01],\n",
      "        [7.7091e-04, 9.9923e-01],\n",
      "        [9.9983e-01, 1.6972e-04],\n",
      "        [9.9958e-01, 4.2079e-04],\n",
      "        [5.6886e-03, 9.9431e-01],\n",
      "        [3.2373e-02, 9.6763e-01],\n",
      "        [2.5946e-05, 9.9997e-01],\n",
      "        [9.9991e-01, 8.7278e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9992e-01, 7.8969e-05],\n",
      "        [9.9944e-01, 5.6052e-04],\n",
      "        [1.4651e-05, 9.9999e-01],\n",
      "        [6.1490e-04, 9.9939e-01],\n",
      "        [9.9963e-01, 3.6603e-04],\n",
      "        [9.2594e-01, 7.4061e-02],\n",
      "        [9.9950e-01, 5.0448e-04],\n",
      "        [3.7961e-04, 9.9962e-01]], grad_fn=<SoftmaxBackward0>), tensor([[5.9015e-03, 9.9410e-01],\n",
      "        [3.5790e-03, 9.9642e-01],\n",
      "        [9.9981e-01, 1.8636e-04],\n",
      "        [9.8927e-01, 1.0731e-02],\n",
      "        [1.6265e-03, 9.9837e-01],\n",
      "        [2.2676e-01, 7.7324e-01],\n",
      "        [3.4766e-04, 9.9965e-01],\n",
      "        [9.9986e-01, 1.4255e-04]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 50.0 and average score: 0.08\n",
      "Results vs random: {'player_0_score': 0.56, 'player_0_win%': 0.74, 'player_1_score': 0.08, 'player_1_win%': 0.5, 'score': 0.32}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0400, 0.0400, 0.3200, 0.0400, 0.2800, 0.0400, 0.1200, 0.0000, 0.1200]), tensor([8.6881e-07, 8.7973e-05, 3.2363e-01, 2.5740e-03, 5.3609e-01, 5.3452e-02,\n",
      "        5.7995e-02, 3.2482e-03, 2.2920e-02]), 0.35767835049101937, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1200, 0.3600, 0.3600, 0.0000, 0.0000, 0.0400, 0.0400, 0.0400, 0.0400]), tensor([1.0172e-01, 2.5626e-01, 6.1345e-01, 0.0000e+00, 0.0000e+00, 9.2180e-07,\n",
      "        1.1662e-04, 2.8451e-02, 1.2925e-06]), 0.2602606199395198, tensor(2))\n",
      "action: 2\n",
      "Player 1 tictactoe_expert action: 6\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.4000, 0.3600, 0.0000, 0.0000, 0.0000, 0.0800, 0.0000, 0.0800, 0.0800]), tensor([9.3018e-01, 8.9636e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7950e-04,\n",
      "        0.0000e+00, 6.7884e-02, 1.6678e-03]), 0.2613041770572846, tensor(0))\n",
      "action: 0\n",
      "Player 1 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.4000, 0.0000]), tensor([0.0000e+00, 1.0412e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3849e-04,\n",
      "        0.0000e+00, 9.9976e-01, 0.0000e+00]), 0.4269301535991522, tensor(7))\n",
      "action: 7\n",
      "Player 1 tictactoe_expert action: 1\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), 0.6284561958163977, tensor(5))\n",
      "action: 5\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 4, 2, 8, 5],\n",
      "        [0, 4, 2, 8, 5],\n",
      "        [0, 8, 7, 2, 3],\n",
      "        [2, 3, 5, 6, 0],\n",
      "        [8, 0, 7, 6, 5],\n",
      "        [3, 4, 8, 2, 0],\n",
      "        [5, 0, 6, 5, 2],\n",
      "        [4, 6, 7, 0, 2]])\n",
      "target value tensor([[-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.2068],\n",
      "        [ 0.1574],\n",
      "        [ 0.4009],\n",
      "        [ 0.5568],\n",
      "        [ 0.2376],\n",
      "        [ 0.1893],\n",
      "        [ 0.7386],\n",
      "        [-0.0408]], grad_fn=<AddmmBackward0>), tensor([[ 0.2103],\n",
      "        [-0.1620],\n",
      "        [-0.2292],\n",
      "        [-0.3204],\n",
      "        [-0.2362],\n",
      "        [ 0.1912],\n",
      "        [ 0.0865],\n",
      "        [-0.0209]], grad_fn=<AddmmBackward0>), tensor([[ 0.2890],\n",
      "        [ 0.1001],\n",
      "        [ 0.1574],\n",
      "        [ 0.2317],\n",
      "        [ 0.0101],\n",
      "        [ 0.1749],\n",
      "        [-0.0579],\n",
      "        [ 0.1924]], grad_fn=<AddmmBackward0>), tensor([[ 0.0990],\n",
      "        [-0.1593],\n",
      "        [ 0.0023],\n",
      "        [-0.0470],\n",
      "        [-0.2270],\n",
      "        [ 0.3056],\n",
      "        [-0.1729],\n",
      "        [ 0.0469]], grad_fn=<AddmmBackward0>), tensor([[ 0.2482],\n",
      "        [ 0.0970],\n",
      "        [ 0.0947],\n",
      "        [ 0.1732],\n",
      "        [ 0.1125],\n",
      "        [ 0.1701],\n",
      "        [ 0.0281],\n",
      "        [-0.0394]], grad_fn=<AddmmBackward0>), tensor([[ 0.1864],\n",
      "        [-0.0572],\n",
      "        [-0.0441],\n",
      "        [-0.0976],\n",
      "        [ 0.0126],\n",
      "        [-0.0435],\n",
      "        [-0.0351],\n",
      "        [-0.0878]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0407],\n",
      "        [-0.1051],\n",
      "        [ 0.1207],\n",
      "        [-0.0797],\n",
      "        [-0.1643],\n",
      "        [ 0.0550],\n",
      "        [ 0.5689],\n",
      "        [ 0.4223]], grad_fn=<AddmmBackward0>), tensor([[ 0.1126],\n",
      "        [-0.1444],\n",
      "        [ 0.0359],\n",
      "        [ 0.0338],\n",
      "        [-0.0557],\n",
      "        [ 0.2123],\n",
      "        [-0.0152],\n",
      "        [ 0.3906]], grad_fn=<AddmmBackward0>), tensor([[ 0.1345],\n",
      "        [-0.0534],\n",
      "        [ 0.1612],\n",
      "        [ 0.1521],\n",
      "        [-0.0667],\n",
      "        [ 0.3417],\n",
      "        [-0.0151],\n",
      "        [ 0.4234]], grad_fn=<AddmmBackward0>), tensor([[ 0.0516],\n",
      "        [-0.1379],\n",
      "        [ 0.2441],\n",
      "        [ 0.2959],\n",
      "        [ 0.1515],\n",
      "        [ 0.4144],\n",
      "        [ 0.0550],\n",
      "        [-0.0970]], grad_fn=<AddmmBackward0>), tensor([[ 0.0983],\n",
      "        [ 0.0580],\n",
      "        [ 0.3281],\n",
      "        [-0.1426],\n",
      "        [ 0.2277],\n",
      "        [ 0.0483],\n",
      "        [ 0.1233],\n",
      "        [ 0.1110]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.1948e-04, 9.9968e-01],\n",
      "        [7.9835e-05, 9.9992e-01],\n",
      "        [1.9551e-02, 9.8045e-01],\n",
      "        [1.1540e-05, 9.9999e-01],\n",
      "        [5.2393e-05, 9.9995e-01],\n",
      "        [1.5100e-04, 9.9985e-01],\n",
      "        [4.1634e-04, 9.9958e-01],\n",
      "        [9.9642e-05, 9.9990e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9943e-01, 5.6646e-04],\n",
      "        [9.9974e-01, 2.5666e-04],\n",
      "        [9.9653e-01, 3.4670e-03],\n",
      "        [9.9999e-01, 7.1001e-06],\n",
      "        [9.9996e-01, 3.7029e-05],\n",
      "        [9.9840e-01, 1.5964e-03],\n",
      "        [9.9923e-01, 7.7203e-04],\n",
      "        [9.9999e-01, 8.5152e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.1512e-04, 9.9908e-01],\n",
      "        [4.5807e-05, 9.9995e-01],\n",
      "        [1.2079e-02, 9.8792e-01],\n",
      "        [2.1494e-05, 9.9998e-01],\n",
      "        [1.0061e-03, 9.9899e-01],\n",
      "        [4.5223e-05, 9.9995e-01],\n",
      "        [3.1613e-04, 9.9968e-01],\n",
      "        [1.0168e-03, 9.9898e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9935e-01, 6.4786e-04],\n",
      "        [9.9996e-01, 4.4727e-05],\n",
      "        [9.9984e-01, 1.6143e-04],\n",
      "        [1.0000e+00, 4.3338e-06],\n",
      "        [9.9999e-01, 6.1797e-06],\n",
      "        [9.9983e-01, 1.6969e-04],\n",
      "        [9.9283e-01, 7.1738e-03],\n",
      "        [9.9895e-01, 1.0482e-03]], grad_fn=<SoftmaxBackward0>), tensor([[2.8780e-04, 9.9971e-01],\n",
      "        [1.1217e-04, 9.9989e-01],\n",
      "        [8.4490e-04, 9.9916e-01],\n",
      "        [2.3603e-05, 9.9998e-01],\n",
      "        [2.9199e-04, 9.9971e-01],\n",
      "        [1.8725e-04, 9.9981e-01],\n",
      "        [7.4142e-04, 9.9926e-01],\n",
      "        [7.3647e-03, 9.9264e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "Player 0 win percentage vs tictactoe_expert: 16.0 and average score: -0.4\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 2\n",
      "1500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 5, 4, 1, 8],\n",
      "        [5, 6, 2, 0, 6],\n",
      "        [1, 4, 0, 4, 6],\n",
      "        [4, 2, 6, 5, 8],\n",
      "        [0, 5, 0, 4, 6],\n",
      "        [3, 1, 6, 0, 6],\n",
      "        [3, 8, 4, 0, 6],\n",
      "        [4, 5, 0, 4, 6]])\n",
      "target value tensor([[ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1047],\n",
      "        [ 0.4968],\n",
      "        [ 0.2156],\n",
      "        [-0.4221],\n",
      "        [ 0.3017],\n",
      "        [ 0.0093],\n",
      "        [ 0.1465],\n",
      "        [ 0.0546]], grad_fn=<AddmmBackward0>), tensor([[-0.3373],\n",
      "        [ 0.5829],\n",
      "        [ 0.0867],\n",
      "        [ 0.2701],\n",
      "        [-0.1121],\n",
      "        [ 0.3325],\n",
      "        [ 0.0777],\n",
      "        [ 0.2238]], grad_fn=<AddmmBackward0>), tensor([[ 0.3990],\n",
      "        [ 0.3382],\n",
      "        [ 0.2951],\n",
      "        [-0.2976],\n",
      "        [ 0.2998],\n",
      "        [ 0.1680],\n",
      "        [ 0.0482],\n",
      "        [ 0.3590]], grad_fn=<AddmmBackward0>), tensor([[-0.3160],\n",
      "        [ 0.2055],\n",
      "        [-0.0186],\n",
      "        [ 0.1695],\n",
      "        [-0.0837],\n",
      "        [ 0.1232],\n",
      "        [ 0.3674],\n",
      "        [-0.0255]], grad_fn=<AddmmBackward0>), tensor([[ 0.6306],\n",
      "        [ 0.1897],\n",
      "        [ 0.1635],\n",
      "        [-0.0661],\n",
      "        [ 0.1590],\n",
      "        [ 0.2319],\n",
      "        [ 0.0664],\n",
      "        [ 0.1586]], grad_fn=<AddmmBackward0>), tensor([[-0.1787],\n",
      "        [ 0.0822],\n",
      "        [ 0.1317],\n",
      "        [ 0.1884],\n",
      "        [ 0.0961],\n",
      "        [ 0.2682],\n",
      "        [ 0.3750],\n",
      "        [ 0.2647]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.0418],\n",
      "        [ 0.2844],\n",
      "        [ 0.1073],\n",
      "        [-0.0446],\n",
      "        [ 0.2412],\n",
      "        [ 0.0433],\n",
      "        [-0.1049],\n",
      "        [ 0.2539]], grad_fn=<AddmmBackward0>), tensor([[-0.0209],\n",
      "        [ 0.4373],\n",
      "        [ 0.2653],\n",
      "        [ 0.0240],\n",
      "        [ 0.2346],\n",
      "        [ 0.1924],\n",
      "        [ 0.0340],\n",
      "        [ 0.3499]], grad_fn=<AddmmBackward0>), tensor([[ 0.0438],\n",
      "        [ 0.4853],\n",
      "        [ 0.0426],\n",
      "        [-0.0044],\n",
      "        [ 0.0584],\n",
      "        [ 0.2875],\n",
      "        [ 0.2741],\n",
      "        [ 0.0662]], grad_fn=<AddmmBackward0>), tensor([[-0.0281],\n",
      "        [ 0.5660],\n",
      "        [ 0.0567],\n",
      "        [ 0.1206],\n",
      "        [-0.0490],\n",
      "        [-0.0316],\n",
      "        [-0.0756],\n",
      "        [ 0.0408]], grad_fn=<AddmmBackward0>), tensor([[0.3486],\n",
      "        [0.3597],\n",
      "        [0.1905],\n",
      "        [0.0882],\n",
      "        [0.0274],\n",
      "        [0.1799],\n",
      "        [0.1755],\n",
      "        [0.0583]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[3.6967e-06, 1.0000e+00],\n",
      "        [9.8675e-01, 1.3252e-02],\n",
      "        [2.0804e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 1.6849e-06],\n",
      "        [9.9967e-01, 3.3196e-04],\n",
      "        [9.9995e-01, 4.7014e-05],\n",
      "        [2.4897e-04, 9.9975e-01],\n",
      "        [9.9974e-01, 2.5644e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9977e-01, 2.3109e-04],\n",
      "        [6.9185e-02, 9.3082e-01],\n",
      "        [9.9992e-01, 7.5830e-05],\n",
      "        [1.0712e-05, 9.9999e-01],\n",
      "        [4.7917e-05, 9.9995e-01],\n",
      "        [1.6250e-03, 9.9837e-01],\n",
      "        [9.9983e-01, 1.6964e-04],\n",
      "        [9.1243e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.5335e-05, 9.9995e-01],\n",
      "        [3.6415e-01, 6.3585e-01],\n",
      "        [4.1780e-04, 9.9958e-01],\n",
      "        [9.9998e-01, 2.0447e-05],\n",
      "        [9.9993e-01, 6.6744e-05],\n",
      "        [9.9993e-01, 7.3382e-05],\n",
      "        [6.7469e-05, 9.9993e-01],\n",
      "        [9.9987e-01, 1.2584e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 6.0361e-06],\n",
      "        [9.5811e-02, 9.0419e-01],\n",
      "        [9.9911e-01, 8.8906e-04],\n",
      "        [4.4360e-05, 9.9996e-01],\n",
      "        [5.8629e-04, 9.9941e-01],\n",
      "        [2.8938e-03, 9.9711e-01],\n",
      "        [9.9986e-01, 1.3503e-04],\n",
      "        [2.0320e-04, 9.9980e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.1343e-04, 9.9979e-01],\n",
      "        [4.8923e-01, 5.1077e-01],\n",
      "        [1.2887e-04, 9.9987e-01],\n",
      "        [9.9994e-01, 6.1171e-05],\n",
      "        [9.9685e-01, 3.1464e-03],\n",
      "        [9.9828e-01, 1.7202e-03],\n",
      "        [4.2753e-03, 9.9572e-01],\n",
      "        [1.0000e+00, 2.0616e-06]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3200, 0.0400, 0.0000, 0.0400, 0.2800, 0.0400, 0.1200, 0.1200, 0.0400]), tensor([2.2352e-02, 3.0475e-08, 0.0000e+00, 1.3712e-03, 9.7556e-01, 1.5719e-05,\n",
      "        5.7431e-04, 1.4257e-05, 1.0838e-04]), -0.1627226950099262, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3600, 0.0000, 0.0000, 0.0400, 0.0000, 0.3600, 0.0400, 0.1600, 0.0400]), tensor([0.3423, 0.0000, 0.0000, 0.2888, 0.0000, 0.1889, 0.0725, 0.0168, 0.0908]), -0.11191982869058847, tensor(0))\n",
      "action: 0\n",
      "Player 0 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.4000, 0.3600, 0.1200, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.0475, 0.0000, 0.5075, 0.2109, 0.2341, 0.0000]), -0.10473361468085876, tensor(5))\n",
      "action: 5\n",
      "Player 0 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5200, 0.4800, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9937, 0.0063, 0.0000]), -0.09601788337414081, tensor(6))\n",
      "action: 6\n",
      "Player 0 tictactoe_expert action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 3, 8, 0, 0],\n",
      "        [2, 1, 5, 6, 0],\n",
      "        [4, 2, 5, 8, 7],\n",
      "        [0, 0, 1, 6, 0],\n",
      "        [3, 8, 0, 6, 0],\n",
      "        [3, 6, 8, 0, 1],\n",
      "        [2, 4, 3, 8, 7],\n",
      "        [4, 6, 2, 7, 0]])\n",
      "target value tensor([[ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.]])\n",
      "predicted values [tensor([[-0.1849],\n",
      "        [ 0.2937],\n",
      "        [ 0.4249],\n",
      "        [ 0.2974],\n",
      "        [ 0.4232],\n",
      "        [ 0.0413],\n",
      "        [ 0.6786],\n",
      "        [ 0.2937]], grad_fn=<AddmmBackward0>), tensor([[ 0.3203],\n",
      "        [-0.1913],\n",
      "        [-0.5893],\n",
      "        [ 0.0243],\n",
      "        [ 0.3701],\n",
      "        [ 0.2315],\n",
      "        [-0.2618],\n",
      "        [-0.3407]], grad_fn=<AddmmBackward0>), tensor([[-0.0633],\n",
      "        [ 0.5938],\n",
      "        [ 0.3716],\n",
      "        [ 0.0891],\n",
      "        [ 0.3699],\n",
      "        [-0.1741],\n",
      "        [ 0.1708],\n",
      "        [ 0.2244]], grad_fn=<AddmmBackward0>), tensor([[ 0.2810],\n",
      "        [-0.0408],\n",
      "        [-0.1325],\n",
      "        [ 0.0256],\n",
      "        [-0.0029],\n",
      "        [ 0.2899],\n",
      "        [ 0.0514],\n",
      "        [ 0.0585]], grad_fn=<AddmmBackward0>), tensor([[-0.1750],\n",
      "        [ 0.3687],\n",
      "        [ 0.1210],\n",
      "        [ 0.2257],\n",
      "        [ 0.0198],\n",
      "        [-0.1231],\n",
      "        [ 0.0753],\n",
      "        [ 0.2918]], grad_fn=<AddmmBackward0>), tensor([[ 0.1296],\n",
      "        [-0.0785],\n",
      "        [-0.0713],\n",
      "        [ 0.0192],\n",
      "        [ 0.1978],\n",
      "        [ 0.4086],\n",
      "        [ 0.0577],\n",
      "        [-0.0803]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.1050],\n",
      "        [-0.0119],\n",
      "        [ 0.0951],\n",
      "        [ 0.2087],\n",
      "        [ 0.3878],\n",
      "        [-0.1462],\n",
      "        [ 0.0254],\n",
      "        [-0.0278]], grad_fn=<AddmmBackward0>), tensor([[ 0.0839],\n",
      "        [-0.2220],\n",
      "        [ 0.0729],\n",
      "        [-0.0607],\n",
      "        [ 0.6086],\n",
      "        [ 0.0324],\n",
      "        [-0.0428],\n",
      "        [-0.0342]], grad_fn=<AddmmBackward0>), tensor([[-0.0461],\n",
      "        [ 0.0625],\n",
      "        [ 0.1630],\n",
      "        [ 0.0453],\n",
      "        [ 0.0738],\n",
      "        [-0.0367],\n",
      "        [ 0.0804],\n",
      "        [-0.0091]], grad_fn=<AddmmBackward0>), tensor([[-0.1038],\n",
      "        [-0.0565],\n",
      "        [ 0.1495],\n",
      "        [ 0.0567],\n",
      "        [-0.0270],\n",
      "        [ 0.1964],\n",
      "        [ 0.1064],\n",
      "        [ 0.0004]], grad_fn=<AddmmBackward0>), tensor([[-0.0938],\n",
      "        [ 0.1734],\n",
      "        [ 0.2814],\n",
      "        [-0.0766],\n",
      "        [-0.0609],\n",
      "        [ 0.3067],\n",
      "        [ 0.0871],\n",
      "        [ 0.1940]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9998e-01, 2.2800e-05],\n",
      "        [1.7064e-05, 9.9998e-01],\n",
      "        [3.6887e-05, 9.9996e-01],\n",
      "        [1.6448e-04, 9.9984e-01],\n",
      "        [1.0000e+00, 2.5193e-06],\n",
      "        [8.0527e-03, 9.9195e-01],\n",
      "        [5.1549e-05, 9.9995e-01],\n",
      "        [3.7799e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>), tensor([[8.2143e-05, 9.9992e-01],\n",
      "        [9.9994e-01, 5.7114e-05],\n",
      "        [1.0000e+00, 2.8186e-06],\n",
      "        [9.9981e-01, 1.9254e-04],\n",
      "        [3.4666e-04, 9.9965e-01],\n",
      "        [9.8434e-01, 1.5657e-02],\n",
      "        [9.9984e-01, 1.5971e-04],\n",
      "        [9.9998e-01, 1.9623e-05]], grad_fn=<SoftmaxBackward0>), tensor([[9.9989e-01, 1.1056e-04],\n",
      "        [8.9825e-06, 9.9999e-01],\n",
      "        [5.5897e-06, 9.9999e-01],\n",
      "        [6.2802e-04, 9.9937e-01],\n",
      "        [9.9410e-01, 5.9004e-03],\n",
      "        [3.2504e-02, 9.6750e-01],\n",
      "        [2.4970e-03, 9.9750e-01],\n",
      "        [1.3806e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[4.0771e-05, 9.9996e-01],\n",
      "        [9.9982e-01, 1.8161e-04],\n",
      "        [9.9999e-01, 6.2216e-06],\n",
      "        [9.9983e-01, 1.6630e-04],\n",
      "        [9.1280e-05, 9.9991e-01],\n",
      "        [9.2916e-01, 7.0838e-02],\n",
      "        [9.9951e-01, 4.8570e-04],\n",
      "        [9.9999e-01, 8.6960e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9915e-01, 8.5310e-04],\n",
      "        [6.7836e-06, 9.9999e-01],\n",
      "        [7.0567e-05, 9.9993e-01],\n",
      "        [1.0856e-04, 9.9989e-01],\n",
      "        [9.8865e-01, 1.1349e-02],\n",
      "        [2.6900e-03, 9.9731e-01],\n",
      "        [2.9841e-03, 9.9702e-01],\n",
      "        [8.1147e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 0.0 and average score: -0.92\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.4, 'player_0_win%': 0.16, 'player_1_score': -0.92, 'player_1_win%': 0.0, 'score': -0.66}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 0, 7, 6, 1],\n",
      "        [3, 7, 4, 5, 8],\n",
      "        [1, 2, 0, 2, 3],\n",
      "        [1, 0, 4, 2, 3],\n",
      "        [1, 6, 0, 2, 3],\n",
      "        [0, 0, 4, 2, 3],\n",
      "        [2, 4, 0, 6, 8],\n",
      "        [7, 0, 8, 0, 3]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.3268],\n",
      "        [-0.2592],\n",
      "        [-0.1351],\n",
      "        [ 0.4559],\n",
      "        [ 0.0910],\n",
      "        [ 0.1976],\n",
      "        [ 0.2541],\n",
      "        [ 0.0352]], grad_fn=<AddmmBackward0>), tensor([[ 0.1088],\n",
      "        [ 0.4196],\n",
      "        [ 0.3676],\n",
      "        [-0.0636],\n",
      "        [ 0.1180],\n",
      "        [ 0.1248],\n",
      "        [-0.1493],\n",
      "        [ 0.1874]], grad_fn=<AddmmBackward0>), tensor([[ 0.0987],\n",
      "        [-0.4268],\n",
      "        [-0.1319],\n",
      "        [ 0.1129],\n",
      "        [ 0.0869],\n",
      "        [ 0.0841],\n",
      "        [ 0.1568],\n",
      "        [ 0.0192]], grad_fn=<AddmmBackward0>), tensor([[ 0.0290],\n",
      "        [ 0.2388],\n",
      "        [ 0.0664],\n",
      "        [-0.1379],\n",
      "        [ 0.1261],\n",
      "        [ 0.0171],\n",
      "        [-0.0096],\n",
      "        [ 0.3674]], grad_fn=<AddmmBackward0>), tensor([[ 0.3020],\n",
      "        [-0.1421],\n",
      "        [-0.0101],\n",
      "        [ 0.2479],\n",
      "        [ 0.1487],\n",
      "        [ 0.2936],\n",
      "        [ 0.3912],\n",
      "        [ 0.0632]], grad_fn=<AddmmBackward0>), tensor([[-0.0253],\n",
      "        [ 0.3294],\n",
      "        [ 0.0781],\n",
      "        [ 0.0171],\n",
      "        [ 0.1836],\n",
      "        [ 0.1145],\n",
      "        [ 0.0809],\n",
      "        [ 0.1115]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 1.5573e-01],\n",
      "        [-1.9496e-02],\n",
      "        [ 3.3073e-01],\n",
      "        [ 7.2069e-01],\n",
      "        [ 1.9975e-01],\n",
      "        [ 2.4758e-01],\n",
      "        [-2.2507e-04],\n",
      "        [-1.5042e-02]], grad_fn=<AddmmBackward0>), tensor([[ 0.2179],\n",
      "        [-0.1189],\n",
      "        [ 0.5417],\n",
      "        [ 0.1728],\n",
      "        [ 0.2650],\n",
      "        [ 0.0246],\n",
      "        [-0.0683],\n",
      "        [ 0.1436]], grad_fn=<AddmmBackward0>), tensor([[ 0.2603],\n",
      "        [ 0.0239],\n",
      "        [ 0.1005],\n",
      "        [ 0.0877],\n",
      "        [-0.0094],\n",
      "        [-0.0010],\n",
      "        [ 0.0260],\n",
      "        [ 0.1944]], grad_fn=<AddmmBackward0>), tensor([[ 0.4717],\n",
      "        [ 0.0776],\n",
      "        [ 0.0223],\n",
      "        [ 0.0792],\n",
      "        [ 0.0292],\n",
      "        [ 0.0300],\n",
      "        [-0.0187],\n",
      "        [ 0.0611]], grad_fn=<AddmmBackward0>), tensor([[0.6020],\n",
      "        [0.0672],\n",
      "        [0.0914],\n",
      "        [0.1377],\n",
      "        [0.0493],\n",
      "        [0.0654],\n",
      "        [0.1235],\n",
      "        [0.2294]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[6.1164e-05, 9.9994e-01],\n",
      "        [1.0000e+00, 1.8502e-06],\n",
      "        [1.8025e-04, 9.9982e-01],\n",
      "        [5.1388e-07, 1.0000e+00],\n",
      "        [2.7409e-04, 9.9973e-01],\n",
      "        [2.5594e-05, 9.9997e-01],\n",
      "        [2.2249e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 1.5662e-06]], grad_fn=<SoftmaxBackward0>), tensor([[9.9998e-01, 1.9291e-05],\n",
      "        [5.1219e-06, 9.9999e-01],\n",
      "        [9.9855e-01, 1.4548e-03],\n",
      "        [9.9999e-01, 1.2383e-05],\n",
      "        [9.9997e-01, 3.3775e-05],\n",
      "        [9.9995e-01, 4.6330e-05],\n",
      "        [9.9999e-01, 8.0508e-06],\n",
      "        [1.3261e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.0352e-05, 9.9997e-01],\n",
      "        [1.0000e+00, 4.1169e-06],\n",
      "        [2.1606e-03, 9.9784e-01],\n",
      "        [4.6448e-06, 1.0000e+00],\n",
      "        [8.9884e-03, 9.9101e-01],\n",
      "        [1.7375e-04, 9.9983e-01],\n",
      "        [1.1352e-05, 9.9999e-01],\n",
      "        [9.9998e-01, 1.5553e-05]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 3.1051e-06],\n",
      "        [3.3021e-05, 9.9997e-01],\n",
      "        [9.9931e-01, 6.9346e-04],\n",
      "        [9.9999e-01, 7.5529e-06],\n",
      "        [9.9983e-01, 1.6601e-04],\n",
      "        [9.9999e-01, 6.3343e-06],\n",
      "        [9.9999e-01, 7.6646e-06],\n",
      "        [1.4494e-03, 9.9855e-01]], grad_fn=<SoftmaxBackward0>), tensor([[3.3971e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 9.5712e-06],\n",
      "        [1.4610e-02, 9.8539e-01],\n",
      "        [9.2256e-05, 9.9991e-01],\n",
      "        [2.0213e-03, 9.9798e-01],\n",
      "        [1.8924e-04, 9.9981e-01],\n",
      "        [3.6677e-05, 9.9996e-01],\n",
      "        [9.9995e-01, 4.9156e-05]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 3, 7, 6, 2],\n",
      "        [1, 0, 7, 0, 2],\n",
      "        [2, 4, 7, 1, 8],\n",
      "        [6, 5, 0, 0, 2],\n",
      "        [5, 4, 2, 7, 0],\n",
      "        [1, 0, 3, 0, 2],\n",
      "        [0, 3, 7, 6, 2],\n",
      "        [3, 0, 6, 7, 1]])\n",
      "target value tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[ 0.1896],\n",
      "        [ 0.3377],\n",
      "        [-0.2847],\n",
      "        [-0.2535],\n",
      "        [-0.1556],\n",
      "        [ 0.7197],\n",
      "        [-0.0842],\n",
      "        [ 0.3807]], grad_fn=<AddmmBackward0>), tensor([[-0.3807],\n",
      "        [-0.2073],\n",
      "        [ 0.3626],\n",
      "        [ 0.2569],\n",
      "        [ 0.0783],\n",
      "        [-0.0261],\n",
      "        [-0.2467],\n",
      "        [ 0.0526]], grad_fn=<AddmmBackward0>), tensor([[ 0.3334],\n",
      "        [ 0.2203],\n",
      "        [-0.3130],\n",
      "        [ 0.1166],\n",
      "        [-0.2740],\n",
      "        [ 0.1033],\n",
      "        [ 0.1453],\n",
      "        [-0.0303]], grad_fn=<AddmmBackward0>), tensor([[-0.4853],\n",
      "        [-0.0366],\n",
      "        [ 0.4777],\n",
      "        [ 0.2724],\n",
      "        [ 0.1921],\n",
      "        [-0.1236],\n",
      "        [-0.4675],\n",
      "        [ 0.0768]], grad_fn=<AddmmBackward0>), tensor([[ 0.3442],\n",
      "        [ 0.0615],\n",
      "        [-0.2093],\n",
      "        [-0.2040],\n",
      "        [-0.1842],\n",
      "        [ 0.0346],\n",
      "        [ 0.2136],\n",
      "        [ 0.1157]], grad_fn=<AddmmBackward0>), tensor([[-0.2175],\n",
      "        [-0.0690],\n",
      "        [ 0.5438],\n",
      "        [ 0.2403],\n",
      "        [ 0.1652],\n",
      "        [ 0.0143],\n",
      "        [-0.2406],\n",
      "        [-0.0098]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[-0.0491],\n",
      "        [ 0.1700],\n",
      "        [-0.0807],\n",
      "        [ 0.0499],\n",
      "        [ 0.0800],\n",
      "        [ 0.9271],\n",
      "        [-0.0935],\n",
      "        [ 0.1813]], grad_fn=<AddmmBackward0>), tensor([[-0.0471],\n",
      "        [ 0.1363],\n",
      "        [ 0.1244],\n",
      "        [ 0.1025],\n",
      "        [ 0.1694],\n",
      "        [ 0.0588],\n",
      "        [-0.0570],\n",
      "        [ 0.1438]], grad_fn=<AddmmBackward0>), tensor([[ 0.0214],\n",
      "        [ 0.3607],\n",
      "        [ 0.0130],\n",
      "        [ 0.1971],\n",
      "        [ 0.0680],\n",
      "        [-0.0106],\n",
      "        [-0.0104],\n",
      "        [ 0.3589]], grad_fn=<AddmmBackward0>), tensor([[ 0.0951],\n",
      "        [-0.0117],\n",
      "        [ 0.4466],\n",
      "        [ 0.0036],\n",
      "        [ 0.3531],\n",
      "        [-0.1033],\n",
      "        [ 0.2546],\n",
      "        [ 0.2133]], grad_fn=<AddmmBackward0>), tensor([[ 0.1635],\n",
      "        [ 0.0278],\n",
      "        [ 0.2884],\n",
      "        [-0.0665],\n",
      "        [-0.0480],\n",
      "        [ 0.0027],\n",
      "        [ 0.2323],\n",
      "        [ 0.4908]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[7.8155e-06, 9.9999e-01],\n",
      "        [3.6421e-05, 9.9996e-01],\n",
      "        [1.0000e+00, 2.5936e-06],\n",
      "        [1.0000e+00, 1.8597e-06],\n",
      "        [1.0000e+00, 4.2276e-06],\n",
      "        [1.2259e-05, 9.9999e-01],\n",
      "        [4.6639e-04, 9.9953e-01],\n",
      "        [1.4649e-04, 9.9985e-01]], grad_fn=<SoftmaxBackward0>), tensor([[9.9999e-01, 7.0445e-06],\n",
      "        [9.9999e-01, 1.0289e-05],\n",
      "        [2.6337e-06, 1.0000e+00],\n",
      "        [4.6583e-04, 9.9953e-01],\n",
      "        [2.0002e-05, 9.9998e-01],\n",
      "        [9.9998e-01, 1.5823e-05],\n",
      "        [9.9997e-01, 3.3027e-05],\n",
      "        [9.9992e-01, 7.6100e-05]], grad_fn=<SoftmaxBackward0>), tensor([[2.2213e-05, 9.9998e-01],\n",
      "        [1.0161e-03, 9.9898e-01],\n",
      "        [9.9999e-01, 5.4987e-06],\n",
      "        [9.9995e-01, 5.1906e-05],\n",
      "        [9.9999e-01, 6.2857e-06],\n",
      "        [7.8714e-04, 9.9921e-01],\n",
      "        [2.0338e-05, 9.9998e-01],\n",
      "        [1.8458e-04, 9.9982e-01]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 2.2019e-06],\n",
      "        [9.9990e-01, 9.8119e-05],\n",
      "        [3.0096e-05, 9.9997e-01],\n",
      "        [2.6954e-03, 9.9730e-01],\n",
      "        [4.7075e-04, 9.9953e-01],\n",
      "        [9.9994e-01, 6.1011e-05],\n",
      "        [1.0000e+00, 3.0414e-06],\n",
      "        [9.9999e-01, 1.2913e-05]], grad_fn=<SoftmaxBackward0>), tensor([[6.9998e-05, 9.9993e-01],\n",
      "        [8.1914e-04, 9.9918e-01],\n",
      "        [9.9997e-01, 2.7773e-05],\n",
      "        [9.9958e-01, 4.1706e-04],\n",
      "        [9.9995e-01, 5.3131e-05],\n",
      "        [1.5314e-05, 9.9998e-01],\n",
      "        [2.7425e-06, 1.0000e+00],\n",
      "        [2.0964e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 1, 7, 0, 0],\n",
      "        [1, 0, 0, 3, 7],\n",
      "        [0, 4, 6, 0, 7],\n",
      "        [6, 2, 7, 3, 0],\n",
      "        [4, 2, 0, 3, 7],\n",
      "        [2, 3, 7, 6, 0],\n",
      "        [0, 0, 0, 3, 7],\n",
      "        [2, 0, 0, 3, 7]])\n",
      "target value tensor([[-1.,  1., -1.,  1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  0.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.]])\n",
      "predicted values [tensor([[-0.2552],\n",
      "        [ 0.6059],\n",
      "        [ 0.1671],\n",
      "        [-0.2846],\n",
      "        [-0.2156],\n",
      "        [ 0.3033],\n",
      "        [ 0.1351],\n",
      "        [ 0.1665]], grad_fn=<AddmmBackward0>), tensor([[ 0.1751],\n",
      "        [ 0.0622],\n",
      "        [-0.0356],\n",
      "        [ 0.2630],\n",
      "        [ 0.3225],\n",
      "        [-0.3705],\n",
      "        [ 0.1454],\n",
      "        [ 0.0515]], grad_fn=<AddmmBackward0>), tensor([[ 0.5137],\n",
      "        [-0.0678],\n",
      "        [-0.0624],\n",
      "        [-0.0121],\n",
      "        [-0.0419],\n",
      "        [ 0.2356],\n",
      "        [-0.1720],\n",
      "        [ 0.2451]], grad_fn=<AddmmBackward0>), tensor([[ 0.3752],\n",
      "        [-0.1156],\n",
      "        [ 0.2836],\n",
      "        [ 0.2152],\n",
      "        [ 0.0060],\n",
      "        [-0.2960],\n",
      "        [ 0.0454],\n",
      "        [-0.0575]], grad_fn=<AddmmBackward0>), tensor([[ 0.0543],\n",
      "        [ 0.1140],\n",
      "        [-0.1307],\n",
      "        [ 0.0499],\n",
      "        [ 0.0296],\n",
      "        [ 0.4860],\n",
      "        [-0.0328],\n",
      "        [ 0.1551]], grad_fn=<AddmmBackward0>), tensor([[-0.0329],\n",
      "        [-0.2917],\n",
      "        [ 0.0739],\n",
      "        [ 0.1130],\n",
      "        [-0.1357],\n",
      "        [-0.3193],\n",
      "        [-0.0390],\n",
      "        [-0.1269]], grad_fn=<AddmmBackward0>)]\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards [tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 0.2448],\n",
      "        [ 0.5564],\n",
      "        [ 0.0662],\n",
      "        [-0.0316],\n",
      "        [-0.0622],\n",
      "        [ 0.0078],\n",
      "        [ 0.2240],\n",
      "        [ 0.0985]], grad_fn=<AddmmBackward0>), tensor([[ 0.3264],\n",
      "        [-0.0440],\n",
      "        [ 0.2928],\n",
      "        [ 0.0165],\n",
      "        [ 0.1197],\n",
      "        [ 0.0135],\n",
      "        [ 0.0388],\n",
      "        [ 0.3318]], grad_fn=<AddmmBackward0>), tensor([[ 0.4149],\n",
      "        [-0.2036],\n",
      "        [ 0.6195],\n",
      "        [-0.0227],\n",
      "        [-0.0802],\n",
      "        [ 0.1906],\n",
      "        [-0.0984],\n",
      "        [ 0.0107]], grad_fn=<AddmmBackward0>), tensor([[ 0.3880],\n",
      "        [-0.0647],\n",
      "        [ 0.0160],\n",
      "        [ 0.1336],\n",
      "        [ 0.0864],\n",
      "        [ 0.1293],\n",
      "        [-0.0008],\n",
      "        [ 0.2300]], grad_fn=<AddmmBackward0>), tensor([[-0.0860],\n",
      "        [-0.1752],\n",
      "        [-0.0162],\n",
      "        [ 0.1903],\n",
      "        [ 0.0188],\n",
      "        [ 0.3595],\n",
      "        [-0.0055],\n",
      "        [ 0.1650]], grad_fn=<AddmmBackward0>)]\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays [tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), tensor([[9.9927e-01, 7.2741e-04],\n",
      "        [5.6595e-06, 9.9999e-01],\n",
      "        [9.9995e-01, 5.2513e-05],\n",
      "        [1.0000e+00, 2.1919e-06],\n",
      "        [9.9995e-01, 4.5340e-05],\n",
      "        [1.1069e-05, 9.9999e-01],\n",
      "        [9.9999e-01, 5.5495e-06],\n",
      "        [1.5749e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>), tensor([[2.8082e-04, 9.9972e-01],\n",
      "        [9.9997e-01, 2.7701e-05],\n",
      "        [1.2511e-03, 9.9875e-01],\n",
      "        [8.6662e-06, 9.9999e-01],\n",
      "        [7.3188e-06, 9.9999e-01],\n",
      "        [9.9996e-01, 4.4517e-05],\n",
      "        [6.7040e-04, 9.9933e-01],\n",
      "        [9.9999e-01, 7.1421e-06]], grad_fn=<SoftmaxBackward0>), tensor([[1.0000e+00, 4.2075e-06],\n",
      "        [7.4939e-05, 9.9993e-01],\n",
      "        [9.9991e-01, 9.2754e-05],\n",
      "        [1.0000e+00, 2.2759e-06],\n",
      "        [9.9973e-01, 2.7108e-04],\n",
      "        [5.8080e-05, 9.9994e-01],\n",
      "        [9.9975e-01, 2.4927e-04],\n",
      "        [1.3895e-04, 9.9986e-01]], grad_fn=<SoftmaxBackward0>), tensor([[6.0098e-05, 9.9994e-01],\n",
      "        [9.9958e-01, 4.2481e-04],\n",
      "        [5.9996e-04, 9.9940e-01],\n",
      "        [5.7378e-06, 9.9999e-01],\n",
      "        [2.9261e-04, 9.9971e-01],\n",
      "        [9.9999e-01, 5.7919e-06],\n",
      "        [1.3643e-03, 9.9864e-01],\n",
      "        [9.9967e-01, 3.3398e-04]], grad_fn=<SoftmaxBackward0>), tensor([[9.9987e-01, 1.2855e-04],\n",
      "        [2.2026e-04, 9.9978e-01],\n",
      "        [9.9903e-01, 9.6911e-04],\n",
      "        [9.9999e-01, 5.3596e-06],\n",
      "        [9.9998e-01, 2.0401e-05],\n",
      "        [2.1193e-06, 1.0000e+00],\n",
      "        [9.9998e-01, 2.3253e-05],\n",
      "        [4.2010e-03, 9.9580e-01]], grad_fn=<SoftmaxBackward0>)]\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/agent.py\", line 572, in run_tests\n",
      "    super().run_tests(stats)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/agent.py\", line 348, in run_tests\n",
      "    test_score = self.test(self.test_trials, dir=training_step_dir)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/agent.py\", line 420, in test\n",
      "    prediction = self.predict(state, info, env=self.test_env.env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, exploratory_policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 68, in run\n",
      "    outputs = inference_fns[\"initial\"](state, model=inference_model)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 985, in predict_initial_inference\n",
      "    values, policies, hidden_states = model.initial_inference(state_inputs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1153, in initial_inference\n",
      "    def initial_inference(self, obs):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1109, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, exploratory_policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1109, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 141, in run\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, exploratory_policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 289, in _run_single_simulation\n",
      "    ) = inference_fns[\"recurrent\"](\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 141, in run\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 289, in _run_single_simulation\n",
      "    ) = inference_fns[\"recurrent\"](\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1166, in recurrent_inference\n",
      "    self.world_model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero_world_model.py\", line 266, in recurrent_inference\n",
      "    reward, next_hidden_state, to_play, reward_hidden = self.dynamics(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1171, in recurrent_inference\n",
      "    value, policy = self.prediction(next_hidden_state)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero_world_model.py\", line 163, in forward\n",
      "    next_hidden_state = self._fuse_and_process(hidden_state, action)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 958, in forward\n",
      "    S = self.net(inputs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero_world_model.py\", line 105, in _fuse_and_process\n",
      "    S = self.net(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 124, in forward\n",
      "    x = self.residual_layers(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 135, in forward\n",
      "    x = layer(x)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 124, in forward\n",
      "    x = self.residual_layers(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 74, in forward\n",
      "    x = self.norm2(x)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 135, in forward\n",
      "    x = layer(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 74, in forward\n",
      "    x = self.norm2(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1109, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, exploratory_policy, target_policy, best_action = self.search.run(\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 141, in run\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 255, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 289, in _run_single_simulation\n",
      "    ) = inference_fns[\"recurrent\"](\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1109, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1055, in predict\n",
      "    root_value, exploratory_policy, target_policy, best_action = self.search.run(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1171, in recurrent_inference\n",
      "    value, policy = self.prediction(next_hidden_state)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 141, in run\n",
      "    self._run_single_simulation(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 289, in _run_single_simulation\n",
      "    ) = inference_fns[\"recurrent\"](\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 999, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 959, in forward\n",
      "    return self.head(S)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 1171, in recurrent_inference\n",
      "    value, policy = self.prediction(next_hidden_state)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 937, in forward\n",
      "    return self.critic(S), self.actor(S)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py\", line 958, in forward\n",
      "    S = self.net(inputs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/critic.py\", line 30, in forward\n",
      "    x = self.net(inputs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 124, in forward\n",
      "    x = self.residual_layers(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 126, in forward\n",
      "    x = self.conv_layers(x)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 135, in forward\n",
      "    x = layer(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 73, in forward\n",
      "    x = self.conv2(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/conv.py\", line 94, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     81\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 83\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:337\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_replay_buffer_size:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[1;32m    327\u001b[0m         (\n\u001b[1;32m    328\u001b[0m             value_loss,\n\u001b[1;32m    329\u001b[0m             policy_loss,\n\u001b[1;32m    330\u001b[0m             reward_loss,\n\u001b[1;32m    331\u001b[0m             to_play_loss,\n\u001b[1;32m    332\u001b[0m             cons_loss,\n\u001b[1;32m    333\u001b[0m             q_loss,\n\u001b[1;32m    334\u001b[0m             sigma_loss,\n\u001b[1;32m    335\u001b[0m             vqvae_commitment_cost,\n\u001b[1;32m    336\u001b[0m             loss,\n\u001b[0;32m--> 337\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:548\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m     (\n\u001b[1;32m    523\u001b[0m         rewards_k,\n\u001b[1;32m    524\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    536\u001b[0m     )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;66;03m# WARNING ACTIONS_K COULD BE NAN\u001b[39;00m\n\u001b[1;32m    540\u001b[0m     (\n\u001b[1;32m    541\u001b[0m         rewards_k,\n\u001b[1;32m    542\u001b[0m         hidden_states,\n\u001b[1;32m    543\u001b[0m         values_k,\n\u001b[1;32m    544\u001b[0m         policies_k,\n\u001b[1;32m    545\u001b[0m         to_plays_k,\n\u001b[1;32m    546\u001b[0m         reward_h_states,\n\u001b[1;32m    547\u001b[0m         reward_c_states,\n\u001b[0;32m--> 548\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_recurrent_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_h_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_c_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m latent_states\u001b[38;5;241m.\u001b[39mappend(hidden_states)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Store the predicted states and outputs\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:999\u001b[0m, in \u001b[0;36mMuZeroAgent.predict_recurrent_inference\u001b[0;34m(self, states, actions_or_codes, reward_h_states, reward_c_states, model)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    998\u001b[0m rewards, states, values, policies, to_play, reward_hidden \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 999\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions_or_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_h_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_c_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# print(reward_hidden)\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m reward_h_states \u001b[38;5;241m=\u001b[39m reward_hidden[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py:1171\u001b[0m, in \u001b[0;36mNetwork.recurrent_inference\u001b[0;34m(self, hidden_state, action, reward_h_states, reward_c_states)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecurrent_inference\u001b[39m(\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1160\u001b[0m     hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     reward_c_states,\n\u001b[1;32m   1164\u001b[0m ):\n\u001b[1;32m   1165\u001b[0m     reward, next_hidden_state, to_play, reward_hidden \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld_model\u001b[38;5;241m.\u001b[39mrecurrent_inference(\n\u001b[1;32m   1167\u001b[0m             hidden_state, action, reward_h_states, reward_c_states\n\u001b[1;32m   1168\u001b[0m         )\n\u001b[1;32m   1169\u001b[0m     )\n\u001b[0;32m-> 1171\u001b[0m     value, policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reward, next_hidden_state, value, policy, to_play, reward_hidden\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py:959\u001b[0m, in \u001b[0;36mPrediction.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    958\u001b[0m     S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(inputs)\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/muzero.py:937\u001b[0m, in \u001b[0;36mPredictionHead.forward\u001b[0;34m(self, S)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, S: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(S)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/critic.py:30\u001b[0m, in \u001b[0;36mCriticNetwork.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py:126\u001b[0m, in \u001b[0;36mNetworkBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_layers(x)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Conv\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Dense\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_dense_layers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/conv.py:94\u001b[0m, in \u001b[0;36mConv2dStack.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Note: We apply activation AFTER the Conv/Norm block\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py:2813\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2811\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2813\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 10,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"stochastic\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": False,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa549b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from muzero.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"dynamics_residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.5,\n",
    "    # \"min_replay_buffer_size\": 1000,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"stochastic_fixed_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5693ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": True,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"efficient_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8494974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 2,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"consistency_loss_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf70d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"reanalyze_noise\": True,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"injection_frac\": 0.25,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"unplugged_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_16_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_8_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "    # \"norm_type\": \"none\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"modular_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -3, -1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-8, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 4, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 3 + 1e-8, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\n",
    "            \"min_replay_buffer_size\", np.log(5000), np.log(5000) + 1e-8, 1000\n",
    "        )\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 2 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 2, 3, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 7, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIGHTLY WIDER IMPROVED SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-10, 2)),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 2, 5, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 2, 8, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL STANDARD SPACE (no picking num filters etc), should be compatible with initial\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8.01, 8.02, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0.91, 0.92, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=200,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
