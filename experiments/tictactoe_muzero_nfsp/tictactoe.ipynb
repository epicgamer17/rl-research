{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ab8a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 40000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using default learning_rate                 : 0.001\n",
      "Using default clipnorm                      : 0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default loss_function                 : <class 'utils.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 8\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using default min_replay_buffer_size        : 8\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : [(24, 3, 1)]\n",
      "Using default conv_layers                   : []\n",
      "Using default dense_layer_widths            : []\n",
      "Using default representation_residual_layers: [(24, 3, 1)]\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: []\n",
      "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : []\n",
      "Using         reward_conv_layers            : [(16, 1, 1)]\n",
      "Using         reward_dense_layer_widths     : []\n",
      "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
      "Using         to_play_dense_layer_widths    : []\n",
      "Using         critic_conv_layers            : [(16, 1, 1)]\n",
      "Using         critic_dense_layer_widths     : []\n",
      "Using         actor_conv_layers             : [(16, 1, 1)]\n",
      "Using         actor_dense_layer_widths      : []\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using default value_loss_factor             : 1.0\n",
      "Using default to_play_loss_factor           : 1.0\n",
      "Using default weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.25\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 25\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using default temperature_updates           : [5]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default value_loss_function           : <utils.utils.MSELoss object at 0x321da5b70>\n",
      "Using default reward_loss_function          : <utils.utils.MSELoss object at 0x321da5b40>\n",
      "Using         policy_loss_function          : <utils.utils.KLDivergenceLoss object at 0x104db7550>\n",
      "Using default to_play_loss_function         : <utils.utils.CategoricalCrossentropyLoss object at 0x321da5ba0>\n",
      "Using         action_function               : <function action_as_plane at 0x31f8a39a0>\n",
      "Using         n_step                        : 9\n",
      "Using default discount_factor               : 1.0\n",
      "Using default unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using default num_workers                   : 4\n",
      "Using default lr_ratio                      : inf\n",
      "Using         transfer_interval             : 1\n",
      "Using         gumbel                        : True\n",
      "Using         gumbel_m                      : 16\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using device: cpu\n",
      "making test env\n",
      "Test env with record video\n",
      "env render mode rgb_array\n",
      "petting zoo env\n",
      "Test env: RecordVideo<ChannelLastToFirstWrapper<TwoPlayerPlayerPlaneWrapper<FrameStackWrapper<ActionMaskInInfoWrapper<tictactoe_v3>>>>>\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (9, 3, 3)\n",
      "Observation dtype: int8\n",
      "num_actions:  9 <class 'int'>\n",
      "Test agents: [<agents.random.RandomAgent object at 0x105197340>, <agents.tictactoe_expert.TicTacToeBestAgent object at 0x321da5c60>]\n",
      "9\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Action function output shape: torch.Size([1, 3, 3])\n",
      "torch.Size([8, 25, 3, 3])\n",
      "dynamics input shape torch.Size([8, 25, 3, 3])\n",
      "25\n",
      "24\n",
      "9\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Action function output shape: torch.Size([1, 3, 3])\n",
      "torch.Size([8, 25, 3, 3])\n",
      "dynamics input shape torch.Size([8, 25, 3, 3])\n",
      "25\n",
      "24\n",
      "Warning: for board games it is recommnded to have n_step >= game length\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 5, in <module>\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    from packages.utils.utils.utils import KLDivergenceLoss\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../packages/utils/__init__.py\", line 1, in <module>\n",
      "    from utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/__init__.py\", line 1, in <module>\n",
      "    from .utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py\", line 24, in <module>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 5, in <module>\n",
      "    from hyperopt import space_eval\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/__init__.py\", line 34, in <module>\n",
      "    from packages.utils.utils.utils import KLDivergenceLoss\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../packages/utils/__init__.py\", line 1, in <module>\n",
      "    from utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/__init__.py\", line 1, in <module>\n",
      "    from .utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py\", line 24, in <module>\n",
      "    from hyperopt import space_eval\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/__init__.py\", line 34, in <module>\n",
      "    from . import atpe\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/atpe.py\", line 21, in <module>\n",
      "    from . import atpe\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/atpe.py\", line 21, in <module>\n",
      "    import scipy.stats\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/__init__.py\", line 616, in <module>\n",
      "    import scipy.stats\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/__init__.py\", line 616, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    from ._multivariate import *\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_multivariate.py\", line 5786, in <module>\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    from ._multivariate import *\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_multivariate.py\", line 3106, in <module>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 5, in <module>\n",
      "    from packages.utils.utils.utils import KLDivergenceLoss\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../packages/utils/__init__.py\", line 1, in <module>\n",
      "    from utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/__init__.py\", line 1, in <module>\n",
      "    from .utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py\", line 24, in <module>\n",
      "    from hyperopt import space_eval\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/__init__.py\", line 34, in <module>\n",
      "    from . import atpe\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/atpe.py\", line 21, in <module>\n",
      "    import scipy.stats\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/__init__.py\", line 613, in <module>\n",
      "    from ._kde import gaussian_kde\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_kde.py\", line 33, in <module>\n",
      "    from . import _mvn\n",
      "KeyboardInterrupt\n",
      "    method_frozen.__doc__ = doccer.docformat(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/_lib/doccer.py\", line 51, in docformat\n",
      "    lines = docstring.expandtabs().splitlines()\n",
      "KeyboardInterrupt\n",
      "    _docfill(method, _ctab_docdict)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_multivariate.py\", line 5776, in _docfill\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    obj.__doc__ = doccer.docformat(template or obj.__doc__, docdict)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/_lib/doccer.py\", line 56, in docformat\n",
      "    icount = indentcount_lines(lines[1:])\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/_lib/doccer.py\", line 199, in indentcount_lines\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    indentno = min(indentno, len(line) - len(stripped))\n",
      "KeyboardInterrupt\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 5, in <module>\n",
      "    from packages.utils.utils.utils import KLDivergenceLoss\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../packages/utils/__init__.py\", line 1, in <module>\n",
      "    from utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/__init__.py\", line 1, in <module>\n",
      "    from .utils import *\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py\", line 24, in <module>\n",
      "    from hyperopt import space_eval\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/__init__.py\", line 34, in <module>\n",
      "    from . import atpe\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/hyperopt/atpe.py\", line 21, in <module>\n",
      "    import scipy.stats\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/__init__.py\", line 624, in <module>\n",
      "    from ._rvs_sampling import rvs_ratio_uniforms\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_rvs_sampling.py\", line 2, in <module>\n",
      "    from scipy.stats.sampling import RatioUniforms\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/sampling.py\", line 59, in <module>\n",
      "    from ._sampling import FastGeneratorInversion, RatioUniforms  # noqa: F401\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_sampling.py\", line 8, in <module>\n",
      "    from ._unuran.unuran_wrapper import NumericalInversePolynomial\n",
      "  File \"<frozen importlib._bootstrap>\", line 404, in parent\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     65\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 67\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:333\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tb))  \u001b[38;5;66;03m# optional: print worker traceback\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrain_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:])\n\u001b[1;32m    337\u001b[0m ):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_process:\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../stats/stats.py:113\u001b[0m, in \u001b[0;36mStatTracker.drain_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_client:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrain_queue() can only be called on the host StatTracker.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mget_nowait()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:129\u001b[0m, in \u001b[0;36mQueue.empty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:923\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(object_list, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    918\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    Wait till an object in object_list is ready/readable.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m    Returns list of those objects in object_list which are ready/readable.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _WaitSelector() \u001b[38;5;28;01mas\u001b[39;00m selector:\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m object_list:\n\u001b[1;32m    925\u001b[0m             selector\u001b[38;5;241m.\u001b[39mregister(obj, selectors\u001b[38;5;241m.\u001b[39mEVENT_READ)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:204\u001b[0m, in \u001b[0;36mBaseSelector.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:269\u001b[0m, in \u001b[0;36m_BaseSelectorImpl.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fd_to_key[key\u001b[38;5;241m.\u001b[39mfd] \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m key\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fd_to_key\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 40000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_test_2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 500\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 40000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"to_play_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 500\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -3, -1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-8, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 4, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 3 + 1e-8, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\n",
    "            \"min_replay_buffer_size\", np.log(5000), np.log(5000) + 1e-8, 1000\n",
    "        )\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 2 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 2, 3, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 7, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIGHTLY WIDER IMPROVED SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-10, 2)),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 2, 5, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 2, 8, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL STANDARD SPACE (no picking num filters etc), should be compatible with initial\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8.01, 8.02, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0.91, 0.92, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=200,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
