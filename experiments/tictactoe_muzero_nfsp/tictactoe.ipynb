{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a4203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 3.419638890039465 11.6839619576931 0.6385402295901486 15.742597579956055\n",
      "Training Step: 1590\n",
      "Losses 4.392728368329699 10.628098160028458 0.24225013199122714 15.26354694366455\n",
      "Training Step: 1591\n",
      "Losses 2.0962335495341904 11.140642084181309 0.3065708397061826 13.554306030273438\n",
      "Training Step: 1592\n",
      "Losses 0.7501933733665282 10.793083544820547 0.2023537909699371 11.7457914352417\n",
      "Training Step: 1593\n",
      "Losses 2.2169411518370907 10.913349814713001 0.2701009461982693 13.407675743103027\n",
      "Training Step: 1594\n",
      "Losses 2.664428472324886 11.360018834471703 0.4330801859762232 14.457640647888184\n",
      "Training Step: 1595\n",
      "Losses 3.054127180395076 10.646770566701889 0.2367844659072489 13.975449562072754\n",
      "Training Step: 1596\n",
      "Losses 3.349326871377343 10.511766975745559 0.32045106927495226 14.181744575500488\n",
      "Training Step: 1597\n",
      "Losses 2.8113004599581473 10.368044584989548 0.11115808960978057 13.290716171264648\n",
      "Training Step: 1598\n",
      "Losses 1.841184319666354 10.227540012449026 0.3009871190007516 12.369997024536133\n",
      "Training Step: 1599\n",
      "Losses 1.658858897581382 12.171574851498008 0.4997911528698751 14.33077621459961\n",
      "Training Step: 1600\n",
      "Saving Checkpoint\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "unroll step 0\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]]], dtype=torch.int8)\n",
      "predicted value tensor([0.0531], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1532, 0.0003, 0.2859, 0.0275, 0.1536, 0.1610, 0.1220, 0.0380, 0.0584],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1200, 0.0000, 0.1200, 0.0000, 0.1600, 0.0000, 0.1200, 0.4800, 0.0000])\n",
      "to_play tensor([[0., 0.]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1, 0], dtype=torch.int16)\n",
      "sample losses tensor([1.1090], grad_fn=<MulBackward0>) tensor(0.) tensor(2.4971, grad_fn=<NegBackward0>) tensor(0.)\n",
      "unroll step 1\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]]], dtype=torch.int8)\n",
      "predicted value tensor([-0.1752], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.0275], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0838, 0.0046, 0.2604, 0.0577, 0.1547, 0.1644, 0.1650, 0.0461, 0.0634],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.2000, 0.1600, 0.1600, 0.1600, 0.2000, 0.1200, 0.0000])\n",
      "to_play tensor([[1.8651e-04, 9.9981e-01]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([0, 1], dtype=torch.int16)\n",
      "sample losses tensor([1.3811], grad_fn=<MulBackward0>) tensor([0.0008], grad_fn=<PowBackward0>) tensor(2.0428, grad_fn=<NegBackward0>) tensor([0.0002], grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]]], dtype=torch.int8)\n",
      "predicted value tensor([0.0422], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.0284], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1091, 0.0024, 0.2707, 0.0537, 0.2033, 0.1197, 0.1415, 0.0264, 0.0732],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.3200, 0.1600, 0.2800, 0.0000, 0.2400, 0.0000, 0.0000])\n",
      "to_play tensor([[9.9997e-01, 3.2000e-05]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1, 0], dtype=torch.int16)\n",
      "sample losses tensor([1.0863], grad_fn=<MulBackward0>) tensor([0.0008], grad_fn=<PowBackward0>) tensor(1.8014, grad_fn=<NegBackward0>) tensor([3.1949e-05], grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]]], dtype=torch.int8)\n",
      "predicted value tensor([-0.1319], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([-0.0026], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.2802, 0.0026, 0.0085, 0.0187, 0.0824, 0.2145, 0.2604, 0.0140, 0.1188],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.0000, 0.0000, 0.2400, 0.2800, 0.2000, 0.2800, 0.0000, 0.0000])\n",
      "to_play tensor([[0.0030, 0.9970]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([0, 1], dtype=torch.int16)\n",
      "sample losses tensor([1.2812], grad_fn=<MulBackward0>) tensor([6.6659e-06], grad_fn=<PowBackward0>) tensor(2.3387, grad_fn=<NegBackward0>) tensor([0.0030], grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]]], dtype=torch.int8)\n",
      "predicted value tensor([0.2039], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.0802], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(1.)\n",
      "predicted policy tensor([0.1131, 0.0243, 0.0287, 0.0786, 0.0230, 0.4847, 0.1150, 0.0449, 0.0878],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111])\n",
      "to_play tensor([[0.9984, 0.0016]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1, 0], dtype=torch.int16)\n",
      "sample losses tensor([0.0416], grad_fn=<MulBackward0>) tensor([0.8460], grad_fn=<PowBackward0>) tensor(2.6876, grad_fn=<NegBackward0>) tensor([0.0016], grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]]], dtype=torch.int8)\n",
      "predicted value tensor([-3.4451e-05], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(0.)\n",
      "predicted reward tensor([0.0543], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0916, 0.0995, 0.0847, 0.1480, 0.0475, 0.2092, 0.1014, 0.1170, 0.1012],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111])\n",
      "to_play tensor([[3.8263e-05, 9.9996e-01]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([0, 0], dtype=torch.int16)\n",
      "sample losses tensor([1.1868e-09], grad_fn=<MulBackward0>) tensor([0.0029], grad_fn=<PowBackward0>) tensor(2.2682, grad_fn=<NegBackward0>) tensor([-0.], grad_fn=<NegBackward0>)\n",
      "Losses 3.345272005541106 10.673558034002781 0.45291463649726893 14.472540855407715\n",
      "Training Step: 1601\n",
      "Losses 2.9200161091612244 10.286371760070324 0.168365271057354 13.382614135742188\n",
      "Training Step: 1602\n",
      "Losses 5.2010516710579395 10.126129284501076 0.17519472630590371 15.503189086914062\n",
      "Training Step: 1603\n",
      "Losses 3.913878492603544 10.790770191699266 0.34825476319747395 15.053457260131836\n",
      "Training Step: 1604\n",
      "Losses 3.2248775117477635 9.595709905028343 0.2845362218331857 13.10535717010498\n",
      "Training Step: 1605\n",
      "Losses 4.13586268232757 10.98517032712698 0.24592814069319502 15.367259979248047\n",
      "Training Step: 1606\n",
      "Losses 2.342311768879881 10.33371951431036 0.18202912031244978 12.859941482543945\n",
      "Training Step: 1607\n",
      "Losses 4.332199497812326 10.729355651885271 0.21958043886888845 15.285091400146484\n",
      "Training Step: 1608\n",
      "Losses 2.7888694736648176 10.04755724966526 0.1936879549260766 13.030872344970703\n",
      "Training Step: 1609\n",
      "Losses 2.2868689693059423 11.285601824522018 0.505685583751756 14.078968048095703\n",
      "Training Step: 1610\n",
      "Losses 3.0952145109195044 10.822045162320137 0.11646510028818635 14.034586906433105\n",
      "Training Step: 1611\n",
      "Losses 2.91744653568594 10.983102098107338 0.363822662424127 14.26447582244873\n",
      "Training Step: 1612\n",
      "Losses 2.9462936507802624 10.152141779661179 0.3586404126746923 13.460650444030762\n",
      "Training Step: 1613\n",
      "Losses 4.935031198661818 10.39470324665308 0.21087879930017195 15.541122436523438\n",
      "Training Step: 1614\n",
      "Losses 1.421750534990224 10.611824777908623 0.3251139672110668 12.359814643859863\n",
      "Training Step: 1615\n",
      "Losses 1.7212440400653577 11.326063089072704 0.4889447885386744 13.53724193572998\n",
      "Training Step: 1616\n",
      "Losses 3.5697783660980775 11.34894472360611 0.3312406989293777 15.251007080078125\n",
      "Training Step: 1617\n",
      "Losses 3.5732515607342066 10.46331937611103 0.14114705197087574 14.178309440612793\n",
      "Training Step: 1618\n",
      "Losses 3.7186836783239414 9.99073376506567 0.4181571945835003 14.128082275390625\n",
      "Training Step: 1619\n",
      "Losses 1.5521269822056638 10.177233349531889 0.31061720054361786 12.04047966003418\n",
      "Training Step: 1620\n",
      "Losses 3.296485872568155 10.76493475586176 0.29085835618595013 14.352590560913086\n",
      "Training Step: 1621\n",
      "Losses 4.12071942860166 11.484720267355442 0.46667386889794216 16.072486877441406\n",
      "Training Step: 1622\n",
      "Losses 2.07081352828709 11.179869316518307 0.32829486540276775 13.579333305358887\n",
      "Training Step: 1623\n",
      "Losses 3.2129735889557196 10.642904378473759 0.40193101499279577 14.259283065795898\n",
      "Training Step: 1624\n",
      "Losses 1.6725264767883345 10.183174680918455 0.06677012485216949 11.924163818359375\n",
      "Training Step: 1625\n",
      "Losses 1.4308590730488504 10.131240878254175 0.19260537388726107 11.75505256652832\n",
      "Training Step: 1626\n",
      "Losses 2.4095785458515966 11.156452775001526 0.21179930645858036 13.7852201461792\n",
      "Training Step: 1627\n",
      "Losses 2.104585761817096 11.277618069201708 0.3438359463643792 13.726570129394531\n",
      "Training Step: 1628\n",
      "Losses 4.17696437044242 10.603918746113777 0.2288909576167697 15.0111083984375\n",
      "Training Step: 1629\n",
      "Losses 3.410219710304034 11.122225977480412 0.32124823970296923 14.854857444763184\n",
      "Training Step: 1630\n",
      "Losses 2.780169670487737 10.237040787935257 0.17454614341578178 13.191906929016113\n",
      "Training Step: 1631\n",
      "Losses 3.560592496265599 10.825962387025356 0.23864868747945422 14.625407218933105\n",
      "Training Step: 1632\n",
      "Losses 3.8525958939862903 10.97828109934926 0.4172668002440787 15.248495101928711\n",
      "Training Step: 1633\n",
      "Losses 2.436180745625961 9.989882327616215 0.1592914412264932 12.585424423217773\n",
      "Training Step: 1634\n",
      "Losses 1.7642181113660627 11.309655238874257 0.32931315296790586 13.403329849243164\n",
      "Training Step: 1635\n",
      "Losses 2.473202551510809 11.420920595526695 0.37198871417604096 14.266227722167969\n",
      "Training Step: 1636\n",
      "Losses 1.8791969725307354 10.732800394296646 0.30918269554331346 12.921319007873535\n",
      "Training Step: 1637\n",
      "Losses 2.4553446505451575 9.660957597196102 0.2330966030180548 12.34947681427002\n",
      "Training Step: 1638\n",
      "Losses 0.955094876466319 12.011028492823243 0.45635356346441824 13.42251205444336\n",
      "Training Step: 1639\n",
      "Losses 1.4660451243910302 9.789715934544802 0.22050359096783723 11.476312637329102\n",
      "Training Step: 1640\n",
      "Losses 2.532348240551073 10.247647058218718 0.2405009642564977 13.020580291748047\n",
      "Training Step: 1641\n",
      "Losses 2.5866362753149588 9.700037613511086 0.16561486376218681 12.452392578125\n",
      "Training Step: 1642\n",
      "Losses 4.0243420578262885 9.656384550035 0.11185950434812852 13.792695999145508\n",
      "Training Step: 1643\n",
      "Losses 4.314172966852993 10.12730585038662 0.11456765039201855 14.556124687194824\n",
      "Training Step: 1644\n",
      "Losses 2.97134840666331 10.438679851591587 0.2073771023833615 13.617457389831543\n",
      "Training Step: 1645\n",
      "Losses 1.2547855265183898 10.329580999910831 0.16985362506396173 11.754345893859863\n",
      "Training Step: 1646\n",
      "Losses 3.815005654056563 11.204021789133549 0.0799898105796224 15.099652290344238\n",
      "Training Step: 1647\n",
      "Losses 1.1341414283187987 10.868926733732224 0.26718597316883574 12.270442962646484\n",
      "Training Step: 1648\n",
      "Losses 4.375810587353044 11.297812387347221 0.7055560984717886 16.37929344177246\n",
      "Training Step: 1649\n",
      "Losses 4.811880158669737 10.535900630056858 0.3500956902188008 15.697990417480469\n",
      "Training Step: 1650\n",
      "Losses 2.2648243312905834 11.025923453271389 0.29294430166744334 13.583823204040527\n",
      "Training Step: 1651\n",
      "Losses 2.150853830023152 10.90216227248311 0.3014361443585969 13.354514122009277\n",
      "Training Step: 1652\n",
      "Losses 2.9229761959704774 11.09630697965622 0.5330897387533149 14.55264663696289\n",
      "Training Step: 1653\n",
      "Losses 2.007541847194034 11.431574508547783 0.40573224732725066 13.845379829406738\n",
      "Training Step: 1654\n",
      "Losses 2.379691642562989 9.909689422696829 0.05969939160013382 12.35239315032959\n",
      "Training Step: 1655\n",
      "Losses 3.612720213867661 10.786170490086079 0.42271179806448345 14.821662902832031\n",
      "Training Step: 1656\n",
      "Losses 2.4619791145705676 11.016896925866604 0.38702568191490627 13.86616039276123\n",
      "Training Step: 1657\n",
      "Losses 2.187779630124748 10.702748011797667 0.3991400341847111 13.289836883544922\n",
      "Training Step: 1658\n",
      "Losses 1.5702248519281738 10.749679351225495 0.17477225512993755 12.496036529541016\n",
      "Training Step: 1659\n",
      "Losses 2.2745791331253713 10.2637819647789 0.18219612810426788 12.720919609069824\n",
      "Training Step: 1660\n",
      "Losses 2.48723202303745 11.223419539630413 0.34624213807546766 14.057029724121094\n",
      "Training Step: 1661\n",
      "Losses 2.4426562168293486 10.595851503312588 0.0671365813573459 13.106974601745605\n",
      "Training Step: 1662\n",
      "Losses 1.9228616265625647 10.846217297017574 0.1483587199863905 12.917529106140137\n",
      "Training Step: 1663\n",
      "Losses 1.9897068151308304 11.19793451577425 0.20702104539998345 13.395190238952637\n",
      "Training Step: 1664\n",
      "Losses 1.0150151488614938 10.101382542401552 0.213520202073596 11.33003044128418\n",
      "Training Step: 1665\n",
      "Losses 1.6766864982782863 11.38681036233902 0.22788564343204598 13.292947769165039\n",
      "Training Step: 1666\n",
      "Losses 2.7127848851595218 10.6110426671803 0.3315026802187617 13.659605026245117\n",
      "Training Step: 1667\n",
      "Losses 3.093997567477552 11.052917949855328 0.3697334071866294 14.516944885253906\n",
      "Training Step: 1668\n",
      "Losses 1.6750049068323563 10.645339533686638 0.42250309992891744 13.012192726135254\n",
      "Training Step: 1669\n",
      "Losses 1.6711259593066643 10.268890179693699 0.15082290990835645 12.091192245483398\n",
      "Training Step: 1670\n",
      "Losses 1.297770511008821 10.875857951119542 0.31041506672241326 12.484171867370605\n",
      "Training Step: 1671\n",
      "Losses 4.266916755227385 10.149237528443336 0.4260475689238774 14.842275619506836\n",
      "Training Step: 1672\n",
      "Losses 1.2960172281182167 10.97051827982068 0.4828965676642838 13.61735725402832\n",
      "Training Step: 1673\n",
      "Losses 3.4148420687852195 10.61032086238265 0.288258007686494 14.313671112060547\n",
      "Training Step: 1674\n",
      "Losses 2.8607670074338785 10.380286395549774 0.3262207539372497 13.567541122436523\n",
      "Training Step: 1675\n",
      "Losses 3.2742373450764717 11.590038411086425 0.33955694191979546 15.204878807067871\n",
      "Training Step: 1676\n",
      "Losses 1.2256880086752062 10.621330961585045 0.19124310907415065 12.038704872131348\n",
      "Training Step: 1677\n",
      "Losses 1.4210376879909745 11.166554301977158 0.470286141859674 13.845674514770508\n",
      "Training Step: 1678\n",
      "Losses 3.5180263664735776 10.391816385090351 0.27341471611000756 14.216649055480957\n",
      "Training Step: 1679\n",
      "Losses 1.4646537931839703 11.650712430477142 0.4078746306058747 13.52340030670166\n",
      "Training Step: 1680\n",
      "Losses 2.929471696199471 10.589467138051987 0.34359472768220556 13.863694190979004\n",
      "Training Step: 1681\n",
      "Losses 1.0938025821407962 11.181290864944458 0.3083575087509063 12.584310531616211\n",
      "Training Step: 1682\n",
      "Losses 3.763750086669461 11.161492368206382 0.4370357308775965 15.364951133728027\n",
      "Training Step: 1683\n",
      "Losses 1.7734439584601205 10.745391502976418 0.25165242263140897 12.772538185119629\n",
      "Training Step: 1684\n",
      "Losses 1.9008488546605804 11.282314337790012 0.48766506164747625 13.68297290802002\n",
      "Training Step: 1685\n",
      "Losses 2.124807865542607 10.199762389063835 0.33642467540524024 12.67155933380127\n",
      "Training Step: 1686\n",
      "Losses 2.1241596798572573 11.415464971214533 0.4190470401595121 13.960043907165527\n",
      "Training Step: 1687\n",
      "Losses 1.5990978121299122 10.437926813960075 0.26576976477895187 12.314969062805176\n",
      "Training Step: 1688\n",
      "Losses 4.281262431060895 10.517687700688839 0.22497331943546328 15.145952224731445\n",
      "Training Step: 1689\n",
      "Losses 3.743588482397172 10.485625676810741 0.22073569501492196 14.491666793823242\n",
      "Training Step: 1690\n",
      "Losses 2.484661911279929 12.230191007256508 0.45049149951586287 15.169923782348633\n",
      "Training Step: 1691\n",
      "Losses 2.512510082537119 11.593527894467115 0.5671625696822957 14.67359733581543\n",
      "Training Step: 1692\n",
      "Losses 1.7073446788572255 11.682173058390617 0.45761450155805505 13.847383499145508\n",
      "Training Step: 1693\n",
      "Losses 2.397701012938569 10.864641574211419 0.2704193462304829 13.533679962158203\n",
      "Training Step: 1694\n",
      "Losses 5.2266940889530815 10.160519175231457 0.1287422751311169 15.516694068908691\n",
      "Training Step: 1695\n",
      "Losses 2.5534224014994606 10.183733720332384 0.23863918973404452 12.976146697998047\n",
      "Training Step: 1696\n",
      "Losses 3.636542270236532 11.451967686414719 0.33268475207205483 15.422027587890625\n",
      "Training Step: 1697\n",
      "Losses 3.0976419631151657 10.822627186775208 0.2913349590917278 14.211897850036621\n",
      "Training Step: 1698\n",
      "Losses 2.078123248106749 10.535846620798111 0.3091294628811738 12.925422668457031\n",
      "Training Step: 1699\n",
      "Losses 2.6124628764464433 10.351874709129333 0.35451802436017144 13.31905746459961\n",
      "Training Step: 1700\n",
      "Saving Checkpoint\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "unroll step 0\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 1],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]]], dtype=torch.int8)\n",
      "predicted value tensor([-0.2277], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1516, 0.1580, 0.0019, 0.1175, 0.0829, 0.0933, 0.2066, 0.0150, 0.1731],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0400, 0.1200, 0.0000, 0.1200, 0.1200, 0.0800, 0.2000, 0.2000, 0.1200])\n",
      "to_play tensor([[0., 0.]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([0, 1], dtype=torch.int16)\n",
      "sample losses tensor([1.5074], grad_fn=<MulBackward0>) tensor(0.) tensor(2.4078, grad_fn=<NegBackward0>) tensor(0.)\n",
      "unroll step 1\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 1],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]]], dtype=torch.int8)\n",
      "predicted value tensor([0.2491], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.0314], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1823, 0.0522, 0.0009, 0.1162, 0.3530, 0.0839, 0.0459, 0.0535, 0.1120],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1600, 0.0000, 0.0000, 0.2800, 0.2000, 0.0400, 0.0000, 0.1200, 0.2000])\n",
      "to_play tensor([[1.0000e+00, 1.9780e-06]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1, 0], dtype=torch.int16)\n",
      "sample losses tensor([1.5604], grad_fn=<MulBackward0>) tensor([0.0010], grad_fn=<PowBackward0>) tensor(1.9715, grad_fn=<NegBackward0>) tensor([2.0266e-06], grad_fn=<NegBackward0>)\n",
      "unroll step 2\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 1],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]]], dtype=torch.int8)\n",
      "predicted value tensor([-0.1919], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.0397], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1721, 0.1246, 0.0043, 0.0056, 0.0904, 0.1201, 0.2110, 0.1369, 0.1350],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.1200, 0.2000, 0.0000, 0.0000, 0.2000, 0.1200, 0.0000, 0.2000, 0.1600])\n",
      "to_play tensor([[1.4529e-05, 9.9999e-01]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([0, 1], dtype=torch.int16)\n",
      "sample losses tensor([1.4207], grad_fn=<MulBackward0>) tensor([0.0016], grad_fn=<PowBackward0>) tensor(2.0808, grad_fn=<NegBackward0>) tensor([1.4544e-05], grad_fn=<NegBackward0>)\n",
      "unroll step 3\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 1],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]]], dtype=torch.int8)\n",
      "predicted value tensor([0.2295], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.0729], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.1630, 0.1536, 0.0278, 0.0306, 0.0200, 0.1517, 0.0942, 0.2283, 0.1308],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.2000, 0.1600, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.2400, 0.2000])\n",
      "to_play tensor([[9.9985e-01, 1.4850e-04]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1, 0], dtype=torch.int16)\n",
      "sample losses tensor([1.5116], grad_fn=<MulBackward0>) tensor([0.0053], grad_fn=<PowBackward0>) tensor(1.8011, grad_fn=<NegBackward0>) tensor([0.0001], grad_fn=<NegBackward0>)\n",
      "unroll step 4\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 1],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]]], dtype=torch.int8)\n",
      "predicted value tensor([-0.2513], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(1.)\n",
      "predicted reward tensor([0.1533], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0338, 0.2312, 0.0346, 0.0405, 0.0170, 0.1906, 0.1311, 0.1873, 0.1339],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.3200, 0.2400])\n",
      "to_play tensor([[4.2773e-06, 1.0000e+00]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([0, 1], dtype=torch.int16)\n",
      "sample losses tensor([1.5657], grad_fn=<MulBackward0>) tensor([0.0235], grad_fn=<PowBackward0>) tensor(1.7092, grad_fn=<NegBackward0>) tensor([4.2915e-06], grad_fn=<NegBackward0>)\n",
      "unroll step 5\n",
      "observation tensor([[[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 1],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 1, 1],\n",
      "          [1, 1, 1],\n",
      "          [1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 1, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0],\n",
      "          [1, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[1, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 1, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 1],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0],\n",
      "          [0, 0, 0],\n",
      "          [0, 0, 0]]]], dtype=torch.int8)\n",
      "predicted value tensor([0.2995], grad_fn=<UnbindBackward0>)\n",
      "target value tensor(-1.)\n",
      "predicted reward tensor([0.0706], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor(0.)\n",
      "predicted policy tensor([0.0455, 0.2392, 0.0402, 0.1135, 0.0273, 0.2426, 0.1277, 0.0347, 0.1294],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.0000, 0.2800, 0.0000, 0.0000, 0.0000, 0.2800, 0.0000, 0.0000, 0.4400])\n",
      "to_play tensor([[9.9999e-01, 5.2889e-06]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1, 0], dtype=torch.int16)\n",
      "sample losses tensor([1.6887], grad_fn=<MulBackward0>) tensor([0.0050], grad_fn=<PowBackward0>) tensor(1.6969, grad_fn=<NegBackward0>) tensor([5.2452e-06], grad_fn=<NegBackward0>)\n",
      "Losses 3.226738998176188 10.891659676097333 0.3581571635817866 14.476719856262207\n",
      "Training Step: 1701\n",
      "Losses 2.6528594263982086 10.383041232824326 0.42250385598089224 13.474125862121582\n",
      "Training Step: 1702\n",
      "Losses 2.679520095640328 11.027425065636635 0.4588805435089398 14.166438102722168\n",
      "Training Step: 1703\n",
      "Losses 3.42303280509077 11.99680457264185 0.46300574147817786 15.913817405700684\n",
      "Training Step: 1704\n",
      "Losses 4.183675451145973 9.845926776528358 0.04791412129264927 14.077682495117188\n",
      "Training Step: 1705\n",
      "Losses 2.583762292400934 10.352432664483786 0.3336581210430438 13.270028114318848\n",
      "Training Step: 1706\n",
      "Losses 1.8681809442059603 10.282691154628992 0.23583296891197847 12.387149810791016\n",
      "Training Step: 1707\n",
      "Losses 1.982495876844041 11.110744521021843 0.2655312513370518 13.35888671875\n",
      "Training Step: 1708\n",
      "Losses 2.713292679458391 10.73796072974801 0.34221174471334415 13.793538093566895\n",
      "Training Step: 1709\n",
      "Losses 2.643290449050255 10.66543360799551 0.40847451339664076 13.717272758483887\n",
      "Training Step: 1710\n",
      "Losses 1.9932718154159375 10.73783113900572 0.34578260241431735 13.077134132385254\n",
      "Training Step: 1711\n",
      "Losses 1.648537164030131 11.043685020878911 0.285180631576182 12.977450370788574\n",
      "Training Step: 1712\n",
      "Losses 2.8550770794099662 10.065666772425175 0.3604668435355052 13.282700538635254\n",
      "Training Step: 1713\n",
      "Losses 2.4956900255183427 11.091494884341955 0.4955341560105603 14.08289623260498\n",
      "Training Step: 1714\n",
      "Losses 2.78370513767004 11.684237912297249 0.2855057387645914 14.7534761428833\n",
      "Training Step: 1715\n",
      "Losses 3.430962346486922 11.413204461336136 0.4207043534643162 15.264907836914062\n",
      "Training Step: 1716\n",
      "Losses 1.6436334979637195 10.714113660156727 0.21038642005441943 12.568269729614258\n",
      "Training Step: 1717\n",
      "Losses 1.335994535460486 11.499649114906788 0.4683760485538073 13.304081916809082\n",
      "Training Step: 1718\n",
      "Losses 1.831346493687306 10.974644802510738 0.4556375511938313 13.261696815490723\n",
      "Training Step: 1719\n",
      "Losses 4.280442885617958 10.317509483546019 0.24147806065109734 14.839488983154297\n",
      "Training Step: 1720\n",
      "Losses 2.1188229824419977 10.334634207189083 0.27619630024491926 12.729734420776367\n",
      "Training Step: 1721\n",
      "Losses 2.1190616793583104 11.064738601446152 0.24982755592083095 13.433757781982422\n",
      "Training Step: 1722\n",
      "Losses 2.3365989424019062 10.757454462349415 0.41025598491762594 13.504364013671875\n",
      "Training Step: 1723\n",
      "Losses 1.602860797662288 11.837901517748833 0.45009467208194565 13.890976905822754\n",
      "Training Step: 1724\n",
      "Losses 5.31154960359072 10.758034363389015 0.19144518884140505 16.261098861694336\n",
      "Training Step: 1725\n",
      "Losses 3.762339870292635 10.391841938719153 0.3124431620232144 14.466815948486328\n",
      "Training Step: 1726\n",
      "Losses 2.441167670374398 10.357200264930725 0.15049901949555533 12.949227333068848\n",
      "Training Step: 1727\n",
      "Losses 3.912829559034435 11.405254021286964 0.532099425171964 15.85027027130127\n",
      "Training Step: 1728\n",
      "Losses 1.4674717998329925 10.720358788967133 0.11703245610169688 12.347429275512695\n",
      "Training Step: 1729\n",
      "Losses 2.6986148631224296 10.913059314712882 0.3609737732513736 13.972769737243652\n",
      "Training Step: 1730\n",
      "Losses 4.845519494793734 11.44737983494997 0.5241708148504358 16.817110061645508\n",
      "Training Step: 1731\n",
      "Losses 3.068438529104924 11.349822018295527 0.39152772333275787 14.809844970703125\n",
      "Training Step: 1732\n",
      "Losses 2.440737142223952 11.089749448001385 0.3969883866066084 13.927504539489746\n",
      "Training Step: 1733\n",
      "Losses 2.4226292137194037 10.94801021553576 0.4569631169379136 13.827648162841797\n",
      "Training Step: 1734\n",
      "Losses 3.0202309879730365 11.0042245388031 0.3957926192096579 14.420771598815918\n",
      "Training Step: 1735\n",
      "Losses 5.4535745336384025 10.834996990859509 0.3142556131249137 16.60291862487793\n",
      "Training Step: 1736\n",
      "Losses 1.6942518018217356 11.044620297849178 0.4613276327910114 13.200301170349121\n",
      "Training Step: 1737\n",
      "Losses 4.946521629479321 10.331658199429512 0.3303178365028998 15.608651161193848\n",
      "Training Step: 1738\n",
      "Losses 4.518809577313732 10.5669711753726 0.4259379403310959 15.511905670166016\n",
      "Training Step: 1739\n",
      "Losses 0.8136563273951651 11.433180898427963 0.25990342654813503 12.507248878479004\n",
      "Training Step: 1740\n",
      "Losses 3.315606079041345 10.375627890229225 0.32666340846753883 14.018123626708984\n",
      "Training Step: 1741\n",
      "Losses 2.940046308649471 10.99622093141079 0.39152077066000857 14.328009605407715\n",
      "Training Step: 1742\n",
      "Losses 4.317729166024947 10.489990968257189 0.36019256271077893 15.168386459350586\n",
      "Training Step: 1743\n",
      "Losses 2.671758937118284 10.195714846253395 0.3370529447900239 13.204642295837402\n",
      "Training Step: 1744\n",
      "Losses 2.835437715127499 10.406768233980983 0.24439332737529185 13.48664665222168\n",
      "Training Step: 1745\n",
      "Losses 1.8890070605193614 10.308937408961356 0.17816102662436606 12.3761568069458\n",
      "Training Step: 1746\n",
      "Losses 4.2928057559765875 10.787899430841208 0.34783085712422235 15.428933143615723\n",
      "Training Step: 1747\n",
      "Losses 2.8262727369328786 10.766463227570057 0.22722222275200465 13.820116996765137\n",
      "Training Step: 1748\n",
      "Losses 3.0114011384284822 12.071799486875534 0.5292678296332269 15.612540245056152\n",
      "Training Step: 1749\n",
      "Losses 2.4966104302461645 9.903102969750762 0.12747755351752943 12.527263641357422\n",
      "Training Step: 1750\n",
      "Losses 4.009866081549262 10.376437529921532 0.04201282690939934 14.428372383117676\n",
      "Training Step: 1751\n",
      "Losses 1.7305659648045548 11.093108005821705 0.20864846061989795 13.033072471618652\n",
      "Training Step: 1752\n",
      "Losses 2.9204703814721142 10.585114818066359 0.351521344328944 13.857150077819824\n",
      "Training Step: 1753\n",
      "Losses 4.4446978218483935 10.652802683413029 0.3115566601916271 15.4091157913208\n",
      "Training Step: 1754\n",
      "Losses 1.776201878892607 10.380829773843288 0.14800694999081543 12.305903434753418\n",
      "Training Step: 1755\n",
      "Losses 4.145239734201368 10.903160764835775 0.3349208427444381 15.383358001708984\n",
      "Training Step: 1756\n",
      "Losses 3.2839355432524826 10.037140933796763 0.21699965953393985 13.538113594055176\n",
      "Training Step: 1757\n",
      "Losses 2.3249916044333077 10.866349495947361 0.3131765042442236 13.504619598388672\n",
      "Training Step: 1758\n",
      "Losses 2.7487734737717346 10.793967008590698 0.1619104304915524 13.70472526550293\n",
      "Training Step: 1759\n",
      "Losses 1.7874017130720858 10.507567420601845 0.3514264777584941 12.646440505981445\n",
      "Training Step: 1760\n",
      "Losses 3.110019311059773 9.89958792924881 0.15841561980098362 13.168169021606445\n",
      "Training Step: 1761\n",
      "Losses 2.3749738219485153 10.381606042385101 0.289223246477718 13.045920372009277\n",
      "Training Step: 1762\n",
      "Losses 2.610667223524615 11.544905867427588 0.3733090388091185 14.528925895690918\n",
      "Training Step: 1763\n",
      "Losses 4.697434708421952 10.335346318781376 0.3034139688385038 15.336419105529785\n",
      "Training Step: 1764\n",
      "Losses 3.1473985641969193 10.193542443215847 0.20509699687749341 13.546262741088867\n",
      "Training Step: 1765\n",
      "Losses 2.0615157470097074 11.17669988097623 0.30769301575151076 13.546329498291016\n",
      "Training Step: 1766\n",
      "Losses 1.8008576782328305 10.76265873759985 0.18698802971766781 12.75057315826416\n",
      "Training Step: 1767\n",
      "Losses 1.4586779831288368 11.025393150746822 0.37706007602173486 12.861260414123535\n",
      "Training Step: 1768\n",
      "Losses 3.5271487346821004 10.684402421116829 0.34750208957802897 14.559093475341797\n",
      "Training Step: 1769\n",
      "Losses 1.5223209634591797 10.882049985229969 0.2683826236696518 12.672819137573242\n",
      "Training Step: 1770\n",
      "Losses 2.013645964761963 11.116802982985973 0.3356871402605748 13.466181755065918\n",
      "Training Step: 1771\n",
      "Losses 4.327199860768815 10.472461022436619 0.4985251790177472 15.298272132873535\n",
      "Training Step: 1772\n",
      "Losses 1.7842087610479211 11.012340269982815 0.2535048487653526 13.050182342529297\n",
      "Training Step: 1773\n",
      "Losses 4.217227480525253 10.30632134526968 0.2607434384750107 14.784576416015625\n",
      "Training Step: 1774\n",
      "Losses 2.2541227046440326 10.682603649795055 0.3307412511784946 13.267763137817383\n",
      "Training Step: 1775\n",
      "Losses 1.3312959941433746 10.665871404111385 0.3882468535955468 12.38546085357666\n",
      "Training Step: 1776\n",
      "Losses 1.594716918272752 9.858273832127452 0.13882811346587687 11.591849327087402\n",
      "Training Step: 1777\n",
      "Losses 1.267113444791903 10.920247089117765 0.2348009384738532 12.422370910644531\n",
      "Training Step: 1778\n",
      "Losses 2.9665727812462137 10.71613310277462 0.31840675109327776 14.001259803771973\n",
      "Training Step: 1779\n",
      "Losses 3.4877047466870863 10.987287424504757 0.4484152249414901 14.923467636108398\n",
      "Training Step: 1780\n",
      "Losses 2.622432638923783 11.122297406196594 0.46919885192187394 14.21435832977295\n",
      "Training Step: 1781\n",
      "Losses 3.8908871050543894 11.898912746459246 0.31740087260683936 16.10722541809082\n",
      "Training Step: 1782\n",
      "Losses 1.968076545531403 10.555147513747215 0.28425928366687003 12.807658195495605\n",
      "Training Step: 1783\n",
      "Losses 1.6326425853148976 11.552972935140133 0.3646046399630922 13.550293922424316\n",
      "Training Step: 1784\n",
      "Losses 2.594775391622534 10.631660792976618 0.5036203574344995 13.730497360229492\n",
      "Training Step: 1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 798, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 735, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 385, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 721, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy, to_play = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 933, in recurrent_inference\n",
      "    reward, hidden_state, to_play = self.dynamics(nn_input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 414, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 153, in forward\n",
      "    x = self.conv2(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 798, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 735, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 385, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 721, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy, to_play = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 934, in recurrent_inference\n",
      "    value, policy = self.prediction(hidden_state)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 605, in forward\n",
      "    S = self.residual_layers(S)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 798, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 735, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 385, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 151, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 721, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy, to_play = self.model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 933, in recurrent_inference\n",
      "    reward, hidden_state, to_play = self.dynamics(nn_input)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 414, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 154, in forward\n",
      "    x = self.bn2(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 219, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 798, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 735, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 385, in monte_carlo_tree_search\n",
      "    self.predict_single_recurrent_inference(\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 721, in predict_single_recurrent_inference\n",
      "    reward, hidden_state, value, policy, to_play = self.model.recurrent_inference(\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 934, in recurrent_inference\n",
      "    value, policy = self.prediction(hidden_state)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 605, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 154, in forward\n",
      "    x = self.bn2(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     57\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 59\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:283\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_replay_buffer_size:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[0;32m--> 283\u001b[0m         value_loss, policy_loss, reward_loss, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:691\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 691\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    693\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)] * 3,\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"to_play_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -3, -1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-8, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 4, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 3 + 1e-8, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\n",
    "            \"min_replay_buffer_size\", np.log(5000), np.log(5000) + 1e-8, 1000\n",
    "        )\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 2 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 2, 3, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 7, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIGHTLY WIDER IMPROVED SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-10, 2)),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 2, 5, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 2, 8, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL STANDARD SPACE (no picking num filters etc), should be compatible with initial\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8.01, 8.02, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0.91, 0.92, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=200,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
