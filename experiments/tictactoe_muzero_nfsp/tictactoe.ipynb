{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7491063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 2,\n",
    "    \"stochastic\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.1,\n",
    "    \"reanalyze_noise\": False,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": False,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a659894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 20000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using default learning_rate                 : 0.001\n",
      "Using default clipnorm                      : 0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using         minibatch_size                : 8\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using default min_replay_buffer_size        : 8\n",
      "Using         n_step                        : 10\n",
      "Using default discount_factor               : 0.99\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_use_initial_max_priority  : True\n",
      "Using default loss_function                 : <class 'losses.basic_losses.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using default print_interval                : 100\n",
      "Using default norm_type                     : none\n",
      "Using default soft_update                   : False\n",
      "Using         world_model_cls               : <class 'modules.world_models.muzero_world_model.MuzeroWorldModel'>\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : [(24, 3, 1)]\n",
      "Using default conv_layers                   : []\n",
      "Using default dense_layer_widths            : []\n",
      "Using default representation_residual_layers: [(24, 3, 1)]\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: []\n",
      "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : []\n",
      "Using         reward_conv_layers            : [(16, 1, 1)]\n",
      "Using         reward_dense_layer_widths     : []\n",
      "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
      "Using         to_play_dense_layer_widths    : []\n",
      "Using         critic_conv_layers            : [(16, 1, 1)]\n",
      "Using         critic_dense_layer_widths     : []\n",
      "Using         actor_conv_layers             : [(16, 1, 1)]\n",
      "Using         actor_dense_layer_widths      : []\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using default to_play_loss_factor           : 1.0\n",
      "Using         num_simulations               : 25\n",
      "Using         root_dirichlet_alpha          : 0.25\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         gumbel                        : False\n",
      "Using         gumbel_m                      : 8\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using default temperature_updates           : [5]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default value_loss_function           : <losses.basic_losses.MSELoss object at 0x326ba1f00>\n",
      "Using default reward_loss_function          : <losses.basic_losses.MSELoss object at 0x326ba1ed0>\n",
      "Using         policy_loss_function          : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x11035d870>\n",
      "Using default to_play_loss_function         : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x326ba1f30>\n",
      "Using default unroll_steps                  : 5\n",
      "Using default atom_size                     : 1\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using         num_workers                   : 4\n",
      "Using default lr_ratio                      : inf\n",
      "Using         transfer_interval             : 1\n",
      "Using         reanalyze_ratio               : 0.0\n",
      "Using         reanalyze_method              : mcts\n",
      "Using default reanalyze_tau                 : 0.3\n",
      "Using         injection_frac                : 0.0\n",
      "Using         reanalyze_noise               : True\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         consistency_loss_factor       : 0.0\n",
      "Using         projector_output_dim          : 128\n",
      "Using         projector_hidden_dim          : 128\n",
      "Using         predictor_output_dim          : 128\n",
      "Using         predictor_hidden_dim          : 64\n",
      "Using default mask_absorbing                : False\n",
      "Using         value_prefix                  : False\n",
      "Using default lstm_horizon_len              : 5\n",
      "Using default lstm_hidden_size              : 64\n",
      "Using default q_estimation_method           : v_mix\n",
      "Using         stochastic                    : False\n",
      "Using default use_true_chance_codes         : False\n",
      "Using default num_chance                    : 32\n",
      "Using default sigma_loss                    : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x326ba1f90>\n",
      "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
      "Using default afterstate_conv_layers        : []\n",
      "Using default afterstate_dense_layer_widths : []\n",
      "Using         chance_conv_layers            : [(16, 1, 1)]\n",
      "Using         chance_dense_layer_widths     : []\n",
      "Using default vqvae_commitment_cost_factor  : 1.0\n",
      "Using default action_embedding_dim          : 32\n",
      "Using default single_action_plane           : False\n",
      "[wm_unroll_test] Using device: cpu\n",
      "Observation dimensions: (9, 3, 3)\n",
      "Num actions: 9 (Discrete: True)\n",
      "Making test env...\n",
      "Test env configured for video recording.\n",
      "MARL Agent 'wm_unroll_test' initialized. Test agents: ['random', 'tictactoe_expert']\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "encoder input shape (8, 18, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "encoder input shape (8, 18, 3, 3)\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'cons_loss' with subkeys None\n",
      "Initializing stat 'q_loss' with subkeys None\n",
      "Initializing stat 'sigma_loss' with subkeys None\n",
      "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'num_codes' with subkeys None\n",
      "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 1] Starting self-play...\n",
      "[Worker 2] Starting self-play...\n",
      "[Worker 3] Starting self-play...\n",
      "0\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 7, 6, 4, 0],\n",
      "        [7, 6, 4, 0, 0],\n",
      "        [7, 6, 4, 0, 0],\n",
      "        [6, 4, 0, 0, 1],\n",
      "        [4, 0, 0, 6, 1],\n",
      "        [0, 0, 1, 6, 1],\n",
      "        [0, 0, 1, 6, 1],\n",
      "        [1, 7, 2, 0, 5]])\n",
      "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801]])\n",
      "predicted values tensor([[[0.0488],\n",
      "         [0.1735],\n",
      "         [0.1996],\n",
      "         [0.2489],\n",
      "         [0.2392],\n",
      "         [0.2216]],\n",
      "\n",
      "        [[0.0369],\n",
      "         [0.0367],\n",
      "         [0.0419],\n",
      "         [0.0642],\n",
      "         [0.0696],\n",
      "         [0.0688]],\n",
      "\n",
      "        [[0.0369],\n",
      "         [0.0367],\n",
      "         [0.0419],\n",
      "         [0.0642],\n",
      "         [0.0696],\n",
      "         [0.0688]],\n",
      "\n",
      "        [[0.0861],\n",
      "         [0.1424],\n",
      "         [0.1362],\n",
      "         [0.1441],\n",
      "         [0.1294],\n",
      "         [0.1267]],\n",
      "\n",
      "        [[0.0181],\n",
      "         [0.1311],\n",
      "         [0.1607],\n",
      "         [0.1615],\n",
      "         [0.1375],\n",
      "         [0.1399]],\n",
      "\n",
      "        [[0.1518],\n",
      "         [0.1902],\n",
      "         [0.2049],\n",
      "         [0.1885],\n",
      "         [0.2214],\n",
      "         [0.2373]],\n",
      "\n",
      "        [[0.1518],\n",
      "         [0.1902],\n",
      "         [0.2049],\n",
      "         [0.1885],\n",
      "         [0.2214],\n",
      "         [0.2373]],\n",
      "\n",
      "        [[0.0488],\n",
      "         [0.1721],\n",
      "         [0.2153],\n",
      "         [0.2324],\n",
      "         [0.2400],\n",
      "         [0.2373]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [-0.2013],\n",
      "         [-0.2079],\n",
      "         [-0.2005],\n",
      "         [-0.1806],\n",
      "         [-0.1812]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1244],\n",
      "         [-0.1328],\n",
      "         [-0.1190],\n",
      "         [-0.1437],\n",
      "         [-0.1838]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1244],\n",
      "         [-0.1328],\n",
      "         [-0.1190],\n",
      "         [-0.1437],\n",
      "         [-0.1838]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0894],\n",
      "         [-0.1041],\n",
      "         [-0.1318],\n",
      "         [-0.1354],\n",
      "         [-0.1404]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0863],\n",
      "         [ 0.0902],\n",
      "         [ 0.1046],\n",
      "         [ 0.0886],\n",
      "         [ 0.0661]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1499],\n",
      "         [-0.1444],\n",
      "         [-0.1402],\n",
      "         [-0.1265],\n",
      "         [-0.1206]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1499],\n",
      "         [-0.1444],\n",
      "         [-0.1402],\n",
      "         [-0.1265],\n",
      "         [-0.1206]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.2074],\n",
      "         [-0.2110],\n",
      "         [-0.2087],\n",
      "         [-0.2134],\n",
      "         [-0.2262]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000, 0.0000],\n",
      "         [0.5455, 0.4545],\n",
      "         [0.5563, 0.4437],\n",
      "         [0.5604, 0.4396],\n",
      "         [0.5696, 0.4304],\n",
      "         [0.5666, 0.4334]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.5485, 0.4515],\n",
      "         [0.5570, 0.4430],\n",
      "         [0.5660, 0.4340],\n",
      "         [0.5599, 0.4401],\n",
      "         [0.5572, 0.4428]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.5485, 0.4515],\n",
      "         [0.5570, 0.4430],\n",
      "         [0.5660, 0.4340],\n",
      "         [0.5599, 0.4401],\n",
      "         [0.5572, 0.4428]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.6011, 0.3989],\n",
      "         [0.6075, 0.3925],\n",
      "         [0.5919, 0.4081],\n",
      "         [0.5922, 0.4078],\n",
      "         [0.5968, 0.4032]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.5713, 0.4287],\n",
      "         [0.5735, 0.4265],\n",
      "         [0.5805, 0.4195],\n",
      "         [0.5805, 0.4195],\n",
      "         [0.5877, 0.4123]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.5885, 0.4115],\n",
      "         [0.5816, 0.4184],\n",
      "         [0.5847, 0.4153],\n",
      "         [0.5893, 0.4107],\n",
      "         [0.5953, 0.4047]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.5885, 0.4115],\n",
      "         [0.5816, 0.4184],\n",
      "         [0.5847, 0.4153],\n",
      "         [0.5893, 0.4107],\n",
      "         [0.5953, 0.4047]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.5647, 0.4353],\n",
      "         [0.5751, 0.4249],\n",
      "         [0.5795, 0.4205],\n",
      "         [0.5789, 0.4211],\n",
      "         [0.5764, 0.4236]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 5, 1, 0, 5],\n",
      "        [4, 5, 8, 7, 6],\n",
      "        [4, 5, 1, 6, 8],\n",
      "        [8, 0, 4, 1, 2],\n",
      "        [0, 7, 6, 4, 3],\n",
      "        [0, 0, 5, 2, 5],\n",
      "        [5, 2, 8, 3, 0],\n",
      "        [6, 1, 0, 2, 5]])\n",
      "target value tensor([[ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.3676],\n",
      "         [ 0.1701],\n",
      "         [ 0.1824],\n",
      "         [ 0.2010],\n",
      "         [ 0.0748],\n",
      "         [ 0.1461]],\n",
      "\n",
      "        [[ 0.1672],\n",
      "         [-0.0971],\n",
      "         [ 0.1145],\n",
      "         [ 0.1389],\n",
      "         [ 0.1478],\n",
      "         [ 0.1017]],\n",
      "\n",
      "        [[-0.0242],\n",
      "         [-0.0872],\n",
      "         [ 0.1418],\n",
      "         [ 0.2402],\n",
      "         [ 0.1296],\n",
      "         [ 0.1080]],\n",
      "\n",
      "        [[-0.0713],\n",
      "         [-0.0657],\n",
      "         [-0.1154],\n",
      "         [-0.0934],\n",
      "         [ 0.1330],\n",
      "         [ 0.0971]],\n",
      "\n",
      "        [[-0.0258],\n",
      "         [-0.1847],\n",
      "         [-0.0797],\n",
      "         [-0.0472],\n",
      "         [-0.0434],\n",
      "         [ 0.0681]],\n",
      "\n",
      "        [[ 0.1288],\n",
      "         [ 0.0653],\n",
      "         [ 0.0276],\n",
      "         [ 0.0313],\n",
      "         [-0.0723],\n",
      "         [ 0.0234]],\n",
      "\n",
      "        [[-0.1840],\n",
      "         [ 0.2174],\n",
      "         [ 0.1450],\n",
      "         [ 0.1332],\n",
      "         [ 0.1559],\n",
      "         [ 0.0783]],\n",
      "\n",
      "        [[ 0.0721],\n",
      "         [ 0.0452],\n",
      "         [ 0.1788],\n",
      "         [ 0.0878],\n",
      "         [ 0.0343],\n",
      "         [ 0.0491]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0776],\n",
      "         [ 0.0884],\n",
      "         [ 0.1147],\n",
      "         [ 0.1008],\n",
      "         [ 0.0739]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1462],\n",
      "         [ 0.0290],\n",
      "         [ 0.0010],\n",
      "         [ 0.0565],\n",
      "         [ 0.1276]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0854],\n",
      "         [ 0.0643],\n",
      "         [ 0.1160],\n",
      "         [ 0.1637],\n",
      "         [ 0.1085]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0110],\n",
      "         [ 0.0712],\n",
      "         [ 0.0590],\n",
      "         [ 0.0451],\n",
      "         [ 0.0481]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0071],\n",
      "         [-0.0418],\n",
      "         [ 0.0019],\n",
      "         [ 0.0329],\n",
      "         [ 0.0989]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1467],\n",
      "         [ 0.0754],\n",
      "         [ 0.0250],\n",
      "         [ 0.0696],\n",
      "         [ 0.0580]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0048],\n",
      "         [ 0.1186],\n",
      "         [ 0.1312],\n",
      "         [ 0.2084],\n",
      "         [ 0.1989]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2752],\n",
      "         [ 0.2042],\n",
      "         [ 0.1062],\n",
      "         [ 0.0625],\n",
      "         [ 0.0374]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000, 0.0000],\n",
      "         [0.3100, 0.6900],\n",
      "         [0.3577, 0.6423],\n",
      "         [0.3953, 0.6047],\n",
      "         [0.3863, 0.6137],\n",
      "         [0.4307, 0.5693]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.3605, 0.6395],\n",
      "         [0.4428, 0.5572],\n",
      "         [0.4266, 0.5734],\n",
      "         [0.4258, 0.5742],\n",
      "         [0.4097, 0.5903]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.7211, 0.2789],\n",
      "         [0.6323, 0.3677],\n",
      "         [0.5337, 0.4663],\n",
      "         [0.4800, 0.5200],\n",
      "         [0.4624, 0.5376]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.7315, 0.2685],\n",
      "         [0.5963, 0.4037],\n",
      "         [0.5628, 0.4372],\n",
      "         [0.5188, 0.4812],\n",
      "         [0.4905, 0.5095]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.2225, 0.7775],\n",
      "         [0.3792, 0.6208],\n",
      "         [0.3922, 0.6078],\n",
      "         [0.4684, 0.5316],\n",
      "         [0.4932, 0.5068]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.2856, 0.7144],\n",
      "         [0.3557, 0.6443],\n",
      "         [0.4096, 0.5904],\n",
      "         [0.4381, 0.5619],\n",
      "         [0.4642, 0.5358]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.6709, 0.3291],\n",
      "         [0.5689, 0.4311],\n",
      "         [0.5378, 0.4622],\n",
      "         [0.5071, 0.4929],\n",
      "         [0.4319, 0.5681]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.3807, 0.6193],\n",
      "         [0.3884, 0.6116],\n",
      "         [0.3857, 0.6143],\n",
      "         [0.4109, 0.5891],\n",
      "         [0.4546, 0.5454]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 2, 0, 6, 1],\n",
      "        [4, 6, 2, 0, 0],\n",
      "        [0, 7, 3, 0, 1],\n",
      "        [4, 8, 0, 6, 7],\n",
      "        [2, 0, 6, 7, 1],\n",
      "        [7, 4, 0, 2, 8],\n",
      "        [0, 5, 7, 3, 4],\n",
      "        [7, 8, 3, 0, 1]])\n",
      "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.4644],\n",
      "         [-0.1693],\n",
      "         [-0.1625],\n",
      "         [ 0.0499],\n",
      "         [-0.0707],\n",
      "         [ 0.1953]],\n",
      "\n",
      "        [[-0.3959],\n",
      "         [-0.0888],\n",
      "         [-0.1305],\n",
      "         [ 0.1585],\n",
      "         [-0.0424],\n",
      "         [ 0.0574]],\n",
      "\n",
      "        [[ 0.2428],\n",
      "         [-0.0968],\n",
      "         [ 0.3237],\n",
      "         [ 0.0472],\n",
      "         [ 0.1204],\n",
      "         [-0.0200]],\n",
      "\n",
      "        [[-0.3337],\n",
      "         [-0.1603],\n",
      "         [-0.2211],\n",
      "         [-0.0074],\n",
      "         [-0.0560],\n",
      "         [ 0.1797]],\n",
      "\n",
      "        [[-0.1912],\n",
      "         [ 0.2446],\n",
      "         [-0.1034],\n",
      "         [ 0.1216],\n",
      "         [ 0.0280],\n",
      "         [ 0.0906]],\n",
      "\n",
      "        [[ 0.1494],\n",
      "         [-0.1289],\n",
      "         [ 0.0746],\n",
      "         [-0.1198],\n",
      "         [ 0.0944],\n",
      "         [ 0.0281]],\n",
      "\n",
      "        [[ 0.1494],\n",
      "         [-0.3245],\n",
      "         [ 0.0759],\n",
      "         [-0.1344],\n",
      "         [-0.0136],\n",
      "         [-0.1846]],\n",
      "\n",
      "        [[ 0.4926],\n",
      "         [-0.0556],\n",
      "         [ 0.2508],\n",
      "         [-0.0135],\n",
      "         [ 0.0874],\n",
      "         [ 0.0027]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0678],\n",
      "         [ 0.2199],\n",
      "         [ 0.0509],\n",
      "         [ 0.2020],\n",
      "         [ 0.1151]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0300],\n",
      "         [ 0.1716],\n",
      "         [ 0.1067],\n",
      "         [ 0.1788],\n",
      "         [ 0.0452]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1260],\n",
      "         [ 0.0336],\n",
      "         [ 0.2572],\n",
      "         [-0.0100],\n",
      "         [ 0.1155]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1398],\n",
      "         [ 0.1697],\n",
      "         [ 0.0535],\n",
      "         [ 0.1790],\n",
      "         [ 0.0504]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0127],\n",
      "         [ 0.1104],\n",
      "         [ 0.0236],\n",
      "         [ 0.0933],\n",
      "         [ 0.0766]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0307],\n",
      "         [ 0.0129],\n",
      "         [ 0.0856],\n",
      "         [ 0.0315],\n",
      "         [ 0.1743]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1260],\n",
      "         [-0.1537],\n",
      "         [ 0.0098],\n",
      "         [-0.0649],\n",
      "         [ 0.1012]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1474],\n",
      "         [ 0.0864],\n",
      "         [ 0.3088],\n",
      "         [ 0.0154],\n",
      "         [ 0.1443]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000, 0.0000],\n",
      "         [0.9820, 0.0180],\n",
      "         [0.0033, 0.9967],\n",
      "         [0.9925, 0.0075],\n",
      "         [0.0034, 0.9966],\n",
      "         [0.9888, 0.0112]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.9906, 0.0094],\n",
      "         [0.0022, 0.9978],\n",
      "         [0.9962, 0.0038],\n",
      "         [0.0021, 0.9979],\n",
      "         [0.9649, 0.0351]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0052, 0.9948],\n",
      "         [0.9938, 0.0062],\n",
      "         [0.0052, 0.9948],\n",
      "         [0.9857, 0.0143],\n",
      "         [0.0045, 0.9955]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.9920, 0.0080],\n",
      "         [0.0030, 0.9970],\n",
      "         [0.9942, 0.0058],\n",
      "         [0.0030, 0.9970],\n",
      "         [0.9932, 0.0068]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.9939, 0.0061],\n",
      "         [0.0026, 0.9974],\n",
      "         [0.9937, 0.0063],\n",
      "         [0.0056, 0.9944],\n",
      "         [0.9862, 0.0138]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0033, 0.9967],\n",
      "         [0.9955, 0.0045],\n",
      "         [0.0021, 0.9979],\n",
      "         [0.9923, 0.0077],\n",
      "         [0.0031, 0.9969]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0035, 0.9965],\n",
      "         [0.9964, 0.0036],\n",
      "         [0.0018, 0.9982],\n",
      "         [0.9931, 0.0069],\n",
      "         [0.0020, 0.9980]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0064, 0.9936],\n",
      "         [0.9907, 0.0093],\n",
      "         [0.0029, 0.9971],\n",
      "         [0.9852, 0.0148],\n",
      "         [0.0050, 0.9950]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 3, 4, 6, 0],\n",
      "        [1, 0, 7, 5, 3],\n",
      "        [4, 2, 6, 5, 3],\n",
      "        [0, 7, 4, 0, 3],\n",
      "        [5, 0, 7, 5, 3],\n",
      "        [1, 8, 3, 0, 5],\n",
      "        [0, 6, 2, 0, 3],\n",
      "        [5, 0, 7, 5, 3]])\n",
      "target value tensor([[-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.1798],\n",
      "         [-0.1052],\n",
      "         [ 0.1794],\n",
      "         [-0.1432],\n",
      "         [ 0.4276],\n",
      "         [-0.1146]],\n",
      "\n",
      "        [[ 0.1399],\n",
      "         [ 0.3846],\n",
      "         [-0.1984],\n",
      "         [ 0.2399],\n",
      "         [-0.1229],\n",
      "         [ 0.1556]],\n",
      "\n",
      "        [[-0.4409],\n",
      "         [ 0.0010],\n",
      "         [-0.2096],\n",
      "         [ 0.2692],\n",
      "         [-0.1350],\n",
      "         [ 0.2912]],\n",
      "\n",
      "        [[-0.1729],\n",
      "         [ 0.1607],\n",
      "         [-0.1673],\n",
      "         [ 0.2052],\n",
      "         [-0.1134],\n",
      "         [ 0.1879]],\n",
      "\n",
      "        [[ 0.3456],\n",
      "         [-0.1041],\n",
      "         [ 0.0448],\n",
      "         [-0.0811],\n",
      "         [ 0.1414],\n",
      "         [-0.1332]],\n",
      "\n",
      "        [[ 0.6743],\n",
      "         [-0.0711],\n",
      "         [ 0.3880],\n",
      "         [-0.0647],\n",
      "         [ 0.2721],\n",
      "         [-0.1093]],\n",
      "\n",
      "        [[-0.2616],\n",
      "         [ 0.1755],\n",
      "         [-0.1016],\n",
      "         [ 0.3569],\n",
      "         [-0.1446],\n",
      "         [ 0.1483]],\n",
      "\n",
      "        [[ 0.3533],\n",
      "         [-0.0688],\n",
      "         [ 0.1122],\n",
      "         [-0.0494],\n",
      "         [ 0.1167],\n",
      "         [-0.1408]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[0.0000e+00],\n",
      "         [2.3283e-01],\n",
      "         [3.1773e-02],\n",
      "         [1.9858e-01],\n",
      "         [2.0191e-01],\n",
      "         [1.9618e-01]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [3.2917e-01],\n",
      "         [2.4560e-01],\n",
      "         [1.0390e-01],\n",
      "         [1.6898e-01],\n",
      "         [7.3389e-02]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [3.3906e-04],\n",
      "         [1.0116e-01],\n",
      "         [1.2488e-01],\n",
      "         [3.4160e-01],\n",
      "         [1.3396e-01]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [1.1135e-01],\n",
      "         [2.8137e-01],\n",
      "         [2.6642e-01],\n",
      "         [2.2427e-01],\n",
      "         [8.3877e-02]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [3.2442e-01],\n",
      "         [9.6762e-02],\n",
      "         [2.3128e-01],\n",
      "         [9.6357e-02],\n",
      "         [2.4656e-01]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [2.7223e-01],\n",
      "         [3.3658e-01],\n",
      "         [3.9976e-01],\n",
      "         [2.6889e-01],\n",
      "         [3.4049e-01]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [1.2816e-01],\n",
      "         [2.4658e-01],\n",
      "         [2.1261e-01],\n",
      "         [1.6402e-01],\n",
      "         [1.0862e-01]],\n",
      "\n",
      "        [[0.0000e+00],\n",
      "         [4.0371e-01],\n",
      "         [6.9994e-02],\n",
      "         [2.1439e-01],\n",
      "         [6.5707e-02],\n",
      "         [2.4412e-01]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [2.6935e-03, 9.9731e-01],\n",
      "         [9.9834e-01, 1.6641e-03],\n",
      "         [5.9519e-04, 9.9940e-01],\n",
      "         [9.9686e-01, 3.1379e-03],\n",
      "         [8.1033e-03, 9.9190e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9910e-01, 9.0427e-04],\n",
      "         [8.5915e-03, 9.9141e-01],\n",
      "         [9.8964e-01, 1.0362e-02],\n",
      "         [7.1891e-03, 9.9281e-01],\n",
      "         [9.8761e-01, 1.2388e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9915e-01, 8.4835e-04],\n",
      "         [7.8853e-04, 9.9921e-01],\n",
      "         [9.9823e-01, 1.7668e-03],\n",
      "         [2.2801e-03, 9.9772e-01],\n",
      "         [9.9553e-01, 4.4707e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9571e-01, 4.2873e-03],\n",
      "         [7.2404e-03, 9.9276e-01],\n",
      "         [9.9663e-01, 3.3732e-03],\n",
      "         [7.0631e-03, 9.9294e-01],\n",
      "         [9.9394e-01, 6.0573e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.6091e-03, 9.9839e-01],\n",
      "         [9.9635e-01, 3.6549e-03],\n",
      "         [1.3229e-02, 9.8677e-01],\n",
      "         [9.9473e-01, 5.2745e-03],\n",
      "         [2.1696e-03, 9.9783e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.8312e-03, 9.9517e-01],\n",
      "         [9.9817e-01, 1.8287e-03],\n",
      "         [1.1216e-03, 9.9888e-01],\n",
      "         [9.9096e-01, 9.0361e-03],\n",
      "         [5.5639e-03, 9.9444e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9921e-01, 7.8981e-04],\n",
      "         [3.1156e-03, 9.9688e-01],\n",
      "         [9.9752e-01, 2.4817e-03],\n",
      "         [1.4955e-02, 9.8505e-01],\n",
      "         [9.9490e-01, 5.1008e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.1293e-04, 9.9939e-01],\n",
      "         [9.9431e-01, 5.6896e-03],\n",
      "         [5.1571e-03, 9.9484e-01],\n",
      "         [9.9641e-01, 3.5942e-03],\n",
      "         [1.5890e-03, 9.9841e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 0, 1, 6, 2],\n",
      "        [7, 3, 6, 5, 1],\n",
      "        [7, 0, 6, 4, 8],\n",
      "        [7, 2, 5, 8, 0],\n",
      "        [4, 7, 0, 2, 6],\n",
      "        [0, 8, 2, 4, 6],\n",
      "        [1, 0, 1, 6, 2],\n",
      "        [3, 5, 1, 0, 2]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.9227, -0.9321,  0.9415, -0.9510,  0.9606, -0.9703],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.4726],\n",
      "         [-0.2016],\n",
      "         [ 0.0946],\n",
      "         [ 0.0669],\n",
      "         [ 0.0740],\n",
      "         [-0.0641]],\n",
      "\n",
      "        [[-0.1539],\n",
      "         [ 0.0965],\n",
      "         [-0.0185],\n",
      "         [ 0.0628],\n",
      "         [-0.0301],\n",
      "         [ 0.1882]],\n",
      "\n",
      "        [[ 0.2252],\n",
      "         [-0.0563],\n",
      "         [ 0.0096],\n",
      "         [-0.0823],\n",
      "         [ 0.1026],\n",
      "         [ 0.0487]],\n",
      "\n",
      "        [[ 0.1700],\n",
      "         [-0.0477],\n",
      "         [ 0.0472],\n",
      "         [-0.0136],\n",
      "         [ 0.0406],\n",
      "         [-0.0293]],\n",
      "\n",
      "        [[ 0.1700],\n",
      "         [-0.1933],\n",
      "         [ 0.2911],\n",
      "         [-0.0614],\n",
      "         [ 0.0335],\n",
      "         [ 0.0044]],\n",
      "\n",
      "        [[-0.0874],\n",
      "         [ 0.0120],\n",
      "         [-0.0208],\n",
      "         [-0.0048],\n",
      "         [ 0.0017],\n",
      "         [ 0.1434]],\n",
      "\n",
      "        [[ 0.0718],\n",
      "         [ 0.0290],\n",
      "         [ 0.0110],\n",
      "         [-0.0077],\n",
      "         [ 0.0541],\n",
      "         [-0.1051]],\n",
      "\n",
      "        [[ 0.2316],\n",
      "         [ 0.0202],\n",
      "         [ 0.0874],\n",
      "         [ 0.0526],\n",
      "         [-0.0044],\n",
      "         [-0.0920]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.3622],\n",
      "         [ 0.1117],\n",
      "         [ 0.1914],\n",
      "         [ 0.0774],\n",
      "         [ 0.1831]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1482],\n",
      "         [ 0.1447],\n",
      "         [ 0.1628],\n",
      "         [ 0.2284],\n",
      "         [ 0.2395]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0551],\n",
      "         [ 0.0671],\n",
      "         [ 0.1556],\n",
      "         [ 0.2014],\n",
      "         [ 0.4042]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0354],\n",
      "         [-0.0693],\n",
      "         [-0.0316],\n",
      "         [ 0.0241],\n",
      "         [-0.0291]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0988],\n",
      "         [-0.1412],\n",
      "         [-0.0282],\n",
      "         [ 0.0948],\n",
      "         [ 0.1370]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0652],\n",
      "         [-0.0751],\n",
      "         [ 0.0377],\n",
      "         [ 0.1911],\n",
      "         [ 0.2085]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3009],\n",
      "         [ 0.0058],\n",
      "         [ 0.0861],\n",
      "         [ 0.0293],\n",
      "         [ 0.1547]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4680],\n",
      "         [ 0.2591],\n",
      "         [ 0.4369],\n",
      "         [ 0.0763],\n",
      "         [ 0.0942]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [5.3428e-04, 9.9947e-01],\n",
      "         [9.9848e-01, 1.5231e-03],\n",
      "         [4.5317e-04, 9.9955e-01],\n",
      "         [9.8930e-01, 1.0696e-02],\n",
      "         [1.7118e-03, 9.9829e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9942e-01, 5.8249e-04],\n",
      "         [1.1966e-03, 9.9880e-01],\n",
      "         [9.9901e-01, 9.8921e-04],\n",
      "         [5.5853e-03, 9.9441e-01],\n",
      "         [9.9643e-01, 3.5703e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.0019e-04, 9.9990e-01],\n",
      "         [9.9887e-01, 1.1329e-03],\n",
      "         [4.1508e-03, 9.9585e-01],\n",
      "         [9.9953e-01, 4.7420e-04],\n",
      "         [2.7651e-03, 9.9723e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.9769e-04, 9.9980e-01],\n",
      "         [9.9979e-01, 2.1220e-04],\n",
      "         [3.3443e-04, 9.9967e-01],\n",
      "         [9.9951e-01, 4.9134e-04],\n",
      "         [4.6004e-03, 9.9540e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.8461e-04, 9.9982e-01],\n",
      "         [9.9980e-01, 2.0459e-04],\n",
      "         [8.3230e-04, 9.9917e-01],\n",
      "         [9.9842e-01, 1.5820e-03],\n",
      "         [2.3446e-03, 9.9766e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9976e-01, 2.3777e-04],\n",
      "         [3.3161e-03, 9.9668e-01],\n",
      "         [9.9963e-01, 3.6719e-04],\n",
      "         [4.1144e-03, 9.9589e-01],\n",
      "         [9.9881e-01, 1.1926e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.7749e-04, 9.9982e-01],\n",
      "         [9.9869e-01, 1.3057e-03],\n",
      "         [8.7964e-04, 9.9912e-01],\n",
      "         [9.9643e-01, 3.5686e-03],\n",
      "         [1.3367e-03, 9.9866e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.9590e-04, 9.9980e-01],\n",
      "         [9.9941e-01, 5.8892e-04],\n",
      "         [1.9865e-04, 9.9980e-01],\n",
      "         [9.9476e-01, 5.2394e-03],\n",
      "         [6.0487e-03, 9.9395e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 0, 0, 1, 5],\n",
      "        [6, 0, 2, 5, 4],\n",
      "        [2, 8, 4, 3, 5],\n",
      "        [1, 4, 3, 7, 8],\n",
      "        [4, 7, 0, 3, 2],\n",
      "        [8, 1, 0, 1, 5],\n",
      "        [3, 2, 5, 6, 0],\n",
      "        [5, 0, 6, 1, 5]])\n",
      "target value tensor([[-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.2980],\n",
      "         [ 0.4277],\n",
      "         [ 0.0143],\n",
      "         [ 0.0928],\n",
      "         [ 0.0800],\n",
      "         [ 0.0669]],\n",
      "\n",
      "        [[ 0.2177],\n",
      "         [-0.1401],\n",
      "         [ 0.1635],\n",
      "         [-0.0336],\n",
      "         [ 0.1400],\n",
      "         [-0.0108]],\n",
      "\n",
      "        [[ 0.0960],\n",
      "         [-0.1805],\n",
      "         [ 0.2363],\n",
      "         [ 0.0671],\n",
      "         [ 0.2950],\n",
      "         [ 0.0499]],\n",
      "\n",
      "        [[-0.3734],\n",
      "         [ 0.3443],\n",
      "         [-0.1236],\n",
      "         [ 0.4447],\n",
      "         [ 0.0174],\n",
      "         [ 0.3293]],\n",
      "\n",
      "        [[-0.2565],\n",
      "         [ 0.2122],\n",
      "         [-0.0101],\n",
      "         [ 0.1550],\n",
      "         [ 0.0522],\n",
      "         [ 0.2330]],\n",
      "\n",
      "        [[ 0.0860],\n",
      "         [ 0.0243],\n",
      "         [ 0.3345],\n",
      "         [ 0.0172],\n",
      "         [ 0.1511],\n",
      "         [ 0.0184]],\n",
      "\n",
      "        [[ 0.2570],\n",
      "         [-0.0537],\n",
      "         [ 0.4387],\n",
      "         [ 0.0409],\n",
      "         [ 0.4379],\n",
      "         [ 0.0050]],\n",
      "\n",
      "        [[ 0.5275],\n",
      "         [ 0.0166],\n",
      "         [ 0.1200],\n",
      "         [ 0.0848],\n",
      "         [ 0.1710],\n",
      "         [ 0.0225]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [ 4.3275e-02],\n",
      "         [ 3.2664e-01],\n",
      "         [-2.1919e-03],\n",
      "         [ 1.6120e-02],\n",
      "         [ 4.3922e-04]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 4.3701e-02],\n",
      "         [-9.5680e-03],\n",
      "         [ 1.9586e-02],\n",
      "         [ 9.7479e-03],\n",
      "         [ 2.3999e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 8.8090e-03],\n",
      "         [ 5.4853e-02],\n",
      "         [ 3.2677e-01],\n",
      "         [ 1.7437e-01],\n",
      "         [ 2.4165e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.6349e-01],\n",
      "         [ 4.2347e-01],\n",
      "         [ 2.0170e-01],\n",
      "         [ 3.9144e-01],\n",
      "         [ 2.8662e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-6.2715e-02],\n",
      "         [-6.7078e-03],\n",
      "         [ 5.1223e-02],\n",
      "         [ 2.0957e-01],\n",
      "         [ 1.6688e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 4.0244e-01],\n",
      "         [ 3.1461e-01],\n",
      "         [ 6.8350e-02],\n",
      "         [ 4.8701e-02],\n",
      "         [ 9.0204e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.1236e-01],\n",
      "         [ 9.6843e-02],\n",
      "         [ 2.6004e-01],\n",
      "         [ 2.6625e-01],\n",
      "         [ 3.8714e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 6.3615e-01],\n",
      "         [ 8.8274e-02],\n",
      "         [ 5.1738e-02],\n",
      "         [ 5.6015e-02],\n",
      "         [ 8.2075e-02]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9973e-01, 2.6806e-04],\n",
      "         [3.0491e-04, 9.9970e-01],\n",
      "         [9.9692e-01, 3.0782e-03],\n",
      "         [1.0901e-03, 9.9891e-01],\n",
      "         [9.9375e-01, 6.2545e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.3427e-04, 9.9987e-01],\n",
      "         [9.9965e-01, 3.4833e-04],\n",
      "         [1.8888e-03, 9.9811e-01],\n",
      "         [9.9746e-01, 2.5357e-03],\n",
      "         [1.2092e-03, 9.9879e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.1653e-04, 9.9988e-01],\n",
      "         [9.9953e-01, 4.6537e-04],\n",
      "         [3.1957e-04, 9.9968e-01],\n",
      "         [9.9520e-01, 4.8022e-03],\n",
      "         [5.0741e-04, 9.9949e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9960e-01, 3.9818e-04],\n",
      "         [6.4193e-05, 9.9994e-01],\n",
      "         [9.9728e-01, 2.7188e-03],\n",
      "         [4.0868e-05, 9.9996e-01],\n",
      "         [9.9896e-01, 1.0440e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9949e-01, 5.0596e-04],\n",
      "         [4.0543e-05, 9.9996e-01],\n",
      "         [9.9935e-01, 6.5004e-04],\n",
      "         [8.1083e-05, 9.9992e-01],\n",
      "         [9.9930e-01, 6.9643e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.3866e-04, 9.9986e-01],\n",
      "         [9.9754e-01, 2.4552e-03],\n",
      "         [1.2092e-03, 9.9879e-01],\n",
      "         [9.8093e-01, 1.9072e-02],\n",
      "         [4.7074e-04, 9.9953e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.7516e-05, 9.9992e-01],\n",
      "         [9.9972e-01, 2.8102e-04],\n",
      "         [8.5312e-05, 9.9991e-01],\n",
      "         [9.9823e-01, 1.7686e-03],\n",
      "         [7.0457e-04, 9.9930e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.1325e-04, 9.9989e-01],\n",
      "         [9.9726e-01, 2.7446e-03],\n",
      "         [8.2703e-04, 9.9917e-01],\n",
      "         [9.7332e-01, 2.6684e-02],\n",
      "         [1.2175e-03, 9.9878e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 5, 4, 1, 6],\n",
      "        [6, 7, 0, 1, 4],\n",
      "        [2, 0, 5, 1, 4],\n",
      "        [8, 3, 5, 7, 1],\n",
      "        [0, 5, 0, 1, 4],\n",
      "        [0, 0, 7, 1, 4],\n",
      "        [4, 0, 1, 8, 3],\n",
      "        [0, 1, 7, 5, 0]])\n",
      "target value tensor([[ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.1289],\n",
      "         [-0.0999],\n",
      "         [ 0.3482],\n",
      "         [-0.2719],\n",
      "         [ 0.6425],\n",
      "         [-0.3376]],\n",
      "\n",
      "        [[ 0.2775],\n",
      "         [ 0.2274],\n",
      "         [ 0.1038],\n",
      "         [-0.0291],\n",
      "         [ 0.0302],\n",
      "         [-0.0457]],\n",
      "\n",
      "        [[ 0.2549],\n",
      "         [-0.1624],\n",
      "         [ 0.1530],\n",
      "         [ 0.0449],\n",
      "         [ 0.2857],\n",
      "         [-0.1264]],\n",
      "\n",
      "        [[ 0.1741],\n",
      "         [ 0.0319],\n",
      "         [ 0.1454],\n",
      "         [ 0.0506],\n",
      "         [ 0.1737],\n",
      "         [ 0.0426]],\n",
      "\n",
      "        [[-0.1486],\n",
      "         [ 0.0803],\n",
      "         [ 0.0627],\n",
      "         [-0.0541],\n",
      "         [ 0.0336],\n",
      "         [-0.0123]],\n",
      "\n",
      "        [[ 0.6767],\n",
      "         [-0.0967],\n",
      "         [-0.0163],\n",
      "         [ 0.0074],\n",
      "         [ 0.0094],\n",
      "         [-0.0665]],\n",
      "\n",
      "        [[ 0.2415],\n",
      "         [-0.2491],\n",
      "         [ 0.0376],\n",
      "         [ 0.0732],\n",
      "         [ 0.0659],\n",
      "         [ 0.0542]],\n",
      "\n",
      "        [[ 0.0771],\n",
      "         [ 0.0430],\n",
      "         [ 0.0616],\n",
      "         [ 0.0757],\n",
      "         [ 0.0099],\n",
      "         [-0.0590]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [-0.1192],\n",
      "         [-0.0700],\n",
      "         [-0.1017],\n",
      "         [-0.0528],\n",
      "         [ 0.0873]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3576],\n",
      "         [ 0.4574],\n",
      "         [ 0.0552],\n",
      "         [ 0.0230],\n",
      "         [ 0.0707]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1346],\n",
      "         [-0.0011],\n",
      "         [ 0.0200],\n",
      "         [ 0.1015],\n",
      "         [ 0.3211]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2685],\n",
      "         [ 0.1660],\n",
      "         [ 0.2808],\n",
      "         [ 0.2672],\n",
      "         [ 0.3554]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2513],\n",
      "         [ 0.3024],\n",
      "         [-0.0184],\n",
      "         [-0.0153],\n",
      "         [ 0.0705]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4039],\n",
      "         [ 0.0396],\n",
      "         [-0.0084],\n",
      "         [-0.0152],\n",
      "         [ 0.0942]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0095],\n",
      "         [ 0.0398],\n",
      "         [ 0.1706],\n",
      "         [ 0.3059],\n",
      "         [ 0.3627]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2260],\n",
      "         [ 0.3010],\n",
      "         [ 0.3139],\n",
      "         [ 0.4036],\n",
      "         [-0.0364]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.3044e-04, 9.9987e-01],\n",
      "         [9.9983e-01, 1.6705e-04],\n",
      "         [1.9219e-05, 9.9998e-01],\n",
      "         [9.9860e-01, 1.3974e-03],\n",
      "         [1.8601e-05, 9.9998e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9867e-01, 1.3317e-03],\n",
      "         [1.2988e-04, 9.9987e-01],\n",
      "         [9.9797e-01, 2.0257e-03],\n",
      "         [1.7977e-02, 9.8202e-01],\n",
      "         [9.7681e-01, 2.3188e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.2618e-04, 9.9977e-01],\n",
      "         [9.9994e-01, 6.1570e-05],\n",
      "         [1.4648e-04, 9.9985e-01],\n",
      "         [9.9868e-01, 1.3161e-03],\n",
      "         [5.6871e-05, 9.9994e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.0826e-05, 9.9994e-01],\n",
      "         [9.9947e-01, 5.2675e-04],\n",
      "         [6.1434e-04, 9.9939e-01],\n",
      "         [9.9841e-01, 1.5933e-03],\n",
      "         [7.2702e-04, 9.9927e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9988e-01, 1.2361e-04],\n",
      "         [3.6397e-04, 9.9964e-01],\n",
      "         [9.9752e-01, 2.4781e-03],\n",
      "         [1.1437e-01, 8.8563e-01],\n",
      "         [9.2277e-01, 7.7232e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.8475e-04, 9.9982e-01],\n",
      "         [9.9878e-01, 1.2179e-03],\n",
      "         [9.7174e-03, 9.9028e-01],\n",
      "         [9.7854e-01, 2.1461e-02],\n",
      "         [3.5871e-03, 9.9641e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.7636e-05, 9.9990e-01],\n",
      "         [9.9984e-01, 1.6291e-04],\n",
      "         [1.4597e-04, 9.9985e-01],\n",
      "         [9.9981e-01, 1.9349e-04],\n",
      "         [7.8809e-05, 9.9992e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9990e-01, 9.6975e-05],\n",
      "         [5.6276e-05, 9.9994e-01],\n",
      "         [9.9923e-01, 7.6940e-04],\n",
      "         [8.7801e-04, 9.9912e-01],\n",
      "         [9.9361e-01, 6.3887e-03]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 8, 0, 8, 5],\n",
      "        [4, 7, 0, 8, 5],\n",
      "        [8, 3, 4, 7, 1],\n",
      "        [8, 5, 0, 3, 0],\n",
      "        [2, 5, 6, 0, 5],\n",
      "        [0, 3, 7, 0, 5],\n",
      "        [0, 2, 4, 0, 5],\n",
      "        [7, 0, 8, 4, 2]])\n",
      "target value tensor([[-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.4306],\n",
      "         [ 0.0462],\n",
      "         [ 0.2394],\n",
      "         [-0.0298],\n",
      "         [ 0.1784],\n",
      "         [ 0.0642]],\n",
      "\n",
      "        [[ 0.1477],\n",
      "         [-0.0295],\n",
      "         [ 0.3546],\n",
      "         [-0.0535],\n",
      "         [ 0.0427],\n",
      "         [ 0.0500]],\n",
      "\n",
      "        [[-0.2498],\n",
      "         [ 0.2640],\n",
      "         [-0.0669],\n",
      "         [-0.0491],\n",
      "         [ 0.1810],\n",
      "         [ 0.4921]],\n",
      "\n",
      "        [[-0.3935],\n",
      "         [ 0.3975],\n",
      "         [-0.0281],\n",
      "         [ 0.0860],\n",
      "         [ 0.1010],\n",
      "         [-0.0457]],\n",
      "\n",
      "        [[-0.2037],\n",
      "         [ 0.0998],\n",
      "         [ 0.1533],\n",
      "         [ 0.1456],\n",
      "         [-0.0022],\n",
      "         [ 0.0530]],\n",
      "\n",
      "        [[ 0.0431],\n",
      "         [ 0.1976],\n",
      "         [ 0.2140],\n",
      "         [ 0.1832],\n",
      "         [ 0.0045],\n",
      "         [ 0.0377]],\n",
      "\n",
      "        [[ 0.1471],\n",
      "         [-0.0916],\n",
      "         [ 0.0076],\n",
      "         [ 0.1032],\n",
      "         [ 0.0296],\n",
      "         [ 0.0920]],\n",
      "\n",
      "        [[-0.1390],\n",
      "         [ 0.1045],\n",
      "         [-0.1499],\n",
      "         [ 0.0046],\n",
      "         [-0.0085],\n",
      "         [ 0.0355]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.1280],\n",
      "         [ 0.1080],\n",
      "         [ 0.1965],\n",
      "         [ 0.0502],\n",
      "         [ 0.1301]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4030],\n",
      "         [ 0.5164],\n",
      "         [ 0.0084],\n",
      "         [-0.0260],\n",
      "         [-0.0521]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0694],\n",
      "         [-0.0847],\n",
      "         [-0.0455],\n",
      "         [ 0.0090],\n",
      "         [ 0.0017]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0523],\n",
      "         [ 0.1854],\n",
      "         [ 0.2111],\n",
      "         [ 0.2509],\n",
      "         [-0.0783]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0711],\n",
      "         [ 0.1370],\n",
      "         [ 0.1695],\n",
      "         [ 0.0352],\n",
      "         [-0.0249]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1340],\n",
      "         [ 0.2845],\n",
      "         [ 0.2776],\n",
      "         [ 0.0562],\n",
      "         [-0.0529]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0339],\n",
      "         [ 0.0489],\n",
      "         [ 0.2307],\n",
      "         [ 0.0662],\n",
      "         [-0.0061]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0034],\n",
      "         [ 0.0014],\n",
      "         [ 0.1106],\n",
      "         [ 0.2310],\n",
      "         [ 0.2617]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [8.0614e-05, 9.9992e-01],\n",
      "         [9.9945e-01, 5.5395e-04],\n",
      "         [4.2055e-04, 9.9958e-01],\n",
      "         [9.9898e-01, 1.0215e-03],\n",
      "         [8.2044e-04, 9.9918e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9951e-01, 4.8533e-04],\n",
      "         [1.1140e-04, 9.9989e-01],\n",
      "         [9.9416e-01, 5.8433e-03],\n",
      "         [1.7820e-02, 9.8218e-01],\n",
      "         [9.6799e-01, 3.2008e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9972e-01, 2.8272e-04],\n",
      "         [5.8774e-05, 9.9994e-01],\n",
      "         [9.9989e-01, 1.0664e-04],\n",
      "         [1.2055e-04, 9.9988e-01],\n",
      "         [9.9933e-01, 6.7478e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9970e-01, 2.9658e-04],\n",
      "         [3.1699e-05, 9.9997e-01],\n",
      "         [9.9976e-01, 2.4063e-04],\n",
      "         [2.4112e-04, 9.9976e-01],\n",
      "         [9.9637e-01, 3.6285e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9989e-01, 1.1259e-04],\n",
      "         [2.0674e-04, 9.9979e-01],\n",
      "         [9.9973e-01, 2.6889e-04],\n",
      "         [3.5412e-03, 9.9646e-01],\n",
      "         [9.8974e-01, 1.0264e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9922e-01, 7.7717e-04],\n",
      "         [4.5673e-05, 9.9995e-01],\n",
      "         [9.9965e-01, 3.4818e-04],\n",
      "         [6.5578e-03, 9.9344e-01],\n",
      "         [9.8996e-01, 1.0035e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9993e-01, 7.0379e-05],\n",
      "         [9.5411e-05, 9.9990e-01],\n",
      "         [9.9973e-01, 2.7154e-04],\n",
      "         [2.3313e-03, 9.9767e-01],\n",
      "         [9.9669e-01, 3.3063e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.9532e-05, 9.9993e-01],\n",
      "         [9.9997e-01, 3.1658e-05],\n",
      "         [2.0902e-04, 9.9979e-01],\n",
      "         [9.9979e-01, 2.0520e-04],\n",
      "         [5.1695e-04, 9.9948e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 0, 2, 8, 8],\n",
      "        [8, 0, 2, 8, 8],\n",
      "        [6, 8, 7, 4, 0],\n",
      "        [0, 2, 3, 6, 8],\n",
      "        [4, 7, 0, 6, 2],\n",
      "        [6, 2, 0, 7, 1],\n",
      "        [7, 0, 2, 8, 8],\n",
      "        [8, 4, 7, 0, 8]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.3362],\n",
      "         [-0.1035],\n",
      "         [ 0.0267],\n",
      "         [-0.0828],\n",
      "         [-0.0499],\n",
      "         [-0.0459]],\n",
      "\n",
      "        [[ 0.2122],\n",
      "         [ 0.1272],\n",
      "         [-0.0758],\n",
      "         [-0.0597],\n",
      "         [-0.0624],\n",
      "         [-0.0454]],\n",
      "\n",
      "        [[-0.0949],\n",
      "         [ 0.0389],\n",
      "         [ 0.1043],\n",
      "         [-0.0196],\n",
      "         [-0.0238],\n",
      "         [-0.0650]],\n",
      "\n",
      "        [[ 0.1546],\n",
      "         [-0.1199],\n",
      "         [ 0.0942],\n",
      "         [ 0.0634],\n",
      "         [-0.0778],\n",
      "         [ 0.0195]],\n",
      "\n",
      "        [[ 0.2894],\n",
      "         [-0.0858],\n",
      "         [ 0.0220],\n",
      "         [-0.1374],\n",
      "         [-0.0153],\n",
      "         [ 0.0210]],\n",
      "\n",
      "        [[ 0.1508],\n",
      "         [-0.2805],\n",
      "         [ 0.0391],\n",
      "         [-0.1289],\n",
      "         [ 0.0098],\n",
      "         [ 0.0582]],\n",
      "\n",
      "        [[ 0.4399],\n",
      "         [ 0.0661],\n",
      "         [-0.0654],\n",
      "         [-0.0510],\n",
      "         [-0.0506],\n",
      "         [-0.0458]],\n",
      "\n",
      "        [[-0.2634],\n",
      "         [ 0.2527],\n",
      "         [-0.0963],\n",
      "         [ 0.0880],\n",
      "         [-0.0820],\n",
      "         [-0.0350]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.1700],\n",
      "         [ 0.0108],\n",
      "         [ 0.0106],\n",
      "         [ 0.0510],\n",
      "         [ 0.0677]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3086],\n",
      "         [ 0.0649],\n",
      "         [ 0.0397],\n",
      "         [ 0.0466],\n",
      "         [ 0.0464]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1386],\n",
      "         [-0.0079],\n",
      "         [ 0.0073],\n",
      "         [ 0.1770],\n",
      "         [-0.0390]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1268],\n",
      "         [-0.0833],\n",
      "         [-0.0975],\n",
      "         [ 0.0158],\n",
      "         [ 0.0305]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0648],\n",
      "         [-0.0934],\n",
      "         [-0.0402],\n",
      "         [-0.0296],\n",
      "         [ 0.0974]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0485],\n",
      "         [-0.0418],\n",
      "         [ 0.0732],\n",
      "         [ 0.0973],\n",
      "         [ 0.1769]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3808],\n",
      "         [ 0.0608],\n",
      "         [ 0.0793],\n",
      "         [ 0.0425],\n",
      "         [ 0.0407]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0067],\n",
      "         [ 0.2387],\n",
      "         [ 0.1732],\n",
      "         [ 0.0119],\n",
      "         [ 0.0322]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [4.2166e-06, 1.0000e+00],\n",
      "         [9.9939e-01, 6.0993e-04],\n",
      "         [1.6755e-03, 9.9832e-01],\n",
      "         [9.9373e-01, 6.2682e-03],\n",
      "         [4.3631e-03, 9.9564e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.7967e-06, 9.9999e-01],\n",
      "         [9.9857e-01, 1.4253e-03],\n",
      "         [2.0137e-03, 9.9799e-01],\n",
      "         [9.7981e-01, 2.0191e-02],\n",
      "         [7.7373e-03, 9.9226e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.5729e-06, 9.9999e-01],\n",
      "         [9.9989e-01, 1.1122e-04],\n",
      "         [1.9717e-05, 9.9998e-01],\n",
      "         [9.9979e-01, 2.1116e-04],\n",
      "         [8.5714e-04, 9.9914e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [8.3926e-05, 9.9992e-01],\n",
      "         [9.9995e-01, 4.9436e-05],\n",
      "         [7.6242e-05, 9.9992e-01],\n",
      "         [9.9985e-01, 1.5415e-04],\n",
      "         [1.2129e-04, 9.9988e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9760e-01, 2.3978e-03],\n",
      "         [2.8615e-05, 9.9997e-01],\n",
      "         [9.9979e-01, 2.1377e-04],\n",
      "         [9.0189e-05, 9.9991e-01],\n",
      "         [9.9989e-01, 1.1260e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.7922e-06, 9.9999e-01],\n",
      "         [9.9993e-01, 7.0673e-05],\n",
      "         [7.0395e-05, 9.9993e-01],\n",
      "         [9.9992e-01, 7.8819e-05],\n",
      "         [8.6560e-05, 9.9991e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.9564e-05, 9.9997e-01],\n",
      "         [9.8415e-01, 1.5845e-02],\n",
      "         [4.6228e-03, 9.9538e-01],\n",
      "         [9.0618e-01, 9.3822e-02],\n",
      "         [2.4369e-02, 9.7563e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9983e-01, 1.7103e-04],\n",
      "         [4.2855e-05, 9.9996e-01],\n",
      "         [9.9948e-01, 5.1961e-04],\n",
      "         [1.1457e-03, 9.9885e-01],\n",
      "         [9.9379e-01, 6.2116e-03]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 0, 2, 1, 3],\n",
      "        [7, 2, 0, 1, 3],\n",
      "        [7, 5, 0, 1, 3],\n",
      "        [3, 0, 2, 1, 3],\n",
      "        [0, 7, 4, 6, 0],\n",
      "        [4, 2, 7, 8, 0],\n",
      "        [2, 7, 6, 8, 0],\n",
      "        [6, 0, 2, 1, 3]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.6930],\n",
      "         [-0.0818],\n",
      "         [ 0.1397],\n",
      "         [ 0.0573],\n",
      "         [ 0.1089],\n",
      "         [ 0.1106]],\n",
      "\n",
      "        [[ 0.2494],\n",
      "         [ 0.0562],\n",
      "         [ 0.1119],\n",
      "         [ 0.0806],\n",
      "         [ 0.1093],\n",
      "         [ 0.1277]],\n",
      "\n",
      "        [[ 0.0805],\n",
      "         [ 0.3585],\n",
      "         [ 0.1059],\n",
      "         [ 0.0253],\n",
      "         [ 0.1145],\n",
      "         [ 0.1075]],\n",
      "\n",
      "        [[ 0.5467],\n",
      "         [ 0.1509],\n",
      "         [ 0.0688],\n",
      "         [ 0.0795],\n",
      "         [ 0.1112],\n",
      "         [ 0.1119]],\n",
      "\n",
      "        [[ 0.1643],\n",
      "         [-0.0557],\n",
      "         [ 0.1807],\n",
      "         [ 0.0756],\n",
      "         [ 0.2589],\n",
      "         [ 0.0378]],\n",
      "\n",
      "        [[-0.0184],\n",
      "         [-0.0974],\n",
      "         [-0.0272],\n",
      "         [ 0.1459],\n",
      "         [ 0.0150],\n",
      "         [ 0.1139]],\n",
      "\n",
      "        [[-0.4091],\n",
      "         [ 0.3117],\n",
      "         [ 0.0976],\n",
      "         [ 0.1785],\n",
      "         [ 0.1706],\n",
      "         [ 0.0279]],\n",
      "\n",
      "        [[ 0.0916],\n",
      "         [-0.0073],\n",
      "         [ 0.0627],\n",
      "         [ 0.0311],\n",
      "         [ 0.1088],\n",
      "         [ 0.1092]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [ 7.0767e-01],\n",
      "         [ 1.4852e-01],\n",
      "         [ 1.1492e-01],\n",
      "         [ 7.2635e-02],\n",
      "         [ 6.9614e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 2.9032e-01],\n",
      "         [ 2.4297e-01],\n",
      "         [ 7.0564e-02],\n",
      "         [ 7.9761e-02],\n",
      "         [ 1.0291e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 3.7923e-01],\n",
      "         [ 5.0898e-01],\n",
      "         [ 6.4849e-02],\n",
      "         [ 2.4912e-02],\n",
      "         [ 2.7431e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 5.5540e-01],\n",
      "         [ 5.2668e-02],\n",
      "         [ 4.5914e-02],\n",
      "         [ 4.6275e-02],\n",
      "         [ 4.7381e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 8.3604e-02],\n",
      "         [ 4.8388e-02],\n",
      "         [ 2.5624e-01],\n",
      "         [ 2.8761e-01],\n",
      "         [ 2.8725e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-1.0522e-01],\n",
      "         [-6.6768e-02],\n",
      "         [-8.9281e-02],\n",
      "         [-2.8255e-02],\n",
      "         [ 5.5167e-04]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-2.2752e-02],\n",
      "         [ 1.6563e-01],\n",
      "         [ 2.6193e-01],\n",
      "         [ 5.3021e-01],\n",
      "         [ 8.0881e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.7577e-01],\n",
      "         [ 3.2518e-02],\n",
      "         [ 2.1557e-02],\n",
      "         [ 8.0204e-02],\n",
      "         [ 7.5950e-02]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.8914e-01, 1.0864e-02],\n",
      "         [4.8600e-02, 9.5140e-01],\n",
      "         [9.3625e-01, 6.3752e-02],\n",
      "         [2.0022e-02, 9.7998e-01],\n",
      "         [6.7148e-01, 3.2852e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [8.9179e-06, 9.9999e-01],\n",
      "         [9.9994e-01, 6.4182e-05],\n",
      "         [1.6778e-03, 9.9832e-01],\n",
      "         [9.8856e-01, 1.1445e-02],\n",
      "         [1.0659e-02, 9.8934e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9963e-01, 3.7493e-04],\n",
      "         [9.1472e-04, 9.9909e-01],\n",
      "         [9.8501e-01, 1.4985e-02],\n",
      "         [4.1481e-02, 9.5852e-01],\n",
      "         [7.2901e-01, 2.7099e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9938e-01, 6.2183e-04],\n",
      "         [6.3124e-02, 9.3688e-01],\n",
      "         [6.1744e-01, 3.8256e-01],\n",
      "         [5.2410e-02, 9.4759e-01],\n",
      "         [5.1352e-01, 4.8648e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.7434e-05, 9.9998e-01],\n",
      "         [9.9981e-01, 1.8777e-04],\n",
      "         [2.1900e-04, 9.9978e-01],\n",
      "         [9.9737e-01, 2.6253e-03],\n",
      "         [1.0971e-01, 8.9029e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9875e-01, 1.2548e-03],\n",
      "         [2.8040e-03, 9.9720e-01],\n",
      "         [9.9961e-01, 3.8848e-04],\n",
      "         [7.6795e-05, 9.9992e-01],\n",
      "         [9.9991e-01, 8.7314e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9987e-01, 1.2671e-04],\n",
      "         [8.8945e-05, 9.9991e-01],\n",
      "         [9.9911e-01, 8.8802e-04],\n",
      "         [2.6177e-03, 9.9738e-01],\n",
      "         [9.5805e-01, 4.1950e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.0802e-05, 9.9998e-01],\n",
      "         [9.9871e-01, 1.2922e-03],\n",
      "         [5.5312e-02, 9.4469e-01],\n",
      "         [6.7212e-01, 3.2788e-01],\n",
      "         [8.0807e-02, 9.1919e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[5, 4, 0, 7, 3],\n",
      "        [4, 6, 2, 5, 7],\n",
      "        [7, 2, 0, 6, 0],\n",
      "        [8, 3, 0, 5, 0],\n",
      "        [0, 3, 5, 1, 0],\n",
      "        [6, 0, 0, 5, 0],\n",
      "        [7, 5, 0, 5, 0],\n",
      "        [2, 7, 8, 1, 0]])\n",
      "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.0041],\n",
      "         [-0.0167],\n",
      "         [ 0.0741],\n",
      "         [ 0.0223],\n",
      "         [ 0.2365],\n",
      "         [ 0.0429]],\n",
      "\n",
      "        [[ 0.1989],\n",
      "         [-0.2679],\n",
      "         [ 0.2195],\n",
      "         [-0.1441],\n",
      "         [ 0.3123],\n",
      "         [ 0.0043]],\n",
      "\n",
      "        [[-0.1780],\n",
      "         [ 0.1755],\n",
      "         [-0.2766],\n",
      "         [ 0.0568],\n",
      "         [ 0.0196],\n",
      "         [-0.0446]],\n",
      "\n",
      "        [[-0.1859],\n",
      "         [ 0.2302],\n",
      "         [-0.0431],\n",
      "         [ 0.0279],\n",
      "         [ 0.0321],\n",
      "         [-0.0440]],\n",
      "\n",
      "        [[ 0.0587],\n",
      "         [ 0.0981],\n",
      "         [ 0.1094],\n",
      "         [ 0.1709],\n",
      "         [ 0.0672],\n",
      "         [-0.0184]],\n",
      "\n",
      "        [[ 0.0262],\n",
      "         [ 0.0868],\n",
      "         [ 0.0512],\n",
      "         [-0.0240],\n",
      "         [ 0.0171],\n",
      "         [-0.0370]],\n",
      "\n",
      "        [[ 0.4262],\n",
      "         [ 0.1501],\n",
      "         [ 0.0782],\n",
      "         [-0.0260],\n",
      "         [ 0.0442],\n",
      "         [-0.0487]],\n",
      "\n",
      "        [[-0.0898],\n",
      "         [ 0.1648],\n",
      "         [ 0.0430],\n",
      "         [ 0.0855],\n",
      "         [ 0.1249],\n",
      "         [-0.0468]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [-0.0188],\n",
      "         [ 0.2903],\n",
      "         [ 0.2944],\n",
      "         [ 0.4125],\n",
      "         [ 0.4841]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0338],\n",
      "         [-0.1111],\n",
      "         [ 0.0008],\n",
      "         [ 0.0392],\n",
      "         [ 0.0804]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1104],\n",
      "         [-0.0236],\n",
      "         [-0.1209],\n",
      "         [ 0.1025],\n",
      "         [ 0.0145]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1847],\n",
      "         [ 0.3955],\n",
      "         [ 0.0850],\n",
      "         [ 0.0273],\n",
      "         [-0.0701]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2576],\n",
      "         [ 0.3649],\n",
      "         [ 0.4399],\n",
      "         [ 0.4714],\n",
      "         [ 0.0310]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3914],\n",
      "         [ 0.2774],\n",
      "         [-0.0301],\n",
      "         [-0.0503],\n",
      "         [-0.1038]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3320],\n",
      "         [ 0.3604],\n",
      "         [ 0.0443],\n",
      "         [-0.0236],\n",
      "         [-0.0832]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0007],\n",
      "         [ 0.0930],\n",
      "         [ 0.1478],\n",
      "         [ 0.3325],\n",
      "         [ 0.1004]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.6091e-06, 1.0000e+00],\n",
      "         [9.9995e-01, 4.5969e-05],\n",
      "         [5.6770e-06, 9.9999e-01],\n",
      "         [9.9975e-01, 2.5328e-04],\n",
      "         [4.2885e-05, 9.9996e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.6233e-06, 1.0000e+00],\n",
      "         [9.9998e-01, 1.6799e-05],\n",
      "         [2.8090e-05, 9.9997e-01],\n",
      "         [9.9996e-01, 4.3562e-05],\n",
      "         [4.6324e-05, 9.9995e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9995e-01, 4.5322e-05],\n",
      "         [2.5883e-05, 9.9997e-01],\n",
      "         [9.9994e-01, 5.5046e-05],\n",
      "         [1.2316e-04, 9.9988e-01],\n",
      "         [9.9906e-01, 9.3558e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9994e-01, 6.1743e-05],\n",
      "         [2.7812e-05, 9.9997e-01],\n",
      "         [9.9972e-01, 2.7672e-04],\n",
      "         [4.4891e-03, 9.9551e-01],\n",
      "         [9.8791e-01, 1.2092e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9993e-01, 6.5423e-05],\n",
      "         [1.3234e-05, 9.9999e-01],\n",
      "         [9.9979e-01, 2.0942e-04],\n",
      "         [4.3578e-05, 9.9996e-01],\n",
      "         [9.9735e-01, 2.6534e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.5951e-05, 9.9997e-01],\n",
      "         [9.9956e-01, 4.4136e-04],\n",
      "         [3.8888e-02, 9.6111e-01],\n",
      "         [9.9213e-01, 7.8677e-03],\n",
      "         [2.9833e-01, 7.0167e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9924e-01, 7.5665e-04],\n",
      "         [5.3330e-05, 9.9995e-01],\n",
      "         [9.9921e-01, 7.9062e-04],\n",
      "         [8.2923e-03, 9.9171e-01],\n",
      "         [9.6674e-01, 3.3259e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.4227e-05],\n",
      "         [3.8342e-04, 9.9962e-01],\n",
      "         [9.9943e-01, 5.6623e-04],\n",
      "         [1.6766e-04, 9.9983e-01],\n",
      "         [9.9684e-01, 3.1613e-03]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.4800, 0.0400, 0.1200, 0.0400, 0.0800]), tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.4800, 0.0400, 0.1200, 0.0400, 0.0800]), 0.18369459778366698, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1200, 0.0800, 0.1600, 0.0000, 0.0000, 0.1200, 0.3600, 0.0400, 0.1200]), tensor([0.1200, 0.0800, 0.1600, 0.0000, 0.0000, 0.1200, 0.3600, 0.0400, 0.1200]), 0.4036472354388077, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 1\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.2800, 0.0000, 0.2800, 0.0000, 0.0000, 0.1600, 0.0000, 0.0800, 0.2000]), tensor([0.2800, 0.0000, 0.2800, 0.0000, 0.0000, 0.1600, 0.0000, 0.0800, 0.2000]), 0.5478772248652115, tensor(0))\n",
      "action: 0\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.5600, 0.0000, 0.0000, 0.3600, 0.0000, 0.0800, 0.0000]), tensor([0.0000, 0.0000, 0.5600, 0.0000, 0.0000, 0.3600, 0.0000, 0.0800, 0.0000]), 0.5981234418650102, tensor(2))\n",
      "action: 2\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 8, 0, 6, 2],\n",
      "        [2, 3, 5, 4, 7],\n",
      "        [4, 0, 3, 5, 7],\n",
      "        [6, 1, 5, 3, 0],\n",
      "        [8, 4, 7, 2, 6],\n",
      "        [3, 1, 5, 0, 2],\n",
      "        [0, 6, 8, 0, 2],\n",
      "        [5, 8, 1, 3, 7]])\n",
      "target value tensor([[-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.2730],\n",
      "         [ 0.4920],\n",
      "         [ 0.0306],\n",
      "         [ 0.0102],\n",
      "         [ 0.0136],\n",
      "         [-0.0085]],\n",
      "\n",
      "        [[-0.1294],\n",
      "         [ 0.1855],\n",
      "         [ 0.0738],\n",
      "         [ 0.0559],\n",
      "         [ 0.0118],\n",
      "         [ 0.2536]],\n",
      "\n",
      "        [[-0.1983],\n",
      "         [ 0.2649],\n",
      "         [-0.3011],\n",
      "         [ 0.4619],\n",
      "         [-0.1497],\n",
      "         [ 0.3524]],\n",
      "\n",
      "        [[ 0.0267],\n",
      "         [ 0.3483],\n",
      "         [ 0.0793],\n",
      "         [ 0.2253],\n",
      "         [ 0.0895],\n",
      "         [-0.0053]],\n",
      "\n",
      "        [[ 0.3220],\n",
      "         [-0.0882],\n",
      "         [ 0.2267],\n",
      "         [-0.1266],\n",
      "         [ 0.1944],\n",
      "         [-0.0523]],\n",
      "\n",
      "        [[ 0.0995],\n",
      "         [ 0.2964],\n",
      "         [ 0.1293],\n",
      "         [ 0.3350],\n",
      "         [-0.0117],\n",
      "         [ 0.0105]],\n",
      "\n",
      "        [[ 0.0552],\n",
      "         [ 0.1344],\n",
      "         [ 0.0662],\n",
      "         [ 0.2897],\n",
      "         [-0.0258],\n",
      "         [ 0.0015]],\n",
      "\n",
      "        [[ 0.1861],\n",
      "         [-0.0493],\n",
      "         [ 0.2114],\n",
      "         [ 0.0398],\n",
      "         [ 0.3027],\n",
      "         [ 0.0948]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.1866],\n",
      "         [ 0.6365],\n",
      "         [ 0.0461],\n",
      "         [ 0.0060],\n",
      "         [ 0.0391]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0746],\n",
      "         [-0.0553],\n",
      "         [ 0.0025],\n",
      "         [ 0.0849],\n",
      "         [ 0.0173]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1089],\n",
      "         [ 0.0905],\n",
      "         [-0.0072],\n",
      "         [ 0.2312],\n",
      "         [ 0.0150]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1748],\n",
      "         [ 0.5336],\n",
      "         [ 0.4379],\n",
      "         [ 0.4635],\n",
      "         [ 0.0150]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0064],\n",
      "         [-0.0255],\n",
      "         [-0.0382],\n",
      "         [-0.0307],\n",
      "         [ 0.0900]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1732],\n",
      "         [ 0.3807],\n",
      "         [ 0.4921],\n",
      "         [ 0.1415],\n",
      "         [ 0.0781]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0387],\n",
      "         [ 0.2144],\n",
      "         [ 0.1874],\n",
      "         [ 0.0598],\n",
      "         [ 0.0524]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2123],\n",
      "         [ 0.0198],\n",
      "         [ 0.3555],\n",
      "         [ 0.4363],\n",
      "         [ 0.5711]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9990e-01, 1.0421e-04],\n",
      "         [3.5569e-04, 9.9964e-01],\n",
      "         [9.8870e-01, 1.1304e-02],\n",
      "         [6.0521e-02, 9.3948e-01],\n",
      "         [9.2532e-01, 7.4678e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9989e-01, 1.0542e-04],\n",
      "         [3.2021e-05, 9.9997e-01],\n",
      "         [9.9955e-01, 4.5450e-04],\n",
      "         [4.7877e-05, 9.9995e-01],\n",
      "         [9.9779e-01, 2.2079e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9987e-01, 1.3047e-04],\n",
      "         [9.9819e-06, 9.9999e-01],\n",
      "         [9.9978e-01, 2.1775e-04],\n",
      "         [2.0310e-04, 9.9980e-01],\n",
      "         [9.9493e-01, 5.0704e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9987e-01, 1.2523e-04],\n",
      "         [2.9009e-04, 9.9971e-01],\n",
      "         [9.9802e-01, 1.9769e-03],\n",
      "         [2.4336e-03, 9.9757e-01],\n",
      "         [9.8462e-01, 1.5379e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.3847e-05, 9.9994e-01],\n",
      "         [9.9949e-01, 5.1131e-04],\n",
      "         [2.1954e-05, 9.9998e-01],\n",
      "         [9.9889e-01, 1.1140e-03],\n",
      "         [3.8495e-04, 9.9962e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9995e-01, 5.2557e-05],\n",
      "         [8.2838e-05, 9.9992e-01],\n",
      "         [9.9765e-01, 2.3475e-03],\n",
      "         [9.5300e-03, 9.9047e-01],\n",
      "         [9.7864e-01, 2.1363e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9981e-01, 1.9267e-04],\n",
      "         [7.0943e-05, 9.9993e-01],\n",
      "         [9.9942e-01, 5.8444e-04],\n",
      "         [1.1615e-02, 9.8839e-01],\n",
      "         [9.6808e-01, 3.1915e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.2136e-06, 9.9999e-01],\n",
      "         [9.9944e-01, 5.5768e-04],\n",
      "         [1.6893e-04, 9.9983e-01],\n",
      "         [9.9803e-01, 1.9733e-03],\n",
      "         [8.4814e-04, 9.9915e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 76.0 and average score: 0.6\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2400, 0.0400, 0.0000, 0.0400, 0.4000, 0.0400, 0.1200, 0.0400, 0.0800]), tensor([0.2400, 0.0400, 0.0000, 0.0400, 0.4000, 0.0400, 0.1200, 0.0400, 0.0800]), -0.09124478882935914, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 5\n",
      "Player 1 prediction: (tensor([0.2400, 0.0400, 0.0000, 0.0400, 0.0000, 0.0000, 0.2400, 0.2000, 0.2400]), tensor([0.2400, 0.0400, 0.0000, 0.0400, 0.0000, 0.0000, 0.2400, 0.2000, 0.2400]), -0.04879743331997842, tensor(0))\n",
      "action: 0\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.4000, 0.2000, 0.2800]), tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.4000, 0.2000, 0.2800]), 0.05556676425357894, tensor(6))\n",
      "action: 6\n",
      "Player 0 random action: 8\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "1200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 4, 0, 2, 8],\n",
      "        [7, 6, 1, 8, 0],\n",
      "        [0, 3, 8, 0, 5],\n",
      "        [3, 0, 4, 3, 5],\n",
      "        [7, 5, 0, 3, 5],\n",
      "        [8, 2, 4, 6, 0],\n",
      "        [5, 0, 6, 1, 2],\n",
      "        [1, 0, 4, 3, 5]])\n",
      "target value tensor([[-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.1938],\n",
      "         [-0.0905],\n",
      "         [-0.0017],\n",
      "         [-0.1732],\n",
      "         [ 0.1080],\n",
      "         [-0.0655]],\n",
      "\n",
      "        [[ 0.1938],\n",
      "         [-0.0905],\n",
      "         [ 0.2258],\n",
      "         [-0.0356],\n",
      "         [ 0.2997],\n",
      "         [-0.1142]],\n",
      "\n",
      "        [[-0.0557],\n",
      "         [ 0.0802],\n",
      "         [ 0.0514],\n",
      "         [ 0.0882],\n",
      "         [-0.0309],\n",
      "         [ 0.0026]],\n",
      "\n",
      "        [[ 0.2364],\n",
      "         [ 0.1792],\n",
      "         [-0.0576],\n",
      "         [-0.0222],\n",
      "         [ 0.0411],\n",
      "         [ 0.0614]],\n",
      "\n",
      "        [[-0.0385],\n",
      "         [ 0.1593],\n",
      "         [ 0.1064],\n",
      "         [-0.0309],\n",
      "         [ 0.0383],\n",
      "         [ 0.0134]],\n",
      "\n",
      "        [[ 0.1938],\n",
      "         [-0.1513],\n",
      "         [ 0.1227],\n",
      "         [-0.2569],\n",
      "         [ 0.3240],\n",
      "         [-0.1932]],\n",
      "\n",
      "        [[-0.2187],\n",
      "         [ 0.1499],\n",
      "         [-0.2662],\n",
      "         [ 0.2896],\n",
      "         [-0.0927],\n",
      "         [ 0.3256]],\n",
      "\n",
      "        [[ 0.4308],\n",
      "         [ 0.0996],\n",
      "         [ 0.0052],\n",
      "         [-0.0187],\n",
      "         [ 0.0298],\n",
      "         [ 0.0246]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0370],\n",
      "         [-0.0189],\n",
      "         [-0.0462],\n",
      "         [ 0.0560],\n",
      "         [ 0.1296]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0370],\n",
      "         [-0.0195],\n",
      "         [ 0.0299],\n",
      "         [ 0.1416],\n",
      "         [ 0.1693]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0277],\n",
      "         [ 0.1012],\n",
      "         [ 0.2495],\n",
      "         [ 0.0225],\n",
      "         [-0.0127]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1680],\n",
      "         [ 0.0015],\n",
      "         [ 0.0666],\n",
      "         [ 0.1228],\n",
      "         [ 0.1498]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3052],\n",
      "         [ 0.4885],\n",
      "         [ 0.0255],\n",
      "         [ 0.0158],\n",
      "         [ 0.0138]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0120],\n",
      "         [-0.0391],\n",
      "         [ 0.0968],\n",
      "         [ 0.0723],\n",
      "         [ 0.1878]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0085],\n",
      "         [ 0.0013],\n",
      "         [ 0.0918],\n",
      "         [ 0.1509],\n",
      "         [ 0.2604]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4541],\n",
      "         [ 0.1166],\n",
      "         [ 0.0835],\n",
      "         [ 0.0822],\n",
      "         [ 0.0682]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [2.8713e-06, 1.0000e+00],\n",
      "         [9.9997e-01, 2.8348e-05],\n",
      "         [7.4737e-05, 9.9993e-01],\n",
      "         [9.9991e-01, 8.9978e-05],\n",
      "         [2.8449e-05, 9.9997e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.8713e-06, 1.0000e+00],\n",
      "         [9.9982e-01, 1.7744e-04],\n",
      "         [1.2254e-04, 9.9988e-01],\n",
      "         [9.9962e-01, 3.7847e-04],\n",
      "         [2.5309e-04, 9.9975e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 1.7632e-05],\n",
      "         [1.0895e-04, 9.9989e-01],\n",
      "         [9.9984e-01, 1.5535e-04],\n",
      "         [2.3141e-02, 9.7686e-01],\n",
      "         [9.7541e-01, 2.4594e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.9460e-05, 9.9995e-01],\n",
      "         [9.9981e-01, 1.8724e-04],\n",
      "         [7.2681e-04, 9.9927e-01],\n",
      "         [9.9577e-01, 4.2329e-03],\n",
      "         [9.4148e-03, 9.9059e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9994e-01, 5.6723e-05],\n",
      "         [1.8227e-05, 9.9998e-01],\n",
      "         [9.9737e-01, 2.6288e-03],\n",
      "         [2.7019e-02, 9.7298e-01],\n",
      "         [9.9280e-01, 7.1965e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.8793e-05, 9.9995e-01],\n",
      "         [9.9992e-01, 8.0113e-05],\n",
      "         [5.0927e-04, 9.9949e-01],\n",
      "         [9.9997e-01, 2.9878e-05],\n",
      "         [1.1496e-03, 9.9885e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 2.1362e-05],\n",
      "         [7.5418e-05, 9.9992e-01],\n",
      "         [9.9987e-01, 1.2952e-04],\n",
      "         [2.8652e-03, 9.9713e-01],\n",
      "         [9.9970e-01, 2.9829e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.3024e-05, 9.9994e-01],\n",
      "         [9.9937e-01, 6.3418e-04],\n",
      "         [2.8790e-03, 9.9712e-01],\n",
      "         [9.9273e-01, 7.2668e-03],\n",
      "         [1.0099e-01, 8.9901e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 54.0 and average score: 0.22\n",
      "Results vs random: {'player_0_score': 0.6, 'player_0_win%': 0.76, 'player_1_score': 0.22, 'player_1_win%': 0.54, 'score': 0.41}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.5200, 0.0400, 0.0800, 0.0400, 0.0800]), tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.5200, 0.0400, 0.0800, 0.0400, 0.0800]), 0.15093647931531928, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 6\n",
      "Player 0 prediction: (tensor([0.2400, 0.1200, 0.2400, 0.0400, 0.0000, 0.0400, 0.0000, 0.1200, 0.2000]), tensor([0.2400, 0.1200, 0.2400, 0.0400, 0.0000, 0.0400, 0.0000, 0.1200, 0.2000]), 0.29216058067493256, tensor(0))\n",
      "action: 0\n",
      "Player 1 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.1600, 0.5600, 0.0800, 0.0000, 0.0800, 0.0000, 0.1200, 0.0000]), tensor([0.0000, 0.1600, 0.5600, 0.0800, 0.0000, 0.0800, 0.0000, 0.1200, 0.0000]), 0.46034999422028977, tensor(2))\n",
      "action: 2\n",
      "Player 1 tictactoe_expert action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 6, 7, 8, 4],\n",
      "        [8, 1, 0, 0, 2],\n",
      "        [0, 1, 6, 2, 3],\n",
      "        [6, 2, 5, 0, 3],\n",
      "        [6, 7, 1, 0, 2],\n",
      "        [7, 0, 3, 4, 2],\n",
      "        [4, 6, 0, 2, 7],\n",
      "        [0, 8, 1, 0, 2]])\n",
      "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.2130],\n",
      "         [ 0.3725],\n",
      "         [-0.3050],\n",
      "         [ 0.4631],\n",
      "         [-0.2618],\n",
      "         [ 0.5005]],\n",
      "\n",
      "        [[-0.0397],\n",
      "         [ 0.3019],\n",
      "         [ 0.1398],\n",
      "         [ 0.0324],\n",
      "         [-0.0163],\n",
      "         [-0.0470]],\n",
      "\n",
      "        [[ 0.2695],\n",
      "         [-0.2764],\n",
      "         [ 0.3489],\n",
      "         [-0.3195],\n",
      "         [ 0.4245],\n",
      "         [-0.1366]],\n",
      "\n",
      "        [[ 0.0901],\n",
      "         [-0.0387],\n",
      "         [ 0.0704],\n",
      "         [ 0.1923],\n",
      "         [-0.2289],\n",
      "         [ 0.3102]],\n",
      "\n",
      "        [[ 0.0013],\n",
      "         [ 0.1024],\n",
      "         [ 0.2066],\n",
      "         [ 0.1489],\n",
      "         [-0.0398],\n",
      "         [-0.0295]],\n",
      "\n",
      "        [[-0.0195],\n",
      "         [-0.0845],\n",
      "         [-0.0617],\n",
      "         [ 0.0135],\n",
      "         [-0.0263],\n",
      "         [-0.0378]],\n",
      "\n",
      "        [[ 0.0901],\n",
      "         [-0.2931],\n",
      "         [ 0.3966],\n",
      "         [-0.3148],\n",
      "         [ 0.2593],\n",
      "         [-0.0824]],\n",
      "\n",
      "        [[-0.0430],\n",
      "         [-0.0302],\n",
      "         [ 0.1667],\n",
      "         [ 0.1782],\n",
      "         [-0.0161],\n",
      "         [-0.0459]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [-0.0058],\n",
      "         [ 0.2438],\n",
      "         [ 0.0738],\n",
      "         [ 0.3718],\n",
      "         [ 0.2386]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2075],\n",
      "         [ 0.4711],\n",
      "         [ 0.3581],\n",
      "         [ 0.0640],\n",
      "         [ 0.0818]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1305],\n",
      "         [ 0.0433],\n",
      "         [ 0.2770],\n",
      "         [ 0.1567],\n",
      "         [ 0.3392]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0983],\n",
      "         [ 0.1003],\n",
      "         [ 0.0569],\n",
      "         [ 0.0748],\n",
      "         [ 0.0109]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.5116],\n",
      "         [ 0.4063],\n",
      "         [ 0.5623],\n",
      "         [ 0.0771],\n",
      "         [ 0.0891]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4800],\n",
      "         [ 0.0667],\n",
      "         [ 0.0689],\n",
      "         [ 0.0783],\n",
      "         [ 0.1251]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0674],\n",
      "         [-0.0028],\n",
      "         [ 0.1131],\n",
      "         [ 0.0774],\n",
      "         [ 0.1540]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1759],\n",
      "         [ 0.4328],\n",
      "         [ 0.6237],\n",
      "         [ 0.1072],\n",
      "         [ 0.0953]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9993e-01, 7.0555e-05],\n",
      "         [7.9084e-05, 9.9992e-01],\n",
      "         [9.9947e-01, 5.2971e-04],\n",
      "         [3.2496e-05, 9.9997e-01],\n",
      "         [9.9977e-01, 2.2668e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 2.4844e-05],\n",
      "         [8.6006e-06, 9.9999e-01],\n",
      "         [9.9833e-01, 1.6691e-03],\n",
      "         [2.0574e-02, 9.7943e-01],\n",
      "         [7.7691e-01, 2.2309e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.1443e-05, 9.9999e-01],\n",
      "         [9.9950e-01, 4.9891e-04],\n",
      "         [1.4270e-04, 9.9986e-01],\n",
      "         [9.9996e-01, 4.4463e-05],\n",
      "         [1.6291e-04, 9.9984e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9518e-05, 9.9990e-01],\n",
      "         [9.9909e-01, 9.0740e-04],\n",
      "         [7.8687e-05, 9.9992e-01],\n",
      "         [9.4749e-01, 5.2514e-02],\n",
      "         [5.0886e-01, 4.9114e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.3504e-05, 9.9998e-01],\n",
      "         [9.9988e-01, 1.2276e-04],\n",
      "         [1.1243e-04, 9.9989e-01],\n",
      "         [8.6044e-01, 1.3956e-01],\n",
      "         [7.5757e-02, 9.2424e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.5207e-06, 1.0000e+00],\n",
      "         [9.9932e-01, 6.8291e-04],\n",
      "         [5.5681e-02, 9.4432e-01],\n",
      "         [9.7009e-01, 2.9908e-02],\n",
      "         [1.1578e-02, 9.8842e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.8768e-05, 9.9995e-01],\n",
      "         [9.9998e-01, 2.3463e-05],\n",
      "         [1.0695e-03, 9.9893e-01],\n",
      "         [9.9992e-01, 7.7068e-05],\n",
      "         [9.0322e-05, 9.9991e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9996e-01, 4.0077e-05],\n",
      "         [3.0141e-05, 9.9997e-01],\n",
      "         [9.9437e-01, 5.6341e-03],\n",
      "         [7.1068e-03, 9.9289e-01],\n",
      "         [8.1873e-01, 1.8127e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "Player 0 win percentage vs tictactoe_expert: 6.0 and average score: -0.86\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 6\n",
      "Player 1 prediction: (tensor([0.2400, 0.0400, 0.1600, 0.0400, 0.4000, 0.0000, 0.0000, 0.0400, 0.0800]), tensor([0.2400, 0.0400, 0.1600, 0.0400, 0.4000, 0.0000, 0.0000, 0.0400, 0.0800]), -0.1927575042884346, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 1\n",
      "Player 1 prediction: (tensor([0.2800, 0.0000, 0.3600, 0.1200, 0.0000, 0.0400, 0.0000, 0.0400, 0.1600]), tensor([0.2800, 0.0000, 0.3600, 0.1200, 0.0000, 0.0400, 0.0000, 0.0400, 0.1600]), 0.03581042099384162, tensor(2))\n",
      "action: 2\n",
      "Player 0 tictactoe_expert action: 3\n",
      "Player 1 prediction: (tensor([0.4800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0800, 0.0000, 0.0800, 0.3600]), tensor([0.4800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0800, 0.0000, 0.0800, 0.3600]), -0.12486127718424601, tensor(0))\n",
      "action: 0\n",
      "Player 0 tictactoe_expert action: 8\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5600, 0.0000, 0.4400, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5600, 0.0000, 0.4400, 0.0000]), -0.03144648677835833, tensor(5))\n",
      "action: 5\n",
      "Player 0 tictactoe_expert action: 7\n",
      "1400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 0, 7, 5, 6],\n",
      "        [4, 6, 0, 2, 3],\n",
      "        [7, 2, 1, 8, 0],\n",
      "        [0, 6, 0, 5, 6],\n",
      "        [3, 0, 7, 5, 6],\n",
      "        [5, 0, 7, 5, 6],\n",
      "        [7, 0, 8, 2, 1],\n",
      "        [8, 0, 7, 5, 6]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.2390],\n",
      "         [ 0.0693],\n",
      "         [ 0.0326],\n",
      "         [-0.0480],\n",
      "         [ 0.0360],\n",
      "         [-0.0250]],\n",
      "\n",
      "        [[-0.1144],\n",
      "         [ 0.0766],\n",
      "         [-0.0541],\n",
      "         [ 0.1771],\n",
      "         [-0.0128],\n",
      "         [ 0.1949]],\n",
      "\n",
      "        [[ 0.0539],\n",
      "         [-0.0575],\n",
      "         [ 0.0546],\n",
      "         [ 0.2379],\n",
      "         [ 0.2502],\n",
      "         [ 0.0060]],\n",
      "\n",
      "        [[-0.3176],\n",
      "         [ 0.3634],\n",
      "         [ 0.1109],\n",
      "         [ 0.0279],\n",
      "         [-0.0086],\n",
      "         [ 0.0467]],\n",
      "\n",
      "        [[ 0.9246],\n",
      "         [ 0.0512],\n",
      "         [ 0.0075],\n",
      "         [-0.0380],\n",
      "         [ 0.0301],\n",
      "         [-0.0224]],\n",
      "\n",
      "        [[ 0.4763],\n",
      "         [ 0.0762],\n",
      "         [ 0.0052],\n",
      "         [-0.0344],\n",
      "         [ 0.0247],\n",
      "         [-0.0203]],\n",
      "\n",
      "        [[ 0.4112],\n",
      "         [-0.3807],\n",
      "         [ 0.2552],\n",
      "         [-0.0365],\n",
      "         [ 0.2711],\n",
      "         [ 0.1259]],\n",
      "\n",
      "        [[ 0.7031],\n",
      "         [ 0.2169],\n",
      "         [ 0.0177],\n",
      "         [ 0.0418],\n",
      "         [-0.0080],\n",
      "         [ 0.0469]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.3496],\n",
      "         [-0.0295],\n",
      "         [-0.0025],\n",
      "         [-0.0245],\n",
      "         [-0.0501]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0242],\n",
      "         [-0.0534],\n",
      "         [-0.0511],\n",
      "         [ 0.1824],\n",
      "         [ 0.0630]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1363],\n",
      "         [-0.1261],\n",
      "         [ 0.0275],\n",
      "         [ 0.1863],\n",
      "         [ 0.2054]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1237],\n",
      "         [ 0.3543],\n",
      "         [-0.0193],\n",
      "         [-0.0459],\n",
      "         [-0.0652]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4974],\n",
      "         [-0.0270],\n",
      "         [-0.0608],\n",
      "         [-0.0559],\n",
      "         [-0.0626]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4288],\n",
      "         [-0.0421],\n",
      "         [-0.0452],\n",
      "         [-0.0511],\n",
      "         [-0.0615]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.1125],\n",
      "         [-0.1253],\n",
      "         [ 0.0523],\n",
      "         [ 0.0694],\n",
      "         [ 0.3527]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4947],\n",
      "         [ 0.0227],\n",
      "         [-0.0189],\n",
      "         [-0.0295],\n",
      "         [-0.0542]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.8208e-06, 1.0000e+00],\n",
      "         [9.9922e-01, 7.7963e-04],\n",
      "         [1.4324e-04, 9.9986e-01],\n",
      "         [9.9189e-01, 8.1116e-03],\n",
      "         [3.5086e-04, 9.9965e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9903e-01, 9.7461e-04],\n",
      "         [9.1993e-07, 1.0000e+00],\n",
      "         [9.9992e-01, 7.5624e-05],\n",
      "         [2.2860e-06, 1.0000e+00],\n",
      "         [9.9964e-01, 3.6475e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.8173e-06, 1.0000e+00],\n",
      "         [9.9992e-01, 7.7594e-05],\n",
      "         [2.5452e-06, 1.0000e+00],\n",
      "         [9.9997e-01, 3.0707e-05],\n",
      "         [3.4335e-06, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9983e-01, 1.6556e-04],\n",
      "         [9.8274e-07, 1.0000e+00],\n",
      "         [9.9870e-01, 1.3025e-03],\n",
      "         [3.9906e-04, 9.9960e-01],\n",
      "         [9.9150e-01, 8.4971e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [5.1231e-06, 9.9999e-01],\n",
      "         [9.9863e-01, 1.3743e-03],\n",
      "         [2.6401e-04, 9.9974e-01],\n",
      "         [9.8110e-01, 1.8900e-02],\n",
      "         [5.5734e-04, 9.9944e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [5.8392e-06, 9.9999e-01],\n",
      "         [9.9579e-01, 4.2095e-03],\n",
      "         [4.0315e-04, 9.9960e-01],\n",
      "         [9.7553e-01, 2.4467e-02],\n",
      "         [6.1497e-04, 9.9938e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.5199e-06, 9.9999e-01],\n",
      "         [9.9986e-01, 1.4472e-04],\n",
      "         [6.7838e-07, 1.0000e+00],\n",
      "         [9.9991e-01, 8.5327e-05],\n",
      "         [1.4395e-06, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9970e-01, 3.0338e-04],\n",
      "         [4.9312e-05, 9.9995e-01],\n",
      "         [9.9853e-01, 1.4722e-03],\n",
      "         [2.6235e-04, 9.9974e-01],\n",
      "         [9.8827e-01, 1.1734e-02]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "Player 1 win percentage vs tictactoe_expert: 2.0 and average score: -0.82\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.86, 'player_0_win%': 0.06, 'player_1_score': -0.82, 'player_1_win%': 0.02, 'score': -0.84}\n",
      "1500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 0, 4, 6, 8],\n",
      "        [8, 0, 2, 6, 3],\n",
      "        [3, 5, 0, 7, 0],\n",
      "        [4, 8, 0, 5, 0],\n",
      "        [4, 2, 0, 5, 1],\n",
      "        [2, 5, 7, 1, 0],\n",
      "        [7, 1, 2, 8, 3],\n",
      "        [1, 0, 4, 7, 0]])\n",
      "target value tensor([[ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.3239],\n",
      "         [-0.1845],\n",
      "         [ 0.0841],\n",
      "         [-0.2023],\n",
      "         [ 0.3045],\n",
      "         [-0.0012]],\n",
      "\n",
      "        [[-0.2733],\n",
      "         [ 0.2655],\n",
      "         [-0.3518],\n",
      "         [ 0.1434],\n",
      "         [-0.0251],\n",
      "         [ 0.1512]],\n",
      "\n",
      "        [[ 0.6217],\n",
      "         [-0.0696],\n",
      "         [ 0.0871],\n",
      "         [-0.0190],\n",
      "         [-0.0247],\n",
      "         [-0.0186]],\n",
      "\n",
      "        [[ 0.1727],\n",
      "         [ 0.1550],\n",
      "         [ 0.2793],\n",
      "         [ 0.1502],\n",
      "         [ 0.0221],\n",
      "         [ 0.0162]],\n",
      "\n",
      "        [[ 0.0602],\n",
      "         [-0.2601],\n",
      "         [ 0.1409],\n",
      "         [-0.3100],\n",
      "         [ 0.0618],\n",
      "         [-0.0375]],\n",
      "\n",
      "        [[ 0.2426],\n",
      "         [-0.0033],\n",
      "         [ 0.1026],\n",
      "         [ 0.2016],\n",
      "         [ 0.1468],\n",
      "         [-0.0102]],\n",
      "\n",
      "        [[-0.0329],\n",
      "         [ 0.1729],\n",
      "         [-0.0515],\n",
      "         [-0.0082],\n",
      "         [ 0.1419],\n",
      "         [ 0.2588]],\n",
      "\n",
      "        [[ 0.5772],\n",
      "         [-0.0008],\n",
      "         [ 0.0290],\n",
      "         [ 0.0159],\n",
      "         [-0.0100],\n",
      "         [-0.0083]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [-0.0678],\n",
      "         [-0.0080],\n",
      "         [-0.0251],\n",
      "         [ 0.0598],\n",
      "         [ 0.3535]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0715],\n",
      "         [-0.0395],\n",
      "         [-0.0446],\n",
      "         [ 0.1084],\n",
      "         [-0.0127]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3914],\n",
      "         [ 0.2815],\n",
      "         [-0.0029],\n",
      "         [-0.0319],\n",
      "         [-0.0656]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0655],\n",
      "         [ 0.0892],\n",
      "         [ 0.1746],\n",
      "         [ 0.2702],\n",
      "         [-0.0156]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0085],\n",
      "         [-0.0378],\n",
      "         [-0.0433],\n",
      "         [-0.0023],\n",
      "         [ 0.0491]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2288],\n",
      "         [ 0.3023],\n",
      "         [ 0.1940],\n",
      "         [ 0.3678],\n",
      "         [ 0.0232]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0123],\n",
      "         [ 0.0013],\n",
      "         [ 0.0421],\n",
      "         [ 0.2450],\n",
      "         [ 0.2386]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.5547],\n",
      "         [ 0.0403],\n",
      "         [-0.0136],\n",
      "         [-0.0137],\n",
      "         [-0.0716]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [4.1865e-06, 1.0000e+00],\n",
      "         [9.9999e-01, 6.8739e-06],\n",
      "         [9.5927e-07, 1.0000e+00],\n",
      "         [9.9995e-01, 4.6506e-05],\n",
      "         [1.2107e-06, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.4778e-05],\n",
      "         [4.7792e-06, 1.0000e+00],\n",
      "         [9.9994e-01, 6.0607e-05],\n",
      "         [6.0955e-06, 9.9999e-01],\n",
      "         [9.9974e-01, 2.6495e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9610e-01, 3.8954e-03],\n",
      "         [2.3073e-05, 9.9998e-01],\n",
      "         [9.9932e-01, 6.8328e-04],\n",
      "         [8.1337e-03, 9.9187e-01],\n",
      "         [6.3004e-01, 3.6996e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9996e-01, 3.5364e-05],\n",
      "         [1.0701e-05, 9.9999e-01],\n",
      "         [9.9997e-01, 2.6654e-05],\n",
      "         [5.7105e-06, 9.9999e-01],\n",
      "         [9.9980e-01, 1.9907e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.4913e-05, 9.9991e-01],\n",
      "         [9.9998e-01, 1.9807e-05],\n",
      "         [2.4243e-05, 9.9998e-01],\n",
      "         [9.9995e-01, 4.9780e-05],\n",
      "         [2.9100e-05, 9.9997e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 2.6999e-05],\n",
      "         [6.1934e-06, 9.9999e-01],\n",
      "         [9.9998e-01, 2.1235e-05],\n",
      "         [1.9426e-05, 9.9998e-01],\n",
      "         [9.9916e-01, 8.3529e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 3.2682e-05],\n",
      "         [4.1417e-05, 9.9996e-01],\n",
      "         [9.9997e-01, 2.6731e-05],\n",
      "         [1.3504e-05, 9.9999e-01],\n",
      "         [9.9961e-01, 3.9430e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.6668e-05, 9.9998e-01],\n",
      "         [9.9950e-01, 5.0415e-04],\n",
      "         [2.7219e-03, 9.9728e-01],\n",
      "         [8.4727e-01, 1.5273e-01],\n",
      "         [1.1679e-01, 8.8321e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 2, 0, 4, 0],\n",
      "        [4, 7, 6, 2, 8],\n",
      "        [7, 0, 4, 4, 0],\n",
      "        [0, 3, 5, 6, 1],\n",
      "        [4, 0, 5, 6, 2],\n",
      "        [0, 1, 3, 0, 0],\n",
      "        [4, 8, 5, 7, 1],\n",
      "        [6, 2, 0, 8, 4]])\n",
      "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9227, -0.9321,  0.9415, -0.9510,  0.9606, -0.9703],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
      "predicted values tensor([[[-0.0601],\n",
      "         [ 0.3145],\n",
      "         [ 0.0719],\n",
      "         [ 0.0056],\n",
      "         [ 0.0143],\n",
      "         [ 0.0055]],\n",
      "\n",
      "        [[ 0.1776],\n",
      "         [-0.3380],\n",
      "         [ 0.4424],\n",
      "         [-0.4539],\n",
      "         [ 0.4265],\n",
      "         [-0.2761]],\n",
      "\n",
      "        [[ 0.3976],\n",
      "         [ 0.2683],\n",
      "         [ 0.0647],\n",
      "         [ 0.1234],\n",
      "         [ 0.0146],\n",
      "         [-0.0011]],\n",
      "\n",
      "        [[ 0.3417],\n",
      "         [-0.4325],\n",
      "         [ 0.5236],\n",
      "         [-0.3009],\n",
      "         [ 0.3803],\n",
      "         [ 0.0740]],\n",
      "\n",
      "        [[ 0.1776],\n",
      "         [-0.3380],\n",
      "         [ 0.3271],\n",
      "         [-0.1870],\n",
      "         [ 0.1220],\n",
      "         [-0.1067]],\n",
      "\n",
      "        [[-0.1276],\n",
      "         [ 0.2524],\n",
      "         [ 0.2347],\n",
      "         [ 0.1034],\n",
      "         [ 0.0241],\n",
      "         [ 0.0073]],\n",
      "\n",
      "        [[ 0.1776],\n",
      "         [-0.3380],\n",
      "         [ 0.5571],\n",
      "         [-0.3661],\n",
      "         [ 0.4922],\n",
      "         [-0.1436]],\n",
      "\n",
      "        [[ 0.1208],\n",
      "         [-0.0888],\n",
      "         [ 0.1824],\n",
      "         [-0.1333],\n",
      "         [ 0.3357],\n",
      "         [-0.0334]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.4061],\n",
      "         [ 0.6440],\n",
      "         [ 0.0039],\n",
      "         [-0.0095],\n",
      "         [-0.0436]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0274],\n",
      "         [-0.0178],\n",
      "         [-0.0008],\n",
      "         [ 0.0274],\n",
      "         [ 0.2089]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4056],\n",
      "         [ 0.4069],\n",
      "         [ 0.1369],\n",
      "         [ 0.0391],\n",
      "         [-0.0253]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0011],\n",
      "         [ 0.0699],\n",
      "         [ 0.2056],\n",
      "         [ 0.1230],\n",
      "         [ 0.4696]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0274],\n",
      "         [-0.0382],\n",
      "         [-0.1107],\n",
      "         [-0.0263],\n",
      "         [ 0.0305]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0918],\n",
      "         [ 0.3818],\n",
      "         [ 0.4579],\n",
      "         [ 0.0293],\n",
      "         [-0.0385]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0274],\n",
      "         [-0.0471],\n",
      "         [ 0.0235],\n",
      "         [ 0.0207],\n",
      "         [ 0.1483]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0453],\n",
      "         [ 0.0074],\n",
      "         [-0.0080],\n",
      "         [ 0.0191],\n",
      "         [ 0.3745]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9994e-01, 6.1540e-05],\n",
      "         [3.6066e-05, 9.9996e-01],\n",
      "         [9.8181e-01, 1.8191e-02],\n",
      "         [1.7240e-01, 8.2760e-01],\n",
      "         [2.4172e-01, 7.5828e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.2023e-04, 9.9988e-01],\n",
      "         [9.9998e-01, 2.1832e-05],\n",
      "         [2.9991e-05, 9.9997e-01],\n",
      "         [9.9999e-01, 6.5165e-06],\n",
      "         [1.4071e-05, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 2.9050e-05],\n",
      "         [6.8835e-05, 9.9993e-01],\n",
      "         [9.9862e-01, 1.3759e-03],\n",
      "         [2.7635e-03, 9.9724e-01],\n",
      "         [6.2026e-01, 3.7974e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.3701e-05, 9.9999e-01],\n",
      "         [9.9998e-01, 2.2541e-05],\n",
      "         [4.5641e-05, 9.9995e-01],\n",
      "         [9.9999e-01, 1.1696e-05],\n",
      "         [3.7783e-04, 9.9962e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.2023e-04, 9.9988e-01],\n",
      "         [9.9999e-01, 1.2439e-05],\n",
      "         [1.1104e-05, 9.9999e-01],\n",
      "         [9.9998e-01, 1.9499e-05],\n",
      "         [1.3562e-04, 9.9986e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.3338e-05],\n",
      "         [1.5617e-06, 1.0000e+00],\n",
      "         [9.9989e-01, 1.0853e-04],\n",
      "         [7.3568e-02, 9.2643e-01],\n",
      "         [5.8719e-01, 4.1281e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.2023e-04, 9.9988e-01],\n",
      "         [9.9997e-01, 3.2034e-05],\n",
      "         [4.7529e-06, 1.0000e+00],\n",
      "         [9.9990e-01, 9.9292e-05],\n",
      "         [5.2489e-06, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.1692e-05, 9.9994e-01],\n",
      "         [9.9999e-01, 5.6989e-06],\n",
      "         [2.2432e-05, 9.9998e-01],\n",
      "         [9.9998e-01, 1.6811e-05],\n",
      "         [9.9626e-06, 9.9999e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 1, 0, 6, 5],\n",
      "        [6, 0, 1, 4, 3],\n",
      "        [7, 0, 5, 6, 5],\n",
      "        [6, 1, 0, 6, 5],\n",
      "        [7, 0, 5, 6, 5],\n",
      "        [6, 7, 0, 1, 3],\n",
      "        [1, 6, 3, 5, 0],\n",
      "        [1, 8, 0, 6, 5]])\n",
      "target value tensor([[-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.0534],\n",
      "         [ 0.0082],\n",
      "         [ 0.4431],\n",
      "         [ 0.0316],\n",
      "         [ 0.0580],\n",
      "         [ 0.0587]],\n",
      "\n",
      "        [[ 0.3701],\n",
      "         [-0.1493],\n",
      "         [ 0.1337],\n",
      "         [ 0.0729],\n",
      "         [-0.1672],\n",
      "         [ 0.4025]],\n",
      "\n",
      "        [[ 0.7109],\n",
      "         [ 0.2024],\n",
      "         [ 0.0857],\n",
      "         [ 0.0541],\n",
      "         [ 0.0336],\n",
      "         [ 0.0666]],\n",
      "\n",
      "        [[ 0.0531],\n",
      "         [ 0.2114],\n",
      "         [ 0.1485],\n",
      "         [ 0.0218],\n",
      "         [ 0.0177],\n",
      "         [ 0.0569]],\n",
      "\n",
      "        [[ 0.8618],\n",
      "         [ 0.0018],\n",
      "         [ 0.0511],\n",
      "         [ 0.0534],\n",
      "         [ 0.0346],\n",
      "         [ 0.0699]],\n",
      "\n",
      "        [[ 0.0758],\n",
      "         [ 0.2424],\n",
      "         [-0.2271],\n",
      "         [ 0.2602],\n",
      "         [ 0.1674],\n",
      "         [ 0.1526]],\n",
      "\n",
      "        [[ 0.5108],\n",
      "         [-0.0553],\n",
      "         [ 0.3304],\n",
      "         [-0.0195],\n",
      "         [ 0.3523],\n",
      "         [ 0.0351]],\n",
      "\n",
      "        [[ 0.4738],\n",
      "         [ 0.1795],\n",
      "         [-0.0051],\n",
      "         [ 0.3606],\n",
      "         [-0.0068],\n",
      "         [ 0.2094]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.4007],\n",
      "         [ 0.4063],\n",
      "         [ 0.0214],\n",
      "         [-0.0135],\n",
      "         [-0.0293]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0277],\n",
      "         [-0.0203],\n",
      "         [-0.1797],\n",
      "         [ 0.0657],\n",
      "         [-0.1031]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3453],\n",
      "         [-0.0096],\n",
      "         [-0.0336],\n",
      "         [-0.0448],\n",
      "         [-0.0440]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1831],\n",
      "         [ 0.3235],\n",
      "         [ 0.0008],\n",
      "         [-0.0254],\n",
      "         [-0.0279]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.5734],\n",
      "         [ 0.0194],\n",
      "         [-0.0390],\n",
      "         [-0.0414],\n",
      "         [-0.0425]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0644],\n",
      "         [-0.0128],\n",
      "         [-0.0968],\n",
      "         [ 0.2148],\n",
      "         [ 0.2766]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2501],\n",
      "         [ 0.1305],\n",
      "         [ 0.1354],\n",
      "         [ 0.3709],\n",
      "         [ 0.0483]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0009],\n",
      "         [ 0.0124],\n",
      "         [ 0.0617],\n",
      "         [ 0.2784],\n",
      "         [ 0.2126]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.1561e-06, 1.0000e+00],\n",
      "         [9.9965e-01, 3.5252e-04],\n",
      "         [2.4992e-03, 9.9750e-01],\n",
      "         [8.3260e-01, 1.6740e-01],\n",
      "         [7.9105e-02, 9.2089e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.3162e-05, 9.9998e-01],\n",
      "         [9.9999e-01, 5.7827e-06],\n",
      "         [4.3755e-05, 9.9996e-01],\n",
      "         [9.9990e-01, 1.0123e-04],\n",
      "         [1.3659e-04, 9.9986e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9871e-01, 1.2933e-03],\n",
      "         [2.4559e-03, 9.9754e-01],\n",
      "         [3.7780e-01, 6.2220e-01],\n",
      "         [5.3786e-01, 4.6214e-01],\n",
      "         [2.2291e-01, 7.7709e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9976e-01, 2.4334e-04],\n",
      "         [1.7384e-06, 1.0000e+00],\n",
      "         [9.9907e-01, 9.2707e-04],\n",
      "         [8.0873e-03, 9.9191e-01],\n",
      "         [8.4046e-01, 1.5954e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [5.8475e-06, 9.9999e-01],\n",
      "         [9.9534e-01, 4.6574e-03],\n",
      "         [3.7324e-02, 9.6268e-01],\n",
      "         [8.1487e-01, 1.8513e-01],\n",
      "         [1.3058e-01, 8.6942e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9990e-01, 9.6483e-05],\n",
      "         [3.4553e-06, 1.0000e+00],\n",
      "         [9.9998e-01, 2.3280e-05],\n",
      "         [9.0786e-07, 1.0000e+00],\n",
      "         [9.9991e-01, 9.1100e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.3281e-07, 1.0000e+00],\n",
      "         [9.9996e-01, 3.8977e-05],\n",
      "         [3.9070e-06, 1.0000e+00],\n",
      "         [9.9996e-01, 4.3390e-05],\n",
      "         [1.9032e-03, 9.9810e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9266e-01, 7.3377e-03],\n",
      "         [9.2254e-06, 9.9999e-01],\n",
      "         [9.9990e-01, 1.0117e-04],\n",
      "         [2.1129e-06, 1.0000e+00],\n",
      "         [9.9995e-01, 5.1849e-05]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 5, 2, 0, 8],\n",
      "        [8, 7, 3, 0, 0],\n",
      "        [4, 0, 8, 1, 0],\n",
      "        [5, 8, 7, 3, 1],\n",
      "        [3, 4, 7, 2, 0],\n",
      "        [3, 0, 1, 6, 5],\n",
      "        [2, 7, 6, 8, 3],\n",
      "        [7, 6, 0, 1, 0]])\n",
      "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.0636],\n",
      "         [ 0.0341],\n",
      "         [-0.1644],\n",
      "         [-0.0374],\n",
      "         [-0.2756],\n",
      "         [ 0.1528]],\n",
      "\n",
      "        [[-0.2604],\n",
      "         [ 0.4031],\n",
      "         [-0.2876],\n",
      "         [ 0.6017],\n",
      "         [-0.2999],\n",
      "         [-0.0106]],\n",
      "\n",
      "        [[ 0.3283],\n",
      "         [ 0.2568],\n",
      "         [-0.0438],\n",
      "         [ 0.0491],\n",
      "         [ 0.0046],\n",
      "         [-0.0496]],\n",
      "\n",
      "        [[ 0.2682],\n",
      "         [-0.3302],\n",
      "         [ 0.1500],\n",
      "         [-0.2323],\n",
      "         [ 0.2812],\n",
      "         [ 0.0169]],\n",
      "\n",
      "        [[ 0.1487],\n",
      "         [-0.0751],\n",
      "         [-0.2979],\n",
      "         [ 0.2564],\n",
      "         [-0.3962],\n",
      "         [ 0.0665]],\n",
      "\n",
      "        [[ 0.1128],\n",
      "         [-0.0089],\n",
      "         [-0.2477],\n",
      "         [ 0.5195],\n",
      "         [-0.4192],\n",
      "         [ 0.5817]],\n",
      "\n",
      "        [[ 0.1858],\n",
      "         [-0.3349],\n",
      "         [ 0.1388],\n",
      "         [-0.3388],\n",
      "         [ 0.2984],\n",
      "         [-0.1781]],\n",
      "\n",
      "        [[ 0.2640],\n",
      "         [ 0.1228],\n",
      "         [ 0.0190],\n",
      "         [-0.0294],\n",
      "         [ 0.0143],\n",
      "         [-0.0474]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0477],\n",
      "         [-0.0066],\n",
      "         [ 0.0417],\n",
      "         [-0.0142],\n",
      "         [ 0.0654]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0142],\n",
      "         [ 0.1228],\n",
      "         [ 0.1717],\n",
      "         [ 0.5314],\n",
      "         [-0.0064]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2373],\n",
      "         [-0.0745],\n",
      "         [ 0.0561],\n",
      "         [-0.0628],\n",
      "         [-0.0680]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0993],\n",
      "         [ 0.0821],\n",
      "         [ 0.2018],\n",
      "         [ 0.3957],\n",
      "         [ 0.3803]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0746],\n",
      "         [-0.0259],\n",
      "         [ 0.0098],\n",
      "         [ 0.0505],\n",
      "         [ 0.2010]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0176],\n",
      "         [ 0.0176],\n",
      "         [-0.0996],\n",
      "         [ 0.3300],\n",
      "         [ 0.0186]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0413],\n",
      "         [-0.0326],\n",
      "         [-0.0127],\n",
      "         [ 0.0965],\n",
      "         [ 0.3619]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1101],\n",
      "         [ 0.3382],\n",
      "         [-0.0094],\n",
      "         [-0.0489],\n",
      "         [-0.0767]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9945e-01, 5.5502e-04],\n",
      "         [4.9166e-06, 1.0000e+00],\n",
      "         [9.9999e-01, 9.8576e-06],\n",
      "         [7.8965e-07, 1.0000e+00],\n",
      "         [9.9999e-01, 1.4781e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.0437e-05],\n",
      "         [6.1415e-06, 9.9999e-01],\n",
      "         [9.9915e-01, 8.5020e-04],\n",
      "         [1.4307e-04, 9.9986e-01],\n",
      "         [9.8974e-01, 1.0265e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9994e-01, 6.2658e-05],\n",
      "         [3.0012e-05, 9.9997e-01],\n",
      "         [9.9902e-01, 9.7540e-04],\n",
      "         [8.1061e-05, 9.9992e-01],\n",
      "         [9.3589e-01, 6.4114e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.3826e-06, 1.0000e+00],\n",
      "         [9.9999e-01, 1.4065e-05],\n",
      "         [1.3795e-05, 9.9999e-01],\n",
      "         [9.9963e-01, 3.7318e-04],\n",
      "         [4.5676e-05, 9.9995e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.4973e-06, 1.0000e+00],\n",
      "         [9.9998e-01, 2.2112e-05],\n",
      "         [2.7456e-06, 1.0000e+00],\n",
      "         [9.9998e-01, 2.0255e-05],\n",
      "         [5.0594e-07, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.5916e-06, 1.0000e+00],\n",
      "         [9.9998e-01, 1.7333e-05],\n",
      "         [1.3928e-05, 9.9999e-01],\n",
      "         [9.9349e-01, 6.5064e-03],\n",
      "         [2.6631e-03, 9.9734e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.3355e-06, 1.0000e+00],\n",
      "         [9.9999e-01, 9.4103e-06],\n",
      "         [2.6133e-06, 1.0000e+00],\n",
      "         [9.9996e-01, 3.6925e-05],\n",
      "         [3.2550e-06, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9995e-01, 4.5504e-05],\n",
      "         [5.1359e-05, 9.9995e-01],\n",
      "         [9.9536e-01, 4.6387e-03],\n",
      "         [2.3191e-02, 9.7681e-01],\n",
      "         [7.4306e-01, 2.5694e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "1900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 4, 0, 7, 2],\n",
      "        [1, 0, 7, 5, 7],\n",
      "        [0, 6, 4, 0, 7],\n",
      "        [7, 2, 0, 5, 7],\n",
      "        [4, 6, 5, 8, 0],\n",
      "        [3, 5, 7, 0, 7],\n",
      "        [0, 6, 8, 1, 0],\n",
      "        [6, 2, 7, 3, 1]])\n",
      "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9227, -0.9321,  0.9415, -0.9510,  0.9606, -0.9703],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
      "predicted values tensor([[[-8.7368e-02],\n",
      "         [ 2.0081e-01],\n",
      "         [-2.4609e-01],\n",
      "         [ 3.2351e-01],\n",
      "         [-2.6967e-01],\n",
      "         [ 2.2899e-01]],\n",
      "\n",
      "        [[ 6.8198e-01],\n",
      "         [ 2.2642e-02],\n",
      "         [ 2.3837e-02],\n",
      "         [-1.4217e-02],\n",
      "         [ 1.3796e-02],\n",
      "         [ 1.4548e-02]],\n",
      "\n",
      "        [[ 3.7791e-01],\n",
      "         [-1.2779e-01],\n",
      "         [ 3.2736e-01],\n",
      "         [-3.2396e-02],\n",
      "         [ 1.5807e-02],\n",
      "         [-3.1387e-02]],\n",
      "\n",
      "        [[-1.4698e-01],\n",
      "         [ 3.2299e-01],\n",
      "         [-2.6422e-01],\n",
      "         [ 1.4649e-01],\n",
      "         [ 9.0003e-02],\n",
      "         [ 4.7317e-02]],\n",
      "\n",
      "        [[ 1.9715e-01],\n",
      "         [-3.1888e-01],\n",
      "         [ 2.1456e-01],\n",
      "         [-1.7605e-01],\n",
      "         [ 2.1254e-01],\n",
      "         [-1.7744e-01]],\n",
      "\n",
      "        [[ 4.3763e-01],\n",
      "         [-5.1376e-02],\n",
      "         [ 2.8084e-01],\n",
      "         [-1.9732e-02],\n",
      "         [-2.0032e-02],\n",
      "         [-3.3901e-04]],\n",
      "\n",
      "        [[-2.1185e-01],\n",
      "         [ 2.5031e-01],\n",
      "         [-1.5177e-02],\n",
      "         [ 4.6421e-01],\n",
      "         [ 3.7179e-02],\n",
      "         [ 6.5860e-03]],\n",
      "\n",
      "        [[ 6.1441e-01],\n",
      "         [-7.8617e-02],\n",
      "         [ 2.7418e-01],\n",
      "         [-1.3800e-01],\n",
      "         [ 2.6339e-01],\n",
      "         [ 6.1436e-02]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [-0.0338],\n",
      "         [-0.0755],\n",
      "         [-0.0309],\n",
      "         [-0.0403],\n",
      "         [ 0.0739]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4580],\n",
      "         [ 0.0073],\n",
      "         [-0.0243],\n",
      "         [-0.0472],\n",
      "         [-0.0494]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0776],\n",
      "         [-0.0759],\n",
      "         [ 0.3032],\n",
      "         [-0.0681],\n",
      "         [-0.0501]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0746],\n",
      "         [ 0.1255],\n",
      "         [ 0.2394],\n",
      "         [ 0.3990],\n",
      "         [ 0.3244]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0355],\n",
      "         [-0.0184],\n",
      "         [-0.0144],\n",
      "         [ 0.0236],\n",
      "         [ 0.2052]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2445],\n",
      "         [ 0.4011],\n",
      "         [ 0.5164],\n",
      "         [ 0.0027],\n",
      "         [-0.0362]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0194],\n",
      "         [ 0.0378],\n",
      "         [ 0.0407],\n",
      "         [ 0.4148],\n",
      "         [-0.0019]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2045],\n",
      "         [ 0.1071],\n",
      "         [ 0.2177],\n",
      "         [ 0.3213],\n",
      "         [ 0.4387]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9965e-01, 3.4650e-04],\n",
      "         [1.2515e-03, 9.9875e-01],\n",
      "         [9.9995e-01, 5.4848e-05],\n",
      "         [1.9845e-05, 9.9998e-01],\n",
      "         [9.9990e-01, 9.7829e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.8236e-05, 9.9998e-01],\n",
      "         [9.9616e-01, 3.8443e-03],\n",
      "         [2.4770e-02, 9.7523e-01],\n",
      "         [9.2447e-01, 7.5528e-02],\n",
      "         [2.8141e-01, 7.1859e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.5864e-05, 9.9998e-01],\n",
      "         [9.9988e-01, 1.2263e-04],\n",
      "         [1.4020e-04, 9.9986e-01],\n",
      "         [9.9508e-01, 4.9194e-03],\n",
      "         [2.1447e-02, 9.7855e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.2337e-04, 9.9978e-01],\n",
      "         [9.9993e-01, 7.3379e-05],\n",
      "         [2.6828e-05, 9.9997e-01],\n",
      "         [9.9967e-01, 3.3302e-04],\n",
      "         [1.0685e-05, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.9394e-04, 9.9971e-01],\n",
      "         [9.9991e-01, 9.0444e-05],\n",
      "         [5.4503e-05, 9.9995e-01],\n",
      "         [9.9991e-01, 9.4444e-05],\n",
      "         [1.2292e-02, 9.8771e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9660e-06, 9.9999e-01],\n",
      "         [9.9992e-01, 7.9154e-05],\n",
      "         [1.1854e-05, 9.9999e-01],\n",
      "         [9.9611e-01, 3.8933e-03],\n",
      "         [3.1227e-02, 9.6877e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9989e-01, 1.0648e-04],\n",
      "         [1.4176e-05, 9.9999e-01],\n",
      "         [9.9993e-01, 7.5035e-05],\n",
      "         [2.0831e-05, 9.9998e-01],\n",
      "         [9.9669e-01, 3.3147e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.4461e-06, 9.9999e-01],\n",
      "         [9.9994e-01, 6.4137e-05],\n",
      "         [4.3942e-05, 9.9996e-01],\n",
      "         [9.9849e-01, 1.5103e-03],\n",
      "         [9.0076e-04, 9.9910e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: -0.47\n",
      "Test score {'score': -0.47, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 0, 7, 6, 1],\n",
      "        [0, 3, 7, 0, 1],\n",
      "        [0, 4, 6, 8, 1],\n",
      "        [5, 6, 0, 2, 0],\n",
      "        [0, 2, 8, 6, 0],\n",
      "        [6, 8, 1, 0, 1],\n",
      "        [2, 8, 4, 0, 6],\n",
      "        [7, 3, 0, 6, 1]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 3.3242e-01],\n",
      "         [ 1.3986e-02],\n",
      "         [-2.0409e-03],\n",
      "         [-1.3833e-02],\n",
      "         [-4.2711e-03],\n",
      "         [-1.1587e-02]],\n",
      "\n",
      "        [[ 1.7621e-01],\n",
      "         [ 2.4772e-01],\n",
      "         [-1.4514e-02],\n",
      "         [ 1.9510e-01],\n",
      "         [-1.5465e-02],\n",
      "         [-2.2665e-02]],\n",
      "\n",
      "        [[ 1.4774e-01],\n",
      "         [-1.6885e-01],\n",
      "         [ 5.6756e-02],\n",
      "         [ 5.9398e-03],\n",
      "         [ 1.8429e-01],\n",
      "         [ 2.9069e-02]],\n",
      "\n",
      "        [[-3.8390e-01],\n",
      "         [ 9.1426e-02],\n",
      "         [-1.7129e-01],\n",
      "         [ 2.1853e-01],\n",
      "         [ 5.7666e-02],\n",
      "         [-1.1806e-02]],\n",
      "\n",
      "        [[-1.2142e-01],\n",
      "         [ 9.5669e-02],\n",
      "         [-6.9754e-02],\n",
      "         [ 3.9018e-02],\n",
      "         [ 1.2917e-01],\n",
      "         [ 9.4280e-03]],\n",
      "\n",
      "        [[ 8.3766e-02],\n",
      "         [ 3.1294e-02],\n",
      "         [ 2.8142e-01],\n",
      "         [ 8.0368e-02],\n",
      "         [-2.4847e-02],\n",
      "         [ 2.8463e-04]],\n",
      "\n",
      "        [[ 1.1676e-01],\n",
      "         [-9.6982e-02],\n",
      "         [ 8.2061e-02],\n",
      "         [-1.2234e-01],\n",
      "         [ 2.0341e-01],\n",
      "         [ 1.0256e-01]],\n",
      "\n",
      "        [[ 1.0333e-01],\n",
      "         [ 1.7446e-01],\n",
      "         [ 6.0920e-02],\n",
      "         [-5.3190e-02],\n",
      "         [-4.1086e-03],\n",
      "         [-1.7916e-02]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.5980],\n",
      "         [ 0.0287],\n",
      "         [ 0.0008],\n",
      "         [ 0.0013],\n",
      "         [ 0.0094]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1180],\n",
      "         [ 0.2402],\n",
      "         [ 0.2532],\n",
      "         [-0.0033],\n",
      "         [ 0.0042]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0870],\n",
      "         [ 0.0549],\n",
      "         [ 0.1218],\n",
      "         [ 0.2465],\n",
      "         [ 0.3578]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0651],\n",
      "         [ 0.1532],\n",
      "         [ 0.1403],\n",
      "         [ 0.5239],\n",
      "         [ 0.0367]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0379],\n",
      "         [-0.0798],\n",
      "         [-0.0408],\n",
      "         [ 0.2609],\n",
      "         [-0.0062]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1064],\n",
      "         [ 0.1824],\n",
      "         [ 0.5116],\n",
      "         [-0.0029],\n",
      "         [ 0.0145]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0549],\n",
      "         [-0.0258],\n",
      "         [ 0.0056],\n",
      "         [ 0.0241],\n",
      "         [ 0.3091]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2640],\n",
      "         [ 0.4416],\n",
      "         [-0.0024],\n",
      "         [ 0.0069],\n",
      "         [ 0.0085]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.5286e-05, 9.9998e-01],\n",
      "         [9.9685e-01, 3.1465e-03],\n",
      "         [4.3805e-02, 9.5620e-01],\n",
      "         [8.7019e-01, 1.2981e-01],\n",
      "         [3.8492e-01, 6.1508e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9983e-01, 1.7456e-04],\n",
      "         [1.7759e-04, 9.9982e-01],\n",
      "         [9.9970e-01, 3.0281e-04],\n",
      "         [1.7767e-02, 9.8223e-01],\n",
      "         [9.3271e-01, 6.7286e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.5281e-05, 9.9997e-01],\n",
      "         [9.9996e-01, 3.5348e-05],\n",
      "         [1.4813e-05, 9.9999e-01],\n",
      "         [9.9939e-01, 6.1031e-04],\n",
      "         [4.7259e-05, 9.9995e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.3692e-05],\n",
      "         [1.2331e-04, 9.9988e-01],\n",
      "         [9.9964e-01, 3.6299e-04],\n",
      "         [2.2449e-04, 9.9978e-01],\n",
      "         [9.6170e-01, 3.8303e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9982e-01, 1.7744e-04],\n",
      "         [1.5206e-04, 9.9985e-01],\n",
      "         [9.9989e-01, 1.0943e-04],\n",
      "         [5.4144e-04, 9.9946e-01],\n",
      "         [9.2394e-01, 7.6060e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.2227e-05, 9.9999e-01],\n",
      "         [9.9988e-01, 1.1967e-04],\n",
      "         [3.7936e-05, 9.9996e-01],\n",
      "         [9.8798e-01, 1.2019e-02],\n",
      "         [6.8616e-02, 9.3138e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.4579e-04, 9.9975e-01],\n",
      "         [9.9995e-01, 5.2084e-05],\n",
      "         [1.1379e-04, 9.9989e-01],\n",
      "         [9.9996e-01, 4.3140e-05],\n",
      "         [1.8016e-04, 9.9982e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9954e-01, 4.5761e-04],\n",
      "         [5.5543e-04, 9.9944e-01],\n",
      "         [9.8163e-01, 1.8366e-02],\n",
      "         [3.1086e-01, 6.8914e-01],\n",
      "         [6.0471e-01, 3.9529e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.4800, 0.0400, 0.1200, 0.0400, 0.0800]), tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.4800, 0.0400, 0.1200, 0.0400, 0.0800]), 0.14908588611021267, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 5\n",
      "Player 0 prediction: (tensor([0.1600, 0.0800, 0.2400, 0.0400, 0.0000, 0.0000, 0.2400, 0.1200, 0.1200]), tensor([0.1600, 0.0800, 0.2400, 0.0400, 0.0000, 0.0000, 0.2400, 0.1200, 0.1200]), 0.15111679092821387, tensor(2))\n",
      "action: 2\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.4400, 0.0800, 0.0000, 0.0800, 0.0000, 0.0000, 0.2800, 0.1200, 0.0000]), tensor([0.4400, 0.0800, 0.0000, 0.0800, 0.0000, 0.0000, 0.2800, 0.1200, 0.0000]), 0.34936300292091094, tensor(0))\n",
      "action: 0\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.7600, 0.0800, 0.0000]), tensor([0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.7600, 0.0800, 0.0000]), 0.7266306733371498, tensor(6))\n",
      "action: 6\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[3, 0, 8, 8, 0],\n",
      "        [5, 0, 8, 8, 0],\n",
      "        [6, 8, 1, 5, 0],\n",
      "        [6, 1, 2, 5, 8],\n",
      "        [2, 3, 5, 0, 8],\n",
      "        [2, 0, 8, 8, 0],\n",
      "        [8, 1, 0, 8, 0],\n",
      "        [0, 8, 2, 7, 3]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.8682],\n",
      "         [ 0.0723],\n",
      "         [ 0.0074],\n",
      "         [ 0.0142],\n",
      "         [ 0.0200],\n",
      "         [ 0.0115]],\n",
      "\n",
      "        [[ 0.6625],\n",
      "         [ 0.1140],\n",
      "         [ 0.0027],\n",
      "         [ 0.0124],\n",
      "         [ 0.0169],\n",
      "         [ 0.0089]],\n",
      "\n",
      "        [[ 0.0875],\n",
      "         [-0.0615],\n",
      "         [ 0.2388],\n",
      "         [ 0.0614],\n",
      "         [ 0.2986],\n",
      "         [ 0.0298]],\n",
      "\n",
      "        [[-0.1511],\n",
      "         [ 0.1649],\n",
      "         [-0.0361],\n",
      "         [ 0.0904],\n",
      "         [ 0.0793],\n",
      "         [ 0.1094]],\n",
      "\n",
      "        [[-0.1511],\n",
      "         [ 0.1252],\n",
      "         [-0.2536],\n",
      "         [ 0.0472],\n",
      "         [-0.0573],\n",
      "         [ 0.3066]],\n",
      "\n",
      "        [[ 0.5478],\n",
      "         [ 0.0769],\n",
      "         [ 0.0237],\n",
      "         [ 0.0313],\n",
      "         [ 0.0349],\n",
      "         [ 0.0085]],\n",
      "\n",
      "        [[-0.1450],\n",
      "         [ 0.3650],\n",
      "         [ 0.0982],\n",
      "         [ 0.0220],\n",
      "         [ 0.0355],\n",
      "         [ 0.0087]],\n",
      "\n",
      "        [[-0.0722],\n",
      "         [ 0.1205],\n",
      "         [ 0.0488],\n",
      "         [ 0.1948],\n",
      "         [-0.0468],\n",
      "         [ 0.3123]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.4717],\n",
      "         [ 0.0283],\n",
      "         [-0.0209],\n",
      "         [-0.0113],\n",
      "         [-0.0236]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.6114],\n",
      "         [ 0.0249],\n",
      "         [-0.0183],\n",
      "         [-0.0164],\n",
      "         [-0.0236]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1867],\n",
      "         [ 0.0941],\n",
      "         [ 0.4125],\n",
      "         [ 0.4746],\n",
      "         [ 0.0172]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0091],\n",
      "         [-0.0682],\n",
      "         [ 0.0169],\n",
      "         [ 0.0127],\n",
      "         [ 0.2743]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0144],\n",
      "         [-0.0249],\n",
      "         [ 0.0867],\n",
      "         [ 0.1217],\n",
      "         [ 0.1401]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.5875],\n",
      "         [ 0.0548],\n",
      "         [ 0.0158],\n",
      "         [-0.0116],\n",
      "         [-0.0237]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0316],\n",
      "         [ 0.4397],\n",
      "         [-0.0193],\n",
      "         [-0.0145],\n",
      "         [-0.0214]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0009],\n",
      "         [ 0.0711],\n",
      "         [ 0.1753],\n",
      "         [ 0.1352],\n",
      "         [ 0.4213]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.1557e-05, 9.9999e-01],\n",
      "         [9.9850e-01, 1.4981e-03],\n",
      "         [2.1570e-02, 9.7843e-01],\n",
      "         [9.3156e-01, 6.8444e-02],\n",
      "         [2.6664e-01, 7.3336e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.4904e-05, 9.9997e-01],\n",
      "         [9.9636e-01, 3.6408e-03],\n",
      "         [6.6369e-02, 9.3363e-01],\n",
      "         [8.0087e-01, 1.9913e-01],\n",
      "         [3.5533e-01, 6.4467e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.3975e-04, 9.9986e-01],\n",
      "         [9.9995e-01, 4.8994e-05],\n",
      "         [8.1110e-05, 9.9992e-01],\n",
      "         [9.9974e-01, 2.5712e-04],\n",
      "         [1.3738e-02, 9.8626e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9927e-01, 7.3396e-04],\n",
      "         [3.7550e-05, 9.9996e-01],\n",
      "         [9.9981e-01, 1.8552e-04],\n",
      "         [2.1977e-05, 9.9998e-01],\n",
      "         [9.9994e-01, 5.7301e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9945e-01, 5.4648e-04],\n",
      "         [4.6970e-03, 9.9530e-01],\n",
      "         [9.9995e-01, 4.7637e-05],\n",
      "         [2.0157e-05, 9.9998e-01],\n",
      "         [9.9961e-01, 3.8514e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.8774e-01, 1.2264e-02],\n",
      "         [7.0850e-03, 9.9291e-01],\n",
      "         [9.7850e-01, 2.1500e-02],\n",
      "         [5.7195e-02, 9.4280e-01],\n",
      "         [6.5582e-01, 3.4418e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9955e-01, 4.5498e-04],\n",
      "         [3.0671e-04, 9.9969e-01],\n",
      "         [9.8201e-01, 1.7995e-02],\n",
      "         [3.5682e-02, 9.6432e-01],\n",
      "         [6.2193e-01, 3.7807e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9964e-01, 3.6145e-04],\n",
      "         [1.0688e-05, 9.9999e-01],\n",
      "         [9.9991e-01, 9.4008e-05],\n",
      "         [1.2882e-04, 9.9987e-01],\n",
      "         [9.9928e-01, 7.1536e-04]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 82.0 and average score: 0.76\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 6\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.0400, 0.1200, 0.0000, 0.5200, 0.0000, 0.0000, 0.0400, 0.0800]), tensor([0.2000, 0.0400, 0.1200, 0.0000, 0.5200, 0.0000, 0.0000, 0.0400, 0.0800]), -0.2668926009943215, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 5\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3200, 0.0400, 0.2400, 0.0400, 0.0000, 0.0000, 0.0000, 0.1600, 0.2000]), tensor([0.3200, 0.0400, 0.2400, 0.0400, 0.0000, 0.0000, 0.0000, 0.1600, 0.2000]), -0.007834208102682063, tensor(0))\n",
      "action: 0\n",
      "Player 0 random action: 2\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0800, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000, 0.4000, 0.4000]), tensor([0.0000, 0.0800, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000, 0.4000, 0.4000]), -0.06521381095126201, tensor(7))\n",
      "action: 7\n",
      "Player 0 random action: 1\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.7600]), tensor([0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.0000, 0.0000, 0.0000, 0.7600]), 0.25197983258908835, tensor(8))\n",
      "action: 8\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2200\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 0, 4, 6, 8],\n",
      "        [6, 8, 1, 7, 2],\n",
      "        [6, 2, 1, 0, 8],\n",
      "        [8, 2, 1, 0, 3],\n",
      "        [5, 0, 4, 6, 2],\n",
      "        [5, 6, 4, 2, 0],\n",
      "        [2, 8, 0, 7, 0],\n",
      "        [2, 7, 8, 1, 0]])\n",
      "target value tensor([[ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9227, -0.9321,  0.9415, -0.9510,  0.9606, -0.9703],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.2500],\n",
      "         [-0.0930],\n",
      "         [-0.0600],\n",
      "         [-0.1586],\n",
      "         [ 0.0616],\n",
      "         [ 0.0126]],\n",
      "\n",
      "        [[-0.1887],\n",
      "         [ 0.0386],\n",
      "         [-0.1814],\n",
      "         [ 0.2191],\n",
      "         [-0.3570],\n",
      "         [ 0.2966]],\n",
      "\n",
      "        [[ 0.3680],\n",
      "         [-0.4748],\n",
      "         [ 0.2933],\n",
      "         [-0.0571],\n",
      "         [ 0.1273],\n",
      "         [-0.0749]],\n",
      "\n",
      "        [[ 0.0759],\n",
      "         [-0.2823],\n",
      "         [ 0.1597],\n",
      "         [-0.0804],\n",
      "         [ 0.1139],\n",
      "         [-0.0611]],\n",
      "\n",
      "        [[ 0.1042],\n",
      "         [-0.0133],\n",
      "         [-0.0305],\n",
      "         [-0.0617],\n",
      "         [-0.0471],\n",
      "         [-0.0727]],\n",
      "\n",
      "        [[ 0.1940],\n",
      "         [-0.1852],\n",
      "         [ 0.0347],\n",
      "         [-0.2536],\n",
      "         [ 0.1192],\n",
      "         [-0.3796]],\n",
      "\n",
      "        [[ 0.0638],\n",
      "         [-0.3009],\n",
      "         [ 0.1377],\n",
      "         [-0.2433],\n",
      "         [ 0.1628],\n",
      "         [-0.0937]],\n",
      "\n",
      "        [[-0.2420],\n",
      "         [ 0.1574],\n",
      "         [-0.5189],\n",
      "         [ 0.4705],\n",
      "         [-0.0759],\n",
      "         [-0.0510]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [-4.6409e-02],\n",
      "         [-9.6774e-03],\n",
      "         [ 3.3628e-03],\n",
      "         [ 5.4732e-02],\n",
      "         [ 3.7726e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-3.5213e-03],\n",
      "         [-3.2040e-02],\n",
      "         [-6.4302e-02],\n",
      "         [ 4.8298e-02],\n",
      "         [ 7.2457e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-5.4680e-02],\n",
      "         [-2.8180e-02],\n",
      "         [-3.9727e-02],\n",
      "         [-6.9094e-04],\n",
      "         [ 2.6889e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-8.7713e-02],\n",
      "         [-2.1252e-02],\n",
      "         [-6.3147e-05],\n",
      "         [ 1.3996e-01],\n",
      "         [ 1.1469e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 3.6535e-01],\n",
      "         [-1.2360e-02],\n",
      "         [-1.9406e-02],\n",
      "         [-5.8056e-02],\n",
      "         [-2.9570e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-2.5823e-02],\n",
      "         [-1.2604e-02],\n",
      "         [-2.3704e-02],\n",
      "         [-1.1530e-02],\n",
      "         [-7.4941e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.5217e-03],\n",
      "         [-4.5408e-02],\n",
      "         [ 2.1146e-01],\n",
      "         [ 8.5277e-02],\n",
      "         [ 1.5013e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-1.7158e-02],\n",
      "         [ 1.4592e-03],\n",
      "         [ 2.2215e-02],\n",
      "         [ 5.0267e-01],\n",
      "         [-7.5359e-02]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.6238e-06, 1.0000e+00],\n",
      "         [9.9999e-01, 1.2922e-05],\n",
      "         [6.8626e-07, 1.0000e+00],\n",
      "         [9.9994e-01, 6.1256e-05],\n",
      "         [4.2727e-06, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9987e-01, 1.3260e-04],\n",
      "         [4.2505e-05, 9.9996e-01],\n",
      "         [9.9969e-01, 3.1262e-04],\n",
      "         [3.8049e-06, 1.0000e+00],\n",
      "         [9.9990e-01, 1.0408e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.7620e-03, 9.9824e-01],\n",
      "         [9.9993e-01, 7.0178e-05],\n",
      "         [6.0796e-05, 9.9994e-01],\n",
      "         [9.9996e-01, 3.7297e-05],\n",
      "         [7.2404e-06, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.1024e-06, 9.9999e-01],\n",
      "         [9.9982e-01, 1.8317e-04],\n",
      "         [5.1377e-06, 9.9999e-01],\n",
      "         [9.9998e-01, 2.3783e-05],\n",
      "         [1.4485e-05, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.1063e-04, 9.9959e-01],\n",
      "         [9.7746e-01, 2.2543e-02],\n",
      "         [6.2374e-02, 9.3763e-01],\n",
      "         [6.6875e-01, 3.3125e-01],\n",
      "         [4.5113e-01, 5.4887e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [6.9030e-06, 9.9999e-01],\n",
      "         [9.9996e-01, 3.8191e-05],\n",
      "         [4.2025e-05, 9.9996e-01],\n",
      "         [9.9998e-01, 2.2584e-05],\n",
      "         [4.4336e-06, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.7612e-05, 9.9995e-01],\n",
      "         [9.9986e-01, 1.4102e-04],\n",
      "         [6.9058e-06, 9.9999e-01],\n",
      "         [9.9997e-01, 3.4415e-05],\n",
      "         [7.2150e-04, 9.9928e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9995e-01, 4.6929e-05],\n",
      "         [1.2720e-04, 9.9987e-01],\n",
      "         [9.9966e-01, 3.4017e-04],\n",
      "         [2.7459e-06, 1.0000e+00],\n",
      "         [9.9739e-01, 2.6114e-03]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 66.0 and average score: 0.36\n",
      "Results vs random: {'player_0_score': 0.76, 'player_0_win%': 0.82, 'player_1_score': 0.36, 'player_1_win%': 0.66, 'score': 0.56}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.4800, 0.0400, 0.1200, 0.0400, 0.0800]), tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.4800, 0.0400, 0.1200, 0.0400, 0.0800]), 0.26464413174753726, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 7\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.0400, 0.2800, 0.0000, 0.0000, 0.0800, 0.3200, 0.0000, 0.1200]), tensor([0.1600, 0.0400, 0.2800, 0.0000, 0.0000, 0.0800, 0.3200, 0.0000, 0.1200]), 0.3126119902362857, tensor(6))\n",
      "action: 6\n",
      "Player 1 tictactoe_expert action: 2\n",
      "Player 0 prediction: (tensor([0.3600, 0.0800, 0.0000, 0.0800, 0.0000, 0.0800, 0.0000, 0.0000, 0.4000]), tensor([0.3600, 0.0800, 0.0000, 0.0800, 0.0000, 0.0800, 0.0000, 0.0000, 0.4000]), 0.39637768409075175, tensor(8))\n",
      "action: 8\n",
      "Player 1 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.5600, 0.0000, 0.3200, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.5600, 0.0000, 0.3200, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000]), 0.2580313244984712, tensor(1))\n",
      "action: 1\n",
      "Player 1 tictactoe_expert action: 5\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), 0.4026405413585566, tensor(3))\n",
      "action: 3\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2300\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 3, 7, 8, 5],\n",
      "        [1, 0, 7, 6, 5],\n",
      "        [7, 5, 4, 6, 0],\n",
      "        [0, 5, 8, 0, 5],\n",
      "        [4, 6, 8, 3, 0],\n",
      "        [5, 0, 7, 6, 5],\n",
      "        [7, 0, 7, 6, 5],\n",
      "        [1, 5, 8, 2, 6]])\n",
      "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
      "predicted values tensor([[[ 0.3413],\n",
      "         [-0.2359],\n",
      "         [ 0.5645],\n",
      "         [-0.3687],\n",
      "         [ 0.7150],\n",
      "         [-0.0608]],\n",
      "\n",
      "        [[ 0.3274],\n",
      "         [ 0.0362],\n",
      "         [-0.0571],\n",
      "         [-0.0259],\n",
      "         [ 0.0167],\n",
      "         [-0.0116]],\n",
      "\n",
      "        [[-0.3049],\n",
      "         [ 0.2117],\n",
      "         [-0.3618],\n",
      "         [ 0.1639],\n",
      "         [-0.1239],\n",
      "         [ 0.1153]],\n",
      "\n",
      "        [[-0.0425],\n",
      "         [ 0.0302],\n",
      "         [ 0.1499],\n",
      "         [ 0.2127],\n",
      "         [ 0.0039],\n",
      "         [-0.0223]],\n",
      "\n",
      "        [[-0.3836],\n",
      "         [ 0.0242],\n",
      "         [-0.2270],\n",
      "         [ 0.2261],\n",
      "         [ 0.2004],\n",
      "         [-0.0100]],\n",
      "\n",
      "        [[ 0.3893],\n",
      "         [ 0.0808],\n",
      "         [-0.0137],\n",
      "         [-0.0102],\n",
      "         [ 0.0222],\n",
      "         [-0.0116]],\n",
      "\n",
      "        [[ 0.4970],\n",
      "         [ 0.0641],\n",
      "         [-0.0100],\n",
      "         [-0.0084],\n",
      "         [ 0.0218],\n",
      "         [-0.0112]],\n",
      "\n",
      "        [[ 0.0897],\n",
      "         [-0.2515],\n",
      "         [ 0.0201],\n",
      "         [-0.1843],\n",
      "         [ 0.1146],\n",
      "         [-0.0500]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [ 4.2435e-01],\n",
      "         [ 9.6378e-03],\n",
      "         [ 3.9710e-01],\n",
      "         [ 7.4942e-02],\n",
      "         [ 5.6454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 5.3530e-01],\n",
      "         [ 6.5677e-02],\n",
      "         [ 3.2567e-02],\n",
      "         [ 2.8454e-02],\n",
      "         [ 1.3308e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-2.0597e-03],\n",
      "         [-1.4736e-02],\n",
      "         [ 1.0468e-02],\n",
      "         [ 5.2103e-02],\n",
      "         [ 1.1220e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.4259e-01],\n",
      "         [ 1.5738e-01],\n",
      "         [ 5.1650e-01],\n",
      "         [ 6.3089e-02],\n",
      "         [ 3.5264e-04]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.2517e-01],\n",
      "         [-1.7167e-02],\n",
      "         [ 1.5728e-01],\n",
      "         [ 4.4668e-01],\n",
      "         [ 4.9468e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 4.2628e-01],\n",
      "         [ 2.0519e-02],\n",
      "         [ 1.9283e-02],\n",
      "         [ 1.4735e-02],\n",
      "         [-1.9365e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 7.5572e-01],\n",
      "         [ 4.6702e-02],\n",
      "         [ 2.0460e-02],\n",
      "         [ 1.4028e-02],\n",
      "         [-2.6248e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 2.2886e-02],\n",
      "         [-2.0204e-02],\n",
      "         [ 6.6024e-02],\n",
      "         [ 8.5929e-02],\n",
      "         [ 2.2491e-01]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.2971e-04, 9.9987e-01],\n",
      "         [9.9889e-01, 1.1102e-03],\n",
      "         [2.0647e-05, 9.9998e-01],\n",
      "         [9.9914e-01, 8.5846e-04],\n",
      "         [1.4410e-05, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.5857e-06, 1.0000e+00],\n",
      "         [9.9902e-01, 9.8136e-04],\n",
      "         [7.3970e-04, 9.9926e-01],\n",
      "         [9.4740e-01, 5.2603e-02],\n",
      "         [2.1841e-02, 9.7816e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9879e-01, 1.2066e-03],\n",
      "         [6.2720e-07, 1.0000e+00],\n",
      "         [9.9993e-01, 7.3358e-05],\n",
      "         [4.5214e-07, 1.0000e+00],\n",
      "         [9.9996e-01, 4.2290e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9958e-01, 4.2089e-04],\n",
      "         [1.4673e-05, 9.9999e-01],\n",
      "         [9.9972e-01, 2.8277e-04],\n",
      "         [1.2877e-03, 9.9871e-01],\n",
      "         [9.9287e-01, 7.1346e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9945e-01, 5.4558e-04],\n",
      "         [5.6474e-06, 9.9999e-01],\n",
      "         [9.9993e-01, 7.3862e-05],\n",
      "         [1.4549e-05, 9.9999e-01],\n",
      "         [9.9706e-01, 2.9435e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.0066e-05, 9.9998e-01],\n",
      "         [9.8290e-01, 1.7101e-02],\n",
      "         [5.5155e-02, 9.4485e-01],\n",
      "         [3.7923e-01, 6.2077e-01],\n",
      "         [1.0976e-01, 8.9024e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.4876e-06, 9.9999e-01],\n",
      "         [9.9117e-01, 8.8324e-03],\n",
      "         [5.4849e-02, 9.4515e-01],\n",
      "         [3.6933e-01, 6.3067e-01],\n",
      "         [1.1823e-01, 8.8177e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.2032e-06, 1.0000e+00],\n",
      "         [9.9996e-01, 3.7304e-05],\n",
      "         [7.1337e-07, 1.0000e+00],\n",
      "         [9.9989e-01, 1.1081e-04],\n",
      "         [1.8289e-06, 1.0000e+00]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 6.0 and average score: -0.84\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 6\n",
      "Player 1 prediction: (tensor([0.1600, 0.0400, 0.1200, 0.0400, 0.5200, 0.0000, 0.0000, 0.0400, 0.0800]), tensor([0.1600, 0.0400, 0.1200, 0.0400, 0.5200, 0.0000, 0.0000, 0.0400, 0.0800]), -0.2812839176922429, tensor(4))\n",
      "action: 4\n",
      "Player 0 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3600, 0.0800, 0.2800, 0.0000, 0.0000, 0.0800, 0.0000, 0.0800, 0.1200]), tensor([0.3600, 0.0800, 0.2800, 0.0000, 0.0000, 0.0800, 0.0000, 0.0800, 0.1200]), 0.1018222245059188, tensor(0))\n",
      "action: 0\n",
      "Player 0 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.1200, 0.5600, 0.0000, 0.0000, 0.1600, 0.0000, 0.1600, 0.0000]), tensor([0.0000, 0.1200, 0.5600, 0.0000, 0.0000, 0.1600, 0.0000, 0.1600, 0.0000]), 0.05538110290608366, tensor(2))\n",
      "action: 2\n",
      "Player 0 tictactoe_expert action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2400\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[4, 0, 0, 7, 6],\n",
      "        [7, 3, 1, 0, 6],\n",
      "        [5, 8, 0, 7, 6],\n",
      "        [5, 7, 0, 7, 6],\n",
      "        [6, 5, 7, 0, 6],\n",
      "        [5, 4, 7, 0, 6],\n",
      "        [4, 2, 6, 8, 7],\n",
      "        [2, 7, 8, 6, 1]])\n",
      "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000]])\n",
      "predicted values tensor([[[ 5.4476e-01],\n",
      "         [-7.7565e-02],\n",
      "         [ 6.3834e-03],\n",
      "         [ 1.3307e-02],\n",
      "         [ 1.1153e-02],\n",
      "         [ 1.9909e-02]],\n",
      "\n",
      "        [[-2.2579e-01],\n",
      "         [ 1.1380e-01],\n",
      "         [ 2.6969e-02],\n",
      "         [ 1.3207e-01],\n",
      "         [ 1.8709e-02],\n",
      "         [-1.8214e-04]],\n",
      "\n",
      "        [[ 6.0163e-02],\n",
      "         [ 4.0369e-01],\n",
      "         [ 5.1031e-02],\n",
      "         [ 5.4931e-04],\n",
      "         [ 7.5078e-03],\n",
      "         [ 1.4760e-02]],\n",
      "\n",
      "        [[-1.8335e-01],\n",
      "         [ 8.8309e-02],\n",
      "         [-3.8613e-02],\n",
      "         [ 2.4250e-02],\n",
      "         [ 1.6380e-02],\n",
      "         [ 1.6408e-02]],\n",
      "\n",
      "        [[ 3.7932e-01],\n",
      "         [-2.0597e-02],\n",
      "         [ 2.6215e-01],\n",
      "         [ 3.1705e-02],\n",
      "         [ 3.5104e-03],\n",
      "         [ 4.3826e-03]],\n",
      "\n",
      "        [[ 2.4308e-01],\n",
      "         [-1.2585e-01],\n",
      "         [ 2.2169e-01],\n",
      "         [-7.8425e-02],\n",
      "         [ 1.3319e-02],\n",
      "         [ 1.6686e-03]],\n",
      "\n",
      "        [[ 1.9375e-02],\n",
      "         [-1.1930e-01],\n",
      "         [ 1.9155e-01],\n",
      "         [-9.8285e-02],\n",
      "         [ 2.2763e-01],\n",
      "         [-7.6670e-02]],\n",
      "\n",
      "        [[ 1.5839e-01],\n",
      "         [-2.0791e-01],\n",
      "         [ 2.0295e-01],\n",
      "         [-5.1030e-02],\n",
      "         [ 1.5803e-01],\n",
      "         [ 1.0947e-01]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [ 3.1955e-01],\n",
      "         [ 2.4265e-03],\n",
      "         [-2.5635e-02],\n",
      "         [ 3.0033e-03],\n",
      "         [-1.5733e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 5.5766e-02],\n",
      "         [ 1.1582e-01],\n",
      "         [ 3.6089e-01],\n",
      "         [-2.6580e-04],\n",
      "         [-9.4208e-04]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 2.1254e-01],\n",
      "         [ 7.2058e-01],\n",
      "         [ 1.6738e-02],\n",
      "         [ 5.0760e-03],\n",
      "         [-6.2786e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.7268e-01],\n",
      "         [ 1.6273e-01],\n",
      "         [ 2.8386e-03],\n",
      "         [-6.5274e-03],\n",
      "         [ 2.8230e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 4.5957e-01],\n",
      "         [ 3.7018e-01],\n",
      "         [ 6.4204e-01],\n",
      "         [ 1.0630e-03],\n",
      "         [-1.2152e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-1.0337e-02],\n",
      "         [ 2.4333e-01],\n",
      "         [ 2.1702e-01],\n",
      "         [-1.6752e-02],\n",
      "         [-1.9175e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-3.1613e-02],\n",
      "         [-1.8497e-02],\n",
      "         [-3.3483e-02],\n",
      "         [ 1.4570e-01],\n",
      "         [ 2.1066e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.8699e-02],\n",
      "         [ 3.3005e-02],\n",
      "         [ 1.3605e-01],\n",
      "         [ 1.8324e-01],\n",
      "         [ 6.0213e-01]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.0464e-06, 1.0000e+00],\n",
      "         [9.9488e-01, 5.1177e-03],\n",
      "         [8.5801e-02, 9.1420e-01],\n",
      "         [7.0379e-01, 2.9621e-01],\n",
      "         [1.6153e-01, 8.3847e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 2.3622e-05],\n",
      "         [7.0338e-04, 9.9930e-01],\n",
      "         [9.9726e-01, 2.7373e-03],\n",
      "         [7.7407e-02, 9.2259e-01],\n",
      "         [3.9309e-01, 6.0691e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.0592e-03, 9.9794e-01],\n",
      "         [9.6894e-01, 3.1060e-02],\n",
      "         [2.7805e-01, 7.2195e-01],\n",
      "         [7.8905e-01, 2.1095e-01],\n",
      "         [1.1935e-01, 8.8065e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.4314e-05],\n",
      "         [2.6160e-05, 9.9997e-01],\n",
      "         [9.8026e-01, 1.9744e-02],\n",
      "         [5.6464e-02, 9.4354e-01],\n",
      "         [4.1789e-01, 5.8211e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.5278e-05, 9.9996e-01],\n",
      "         [9.9988e-01, 1.2011e-04],\n",
      "         [2.8783e-04, 9.9971e-01],\n",
      "         [9.0934e-01, 9.0660e-02],\n",
      "         [9.6076e-02, 9.0392e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.2816e-05, 9.9999e-01],\n",
      "         [9.9984e-01, 1.6470e-04],\n",
      "         [2.3758e-06, 1.0000e+00],\n",
      "         [9.9465e-01, 5.3456e-03],\n",
      "         [2.0134e-02, 9.7987e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.2645e-06, 1.0000e+00],\n",
      "         [9.9997e-01, 2.8560e-05],\n",
      "         [1.3533e-05, 9.9999e-01],\n",
      "         [9.9967e-01, 3.3032e-04],\n",
      "         [6.1177e-05, 9.9994e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.3033e-05, 9.9998e-01],\n",
      "         [9.9998e-01, 1.4971e-05],\n",
      "         [1.4567e-05, 9.9999e-01],\n",
      "         [9.9842e-01, 1.5789e-03],\n",
      "         [1.2873e-04, 9.9987e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 4.0 and average score: -0.9\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.84, 'player_0_win%': 0.06, 'player_1_score': -0.9, 'player_1_win%': 0.04, 'score': -0.87}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2500\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[7, 1, 8, 0, 5],\n",
      "        [2, 0, 7, 8, 4],\n",
      "        [6, 4, 0, 2, 7],\n",
      "        [2, 6, 8, 5, 1],\n",
      "        [6, 0, 7, 0, 5],\n",
      "        [8, 6, 0, 2, 0],\n",
      "        [7, 6, 8, 0, 5],\n",
      "        [7, 5, 0, 4, 5]])\n",
      "target value tensor([[ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.4147],\n",
      "         [ 0.2470],\n",
      "         [ 0.4335],\n",
      "         [ 0.0681],\n",
      "         [-0.0126],\n",
      "         [-0.0009]],\n",
      "\n",
      "        [[-0.1659],\n",
      "         [ 0.3011],\n",
      "         [-0.1835],\n",
      "         [ 0.3092],\n",
      "         [-0.0873],\n",
      "         [ 0.2756]],\n",
      "\n",
      "        [[-0.1639],\n",
      "         [ 0.1443],\n",
      "         [-0.3312],\n",
      "         [ 0.3848],\n",
      "         [ 0.0228],\n",
      "         [ 0.1633]],\n",
      "\n",
      "        [[ 0.1943],\n",
      "         [-0.1906],\n",
      "         [ 0.2936],\n",
      "         [-0.0217],\n",
      "         [ 0.1105],\n",
      "         [ 0.0961]],\n",
      "\n",
      "        [[ 0.2113],\n",
      "         [-0.1360],\n",
      "         [ 0.2647],\n",
      "         [-0.1188],\n",
      "         [ 0.0027],\n",
      "         [-0.0044]],\n",
      "\n",
      "        [[ 0.1079],\n",
      "         [ 0.0992],\n",
      "         [ 0.0860],\n",
      "         [-0.0174],\n",
      "         [ 0.1715],\n",
      "         [ 0.0071]],\n",
      "\n",
      "        [[ 0.3049],\n",
      "         [-0.3304],\n",
      "         [ 0.2881],\n",
      "         [ 0.0589],\n",
      "         [ 0.0024],\n",
      "         [-0.0052]],\n",
      "\n",
      "        [[ 0.0761],\n",
      "         [ 0.1340],\n",
      "         [ 0.0989],\n",
      "         [-0.0442],\n",
      "         [-0.0299],\n",
      "         [ 0.0087]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0877],\n",
      "         [ 0.2755],\n",
      "         [ 0.6861],\n",
      "         [ 0.0053],\n",
      "         [-0.0427]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0461],\n",
      "         [ 0.0268],\n",
      "         [ 0.0634],\n",
      "         [ 0.0847],\n",
      "         [ 0.0177]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0411],\n",
      "         [-0.0315],\n",
      "         [-0.0182],\n",
      "         [ 0.1535],\n",
      "         [ 0.0941]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0187],\n",
      "         [-0.0349],\n",
      "         [ 0.0225],\n",
      "         [ 0.1647],\n",
      "         [ 0.2781]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1544],\n",
      "         [ 0.1293],\n",
      "         [ 0.1669],\n",
      "         [-0.0152],\n",
      "         [-0.0509]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0173],\n",
      "         [ 0.1609],\n",
      "         [ 0.1861],\n",
      "         [ 0.2569],\n",
      "         [-0.0792]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1036],\n",
      "         [ 0.0106],\n",
      "         [ 0.4988],\n",
      "         [-0.0391],\n",
      "         [-0.0610]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1926],\n",
      "         [ 0.3330],\n",
      "         [ 0.0068],\n",
      "         [-0.0306],\n",
      "         [-0.0067]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [8.1845e-04, 9.9918e-01],\n",
      "         [9.9684e-01, 3.1626e-03],\n",
      "         [3.5148e-05, 9.9996e-01],\n",
      "         [9.9210e-01, 7.9037e-03],\n",
      "         [3.1108e-02, 9.6889e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9996e-01, 4.4298e-05],\n",
      "         [3.0002e-06, 1.0000e+00],\n",
      "         [9.9993e-01, 6.6751e-05],\n",
      "         [1.0617e-06, 1.0000e+00],\n",
      "         [9.9952e-01, 4.7644e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9984e-01, 1.5608e-04],\n",
      "         [3.5446e-05, 9.9996e-01],\n",
      "         [9.9989e-01, 1.1383e-04],\n",
      "         [8.9818e-05, 9.9991e-01],\n",
      "         [9.9856e-01, 1.4442e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.8723e-04, 9.9971e-01],\n",
      "         [9.9997e-01, 3.2119e-05],\n",
      "         [3.3756e-06, 1.0000e+00],\n",
      "         [9.9940e-01, 5.9735e-04],\n",
      "         [1.2619e-04, 9.9987e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.6535e-06, 1.0000e+00],\n",
      "         [9.9998e-01, 1.6327e-05],\n",
      "         [2.7440e-05, 9.9997e-01],\n",
      "         [9.9582e-01, 4.1834e-03],\n",
      "         [3.1939e-03, 9.9681e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.3316e-07, 1.0000e+00],\n",
      "         [9.9996e-01, 4.3885e-05],\n",
      "         [2.5270e-05, 9.9997e-01],\n",
      "         [9.9988e-01, 1.2155e-04],\n",
      "         [9.4231e-03, 9.9058e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.2545e-05, 9.9996e-01],\n",
      "         [9.9996e-01, 4.0180e-05],\n",
      "         [8.5615e-05, 9.9991e-01],\n",
      "         [9.7480e-01, 2.5197e-02],\n",
      "         [2.9103e-02, 9.7090e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9987e-01, 1.3050e-04],\n",
      "         [4.8627e-06, 1.0000e+00],\n",
      "         [9.9766e-01, 2.3429e-03],\n",
      "         [3.2349e-02, 9.6765e-01],\n",
      "         [1.8044e-01, 8.1956e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2600\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[8, 7, 1, 2, 0],\n",
      "        [1, 0, 0, 1, 8],\n",
      "        [2, 8, 0, 1, 8],\n",
      "        [2, 0, 6, 1, 8],\n",
      "        [8, 7, 5, 0, 8],\n",
      "        [2, 6, 8, 5, 7],\n",
      "        [8, 5, 7, 1, 0],\n",
      "        [1, 2, 8, 5, 7]])\n",
      "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.0983],\n",
      "         [ 0.1667],\n",
      "         [ 0.1751],\n",
      "         [ 0.1062],\n",
      "         [ 0.0168],\n",
      "         [-0.0499]],\n",
      "\n",
      "        [[-0.1085],\n",
      "         [ 0.2941],\n",
      "         [-0.0384],\n",
      "         [-0.0262],\n",
      "         [-0.0286],\n",
      "         [-0.0423]],\n",
      "\n",
      "        [[-0.0855],\n",
      "         [ 0.1168],\n",
      "         [ 0.0911],\n",
      "         [-0.0645],\n",
      "         [ 0.0041],\n",
      "         [-0.0590]],\n",
      "\n",
      "        [[-0.0375],\n",
      "         [-0.1658],\n",
      "         [-0.0706],\n",
      "         [-0.0637],\n",
      "         [-0.0180],\n",
      "         [-0.0488]],\n",
      "\n",
      "        [[ 0.0254],\n",
      "         [-0.0372],\n",
      "         [ 0.3532],\n",
      "         [ 0.0577],\n",
      "         [-0.0575],\n",
      "         [-0.0517]],\n",
      "\n",
      "        [[-0.0955],\n",
      "         [ 0.0576],\n",
      "         [-0.2074],\n",
      "         [ 0.1945],\n",
      "         [-0.1060],\n",
      "         [ 0.3747]],\n",
      "\n",
      "        [[-0.4962],\n",
      "         [ 0.2519],\n",
      "         [-0.0419],\n",
      "         [ 0.3853],\n",
      "         [-0.0310],\n",
      "         [-0.0589]],\n",
      "\n",
      "        [[-0.1067],\n",
      "         [ 0.4138],\n",
      "         [-0.1567],\n",
      "         [ 0.3185],\n",
      "         [-0.0569],\n",
      "         [ 0.3747]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0981],\n",
      "         [ 0.3449],\n",
      "         [ 0.5248],\n",
      "         [ 0.7363],\n",
      "         [ 0.0077]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.4266],\n",
      "         [ 0.4731],\n",
      "         [ 0.0171],\n",
      "         [ 0.0203],\n",
      "         [ 0.0119]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0825],\n",
      "         [ 0.2327],\n",
      "         [ 0.0676],\n",
      "         [ 0.0799],\n",
      "         [ 0.0454]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2294],\n",
      "         [-0.0098],\n",
      "         [ 0.0538],\n",
      "         [ 0.0685],\n",
      "         [ 0.0197]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2021],\n",
      "         [ 0.1721],\n",
      "         [ 0.4872],\n",
      "         [ 0.0102],\n",
      "         [-0.0196]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0157],\n",
      "         [ 0.0618],\n",
      "         [ 0.0742],\n",
      "         [ 0.1653],\n",
      "         [ 0.3557]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0636],\n",
      "         [ 0.5401],\n",
      "         [ 0.2723],\n",
      "         [ 0.6111],\n",
      "         [ 0.0125]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0485],\n",
      "         [ 0.5299],\n",
      "         [-0.0100],\n",
      "         [ 0.3141],\n",
      "         [ 0.3540]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9860e-01, 1.4007e-03],\n",
      "         [9.0300e-07, 1.0000e+00],\n",
      "         [9.7157e-01, 2.8432e-02],\n",
      "         [2.6022e-06, 1.0000e+00],\n",
      "         [9.2996e-01, 7.0045e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9096e-01, 9.0413e-03],\n",
      "         [5.4609e-05, 9.9995e-01],\n",
      "         [8.5528e-01, 1.4472e-01],\n",
      "         [2.1012e-02, 9.7899e-01],\n",
      "         [7.3115e-01, 2.6885e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.0148e-06, 1.0000e+00],\n",
      "         [9.9952e-01, 4.7519e-04],\n",
      "         [1.9171e-05, 9.9998e-01],\n",
      "         [9.6871e-01, 3.1294e-02],\n",
      "         [6.4688e-03, 9.9353e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.7168e-08, 1.0000e+00],\n",
      "         [9.9391e-01, 6.0875e-03],\n",
      "         [3.2572e-04, 9.9967e-01],\n",
      "         [9.3862e-01, 6.1375e-02],\n",
      "         [3.0251e-02, 9.6975e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.4133e-07, 1.0000e+00],\n",
      "         [9.9869e-01, 1.3056e-03],\n",
      "         [2.3297e-07, 1.0000e+00],\n",
      "         [9.7775e-01, 2.2245e-02],\n",
      "         [1.6868e-02, 9.8313e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9978e-01, 2.2458e-04],\n",
      "         [7.1264e-07, 1.0000e+00],\n",
      "         [9.9969e-01, 3.1244e-04],\n",
      "         [5.0210e-07, 1.0000e+00],\n",
      "         [9.9898e-01, 1.0225e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9813e-01, 1.8722e-03],\n",
      "         [1.8645e-06, 1.0000e+00],\n",
      "         [9.9884e-01, 1.1556e-03],\n",
      "         [3.1174e-07, 1.0000e+00],\n",
      "         [9.8755e-01, 1.2451e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9947e-01, 5.3366e-04],\n",
      "         [6.5672e-07, 1.0000e+00],\n",
      "         [9.9978e-01, 2.1561e-04],\n",
      "         [1.1309e-06, 1.0000e+00],\n",
      "         [9.9789e-01, 2.1073e-03]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2700\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[6, 0, 0, 1, 7],\n",
      "        [1, 5, 7, 8, 0],\n",
      "        [1, 2, 7, 0, 7],\n",
      "        [3, 5, 0, 1, 7],\n",
      "        [8, 1, 0, 1, 7],\n",
      "        [8, 6, 0, 3, 0],\n",
      "        [8, 0, 8, 1, 7],\n",
      "        [4, 2, 6, 8, 0]])\n",
      "target value tensor([[-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-0.3837],\n",
      "         [ 0.3657],\n",
      "         [-0.3801],\n",
      "         [-0.0390],\n",
      "         [-0.0317],\n",
      "         [-0.0291]],\n",
      "\n",
      "        [[-0.3340],\n",
      "         [ 0.7752],\n",
      "         [-0.2459],\n",
      "         [ 0.5770],\n",
      "         [-0.0248],\n",
      "         [-0.0404]],\n",
      "\n",
      "        [[ 0.5161],\n",
      "         [-0.2506],\n",
      "         [ 0.2064],\n",
      "         [-0.3283],\n",
      "         [-0.0388],\n",
      "         [-0.0451]],\n",
      "\n",
      "        [[-0.0872],\n",
      "         [ 0.1885],\n",
      "         [ 0.0115],\n",
      "         [-0.0579],\n",
      "         [-0.0279],\n",
      "         [-0.0225]],\n",
      "\n",
      "        [[-0.2601],\n",
      "         [ 0.4791],\n",
      "         [-0.0856],\n",
      "         [-0.0464],\n",
      "         [-0.0442],\n",
      "         [-0.0268]],\n",
      "\n",
      "        [[-0.4043],\n",
      "         [ 0.2214],\n",
      "         [-0.2462],\n",
      "         [ 0.0860],\n",
      "         [ 0.0722],\n",
      "         [-0.0665]],\n",
      "\n",
      "        [[-0.1354],\n",
      "         [-0.0561],\n",
      "         [-0.1148],\n",
      "         [-0.0847],\n",
      "         [-0.0342],\n",
      "         [-0.0231]],\n",
      "\n",
      "        [[ 0.3128],\n",
      "         [-0.4032],\n",
      "         [ 0.1002],\n",
      "         [-0.2981],\n",
      "         [ 0.1661],\n",
      "         [-0.2744]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [ 5.9780e-02],\n",
      "         [ 1.1313e-01],\n",
      "         [-1.7935e-02],\n",
      "         [-3.8892e-02],\n",
      "         [ 7.0585e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 6.2840e-02],\n",
      "         [ 5.1160e-01],\n",
      "         [ 1.3651e-01],\n",
      "         [ 8.0780e-01],\n",
      "         [ 1.5849e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.2472e-02],\n",
      "         [-4.0975e-04],\n",
      "         [ 2.6505e-01],\n",
      "         [ 1.4771e-03],\n",
      "         [-4.7670e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 3.9446e-01],\n",
      "         [ 4.1098e-01],\n",
      "         [-1.4433e-02],\n",
      "         [-1.4582e-02],\n",
      "         [ 9.8373e-04]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-2.6522e-02],\n",
      "         [ 6.7917e-01],\n",
      "         [ 6.0087e-03],\n",
      "         [ 1.2034e-02],\n",
      "         [ 1.0303e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.8321e-01],\n",
      "         [ 2.9352e-01],\n",
      "         [ 2.5858e-01],\n",
      "         [ 5.6949e-01],\n",
      "         [-2.8307e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-4.4390e-04],\n",
      "         [-7.9282e-02],\n",
      "         [-6.6135e-02],\n",
      "         [-1.5889e-02],\n",
      "         [ 9.0158e-03]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.6127e-02],\n",
      "         [-3.1809e-03],\n",
      "         [ 8.5063e-04],\n",
      "         [-7.0870e-02],\n",
      "         [ 1.2416e-01]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9971e-01, 2.9024e-04],\n",
      "         [1.0551e-04, 9.9989e-01],\n",
      "         [9.9597e-01, 4.0332e-03],\n",
      "         [2.2146e-02, 9.7785e-01],\n",
      "         [9.8457e-01, 1.5425e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9989e-01, 1.0533e-04],\n",
      "         [1.9327e-04, 9.9981e-01],\n",
      "         [9.9942e-01, 5.8405e-04],\n",
      "         [1.8623e-04, 9.9981e-01],\n",
      "         [9.7135e-01, 2.8650e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.2574e-07, 1.0000e+00],\n",
      "         [9.9996e-01, 3.9387e-05],\n",
      "         [3.9371e-04, 9.9961e-01],\n",
      "         [9.9047e-01, 9.5302e-03],\n",
      "         [6.0891e-02, 9.3911e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9939e-01, 6.1181e-04],\n",
      "         [1.6930e-05, 9.9998e-01],\n",
      "         [9.8910e-01, 1.0899e-02],\n",
      "         [3.5563e-01, 6.4437e-01],\n",
      "         [9.2863e-01, 7.1374e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 2.7093e-05],\n",
      "         [4.0087e-06, 1.0000e+00],\n",
      "         [9.9670e-01, 3.2961e-03],\n",
      "         [1.9313e-02, 9.8069e-01],\n",
      "         [9.8445e-01, 1.5551e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9988e-01, 1.1842e-04],\n",
      "         [4.3355e-05, 9.9996e-01],\n",
      "         [9.9990e-01, 1.0339e-04],\n",
      "         [2.2460e-04, 9.9978e-01],\n",
      "         [9.9291e-01, 7.0908e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.1890e-06, 9.9999e-01],\n",
      "         [9.9913e-01, 8.7091e-04],\n",
      "         [2.2765e-02, 9.7723e-01],\n",
      "         [9.8230e-01, 1.7700e-02],\n",
      "         [5.5660e-01, 4.4340e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.1697e-04, 9.9978e-01],\n",
      "         [9.9998e-01, 1.7520e-05],\n",
      "         [1.9807e-04, 9.9980e-01],\n",
      "         [9.9995e-01, 4.9040e-05],\n",
      "         [4.8386e-05, 9.9995e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2800\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[1, 0, 7, 8, 0],\n",
      "        [3, 6, 4, 0, 8],\n",
      "        [4, 2, 8, 0, 6],\n",
      "        [0, 6, 8, 1, 0],\n",
      "        [0, 6, 7, 8, 4],\n",
      "        [1, 5, 0, 5, 8],\n",
      "        [2, 8, 6, 0, 0],\n",
      "        [6, 7, 3, 1, 0]])\n",
      "target value tensor([[-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[-4.1731e-02],\n",
      "         [ 6.4077e-01],\n",
      "         [-4.0882e-02],\n",
      "         [ 6.4804e-01],\n",
      "         [ 4.0912e-02],\n",
      "         [ 3.7101e-02]],\n",
      "\n",
      "        [[ 2.5886e-01],\n",
      "         [ 1.5405e-01],\n",
      "         [ 1.8821e-02],\n",
      "         [ 9.7592e-02],\n",
      "         [ 1.1604e-03],\n",
      "         [ 1.1143e-02]],\n",
      "\n",
      "        [[ 3.0420e-01],\n",
      "         [-1.9266e-01],\n",
      "         [ 1.5903e-01],\n",
      "         [-1.8192e-01],\n",
      "         [ 2.4719e-01],\n",
      "         [ 9.7943e-02]],\n",
      "\n",
      "        [[-3.1397e-01],\n",
      "         [ 2.1206e-01],\n",
      "         [-2.8721e-01],\n",
      "         [ 7.0125e-01],\n",
      "         [ 8.3822e-02],\n",
      "         [-2.5149e-02]],\n",
      "\n",
      "        [[-1.4859e-01],\n",
      "         [ 1.4772e-01],\n",
      "         [-5.2994e-02],\n",
      "         [ 1.3680e-01],\n",
      "         [-2.3063e-02],\n",
      "         [ 2.8058e-01]],\n",
      "\n",
      "        [[ 1.4893e-01],\n",
      "         [ 4.4451e-01],\n",
      "         [ 6.4034e-02],\n",
      "         [ 1.5700e-02],\n",
      "         [ 1.7212e-02],\n",
      "         [ 2.2635e-02]],\n",
      "\n",
      "        [[ 4.9648e-02],\n",
      "         [-4.3601e-02],\n",
      "         [ 1.8755e-01],\n",
      "         [-2.5459e-02],\n",
      "         [ 3.3952e-01],\n",
      "         [ 4.0111e-04]],\n",
      "\n",
      "        [[-1.7669e-01],\n",
      "         [ 3.6145e-01],\n",
      "         [-1.8431e-02],\n",
      "         [ 2.4828e-01],\n",
      "         [ 1.2827e-01],\n",
      "         [ 4.3461e-03]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.1835],\n",
      "         [ 0.5286],\n",
      "         [ 0.2230],\n",
      "         [ 0.8504],\n",
      "         [ 0.0320]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0692],\n",
      "         [ 0.2257],\n",
      "         [ 0.4138],\n",
      "         [ 0.0310],\n",
      "         [-0.0086]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0270],\n",
      "         [ 0.0155],\n",
      "         [-0.0994],\n",
      "         [-0.0588],\n",
      "         [ 0.0530]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0958],\n",
      "         [ 0.0869],\n",
      "         [ 0.0091],\n",
      "         [ 0.7249],\n",
      "         [ 0.0541]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0507],\n",
      "         [-0.0074],\n",
      "         [-0.0234],\n",
      "         [ 0.0595],\n",
      "         [ 0.0244]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.3883],\n",
      "         [ 0.6136],\n",
      "         [-0.0164],\n",
      "         [-0.0088],\n",
      "         [-0.0216]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0256],\n",
      "         [ 0.0289],\n",
      "         [ 0.0183],\n",
      "         [ 0.2047],\n",
      "         [ 0.0667]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1327],\n",
      "         [ 0.1359],\n",
      "         [ 0.4447],\n",
      "         [ 0.5954],\n",
      "         [-0.0445]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9935e-01, 6.4994e-04],\n",
      "         [4.9018e-04, 9.9951e-01],\n",
      "         [9.9965e-01, 3.5046e-04],\n",
      "         [3.0382e-04, 9.9970e-01],\n",
      "         [9.9302e-01, 6.9822e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [5.5811e-05, 9.9994e-01],\n",
      "         [9.9995e-01, 5.0072e-05],\n",
      "         [6.4981e-05, 9.9994e-01],\n",
      "         [9.9176e-01, 8.2369e-03],\n",
      "         [2.9842e-02, 9.7016e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.9676e-04, 9.9980e-01],\n",
      "         [9.9998e-01, 1.5460e-05],\n",
      "         [7.9928e-05, 9.9992e-01],\n",
      "         [9.9994e-01, 5.7911e-05],\n",
      "         [1.4797e-05, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 3.3731e-05],\n",
      "         [5.1839e-06, 9.9999e-01],\n",
      "         [9.9998e-01, 1.5037e-05],\n",
      "         [1.3134e-05, 9.9999e-01],\n",
      "         [9.9954e-01, 4.6487e-04]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9996e-01, 4.0056e-05],\n",
      "         [1.3255e-05, 9.9999e-01],\n",
      "         [9.9992e-01, 7.7904e-05],\n",
      "         [5.7123e-06, 9.9999e-01],\n",
      "         [9.9998e-01, 2.0494e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9964e-01, 3.5880e-04],\n",
      "         [6.2015e-04, 9.9938e-01],\n",
      "         [9.9013e-01, 9.8690e-03],\n",
      "         [1.1669e-01, 8.8331e-01],\n",
      "         [9.5407e-01, 4.5927e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.4839e-05, 9.9998e-01],\n",
      "         [9.9998e-01, 2.4890e-05],\n",
      "         [2.2027e-05, 9.9998e-01],\n",
      "         [9.9998e-01, 2.1016e-05],\n",
      "         [4.4547e-05, 9.9996e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 1.7266e-05],\n",
      "         [4.1177e-04, 9.9959e-01],\n",
      "         [9.9983e-01, 1.6603e-04],\n",
      "         [9.7771e-04, 9.9902e-01],\n",
      "         [9.5806e-01, 4.1936e-02]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False, False]]) tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: 0.88\n",
      "Test score {'score': 0.88, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "2900\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 7, 1, 4, 8],\n",
      "        [3, 1, 0, 2, 5],\n",
      "        [6, 2, 4, 0, 8],\n",
      "        [8, 3, 0, 6, 4],\n",
      "        [7, 3, 0, 2, 5],\n",
      "        [6, 8, 1, 5, 0],\n",
      "        [7, 6, 8, 0, 1],\n",
      "        [0, 4, 2, 7, 8]])\n",
      "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
      "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000]])\n",
      "predicted values tensor([[[-0.0454],\n",
      "         [ 0.3458],\n",
      "         [-0.0354],\n",
      "         [ 0.3829],\n",
      "         [-0.3022],\n",
      "         [ 0.4333]],\n",
      "\n",
      "        [[ 0.0698],\n",
      "         [ 0.1510],\n",
      "         [ 0.2952],\n",
      "         [ 0.0083],\n",
      "         [ 0.0034],\n",
      "         [ 0.0143]],\n",
      "\n",
      "        [[ 0.1838],\n",
      "         [-0.2432],\n",
      "         [ 0.3620],\n",
      "         [-0.1619],\n",
      "         [ 0.1037],\n",
      "         [ 0.1130]],\n",
      "\n",
      "        [[ 0.1838],\n",
      "         [-0.2147],\n",
      "         [ 0.2261],\n",
      "         [-0.4480],\n",
      "         [ 0.4757],\n",
      "         [-0.1121]],\n",
      "\n",
      "        [[ 0.2742],\n",
      "         [-0.0983],\n",
      "         [ 0.4133],\n",
      "         [ 0.0147],\n",
      "         [ 0.0158],\n",
      "         [ 0.0189]],\n",
      "\n",
      "        [[-0.3851],\n",
      "         [-0.0957],\n",
      "         [ 0.0923],\n",
      "         [ 0.3089],\n",
      "         [ 0.0447],\n",
      "         [ 0.0068]],\n",
      "\n",
      "        [[ 0.1515],\n",
      "         [-0.4604],\n",
      "         [ 0.3964],\n",
      "         [-0.2405],\n",
      "         [ 0.4208],\n",
      "         [ 0.1725]],\n",
      "\n",
      "        [[-0.0454],\n",
      "         [ 0.1490],\n",
      "         [-0.1750],\n",
      "         [ 0.1663],\n",
      "         [-0.5618],\n",
      "         [ 0.6206]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0933],\n",
      "         [ 0.0086],\n",
      "         [ 0.0732],\n",
      "         [ 0.2530],\n",
      "         [ 0.0492]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1826],\n",
      "         [ 0.5966],\n",
      "         [-0.0022],\n",
      "         [ 0.0478],\n",
      "         [ 0.0125]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0088],\n",
      "         [ 0.0760],\n",
      "         [ 0.0329],\n",
      "         [-0.0059],\n",
      "         [ 0.1671]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0352],\n",
      "         [-0.0810],\n",
      "         [-0.0928],\n",
      "         [-0.0153],\n",
      "         [ 0.3531]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1770],\n",
      "         [ 0.3362],\n",
      "         [ 0.0657],\n",
      "         [ 0.0485],\n",
      "         [ 0.0180]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0587],\n",
      "         [ 0.2133],\n",
      "         [ 0.5883],\n",
      "         [ 0.5856],\n",
      "         [ 0.0396]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0260],\n",
      "         [-0.0416],\n",
      "         [-0.0075],\n",
      "         [-0.0280],\n",
      "         [ 0.5715]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0020],\n",
      "         [-0.0095],\n",
      "         [-0.0091],\n",
      "         [ 0.0006],\n",
      "         [ 0.0446]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [9.9999e-01, 1.3510e-05],\n",
      "         [8.4107e-07, 1.0000e+00],\n",
      "         [9.9993e-01, 7.2081e-05],\n",
      "         [7.9946e-05, 9.9992e-01],\n",
      "         [9.9996e-01, 3.6208e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.7832e-04, 9.9922e-01],\n",
      "         [9.9967e-01, 3.3324e-04],\n",
      "         [7.1604e-02, 9.2840e-01],\n",
      "         [7.3238e-01, 2.6762e-01],\n",
      "         [3.0126e-01, 6.9874e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.9480e-05, 9.9992e-01],\n",
      "         [9.9998e-01, 2.3237e-05],\n",
      "         [4.2732e-05, 9.9996e-01],\n",
      "         [9.9999e-01, 1.1135e-05],\n",
      "         [7.6466e-06, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.0819e-06, 1.0000e+00],\n",
      "         [9.9996e-01, 3.8477e-05],\n",
      "         [7.3234e-05, 9.9993e-01],\n",
      "         [9.9968e-01, 3.1586e-04],\n",
      "         [1.9697e-04, 9.9980e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.7767e-04, 9.9982e-01],\n",
      "         [9.9985e-01, 1.5009e-04],\n",
      "         [1.0610e-02, 9.8939e-01],\n",
      "         [9.0258e-01, 9.7418e-02],\n",
      "         [1.7762e-01, 8.2238e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 2.6671e-05],\n",
      "         [5.8076e-06, 9.9999e-01],\n",
      "         [9.9983e-01, 1.6897e-04],\n",
      "         [1.2440e-04, 9.9988e-01],\n",
      "         [8.9295e-01, 1.0705e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [2.6603e-04, 9.9973e-01],\n",
      "         [9.9994e-01, 6.4159e-05],\n",
      "         [5.6577e-06, 9.9999e-01],\n",
      "         [9.9999e-01, 7.3964e-06],\n",
      "         [2.4825e-05, 9.9998e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 1.8005e-05],\n",
      "         [1.2146e-04, 9.9988e-01],\n",
      "         [9.9998e-01, 2.0346e-05],\n",
      "         [4.7709e-05, 9.9995e-01],\n",
      "         [9.9995e-01, 4.9536e-05]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3000\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[2, 8, 4, 3, 5],\n",
      "        [7, 2, 8, 0, 0],\n",
      "        [7, 0, 2, 3, 1],\n",
      "        [4, 2, 0, 8, 7],\n",
      "        [5, 4, 0, 2, 6],\n",
      "        [4, 2, 6, 8, 7],\n",
      "        [6, 0, 0, 3, 1],\n",
      "        [4, 0, 7, 8, 1]])\n",
      "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.9227, -0.9321,  0.9415, -0.9510,  0.9606, -0.9703],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.4043],\n",
      "         [-0.3887],\n",
      "         [ 0.0706],\n",
      "         [-0.0241],\n",
      "         [ 0.2107],\n",
      "         [-0.0730]],\n",
      "\n",
      "        [[ 0.3027],\n",
      "         [-0.5495],\n",
      "         [ 0.2459],\n",
      "         [-0.0975],\n",
      "         [ 0.2015],\n",
      "         [-0.0190]],\n",
      "\n",
      "        [[ 0.1462],\n",
      "         [ 0.3054],\n",
      "         [-0.0249],\n",
      "         [-0.0337],\n",
      "         [-0.0380],\n",
      "         [-0.0436]],\n",
      "\n",
      "        [[-0.1154],\n",
      "         [-0.1492],\n",
      "         [-0.0651],\n",
      "         [-0.0301],\n",
      "         [-0.2279],\n",
      "         [ 0.0134]],\n",
      "\n",
      "        [[ 0.0904],\n",
      "         [-0.0383],\n",
      "         [-0.1541],\n",
      "         [-0.1921],\n",
      "         [ 0.1110],\n",
      "         [-0.1359]],\n",
      "\n",
      "        [[ 0.0904],\n",
      "         [-0.3126],\n",
      "         [ 0.1127],\n",
      "         [-0.1036],\n",
      "         [ 0.0456],\n",
      "         [-0.1796]],\n",
      "\n",
      "        [[-0.2785],\n",
      "         [ 0.2525],\n",
      "         [-0.0692],\n",
      "         [-0.0398],\n",
      "         [-0.0458],\n",
      "         [-0.0377]],\n",
      "\n",
      "        [[ 0.1437],\n",
      "         [-0.2509],\n",
      "         [-0.0216],\n",
      "         [-0.5621],\n",
      "         [ 0.5822],\n",
      "         [ 0.0173]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "predicted rewards tensor([[[ 0.0000e+00],\n",
      "         [ 3.9251e-02],\n",
      "         [-1.2208e-01],\n",
      "         [ 3.0346e-01],\n",
      "         [ 1.9427e-01],\n",
      "         [ 5.2900e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.0164e-01],\n",
      "         [ 3.6304e-02],\n",
      "         [ 1.6354e-01],\n",
      "         [ 1.0699e-01],\n",
      "         [-4.2178e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 4.9979e-01],\n",
      "         [-2.0549e-02],\n",
      "         [ 1.7970e-02],\n",
      "         [-1.8821e-02],\n",
      "         [-2.9921e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-2.6833e-02],\n",
      "         [ 5.4702e-02],\n",
      "         [-5.4934e-02],\n",
      "         [ 7.1633e-02],\n",
      "         [ 2.0083e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 3.4412e-02],\n",
      "         [ 4.2935e-02],\n",
      "         [-1.9495e-03],\n",
      "         [ 9.1542e-03],\n",
      "         [-7.4693e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 1.0327e-02],\n",
      "         [ 4.1364e-02],\n",
      "         [ 5.3471e-02],\n",
      "         [-2.7454e-03],\n",
      "         [-3.1242e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [-2.9234e-02],\n",
      "         [ 3.5838e-01],\n",
      "         [-7.9424e-03],\n",
      "         [-9.7860e-03],\n",
      "         [-3.4843e-02]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 4.9076e-02],\n",
      "         [-2.3668e-02],\n",
      "         [-3.9892e-04],\n",
      "         [ 2.1146e-03],\n",
      "         [ 6.2079e-01]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [1.2352e-05, 9.9999e-01],\n",
      "         [9.9993e-01, 7.1347e-05],\n",
      "         [2.9550e-04, 9.9970e-01],\n",
      "         [9.9990e-01, 1.0135e-04],\n",
      "         [1.1824e-04, 9.9988e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.2389e-04, 9.9988e-01],\n",
      "         [9.9998e-01, 2.3566e-05],\n",
      "         [2.4666e-04, 9.9975e-01],\n",
      "         [9.9998e-01, 1.8173e-05],\n",
      "         [6.4713e-03, 9.9353e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9998e-01, 2.2394e-05],\n",
      "         [6.2972e-03, 9.9370e-01],\n",
      "         [9.8585e-01, 1.4150e-02],\n",
      "         [5.0483e-01, 4.9517e-01],\n",
      "         [9.6621e-01, 3.3786e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9983e-01, 1.7429e-04],\n",
      "         [3.4151e-04, 9.9966e-01],\n",
      "         [9.9993e-01, 7.3827e-05],\n",
      "         [2.6528e-06, 1.0000e+00],\n",
      "         [9.9994e-01, 6.1834e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.9002e-06, 1.0000e+00],\n",
      "         [9.9995e-01, 4.5897e-05],\n",
      "         [7.5750e-06, 9.9999e-01],\n",
      "         [9.9993e-01, 7.4685e-05],\n",
      "         [6.5745e-06, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [4.1213e-05, 9.9996e-01],\n",
      "         [9.9999e-01, 9.1847e-06],\n",
      "         [4.5936e-04, 9.9954e-01],\n",
      "         [9.9994e-01, 5.8115e-05],\n",
      "         [1.3155e-04, 9.9987e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 2.6159e-05],\n",
      "         [2.0604e-05, 9.9998e-01],\n",
      "         [9.9525e-01, 4.7523e-03],\n",
      "         [1.4324e-01, 8.5676e-01],\n",
      "         [9.9228e-01, 7.7207e-03]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [3.4897e-04, 9.9965e-01],\n",
      "         [9.9996e-01, 3.7374e-05],\n",
      "         [1.9913e-05, 9.9998e-01],\n",
      "         [9.9999e-01, 6.2908e-06],\n",
      "         [6.3941e-06, 9.9999e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.5200, 0.0400, 0.0800, 0.0400, 0.0800]), tensor([0.0800, 0.0400, 0.0800, 0.0400, 0.5200, 0.0400, 0.0800, 0.0400, 0.0800]), 0.15850763155968092, tensor(4))\n",
      "action: 4\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.2400, 0.0400, 0.2800, 0.0000, 0.0000, 0.0400, 0.2800, 0.1200, 0.0000]), tensor([0.2400, 0.0400, 0.2800, 0.0000, 0.0000, 0.0400, 0.2800, 0.1200, 0.0000]), 0.22598985096194957, tensor(2))\n",
      "action: 2\n",
      "Player 1 random action: 7\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.2800, 0.0800, 0.0000, 0.0400, 0.0000, 0.0800, 0.5200, 0.0000, 0.0000]), tensor([0.2800, 0.0800, 0.0000, 0.0400, 0.0000, 0.0800, 0.5200, 0.0000, 0.0000]), 0.3059625869076788, tensor(6))\n",
      "action: 6\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "3100\n",
      "actions shape torch.Size([8, 5])\n",
      "target value shape torch.Size([8, 6])\n",
      "predicted values shape torch.Size([8, 6, 1])\n",
      "target rewards shape torch.Size([8, 6])\n",
      "predicted rewards shape torch.Size([8, 6, 1])\n",
      "target to plays shape torch.Size([8, 6, 2])\n",
      "predicted to_plays shape torch.Size([8, 6, 2])\n",
      "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
      "actions tensor([[0, 2, 6, 0, 7],\n",
      "        [5, 0, 2, 1, 7],\n",
      "        [2, 6, 0, 1, 8],\n",
      "        [5, 7, 3, 0, 7],\n",
      "        [8, 1, 7, 5, 0],\n",
      "        [0, 8, 2, 4, 6],\n",
      "        [5, 4, 3, 0, 6],\n",
      "        [3, 0, 8, 2, 5]])\n",
      "target value tensor([[ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
      "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "predicted values tensor([[[ 0.2037],\n",
      "         [-0.1403],\n",
      "         [ 0.2512],\n",
      "         [ 0.1779],\n",
      "         [ 0.0084],\n",
      "         [ 0.0184]],\n",
      "\n",
      "        [[ 0.0530],\n",
      "         [-0.0017],\n",
      "         [ 0.0060],\n",
      "         [ 0.0265],\n",
      "         [ 0.0119],\n",
      "         [ 0.0191]],\n",
      "\n",
      "        [[-0.2790],\n",
      "         [ 0.2506],\n",
      "         [-0.0575],\n",
      "         [ 0.0483],\n",
      "         [ 0.1320],\n",
      "         [ 0.1287]],\n",
      "\n",
      "        [[ 0.2718],\n",
      "         [-0.0690],\n",
      "         [ 0.2979],\n",
      "         [ 0.1232],\n",
      "         [-0.0140],\n",
      "         [ 0.0271]],\n",
      "\n",
      "        [[-0.1844],\n",
      "         [ 0.2810],\n",
      "         [ 0.0705],\n",
      "         [ 0.2747],\n",
      "         [ 0.0405],\n",
      "         [ 0.0013]],\n",
      "\n",
      "        [[ 0.1343],\n",
      "         [-0.1312],\n",
      "         [ 0.1103],\n",
      "         [-0.0859],\n",
      "         [ 0.1850],\n",
      "         [ 0.1364]],\n",
      "\n",
      "        [[ 0.2282],\n",
      "         [ 0.0262],\n",
      "         [-0.0008],\n",
      "         [ 0.2341],\n",
      "         [ 0.0329],\n",
      "         [ 0.1642]],\n",
      "\n",
      "        [[-0.1019],\n",
      "         [ 0.1682],\n",
      "         [-0.3345],\n",
      "         [ 0.3679],\n",
      "         [-0.0975],\n",
      "         [ 0.2170]]], grad_fn=<PermuteBackward0>)\n",
      "target rewards tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "predicted rewards tensor([[[ 0.0000],\n",
      "         [ 0.0955],\n",
      "         [ 0.1131],\n",
      "         [ 0.5937],\n",
      "         [ 0.0545],\n",
      "         [ 0.0436]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.2201],\n",
      "         [ 0.0068],\n",
      "         [ 0.0115],\n",
      "         [ 0.0276],\n",
      "         [ 0.0359]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1072],\n",
      "         [ 0.0502],\n",
      "         [ 0.0298],\n",
      "         [ 0.0113],\n",
      "         [ 0.1146]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.1198],\n",
      "         [ 0.4948],\n",
      "         [ 0.6383],\n",
      "         [ 0.0102],\n",
      "         [ 0.0561]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0581],\n",
      "         [ 0.3715],\n",
      "         [ 0.3427],\n",
      "         [ 0.5004],\n",
      "         [ 0.0298]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0826],\n",
      "         [-0.0239],\n",
      "         [ 0.0494],\n",
      "         [ 0.0991],\n",
      "         [ 0.3182]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [-0.0110],\n",
      "         [ 0.0812],\n",
      "         [ 0.2156],\n",
      "         [ 0.3872],\n",
      "         [ 0.3633]],\n",
      "\n",
      "        [[ 0.0000],\n",
      "         [ 0.0414],\n",
      "         [ 0.0620],\n",
      "         [ 0.0945],\n",
      "         [ 0.5307],\n",
      "         [ 0.4691]]], grad_fn=<PermuteBackward0>)\n",
      "target to plays tensor([[[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]])\n",
      "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
      "         [3.3740e-05, 9.9997e-01],\n",
      "         [9.9978e-01, 2.1898e-04],\n",
      "         [4.3649e-05, 9.9996e-01],\n",
      "         [9.5415e-01, 4.5851e-02],\n",
      "         [2.9375e-01, 7.0625e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [8.8880e-05, 9.9991e-01],\n",
      "         [9.2601e-01, 7.3987e-02],\n",
      "         [1.5382e-01, 8.4618e-01],\n",
      "         [6.7811e-01, 3.2189e-01],\n",
      "         [5.3159e-01, 4.6841e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9997e-01, 3.3244e-05],\n",
      "         [5.7264e-04, 9.9943e-01],\n",
      "         [9.9992e-01, 8.2970e-05],\n",
      "         [5.6337e-05, 9.9994e-01],\n",
      "         [9.9992e-01, 7.9189e-05]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.1006e-05, 9.9999e-01],\n",
      "         [9.9988e-01, 1.1563e-04],\n",
      "         [2.7573e-05, 9.9997e-01],\n",
      "         [9.9487e-01, 5.1315e-03],\n",
      "         [1.1300e-01, 8.8700e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [9.9960e-01, 4.0207e-04],\n",
      "         [3.7133e-05, 9.9996e-01],\n",
      "         [9.9986e-01, 1.4463e-04],\n",
      "         [1.3708e-04, 9.9986e-01],\n",
      "         [9.0187e-01, 9.8130e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [7.2028e-06, 9.9999e-01],\n",
      "         [1.0000e+00, 4.3836e-06],\n",
      "         [2.5691e-04, 9.9974e-01],\n",
      "         [9.9981e-01, 1.8616e-04],\n",
      "         [1.2562e-05, 9.9999e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [8.4904e-05, 9.9992e-01],\n",
      "         [9.9999e-01, 8.3986e-06],\n",
      "         [1.4409e-04, 9.9986e-01],\n",
      "         [9.9965e-01, 3.4961e-04],\n",
      "         [5.0399e-05, 9.9995e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [8.8269e-05, 9.9991e-01],\n",
      "         [9.9990e-01, 1.0185e-04],\n",
      "         [9.4405e-05, 9.9991e-01],\n",
      "         [9.9726e-01, 2.7408e-03],\n",
      "         [3.7892e-05, 9.9996e-01]]], grad_fn=<PermuteBackward0>)\n",
      "masks tensor([[ True,  True,  True, False, False, False],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 96.0 and average score: 0.96\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.1200, 0.0000, 0.1600, 0.0400, 0.3600, 0.0400, 0.1200, 0.0400, 0.1200]), tensor([0.1200, 0.0000, 0.1600, 0.0400, 0.3600, 0.0400, 0.1200, 0.0400, 0.1200]), -0.05629817862032936, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 0\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.5600, 0.0800, 0.0000, 0.0800, 0.1200, 0.0800, 0.0800]), tensor([0.0000, 0.0000, 0.5600, 0.0800, 0.0000, 0.0800, 0.1200, 0.0800, 0.0800]), -0.03195888177541213, tensor(2))\n",
      "action: 2\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.5600, 0.0800, 0.1600]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.5600, 0.0800, 0.1600]), 0.11212732627836856, tensor(6))\n",
      "action: 6\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.world_models.muzero_world_model import MuzeroWorldModel\n",
    "from losses.basic_losses import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 10,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"stochastic\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": False,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "    \"discount_factor\": 1.0,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"wm_unroll_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa549b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from muzero.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"dynamics_residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.5,\n",
    "    # \"min_replay_buffer_size\": 1000,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"stochastic_fixed_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5693ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": True,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"efficient_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8494974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 2,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"consistency_loss_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf70d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"reanalyze_noise\": True,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"injection_frac\": 0.25,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"unplugged_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_16_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_8_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "    # \"norm_type\": \"none\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"modular_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -3, -1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-8, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 4, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 3 + 1e-8, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\n",
    "            \"min_replay_buffer_size\", np.log(5000), np.log(5000) + 1e-8, 1000\n",
    "        )\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 2 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 2, 3, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 7, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIGHTLY WIDER IMPROVED SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-10, 2)),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 2, 5, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 2, 8, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL STANDARD SPACE (no picking num filters etc), should be compatible with initial\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8.01, 8.02, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0.91, 0.92, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=200,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
