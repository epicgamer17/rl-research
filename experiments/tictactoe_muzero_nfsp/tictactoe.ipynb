{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a659894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"chance_dense_layer_widths\": [],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"stochastic\": True,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"reanalyze_ratio\": 0.1,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": True,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"everything_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa549b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 30000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using default learning_rate                 : 0.001\n",
      "Using default clipnorm                      : 0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default loss_function                 : <class 'modules.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 8\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using default min_replay_buffer_size        : 8\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : [(24, 3, 1)]\n",
      "Using default conv_layers                   : []\n",
      "Using default dense_layer_widths            : []\n",
      "Using default representation_residual_layers: [(24, 3, 1)]\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: []\n",
      "Using         dynamics_residual_layers      : [(24, 3, 1)]\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : []\n",
      "Using         reward_conv_layers            : [(16, 1, 1)]\n",
      "Using         reward_dense_layer_widths     : []\n",
      "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
      "Using         to_play_dense_layer_widths    : []\n",
      "Using         critic_conv_layers            : [(16, 1, 1)]\n",
      "Using         critic_dense_layer_widths     : []\n",
      "Using         actor_conv_layers             : [(16, 1, 1)]\n",
      "Using         actor_dense_layer_widths      : []\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using default to_play_loss_factor           : 1.0\n",
      "Using default weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.25\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 50\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using default temperature_updates           : [5]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default value_loss_function           : <modules.utils.MSELoss object at 0x3203aed10>\n",
      "Using default reward_loss_function          : <modules.utils.MSELoss object at 0x3203aed40>\n",
      "Using         policy_loss_function          : <modules.utils.CategoricalCrossentropyLoss object at 0x103319780>\n",
      "Using default to_play_loss_function         : <modules.utils.CategoricalCrossentropyLoss object at 0x3203aece0>\n",
      "Using         n_step                        : 9\n",
      "Using default discount_factor               : 1.0\n",
      "Using default unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using default num_workers                   : 4\n",
      "Using default lr_ratio                      : inf\n",
      "Using         transfer_interval             : 1\n",
      "Using default reanalyze_ratio               : 0.0\n",
      "Using default reanalyze_method              : mcts\n",
      "Using default reanalyze_tau                 : 0.3\n",
      "Using default injection_frac                : 0.0\n",
      "Using default reanalyze_noise               : False\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         gumbel                        : False\n",
      "Using         gumbel_m                      : 16\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using default consistency_loss_factor       : 0.0\n",
      "Using default projector_output_dim          : 128\n",
      "Using default projector_hidden_dim          : 128\n",
      "Using default predictor_output_dim          : 128\n",
      "Using default predictor_hidden_dim          : 64\n",
      "Using default mask_absorbing                : True\n",
      "Using default value_prefix                  : False\n",
      "Using default lstm_horizon_len              : 5\n",
      "Using default lstm_hidden_size              : 64\n",
      "Using default q_estimation_method           : v_mix\n",
      "Using         stochastic                    : True\n",
      "Using default num_chance                    : 32\n",
      "Using default sigma_loss                    : <modules.utils.CategoricalCrossentropyLoss object at 0x3203aec80>\n",
      "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
      "Using default afterstate_conv_layers        : []\n",
      "Using default afterstate_dense_layer_widths : []\n",
      "Using default chance_conv_layers            : [(32, 3, 1)]\n",
      "Using default chance_dense_layer_widths     : [256]\n",
      "Using         vqvae_commitment_cost_factor  : 0.5\n",
      "Using device: cpu\n",
      "making test env\n",
      "Test env with record video\n",
      "env render mode rgb_array\n",
      "petting zoo env\n",
      "Test env: RecordVideo<ChannelLastToFirstWrapper<TwoPlayerPlayerPlaneWrapper<FrameStackWrapper<ActionMaskInInfoWrapper<tictactoe_v3>>>>>\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (9, 3, 3)\n",
      "Observation dtype: int8\n",
      "num_actions:  9 <class 'int'>\n",
      "Test agents: [<agents.random.RandomAgent object at 0x103318e20>, <agents.tictactoe_expert.TicTacToeBestAgent object at 0x3203aebf0>]\n",
      "9\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "24\n",
      "24\n",
      "dynamics input shape (8, 24, 3, 3)\n",
      "24\n",
      "24\n",
      "9\n",
      "Hidden state shape: (8, 24, 3, 3)\n",
      "24\n",
      "24\n",
      "dynamics input shape (8, 24, 3, 3)\n",
      "24\n",
      "24\n",
      "Warning: for board games it is recommnded to have n_step >= game length\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'cons_loss' with subkeys None\n",
      "Initializing stat 'q_loss' with subkeys None\n",
      "Initializing stat 'sigma_loss' with subkeys None\n",
      "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'num_codes' with subkeys None\n",
      "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 1] Starting self-play...\n",
      "[Worker 2] Starting self-play...\n",
      "[Worker 3] Starting self-play...\n",
      "Buffer size: 6\n",
      "Buffer size: 12\n",
      "Buffer size: 20\n",
      "learned\n",
      "Buffer size: 29\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 36\n",
      "learned\n",
      "Buffer size: 43\n",
      "learned\n",
      "learned\n",
      "Buffer size: 52\n",
      "learned\n",
      "Buffer size: 61\n",
      "learned\n",
      "learned\n",
      "Buffer size: 70\n",
      "learned\n",
      "Buffer size: 79\n",
      "learned\n",
      "Buffer size: 86\n",
      "learned\n",
      "Buffer size: 93\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 99\n",
      "learned\n",
      "Buffer size: 107\n",
      "Buffer size: 113\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 121\n",
      "learned\n",
      "learned\n",
      "Buffer size: 129\n",
      "learned\n",
      "Buffer size: 137\n",
      "learned\n",
      "Buffer size: 146\n",
      "learned\n",
      "learned\n",
      "Buffer size: 151\n",
      "learned\n",
      "Buffer size: 160\n",
      "learned\n",
      "learned\n",
      "Buffer size: 167\n",
      "Buffer size: 175\n",
      "learned\n",
      "learned\n",
      "Buffer size: 182\n",
      "learned\n",
      "learned\n",
      "Buffer size: 190\n",
      "learned\n",
      "learned\n",
      "Buffer size: 195\n",
      "learned\n",
      "Buffer size: 203\n",
      "Buffer size: 212\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 217\n",
      "learned\n",
      "Buffer size: 226\n",
      "learned\n",
      "learned\n",
      "Buffer size: 235\n",
      "learned\n",
      "Buffer size: 244\n",
      "learned\n",
      "learned\n",
      "Buffer size: 251\n",
      "learned\n",
      "Buffer size: 256\n",
      "learned\n",
      "Buffer size: 265\n",
      "learned\n",
      "Buffer size: 272\n",
      "learned\n",
      "learned\n",
      "Buffer size: 279\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 288\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 297\n",
      "Buffer size: 303\n",
      "Buffer size: 312\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 320\n",
      "learned\n",
      "learned\n",
      "Buffer size: 327\n",
      "learned\n",
      "Buffer size: 335\n",
      "Buffer size: 344\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 353\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 362\n",
      "learned\n",
      "Buffer size: 370\n",
      "Buffer size: 379\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 386\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 393\n",
      "learned\n",
      "Buffer size: 402\n",
      "Buffer size: 410\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 418\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 426\n",
      "learned\n",
      "Buffer size: 434\n",
      "Buffer size: 443\n",
      "learned\n",
      "learned\n",
      "Buffer size: 450\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 459\n",
      "Buffer size: 465\n",
      "Buffer size: 474\n",
      "learned\n",
      "learned\n",
      "Buffer size: 483\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Buffer size: 490\n",
      "Buffer size: 498\n",
      "Buffer size: 505\n",
      "Buffer size: 514\n",
      "Buffer size: 521\n",
      "Buffer size: 529\n",
      "learned\n",
      "Buffer size: 538\n",
      "learned\n",
      "Buffer size: 546\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 554\n",
      "Buffer size: 563\n",
      "learned\n",
      "learned\n",
      "Buffer size: 572\n",
      "Buffer size: 581\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 588\n",
      "Buffer size: 595\n",
      "learned\n",
      "Buffer size: 601\n",
      "learned\n",
      "learned\n",
      "Buffer size: 609\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 616\n",
      "learned\n",
      "learned\n",
      "Buffer size: 624\n",
      "Buffer size: 633\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 642\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 651\n",
      "Buffer size: 659\n",
      "learned\n",
      "Buffer size: 668\n",
      "learned\n",
      "Buffer size: 675\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 680\n",
      "learned\n",
      "Buffer size: 686\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 693\n",
      "learned\n",
      "Buffer size: 702\n",
      "learned\n",
      "learned\n",
      "Buffer size: 710\n",
      "learned\n",
      "learned\n",
      "Buffer size: 718\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 727\n",
      "Buffer size: 736\n",
      "learned\n",
      "Buffer size: 742\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 751\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 759\n",
      "learned\n",
      "Buffer size: 768\n",
      "Buffer size: 777\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 786\n",
      "learned\n",
      "Buffer size: 793\n",
      "learned\n",
      "learned\n",
      "Buffer size: 802\n",
      "learned\n",
      "Buffer size: 810\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 819\n",
      "learned\n",
      "Buffer size: 825\n",
      "learned\n",
      "learned\n",
      "Buffer size: 834\n",
      "learned\n",
      "learned\n",
      "Buffer size: 843\n",
      "learned\n",
      "Buffer size: 848\n",
      "learned\n",
      "learned\n",
      "Buffer size: 856\n",
      "learned\n",
      "learned\n",
      "Buffer size: 864\n",
      "learned\n",
      "learned\n",
      "Buffer size: 871\n",
      "Buffer size: 878\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 887\n",
      "learned\n",
      "learned\n",
      "Buffer size: 896\n",
      "learned\n",
      "Buffer size: 905\n",
      "learned\n",
      "Buffer size: 914\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Buffer size: 921\n",
      "Buffer size: 930\n",
      "Buffer size: 935\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Buffer size: 944\n",
      "Buffer size: 953\n",
      "Buffer size: 962\n",
      "Buffer size: 971\n",
      "Buffer size: 980\n",
      "Buffer size: 985\n",
      "Buffer size: 992\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0600, 0.1400, 0.1800, 0.0600, 0.0000, 0.1200, 0.1200, 0.1400, 0.1800]), tensor([0.0600, 0.1400, 0.1800, 0.0600, 0.0000, 0.1200, 0.1200, 0.1400, 0.1800]), 0.050733608358046585, tensor(2))\n",
      "action: 2\n",
      "Player 1 random action: 1\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.2200, 0.0000, 0.0000, 0.1400, 0.1200, 0.1600, 0.1000, 0.0600, 0.2000]), tensor([0.2200, 0.0000, 0.0000, 0.1400, 0.1200, 0.1600, 0.1000, 0.0600, 0.2000]), 0.19037731167148142, tensor(0))\n",
      "action: 0\n",
      "Player 1 random action: 4\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.1400, 0.0000, 0.1400, 0.4000, 0.2000, 0.1200]), tensor([0.0000, 0.0000, 0.0000, 0.1400, 0.0000, 0.1400, 0.4000, 0.2000, 0.1200]), 0.19814658048106173, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 5\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000, 0.4400, 0.4400]), tensor([0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000, 0.4400, 0.4400]), 0.18039836588443495, tensor(7))\n",
      "action: 7\n",
      "Player 1 random action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), 0.3700507941345374, tensor(3))\n",
      "action: 3\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs random: 60.0 and average score: 0.36\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 4\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2600, 0.0400, 0.2800, 0.0200, 0.0000, 0.3200, 0.0200, 0.0200, 0.0400]), tensor([0.2600, 0.0400, 0.2800, 0.0200, 0.0000, 0.3200, 0.0200, 0.0200, 0.0400]), 0.004959554794956656, tensor(5))\n",
      "action: 5\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3200, 0.0200, 0.3400, 0.0000, 0.0000, 0.0000, 0.0400, 0.2400, 0.0400]), tensor([0.3200, 0.0200, 0.3400, 0.0000, 0.0000, 0.0000, 0.0400, 0.2400, 0.0400]), -0.1334750189938966, tensor(2))\n",
      "action: 2\n",
      "Player 0 random action: 8\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.2000, 0.3600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0800, 0.3600, 0.0000]), tensor([0.2000, 0.3600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0800, 0.3600, 0.0000]), -0.14430324539688288, tensor(1))\n",
      "action: 1\n",
      "Player 0 random action: 6\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.1800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8200, 0.0000]), tensor([0.1800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8200, 0.0000]), 0.23068391911539377, tensor(7))\n",
      "action: 7\n",
      "Player 0 random action: 0\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 30.0 and average score: -0.24\n",
      "Results vs random: {'player_0_score': 0.36, 'player_0_win%': 0.6, 'player_1_score': -0.24, 'player_1_win%': 0.3, 'score': 0.06}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.1200, 0.0800, 0.1000, 0.0600, 0.1000, 0.1000, 0.2000, 0.1600]), tensor([0.0800, 0.1200, 0.0800, 0.1000, 0.0600, 0.1000, 0.1000, 0.2000, 0.1600]), 0.18903364767046535, tensor(7))\n",
      "action: 7\n",
      "Player 1 tictactoe_expert action: 1\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1400, 0.0000, 0.1800, 0.1400, 0.1400, 0.1000, 0.1800, 0.0000, 0.1200]), tensor([0.1400, 0.0000, 0.1800, 0.1400, 0.1400, 0.1000, 0.1800, 0.0000, 0.1200]), 0.2547403330019876, tensor(2))\n",
      "action: 2\n",
      "Player 1 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.2800, 0.0000, 0.0000, 0.1200, 0.1800, 0.1400, 0.2800, 0.0000, 0.0000]), tensor([0.2800, 0.0000, 0.0000, 0.1200, 0.1800, 0.1400, 0.2800, 0.0000, 0.0000]), 0.24841757878368975, tensor(0))\n",
      "action: 0\n",
      "Player 1 tictactoe_expert action: 6\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.1600, 0.6200, 0.2200, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.1600, 0.6200, 0.2200, 0.0000, 0.0000, 0.0000]), 0.2777291537792075, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.]), 0.39864687826119216, tensor(5))\n",
      "action: 5\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 8.0 and average score: -0.6\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 5\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0400, 0.0200, 0.0400, 0.3400, 0.1400, 0.0000, 0.2600, 0.0400, 0.1200]), tensor([0.0400, 0.0200, 0.0400, 0.3400, 0.1400, 0.0000, 0.2600, 0.0400, 0.1200]), -0.05035598477458253, tensor(3))\n",
      "action: 3\n",
      "Player 0 tictactoe_expert action: 1\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.1200, 0.0000, 0.2600, 0.0000, 0.1600, 0.0000, 0.1800, 0.1400, 0.1400]), tensor([0.1200, 0.0000, 0.2600, 0.0000, 0.1600, 0.0000, 0.1800, 0.1400, 0.1400]), -0.012632391105095545, tensor(2))\n",
      "action: 2\n",
      "Player 0 tictactoe_expert action: 6\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.3200, 0.0000, 0.0000, 0.0000, 0.1800, 0.0000, 0.0000, 0.1600, 0.3400]), tensor([0.3200, 0.0000, 0.0000, 0.0000, 0.1800, 0.0000, 0.0000, 0.1600, 0.3400]), 0.01995889017102765, tensor(8))\n",
      "action: 8\n",
      "Player 0 tictactoe_expert action: 4\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.6400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600, 0.0000]), tensor([0.6400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600, 0.0000]), 0.04032944084382525, tensor(0))\n",
      "action: 0\n",
      "Player 0 tictactoe_expert action: 7\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "Player 1 win percentage vs tictactoe_expert: 0.0 and average score: -0.86\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.6, 'player_0_win%': 0.08, 'player_1_score': -0.86, 'player_1_win%': 0.0, 'score': -0.73}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0600, 0.1800, 0.0800, 0.1000, 0.0600, 0.1800, 0.1000, 0.0600, 0.1800]), tensor([0.0600, 0.1800, 0.0800, 0.1000, 0.0600, 0.1800, 0.1000, 0.0600, 0.1800]), 0.1636958702777823, tensor(1))\n",
      "action: 1\n",
      "Player 1 random action: 3\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.0000, 0.3200, 0.0000, 0.1000, 0.0800, 0.1800, 0.0800, 0.0800]), tensor([0.1600, 0.0000, 0.3200, 0.0000, 0.1000, 0.0800, 0.1800, 0.0800, 0.0800]), 0.22516854901743286, tensor(2))\n",
      "action: 2\n",
      "Player 1 random action: 5\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1600, 0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.4200, 0.1400, 0.1600]), tensor([0.1600, 0.0000, 0.0000, 0.0000, 0.1200, 0.0000, 0.4200, 0.1400, 0.1600]), 0.18489962772411458, tensor(6))\n",
      "action: 6\n",
      "Player 1 random action: 7\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.4800, 0.0000, 0.0000, 0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.1600]), tensor([0.4800, 0.0000, 0.0000, 0.0000, 0.3600, 0.0000, 0.0000, 0.0000, 0.1600]), 0.25681365321518157, tensor(0))\n",
      "action: 0\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "Player 0 win percentage vs random: 62.0 and average score: 0.38\n",
      "Testing Player 1 vs Agent random\n",
      "Player 0 random action: 8\n",
      "Player 1 prediction: (tensor([0.2000, 0.0200, 0.0200, 0.0200, 0.2000, 0.2400, 0.0400, 0.2600, 0.0000]), tensor([0.2000, 0.0200, 0.0200, 0.0200, 0.2000, 0.2400, 0.0400, 0.2600, 0.0000]), 0.002916655338862363, tensor(7))\n",
      "action: 7\n",
      "Player 0 random action: 5\n",
      "Player 1 prediction: (tensor([0.2400, 0.0400, 0.0400, 0.2200, 0.2400, 0.0000, 0.2200, 0.0000, 0.0000]), tensor([0.2400, 0.0400, 0.0400, 0.2200, 0.2400, 0.0000, 0.2200, 0.0000, 0.0000]), -0.11487355404624752, tensor(0))\n",
      "action: 0\n",
      "Player 0 random action: 6\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.1200, 0.2000, 0.2400, 0.4400, 0.0000, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.1200, 0.2000, 0.2400, 0.4400, 0.0000, 0.0000, 0.0000, 0.0000]), -0.1178564053832316, tensor(4))\n",
      "action: 4\n",
      "Player 0 random action: 1\n",
      "learned\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.7200, 0.2800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), tensor([0.0000, 0.0000, 0.7200, 0.2800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), -0.052181595918156354, tensor(2))\n",
      "action: 2\n",
      "Player 0 random action: 3\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs random: 32.0 and average score: -0.16\n",
      "Results vs random: {'player_0_score': 0.38, 'player_0_win%': 0.62, 'player_1_score': -0.16, 'player_1_win%': 0.32, 'score': 0.11}\n",
      "Testing Player 0 vs Agent tictactoe_expert\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0600, 0.1600, 0.1400, 0.1600, 0.0600, 0.1400, 0.1000, 0.0800, 0.1000]), tensor([0.0600, 0.1600, 0.1400, 0.1600, 0.0600, 0.1400, 0.1000, 0.0800, 0.1000]), 0.10108810608439586, tensor(1))\n",
      "action: 1\n",
      "Player 1 tictactoe_expert action: 8\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.2600, 0.0000, 0.0800, 0.2200, 0.1400, 0.2000, 0.0400, 0.0600, 0.0000]), tensor([0.2600, 0.0000, 0.0800, 0.2200, 0.1400, 0.2000, 0.0400, 0.0600, 0.0000]), 0.016607456895358422, tensor(0))\n",
      "action: 0\n",
      "Player 1 tictactoe_expert action: 2\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0000, 0.0000, 0.0000, 0.0400, 0.3800, 0.3000, 0.1200, 0.1600, 0.0000]), tensor([0.0000, 0.0000, 0.0000, 0.0400, 0.3800, 0.3000, 0.1200, 0.1600, 0.0000]), 0.17217414621628968, tensor(4))\n",
      "action: 4\n",
      "Player 1 tictactoe_expert action: 5\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "average score: 0.15\n",
      "Test score {'score': 0.15, 'max_score': 1, 'min_score': -1}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 win percentage vs tictactoe_expert: 6.0 and average score: -0.72\n",
      "Testing Player 1 vs Agent tictactoe_expert\n",
      "Player 0 tictactoe_expert action: 4\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0400, 0.2600, 0.0400, 0.0200, 0.0000, 0.2200, 0.1800, 0.1800, 0.0600]), tensor([0.0400, 0.2600, 0.0400, 0.0200, 0.0000, 0.2200, 0.1800, 0.1800, 0.0600]), 0.059240780770778656, tensor(1))\n",
      "action: 1\n",
      "Player 0 tictactoe_expert action: 0\n",
      "learned\n",
      "Player 1 prediction: (tensor([0.0000, 0.0000, 0.3200, 0.2600, 0.0000, 0.0400, 0.1600, 0.1400, 0.0800]), tensor([0.0000, 0.0000, 0.3200, 0.2600, 0.0000, 0.0400, 0.1600, 0.1400, 0.0800]), -0.006823608030875524, tensor(2))\n",
      "action: 2\n",
      "Player 0 tictactoe_expert action: 8\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 1 win percentage vs tictactoe_expert: 2.0 and average score: -0.86\n",
      "Results vs tictactoe_expert: {'player_0_score': -0.72, 'player_0_win%': 0.06, 'player_1_score': -0.86, 'player_1_win%': 0.02, 'score': -0.79}\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Testing Player 0 vs Agent random\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0600, 0.1400, 0.0600, 0.0600, 0.1400, 0.1400, 0.1000, 0.2000, 0.1000]), tensor([0.0600, 0.1400, 0.0600, 0.0600, 0.1400, 0.1400, 0.1000, 0.2000, 0.1000]), 0.151542108040303, tensor(7))\n",
      "action: 7\n",
      "Player 1 random action: 6\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.1000, 0.3400, 0.1200, 0.0600, 0.2400, 0.0600, 0.0000, 0.0000, 0.0800]), tensor([0.1000, 0.3400, 0.1200, 0.0600, 0.2400, 0.0600, 0.0000, 0.0000, 0.0800]), -0.05076888879286308, tensor(1))\n",
      "action: 1\n",
      "Player 1 random action: 5\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.0800, 0.0000, 0.0800, 0.6600, 0.1400, 0.0000, 0.0000, 0.0000, 0.0400]), tensor([0.0800, 0.0000, 0.0800, 0.6600, 0.1400, 0.0000, 0.0000, 0.0000, 0.0400]), 0.1336060569110308, tensor(3))\n",
      "action: 3\n",
      "Player 1 random action: 4\n",
      "learned\n",
      "Player 0 prediction: (tensor([0.3200, 0.0000, 0.4800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000]), tensor([0.3200, 0.0000, 0.4800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000]), 0.16289711497066653, tensor(2))\n",
      "action: 2\n",
      "Player 1 random action: 0\n",
      "learned\n",
      "Player 0 prediction: (tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]), 0.4855912239747305, tensor(8))\n",
      "action: 8\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "learned\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting test_score_vs_random\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n",
      "plotting test_score_vs_tictactoe_expert\n",
      "  subkey score\n",
      "  subkey player_0_score\n",
      "  subkey player_1_score\n",
      "  subkey player_0_win%\n",
      "  subkey player_1_win%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Process Process-6:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 303, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1639, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1580, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 633, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 867, in _run_single_simulation\n",
      "    ) = self.predict_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1531, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 1633, in recurrent_inference\n",
      "    reward, hidden_state, to_play, reward_hidden = self.dynamics(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 470, in forward\n",
      "    S.view(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 303, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1639, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1580, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 633, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 908, in _run_single_simulation\n",
      "    self.predict_afterstate_recurrent_inference(  # <--- YOU NEED THIS METHOD\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1560, in predict_afterstate_recurrent_inference\n",
      "    afterstates, value, chance_probs = model.afterstate_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 1701, in afterstate_recurrent_inference\n",
      "    afterstate = self.afterstate_dynamics(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 882, in forward\n",
      "    flattened_chance_vector = self.chance_dense_layers(flattened_chance_vector)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1780, in _call_impl\n",
      "    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../base_agent/agent.py\", line 594, in run_tests\n",
      "    results = self.test_vs_agent(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../base_agent/agent.py\", line 520, in test_vs_agent\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1580, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 633, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 908, in _run_single_simulation\n",
      "    self.predict_afterstate_recurrent_inference(  # <--- YOU NEED THIS METHOD\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1560, in predict_afterstate_recurrent_inference\n",
      "    afterstates, value, chance_probs = model.afterstate_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 1701, in afterstate_recurrent_inference\n",
      "    afterstate = self.afterstate_dynamics(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 854, in forward\n",
      "    S.view(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 303, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1639, in play_game\n",
      "    prediction = self.predict(\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1580, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 633, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 908, in _run_single_simulation\n",
      "    self.predict_afterstate_recurrent_inference(  # <--- YOU NEED THIS METHOD\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1560, in predict_afterstate_recurrent_inference\n",
      "    afterstates, value, chance_probs = model.afterstate_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 1704, in afterstate_recurrent_inference\n",
      "    value, sigma = self.afterstate_prediction(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 993, in forward\n",
      "    x = self.residual_layers(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 151, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 303, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1639, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1580, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 633, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 908, in _run_single_simulation\n",
      "    self.predict_afterstate_recurrent_inference(  # <--- YOU NEED THIS METHOD\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1560, in predict_afterstate_recurrent_inference\n",
      "    afterstates, value, chance_probs = model.afterstate_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 1701, in afterstate_recurrent_inference\n",
      "    afterstate = self.afterstate_dynamics(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 834, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 151, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py\", line 2813, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../base_agent/agent.py\", line 607, in run_tests\n",
      "    super().run_tests(stats)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../base_agent/agent.py\", line 385, in run_tests\n",
      "    test_score = self.test(self.test_trials, dir=training_step_dir)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../base_agent/agent.py\", line 455, in test\n",
      "    prediction = self.predict(state, info, env=self.test_env.env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1580, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 633, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 867, in _run_single_simulation\n",
      "    ) = self.predict_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py\", line 1531, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 1633, in recurrent_inference\n",
      "    reward, hidden_state, to_play, reward_hidden = self.dynamics(\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_network.py\", line 439, in forward\n",
      "    S = self.residual_layers(S)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 63, in forward\n",
      "    x = self.activation(layer(x))\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 150, in forward\n",
      "    x = self.conv1(inputs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"dynamics_residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 30000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.5,\n",
    "    # \"min_replay_buffer_size\": 1000,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"learning_rate\": 0.002,\n",
    "    # \"clipnorm\": 10,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"stochastic_fixed_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.stats.plot_graphs(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import get_legal_moves\n",
    "\n",
    "env.reset()\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "v_pi_raw, policy, hidden_state = agent.predict_initial_inference(\n",
    "    state, model=agent.model\n",
    ")\n",
    "policy = policy[0]\n",
    "reward_h_state = torch.zeros(1, 1, agent.config.lstm_hidden_size).to(agent.device)\n",
    "reward_c_state = torch.zeros(1, 1, agent.config.lstm_hidden_size).to(agent.device)\n",
    "\n",
    "v_pi_scalar = float(v_pi_raw.item())\n",
    "\n",
    "legal_moves = get_legal_moves(info)[0]  # [0]\n",
    "print(v_pi_raw, policy, hidden_state)\n",
    "\n",
    "print(agent.predict_afterstate_recurrent_inference(hidden_state, torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5693ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "    \"value_prefix\": True,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"efficient_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8494974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=None)\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 2,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    # \"learning_rate\": 0.01,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"consistency_loss_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf70d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"reanalyze_noise\": True,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"injection_frac\": 0.25,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"unplugged_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_16_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.0,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"gumbel_m_8_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3352040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"q_estimation_method\": \"zero\",\n",
    "    \"value_loss_factor\": 0.25,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"q_estimation_zero_0.25_value_loss_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"q_estimation_method\": \"zero\",\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"q_estimation_zero_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_plane\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"n_step\": 9,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"residual_layers\": [(24, 3, 1)],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"stochastic_comparison_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "# from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "# import dill as pickle\n",
    "# from hyperopt import hp\n",
    "# from hyperopt.pyll import scope\n",
    "# from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# from muzero.action_functions import action_as_plane as action_function\n",
    "# from torch.optim import Adam, SGD\n",
    "\n",
    "# search_space = {\n",
    "#     \"kernel_initializer\": hp.choice(\n",
    "#         \"kernel_initializer\",\n",
    "#         [\n",
    "#             \"he_uniform\",\n",
    "#             \"he_normal\",\n",
    "#             \"glorot_uniform\",\n",
    "#             \"glorot_normal\",\n",
    "#             \"orthogonal\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"optimizer\": hp.choice(\n",
    "#         \"optimizer\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"optimizer\": \"adam\",\n",
    "#                 # \"adam_epsilon\": hp.qloguniform(\n",
    "#                 #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "#                 # ),\n",
    "#                 \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "#             },\n",
    "#             {\n",
    "#                 \"optimizer\": \"sgd\",\n",
    "#                 \"momentum\": hp.quniform(\"momentum\", 0, 1, 0.1),\n",
    "#             },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     # \"learning_rate\": hp.qloguniform(\n",
    "#     #     \"learning_rate\", np.log(0.0001), np.log(0.01), 0.0001\n",
    "#     # ),\n",
    "#     \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "#     \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "#     \"residual_filters\": scope.int(\n",
    "#         hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "#     ),\n",
    "#     \"residual_stacks\": scope.int(\n",
    "#         hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "#     ),\n",
    "#     \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "#     \"actor_and_critic_conv_filters\": scope.int(\n",
    "#         hp.qloguniform(\n",
    "#             \"actor_and_critic_conv_filters\", np.log(0 + 8), np.log(32 + 8), 8\n",
    "#         )\n",
    "#         - 8  # to make 0 an option\n",
    "#     ),\n",
    "#     \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "#     \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "#     \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "#     \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "#     \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "#     \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "#     \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "#     \"root_dirichlet_alpha\": hp.quniform(\n",
    "#         \"root_dirichlet_alpha\", 0.1, 2.0, 0.1\n",
    "#     ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "#     \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "#     \"num_simulations\": scope.int(\n",
    "#         hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "#     ),\n",
    "# \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 4, 1))],\n",
    "# \"temperatures\": hp.choice(\"temperatures\", [1.0, 0.1]),\n",
    "# \"temperature_with_training_steps\": hp.choice(\n",
    "#     \"temperature_with_training_steps\", False\n",
    "# ),\n",
    "#     \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "#     \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "#     \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "#     \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "#     \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "#     \"policy_loss_function\": hp.choice(\n",
    "#         \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "#     ),\n",
    "#     \"training_steps\": scope.int(\n",
    "#         hp.qloguniform(\"training_steps\", np.log(10000), np.log(30000), 10000)\n",
    "#     ),\n",
    "#     # \"minibatch_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "#     # ),\n",
    "#     # \"min_replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     # ),\n",
    "#     # \"replay_buffer_size\": scope.int(\n",
    "#     #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "#     # ),\n",
    "#     \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "#     \"min_replay_buffer_size\": scope.int(\n",
    "#         hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "#     ),\n",
    "#     \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "#     \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "#     \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "#     \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "#     \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "#     \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "#     \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "#     \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "#     \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "#     \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "#     \"multi_process\": hp.choice(\n",
    "#         \"multi_process\",\n",
    "#         [\n",
    "#             {\n",
    "#                 \"multi_process\": True,\n",
    "#                 \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "#             },\n",
    "#             # {\n",
    "#             #     \"multi_process\": False,\n",
    "#             #     \"games_per_generation\": scope.int(\n",
    "#             #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "#             #     ),\n",
    "#             # },\n",
    "#         ],\n",
    "#     ),\n",
    "#     \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "# }\n",
    "\n",
    "# initial_best_config = []\n",
    "\n",
    "# search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -3, -1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-8, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 4, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 3 + 1e-8, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\n",
    "            \"min_replay_buffer_size\", np.log(5000), np.log(5000) + 1e-8, 1000\n",
    "        )\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 2 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 2, 3, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 1, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 7, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    print(params[\"clipnorm\"])\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIGHTLY WIDER IMPROVED SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-10, 2)),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 2, 5, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "                \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 3, 1)),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SPACE\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 2, 8, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(8), np.log(32), 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(3), 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(0 + 8), np.log(32 + 8), 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL STANDARD SPACE (no picking num filters etc), should be compatible with initial\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_plane as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8.01, 8.02, 2)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0.91, 0.92, 0.1),\n",
    "                # \"momentum\": hp.choice(\n",
    "                #     \"momentum\", [0.0, 0.9]\n",
    "                # ),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_filters\": scope.int(\n",
    "        hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    ),\n",
    "    \"residual_stacks\": scope.int(\n",
    "        hp.qloguniform(\"residual_stacks\", np.log(1), np.log(1) + 1e-8, 1)\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"output_filters\": scope.int(\n",
    "        hp.qloguniform(\"output_filters\", np.log(16 + 8), np.log(16 + 8) + 1e-8, 8)\n",
    "        - 8  # to make 0 an option\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\"root_dirichlet_alpha\", 0.1, 2.0, 0.1),\n",
    "    # \"root_dirichlet_alpha\": 2\n",
    "    # ** (\n",
    "    #     hp.quniform(\"root_dirichlet_alpha\", -2, 2, 1.0)\n",
    "    # ),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(25) + 1e-10, 25)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 0, 8, 1))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(11000), np.log(33000), 11000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 6, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    # \"clipnorm\": hp.choice(\n",
    "    #     \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clipnorm\", 0, 2, 1)))]\n",
    "    # ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    assert params[\"output_filters\"] <= params[\"residual_filters\"]\n",
    "\n",
    "    params[\"residual_layers\"] = [(params[\"residual_filters\"], 3, 1)] * params[\n",
    "        \"residual_stacks\"\n",
    "    ]\n",
    "    del params[\"residual_filters\"]\n",
    "    del params[\"residual_stacks\"]\n",
    "    if params[\"output_filters\"] != 0:\n",
    "        params[\"actor_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"critic_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "        params[\"reward_conv_layers\"] = [(params[\"output_filters\"], 1, 1)]\n",
    "    else:\n",
    "        params[\"actor_conv_layers\"] = []\n",
    "        params[\"critic_conv_layers\"] = []\n",
    "    del params[\"output_filters\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.tictactoe_config import TicTacToeConfig\n",
    "import torch\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = TicTacToeConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=TicTacToeConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=200,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"elo\",\n",
    "        best_agent=TicTacToeBestAgent(),\n",
    "        make_env=tictactoe_v3.env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=TicTacToeConfig,\n",
    "        games_per_pair=100,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=250,\n",
    "        test_trials=25,\n",
    "        test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    try:\n",
    "        best = fmin(\n",
    "            fn=marl_objective,  # Objective Function to optimize\n",
    "            space=search_space,  # Hyperparameter's Search Space\n",
    "            algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "            max_evals=max_trials,  # Number of optimization attempts\n",
    "            trials=trials,  # Record the results\n",
    "            # early_stop_fn=no_progress_loss(5, 1),\n",
    "            trials_save_file=f\"./{file_name}_trials.p\",\n",
    "            points_to_evaluate=initial_best_config,\n",
    "            show_progressbar=False,\n",
    "        )\n",
    "    except AllTrialsFailed:\n",
    "        print(\"trial failed\")\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443809d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 10000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"conv_layers\": [(32, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.06,\n",
    "    \"eg_epsilon\": 0.0,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [(128, 3, 1)] * 3,\n",
    "    \"sl_conv_layers\": [(32, 3, 1)],\n",
    "    \"sl_dense_layer_widths\": [],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 3,\n",
    "    \"atom_size\": 51,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=TicTacToeConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe-Rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 100\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
