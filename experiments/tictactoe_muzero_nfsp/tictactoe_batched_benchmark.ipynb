{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "init_setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "sys.path.append(\"../../\")\n",
                "import time\n",
                "import torch\n",
                "from losses.basic_losses import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
                "from agents.random import RandomAgent\n",
                "from agents.muzero import MuZeroAgent\n",
                "from agent_configs.muzero_config import MuZeroConfig\n",
                "from game_configs.tictactoe_config import TicTacToeConfig\n",
                "from agents.tictactoe_expert import TicTacToeBestAgent\n",
                "from modules.world_models.muzero_world_model import MuzeroWorldModel\n",
                "\n",
                "# Ensure we use CPU for fairness/comparibility or GPU if available\n",
                "device = \"cpu\" # or \"cuda\" if available\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "muzero_benchmark",
            "metadata": {},
            "source": [
                "# MuZero Benchmark (Iterative vs Batched)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "09299485",
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"num_simulations\": 25,\n",
                "    \"per_alpha\": 0.0,\n",
                "    \"per_beta\": 0.0,\n",
                "    \"per_beta_final\": 0.0,\n",
                "    \"n_step\": 10,\n",
                "    \"root_dirichlet_alpha\": 0.25,\n",
                "    \"residual_layers\": [(24, 3, 1)],\n",
                "    \"reward_dense_layer_widths\": [],\n",
                "    \"reward_conv_layers\": [(16, 1, 1)],\n",
                "    \"actor_dense_layer_widths\": [],\n",
                "    \"actor_conv_layers\": [(16, 1, 1)],\n",
                "    \"critic_dense_layer_widths\": [],\n",
                "    \"critic_conv_layers\": [(16, 1, 1)],\n",
                "    \"to_play_dense_layer_widths\": [],\n",
                "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
                "    \"known_bounds\": [-1, 1],\n",
                "    \"support_range\": None,\n",
                "    \"minibatch_size\": 8,\n",
                "    \"replay_buffer_size\": 100000,\n",
                "    \"gumbel\": False,\n",
                "    \"gumbel_m\": 16,\n",
                "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
                "    \"training_steps\": 20000, # Reduced for benchmark speed\n",
                "    \"transfer_interval\": 1,\n",
                "    \"num_workers\": 4,\n",
                "    \"world_model_cls\": MuzeroWorldModel,\n",
                "    \"search_batch_size\": 0, # Iterative\n",
                "    \"use_virtual_mean\": False,\n",
                "    \"virtual_loss\": 3.0,\n",
                "    \"use_torch_compile\": True,\n",
                "    \"use_mixed_precision\": True,\n",
                "    \"use_quantization\": False\n",
                "}\n",
                "\n",
                "game_config = TicTacToeConfig()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "a8cbfbda",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Running MuZero Batched Search Max Fast ---\n",
                        "Using default save_intermediate_weights     : False\n",
                        "Using         training_steps                : 20000\n",
                        "Using default adam_epsilon                  : 1e-08\n",
                        "Using default momentum                      : 0.9\n",
                        "Using default learning_rate                 : 0.001\n",
                        "Using default clipnorm                      : 0\n",
                        "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
                        "Using default weight_decay                  : 0.0\n",
                        "Using default num_minibatches               : 1\n",
                        "Using default training_iterations           : 1\n",
                        "Using default lr_schedule_type              : none\n",
                        "Using default lr_schedule_steps             : []\n",
                        "Using default lr_schedule_steps             : []\n",
                        "Using default lr_schedule_values            : []\n",
                        "Using         use_mixed_precision           : True\n",
                        "Using         use_torch_compile             : True\n",
                        "Using default compile_mode                  : reduce-overhead\n",
                        "Using         minibatch_size                : 8\n",
                        "Using         replay_buffer_size            : 100000\n",
                        "Using default min_replay_buffer_size        : 8\n",
                        "Using         n_step                        : 10\n",
                        "Using default discount_factor               : 0.99\n",
                        "Using         per_alpha                     : 0.0\n",
                        "Using         per_beta                      : 0.0\n",
                        "Using         per_beta_final                : 0.0\n",
                        "Using default per_epsilon                   : 1e-06\n",
                        "Using default per_use_batch_weights         : False\n",
                        "Using default per_use_initial_max_priority  : True\n",
                        "Using default loss_function                 : <class 'losses.basic_losses.MSELoss'>\n",
                        "Using default activation                    : relu\n",
                        "Using         kernel_initializer            : None\n",
                        "Using         prob_layer_initializer        : None\n",
                        "Using default norm_type                     : none\n",
                        "Using default soft_update                   : False\n",
                        "Using default min_max_epsilon               : 0.01\n",
                        "Using         world_model_cls               : <class 'modules.world_models.muzero_world_model.MuzeroWorldModel'>\n",
                        "Using         known_bounds                  : [-1, 1]\n",
                        "Using         residual_layers               : [(24, 3, 1)]\n",
                        "Using default conv_layers                   : []\n",
                        "Using default dense_layer_widths            : []\n",
                        "Using default representation_residual_layers: [(24, 3, 1)]\n",
                        "Using default representation_conv_layers    : []\n",
                        "Using default representation_dense_layer_widths: []\n",
                        "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
                        "Using default dynamics_conv_layers          : []\n",
                        "Using default dynamics_dense_layer_widths   : []\n",
                        "Using         reward_conv_layers            : [(16, 1, 1)]\n",
                        "Using         reward_dense_layer_widths     : []\n",
                        "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
                        "Using         to_play_dense_layer_widths    : []\n",
                        "Using         critic_conv_layers            : [(16, 1, 1)]\n",
                        "Using         critic_dense_layer_widths     : []\n",
                        "Using         actor_conv_layers             : [(16, 1, 1)]\n",
                        "Using         actor_dense_layer_widths      : []\n",
                        "Using default noisy_sigma                   : 0.0\n",
                        "Using default games_per_generation          : 100\n",
                        "Using default value_loss_factor             : 1.0\n",
                        "Using default to_play_loss_factor           : 1.0\n",
                        "Using         num_simulations               : 25\n",
                        "Using         search_batch_size             : 5\n",
                        "Using         use_virtual_mean              : True\n",
                        "Using         virtual_loss                  : 3.0\n",
                        "Using         root_dirichlet_alpha          : 0.25\n",
                        "Using default root_exploration_fraction     : 0.25\n",
                        "Using default root_dirichlet_alpha_adaptive : False\n",
                        "Using         gumbel                        : False\n",
                        "Using         gumbel_m                      : 16\n",
                        "Using default gumbel_cvisit                 : 50\n",
                        "Using default gumbel_cscale                 : 1.0\n",
                        "Using default pb_c_base                     : 19652\n",
                        "Using default pb_c_init                     : 1.25\n",
                        "Using default temperatures                  : [1.0, 0.0]\n",
                        "Using default temperature_updates           : [5]\n",
                        "Using default temperature_with_training_steps: False\n",
                        "Using default clip_low_prob                 : 0.0\n",
                        "Using default value_loss_function           : <losses.basic_losses.MSELoss object at 0x375ad2380>\n",
                        "Using default reward_loss_function          : <losses.basic_losses.MSELoss object at 0x375ad1a20>\n",
                        "Using         policy_loss_function          : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x375ad2710>\n",
                        "Using default to_play_loss_function         : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x375ad2a40>\n",
                        "Using default unroll_steps                  : 5\n",
                        "Using default atom_size                     : 1\n",
                        "Using         support_range                 : None\n",
                        "Using default multi_process                 : True\n",
                        "Using         num_workers                   : 4\n",
                        "Using default lr_ratio                      : inf\n",
                        "Using         transfer_interval             : 1\n",
                        "Using default reanalyze_ratio               : 0.0\n",
                        "Using         use_quantization              : True\n",
                        "Using default reanalyze_method              : mcts\n",
                        "Using default reanalyze_tau                 : 0.3\n",
                        "Using default injection_frac                : 0.0\n",
                        "Using default reanalyze_noise               : False\n",
                        "Using default reanalyze_update_priorities   : False\n",
                        "Using default consistency_loss_factor       : 0.0\n",
                        "Using default projector_output_dim          : 128\n",
                        "Using default projector_hidden_dim          : 128\n",
                        "Using default predictor_output_dim          : 128\n",
                        "Using default predictor_hidden_dim          : 64\n",
                        "Using default mask_absorbing                : True\n",
                        "Using default value_prefix                  : False\n",
                        "Using default lstm_horizon_len              : 5\n",
                        "Using default lstm_hidden_size              : 64\n",
                        "Using default q_estimation_method           : v_mix\n",
                        "Using default stochastic                    : False\n",
                        "Using default use_true_chance_codes         : False\n",
                        "Using default num_chance                    : 32\n",
                        "Using default sigma_loss                    : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x375ad2aa0>\n",
                        "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
                        "Using default afterstate_conv_layers        : []\n",
                        "Using default afterstate_dense_layer_widths : []\n",
                        "Using default chance_conv_layers            : [(32, 3, 1)]\n",
                        "Using default chance_dense_layer_widths     : [256]\n",
                        "Using default vqvae_commitment_cost_factor  : 1.0\n",
                        "Using default action_embedding_dim          : 32\n",
                        "Using default single_action_plane           : False\n",
                        "Using default latent_viz_method             : umap\n",
                        "Using default latent_viz_interval           : 10\n",
                        "[muzero_batched_bench_fast] Using device: cpu\n",
                        "Observation dimensions: torch.Size([9, 3, 3])\n",
                        "Num actions: 9 (Discrete: True)\n",
                        "Making test env...\n",
                        "Test env configured for video recording.\n",
                        "MARL Agent 'muzero_batched_bench_fast' initialized. Test agents: ['random', 'tictactoe_expert']\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Max size: 100000\n",
                        "Initializing stat 'score' with subkeys None\n",
                        "Initializing stat 'policy_loss' with subkeys None\n",
                        "Initializing stat 'value_loss' with subkeys None\n",
                        "Initializing stat 'reward_loss' with subkeys None\n",
                        "Initializing stat 'to_play_loss' with subkeys None\n",
                        "Initializing stat 'cons_loss' with subkeys None\n",
                        "Initializing stat 'loss' with subkeys None\n",
                        "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
                        "Initializing stat 'episode_length' with subkeys None\n",
                        "Initializing stat 'policy_entropy' with subkeys None\n",
                        "Initializing stat 'value_diff' with subkeys None\n",
                        "Initializing stat 'policy_improvement' with subkeys ['network', 'search']\n",
                        "Initializing stat 'root_children_values' with subkeys None\n",
                        "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
                        "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
                        "Preparing main model for QAT...\n",
                        "Using Quantization Backend: qnnpack\n",
                        "[Worker 2] Starting self-play...\n",
                        "[Worker 1] Starting self-play...\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "[Worker 0] Starting self-play...\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "[Worker 3] Starting self-play...\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Using Quantization Backend: qnnpack\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Using Quantization Backend: qnnpack\n",
                        "Using Quantization Backend: qnnpack\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Using Quantization Backend: qnnpack\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.10/site-packages/torch/ao/quantization/utils.py:435: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
                        "  warnings.warn(\n",
                        "/opt/homebrew/lib/python3.10/site-packages/torch/ao/quantization/utils.py:435: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
                        "  warnings.warn(\n",
                        "/opt/homebrew/lib/python3.10/site-packages/torch/ao/quantization/utils.py:435: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
                        "  warnings.warn(\n",
                        "/opt/homebrew/lib/python3.10/site-packages/torch/ao/quantization/utils.py:435: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/2/episode_000000.mp4Started recording episode 0 to ./videos/muzero_batched_bench_fast/0/episode_000000.mp4\n",
                        "\n",
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/1/episode_000000.mp4\n",
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/3/episode_000000.mp4\n",
                        "Stopped recording episode 0. Recorded 8 frames.learning\n",
                        "\n",
                        "Size: 0\n",
                        "Stopped recording episode 0. Recorded 9 frames.\n",
                        "Size: 8\n",
                        "Stopped recording episode 0. Recorded 9 frames.\n",
                        "Size: 17\n",
                        "0\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0],\n",
                        "        [2, 3, 4, 7, 0]])\n",
                        "target value tensor([[ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
                        "predicted values tensor([[[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]],\n",
                        "\n",
                        "        [[0.0706],\n",
                        "         [0.0715],\n",
                        "         [0.0723],\n",
                        "         [0.0732],\n",
                        "         [0.0745],\n",
                        "         [0.0757]]])\n",
                        "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1209],\n",
                        "         [-0.1028],\n",
                        "         [-0.0888],\n",
                        "         [-0.0672],\n",
                        "         [-0.0500]]])\n",
                        "target to plays tensor([[[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4917, 0.5083],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4946, 0.5054],\n",
                        "         [0.4921, 0.5079],\n",
                        "         [0.4913, 0.5087]]])\n",
                        "masks tensor([[True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True]]) tensor([[True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True],\n",
                        "        [True, True, True, True, True, True]])\n",
                        "Stopped recording episode 0. Recorded 10 frames.\n",
                        "Size: 26\n",
                        "Initializing stat 'q_loss' with subkeys None\n",
                        "Initializing stat 'sigma_loss' with subkeys None\n",
                        "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
                        "learning\n",
                        "learning\n",
                        "Size: 36\n",
                        "learning\n",
                        "Size: 44\n",
                        "learning\n",
                        "Size: 53\n",
                        "learning\n",
                        "Size: 63\n",
                        "Size: 73\n",
                        "learning\n",
                        "learning\n",
                        "Size: 79\n",
                        "learning\n",
                        "Size: 86\n",
                        "Size: 94\n",
                        "learning\n",
                        "Size: 104\n",
                        "learning\n",
                        "Size: 114\n",
                        "learning\n",
                        "Size: 123\n",
                        "Size: 131\n",
                        "learning\n",
                        "learning\n",
                        "Size: 139\n",
                        "Size: 148\n",
                        "learning\n",
                        "learning\n",
                        "Size: 156\n",
                        "learning\n",
                        "Size: 165\n",
                        "Size: 175\n",
                        "learning\n",
                        "Size: 181\n",
                        "learning\n",
                        "learning\n",
                        "Size: 191\n",
                        "learning\n",
                        "Size: 198\n",
                        "Size: 207\n",
                        "learning\n",
                        "Size: 217\n",
                        "learning\n",
                        "Size: 226\n",
                        "learning\n",
                        "learning\n",
                        "Size: 233\n",
                        "Size: 243\n",
                        "learning\n",
                        "Size: 253\n",
                        "Size: 261\n",
                        "learning\n",
                        "learning\n",
                        "Size: 268\n",
                        "Size: 276\n",
                        "learning\n",
                        "Size: 284\n",
                        "Size: 293\n",
                        "learning\n",
                        "learning\n",
                        "Size: 303\n",
                        "Size: 310\n",
                        "learning\n",
                        "Size: 319\n",
                        "learning\n",
                        "Size: 327\n",
                        "learning\n",
                        "learning\n",
                        "Size: 336\n",
                        "learning\n",
                        "Size: 346\n",
                        "Size: 356\n",
                        "learning\n",
                        "Size: 366\n",
                        "learning\n",
                        "Size: 375\n",
                        "Size: 381\n",
                        "learning\n",
                        "Size: 390\n",
                        "learning\n",
                        "learning\n",
                        "Size: 396\n",
                        "learning\n",
                        "Size: 406\n",
                        "learning\n",
                        "Size: 414\n",
                        "Size: 422\n",
                        "learning\n",
                        "Size: 432\n",
                        "learning\n",
                        "Size: 439\n",
                        "learning\n",
                        "learning\n",
                        "Size: 447\n",
                        "Size: 457\n",
                        "learning\n",
                        "Size: 467\n",
                        "learning\n",
                        "Size: 476\n",
                        "learning\n",
                        "learning\n",
                        "Size: 484\n",
                        "learning\n",
                        "Size: 494\n",
                        "Size: 504\n",
                        "learning\n",
                        "Size: 514\n",
                        "learning\n",
                        "learning\n",
                        "Size: 524\n",
                        "Size: 532\n",
                        "learning\n",
                        "Size: 540\n",
                        "learning\n",
                        "learning\n",
                        "Size: 547\n",
                        "learning\n",
                        "Size: 557\n",
                        "Size: 567\n",
                        "Size: 575\n",
                        "learning\n",
                        "learning\n",
                        "Size: 585\n",
                        "learning\n",
                        "learning\n",
                        "Size: 593\n",
                        "learning\n",
                        "Size: 602\n",
                        "Size: 612\n",
                        "learning\n",
                        "learning\n",
                        "Size: 621\n",
                        "Size: 631\n",
                        "learning\n",
                        "Size: 637\n",
                        "learning\n",
                        "Size: 645\n",
                        "learning\n",
                        "Size: 655\n",
                        "learning\n",
                        "Size: 663\n",
                        "learning\n",
                        "Size: 673\n",
                        "Size: 683\n",
                        "learning\n",
                        "Size: 690\n",
                        "learning\n",
                        "learning\n",
                        "Size: 699\n",
                        "Size: 706\n",
                        "Size: 712\n",
                        "learning\n",
                        "learning\n",
                        "Size: 722\n",
                        "learning\n",
                        "Size: 732\n",
                        "learning\n",
                        "Size: 740\n",
                        "Size: 749\n",
                        "learning\n",
                        "learning\n",
                        "Size: 758\n",
                        "learning\n",
                        "Size: 768\n",
                        "learning\n",
                        "Size: 776\n",
                        "Size: 784\n",
                        "learning\n",
                        "learning\n",
                        "Size: 793\n",
                        "Size: 801\n",
                        "Size: 807\n",
                        "learning\n",
                        "learning\n",
                        "Size: 817\n",
                        "learning\n",
                        "learning\n",
                        "Size: 827\n",
                        "Size: 837\n",
                        "learning\n",
                        "Size: 846\n",
                        "Size: 856learning\n",
                        "\n",
                        "learning\n",
                        "learning\n",
                        "Size: 864\n",
                        "Size: 873\n",
                        "Size: 883\n",
                        "learning\n",
                        "learning\n",
                        "Size: 893\n",
                        "learning\n",
                        "Size: 903\n",
                        "learning\n",
                        "Size: 909\n",
                        "learning\n",
                        "Size: 918\n",
                        "learning\n",
                        "Size: 928\n",
                        "learning\n",
                        "Size: 938\n",
                        "Size: 948\n",
                        "Size: 954\n",
                        "learning\n",
                        "plotting scoreSize: 961\n",
                        "\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "Size: 969\n",
                        "Size: 976\n",
                        "Size: 984\n",
                        "Size: 992\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.10/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
                        "  warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "learning\n",
                        "100\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[4, 7, 0, 5, 8],\n",
                        "        [2, 0, 0, 6, 6],\n",
                        "        [7, 2, 0, 6, 6],\n",
                        "        [1, 6, 2, 8, 0],\n",
                        "        [0, 5, 1, 7, 2],\n",
                        "        [6, 0, 1, 4, 2],\n",
                        "        [1, 5, 3, 2, 0],\n",
                        "        [4, 3, 0, 7, 1]])\n",
                        "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
                        "predicted values tensor([[[ 0.0654],\n",
                        "         [ 0.1142],\n",
                        "         [ 0.1584],\n",
                        "         [ 0.1587],\n",
                        "         [ 0.1463],\n",
                        "         [ 0.1341]],\n",
                        "\n",
                        "        [[ 0.0572],\n",
                        "         [ 0.0849],\n",
                        "         [ 0.1021],\n",
                        "         [ 0.1190],\n",
                        "         [ 0.0970],\n",
                        "         [ 0.0730]],\n",
                        "\n",
                        "        [[-0.0338],\n",
                        "         [ 0.0398],\n",
                        "         [ 0.0428],\n",
                        "         [ 0.0674],\n",
                        "         [ 0.0717],\n",
                        "         [ 0.0678]],\n",
                        "\n",
                        "        [[ 0.1098],\n",
                        "         [ 0.1473],\n",
                        "         [ 0.1584],\n",
                        "         [ 0.1587],\n",
                        "         [ 0.1590],\n",
                        "         [ 0.1594]],\n",
                        "\n",
                        "        [[-0.0030],\n",
                        "         [ 0.1322],\n",
                        "         [ 0.0871],\n",
                        "         [ 0.0861],\n",
                        "         [ 0.0799],\n",
                        "         [ 0.0767]],\n",
                        "\n",
                        "        [[ 0.0542],\n",
                        "         [ 0.1090],\n",
                        "         [ 0.1456],\n",
                        "         [ 0.1587],\n",
                        "         [ 0.1590],\n",
                        "         [ 0.1594]],\n",
                        "\n",
                        "        [[-0.0098],\n",
                        "         [ 0.1007],\n",
                        "         [ 0.0773],\n",
                        "         [ 0.1063],\n",
                        "         [ 0.1082],\n",
                        "         [ 0.1251]],\n",
                        "\n",
                        "        [[-0.0338],\n",
                        "         [-0.0248],\n",
                        "         [-0.0030],\n",
                        "         [ 0.0367],\n",
                        "         [ 0.0791],\n",
                        "         [ 0.0991]]])\n",
                        "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [ 0.0510],\n",
                        "         [ 0.0206],\n",
                        "         [-0.0375],\n",
                        "         [-0.0769],\n",
                        "         [-0.0671]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0380],\n",
                        "         [ 0.0443],\n",
                        "         [ 0.0306],\n",
                        "         [ 0.0238],\n",
                        "         [ 0.0100]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0517],\n",
                        "         [-0.0250],\n",
                        "         [ 0.0012],\n",
                        "         [ 0.0456],\n",
                        "         [ 0.0514]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0679],\n",
                        "         [-0.1079],\n",
                        "         [-0.1081],\n",
                        "         [-0.1082],\n",
                        "         [-0.1085]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1077],\n",
                        "         [-0.1079],\n",
                        "         [-0.1081],\n",
                        "         [-0.1082],\n",
                        "         [-0.1004]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0293],\n",
                        "         [ 0.0200],\n",
                        "         [ 0.0075],\n",
                        "         [-0.0100],\n",
                        "         [-0.0201]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0510],\n",
                        "         [ 0.0362],\n",
                        "         [ 0.0094],\n",
                        "         [-0.0181],\n",
                        "         [-0.0395]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0510],\n",
                        "         [ 0.0512],\n",
                        "         [ 0.0512],\n",
                        "         [ 0.0300],\n",
                        "         [ 0.0157]]])\n",
                        "target to plays tensor([[[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.5059, 0.4941],\n",
                        "         [0.4914, 0.5086],\n",
                        "         [0.4867, 0.5133],\n",
                        "         [0.4958, 0.5042],\n",
                        "         [0.4911, 0.5089]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4909, 0.5091],\n",
                        "         [0.4843, 0.5157],\n",
                        "         [0.4830, 0.5170],\n",
                        "         [0.4874, 0.5126],\n",
                        "         [0.4909, 0.5091]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4978, 0.5022],\n",
                        "         [0.5152, 0.4848],\n",
                        "         [0.5179, 0.4821],\n",
                        "         [0.5185, 0.4815],\n",
                        "         [0.5168, 0.4832]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4444, 0.5556],\n",
                        "         [0.4470, 0.5530],\n",
                        "         [0.4505, 0.5495],\n",
                        "         [0.4569, 0.5431],\n",
                        "         [0.4594, 0.5406]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5223, 0.4777],\n",
                        "         [0.5230, 0.4770],\n",
                        "         [0.5182, 0.4818],\n",
                        "         [0.5187, 0.4813],\n",
                        "         [0.5128, 0.4872]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4902, 0.5098],\n",
                        "         [0.4880, 0.5120],\n",
                        "         [0.4911, 0.5089],\n",
                        "         [0.4968, 0.5032],\n",
                        "         [0.5002, 0.4998]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5049, 0.4951],\n",
                        "         [0.5162, 0.4838],\n",
                        "         [0.5216, 0.4784],\n",
                        "         [0.5170, 0.4830],\n",
                        "         [0.5180, 0.4820]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5379, 0.4621],\n",
                        "         [0.5347, 0.4653],\n",
                        "         [0.5219, 0.4781],\n",
                        "         [0.5113, 0.4887],\n",
                        "         [0.5000, 0.5000]]])\n",
                        "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.10/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
                        "  warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "learning\n",
                        "200\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[3, 1, 4, 8, 0],\n",
                        "        [1, 2, 0, 4, 8],\n",
                        "        [2, 7, 0, 8, 1],\n",
                        "        [6, 3, 7, 0, 8],\n",
                        "        [0, 7, 3, 8, 0],\n",
                        "        [0, 1, 4, 2, 5],\n",
                        "        [2, 6, 7, 4, 3],\n",
                        "        [5, 7, 0, 6, 8]])\n",
                        "target value tensor([[-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9227, -0.9321,  0.9415, -0.9510,  0.9606, -0.9703],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
                        "predicted values tensor([[[-0.0252],\n",
                        "         [ 0.0058],\n",
                        "         [ 0.0596],\n",
                        "         [ 0.1155],\n",
                        "         [ 0.1561],\n",
                        "         [ 0.1616]],\n",
                        "\n",
                        "        [[-0.0252],\n",
                        "         [ 0.0281],\n",
                        "         [ 0.0467],\n",
                        "         [ 0.0954],\n",
                        "         [ 0.1511],\n",
                        "         [ 0.1616]],\n",
                        "\n",
                        "        [[ 0.0699],\n",
                        "         [ 0.1590],\n",
                        "         [ 0.1594],\n",
                        "         [ 0.1600],\n",
                        "         [ 0.1611],\n",
                        "         [ 0.1616]],\n",
                        "\n",
                        "        [[ 0.1001],\n",
                        "         [ 0.0518],\n",
                        "         [ 0.0725],\n",
                        "         [ 0.1234],\n",
                        "         [ 0.1461],\n",
                        "         [ 0.1437]],\n",
                        "\n",
                        "        [[ 0.0792],\n",
                        "         [ 0.0698],\n",
                        "         [ 0.0567],\n",
                        "         [ 0.0782],\n",
                        "         [ 0.0960],\n",
                        "         [ 0.1187]],\n",
                        "\n",
                        "        [[ 0.0699],\n",
                        "         [ 0.1539],\n",
                        "         [ 0.1594],\n",
                        "         [ 0.1600],\n",
                        "         [ 0.1611],\n",
                        "         [ 0.1616]],\n",
                        "\n",
                        "        [[ 0.0699],\n",
                        "         [ 0.1590],\n",
                        "         [ 0.1594],\n",
                        "         [ 0.1600],\n",
                        "         [ 0.1611],\n",
                        "         [ 0.1616]],\n",
                        "\n",
                        "        [[ 0.0699],\n",
                        "         [ 0.0921],\n",
                        "         [ 0.1220],\n",
                        "         [ 0.1413],\n",
                        "         [ 0.1497],\n",
                        "         [ 0.1616]]])\n",
                        "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [ 0.0519],\n",
                        "         [ 0.0346],\n",
                        "         [ 0.0077],\n",
                        "         [ 0.0076],\n",
                        "         [-0.0126]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0154],\n",
                        "         [ 0.0064],\n",
                        "         [-0.0089],\n",
                        "         [-0.0603],\n",
                        "         [-0.0778]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1115],\n",
                        "         [-0.0832],\n",
                        "         [-0.0747],\n",
                        "         [-0.0343],\n",
                        "         [-0.0335]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0122],\n",
                        "         [-0.0058],\n",
                        "         [-0.0230],\n",
                        "         [-0.0121],\n",
                        "         [ 0.0082]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1115],\n",
                        "         [-0.1114],\n",
                        "         [-0.1053],\n",
                        "         [-0.0756],\n",
                        "         [-0.0683]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1115],\n",
                        "         [-0.0800],\n",
                        "         [-0.0626],\n",
                        "         [-0.0597],\n",
                        "         [-0.0733]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1115],\n",
                        "         [-0.0884],\n",
                        "         [-0.0683],\n",
                        "         [-0.0616],\n",
                        "         [-0.0563]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0487],\n",
                        "         [-0.0672],\n",
                        "         [-0.0747],\n",
                        "         [-0.0750],\n",
                        "         [-0.0474]]])\n",
                        "target to plays tensor([[[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.4833, 0.5167],\n",
                        "         [0.4824, 0.5176],\n",
                        "         [0.4839, 0.5161],\n",
                        "         [0.4783, 0.5217],\n",
                        "         [0.4703, 0.5297]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4857, 0.5143],\n",
                        "         [0.4783, 0.5217],\n",
                        "         [0.4860, 0.5140],\n",
                        "         [0.4961, 0.5039],\n",
                        "         [0.5010, 0.4990]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4915, 0.5085],\n",
                        "         [0.4862, 0.5138],\n",
                        "         [0.4853, 0.5147],\n",
                        "         [0.4783, 0.5217],\n",
                        "         [0.4619, 0.5381]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4640, 0.5360],\n",
                        "         [0.4699, 0.5301],\n",
                        "         [0.4800, 0.5200],\n",
                        "         [0.4812, 0.5188],\n",
                        "         [0.4831, 0.5169]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4840, 0.5160],\n",
                        "         [0.4884, 0.5116],\n",
                        "         [0.4908, 0.5092],\n",
                        "         [0.4877, 0.5123],\n",
                        "         [0.4787, 0.5213]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4660, 0.5340],\n",
                        "         [0.4538, 0.5462],\n",
                        "         [0.4479, 0.5521],\n",
                        "         [0.4409, 0.5591],\n",
                        "         [0.4413, 0.5587]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4915, 0.5085],\n",
                        "         [0.4838, 0.5162],\n",
                        "         [0.4858, 0.5142],\n",
                        "         [0.4855, 0.5145],\n",
                        "         [0.4771, 0.5229]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4896, 0.5104],\n",
                        "         [0.4928, 0.5072],\n",
                        "         [0.4937, 0.5063],\n",
                        "         [0.4855, 0.5145],\n",
                        "         [0.4725, 0.5275]]])\n",
                        "masks tensor([[ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/1/episode_000100.mp4\n",
                        "learning\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/3/episode_000100.mp4\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/0/episode_000100.mp4\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/2/episode_000100.mp4\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 100. Recorded 6 frames.\n",
                        "Stopped recording episode 100. Recorded 7 frames.\n",
                        "Stopped recording episode 100. Recorded 7 frames.\n",
                        "learning\n",
                        "Stopped recording episode 100. Recorded 8 frames.\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.10/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
                        "  warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "learning\n",
                        "300\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[0, 7, 4, 8, 1],\n",
                        "        [0, 1, 4, 5, 3],\n",
                        "        [7, 6, 0, 5, 1],\n",
                        "        [8, 5, 0, 1, 3],\n",
                        "        [7, 1, 8, 0, 1],\n",
                        "        [0, 0, 3, 5, 1],\n",
                        "        [5, 8, 0, 0, 1],\n",
                        "        [5, 7, 0, 2, 6]])\n",
                        "target value tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
                        "predicted values tensor([[[ 0.0704],\n",
                        "         [ 0.1550],\n",
                        "         [ 0.1698],\n",
                        "         [ 0.1707],\n",
                        "         [ 0.1720],\n",
                        "         [ 0.1727]],\n",
                        "\n",
                        "        [[ 0.0290],\n",
                        "         [ 0.0320],\n",
                        "         [ 0.0889],\n",
                        "         [ 0.1444],\n",
                        "         [ 0.1720],\n",
                        "         [ 0.1727]],\n",
                        "\n",
                        "        [[ 0.0733],\n",
                        "         [ 0.1689],\n",
                        "         [ 0.1698],\n",
                        "         [ 0.1707],\n",
                        "         [ 0.1720],\n",
                        "         [ 0.1727]],\n",
                        "\n",
                        "        [[ 0.0704],\n",
                        "         [ 0.1303],\n",
                        "         [ 0.1064],\n",
                        "         [ 0.1065],\n",
                        "         [ 0.1450],\n",
                        "         [ 0.1610]],\n",
                        "\n",
                        "        [[-0.0167],\n",
                        "         [ 0.0036],\n",
                        "         [ 0.0277],\n",
                        "         [ 0.0627],\n",
                        "         [ 0.0955],\n",
                        "         [ 0.1166]],\n",
                        "\n",
                        "        [[ 0.0748],\n",
                        "         [ 0.1084],\n",
                        "         [ 0.1195],\n",
                        "         [ 0.1181],\n",
                        "         [ 0.0875],\n",
                        "         [ 0.1006]],\n",
                        "\n",
                        "        [[ 0.0748],\n",
                        "         [ 0.1186],\n",
                        "         [ 0.1188],\n",
                        "         [ 0.1502],\n",
                        "         [ 0.1720],\n",
                        "         [ 0.1727]],\n",
                        "\n",
                        "        [[ 0.1358],\n",
                        "         [ 0.1689],\n",
                        "         [ 0.1698],\n",
                        "         [ 0.1707],\n",
                        "         [ 0.1720],\n",
                        "         [ 0.1727]]])\n",
                        "target rewards tensor([[0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [-0.1139],\n",
                        "         [-0.0703],\n",
                        "         [-0.0392],\n",
                        "         [-0.0182],\n",
                        "         [-0.0298]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0072],\n",
                        "         [-0.0210],\n",
                        "         [-0.0438],\n",
                        "         [-0.0475],\n",
                        "         [-0.0635]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1139],\n",
                        "         [-0.1097],\n",
                        "         [-0.0883],\n",
                        "         [-0.0846],\n",
                        "         [-0.0823]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0020],\n",
                        "         [ 0.0197],\n",
                        "         [ 0.0052],\n",
                        "         [ 0.0104],\n",
                        "         [ 0.0019]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0336],\n",
                        "         [ 0.0000],\n",
                        "         [-0.0085],\n",
                        "         [-0.0306],\n",
                        "         [-0.0395]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0731],\n",
                        "         [-0.0683],\n",
                        "         [-0.0451],\n",
                        "         [-0.0300],\n",
                        "         [-0.0110]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0020],\n",
                        "         [-0.0250],\n",
                        "         [-0.0523],\n",
                        "         [-0.0527],\n",
                        "         [-0.0408]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0816],\n",
                        "         [-0.0814],\n",
                        "         [-0.0746],\n",
                        "         [-0.0501],\n",
                        "         [-0.0518]]])\n",
                        "target to plays tensor([[[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.4659, 0.5341],\n",
                        "         [0.4612, 0.5388],\n",
                        "         [0.4491, 0.5509],\n",
                        "         [0.4403, 0.5597],\n",
                        "         [0.4401, 0.5599]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4680, 0.5320],\n",
                        "         [0.4601, 0.5399],\n",
                        "         [0.4641, 0.5359],\n",
                        "         [0.4758, 0.5242],\n",
                        "         [0.4701, 0.5299]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4861, 0.5139],\n",
                        "         [0.4875, 0.5125],\n",
                        "         [0.4911, 0.5089],\n",
                        "         [0.4859, 0.5141],\n",
                        "         [0.4759, 0.5241]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4767, 0.5233],\n",
                        "         [0.4678, 0.5322],\n",
                        "         [0.4711, 0.5289],\n",
                        "         [0.4657, 0.5343],\n",
                        "         [0.4600, 0.5400]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4563, 0.5437],\n",
                        "         [0.4493, 0.5507],\n",
                        "         [0.4454, 0.5546],\n",
                        "         [0.4605, 0.5395],\n",
                        "         [0.4639, 0.5361]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4845, 0.5155],\n",
                        "         [0.4866, 0.5134],\n",
                        "         [0.4934, 0.5066],\n",
                        "         [0.4889, 0.5111],\n",
                        "         [0.4799, 0.5201]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5033, 0.4967],\n",
                        "         [0.5005, 0.4995],\n",
                        "         [0.4958, 0.5042],\n",
                        "         [0.4871, 0.5129],\n",
                        "         [0.4790, 0.5210]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5026, 0.4974],\n",
                        "         [0.4866, 0.5134],\n",
                        "         [0.4702, 0.5298],\n",
                        "         [0.4634, 0.5366],\n",
                        "         [0.4649, 0.5351]]])\n",
                        "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Process Process-2:\n",
                        "Process Process-1:\n",
                        "Process Process-3:\n",
                        "Process Process-4:\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 428, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1110, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1057, in predict\n",
                        "    root_value, exploratory_policy, target_policy, best_action, search_metadata = self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 145, in run\n",
                        "    self._run_batched_simulations(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 804, in _run_batched_simulations\n",
                        "    node.expand(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/nodes.py\", line 235, in expand\n",
                        "    self._populate_children(allowed_actions, priors)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/nodes.py\", line 240, in _populate_children\n",
                        "    allowed_priors_sum = sum(allowed_priors.values())\n",
                        "KeyboardInterrupt\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 428, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1110, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1057, in predict\n",
                        "    root_value, exploratory_policy, target_policy, best_action, search_metadata = self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 145, in run\n",
                        "    self._run_batched_simulations(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 522, in _run_batched_simulations\n",
                        "    action_or_code, node = self.root_selection_strategy.select_child(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/action_selectors.py\", line 46, in select_child\n",
                        "    scores_dict = self.scoring_method.get_scores(node, min_max_stats)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/scoring_methods.py\", line 29, in get_scores\n",
                        "    scores[action] = self.score(node, child, min_max_stats)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/scoring_methods.py\", line 47, in score\n",
                        "    value_score = min_max_stats.normalize(node.get_child_q_from_parent(child))\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/nodes.py\", line 327, in get_child_q_from_parent\n",
                        "    if not child.expanded():\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/nodes.py\", line 251, in expanded\n",
                        "    return len(self.children) > 0\n",
                        "KeyboardInterrupt\n",
                        "Traceback (most recent call last):\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 428, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1110, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1057, in predict\n",
                        "    root_value, exploratory_policy, target_policy, best_action, search_metadata = self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 145, in run\n",
                        "    self._run_batched_simulations(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 699, in _run_batched_simulations\n",
                        "    ) = inference_fns[\"recurrent\"](\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1001, in predict_recurrent_inference\n",
                        "    model.recurrent_inference(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 308, in recurrent_inference\n",
                        "    value, policy = self.prediction(next_hidden_state)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 57, in forward\n",
                        "    S = self.net(inputs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 124, in forward\n",
                        "    x = self.residual_layers(x)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 428, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1110, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1057, in predict\n",
                        "    root_value, exploratory_policy, target_policy, best_action, search_metadata = self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 137, in forward\n",
                        "    x = layer(x)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 145, in run\n",
                        "    self._run_batched_simulations(\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 699, in _run_batched_simulations\n",
                        "    ) = inference_fns[\"recurrent\"](\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1001, in predict_recurrent_inference\n",
                        "    model.recurrent_inference(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 74, in forward\n",
                        "    x = self.conv2(x)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 300, in recurrent_inference\n",
                        "    wm_output = self.world_model.recurrent_inference(\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/world_models/muzero_world_model.py\", line 278, in recurrent_inference\n",
                        "    reward, next_hidden_state, to_play, reward_hidden = self.dynamics(\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
                        "    return self._conv_forward(input, self.weight, self.bias)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
                        "    return F.conv2d(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/world_models/muzero_world_model.py\", line 175, in forward\n",
                        "    next_hidden_state = self._fuse_and_process(hidden_state, action)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/world_models/muzero_world_model.py\", line 116, in _fuse_and_process\n",
                        "    S = self.net(S)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "KeyboardInterrupt\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 124, in forward\n",
                        "    x = self.residual_layers(x)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 137, in forward\n",
                        "    x = layer(x)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/residual.py\", line 74, in forward\n",
                        "    x = self.conv2(x)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
                        "    return self._conv_forward(input, self.weight, self.bias)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
                        "    return F.conv2d(\n",
                        "KeyboardInterrupt\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m agent_batch\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     25\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m \u001b[43magent_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMuZero Batched Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:495\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tb))  \u001b[38;5;66;03m# optional: print worker traceback\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrain_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m training_game \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgames_per_generation)):\n",
                        "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../stats/stats.py:152\u001b[0m, in \u001b[0;36mStatTracker.drain_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mget_nowait()\n\u001b[1;32m    151\u001b[0m     method_name, \u001b[38;5;241m*\u001b[39margs \u001b[38;5;241m=\u001b[39m message\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../stats/stats.py:132\u001b[0m, in \u001b[0;36mStatTracker.append\u001b[0;34m(self, key, value, subkey)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires a subkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[key][subkey] \u001b[38;5;241m=\u001b[39m \u001b[43mappend_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[key] \u001b[38;5;241m=\u001b[39m append_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[key], new_val)\n",
                        "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../stats/stats.py:127\u001b[0m, in \u001b[0;36mStatTracker.append.<locals>.append_to_tensor\u001b[0;34m(current_tensor, new_data)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_tensor\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_data\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "print(\"--- Running MuZero Batched Search Max Fast ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"num_workers\"] = 4\n",
                "params_batched[\"search_batch_size\"] = 5 \n",
                "params_batched[\"use_virtual_mean\"] = True\n",
                "params_batched[\"use_mixed_precision\"] = True\n",
                "params_batched[\"use_torch_compile\"] = True\n",
                "params_batched[\"use_quantization\"] = True\n",
                "# params_batched[\"num_envs_per_worker\"] = 4\n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_fast\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "muzero_iterative_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Iterative Search (Batch=0) ---\")\n",
                "env_iter = TicTacToeConfig().make_env()\n",
                "config_iter = MuZeroConfig(config_dict=params, game_config=game_config)\n",
                "config_iter.search_batch_size = 0 # Explicitly set\n",
                "\n",
                "agent_iter = MuZeroAgent(\n",
                "    env=env_iter,\n",
                "    config=config_iter,\n",
                "    name=\"muzero_iterative_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_iter.checkpoint_interval = 100\n",
                "agent_iter.test_interval = 1000\n",
                "agent_iter.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_iter.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Iterative Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1fe0b3c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Iterative Search (Batch=1) ---\")\n",
                "env_iter = TicTacToeConfig().make_env()\n",
                "config_iter = MuZeroConfig(config_dict=params, game_config=game_config)\n",
                "config_iter.search_batch_size = 1 # Explicitly set\n",
                "\n",
                "agent_iter = MuZeroAgent(\n",
                "    env=env_iter,\n",
                "    config=config_iter,\n",
                "    name=\"muzero_iterative_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_iter.checkpoint_interval = 100\n",
                "agent_iter.test_interval = 1000\n",
                "agent_iter.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_iter.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Iterative Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "muzero_batched_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Batched Search (Batch=5) ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"search_batch_size\"] = 5 \n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "config_batch.search_batch_size = 5 # Explicitly set\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_size_5\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e22fc8c",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Batched Search (Batch=5) ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"search_batch_size\"] = 5 \n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "config_batch.search_batch_size = 5 # Explicitly set\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_size_5\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f52b0d54",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Batched Search (Batch=5) Virtual Mean ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"search_batch_size\"] = 5 \n",
                "params_batched[\"use_virtual_mean\"] = True\n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "config_batch.search_batch_size = 5 # Explicitly set\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_size_5_virtual_mean_1\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gumbel_benchmark",
            "metadata": {},
            "source": [
                "# Gumbel MuZero Benchmark (Iterative vs Batched)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "695b2ba9",
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"num_simulations\": 25,\n",
                "    \"per_alpha\": 0.0,\n",
                "    \"per_beta\": 0.0,\n",
                "    \"per_beta_final\": 0.0,\n",
                "    \"n_step\": 10,\n",
                "    \"root_dirichlet_alpha\": 0.25,\n",
                "    \"residual_layers\": [(24, 3, 1)],\n",
                "    \"reward_dense_layer_widths\": [],\n",
                "    \"reward_conv_layers\": [(16, 1, 1)],\n",
                "    \"actor_dense_layer_widths\": [],\n",
                "    \"actor_conv_layers\": [(16, 1, 1)],\n",
                "    \"critic_dense_layer_widths\": [],\n",
                "    \"critic_conv_layers\": [(16, 1, 1)],\n",
                "    \"to_play_dense_layer_widths\": [],\n",
                "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
                "    \"known_bounds\": [-1, 1],\n",
                "    \"support_range\": None,\n",
                "    \"minibatch_size\": 8,\n",
                "    \"replay_buffer_size\": 100000,\n",
                "    \"gumbel\": True,\n",
                "    \"gumbel_m\": 4,\n",
                "    \"policy_loss_function\": KLDivergenceLoss(),\n",
                "    \"training_steps\": 20000, # Reduced for benchmark speed\n",
                "    \"transfer_interval\": 1,\n",
                "    \"num_workers\": 4,\n",
                "    \"world_model_cls\": MuzeroWorldModel,\n",
                "    \"search_batch_size\": 0, # Iterative\n",
                "    \"use_virtual_mean\": False,\n",
                "    \"virtual_loss\": 3.0,\n",
                "}\n",
                "\n",
                "game_config = TicTacToeConfig()\n",
                "\n",
                "params_gumbel = params.copy()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gumbel_iterative_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running Gumbel MuZero Iterative Search (Batch=1) ---\")\n",
                "params_gumbel[\"search_batch_size\"] = 0\n",
                "\n",
                "env_gumbel = TicTacToeConfig().make_env()\n",
                "config_gumbel = MuZeroConfig(config_dict=params_gumbel, game_config=game_config)\n",
                "\n",
                "agent_gumbel = MuZeroAgent(\n",
                "    env=env_gumbel,\n",
                "    config=config_gumbel,\n",
                "    name=\"gumbel_iterative_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_gumbel.checkpoint_interval = 100\n",
                "agent_gumbel.test_interval = 1000\n",
                "agent_gumbel.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_gumbel.train()\n",
                "end_time = time.time()\n",
                "print(f\"Gumbel Iterative Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gumbel_batched_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running Gumbel MuZero Batched Search (Batch=5) ---\")\n",
                "params_gumbel_batch = params_gumbel.copy()\n",
                "params_gumbel_batch[\"search_batch_size\"] = 5\n",
                "params_gumbel_batch[\"use_virtual_mean\"] = True\n",
                "\n",
                "env_gumbel_batch = TicTacToeConfig().make_env()\n",
                "config_gumbel_batch = MuZeroConfig(config_dict=params_gumbel_batch, game_config=game_config)\n",
                "\n",
                "agent_gumbel_batch = MuZeroAgent(\n",
                "    env=env_gumbel_batch,\n",
                "    config=config_gumbel_batch,\n",
                "    name=\"gumbel_batched_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_gumbel_batch.checkpoint_interval = 100\n",
                "agent_gumbel_batch.test_interval = 1000\n",
                "agent_gumbel_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_gumbel_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"Gumbel Batched Time: {end_time - start_time:.2f}s\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
