{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "init_setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "\n",
                "sys.path.append(\"../../\")\n",
                "import time\n",
                "import torch\n",
                "from losses.basic_losses import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
                "from agents.random import RandomAgent\n",
                "from agents.muzero import MuZeroAgent\n",
                "from agent_configs.muzero_config import MuZeroConfig\n",
                "from game_configs.tictactoe_config import TicTacToeConfig\n",
                "from agents.tictactoe_expert import TicTacToeBestAgent\n",
                "from modules.world_models.muzero_world_model import MuzeroWorldModel\n",
                "\n",
                "# Ensure we use CPU for fairness/comparibility or GPU if available\n",
                "device = \"cpu\"  # or \"cuda\" if available\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "muzero_benchmark",
            "metadata": {},
            "source": [
                "# MuZero Benchmark (Iterative vs Batched)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "09299485",
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"num_simulations\": 25,\n",
                "    \"per_alpha\": 0.0,\n",
                "    \"per_beta\": 0.0,\n",
                "    \"per_beta_final\": 0.0,\n",
                "    \"n_step\": 10,\n",
                "    \"root_dirichlet_alpha\": 0.25,\n",
                "    \"residual_layers\": [(24, 3, 1)],\n",
                "    \"reward_dense_layer_widths\": [],\n",
                "    \"reward_conv_layers\": [(16, 1, 1)],\n",
                "    \"actor_dense_layer_widths\": [],\n",
                "    \"actor_conv_layers\": [(16, 1, 1)],\n",
                "    \"critic_dense_layer_widths\": [],\n",
                "    \"critic_conv_layers\": [(16, 1, 1)],\n",
                "    \"to_play_dense_layer_widths\": [],\n",
                "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
                "    \"known_bounds\": [-1, 1],\n",
                "    \"support_range\": None,\n",
                "    \"minibatch_size\": 8,\n",
                "    \"replay_buffer_size\": 100000,\n",
                "    \"gumbel\": False,\n",
                "    \"gumbel_m\": 16,\n",
                "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
                "    \"training_steps\": 20000,  # Reduced for benchmark speed\n",
                "    \"transfer_interval\": 1,\n",
                "    \"num_workers\": 4,\n",
                "    \"world_model_cls\": MuzeroWorldModel,\n",
                "    \"search_batch_size\": 0,  # Iterative\n",
                "    \"use_virtual_mean\": False,\n",
                "    \"virtual_loss\": 3.0,\n",
                "    \"use_torch_compile\": True,\n",
                "    \"use_mixed_precision\": True,\n",
                "    \"use_quantization\": False,\n",
                "}\n",
                "\n",
                "game_config = TicTacToeConfig()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8cbfbda",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Running MuZero Batched Search Max Fast ---\n",
                        "Using default save_intermediate_weights     : False\n",
                        "Using         training_steps                : 20000\n",
                        "Using default adam_epsilon                  : 1e-08\n",
                        "Using default momentum                      : 0.9\n",
                        "Using default learning_rate                 : 0.001\n",
                        "Using default clipnorm                      : 0\n",
                        "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
                        "Using default weight_decay                  : 0.0\n",
                        "Using default num_minibatches               : 1\n",
                        "Using default training_iterations           : 1\n",
                        "Using default lr_schedule_type              : none\n",
                        "Using default lr_schedule_steps             : []\n",
                        "Using default lr_schedule_steps             : []\n",
                        "Using default lr_schedule_values            : []\n",
                        "Using         use_mixed_precision           : True\n",
                        "Using         use_torch_compile             : True\n",
                        "Using default compile_mode                  : reduce-overhead\n",
                        "Using         minibatch_size                : 8\n",
                        "Using         replay_buffer_size            : 100000\n",
                        "Using default min_replay_buffer_size        : 8\n",
                        "Using         n_step                        : 10\n",
                        "Using default discount_factor               : 0.99\n",
                        "Using         per_alpha                     : 0.0\n",
                        "Using         per_beta                      : 0.0\n",
                        "Using         per_beta_final                : 0.0\n",
                        "Using default per_epsilon                   : 1e-06\n",
                        "Using default per_use_batch_weights         : False\n",
                        "Using default per_use_initial_max_priority  : True\n",
                        "Using default loss_function                 : <class 'losses.basic_losses.MSELoss'>\n",
                        "Using default activation                    : relu\n",
                        "Using         kernel_initializer            : None\n",
                        "Using         prob_layer_initializer        : None\n",
                        "Using default norm_type                     : none\n",
                        "Using default soft_update                   : False\n",
                        "Using default min_max_epsilon               : 0.01\n",
                        "Using         world_model_cls               : <class 'modules.world_models.muzero_world_model.MuzeroWorldModel'>\n",
                        "Using         known_bounds                  : [-1, 1]\n",
                        "Using         residual_layers               : [(24, 3, 1)]\n",
                        "Using default conv_layers                   : []\n",
                        "Using default dense_layer_widths            : []\n",
                        "Using default representation_residual_layers: [(24, 3, 1)]\n",
                        "Using default representation_conv_layers    : []\n",
                        "Using default representation_dense_layer_widths: []\n",
                        "Using default dynamics_residual_layers      : [(24, 3, 1)]\n",
                        "Using default dynamics_conv_layers          : []\n",
                        "Using default dynamics_dense_layer_widths   : []\n",
                        "Using         reward_conv_layers            : [(16, 1, 1)]\n",
                        "Using         reward_dense_layer_widths     : []\n",
                        "Using         to_play_conv_layers           : [(16, 1, 1)]\n",
                        "Using         to_play_dense_layer_widths    : []\n",
                        "Using         critic_conv_layers            : [(16, 1, 1)]\n",
                        "Using         critic_dense_layer_widths     : []\n",
                        "Using         actor_conv_layers             : [(16, 1, 1)]\n",
                        "Using         actor_dense_layer_widths      : []\n",
                        "Using default noisy_sigma                   : 0.0\n",
                        "Using default games_per_generation          : 100\n",
                        "Using default value_loss_factor             : 1.0\n",
                        "Using default to_play_loss_factor           : 1.0\n",
                        "Using         num_simulations               : 25\n",
                        "Using         search_batch_size             : 5\n",
                        "Using         use_virtual_mean              : True\n",
                        "Using         virtual_loss                  : 3.0\n",
                        "Using         root_dirichlet_alpha          : 0.25\n",
                        "Using default root_exploration_fraction     : 0.25\n",
                        "Using default root_dirichlet_alpha_adaptive : False\n",
                        "Using         gumbel                        : False\n",
                        "Using         gumbel_m                      : 16\n",
                        "Using default gumbel_cvisit                 : 50\n",
                        "Using default gumbel_cscale                 : 1.0\n",
                        "Using default pb_c_base                     : 19652\n",
                        "Using default pb_c_init                     : 1.25\n",
                        "Using default temperatures                  : [1.0, 0.0]\n",
                        "Using default temperature_updates           : [5]\n",
                        "Using default temperature_with_training_steps: False\n",
                        "Using default clip_low_prob                 : 0.0\n",
                        "Using default value_loss_function           : <losses.basic_losses.MSELoss object at 0x37804ff10>\n",
                        "Using default reward_loss_function          : <losses.basic_losses.MSELoss object at 0x37804e8f0>\n",
                        "Using         policy_loss_function          : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x37804e620>\n",
                        "Using default to_play_loss_function         : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x37804e920>\n",
                        "Using default unroll_steps                  : 5\n",
                        "Using default atom_size                     : 1\n",
                        "Using         support_range                 : None\n",
                        "Using default multi_process                 : True\n",
                        "Using         num_workers                   : 4\n",
                        "Using default lr_ratio                      : inf\n",
                        "Using         transfer_interval             : 100\n",
                        "Using default reanalyze_ratio               : 0.0\n",
                        "Using         use_quantization              : True\n",
                        "Using default reanalyze_method              : mcts\n",
                        "Using default reanalyze_tau                 : 0.3\n",
                        "Using default injection_frac                : 0.0\n",
                        "Using default reanalyze_noise               : False\n",
                        "Using default reanalyze_update_priorities   : False\n",
                        "Using default consistency_loss_factor       : 0.0\n",
                        "Using default projector_output_dim          : 128\n",
                        "Using default projector_hidden_dim          : 128\n",
                        "Using default predictor_output_dim          : 128\n",
                        "Using default predictor_hidden_dim          : 64\n",
                        "Using default mask_absorbing                : True\n",
                        "Using default value_prefix                  : False\n",
                        "Using default lstm_horizon_len              : 5\n",
                        "Using default lstm_hidden_size              : 64\n",
                        "Using default q_estimation_method           : v_mix\n",
                        "Using default stochastic                    : False\n",
                        "Using default use_true_chance_codes         : False\n",
                        "Using default num_chance                    : 32\n",
                        "Using default sigma_loss                    : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x37804e980>\n",
                        "Using default afterstate_residual_layers    : [(24, 3, 1)]\n",
                        "Using default afterstate_conv_layers        : []\n",
                        "Using default afterstate_dense_layer_widths : []\n",
                        "Using default chance_conv_layers            : [(32, 3, 1)]\n",
                        "Using default chance_dense_layer_widths     : [256]\n",
                        "Using default vqvae_commitment_cost_factor  : 1.0\n",
                        "Using default action_embedding_dim          : 32\n",
                        "Using default single_action_plane           : False\n",
                        "Using default latent_viz_method             : umap\n",
                        "Using default latent_viz_interval           : 10\n",
                        "[muzero_batched_bench_fast] Using device: cpu\n",
                        "Observation dimensions: torch.Size([9, 3, 3])\n",
                        "Num actions: 9 (Discrete: True)\n",
                        "Making test env...\n",
                        "Test env configured for video recording.\n",
                        "MARL Agent 'muzero_batched_bench_fast' initialized. Test agents: ['random', 'tictactoe_expert']\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Max size: 100000\n",
                        "Initializing stat 'score' with subkeys None\n",
                        "Initializing stat 'policy_loss' with subkeys None\n",
                        "Initializing stat 'value_loss' with subkeys None\n",
                        "Initializing stat 'reward_loss' with subkeys None\n",
                        "Initializing stat 'to_play_loss' with subkeys None\n",
                        "Initializing stat 'cons_loss' with subkeys None\n",
                        "Initializing stat 'loss' with subkeys None\n",
                        "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
                        "Initializing stat 'episode_length' with subkeys None\n",
                        "Initializing stat 'policy_entropy' with subkeys None\n",
                        "Initializing stat 'value_diff' with subkeys None\n",
                        "Initializing stat 'policy_improvement' with subkeys ['network', 'search']\n",
                        "Initializing stat 'root_children_values' with subkeys None\n",
                        "Initializing stat 'test_score_vs_random' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
                        "Initializing stat 'test_score_vs_tictactoe_expert' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
                        "Compiling models in train()...\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "Hidden state shape: (8, 24, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "encoder input shape (8, 18, 3, 3)\n",
                        "[Worker 3] Starting self-play...\n",
                        "[Worker 1] Starting self-play...\n",
                        "[Worker 2] Starting self-play...\n",
                        "[Worker 0] Starting self-play...\n",
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/2/episode_000000.mp4\n",
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/1/episode_000000.mp4\n",
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/3/episode_000000.mp4\n",
                        "Started recording episode 0 to ./videos/muzero_batched_bench_fast/0/episode_000000.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[W127 12:18:36.629874000 qlinear_dynamic.cpp:251] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n",
                        "[W127 12:18:36.632766000 qlinear_dynamic.cpp:251] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n",
                        "[W127 12:18:36.648843000 qlinear_dynamic.cpp:251] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n",
                        "[W127 12:18:36.649092000 qlinear_dynamic.cpp:251] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stopped recording episode 0. Recorded 6 frames.\n",
                        "Size: 0\n",
                        "Stopped recording episode 0. Recorded 8 frames.\n",
                        "learning\n",
                        "Size: 6\n",
                        "Stopped recording episode 0. Recorded 10 frames.\n",
                        "Size: 14\n",
                        "0\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions Stopped recording episode 0. Recorded 10 frames.\n",
                        "Size: 24\n",
                        "tensor([[7, 0, 4, 8, 1],\n",
                        "        [0, 4, 8, 1, 0],\n",
                        "        [0, 4, 8, 1, 0],\n",
                        "        [4, 8, 1, 0, 3],\n",
                        "        [8, 1, 0, 7, 3],\n",
                        "        [1, 0, 8, 7, 3],\n",
                        "        [6, 2, 1, 0, 8],\n",
                        "        [6, 2, 1, 0, 8]])\n",
                        "target value tensor([[ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
                        "predicted values tensor([[[-0.0952],\n",
                        "         [-0.0879],\n",
                        "         [-0.0825],\n",
                        "         [-0.0574],\n",
                        "         [-0.0306],\n",
                        "         [-0.0227]],\n",
                        "\n",
                        "        [[-0.0811],\n",
                        "         [-0.0393],\n",
                        "         [-0.0410],\n",
                        "         [-0.0014],\n",
                        "         [ 0.0026],\n",
                        "         [-0.0200]],\n",
                        "\n",
                        "        [[-0.0811],\n",
                        "         [-0.0393],\n",
                        "         [-0.0410],\n",
                        "         [-0.0014],\n",
                        "         [ 0.0026],\n",
                        "         [-0.0200]],\n",
                        "\n",
                        "        [[-0.1021],\n",
                        "         [-0.0703],\n",
                        "         [-0.0815],\n",
                        "         [-0.0991],\n",
                        "         [-0.1152],\n",
                        "         [-0.1475]],\n",
                        "\n",
                        "        [[-0.0525],\n",
                        "         [-0.0349],\n",
                        "         [-0.0503],\n",
                        "         [-0.0525],\n",
                        "         [-0.0791],\n",
                        "         [-0.1060]],\n",
                        "\n",
                        "        [[-0.0018],\n",
                        "         [-0.0229],\n",
                        "         [-0.0542],\n",
                        "         [-0.0449],\n",
                        "         [-0.0554],\n",
                        "         [-0.0413]],\n",
                        "\n",
                        "        [[-0.0952],\n",
                        "         [-0.0669],\n",
                        "         [-0.0664],\n",
                        "         [-0.0815],\n",
                        "         [-0.0894],\n",
                        "         [-0.0815]],\n",
                        "\n",
                        "        [[-0.0952],\n",
                        "         [-0.0669],\n",
                        "         [-0.0664],\n",
                        "         [-0.0815],\n",
                        "         [-0.0894],\n",
                        "         [-0.0815]]], dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
                        "target rewards tensor([[0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [-0.1289],\n",
                        "         [-0.1182],\n",
                        "         [-0.0874],\n",
                        "         [-0.0776],\n",
                        "         [-0.0923]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0942],\n",
                        "         [-0.1064],\n",
                        "         [-0.1035],\n",
                        "         [-0.0952],\n",
                        "         [-0.1108]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0942],\n",
                        "         [-0.1064],\n",
                        "         [-0.1035],\n",
                        "         [-0.0952],\n",
                        "         [-0.1108]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0172],\n",
                        "         [-0.0006],\n",
                        "         [-0.0068],\n",
                        "         [-0.0437],\n",
                        "         [-0.0732]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1318],\n",
                        "         [-0.1289],\n",
                        "         [-0.1309],\n",
                        "         [-0.1504],\n",
                        "         [-0.1680]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.1021],\n",
                        "         [-0.0986],\n",
                        "         [-0.1064],\n",
                        "         [-0.1172],\n",
                        "         [-0.1367]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0840],\n",
                        "         [-0.0410],\n",
                        "         [-0.0320],\n",
                        "         [-0.0284],\n",
                        "         [-0.0306]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0840],\n",
                        "         [-0.0410],\n",
                        "         [-0.0320],\n",
                        "         [-0.0284],\n",
                        "         [-0.0306]]], grad_fn=<PermuteBackward0>)\n",
                        "target to plays tensor([[[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.4316, 0.5703],\n",
                        "         [0.4395, 0.5625],\n",
                        "         [0.4492, 0.5508],\n",
                        "         [0.4551, 0.5469],\n",
                        "         [0.4570, 0.5430]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5273, 0.4727],\n",
                        "         [0.5156, 0.4824],\n",
                        "         [0.5039, 0.4961],\n",
                        "         [0.4941, 0.5039],\n",
                        "         [0.4922, 0.5078]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5273, 0.4727],\n",
                        "         [0.5156, 0.4824],\n",
                        "         [0.5039, 0.4961],\n",
                        "         [0.4941, 0.5039],\n",
                        "         [0.4922, 0.5078]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4805, 0.5195],\n",
                        "         [0.4805, 0.5195],\n",
                        "         [0.4805, 0.5195],\n",
                        "         [0.4746, 0.5273],\n",
                        "         [0.4746, 0.5273]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.5078, 0.4922],\n",
                        "         [0.5039, 0.4980],\n",
                        "         [0.4922, 0.5078],\n",
                        "         [0.4824, 0.5195],\n",
                        "         [0.4863, 0.5156]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4648, 0.5352],\n",
                        "         [0.4727, 0.5273],\n",
                        "         [0.4805, 0.5195],\n",
                        "         [0.4785, 0.5195],\n",
                        "         [0.4785, 0.5234]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4336, 0.5664],\n",
                        "         [0.4434, 0.5547],\n",
                        "         [0.4590, 0.5391],\n",
                        "         [0.4707, 0.5273],\n",
                        "         [0.4824, 0.5195]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.4336, 0.5664],\n",
                        "         [0.4434, 0.5547],\n",
                        "         [0.4590, 0.5391],\n",
                        "         [0.4707, 0.5273],\n",
                        "         [0.4824, 0.5195]]], grad_fn=<PermuteBackward0>)\n",
                        "masks tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "Size: 34\n",
                        "Initializing stat 'q_loss' with subkeys None\n",
                        "Initializing stat 'sigma_loss' with subkeys None\n",
                        "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
                        "learning\n",
                        "Size: 42\n",
                        "learning\n",
                        "Size: 49\n",
                        "learning\n",
                        "Size: 57\n",
                        "Size: 67\n",
                        "Size: 73\n",
                        "learning\n",
                        "Size: 82\n",
                        "learning\n",
                        "Size: 88\n",
                        "Size: 98\n",
                        "Size: 107\n",
                        "learning\n",
                        "learning\n",
                        "Size: 116\n",
                        "Size: 125\n",
                        "Size: 135\n",
                        "Size: 145\n",
                        "learning\n",
                        "Size: 155\n",
                        "learning\n",
                        "Size: 162\n",
                        "Size: 172\n",
                        "Size: 182\n",
                        "learning\n",
                        "Size: 191\n",
                        "learning\n",
                        "Size: 198\n",
                        "learning\n",
                        "Size: 207\n",
                        "Size: 215\n",
                        "Size: 225\n",
                        "learning\n",
                        "Size: 235\n",
                        "Size: 243\n",
                        "learning\n",
                        "Size: 251\n",
                        "Size: 261\n",
                        "learning\n",
                        "Size: 271\n",
                        "learning\n",
                        "Size: 281\n",
                        "Size: 290\n",
                        "Size: 300\n",
                        "learning\n",
                        "Size: 310\n",
                        "Size: 317\n",
                        "learning\n",
                        "Size: 327\n",
                        "Size: 335\n",
                        "learning\n",
                        "Size: 345\n",
                        "Size: 353\n",
                        "Size: 363\n",
                        "learning\n",
                        "Size: 369\n",
                        "learning\n",
                        "Size: 378\n",
                        "Size: 387\n",
                        "Size: 397\n",
                        "learning\n",
                        "Size: 406\n",
                        "learning\n",
                        "Size: 414\n",
                        "learning\n",
                        "Size: 422\n",
                        "Size: 430\n",
                        "Size: 439\n",
                        "learning\n",
                        "Size: 448\n",
                        "learning\n",
                        "Size: 456\n",
                        "learning\n",
                        "Size: 462\n",
                        "Size: 472\n",
                        "learning\n",
                        "Size: 482\n",
                        "Size: 490\n",
                        "learning\n",
                        "learning\n",
                        "Size: 499\n",
                        "Size: 509\n",
                        "learning\n",
                        "Size: 519\n",
                        "Size: 529\n",
                        "learning\n",
                        "Size: 539\n",
                        "learning\n",
                        "Size: 549\n",
                        "Size: 559\n",
                        "Size: 568\n",
                        "learning\n",
                        "Size: 576\n",
                        "learning\n",
                        "Size: 583\n",
                        "Size: 593\n",
                        "learning\n",
                        "Size: 601\n",
                        "Size: 608\n",
                        "learning\n",
                        "Size: 618\n",
                        "learning\n",
                        "Size: 626\n",
                        "Size: 636\n",
                        "Size: 644\n",
                        "learning\n",
                        "Size: 652\n",
                        "learning\n",
                        "Size: 658\n",
                        "Size: 666\n",
                        "Size: 676\n",
                        "learning\n",
                        "Size: 686\n",
                        "learning\n",
                        "Size: 696\n",
                        "learning\n",
                        "Size: 706\n",
                        "Size: 715\n",
                        "Size: 725\n",
                        "learning\n",
                        "learning\n",
                        "Size: 734\n",
                        "Size: 744\n",
                        "Size: 753\n",
                        "learning\n",
                        "Size: 761\n",
                        "learning\n",
                        "Size: 769\n",
                        "Size: 777\n",
                        "learning\n",
                        "Size: 785\n",
                        "learning\n",
                        "Size: 795\n",
                        "Size: 805\n",
                        "learning\n",
                        "Size: 812\n",
                        "Size: 818\n",
                        "learning\n",
                        "Size: 828\n",
                        "Size: 837\n",
                        "learning\n",
                        "Size: 845\n",
                        "Size: 852\n",
                        "learning\n",
                        "learning\n",
                        "Size: 860\n",
                        "Size: 868\n",
                        "Size: 878\n",
                        "learning\n",
                        "Size: 888\n",
                        "learning\n",
                        "Size: 898\n",
                        "learning\n",
                        "Size: 907\n",
                        "Size: 917\n",
                        "Size: 927\n",
                        "learning\n",
                        "learning\n",
                        "Size: 934\n",
                        "Size: 942\n",
                        "Size: 950\n",
                        "Size: 959\n",
                        "learning\n",
                        "Size: 969\n",
                        "learning\n",
                        "Size: 975\n",
                        "Size: 983\n",
                        "Size: 992\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n",
                        "learning\n",
                        "100\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[3, 7, 5, 8, 0],\n",
                        "        [5, 6, 4, 0, 2],\n",
                        "        [6, 0, 2, 4, 5],\n",
                        "        [8, 1, 2, 5, 0],\n",
                        "        [8, 6, 0, 2, 2],\n",
                        "        [8, 3, 0, 2, 2],\n",
                        "        [4, 3, 0, 1, 5],\n",
                        "        [8, 1, 2, 3, 6]])\n",
                        "target value tensor([[-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9415, -0.9510,  0.9606, -0.9703,  0.9801, -0.9900]])\n",
                        "predicted values tensor([[[ 0.4062],\n",
                        "         [-0.0835],\n",
                        "         [ 0.1338],\n",
                        "         [ 0.1133],\n",
                        "         [ 0.1235],\n",
                        "         [-0.0781]],\n",
                        "\n",
                        "        [[ 0.2139],\n",
                        "         [ 0.4668],\n",
                        "         [-0.0554],\n",
                        "         [-0.2090],\n",
                        "         [-0.2148],\n",
                        "         [-0.1758]],\n",
                        "\n",
                        "        [[ 0.6133],\n",
                        "         [-0.3926],\n",
                        "         [-0.0596],\n",
                        "         [-0.1494],\n",
                        "         [-0.1592],\n",
                        "         [-0.0654]],\n",
                        "\n",
                        "        [[-0.3340],\n",
                        "         [ 0.3047],\n",
                        "         [-0.0184],\n",
                        "         [-0.0103],\n",
                        "         [ 0.0198],\n",
                        "         [-0.0583]],\n",
                        "\n",
                        "        [[-0.1826],\n",
                        "         [ 0.3633],\n",
                        "         [-0.2109],\n",
                        "         [-0.0859],\n",
                        "         [-0.0107],\n",
                        "         [-0.0435]],\n",
                        "\n",
                        "        [[-0.2969],\n",
                        "         [ 0.3672],\n",
                        "         [ 0.0786],\n",
                        "         [-0.0801],\n",
                        "         [ 0.0111],\n",
                        "         [-0.1094]],\n",
                        "\n",
                        "        [[-0.2773],\n",
                        "         [ 0.1826],\n",
                        "         [-0.0679],\n",
                        "         [-0.0801],\n",
                        "         [-0.0864],\n",
                        "         [ 0.0391]],\n",
                        "\n",
                        "        [[ 0.6133],\n",
                        "         [-0.2578],\n",
                        "         [ 0.0684],\n",
                        "         [-0.1182],\n",
                        "         [-0.0172],\n",
                        "         [-0.1191]]], dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
                        "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [ 0.0004],\n",
                        "         [-0.0221],\n",
                        "         [-0.0023],\n",
                        "         [ 0.1167],\n",
                        "         [ 0.0806]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0026],\n",
                        "         [ 0.1206],\n",
                        "         [ 0.1191],\n",
                        "         [ 0.0149],\n",
                        "         [ 0.0277]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0415],\n",
                        "         [-0.0251],\n",
                        "         [ 0.0126],\n",
                        "         [ 0.0854],\n",
                        "         [ 0.0457]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0183],\n",
                        "         [ 0.0933],\n",
                        "         [ 0.1201],\n",
                        "         [ 0.0923],\n",
                        "         [ 0.0498]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0513],\n",
                        "         [ 0.1001],\n",
                        "         [ 0.0845],\n",
                        "         [ 0.1104],\n",
                        "         [ 0.1299]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1299],\n",
                        "         [ 0.1748],\n",
                        "         [ 0.1230],\n",
                        "         [ 0.1328],\n",
                        "         [ 0.0938]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0703],\n",
                        "         [ 0.0598],\n",
                        "         [ 0.0085],\n",
                        "         [ 0.0262],\n",
                        "         [-0.0138]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0688],\n",
                        "         [ 0.0016],\n",
                        "         [-0.0097],\n",
                        "         [ 0.0688],\n",
                        "         [ 0.1416]]], grad_fn=<PermuteBackward0>)\n",
                        "target to plays tensor([[[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]]])\n",
                        "predicted to_plays tensor([[[0.0000, 0.0000],\n",
                        "         [0.2432, 0.7578],\n",
                        "         [0.5820, 0.4160],\n",
                        "         [0.5625, 0.4395],\n",
                        "         [0.5273, 0.4727],\n",
                        "         [0.3789, 0.6211]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.8164, 0.1855],\n",
                        "         [0.3770, 0.6250],\n",
                        "         [0.4004, 0.6016],\n",
                        "         [0.4238, 0.5742],\n",
                        "         [0.4551, 0.5430]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.1787, 0.8203],\n",
                        "         [0.5859, 0.4141],\n",
                        "         [0.4824, 0.5195],\n",
                        "         [0.5391, 0.4590],\n",
                        "         [0.4941, 0.5078]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.8203, 0.1807],\n",
                        "         [0.3984, 0.6016],\n",
                        "         [0.4453, 0.5547],\n",
                        "         [0.4316, 0.5664],\n",
                        "         [0.4277, 0.5742]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.8555, 0.1436],\n",
                        "         [0.4570, 0.5430],\n",
                        "         [0.4277, 0.5703],\n",
                        "         [0.4746, 0.5273],\n",
                        "         [0.4355, 0.5625]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.8477, 0.1543],\n",
                        "         [0.4941, 0.5039],\n",
                        "         [0.4238, 0.5742],\n",
                        "         [0.4609, 0.5391],\n",
                        "         [0.4668, 0.5312]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.8594, 0.1396],\n",
                        "         [0.4414, 0.5586],\n",
                        "         [0.4316, 0.5703],\n",
                        "         [0.4688, 0.5312],\n",
                        "         [0.4883, 0.5117]],\n",
                        "\n",
                        "        [[0.0000, 0.0000],\n",
                        "         [0.1797, 0.8203],\n",
                        "         [0.7148, 0.2871],\n",
                        "         [0.5391, 0.4590],\n",
                        "         [0.5938, 0.4082],\n",
                        "         [0.5195, 0.4805]]], grad_fn=<PermuteBackward0>)\n",
                        "masks tensor([[ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/2/episode_000100.mp4\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 100. Recorded 9 frames.\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/1/episode_000100.mp4\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/3/episode_000100.mp4\n",
                        "learning\n",
                        "Stopped recording episode 100. Recorded 6 frames.\n",
                        "learning\n",
                        "Started recording episode 100 to ./videos/muzero_batched_bench_fast/0/episode_000100.mp4\n",
                        "learning\n",
                        "Stopped recording episode 100. Recorded 9 frames.\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 100. Recorded 8 frames.\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n",
                        "learning\n",
                        "200\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[4, 0, 7, 1, 0],\n",
                        "        [2, 4, 8, 5, 3],\n",
                        "        [8, 5, 7, 0, 0],\n",
                        "        [7, 0, 7, 1, 0],\n",
                        "        [4, 1, 6, 2, 0],\n",
                        "        [8, 4, 0, 7, 0],\n",
                        "        [2, 1, 4, 0, 0],\n",
                        "        [6, 5, 8, 3, 7]])\n",
                        "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9801, -0.9900,  1.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000]])\n",
                        "predicted values tensor([[[-2.1362e-02],\n",
                        "         [ 2.6758e-01],\n",
                        "         [-1.7871e-01],\n",
                        "         [ 1.6309e-01],\n",
                        "         [-7.2266e-02],\n",
                        "         [-1.4099e-02]],\n",
                        "\n",
                        "        [[ 4.4727e-01],\n",
                        "         [-2.7734e-01],\n",
                        "         [ 1.5625e-01],\n",
                        "         [-2.3730e-01],\n",
                        "         [ 4.3359e-01],\n",
                        "         [-1.0254e-01]],\n",
                        "\n",
                        "        [[ 3.2812e-01],\n",
                        "         [-1.3770e-01],\n",
                        "         [ 3.3984e-01],\n",
                        "         [-1.5430e-01],\n",
                        "         [-3.5524e-05],\n",
                        "         [-1.0254e-01]],\n",
                        "\n",
                        "        [[ 7.3828e-01],\n",
                        "         [-9.0332e-02],\n",
                        "         [ 1.0840e-01],\n",
                        "         [-6.1279e-02],\n",
                        "         [ 2.1582e-01],\n",
                        "         [-1.8750e-01]],\n",
                        "\n",
                        "        [[-2.0312e-01],\n",
                        "         [ 2.3633e-01],\n",
                        "         [-2.0410e-01],\n",
                        "         [ 1.9141e-01],\n",
                        "         [-1.0840e-01],\n",
                        "         [ 4.3945e-02]],\n",
                        "\n",
                        "        [[-3.2617e-01],\n",
                        "         [ 3.4375e-01],\n",
                        "         [-2.5391e-01],\n",
                        "         [ 1.1279e-01],\n",
                        "         [-1.3184e-01],\n",
                        "         [-3.3203e-02]],\n",
                        "\n",
                        "        [[ 1.4453e-01],\n",
                        "         [ 3.8477e-01],\n",
                        "         [-5.4443e-02],\n",
                        "         [ 3.6328e-01],\n",
                        "         [-1.5625e-01],\n",
                        "         [ 9.4238e-02]],\n",
                        "\n",
                        "        [[ 4.1406e-01],\n",
                        "         [-4.3945e-01],\n",
                        "         [ 4.4727e-01],\n",
                        "         [-2.2949e-01],\n",
                        "         [ 5.1562e-01],\n",
                        "         [-1.8652e-01]]], dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
                        "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 1., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 1.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [ 0.2852],\n",
                        "         [ 0.3242],\n",
                        "         [ 0.0366],\n",
                        "         [ 0.1592],\n",
                        "         [-0.0095]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0928],\n",
                        "         [ 0.0610],\n",
                        "         [ 0.1855],\n",
                        "         [ 0.0854],\n",
                        "         [ 0.2578]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1436],\n",
                        "         [ 0.0771],\n",
                        "         [ 0.2227],\n",
                        "         [-0.0194],\n",
                        "         [ 0.0114]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.4180],\n",
                        "         [ 0.1240],\n",
                        "         [ 0.1611],\n",
                        "         [ 0.0601],\n",
                        "         [ 0.1387]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.2344],\n",
                        "         [ 0.2871],\n",
                        "         [ 0.1592],\n",
                        "         [ 0.3340],\n",
                        "         [ 0.0757]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0417],\n",
                        "         [ 0.3242],\n",
                        "         [ 0.0898],\n",
                        "         [ 0.2295],\n",
                        "         [-0.0447]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1299],\n",
                        "         [ 0.1699],\n",
                        "         [ 0.2578],\n",
                        "         [ 0.1934],\n",
                        "         [-0.0835]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0308],\n",
                        "         [-0.1064],\n",
                        "         [ 0.1152],\n",
                        "         [ 0.1074],\n",
                        "         [ 0.2314]]], grad_fn=<PermuteBackward0>)\n",
                        "target to plays tensor([[[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]]])\n",
                        "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
                        "         [9.8828e-01, 1.1169e-02],\n",
                        "         [1.7014e-03, 1.0000e+00],\n",
                        "         [9.8438e-01, 1.7456e-02],\n",
                        "         [2.7618e-03, 9.9609e-01],\n",
                        "         [8.9844e-01, 1.0107e-01]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.3428e-03, 1.0000e+00],\n",
                        "         [9.9609e-01, 3.9978e-03],\n",
                        "         [1.5717e-03, 1.0000e+00],\n",
                        "         [9.9609e-01, 3.0212e-03],\n",
                        "         [1.5717e-03, 1.0000e+00]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.3275e-03, 1.0000e+00],\n",
                        "         [9.9609e-01, 3.7079e-03],\n",
                        "         [1.4114e-03, 1.0000e+00],\n",
                        "         [9.8438e-01, 1.4038e-02],\n",
                        "         [7.5684e-03, 9.9219e-01]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0452e-03, 1.0000e+00],\n",
                        "         [9.9219e-01, 6.1951e-03],\n",
                        "         [3.9978e-03, 9.9609e-01],\n",
                        "         [9.9219e-01, 9.5825e-03],\n",
                        "         [3.4790e-03, 9.9609e-01]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [9.9609e-01, 3.9368e-03],\n",
                        "         [2.1820e-03, 9.9609e-01],\n",
                        "         [9.9609e-01, 4.3335e-03],\n",
                        "         [1.0681e-03, 1.0000e+00],\n",
                        "         [9.7266e-01, 2.8442e-02]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [9.9609e-01, 1.9531e-03],\n",
                        "         [2.0447e-03, 9.9609e-01],\n",
                        "         [9.9609e-01, 4.9133e-03],\n",
                        "         [1.7853e-03, 1.0000e+00],\n",
                        "         [9.9219e-01, 6.8970e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [9.9609e-01, 5.4626e-03],\n",
                        "         [3.4332e-03, 9.9609e-01],\n",
                        "         [9.9609e-01, 3.7689e-03],\n",
                        "         [2.2888e-03, 9.9609e-01],\n",
                        "         [9.9609e-01, 4.4556e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [9.5367e-04, 1.0000e+00],\n",
                        "         [9.9609e-01, 3.2806e-03],\n",
                        "         [9.9945e-04, 1.0000e+00],\n",
                        "         [9.9219e-01, 7.8125e-03],\n",
                        "         [2.1210e-03, 9.9609e-01]]], grad_fn=<PermuteBackward0>)\n",
                        "masks tensor([[ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True]]) tensor([[ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 200 to ./videos/muzero_batched_bench_fast/2/episode_000200.mp4\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 200. Recorded 7 frames.\n",
                        "Started recording episode 200 to ./videos/muzero_batched_bench_fast/1/episode_000200.mp4\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 200 to ./videos/muzero_batched_bench_fast/0/episode_000200.mp4\n",
                        "learning\n",
                        "Stopped recording episode 200. Recorded 8 frames.\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 200. Recorded 8 frames.\n",
                        "Started recording episode 200 to ./videos/muzero_batched_bench_fast/3/episode_000200.mp4\n",
                        "learning\n",
                        "learning\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "Stopped recording episode 200. Recorded 6 frames.\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n",
                        "learning\n",
                        "300\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[6, 2, 5, 7, 0],\n",
                        "        [0, 3, 5, 8, 0],\n",
                        "        [2, 6, 0, 5, 4],\n",
                        "        [8, 4, 1, 7, 0],\n",
                        "        [3, 0, 8, 1, 2],\n",
                        "        [6, 0, 1, 5, 4],\n",
                        "        [0, 5, 6, 7, 1],\n",
                        "        [7, 4, 8, 5, 0]])\n",
                        "target value tensor([[-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9900,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
                        "predicted values tensor([[[-0.3496],\n",
                        "         [ 0.4121],\n",
                        "         [-0.0161],\n",
                        "         [ 0.4551],\n",
                        "         [-0.0942],\n",
                        "         [ 0.3340]],\n",
                        "\n",
                        "        [[-0.0190],\n",
                        "         [ 0.2969],\n",
                        "         [ 0.0786],\n",
                        "         [ 0.5117],\n",
                        "         [-0.0215],\n",
                        "         [ 0.2158]],\n",
                        "\n",
                        "        [[-0.1357],\n",
                        "         [ 0.5312],\n",
                        "         [-0.0679],\n",
                        "         [ 0.3633],\n",
                        "         [-0.0115],\n",
                        "         [ 0.4277]],\n",
                        "\n",
                        "        [[-0.1670],\n",
                        "         [ 0.3848],\n",
                        "         [-0.1533],\n",
                        "         [ 0.4258],\n",
                        "         [-0.0469],\n",
                        "         [ 0.3320]],\n",
                        "\n",
                        "        [[-0.4707],\n",
                        "         [ 0.3887],\n",
                        "         [-0.2041],\n",
                        "         [ 0.4238],\n",
                        "         [-0.1060],\n",
                        "         [ 0.4297]],\n",
                        "\n",
                        "        [[ 0.7695],\n",
                        "         [-0.0845],\n",
                        "         [ 0.3945],\n",
                        "         [-0.0157],\n",
                        "         [ 0.5586],\n",
                        "         [-0.1279]],\n",
                        "\n",
                        "        [[-0.2217],\n",
                        "         [ 0.1787],\n",
                        "         [-0.0898],\n",
                        "         [ 0.4473],\n",
                        "         [-0.0640],\n",
                        "         [ 0.4531]],\n",
                        "\n",
                        "        [[-0.1367],\n",
                        "         [ 0.3438],\n",
                        "         [-0.1611],\n",
                        "         [ 0.3691],\n",
                        "         [ 0.0110],\n",
                        "         [ 0.2832]]], dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
                        "target rewards tensor([[0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 1., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [ 0.0216],\n",
                        "         [ 0.1572],\n",
                        "         [ 0.0762],\n",
                        "         [ 0.1748],\n",
                        "         [ 0.0593]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0339],\n",
                        "         [ 0.2188],\n",
                        "         [ 0.1523],\n",
                        "         [ 0.4082],\n",
                        "         [ 0.0134]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1289],\n",
                        "         [ 0.1523],\n",
                        "         [ 0.0571],\n",
                        "         [ 0.1670],\n",
                        "         [ 0.2305]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1060],\n",
                        "         [ 0.2188],\n",
                        "         [ 0.1777],\n",
                        "         [ 0.2676],\n",
                        "         [ 0.0530]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0245],\n",
                        "         [ 0.0017],\n",
                        "         [ 0.0238],\n",
                        "         [ 0.1416],\n",
                        "         [ 0.1206]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1406],\n",
                        "         [ 0.0588],\n",
                        "         [ 0.2148],\n",
                        "         [ 0.0542],\n",
                        "         [ 0.3516]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0659],\n",
                        "         [-0.0364],\n",
                        "         [ 0.0253],\n",
                        "         [ 0.1196],\n",
                        "         [-0.0101]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1455],\n",
                        "         [ 0.3887],\n",
                        "         [ 0.2598],\n",
                        "         [ 0.3828],\n",
                        "         [ 0.0079]]], grad_fn=<PermuteBackward0>)\n",
                        "target to plays tensor([[[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.]]])\n",
                        "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 6.4468e-04],\n",
                        "         [8.1635e-04, 1.0000e+00],\n",
                        "         [9.9609e-01, 2.4719e-03],\n",
                        "         [8.8120e-04, 1.0000e+00],\n",
                        "         [9.9609e-01, 3.7689e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 1.5717e-03],\n",
                        "         [1.3657e-03, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.7014e-03],\n",
                        "         [3.0136e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.8692e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 4.8828e-04],\n",
                        "         [8.4305e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.5945e-03],\n",
                        "         [3.1471e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.7853e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 1.0834e-03],\n",
                        "         [1.6479e-03, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.3657e-03],\n",
                        "         [1.0300e-03, 1.0000e+00],\n",
                        "         [9.9609e-01, 3.5400e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 4.7302e-04],\n",
                        "         [8.2779e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 7.5531e-04],\n",
                        "         [6.8665e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.5030e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.9531e-03, 9.9609e-01],\n",
                        "         [1.0000e+00, 1.0147e-03],\n",
                        "         [8.9645e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.7014e-03],\n",
                        "         [4.8828e-04, 1.0000e+00]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 2.7847e-04],\n",
                        "         [1.0452e-03, 1.0000e+00],\n",
                        "         [1.0000e+00, 5.4550e-04],\n",
                        "         [5.9891e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.5488e-03]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 6.4468e-04],\n",
                        "         [1.1673e-03, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.1368e-03],\n",
                        "         [3.4523e-04, 1.0000e+00],\n",
                        "         [9.9609e-01, 2.5482e-03]]], grad_fn=<PermuteBackward0>)\n",
                        "masks tensor([[ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "plotting score\n",
                        "plotting policy_loss\n",
                        "plotting value_loss\n",
                        "plotting reward_loss\n",
                        "plotting to_play_loss\n",
                        "plotting cons_loss\n",
                        "plotting loss\n",
                        "plotting episode_length\n",
                        "plotting root_children_values\n",
                        "plotting q_loss\n",
                        "plotting sigma_loss\n",
                        "plotting vqvae_commitment_cost\n",
                        "plotting policy_entropy\n",
                        "plotting value_diff\n",
                        "plotting policy_improvement\n",
                        "  subkey network\n",
                        "  subkey search\n",
                        "plotting latent viz latent_root using umap\n",
                        "  Saving latent viz to checkpoints/muzero_batched_bench_fast/graphs/muzero_batched_bench_fast_latent_root_umap.png\n",
                        "learning\n",
                        "400\n",
                        "actions shape torch.Size([8, 5])\n",
                        "target value shape torch.Size([8, 6])\n",
                        "predicted values shape torch.Size([8, 6, 1])\n",
                        "target rewards shape torch.Size([8, 6])\n",
                        "predicted rewards shape torch.Size([8, 6, 1])\n",
                        "target to plays shape torch.Size([8, 6, 2])\n",
                        "predicted to_plays shape torch.Size([8, 6, 2])\n",
                        "masks shape torch.Size([8, 6]) torch.Size([8, 6])\n",
                        "actions tensor([[0, 0, 4, 2, 3],\n",
                        "        [4, 6, 8, 3, 0],\n",
                        "        [2, 0, 4, 2, 3],\n",
                        "        [7, 0, 3, 6, 8],\n",
                        "        [8, 3, 1, 5, 4],\n",
                        "        [0, 2, 5, 4, 1],\n",
                        "        [4, 5, 8, 2, 1],\n",
                        "        [1, 5, 7, 0, 0]])\n",
                        "target value tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000],\n",
                        "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
                        "        [-0.9321,  0.9415, -0.9510,  0.9606, -0.9703,  0.9801],\n",
                        "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
                        "        [-0.9510,  0.9606, -0.9703,  0.9801, -0.9900,  1.0000],\n",
                        "        [ 0.9606, -0.9703,  0.9801, -0.9900,  1.0000,  0.0000],\n",
                        "        [-0.9703,  0.9801, -0.9900,  1.0000,  0.0000,  0.0000]])\n",
                        "predicted values tensor([[[ 0.3398],\n",
                        "         [-0.0903],\n",
                        "         [ 0.1748],\n",
                        "         [-0.2383],\n",
                        "         [ 0.3418],\n",
                        "         [-0.0244]],\n",
                        "\n",
                        "        [[ 0.1367],\n",
                        "         [-0.2988],\n",
                        "         [ 0.3281],\n",
                        "         [-0.0349],\n",
                        "         [ 0.3984],\n",
                        "         [-0.0640]],\n",
                        "\n",
                        "        [[ 0.6680],\n",
                        "         [ 0.0874],\n",
                        "         [ 0.1846],\n",
                        "         [-0.1523],\n",
                        "         [ 0.3359],\n",
                        "         [ 0.0215]],\n",
                        "\n",
                        "        [[-0.4062],\n",
                        "         [ 0.3398],\n",
                        "         [-0.2119],\n",
                        "         [ 0.2695],\n",
                        "         [-0.2578],\n",
                        "         [ 0.2520]],\n",
                        "\n",
                        "        [[-0.2480],\n",
                        "         [ 0.1221],\n",
                        "         [-0.1050],\n",
                        "         [ 0.3496],\n",
                        "         [-0.1045],\n",
                        "         [ 0.2930]],\n",
                        "\n",
                        "        [[ 0.4824],\n",
                        "         [-0.0859],\n",
                        "         [ 0.3320],\n",
                        "         [-0.0393],\n",
                        "         [ 0.2441],\n",
                        "         [-0.0889]],\n",
                        "\n",
                        "        [[-0.3047],\n",
                        "         [ 0.1406],\n",
                        "         [-0.0444],\n",
                        "         [ 0.2754],\n",
                        "         [ 0.0250],\n",
                        "         [ 0.4219]],\n",
                        "\n",
                        "        [[ 0.3418],\n",
                        "         [-0.1543],\n",
                        "         [ 0.4941],\n",
                        "         [-0.0933],\n",
                        "         [ 0.3633],\n",
                        "         [-0.0452]]], dtype=torch.bfloat16, grad_fn=<PermuteBackward0>)\n",
                        "target rewards tensor([[0., 1., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 1., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 0.],\n",
                        "        [0., 0., 0., 0., 0., 1.],\n",
                        "        [0., 0., 0., 0., 1., 0.]])\n",
                        "predicted rewards tensor([[[ 0.0000],\n",
                        "         [ 0.4219],\n",
                        "         [ 0.0557],\n",
                        "         [ 0.3711],\n",
                        "         [ 0.2012],\n",
                        "         [ 0.1992]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.2578],\n",
                        "         [ 0.2041],\n",
                        "         [ 0.4551],\n",
                        "         [ 0.3359],\n",
                        "         [ 0.2168]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.5586],\n",
                        "         [ 0.1289],\n",
                        "         [ 0.4141],\n",
                        "         [ 0.2539],\n",
                        "         [ 0.2598]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0234],\n",
                        "         [-0.0225],\n",
                        "         [ 0.1040],\n",
                        "         [ 0.2461],\n",
                        "         [ 0.2129]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.0437],\n",
                        "         [-0.0053],\n",
                        "         [ 0.0835],\n",
                        "         [ 0.0894],\n",
                        "         [ 0.2852]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [-0.0369],\n",
                        "         [ 0.0569],\n",
                        "         [ 0.1021],\n",
                        "         [ 0.2969],\n",
                        "         [ 0.3047]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1494],\n",
                        "         [ 0.2441],\n",
                        "         [ 0.2969],\n",
                        "         [ 0.4551],\n",
                        "         [ 0.3477]],\n",
                        "\n",
                        "        [[ 0.0000],\n",
                        "         [ 0.1504],\n",
                        "         [ 0.0713],\n",
                        "         [ 0.1924],\n",
                        "         [ 0.0923],\n",
                        "         [ 0.1787]]], grad_fn=<PermuteBackward0>)\n",
                        "target to plays tensor([[[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.],\n",
                        "         [0., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.]],\n",
                        "\n",
                        "        [[0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.]],\n",
                        "\n",
                        "        [[1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 1.],\n",
                        "         [1., 0.],\n",
                        "         [0., 0.]]])\n",
                        "predicted to_plays tensor([[[0.0000e+00, 0.0000e+00],\n",
                        "         [1.2684e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 5.6076e-04],\n",
                        "         [5.9891e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 2.0981e-04],\n",
                        "         [4.1008e-04, 1.0000e+00]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [5.2643e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 3.0518e-04],\n",
                        "         [1.7357e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 5.1117e-04],\n",
                        "         [4.5776e-04, 1.0000e+00]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.6308e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 6.4468e-04],\n",
                        "         [7.2098e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 3.0518e-04],\n",
                        "         [4.8065e-04, 1.0000e+00]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 2.9182e-04],\n",
                        "         [4.4441e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 5.8746e-04],\n",
                        "         [4.2343e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 3.0136e-04]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 2.0981e-04],\n",
                        "         [1.0681e-03, 1.0000e+00],\n",
                        "         [1.0000e+00, 3.0518e-04],\n",
                        "         [3.5667e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 2.9564e-04]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [6.4468e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 1.8501e-04],\n",
                        "         [3.7384e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 4.5204e-04],\n",
                        "         [7.3242e-04, 1.0000e+00]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [1.0000e+00, 2.5749e-04],\n",
                        "         [3.1471e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 4.1008e-04],\n",
                        "         [2.5368e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 6.8665e-04]],\n",
                        "\n",
                        "        [[0.0000e+00, 0.0000e+00],\n",
                        "         [7.6675e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 2.0981e-04],\n",
                        "         [2.7847e-04, 1.0000e+00],\n",
                        "         [1.0000e+00, 5.9891e-04],\n",
                        "         [3.3569e-04, 1.0000e+00]]], grad_fn=<PermuteBackward0>)\n",
                        "masks tensor([[ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True, False],\n",
                        "        [ True,  True, False, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True, False]]) tensor([[ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True, False, False, False],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True],\n",
                        "        [ True,  True,  True,  True,  True,  True]])\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 300 to ./videos/muzero_batched_bench_fast/2/episode_000300.mp4\n",
                        "Started recording episode 300 to ./videos/muzero_batched_bench_fast/1/episode_000300.mp4\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 300. Recorded 6 frames.\n",
                        "Stopped recording episode 300. Recorded 7 frames.learning\n",
                        "\n",
                        "learning\n",
                        "learning\n",
                        "Started recording episode 300 to ./videos/muzero_batched_bench_fast/0/episode_000300.mp4\n",
                        "learning\n",
                        "Started recording episode 300 to ./videos/muzero_batched_bench_fast/3/episode_000300.mp4\n",
                        "learning\n",
                        "learning\n",
                        "Stopped recording episode 300. Recorded 6 frames.\n",
                        "learning\n",
                        "Stopped recording episode 300. Recorded 8 frames.\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n",
                        "learning\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Process Process-2:\n",
                        "Process Process-4:\n",
                        "Process Process-3:\n",
                        "Process Process-1:\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 368, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1088, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1028, in predict\n",
                        "    self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 151, in run\n",
                        "    min_max_stats,\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 739, in _run_batched_simulations\n",
                        "    )\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 962, in predict_recurrent_inference\n",
                        "    model.recurrent_inference(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 371, in recurrent_inference\n",
                        "    wm_output = self.world_model.recurrent_inference(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/world_models/muzero_world_model.py\", line 269, in recurrent_inference\n",
                        "    reward, next_hidden_state, to_play, reward_hidden = self.dynamics(\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/world_models/muzero_world_model.py\", line 166, in forward\n",
                        "    next_hidden_state = self._fuse_and_process(hidden_state, action)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/world_models/muzero_world_model.py\", line 99, in _fuse_and_process\n",
                        "    x = self.fusion(x)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
                        "    return self._conv_forward(input, self.weight, self.bias)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
                        "    return F.conv2d(\n",
                        "KeyboardInterrupt\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 368, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1088, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1028, in predict\n",
                        "    self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 151, in run\n",
                        "    min_max_stats,\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 739, in _run_batched_simulations\n",
                        "    )\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 962, in predict_recurrent_inference\n",
                        "    model.recurrent_inference(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 379, in recurrent_inference\n",
                        "    value, policy = self.prediction(next_hidden_state)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 62, in forward\n",
                        "    return self.head(S)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 40, in forward\n",
                        "    return self.critic(S), self.actor(S)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/actor.py\", line 51, in forward\n",
                        "    x = self.net(inputs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 126, in forward\n",
                        "    # Dense\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/conv.py\", line 94, in forward\n",
                        "    x = layer(x)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
                        "    input = module(input)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
                        "    return self._conv_forward(input, self.weight, self.bias)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
                        "    return F.conv2d(\n",
                        "KeyboardInterrupt\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 368, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1088, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1028, in predict\n",
                        "    self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 151, in run\n",
                        "    min_max_stats,\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 739, in _run_batched_simulations\n",
                        "    )\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 962, in predict_recurrent_inference\n",
                        "    model.recurrent_inference(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 379, in recurrent_inference\n",
                        "    value, policy = self.prediction(next_hidden_state)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 62, in forward\n",
                        "    return self.head(S)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/agent_nets/muzero.py\", line 40, in forward\n",
                        "    return self.critic(S), self.actor(S)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/actor.py\", line 51, in forward\n",
                        "    x = self.net(inputs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/network_block.py\", line 126, in forward\n",
                        "    # Dense\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../modules/conv.py\", line 94, in forward\n",
                        "    x = layer(x)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
                        "    input = module(input)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
                        "    return self._call_impl(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
                        "    return forward_call(*args, **kwargs)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
                        "    return self._conv_forward(input, self.weight, self.bias)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
                        "    return F.conv2d(\n",
                        "KeyboardInterrupt\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
                        "    self.run()\n",
                        "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
                        "    self._target(*self._args, **self._kwargs)\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 368, in worker_fn\n",
                        "    score, num_steps = self.play_game(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1088, in play_game\n",
                        "    prediction = self.predict(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py\", line 1028, in predict\n",
                        "    self.search.run(\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 151, in run\n",
                        "    min_max_stats,\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/modular_search.py\", line 575, in _run_batched_simulations\n",
                        "    pruned_actionset=pruned_actionset,\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/action_selectors.py\", line 55, in select_child\n",
                        "    tied_actions = [\n",
                        "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../search/action_selectors.py\", line 56, in <listcomp>\n",
                        "    a for a, s in relevant_scores.items() if np.isclose(s, max_score)\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/numpy/core/numeric.py\", line 2350, in isclose\n",
                        "    if all(xfin) and all(yfin):\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2504, in all\n",
                        "    return _wrapreduction(a, np.logical_and, 'all', axis, None, out,\n",
                        "  File \"/opt/homebrew/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 72, in _wrapreduction\n",
                        "    passkwargs = {k: v for k, v in kwargs.items()\n",
                        "KeyboardInterrupt\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m agent_batch\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     27\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 28\u001b[0m \u001b[43magent_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMuZero Batched Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:487\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    477\u001b[0m     (\n\u001b[1;32m    478\u001b[0m         value_loss,\n\u001b[1;32m    479\u001b[0m         policy_loss,\n\u001b[1;32m    480\u001b[0m         reward_loss,\n\u001b[1;32m    481\u001b[0m         to_play_loss,\n\u001b[1;32m    482\u001b[0m         cons_loss,\n\u001b[1;32m    483\u001b[0m         q_loss,\n\u001b[1;32m    484\u001b[0m         sigma_loss,\n\u001b[1;32m    485\u001b[0m         vqvae_commitment_cost,\n\u001b[1;32m    486\u001b[0m         loss,\n\u001b[0;32m--> 487\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
                        "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../agents/muzero.py:698\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_mixed_precision:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n",
                        "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "print(\"--- Running MuZero Batched Search Max Fast ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"num_workers\"] = 4\n",
                "params_batched[\"search_batch_size\"] = 5\n",
                "params_batched[\"use_virtual_mean\"] = True\n",
                "params_batched[\"use_mixed_precision\"] = True\n",
                "params_batched[\"use_torch_compile\"] = True\n",
                "params_batched[\"use_quantization\"] = True\n",
                "params_batched[\"qat\"] = True\n",
                "params_batched[\"transfer_interval\"] = 100\n",
                "\n",
                "# params_batched[\"num_envs_per_worker\"] = 4\n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_fast\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "muzero_iterative_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Iterative Search (Batch=0) ---\")\n",
                "env_iter = TicTacToeConfig().make_env()\n",
                "config_iter = MuZeroConfig(config_dict=params, game_config=game_config)\n",
                "config_iter.search_batch_size = 0  # Explicitly set\n",
                "\n",
                "agent_iter = MuZeroAgent(\n",
                "    env=env_iter,\n",
                "    config=config_iter,\n",
                "    name=\"muzero_iterative_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_iter.checkpoint_interval = 100\n",
                "agent_iter.test_interval = 1000\n",
                "agent_iter.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_iter.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Iterative Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1fe0b3c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Iterative Search (Batch=1) ---\")\n",
                "env_iter = TicTacToeConfig().make_env()\n",
                "config_iter = MuZeroConfig(config_dict=params, game_config=game_config)\n",
                "config_iter.search_batch_size = 1  # Explicitly set\n",
                "\n",
                "agent_iter = MuZeroAgent(\n",
                "    env=env_iter,\n",
                "    config=config_iter,\n",
                "    name=\"muzero_iterative_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_iter.checkpoint_interval = 100\n",
                "agent_iter.test_interval = 1000\n",
                "agent_iter.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_iter.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Iterative Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "muzero_batched_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Batched Search (Batch=5) ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"search_batch_size\"] = 5\n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "config_batch.search_batch_size = 5  # Explicitly set\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_size_5\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e22fc8c",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Batched Search (Batch=5) ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"search_batch_size\"] = 5\n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "config_batch.search_batch_size = 5  # Explicitly set\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_size_5\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f52b0d54",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running MuZero Batched Search (Batch=5) Virtual Mean ---\")\n",
                "params_batched = params.copy()\n",
                "params_batched[\"search_batch_size\"] = 5\n",
                "params_batched[\"use_virtual_mean\"] = True\n",
                "\n",
                "env_batch = TicTacToeConfig().make_env()\n",
                "config_batch = MuZeroConfig(config_dict=params_batched, game_config=game_config)\n",
                "config_batch.search_batch_size = 5  # Explicitly set\n",
                "\n",
                "agent_batch = MuZeroAgent(\n",
                "    env=env_batch,\n",
                "    config=config_batch,\n",
                "    name=\"muzero_batched_bench_size_5_virtual_mean_1\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_batch.checkpoint_interval = 100\n",
                "agent_batch.test_interval = 1000\n",
                "agent_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"MuZero Batched Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gumbel_benchmark",
            "metadata": {},
            "source": [
                "# Gumbel MuZero Benchmark (Iterative vs Batched)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "695b2ba9",
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    \"num_simulations\": 25,\n",
                "    \"per_alpha\": 0.0,\n",
                "    \"per_beta\": 0.0,\n",
                "    \"per_beta_final\": 0.0,\n",
                "    \"n_step\": 10,\n",
                "    \"root_dirichlet_alpha\": 0.25,\n",
                "    \"residual_layers\": [(24, 3, 1)],\n",
                "    \"reward_dense_layer_widths\": [],\n",
                "    \"reward_conv_layers\": [(16, 1, 1)],\n",
                "    \"actor_dense_layer_widths\": [],\n",
                "    \"actor_conv_layers\": [(16, 1, 1)],\n",
                "    \"critic_dense_layer_widths\": [],\n",
                "    \"critic_conv_layers\": [(16, 1, 1)],\n",
                "    \"to_play_dense_layer_widths\": [],\n",
                "    \"to_play_conv_layers\": [(16, 1, 1)],\n",
                "    \"known_bounds\": [-1, 1],\n",
                "    \"support_range\": None,\n",
                "    \"minibatch_size\": 8,\n",
                "    \"replay_buffer_size\": 100000,\n",
                "    \"gumbel\": True,\n",
                "    \"gumbel_m\": 4,\n",
                "    \"policy_loss_function\": KLDivergenceLoss(),\n",
                "    \"training_steps\": 20000,  # Reduced for benchmark speed\n",
                "    \"transfer_interval\": 1,\n",
                "    \"num_workers\": 4,\n",
                "    \"world_model_cls\": MuzeroWorldModel,\n",
                "    \"search_batch_size\": 0,  # Iterative\n",
                "    \"use_virtual_mean\": False,\n",
                "    \"virtual_loss\": 3.0,\n",
                "}\n",
                "\n",
                "game_config = TicTacToeConfig()\n",
                "\n",
                "params_gumbel = params.copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gumbel_iterative_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running Gumbel MuZero Iterative Search (Batch=1) ---\")\n",
                "params_gumbel[\"search_batch_size\"] = 0\n",
                "\n",
                "env_gumbel = TicTacToeConfig().make_env()\n",
                "config_gumbel = MuZeroConfig(config_dict=params_gumbel, game_config=game_config)\n",
                "\n",
                "agent_gumbel = MuZeroAgent(\n",
                "    env=env_gumbel,\n",
                "    config=config_gumbel,\n",
                "    name=\"gumbel_iterative_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_gumbel.checkpoint_interval = 100\n",
                "agent_gumbel.test_interval = 1000\n",
                "agent_gumbel.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_gumbel.train()\n",
                "end_time = time.time()\n",
                "print(f\"Gumbel Iterative Time: {end_time - start_time:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gumbel_batched_run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Running Gumbel MuZero Batched Search (Batch=5) ---\")\n",
                "params_gumbel_batch = params_gumbel.copy()\n",
                "params_gumbel_batch[\"search_batch_size\"] = 5\n",
                "params_gumbel_batch[\"use_virtual_mean\"] = True\n",
                "\n",
                "env_gumbel_batch = TicTacToeConfig().make_env()\n",
                "config_gumbel_batch = MuZeroConfig(\n",
                "    config_dict=params_gumbel_batch, game_config=game_config\n",
                ")\n",
                "\n",
                "agent_gumbel_batch = MuZeroAgent(\n",
                "    env=env_gumbel_batch,\n",
                "    config=config_gumbel_batch,\n",
                "    name=\"gumbel_batched_bench\",\n",
                "    device=\"cpu\",\n",
                "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
                ")\n",
                "agent_gumbel_batch.checkpoint_interval = 100\n",
                "agent_gumbel_batch.test_interval = 1000\n",
                "agent_gumbel_batch.test_trials = 100\n",
                "\n",
                "start_time = time.time()\n",
                "agent_gumbel_batch.train()\n",
                "end_time = time.time()\n",
                "print(f\"Gumbel Batched Time: {end_time - start_time:.2f}s\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
