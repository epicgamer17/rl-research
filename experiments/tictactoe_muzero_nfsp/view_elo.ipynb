{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import pickle\n",
    "from hyperopt import hp\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "\n",
    "def action_function(x):\n",
    "    onehot_action = torch.zeros((3, 3)).view(-1)\n",
    "    onehot_action[x] = 1\n",
    "    return onehot_action.view(1, 3, 3)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01, 0.001, 0.0001, 0.00001]),\n",
    "    \"adam_epsilon\": hp.choice(\"adam_epsilon\", [0.3125, 0.03125, 0.003125, 0.0003125]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    \"residual_layers\": hp.choice(\n",
    "        \"residual_layers\",\n",
    "        [\n",
    "            [(16, 3, 1)] * 1,\n",
    "            [(32, 3, 1)] * 1,\n",
    "            [(64, 3, 1)] * 1,\n",
    "            [(128, 3, 1)] * 1,\n",
    "            [(16, 3, 1)] * 3,\n",
    "            [(32, 3, 1)] * 3,\n",
    "            [(64, 3, 1)] * 3,\n",
    "            [(128, 3, 1)] * 3,\n",
    "            [(16, 3, 1)] * 5,\n",
    "            [(32, 3, 1)] * 5,\n",
    "            [(64, 3, 1)] * 5,\n",
    "            [(128, 3, 1)] * 5,\n",
    "            # [(16, 3, 1)] * 10,\n",
    "            # [(32, 3, 1)] * 10,\n",
    "            # [(64, 3, 1)] * 10,\n",
    "            # [(128, 3, 1)] * 10,\n",
    "            # [(16, 3, 1)] * 20,\n",
    "            # [(32, 3, 1)] * 20,\n",
    "            # [(64, 3, 1)] * 20,\n",
    "            # [(128, 3, 1)] * 20,\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"dense_layers\": hp.choice(\"dense_layers\", [[]]),\n",
    "    \"actor_conv_layers\": hp.choice(\n",
    "        \"actor_conv_layers\", [[], [(32, 1, 1)], [(64, 1, 1)], [(128, 1, 1)]]\n",
    "    ),\n",
    "    \"critic_conv_layers\": hp.choice(\n",
    "        \"critic_conv_layers\", [[], [(32, 1, 1)], [(64, 1, 1)], [(128, 1, 1)]]\n",
    "    ),\n",
    "    \"reward_conv_layers\": hp.choice(\n",
    "        \"reward_conv_layers\", [[], [(32, 1, 1)], [(64, 1, 1)], [(128, 1, 1)]]\n",
    "    ),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\"actor_dense_layer_widths\", [[]]),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\"critic_dense_layer_widths\", [[]]),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\"reward_dense_layer_widths\", [[]]),\n",
    "    \"dense_layer_widths\": hp.choice(\"dense_layer_widths\", [[]]),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"games_per_generation\": hp.choice(\n",
    "        \"games_per_generation\",\n",
    "        [\n",
    "            32,\n",
    "            64,\n",
    "            # 128\n",
    "        ],\n",
    "    ),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [0.25, 1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": hp.choice(\n",
    "        \"num_simulations\",\n",
    "        [\n",
    "            25,\n",
    "            50,\n",
    "            100,\n",
    "            200,\n",
    "            # 400,\n",
    "            # 800\n",
    "        ],\n",
    "    ),\n",
    "    \"num_sampling_moves\": hp.choice(\"num_sampling_moves\", [0, 1, 2, 3, 5, 9]),\n",
    "    \"exploration_temperature\": hp.choice(\"exploration_temperature\", [1.0]),\n",
    "    \"exploitation_temperature\": hp.choice(\"exploitation_temperature\", [0.1]),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": hp.choice(\"training_steps\", [200]),\n",
    "    \"minibatch_size\": hp.choice(\n",
    "        \"minibatch_size\",\n",
    "        [\n",
    "            32,\n",
    "            64,\n",
    "            # 128\n",
    "        ],\n",
    "    ),\n",
    "    \"min_replay_buffer_size\": hp.choice(\n",
    "        \"min_replay_buffer_size\",\n",
    "        [\n",
    "            32,\n",
    "            1024,\n",
    "            # 2048\n",
    "        ],\n",
    "    ),\n",
    "    \"replay_buffer_size\": hp.choice(\"replay_buffer_size\", [4000, 8000, 16000, 32000]),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [9]),\n",
    "    \"clipnorm\": hp.choice(\"clipnorm\", [0.0, 1.0, 10.0]),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-5, 1e-4, 1e-3]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0, 0.5, 1.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0, 0.5, 1.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0, 0.5, 1.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "}\n",
    "\n",
    "initial_best_config = [{}]\n",
    "\n",
    "\n",
    "pickle.dump(search_space, open(\"./search_spaces/search_space.pkl\", \"wb\"))\n",
    "pickle.dump(\n",
    "    initial_best_config,\n",
    "    open(\"./search_spaces/initial_best_config.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd34594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Elo table': Empty DataFrame\n",
      "Columns: [Elo, Games, Score, Draws]\n",
      "Index: [], 'eloAdvantage': 0.0, 'eloDraw': 1000.0}\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "players = []\n",
    "games_per_pair = 10\n",
    "\n",
    "player_names = []\n",
    "table = StandingsTable(player_names, start_elo=1000)\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(table.bayes_elo())\n",
    "print(table.get_win_table())\n",
    "print(table.get_draw_table())\n",
    "file = \"tic_tac_toe_1v1_tournament_results.pkl\"\n",
    "pickle.dump(table, open(file, \"wb\"))\n",
    "\n",
    "\n",
    "def play_1v1_tournament(players, games_per_pair, play_game):\n",
    "    tournament_results = []\n",
    "    for player1 in players:\n",
    "        results = play_matches(player1, players, games_per_pair, play_game)\n",
    "        tournament_results.extend(results)\n",
    "    tournament_results = pd.DataFrame(\n",
    "        tournament_results, columns=[\"player1\", \"player2\", \"result\"]\n",
    "    )\n",
    "    return tournament_results\n",
    "\n",
    "\n",
    "def play_matches(player1, players, games_per_pair, play_game):\n",
    "    results = []\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(\n",
    "                    f\"Playing {player1.model_name} vs {opponent.model_name} game {_+1}\"\n",
    "                )\n",
    "                result = play_game(player1, opponent)\n",
    "                results.append((player1.model_name, opponent.model_name, result))\n",
    "\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(\n",
    "                    f\"Playing {opponent.model_name} vs {player1.model_name} game {_+1}\"\n",
    "                )\n",
    "                result = play_game(opponent, player1)\n",
    "                results.append(\n",
    "                    (\n",
    "                        opponent.model_name,\n",
    "                        player1.model_name,\n",
    "                        result,\n",
    "                    )\n",
    "                )\n",
    "    table.add_results_from_array(results)\n",
    "    print(table.bayes_elo())\n",
    "    pickle.dump(table, open(file, \"wb\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48758b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from packages.utils.utils.utils import process_petting_zoo_obs\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get action from average strategy\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 58/64 [02:12<00:17,  2.89s/it]"
     ]
    }
   ],
   "source": [
    "search_space_path, initial_best_config_path = (\n",
    "    \"./search_spaces/search_space.pkl\",\n",
    "    \"./search_spaces/initial_best_config.pkl\",\n",
    ")\n",
    "search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "eval_method = \"elo\"  # elo?\n",
    "assert (\n",
    "    eval_method == \"final_score\"\n",
    "    or eval_method == \"rolling_average\"\n",
    "    or eval_method == \"final_score_rolling_average\"\n",
    "    or eval_method == \"elo\"\n",
    ")\n",
    "max_trials = 1\n",
    "trials_step = 64  # how many additional trials to do after loading the last ones\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    trials = None\n",
    "\n",
    "for i in range(trials_step):\n",
    "    best = fmin(\n",
    "        fn=objective,  # Objective Function to optimize\n",
    "        space=search_space,  # Hyperparameter's Search Space\n",
    "        algo=tpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "        max_evals=max_trials,  # Number of optimization attempts\n",
    "        trials=trials,  # Record the results\n",
    "        # early_stop_fn=no_progress_loss(5, 1),\n",
    "        trials_save_file=f\"./{file_name}_trials.p\",\n",
    "        # points_to_evaluate=initial_best_config,\n",
    "        show_progressbar=False,\n",
    "    )\n",
    "\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading and Updating...\")\n",
    "    try:\n",
    "        elo_table = table.bayes_elo()[\"Elo table\"]\n",
    "        for trial in range(len(trials.trials)):\n",
    "            trial_elo = elo_table.iloc[trial][\"Elo\"]\n",
    "            print(f\"Trial {trials.trials[trial]['tid']} ELO: {trial_elo}\")\n",
    "            trials.trials[trial][\"result\"][\"loss\"] = -trial_elo\n",
    "            pickle.dump(trials, open(f\"./{file_name}_trials.p\", \"wb\"))\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Not enough players to calculate elo.\")\n",
    "    max_trials = len(trials.trials) + 1\n",
    "    print(best)\n",
    "    best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5d0d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "[{'state': 2, 'tid': 0, 'spec': None, 'result': {'status': 'ok', 'loss': -917}, 'misc': {'tid': 0, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'action_function': [0], 'actor_conv_layers': [0], 'actor_dense_layer_widths': [0], 'adam_epsilon': [0], 'clip_low_prob': [0], 'clipnorm': [0], 'conv_layers': [0], 'critic_conv_layers': [0], 'critic_dense_layer_widths': [0], 'dense_layer_widths': [0], 'dense_layers': [0], 'exploitation_temperature': [0], 'exploration_temperature': [0], 'games_per_generation': [0], 'kernel_initializer': [0], 'known_bounds': [0], 'learning_rate': [0], 'min_replay_buffer_size': [0], 'minibatch_size': [0], 'n_step': [0], 'noisy_sigma': [0], 'num_sampling_moves': [0], 'num_simulations': [0], 'pb_c_base': [0], 'pb_c_init': [0], 'per_alpha': [0], 'per_beta': [0], 'per_beta_final': [0], 'per_epsilon': [0], 'policy_loss_function': [0], 'replay_buffer_size': [0], 'residual_layers': [0], 'reward_conv_layers': [0], 'reward_dense_layer_widths': [0], 'reward_loss_function': [0], 'root_dirichlet_alpha': [0], 'root_exploration_fraction': [0], 'training_steps': [0], 'unroll_steps': [0], 'value_loss_factor': [0], 'value_loss_function': [0], 'weight_decay': [0]}, 'vals': {'action_function': [0], 'actor_conv_layers': [0], 'actor_dense_layer_widths': [0], 'adam_epsilon': [2], 'clip_low_prob': [0], 'clipnorm': [2], 'conv_layers': [0], 'critic_conv_layers': [2], 'critic_dense_layer_widths': [0], 'dense_layer_widths': [0], 'dense_layers': [0], 'exploitation_temperature': [0], 'exploration_temperature': [0], 'games_per_generation': [1], 'kernel_initializer': [3], 'known_bounds': [0], 'learning_rate': [1], 'min_replay_buffer_size': [0], 'minibatch_size': [0], 'n_step': [0], 'noisy_sigma': [0], 'num_sampling_moves': [4], 'num_simulations': [0], 'pb_c_base': [0], 'pb_c_init': [0], 'per_alpha': [1], 'per_beta': [2], 'per_beta_final': [0], 'per_epsilon': [0], 'policy_loss_function': [0], 'replay_buffer_size': [1], 'residual_layers': [9], 'reward_conv_layers': [2], 'reward_dense_layer_widths': [0], 'reward_loss_function': [0], 'root_dirichlet_alpha': [1], 'root_exploration_fraction': [0], 'training_steps': [0], 'unroll_steps': [0], 'value_loss_factor': [0], 'value_loss_function': [0], 'weight_decay': [1]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2025, 9, 17, 16, 34, 54, 92000), 'refresh_time': datetime.datetime(2025, 9, 17, 21, 5, 11, 511000)}, {'state': 2, 'tid': 1, 'spec': None, 'result': {'status': 'ok', 'loss': -1035}, 'misc': {'tid': 1, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'action_function': [1], 'actor_conv_layers': [1], 'actor_dense_layer_widths': [1], 'adam_epsilon': [1], 'clip_low_prob': [1], 'clipnorm': [1], 'conv_layers': [1], 'critic_conv_layers': [1], 'critic_dense_layer_widths': [1], 'dense_layer_widths': [1], 'dense_layers': [1], 'exploitation_temperature': [1], 'exploration_temperature': [1], 'games_per_generation': [1], 'kernel_initializer': [1], 'known_bounds': [1], 'learning_rate': [1], 'min_replay_buffer_size': [1], 'minibatch_size': [1], 'n_step': [1], 'noisy_sigma': [1], 'num_sampling_moves': [1], 'num_simulations': [1], 'pb_c_base': [1], 'pb_c_init': [1], 'per_alpha': [1], 'per_beta': [1], 'per_beta_final': [1], 'per_epsilon': [1], 'policy_loss_function': [1], 'replay_buffer_size': [1], 'residual_layers': [1], 'reward_conv_layers': [1], 'reward_dense_layer_widths': [1], 'reward_loss_function': [1], 'root_dirichlet_alpha': [1], 'root_exploration_fraction': [1], 'training_steps': [1], 'unroll_steps': [1], 'value_loss_factor': [1], 'value_loss_function': [1], 'weight_decay': [1]}, 'vals': {'action_function': [0], 'actor_conv_layers': [0], 'actor_dense_layer_widths': [0], 'adam_epsilon': [0], 'clip_low_prob': [0], 'clipnorm': [2], 'conv_layers': [0], 'critic_conv_layers': [0], 'critic_dense_layer_widths': [0], 'dense_layer_widths': [0], 'dense_layers': [0], 'exploitation_temperature': [0], 'exploration_temperature': [0], 'games_per_generation': [0], 'kernel_initializer': [1], 'known_bounds': [0], 'learning_rate': [0], 'min_replay_buffer_size': [0], 'minibatch_size': [0], 'n_step': [0], 'noisy_sigma': [0], 'num_sampling_moves': [0], 'num_simulations': [2], 'pb_c_base': [0], 'pb_c_init': [0], 'per_alpha': [1], 'per_beta': [0], 'per_beta_final': [2], 'per_epsilon': [0], 'policy_loss_function': [0], 'replay_buffer_size': [1], 'residual_layers': [2], 'reward_conv_layers': [1], 'reward_dense_layer_widths': [0], 'reward_loss_function': [0], 'root_dirichlet_alpha': [2], 'root_exploration_fraction': [0], 'training_steps': [0], 'unroll_steps': [0], 'value_loss_factor': [0], 'value_loss_function': [0], 'weight_decay': [1]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2025, 9, 17, 21, 5, 11, 555000), 'refresh_time': datetime.datetime(2025, 9, 18, 2, 51, 12, 784000)}, {'state': 2, 'tid': 2, 'spec': None, 'result': {'status': 'ok', 'loss': -1047}, 'misc': {'tid': 2, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'action_function': [2], 'actor_conv_layers': [2], 'actor_dense_layer_widths': [2], 'adam_epsilon': [2], 'clip_low_prob': [2], 'clipnorm': [2], 'conv_layers': [2], 'critic_conv_layers': [2], 'critic_dense_layer_widths': [2], 'dense_layer_widths': [2], 'dense_layers': [2], 'exploitation_temperature': [2], 'exploration_temperature': [2], 'games_per_generation': [2], 'kernel_initializer': [2], 'known_bounds': [2], 'learning_rate': [2], 'min_replay_buffer_size': [2], 'minibatch_size': [2], 'n_step': [2], 'noisy_sigma': [2], 'num_sampling_moves': [2], 'num_simulations': [2], 'pb_c_base': [2], 'pb_c_init': [2], 'per_alpha': [2], 'per_beta': [2], 'per_beta_final': [2], 'per_epsilon': [2], 'policy_loss_function': [2], 'replay_buffer_size': [2], 'residual_layers': [2], 'reward_conv_layers': [2], 'reward_dense_layer_widths': [2], 'reward_loss_function': [2], 'root_dirichlet_alpha': [2], 'root_exploration_fraction': [2], 'training_steps': [2], 'unroll_steps': [2], 'value_loss_factor': [2], 'value_loss_function': [2], 'weight_decay': [2]}, 'vals': {'action_function': [0], 'actor_conv_layers': [2], 'actor_dense_layer_widths': [0], 'adam_epsilon': [0], 'clip_low_prob': [0], 'clipnorm': [2], 'conv_layers': [0], 'critic_conv_layers': [2], 'critic_dense_layer_widths': [0], 'dense_layer_widths': [0], 'dense_layers': [0], 'exploitation_temperature': [0], 'exploration_temperature': [0], 'games_per_generation': [1], 'kernel_initializer': [2], 'known_bounds': [0], 'learning_rate': [3], 'min_replay_buffer_size': [1], 'minibatch_size': [1], 'n_step': [0], 'noisy_sigma': [0], 'num_sampling_moves': [3], 'num_simulations': [2], 'pb_c_base': [0], 'pb_c_init': [0], 'per_alpha': [0], 'per_beta': [1], 'per_beta_final': [0], 'per_epsilon': [0], 'policy_loss_function': [0], 'replay_buffer_size': [3], 'residual_layers': [11], 'reward_conv_layers': [2], 'reward_dense_layer_widths': [0], 'reward_loss_function': [0], 'root_dirichlet_alpha': [0], 'root_exploration_fraction': [0], 'training_steps': [0], 'unroll_steps': [0], 'value_loss_factor': [0], 'value_loss_function': [0], 'weight_decay': [1]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2025, 9, 18, 2, 51, 12, 976000), 'refresh_time': datetime.datetime(2025, 9, 18, 14, 19, 12, 467000)}]\n",
      "{'Elo table':                      Elo Games Score Draws\n",
      "tictactoe_muzero_1   917    20  0.15   0.0\n",
      "tictactoe_muzero_2  1035    20  0.65   0.1\n",
      "tictactoe_muzero_3  1047    20   0.7   0.1, 'eloAdvantage': -164.53125, 'eloDraw': 1164.52734375}\n",
      "Trial 0 ELO: 917\n",
      "Trial 1 ELO: 1035\n",
      "Trial 2 ELO: 1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../elo/elo.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  +0.5 * drawTable[i][j] * np.log(\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../elo/elo.py:15: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  +0.5 * drawTable[i][j] * np.log(\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../elo/elo.py:7: RuntimeWarning: overflow encountered in power\n",
      "  return 1.0 / (1 + np.power(10, D / 400))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../elo/elo.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  l += winTable[i][j] * np.log(f(elos[i] - elos[j] - eloAdvantage + eloDraw))\n",
      "/opt/homebrew/lib/python3.10/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../elo/elo.py:14: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  l += winTable[i][j] * np.log(f(elos[i] - elos[j] - eloAdvantage + eloDraw))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../elo/elo.py:15: RuntimeWarning: invalid value encountered in log\n",
      "  +0.5 * drawTable[i][j] * np.log(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "file_name = \"tictactoe_muzero\"\n",
    "\n",
    "trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "print(\"Found saved Trials! Loading...\")\n",
    "\n",
    "print(trials.trials)\n",
    "print(table.bayes_elo())\n",
    "\n",
    "# since the elo only seems to be updating in the trials and not being saved to the pickle file\n",
    "for trial in trials.trials:\n",
    "    trial_elo = trial[\"result\"][\"loss\"]\n",
    "    print(f\"Trial {trial['tid']} ELO: {-trial_elo}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
