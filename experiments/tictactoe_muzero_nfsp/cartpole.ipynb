{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204362f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 10000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using default learning_rate                 : 0.001\n",
      "Using default clipnorm                      : 0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default loss_function                 : <class 'utils.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 256\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using default min_replay_buffer_size        : 256\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : [1, 500]\n",
      "Using         residual_layers               : []\n",
      "Using default conv_layers                   : []\n",
      "Using         dense_layer_widths            : [256, 256]\n",
      "Using default representation_residual_layers: []\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: [256, 256]\n",
      "Using default dynamics_residual_layers      : []\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : [256, 256]\n",
      "Using         reward_conv_layers            : []\n",
      "Using         reward_dense_layer_widths     : []\n",
      "Using         to_play_conv_layers           : []\n",
      "Using         to_play_dense_layer_widths    : []\n",
      "Using         critic_conv_layers            : []\n",
      "Using         critic_dense_layer_widths     : []\n",
      "Using         actor_conv_layers             : []\n",
      "Using         actor_dense_layer_widths      : []\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 0.25\n",
      "Using         to_play_loss_factor           : 0.0\n",
      "Using default weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.25\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 50\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using default temperature_updates           : [5]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using         value_loss_function           : <utils.utils.CategoricalCrossentropyLoss object at 0x3247c2cb0>\n",
      "Using         reward_loss_function          : <utils.utils.CategoricalCrossentropyLoss object at 0x108731540>\n",
      "Using         policy_loss_function          : <utils.utils.CategoricalCrossentropyLoss object at 0x10837b550>\n",
      "Using default to_play_loss_function         : <utils.utils.CategoricalCrossentropyLoss object at 0x3247c2dd0>\n",
      "Using         action_function               : <function action_as_onehot at 0x3247baef0>\n",
      "Using         n_step                        : 10\n",
      "Using         discount_factor               : 0.997\n",
      "Using         unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.5\n",
      "Using         per_beta                      : 1.0\n",
      "Using         per_beta_final                : 1.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : 21\n",
      "Using default multi_process                 : True\n",
      "Using default num_workers                   : 4\n",
      "Using default lr_ratio                      : inf\n",
      "Using         transfer_interval             : 1\n",
      "Using         reanalyze_ratio               : 0.8\n",
      "Using         reanalyze_method              : mcts\n",
      "Using default injection_frac                : 0.0\n",
      "Using default reanalyze_noise               : False\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         gumbel                        : False\n",
      "Using         gumbel_m                      : 16\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using device: cpu\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: <TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "Observation dimensions: (4,)\n",
      "Observation dtype: float32\n",
      "num_actions:  2 <class 'int'>\n",
      "Test agents: []\n",
      "Hidden state shape: (256, 256)\n",
      "Action function output shape: torch.Size([2])\n",
      "torch.Size([256, 258])\n",
      "dynamics input shape torch.Size([256, 258])\n",
      "Hidden state shape: (256, 256)\n",
      "Action function output shape: torch.Size([2])\n",
      "torch.Size([256, 258])\n",
      "dynamics input shape torch.Size([256, 258])\n",
      "Warning: for board games it is recommnded to have n_step >= game length\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 1] Starting self-play...\n",
      "[Worker 2] Starting self-play...[Worker 3] Starting self-play...\n",
      "\n",
      "Buffer size: 12\n",
      "Buffer size: 27\n",
      "Buffer size: 51\n",
      "Buffer size: 73\n",
      "Buffer size: 88\n",
      "Buffer size: 111\n",
      "Buffer size: 126\n",
      "Buffer size: 151\n",
      "Buffer size: 165\n",
      "Buffer size: 183\n",
      "Buffer size: 193\n",
      "Buffer size: 207\n",
      "Buffer size: 236\n",
      "Buffer size: 249\n",
      "Buffer size: 261\n",
      "Buffer size: 272\n",
      "unroll step 0\n",
      "observation tensor([-0.0290, -0.0348, -0.0037, -0.0020])\n",
      "predicted value tensor([0.0240, 0.0240, 0.0220, 0.0246, 0.0230, 0.0240, 0.0229, 0.0226, 0.0232,\n",
      "        0.0236, 0.0222, 0.0213, 0.0222, 0.0242, 0.0233, 0.0239, 0.0250, 0.0230,\n",
      "        0.0252, 0.0241, 0.0241, 0.0253, 0.0241, 0.0231, 0.0239, 0.0220, 0.0220,\n",
      "        0.0242, 0.0225, 0.0221, 0.0238, 0.0230, 0.0238, 0.0231, 0.0241, 0.0232,\n",
      "        0.0213, 0.0223, 0.0220, 0.0230, 0.0230, 0.0233, 0.0223],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target value tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8275, 0.1725, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "predicted reward tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.], grad_fn=<UnbindBackward0>)\n",
      "target reward tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.])\n",
      "predicted policy tensor([0.4928, 0.5072], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.5000, 0.5000])\n",
      "to_play tensor([[0.]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1], dtype=torch.int16)\n",
      "sample losses tensor(0.9404, grad_fn=<MulBackward0>) tensor(0.) tensor(0.6932, grad_fn=<NegBackward0>) tensor(0.)\n",
      "unroll step 1\n",
      "observation tensor([-0.0290, -0.0348, -0.0037, -0.0020])\n",
      "predicted value tensor([0.0242, 0.0242, 0.0217, 0.0246, 0.0232, 0.0246, 0.0224, 0.0226, 0.0232,\n",
      "        0.0237, 0.0223, 0.0216, 0.0217, 0.0246, 0.0232, 0.0238, 0.0250, 0.0229,\n",
      "        0.0248, 0.0242, 0.0236, 0.0248, 0.0240, 0.0231, 0.0239, 0.0228, 0.0213,\n",
      "        0.0242, 0.0225, 0.0217, 0.0238, 0.0228, 0.0239, 0.0230, 0.0241, 0.0231,\n",
      "        0.0216, 0.0227, 0.0221, 0.0228, 0.0235, 0.0243, 0.0220],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target value tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8283, 0.1717, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "predicted reward tensor([0.0234, 0.0231, 0.0198, 0.0204, 0.0214, 0.0208, 0.0247, 0.0236, 0.0263,\n",
      "        0.0249, 0.0268, 0.0209, 0.0213, 0.0205, 0.0279, 0.0230, 0.0226, 0.0280,\n",
      "        0.0258, 0.0283, 0.0216, 0.0249, 0.0197, 0.0260, 0.0221, 0.0202, 0.0264,\n",
      "        0.0266, 0.0230, 0.0227, 0.0225, 0.0199, 0.0216, 0.0251, 0.0222, 0.0251,\n",
      "        0.0171, 0.0234, 0.0265, 0.0281, 0.0206, 0.0190, 0.0219],\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "target reward tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.5848, 0.4152, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "predicted policy tensor([0.5018, 0.4982], grad_fn=<UnbindBackward0>)\n",
      "target policy tensor([0.5000, 0.5000])\n",
      "to_play tensor([[1.]], grad_fn=<UnbindBackward0>)\n",
      "target to_player tensor([1], dtype=torch.int16)\n",
      "sample losses tensor(0.9407, grad_fn=<MulBackward0>) tensor(3.7912, grad_fn=<NegBackward0>) tensor(0.6932, grad_fn=<NegBackward0>) tensor([0.], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [] doesn't match the broadcast shape [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     73\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:387\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_replay_buffer_size:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[1;32m    386\u001b[0m         value_loss, policy_loss, reward_loss, to_play_loss, loss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 387\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m         )\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss)\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/tictactoe_muzero_nfsp/../../muzero/muzero_agent_torch.py:1094\u001b[0m, in \u001b[0;36mMuZeroAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         pol_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m policy_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1093\u001b[0m         tp_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m to_play_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m-> 1094\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m scaled_loss\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;66;03m# compute losses\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [] doesn't match the broadcast shape [1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 284\n",
      "Buffer size: 295\n",
      "Buffer size: 321\n",
      "Buffer size: 334\n",
      "Buffer size: 359\n",
      "Buffer size: 382\n",
      "Buffer size: 399\n",
      "Buffer size: 438\n",
      "Buffer size: 465\n",
      "Buffer size: 491\n",
      "Buffer size: 515\n",
      "Buffer size: 535\n",
      "Buffer size: 562\n",
      "Buffer size: 585\n",
      "Buffer size: 612\n",
      "Buffer size: 630\n",
      "Buffer size: 641\n",
      "Buffer size: 656\n",
      "Buffer size: 672\n",
      "Buffer size: 692\n",
      "Buffer size: 705\n",
      "Buffer size: 717\n",
      "Buffer size: 740\n",
      "Buffer size: 766\n",
      "Buffer size: 790\n",
      "Buffer size: 801\n",
      "Buffer size: 816\n",
      "Buffer size: 847\n",
      "Buffer size: 877\n",
      "Buffer size: 892\n",
      "Buffer size: 919\n",
      "Buffer size: 938\n",
      "Buffer size: 956\n",
      "Buffer size: 972\n",
      "Buffer size: 984\n",
      "Buffer size: 996\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import CartPoleConfig, TicTacToeConfig\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "env = CartPoleConfig().make_env()\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 1.0,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"dense_layer_widths\": [256, 256],\n",
    "    \"residual_layers\": [],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [1, 500],\n",
    "    \"support_range\": 21,\n",
    "    \"minibatch_size\": 256,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 10000,\n",
    "    \"transfer_interval\": 1,\n",
    "    # \"num_workers\": 1,\n",
    "    \"reanalyze_ratio\": 0.8,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"discount_factor\": 0.997,\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 10,\n",
    "}\n",
    "game_config = CartPoleConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"cartpole_reanalyze_test\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[],\n",
    ")\n",
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 500\n",
    "agent.test_trials = 100\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_onehot as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": hp.qloguniform(\n",
    "                #     \"adam_epsilon\", np.log(1e-8), np.log(0.5), 1e-8\n",
    "                # ),\n",
    "                \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 1, 8, 1)),\n",
    "            },\n",
    "            {\n",
    "                \"optimizer\": \"sgd\",\n",
    "                \"momentum\": hp.quniform(\"momentum\", 0, 0.9, 0.1),\n",
    "            },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"learning_rate\": 10 ** (-hp.quniform(\"learning_rate\", 1, 4, 1)),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[0, 500]]),\n",
    "    \"residual_filters\": hp.choice(\"residual_filters\", [[]]),\n",
    "    \"residual_stacks\": hp.choice(\"residual_stacks\", [[]]),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"actor_and_critic_conv_filters\": hp.choice(\"actor_and_critic_conv_filters\", [[]]),\n",
    "    \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "    \"actor_dense_layer_widths\": hp.choice(\n",
    "        \"actor_dense_layer_widths\", [[512], [256], [128], []]\n",
    "    ),\n",
    "    \"critic_dense_layer_widths\": hp.choice(\n",
    "        \"critic_dense_layer_widths\", [[512], [256], [128], []]\n",
    "    ),\n",
    "    \"reward_dense_layer_widths\": hp.choice(\n",
    "        \"reward_dense_layer_widths\", [[512], [256], [128], []]\n",
    "    ),\n",
    "    \"dense_layer_widths\": hp.choice(\n",
    "        \"dense_layer_widths\", [[512, 64], [256, 64], [128, 64]]\n",
    "    ),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": hp.quniform(\n",
    "        \"root_dirichlet_alpha\", 0.1, 5.0, 0.1\n",
    "    ),  # hp.choice(\"root_dirichlet_alpha\", [0.3, 1.0, 2.0]),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        hp.qloguniform(\"num_simulations\", np.log(25), np.log(50), 25)\n",
    "    ),\n",
    "    # \"temperature_updates\": hp.choice(\"temperature_updates\", [[15000, 30000]]),\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.5, 0.25]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [True]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\n",
    "        \"value_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"reward_loss_function\": hp.choice(\n",
    "        \"reward_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(hp.quniform(\"training_steps\", 15000, 45000, 15000)),\n",
    "    # \"minibatch_size\": scope.int(\n",
    "    #     hp.qloguniform(\"minibatch_size\", np.log(8), np.log(64), 8)\n",
    "    # ),\n",
    "    # \"min_replay_buffer_size\": scope.int(\n",
    "    #     hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(10000), 1000)\n",
    "    # ),\n",
    "    # \"replay_buffer_size\": scope.int(\n",
    "    #     hp.qloguniform(\"replay_buffer_size\", np.log(10000), np.log(200000), 10000)\n",
    "    # ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 7, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        hp.qloguniform(\"min_replay_buffer_size\", np.log(1000), np.log(20000), 1000)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(10 ** (hp.quniform(\"replay_buffer_size\", 4, 6, 1))),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [10]),\n",
    "    \"clipnorm\": scope.int(hp.quniform(\"clipnorm\", 0, 10.0, 1)),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 1, 3, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "    \"support_range\": scope.int(hp.quniform(\"support_range\", 5, 15, 5)),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_params(params):\n",
    "    params[\"residual_layers\"] = []\n",
    "    params[\"actor_conv_layers\"] = []\n",
    "    params[\"critic_conv_layers\"] = []\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "    params[\"temperature_updates\"] = [\n",
    "        params[\"training_steps\"] / 3,\n",
    "        2 * params[\"training_steps\"] / 3,\n",
    "    ]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    sarl_objective,\n",
    "    set_sarl_config,\n",
    "    SarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from game_configs import CartPoleConfig\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"cartpole_muzero\"\n",
    "max_trials = 1\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "set_sarl_config(\n",
    "    SarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"rolling_average\",\n",
    "        make_env=CartPoleConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=CartPoleConfig,\n",
    "        checkpoint_interval=50,\n",
    "        test_interval=100,\n",
    "        test_trials=25,\n",
    "        last_n_rolling_avg=10,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=sarl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
