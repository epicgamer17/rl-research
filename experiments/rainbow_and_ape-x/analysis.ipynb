{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rainbow and Ape-X Expiriments \n",
    "    1. We release a set of hyper parameters for CartPole-v1 and Classic Control and Atari\n",
    "    2. We release code for Rainbow that can train X steps in Y minutes on a Mac M2 Chip\n",
    "    3. We also release a version of Ape-X as described in the original paper, and an Ape-X with rainbow\n",
    "        1. Compare results of each \n",
    "        2. Compare Ape-X with different Rainbow components added or removed\n",
    "    4. We compare the different models of DQN as seen in their papers to rainbow, the different individual components to rainbow, and rainbow with individual components removed\n",
    "    10. Compare rainbow training speeds with different levels of numerical precision and datatypes\n",
    "        1. Mixed precision using torch.amp \n",
    "        2. Lower matmul precision\n",
    "            1. comparing medium, high, and highest \n",
    "            2. https://pytorch.org/docs/master/generated/torch.set_float32_matmul_precision.html?highlight=precision#torch.set_float32_matmul_precision\n",
    "    11. Ape-X Hyper parameter sweep and sensitivities\n",
    "    12. Exploration methods for Rainbow Ape-X\n",
    "        1. Just noisy nets (same for all actors)\n",
    "        2. Noisy nets and varying epsilon \n",
    "        3. Adding a constant that changes variance of noisy nets for action selection\n",
    "        4. AlphaStar Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow on CartPole-v1\n",
    "Hyperparameters are based on the hyperopt experiments, quantized trial 27 with some minor changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "sys.path.append('../..')\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "config_dict = {\n",
    "  \"dense_layers_widths\": [128, 128],\n",
    "  \"value_hidden_layers_widths\": [64, 64],\n",
    "  \"advatage_hidden_layers_widths\": [64, 64],\n",
    "  \"adam_epsilon\": 0.00375,\n",
    "  \"learning_rate\": 0.005,\n",
    "  \"training_steps\": 10000,\n",
    "  \"per_epsilon\": 0.05,\n",
    "  \"per_alpha\": 0.8,\n",
    "  \"per_beta\": 0.45,\n",
    "  \"minibatch_size\": 128,\n",
    "  \"replay_buffer_size\": 10000,\n",
    "  \"min_replay_buffer_size\": 1250,\n",
    "  \"transfer_interval\": 10,\n",
    "  \"n_step\": 9,\n",
    "  \"kernel_initializer\": \"glorot_uniform\",\n",
    "  \"loss_function\": KLDivergenceLoss(), # could do categorical cross entropy \n",
    "  \"clipnorm\": 2.0,\n",
    "  \"discount_factor\": 0.99,\n",
    "  \"atom_size\": 81,\n",
    "  \"replay_interval\": 4,\n",
    "}\n",
    "game_config = CartPoleConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_CartPole-v1\", device=device)\n",
    "\n",
    "for param in agent.model.parameters():\n",
    "  print(param)\n",
    "print(\"start\")\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow on Classic Control\n",
    "Hyperparameters come from revisitting rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "import torch \n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "sys.path.append('../..')\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import ClassicControlConfig\n",
    "\n",
    "config_dict = {\n",
    "  \"dense_layers_widths\": [512, 512],\n",
    "  \"value_hidden_layers_widths\": [], # \n",
    "  \"advatage_hidden_layers_widths\": [], # \n",
    "  \"adam_epsilon\": 3.125e-4,\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"training_steps\": 30000,\n",
    "  \"per_epsilon\": 1e-6, # \n",
    "  \"per_alpha\": 0.5,\n",
    "  \"per_beta\": 0.5, # For RIAYN should be no annealing # 0.4\n",
    "  \"minibatch_size\": 128,\n",
    "  \"replay_buffer_size\": 50000,\n",
    "  \"min_replay_buffer_size\": 500,\n",
    "  \"transfer_interval\": 100,\n",
    "  \"n_step\": 3,\n",
    "  \"kernel_initializer\": \"orthogonal\", #\n",
    "  \"loss_function\": KLDivergenceLoss(), # KLDivergence()\n",
    "  \"clipnorm\": 0.0, # 2.0 \n",
    "  \"discount_factor\": 0.99,\n",
    "  \"atom_size\": 51,\n",
    "  \"replay_interval\": 2,\n",
    "}\n",
    "game_config = ClassicControlConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# game_config.v_min = 0\n",
    "# game_config.v_max = 500\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "# agent = RainbowAgent(env, config, name=\"Rainbow_ClassicControl_CartPole-v1\", device=device)\n",
    "# agent.train()\n",
    "\n",
    "# game_config.v_min = -500\n",
    "# game_config.v_max = 0\n",
    "# env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "# agent = RainbowAgent(env, config, name=\"Rainbow_ClassicControl_Acrobot-v1\", device=device)\n",
    "# agent.train()\n",
    "\n",
    "# game_config.v_min = -200\n",
    "# game_config.v_max = 200\n",
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "# agent = RainbowAgent(env, config, name=\"Rainbow_ClassicControl_LunarLander-v2\", device=device)\n",
    "# agent.train()\n",
    "\n",
    "# game_config.v_min = -200\n",
    "# game_config.v_max = -100\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\", max_episode_steps=600)\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_ClassicControl_MountainCar-v0\", device=device)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow on Atari\n",
    "Takes around 18GB RAM, and many hours even just to fill replay buffer with a Mac M2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainbow MsPacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "observation_buffer = np.zeros((3, 1, 2), dtype=np.object_)\n",
    "print(observation_buffer)\n",
    "observation_buffer[0] = [1, 1]\n",
    "print(observation_buffer)\n",
    "observation_buffer = np.array(observation_buffer, dtype=np.int8)\n",
    "print(observation_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "sys.path.append('../..')\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import AtariConfig\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "\n",
    "config_dict = {\n",
    "  \"conv_layers\": [\n",
    "      (32, 8, 4),\n",
    "      (64, 4, 2),\n",
    "      (64, 3, 1),\n",
    "  ],\n",
    "  \"dense_layers_widths\": [512], \n",
    "  \"value_hidden_layers_widths\": [], # \n",
    "  \"advatage_hidden_layers_widths\": [], # \n",
    "  \"adam_epsilon\": 1.5e-4,\n",
    "  \"learning_rate\": 0.00025/4,\n",
    "  \"training_steps\": 50000000, # Agent saw 200,000,000 frames\n",
    "  \"per_epsilon\": 1e-6, # \n",
    "  \"per_alpha\": 0.5, \n",
    "  \"per_beta\": 0.4, \n",
    "  \"minibatch_size\": 32,\n",
    "  \"replay_buffer_size\": 1000000,\n",
    "  \"min_replay_buffer_size\": 80000, #80000\n",
    "  \"transfer_interval\": 32000,\n",
    "  \"n_step\": 3,\n",
    "  \"kernel_initializer\": \"orthogonal\", #\n",
    "  \"loss_function\": KLDivergenceLoss(),\n",
    "  \"clipnorm\": 0.0, # \n",
    "  \"discount_factor\": 0.99,\n",
    "  \"atom_size\": 51,\n",
    "  \"replay_interval\": 4,\n",
    "}\n",
    "game_config = AtariConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "\n",
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "    \n",
    "env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\", max_episode_steps=108000)\n",
    "env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
    "env = FrameStack(env, 4, lz4_compress=True)\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_Atari_MsPacmanNoFrameskip-v4\")\n",
    "agent.checkpoint_interval = 1000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow with  turn-based zero-sum 2-player perfect information deterministic games\n",
    "Not working yet because of zero sum rewards, could get it to work by doing an NFSP sort of thing when storing experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainbow Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Framestacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "from packages.game_configs.game_configs.tictactoe_config import TicTacToeConfig\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "sys.path.append('../..')\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import AtariConfig\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "\n",
    "config_dict = {\n",
    "  \"conv_layers\": [\n",
    "      (32, 8, 4),\n",
    "      (64, 4, 2),\n",
    "      (64, 3, 1),\n",
    "  ],\n",
    "  \"dense_layers_widths\": [512], \n",
    "  \"value_hidden_layers_widths\": [], # \n",
    "  \"advatage_hidden_layers_widths\": [], # \n",
    "  \"adam_epsilon\": 1.5e-4,\n",
    "  \"learning_rate\": 0.00025/4,\n",
    "  \"training_steps\": 50000000, # Agent saw 200,000,000 frames\n",
    "  \"per_epsilon\": 1e-6, # \n",
    "  \"per_alpha\": 0.5, \n",
    "  \"per_beta\": 0.4, \n",
    "  \"minibatch_size\": 32,\n",
    "  \"replay_buffer_size\": 1000000,\n",
    "  \"min_replay_buffer_size\": 80000,\n",
    "  \"transfer_interval\": 32000,\n",
    "  \"n_step\": 3,\n",
    "  \"kernel_initializer\": \"orthogonal\", #\n",
    "  \"loss_function\": KLDivergenceLoss(),\n",
    "  \"clipnorm\": 0.0, # \n",
    "  \"discount_factor\": 0.99,\n",
    "  \"atom_size\": 51,\n",
    "  \"replay_interval\": 4,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "\n",
    "env = gym.make(\"TicTacToe-v0\", render_mode=\"rgb_array\")\n",
    "env = FrameStack(env, 4)\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_TicTacToe-v0\")\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No framestacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "from packages.game_configs.game_configs.tictactoe_config import TicTacToeConfig\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "sys.path.append('../..')\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import AtariConfig\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "\n",
    "config_dict = {\n",
    "  \"conv_layers\": [\n",
    "      (32, 8, 4),\n",
    "      (64, 4, 2),\n",
    "      (64, 3, 1),\n",
    "  ],\n",
    "  \"dense_layers_widths\": [512], \n",
    "  \"value_hidden_layers_widths\": [], # \n",
    "  \"advatage_hidden_layers_widths\": [], # \n",
    "  \"adam_epsilon\": 1.5e-4,\n",
    "  \"learning_rate\": 0.00025/4,\n",
    "  \"training_steps\": 50000000, # Agent saw 200,000,000 frames\n",
    "  \"per_epsilon\": 1e-6, # \n",
    "  \"per_alpha\": 0.5, \n",
    "  \"per_beta\": 0.4, \n",
    "  \"minibatch_size\": 32,\n",
    "  \"replay_buffer_size\": 1000000,\n",
    "  \"min_replay_buffer_size\": 80000,\n",
    "  \"transfer_interval\": 32000,\n",
    "  \"n_step\": 3,\n",
    "  \"kernel_initializer\": \"orthogonal\", #\n",
    "  \"loss_function\": KLDivergenceLoss(),\n",
    "  \"clipnorm\": 0.0, # \n",
    "  \"discount_factor\": 0.99,\n",
    "  \"atom_size\": 51,\n",
    "  \"replay_interval\": 4,\n",
    "}\n",
    "game_config = TicTacToeConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "\n",
    "env = gym.make(\"TicTacToe-v0\", render_mode=\"rgb_array\")\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_TicTacToe-v0\")\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainbow Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainbow Chess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
