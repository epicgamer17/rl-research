{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5607c5be",
   "metadata": {},
   "source": [
    "*** TO DO FOR CATAN: ***\n",
    "RAINBOW: \n",
    "    1. vs Random\n",
    "    2. vs Weighted Random\n",
    "    3. vs MTCS\n",
    "    4. vs Victory Point\n",
    "    5. vs AlphaBeta\n",
    "Masked PPO the same \n",
    "NFSP \n",
    "MuZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d2733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 20000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using         learning_rate                 : 0.0003\n",
      "Using         clipnorm                      : 10\n",
      "Using         optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default lr_schedule_type              : none\n",
      "Using default lr_schedule_steps             : []\n",
      "Using default lr_schedule_steps             : []\n",
      "Using default lr_schedule_values            : []\n",
      "Using         use_mixed_precision           : True\n",
      "Using         compile                       : True\n",
      "Using default compile_mode                  : default\n",
      "Using         minibatch_size                : 32\n",
      "Using         replay_buffer_size            : 20000\n",
      "Using         min_replay_buffer_size        : 32\n",
      "Using         n_step                        : 500\n",
      "Using         discount_factor               : 1.0\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_use_initial_max_priority  : True\n",
      "Using default loss_function                 : <class 'losses.basic_losses.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         prob_layer_initializer        : None\n",
      "Using default norm_type                     : none\n",
      "Using default soft_update                   : False\n",
      "Using         min_max_epsilon               : 0.01\n",
      "Using         world_model_cls               : <class 'modules.world_models.muzero_world_model.MuzeroWorldModel'>\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : []\n",
      "Using default conv_layers                   : []\n",
      "Using         dense_layer_widths            : [256, 256, 256]\n",
      "Using default representation_residual_layers: []\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: [256, 256, 256]\n",
      "Using default dynamics_residual_layers      : []\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : [256, 256, 256]\n",
      "Using         reward_conv_layers            : []\n",
      "Using         reward_dense_layer_widths     : [32]\n",
      "Using         to_play_conv_layers           : []\n",
      "Using         to_play_dense_layer_widths    : [32]\n",
      "Using         critic_conv_layers            : []\n",
      "Using         critic_dense_layer_widths     : [32]\n",
      "Using         actor_conv_layers             : []\n",
      "Using         actor_dense_layer_widths      : [32]\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using         to_play_loss_factor           : 2.0\n",
      "Using         num_simulations               : 64\n",
      "Using         search_batch_size             : 8\n",
      "Using         use_virtual_mean              : True\n",
      "Using default virtual_loss                  : 3.0\n",
      "Using         root_dirichlet_alpha          : 0.03\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using default root_dirichlet_alpha_adaptive : False\n",
      "Using         gumbel                        : False\n",
      "Using         gumbel_m                      : 16\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using         temperature_updates           : [30]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default value_loss_function           : <losses.basic_losses.MSELoss object at 0x11759d880>\n",
      "Using default reward_loss_function          : <losses.basic_losses.MSELoss object at 0x117ca33e0>\n",
      "Using         policy_loss_function          : <losses.basic_losses.KLDivergenceLoss object at 0x11769a480>\n",
      "Using default to_play_loss_function         : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x1174359a0>\n",
      "Using default unroll_steps                  : 5\n",
      "Using default atom_size                     : 1\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using         num_workers                   : 5\n",
      "Using         num_envs_per_worker           : 6\n",
      "Using         lr_ratio                      : 1.0\n",
      "Using         transfer_interval             : 1\n",
      "Using         reanalyze_ratio               : 0.0\n",
      "Using default quantize                      : False\n",
      "Using         reanalyze_method              : mcts\n",
      "Using default reanalyze_tau                 : 0.3\n",
      "Using         injection_frac                : 0.25\n",
      "Using         reanalyze_noise               : False\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         consistency_loss_factor       : 0.0\n",
      "Using         projector_output_dim          : 128\n",
      "Using         projector_hidden_dim          : 128\n",
      "Using         predictor_output_dim          : 128\n",
      "Using         predictor_hidden_dim          : 64\n",
      "Using default mask_absorbing                : True\n",
      "Using         value_prefix                  : False\n",
      "Using         lstm_horizon_len              : 5\n",
      "Using         lstm_hidden_size              : 64\n",
      "Using         q_estimation_method           : v_mix\n",
      "Using         stochastic                    : True\n",
      "Using default use_true_chance_codes         : False\n",
      "Using default num_chance                    : 32\n",
      "Using default sigma_loss                    : <losses.basic_losses.CategoricalCrossentropyLoss object at 0x1173a3d40>\n",
      "Using default afterstate_residual_layers    : []\n",
      "Using default afterstate_conv_layers        : []\n",
      "Using default afterstate_dense_layer_widths : [256, 256, 256]\n",
      "Using         chance_conv_layers            : []\n",
      "Using         chance_dense_layer_widths     : [32]\n",
      "Using         vqvae_commitment_cost_factor  : 1.0\n",
      "Using default action_embedding_dim          : 32\n",
      "Using default single_action_plane           : False\n",
      "Using default latent_viz_method             : umap\n",
      "Using default latent_viz_interval           : 10\n",
      "[catan_mini_map_stochastic_higher_to_play_loss_vectorized] Using device: cpu\n",
      "Observation dimensions: torch.Size([263])\n",
      "Num actions: 290 (Discrete: True)\n",
      "Making test env...\n",
      "Test env configured for video recording.\n",
      "MARL Agent 'catan_mini_map_stochastic_higher_to_play_loss_vectorized' initialized. Test agents: ['RandomPlayer']\n",
      "Hidden state shape: (32, 256)\n",
      "Hidden state shape: (32, 256)\n",
      "encoder input shape (32, 526)\n",
      "Hidden state shape: (32, 256)\n",
      "Hidden state shape: (32, 256)\n",
      "encoder input shape (32, 526)\n",
      "Compiling models...\n",
      "Max size: 20000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'cons_loss' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'policy_entropy' with subkeys None\n",
      "Initializing stat 'value_diff' with subkeys None\n",
      "Initializing stat 'policy_improvement' with subkeys ['network', 'search']\n",
      "Initializing stat 'root_children_values' with subkeys None\n",
      "Initializing stat 'test_score_vs_RandomPlayer' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'num_codes' with subkeys None\n",
      "Initializing stat 'chance_probs' with subkeys None\n",
      "Initializing stat 'chance_entropy' with subkeys None\n",
      "Initializing stat 'q_loss' with subkeys None\n",
      "Initializing stat 'sigma_loss' with subkeys None\n",
      "Initializing stat 'vqvae_commitment_cost' with subkeys None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker 0] Starting self-play with 6 vector envs...\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/0_0/episode_000000.mp4\n",
      "[Worker 1] Starting self-play with 6 vector envs...\n",
      "[Worker 2] Starting self-play with 6 vector envs...\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/0_1/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/0_2/episode_000000.mp4\n",
      "[Worker 3] Starting self-play with 6 vector envs...\n",
      "[Worker 4] Starting self-play with 6 vector envs...\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/0_3/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/0_4/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/2_0/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/1_0/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/0_5/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/2_1/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/1_1/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/4_0/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/3_0/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/2_2/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/1_2/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/2_3/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/1_3/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/4_1/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/3_1/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/2_4/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/1_4/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/4_2/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/3_2/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/2_5/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/1_5/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/3_3/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/4_3/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/3_4/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/4_4/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/3_5/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_mini_map_stochastic_higher_to_play_loss_vectorized/4_5/episode_000000.mp4\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from losses.basic_losses import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs.muzero_config import MuZeroConfig\n",
    "from modules.world_models.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "from catanatron import RandomPlayer, Color\n",
    "from wrappers import record_video_wrapper\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Note: The agent will now create its own worker environments using config.game.make_env()\n",
    "# We still initialize one here to determine observation/action dimensions.\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"MINI\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=True,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"num_simulations\": 64,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 500,\n",
    "    \"residual_layers\": [],\n",
    "    \"dense_layer_widths\": [256] * 3,\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"chance_dense_layer_widths\": [32],\n",
    "    \"chance_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 20000,\n",
    "    \"lr_ratio\": 1.0,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"num_workers\": 5,\n",
    "    \"num_envs_per_worker\": 6,  # Added: Each worker now runs 4 envs in parallel (Total 20 envs)\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"root_dirichlet_alpha\": 0.03,\n",
    "    \"optimizer\": Adam,\n",
    "    \"temperature_updates\": [30],\n",
    "    \"to_play_loss_factor\": 2.0,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 20000,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": False,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"injection_frac\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    \"value_prefix\": False,\n",
    "    \"lstm_horizon_len\": 5,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"q_estimation_method\": \"v_mix\",\n",
    "    \"clipnorm\": 10,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 1.0,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "    \"discount_factor\": 1.0,\n",
    "    \"search_batch_size\": 8,\n",
    "    \"use_virtual_mean\": True,\n",
    "    \"min_max_epsilon\": 0.01,\n",
    "    \"use_mixed_precision\": True,\n",
    "    \"compile\": True,\n",
    "}\n",
    "\n",
    "game_config = CatanConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"catan_mini_map_stochastic_higher_to_play_loss_vectorized\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[\n",
    "        CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "    ],\n",
    ")\n",
    "\n",
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 10\n",
    "\n",
    "# The train() call will now automatically use play_game_vec inside the workers\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from losses.basic_losses import (\n",
    "    CategoricalCrossentropyLoss,\n",
    "    MSELoss,\n",
    "    generate_layer_widths,\n",
    ")\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    # \"residual_filters\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    # ),\n",
    "    # \"residual_stacks\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    # ),\n",
    "    \"residual_layers\": hp.choice(\"residual_layers\", [[]]),\n",
    "    \"actor_conv_layers\": hp.choice(\"actor_conv_layers\", [[]]),\n",
    "    \"critic_conv_layers\": hp.choice(\"critic_conv_layers\", [[]]),\n",
    "    \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "    \"to_play_conv_layers\": hp.choice(\"to_play_conv_layers\", [[]]),\n",
    "    \"output_layer_widths\": scope.int(\n",
    "        hp.quniform(\"output_layer_widths\", 16, 16 + 1e-8, 16)\n",
    "    ),\n",
    "    \"dense_layer_width\": scope.int(2 ** hp.quniform(\"dense_layer_width\", 6, 9, 1)),\n",
    "    \"dense_layers\": scope.int(hp.quniform(\"dense_layers\", 1, 1 + 1e-8, 1)),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": 2 ** hp.quniform(\"value_loss_factor\", 0.0, 0.0 + 1e-8, 1.0),\n",
    "    \"to_play_loss_factor\": 2 ** hp.quniform(\"to_play_loss_factor\", 0.0, 1.0, 1.0),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -5, 0, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        800 * 2 ** hp.quniform(\"num_simulations\", -5, -4 + 1e-8, 1)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 32, 512, 32))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"to_play_loss_function\": hp.choice(\n",
    "        \"to_play_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(500), np.log(1000), 100)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 11, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        2 ** hp.quniform(\"min_replay_buffer_size\", 5, 10, 1)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [750]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\",\n",
    "        [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))],\n",
    "        # \"clipnorm\",\n",
    "        # [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.quniform(\"per_alpha\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta\": hp.quniform(\"per_beta\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta_final\": hp.quniform(\"per_beta_final\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 4, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\"), 0.1]),\n",
    "    \"transfer_interval\": scope.int(10 ** hp.quniform(\"transfer_interval\", 0, 1, 1)),\n",
    "    \"vps_to_win\": hp.quniform(\"vps_to_win\", 3, 3 + 1e-8, 1),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    params[\"dense_layer_widths\"] = [params[\"dense_layer_width\"]] * params[\n",
    "        \"dense_layers\"\n",
    "    ]\n",
    "    del params[\"dense_layer_width\"]\n",
    "    del params[\"dense_layers\"]\n",
    "    if params[\"output_layer_widths\"] != 0:\n",
    "        params[\"actor_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"critic_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"reward_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"to_play_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "    else:\n",
    "        params[\"actor_dense_layer_widths\"] = []\n",
    "        params[\"critic_dense_layer_widths\"] = []\n",
    "        params[\"reward_dense_layer_widths\"] = []\n",
    "        params[\"to_play_dense_layer_widths\"] = []\n",
    "    del params[\"output_layer_widths\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.catan_config import CatanConfig\n",
    "import torch\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = CatanConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "            episode_length += 1\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"catan_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)\n",
    "\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        make_env=CatanConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=CatanConfig,\n",
    "        games_per_pair=10,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=1,\n",
    "        test_interval=1000,\n",
    "        test_trials=100,\n",
    "        test_agents=[\n",
    "            CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "            CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        ],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65445e1d",
   "metadata": {},
   "source": [
    "RAINBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"/content/rl-research\")\n",
    "sys.path.append(\"../..\")\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from wrappers import CatanatronWrapper\n",
    "from losses.basic_losses import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.rainbow_dqn import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs.catan_config import SinglePlayerCatanConfig\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [512],\n",
    "    \"value_hidden_layers_widths\": [512],  #\n",
    "    \"advatage_hidden_layers_widths\": [512],  #\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 30000,\n",
    "    \"per_epsilon\": 1e-6,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"minibatch_size\": 64,\n",
    "    \"replay_buffer_size\": 50000,\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"n_step\": 9,\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 0.99,  # or 0.999 or even 0.9999 not 0.99 < this makes the start of the game possibly 0.05 after bootstrapping\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 16,\n",
    "}\n",
    "game_config = SinglePlayerCatanConfig()\n",
    "game_config.min_score = -1000\n",
    "game_config.max_score = 1010\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import catanatron.gym\n",
    "import gymnasium as gym\n",
    "\n",
    "from catanatron.gym.envs.catanatron_env import simple_reward\n",
    "from catanatron.gym.utils import get_tournament_total_return\n",
    "from catanatron.gym.utils import get_actual_victory_points\n",
    "\n",
    "\n",
    "def tournament_reward_function(game, p0_color):\n",
    "    winning_color = game.winning_color()\n",
    "    if p0_color == winning_color:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "    elif winning_color is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "\n",
    "\n",
    "env = CatanatronWrapper(\n",
    "    gym.make(\n",
    "        \"catanatron/Catanatron-v0\",\n",
    "        config={\n",
    "            \"enemies\": [RandomPlayer(Color.RED)],\n",
    "            \"invalid_action_reward\": -10,\n",
    "            \"map_type\": \"BASE\",\n",
    "            \"vps_to_win\": 3,\n",
    "            \"representation\": \"vector\",\n",
    "            \"reward_function\": tournament_reward_function,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "agent = RainbowAgent(env, config, \"rainbow-catan-3vps\", device)\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.nfsp import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import CatanConfig\n",
    "from losses.basic_losses import (\n",
    "    KLDivergenceLoss,\n",
    "    CategoricalCrossentropyLoss,\n",
    "    HuberLoss,\n",
    "    MSELoss,\n",
    ")\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from wrappers import ActionMaskInInfoWrapper, FrameStackWrapper\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 500000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 2048,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"minibatch_size\": 1024,\n",
    "    \"replay_buffer_size\": 1e5,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [1024],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.12,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 5000,\n",
    "    \"sl_minibatch_size\": 1024,\n",
    "    \"sl_replay_buffer_size\": 100000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [1024, 1024, 1024],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 1.0,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 9,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=CatanConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = False\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    ")\n",
    "\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"nfsp-catan\", device=\"cpu\")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import catanatron.gym\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"catanatron/Catanatron-v0\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    # your agent here (this takes random actions)\n",
    "    action = random.choice(info[\"valid_actions\"])\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "# Instantiate two random players\n",
    "player1 = RandomPlayer(Color.RED)\n",
    "player2 = RandomPlayer(Color.BLUE)\n",
    "\n",
    "# Create a 2-player game (you can fill remaining slots with random agents if needed)\n",
    "players = [player1, player2]\n",
    "game = Game(players)\n",
    "\n",
    "winner = game.play()\n",
    "print(f\"Winner: {winner}\")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "    player1 = player1(Color.RED)\n",
    "    player2 = player2(Color.BLUE)\n",
    "    game = Game([player1, player2])\n",
    "    winner = game.play()\n",
    "\n",
    "    if winner == Color.RED:\n",
    "        return 1\n",
    "    elif winner == Color.BLUE:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "players = [\n",
    "    RandomPlayer,\n",
    "    MCTSPlayer,\n",
    "    AlphaBetaPlayer,\n",
    "    # GreedyPlayoutsPlayer,\n",
    "    VictoryPointPlayer,\n",
    "    WeightedRandomPlayer,\n",
    "    # ValueFunctionPlayer,\n",
    "]\n",
    "games_per_pair = 10\n",
    "\n",
    "player_names = [p.__name__ for p in players]\n",
    "table = StandingsTable(player_names, start_elo=1000)\n",
    "\n",
    "\n",
    "def play_1v1_tournament(players, games_per_pair, play_game):\n",
    "    tournament_results = []\n",
    "    for player1 in players:\n",
    "        results = play_matches(player1, players, games_per_pair, play_game)\n",
    "        tournament_results.extend(results)\n",
    "    tournament_results = pd.DataFrame(\n",
    "        tournament_results, columns=[\"player1\", \"player2\", \"result\"]\n",
    "    )\n",
    "    return tournament_results\n",
    "\n",
    "\n",
    "def play_matches(player1, players, games_per_pair, play_game):\n",
    "    results = []\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {player1.__name__} vs {opponent.__name__} game {_+1}\")\n",
    "                result = play_game(player1, opponent)\n",
    "                results.append((player1.__name__, opponent.__name__, result))\n",
    "\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {opponent.__name__} vs {player1.__name__} game {_+1}\")\n",
    "                result = play_game(opponent, player1)\n",
    "                results.append(\n",
    "                    (\n",
    "                        player_names[players.index(opponent)],\n",
    "                        player_names[players.index(player1)],\n",
    "                        result,\n",
    "                    )\n",
    "                )\n",
    "    table.add_results_from_array(results)\n",
    "    print(table.bayes_elo())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(table.bayes_elo())\n",
    "print(table.get_win_table())\n",
    "print(table.get_draw_table())\n",
    "file = \"catan_1v1_tournament_results.pkl\"\n",
    "pickle.dump(table, open(file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "table.add_player(GreedyPlayoutsPlayer.__name__)\n",
    "players.append(GreedyPlayoutsPlayer) if GreedyPlayoutsPlayer not in players else None\n",
    "\n",
    "play_matches(GreedyPlayoutsPlayer, players, games_per_pair * 2, play_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play_1v1_tournament(players, games_per_pair, play_game)\n",
    "\n",
    "\n",
    "# table.add_results_from_dataframe(results)  # Adding multiple results\n",
    "print(table.bayes_elo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a petting zoo environment to see if it has all the functions and attributes needed\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "# env = catan_env(\n",
    "#     render_mode=\"rgb_array\",\n",
    "#     num_players=2,\n",
    "#     map_type=\"BASE\",\n",
    "#     vps_to_win=10,\n",
    "#     representation=\"vector\",\n",
    "#     invalid_action_reward=-1,\n",
    "#     auto_play_single_action=False,\n",
    "# )\n",
    "\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=False,\n",
    ")\n",
    "env = record_video_wrapper(env, \"./videos\", 1)\n",
    "env.reset()\n",
    "\n",
    "ab_player = CatanPlayerWrapper(AlphaBetaPlayer, Color.RED)\n",
    "\n",
    "for i in range(100):\n",
    "    state, reward, termination, truncation, info = env.last()\n",
    "    prediction = ab_player.predict(state, info, env)\n",
    "    action = ab_player.select_actions(prediction, info).item()\n",
    "    print(action)\n",
    "    print(info)\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95931df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    render_mode=\"rgb_array\",\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-1,\n",
    ")\n",
    "\n",
    "# Set metadata fps for 'human' if desired (not required for rgb_array)\n",
    "env.metadata[\"render_fps\"] = 10\n",
    "\n",
    "# Reset the environment (seed optional)\n",
    "env.reset(seed=0)\n",
    "print(env.last())\n",
    "print(\"Starting agent selection:\", env.agent_selection)\n",
    "\n",
    "# # Render one frame as an RGB array, then plot it\n",
    "# frame = env.render()  # will return a HxWx3 numpy array if renderer implemented\n",
    "# if frame is None:\n",
    "#     print(\n",
    "#         \"Renderer returned None. Make sure env.render(mode='rgb_array') is implemented.\"\n",
    "#     )\n",
    "# else:\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(frame)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# Setup matplotlib figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "print(\"Starting Catan Game Visualization...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run game for a few steps\n",
    "step_count = 0\n",
    "max_steps = 20\n",
    "\n",
    "while step_count < max_steps:\n",
    "    # Render current state\n",
    "    rgb_array = env.render()\n",
    "    print(env.last())\n",
    "\n",
    "    if rgb_array is not None:\n",
    "        # Display in notebook\n",
    "        # clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.imshow(rgb_array)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Catan Game - Step {step_count}\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "\n",
    "        # Print game info\n",
    "        current_agent = env.agent_selection\n",
    "        print(f\"\\nStep {step_count}\")\n",
    "        print(f\"Current Player: {current_agent}\")\n",
    "\n",
    "        if hasattr(env, \"game\") and hasattr(env.game, \"state\"):\n",
    "            print(f\"Turn Number: {env.game.state.num_turns}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Check if game is done\n",
    "    if env.terminations[env.agent_selection] or env.truncations[env.agent_selection]:\n",
    "        print(\"\\nGame Over!\")\n",
    "        break\n",
    "\n",
    "    # Get valid actions\n",
    "    obs = env.observe(env.agent_selection)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "    valid_actions = np.where(action_mask == 1)[0]\n",
    "\n",
    "    if len(valid_actions) > 0:\n",
    "        # Take a random valid action\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(action)\n",
    "    else:\n",
    "        print(\"No valid actions available!\")\n",
    "        break\n",
    "\n",
    "    step_count += 1\n",
    "    time.sleep(0.5)  # Pause to see each frame\n",
    "\n",
    "print(\"\\nTest completed!\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a11a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from custom_gym_envs.envs.catan import ActionType\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import ACTIONS_ARRAY\n",
    "\n",
    "n_steps = 25\n",
    "delay = 0.25  # seconds between frames\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # If the current agent is terminated/truncated, we still call env_instance.step per PettingZoo convention:\n",
    "    current_agent = env.agent_selection\n",
    "\n",
    "    # get observation for current agent\n",
    "    obs = env.observe(current_agent)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "\n",
    "    legal_indices = np.nonzero(action_mask)[0]\n",
    "    if len(legal_indices) == 0:\n",
    "        # no legal moves available (should rarely happen). Choose end-turn fallback if available:\n",
    "        try:\n",
    "            end_turn_index = ACTIONS_ARRAY.index((ActionType.END_TURN, None))\n",
    "            action_choice = end_turn_index\n",
    "        except Exception:\n",
    "            # fallback to 0\n",
    "            action_choice = 0\n",
    "    else:\n",
    "        action_choice = int(random.choice(legal_indices))\n",
    "\n",
    "    # Step the env with the chosen action\n",
    "    env.step(action_choice)\n",
    "\n",
    "    # Render and display frame\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(frame)\n",
    "        plt.title(f\"Step {step+1} - Agent: {current_agent} - Action: {action_choice}\")\n",
    "        plt.axis(\"off\")\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No frame returned (render returned None)\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# After rollout, print summary\n",
    "print(\"Done. Final agent selection:\", env.agent_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
