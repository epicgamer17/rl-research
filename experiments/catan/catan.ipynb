{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5607c5be",
   "metadata": {},
   "source": [
    "*** TO DO FOR CATAN: ***\n",
    "RAINBOW: \n",
    "    1. vs Random\n",
    "    2. vs Weighted Random\n",
    "    3. vs MTCS\n",
    "    4. vs Victory Point\n",
    "    5. vs AlphaBeta\n",
    "Masked PPO the same \n",
    "NFSP \n",
    "MuZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa0589",
   "metadata": {},
   "source": [
    "MUZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_onehot as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    # \"residual_filters\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    # ),\n",
    "    # \"residual_stacks\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    # ),\n",
    "    \"residual_layers\": hp.choice(\"residual_layers\", [[]]),\n",
    "    \"actor_conv_layers\": hp.choice(\"actor_conv_layers\", [[]]),\n",
    "    \"critic_conv_layers\": hp.choice(\"critic_conv_layers\", [[]]),\n",
    "    \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "    \"output_layer_widths\": scope.int(hp.quniform(\"output_layer_widths\", 0, 16, 16)),\n",
    "    \"dense_layer_width\": scope.int(\n",
    "        hp.quniform(\"dense_layer_width\", 128, 128 + 1e-8, 128)\n",
    "    ),\n",
    "    \"dense_layers\": scope.int(hp.quniform(\"dense_layers\", 1, 3, 1)),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": hp.choice(\"value_loss_factor\", [1.0]),\n",
    "    \"root_dirichlet_alpha\": 2\n",
    "    ** (hp.quniform(\"root_dirichlet_alpha\", -1, -1 + 1e-8, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(800 * 2 ** hp.quniform(\"num_simulations\", -5, 0, 1)),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 16, 32, 8))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(35000), np.log(45000), 10000)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 5, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        10 ** hp.quniform(\"min_replay_buffer_size\", 2, 2 + 1e-8, 1)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 6 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [1000]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        # \"clipnorm\", [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))]\n",
    "        \"clipnorm\",\n",
    "        [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.choice(\"per_alpha\", [0.0]),\n",
    "    \"per_beta\": hp.choice(\"per_beta\", [0.0]),\n",
    "    \"per_beta_final\": hp.choice(\"per_beta_final\", [0.0]),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 4 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\")]),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    params[\"dense_layer_widths\"] = [params[\"dense_layer_width\"]] * params[\n",
    "        \"dense_layers\"\n",
    "    ]\n",
    "    del params[\"dense_layer_width\"]\n",
    "    del params[\"dense_layers\"]\n",
    "    if params[\"output_layer_widths\"] != 0:\n",
    "        params[\"actor_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"critic_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"reward_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "    else:\n",
    "        params[\"actor_dense_layer_widths\"] = []\n",
    "        params[\"critic_dense_layer_widths\"] = []\n",
    "        params[\"reward_dense_layer_widths\"] = []\n",
    "    del params[\"output_layer_widths\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.catan_config import CatanConfig\n",
    "import torch\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = CatanConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env, temperature=0.05)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env, temperature=0.05)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "            episode_length += 1\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0da9bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved Trials! Starting from scratch.\n",
      "Params:  {'action_function': <function action_as_onehot at 0x314d30280>, 'actor_conv_layers': (), 'clip_low_prob': 0.0, 'clipnorm': 0.0, 'conv_layers': (), 'critic_conv_layers': (), 'dense_layer_width': 128, 'dense_layers': 1, 'kernel_initializer': 'glorot_normal', 'known_bounds': (-1, 1), 'lr_ratio': inf, 'min_replay_buffer_size': 100, 'minibatch_size': 16, 'multi_process': {'multi_process': True, 'num_workers': 2}, 'n_step': 1000, 'noisy_sigma': 0.0, 'num_simulations': 200, 'optimizer': {'adam_epsilon': 1e-08, 'adam_learning_rate': 0.001, 'optimizer': 'adam'}, 'output_layer_widths': 16, 'pb_c_base': 19652, 'pb_c_init': 1.25, 'per_alpha': 0.0, 'per_beta': 0.0, 'per_beta_final': 0.0, 'per_epsilon': 0.0001, 'policy_loss_function': <utils.utils.CategoricalCrossentropyLoss object at 0x314d236d0>, 'replay_buffer_size': 100000, 'residual_layers': (), 'reward_conv_layers': (), 'reward_loss_function': <utils.utils.MSELoss object at 0x314d23580>, 'root_dirichlet_alpha': 0.5, 'root_exploration_fraction': 0.25, 'temperature_updates': (24,), 'temperature_with_training_steps': False, 'temperatures': (1.0, 0.1), 'training_steps': 40000, 'unroll_steps': 5, 'value_loss_factor': 1.0, 'value_loss_function': <utils.utils.MSELoss object at 0x314d23430>, 'weight_decay': 0.0001}\n",
      "Making environments\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 40000\n",
      "Using         adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using         learning_rate                 : 0.001\n",
      "Using         clipnorm                      : 0.0\n",
      "Using         optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using         weight_decay                  : 0.0001\n",
      "Using default loss_function                 : <class 'utils.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : glorot_normal\n",
      "Using         minibatch_size                : 16\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using         min_replay_buffer_size        : 100\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : (-1, 1)\n",
      "Using         residual_layers               : ()\n",
      "Using         conv_layers                   : ()\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using default representation_residual_layers: ()\n",
      "Using default representation_conv_layers    : ()\n",
      "Using default representation_dense_layer_widths: [128]\n",
      "Using default dynamics_residual_layers      : ()\n",
      "Using default dynamics_conv_layers          : ()\n",
      "Using default dynamics_dense_layer_widths   : [128]\n",
      "Using         reward_conv_layers            : ()\n",
      "Using         reward_dense_layer_widths     : [16]\n",
      "Using         critic_conv_layers            : ()\n",
      "Using         critic_dense_layer_widths     : [16]\n",
      "Using         actor_conv_layers             : ()\n",
      "Using         actor_dense_layer_widths      : [16]\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using         weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.5\n",
      "Using         root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 200\n",
      "Using         temperatures                  : (1.0, 0.1)\n",
      "Using         temperature_updates           : (24,)\n",
      "Using         temperature_with_training_steps: False\n",
      "Using         clip_low_prob                 : 0.0\n",
      "Using         pb_c_base                     : 19652\n",
      "Using         pb_c_init                     : 1.25\n",
      "Using         value_loss_function           : <utils.utils.MSELoss object at 0x314d23430>\n",
      "Using         reward_loss_function          : <utils.utils.MSELoss object at 0x314d23580>\n",
      "Using         policy_loss_function          : <utils.utils.CategoricalCrossentropyLoss object at 0x314d236d0>\n",
      "Using         action_function               : <function action_as_onehot at 0x314d30280>\n",
      "Using         n_step                        : 1000\n",
      "Using default discount_factor               : 1.0\n",
      "Using         unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 0.0001\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : None\n",
      "Using         multi_process                 : True\n",
      "Using         num_workers                   : 2\n",
      "Using         lr_ratio                      : inf\n",
      "No initial best config matched, first trial\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 40000\n",
      "Using         adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using         learning_rate                 : 0.001\n",
      "Using         clipnorm                      : 0.0\n",
      "Using         optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using         weight_decay                  : 0.0001\n",
      "Using default loss_function                 : <class 'utils.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : glorot_normal\n",
      "Using         minibatch_size                : 16\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using         min_replay_buffer_size        : 100\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : (-1, 1)\n",
      "Using         residual_layers               : ()\n",
      "Using         conv_layers                   : ()\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using default representation_residual_layers: ()\n",
      "Using default representation_conv_layers    : ()\n",
      "Using default representation_dense_layer_widths: [128]\n",
      "Using default dynamics_residual_layers      : ()\n",
      "Using default dynamics_conv_layers          : ()\n",
      "Using default dynamics_dense_layer_widths   : [128]\n",
      "Using         reward_conv_layers            : ()\n",
      "Using         reward_dense_layer_widths     : [16]\n",
      "Using         critic_conv_layers            : ()\n",
      "Using         critic_dense_layer_widths     : [16]\n",
      "Using         actor_conv_layers             : ()\n",
      "Using         actor_dense_layer_widths      : [16]\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using         weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.5\n",
      "Using         root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 200\n",
      "Using         temperatures                  : (1.0, 0.1)\n",
      "Using         temperature_updates           : (24,)\n",
      "Using         temperature_with_training_steps: False\n",
      "Using         clip_low_prob                 : 0.0\n",
      "Using         pb_c_base                     : 19652\n",
      "Using         pb_c_init                     : 1.25\n",
      "Using         value_loss_function           : <utils.utils.MSELoss object at 0x314d23430>\n",
      "Using         reward_loss_function          : <utils.utils.MSELoss object at 0x314d23580>\n",
      "Using         policy_loss_function          : <utils.utils.CategoricalCrossentropyLoss object at 0x314d236d0>\n",
      "Using         action_function               : <function action_as_onehot at 0x314d30280>\n",
      "Using         n_step                        : 1000\n",
      "Using default discount_factor               : 1.0\n",
      "Using         unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 0.0001\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : None\n",
      "Using         multi_process                 : True\n",
      "Using         num_workers                   : 2\n",
      "Using         lr_ratio                      : inf\n",
      "Using device: cpu\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: FrameStackWrapper<ActionMaskInInfoWrapper<catanatron_v1>>\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (2456,)\n",
      "Observation dtype: float32\n",
      "num_actions:  290 <class 'int'>\n",
      "Test agents: [<agents.catan_player_wrapper.CatanPlayerWrapper object at 0x3232dbe50>, <agents.catan_player_wrapper.CatanPlayerWrapper object at 0x3262103a0>]\n",
      "Hidden state shape: (16, 128)\n",
      "Action function output shape: torch.Size([290])\n",
      "torch.Size([16, 418])\n",
      "dynamics input shape torch.Size([16, 418])\n",
      "Layer weights:\n",
      "representation.dense_layers.dense_layers.0.layer.weight:\n",
      "tensor([[ 0.0005,  0.0027, -0.0188,  ..., -0.0091,  0.0024, -0.0152],\n",
      "        [ 0.0137,  0.0048,  0.0053,  ...,  0.0189,  0.0152,  0.0042],\n",
      "        [-0.0144, -0.0180, -0.0175,  ..., -0.0135,  0.0153, -0.0032],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0200, -0.0077,  ...,  0.0061,  0.0143,  0.0085],\n",
      "        [ 0.0007, -0.0022,  0.0045,  ..., -0.0024, -0.0044, -0.0040],\n",
      "        [-0.0070,  0.0052, -0.0120,  ...,  0.0147, -0.0076, -0.0096]])\n",
      "Shape: torch.Size([128, 2456]), std: 0.0116, mean: -0.0000\n",
      "\n",
      "representation.dense_layers.dense_layers.0.layer.bias:\n",
      "tensor([-1.8732e-02,  3.0475e-03,  6.9338e-04,  5.6465e-03,  1.5141e-02,\n",
      "         7.7443e-03, -6.4144e-03, -9.5709e-03,  1.2549e-02,  1.9432e-02,\n",
      "         1.7897e-02,  8.2186e-03,  4.9948e-03, -1.8204e-02,  8.5775e-03,\n",
      "         4.9376e-03, -1.0047e-02, -1.0423e-02,  2.4787e-03,  1.7946e-02,\n",
      "        -4.7591e-03, -1.8200e-02,  9.4090e-03, -1.9196e-02,  7.2745e-03,\n",
      "        -3.8441e-03,  1.7320e-04,  9.3607e-03,  1.1314e-02,  3.4660e-03,\n",
      "         5.0109e-03,  5.6923e-03,  1.3218e-02, -1.3481e-02, -3.8199e-03,\n",
      "        -1.2490e-02, -1.6169e-02, -1.8037e-02,  3.8871e-03, -2.2179e-03,\n",
      "         1.2183e-02, -1.5118e-02, -5.5780e-03, -1.2602e-02,  1.2312e-02,\n",
      "         1.3077e-02, -9.1617e-03,  1.2504e-02, -1.1719e-02,  1.5034e-02,\n",
      "         7.0642e-04, -1.3485e-03,  1.8601e-02, -8.8262e-04,  1.9930e-03,\n",
      "        -6.1672e-03,  8.4909e-03, -2.9159e-03, -1.0059e-02, -1.1357e-02,\n",
      "         1.2535e-02, -2.3874e-04, -1.4477e-02,  1.3597e-02, -1.1965e-02,\n",
      "        -1.8847e-02, -2.9902e-03, -9.6347e-03, -1.1051e-02, -1.3253e-02,\n",
      "         1.1673e-02, -1.3867e-02,  6.9914e-03, -1.4917e-02, -1.0605e-02,\n",
      "         9.6559e-03,  9.4533e-03, -1.1020e-02,  3.5069e-03,  1.5080e-02,\n",
      "        -7.3230e-03, -1.4822e-02, -2.1073e-03,  4.1078e-03,  1.4665e-02,\n",
      "         1.5597e-02,  4.4209e-03,  1.3214e-03,  1.4751e-02,  7.5094e-03,\n",
      "        -2.3614e-03,  8.4953e-03,  1.9714e-02, -1.4216e-02,  1.9196e-02,\n",
      "         1.4736e-02, -3.7592e-03,  2.6183e-03,  1.5278e-02,  1.1112e-02,\n",
      "         8.7091e-03,  8.8692e-03, -7.8160e-05, -7.3655e-03,  9.7495e-03,\n",
      "         1.1021e-03, -1.5723e-02,  1.5469e-02, -1.8412e-02,  1.0386e-02,\n",
      "        -7.4262e-03,  6.9225e-03,  2.9531e-03,  1.6487e-02, -1.1793e-02,\n",
      "         1.4484e-02, -6.2263e-03, -1.4533e-02,  2.8414e-03, -3.7218e-03,\n",
      "         1.5951e-02, -7.0788e-03, -1.9647e-03, -3.4374e-03,  1.5693e-02,\n",
      "         1.1397e-02, -1.4346e-02,  1.1791e-02])\n",
      "Shape: torch.Size([128]), std: 0.0111, mean: 0.0009\n",
      "\n",
      "dynamics.dense_layers.dense_layers.0.layer.weight:\n",
      "tensor([[ 0.0238, -0.0130,  0.0189,  ...,  0.0433, -0.0105, -0.0424],\n",
      "        [ 0.0016,  0.0327,  0.0354,  ...,  0.0367, -0.0058,  0.0028],\n",
      "        [ 0.0129, -0.0477, -0.0135,  ...,  0.0229, -0.0412,  0.0343],\n",
      "        ...,\n",
      "        [-0.0325, -0.0184, -0.0023,  ..., -0.0204,  0.0044, -0.0469],\n",
      "        [-0.0086,  0.0157, -0.0457,  ...,  0.0408,  0.0429, -0.0218],\n",
      "        [ 0.0367,  0.0215,  0.0472,  ...,  0.0156, -0.0098, -0.0330]])\n",
      "Shape: torch.Size([128, 418]), std: 0.0283, mean: 0.0000\n",
      "\n",
      "dynamics.dense_layers.dense_layers.0.layer.bias:\n",
      "tensor([-0.0196,  0.0382, -0.0305, -0.0325, -0.0482,  0.0191, -0.0257,  0.0317,\n",
      "        -0.0012, -0.0086,  0.0170,  0.0467,  0.0357,  0.0449, -0.0033,  0.0175,\n",
      "         0.0132,  0.0445,  0.0180, -0.0245, -0.0306,  0.0335, -0.0475, -0.0064,\n",
      "         0.0340,  0.0120,  0.0346,  0.0354, -0.0112,  0.0023,  0.0038,  0.0132,\n",
      "        -0.0323,  0.0256, -0.0163,  0.0074,  0.0212,  0.0408, -0.0481,  0.0483,\n",
      "        -0.0117,  0.0430, -0.0159,  0.0181,  0.0013, -0.0216,  0.0044, -0.0319,\n",
      "        -0.0285,  0.0418, -0.0251, -0.0191,  0.0414,  0.0240, -0.0372,  0.0196,\n",
      "         0.0024, -0.0405,  0.0236, -0.0358, -0.0008,  0.0160,  0.0402, -0.0180,\n",
      "         0.0291, -0.0015,  0.0453, -0.0217, -0.0179, -0.0404, -0.0425,  0.0342,\n",
      "         0.0003, -0.0368,  0.0106,  0.0136, -0.0311, -0.0277, -0.0456,  0.0015,\n",
      "         0.0264,  0.0289,  0.0293, -0.0260,  0.0398,  0.0106, -0.0259,  0.0060,\n",
      "        -0.0331, -0.0009, -0.0057,  0.0036, -0.0275,  0.0170, -0.0035,  0.0154,\n",
      "        -0.0339,  0.0385, -0.0189, -0.0301, -0.0179,  0.0098,  0.0276,  0.0453,\n",
      "         0.0488, -0.0160, -0.0010, -0.0022,  0.0320, -0.0223,  0.0346, -0.0087,\n",
      "        -0.0154,  0.0079,  0.0388, -0.0257,  0.0045,  0.0091,  0.0328,  0.0211,\n",
      "         0.0298,  0.0171, -0.0376, -0.0079, -0.0164,  0.0206,  0.0334,  0.0068])\n",
      "Shape: torch.Size([128]), std: 0.0272, mean: 0.0029\n",
      "\n",
      "dynamics.reward_dense_layers.dense_layers.0.layer.weight:\n",
      "tensor([[ 0.0059, -0.0258, -0.0013,  ...,  0.0560,  0.0480,  0.0511],\n",
      "        [ 0.0319,  0.0205, -0.0761,  ...,  0.0377, -0.0049,  0.0504],\n",
      "        [ 0.0166, -0.0469,  0.0061,  ..., -0.0710,  0.0083,  0.0785],\n",
      "        ...,\n",
      "        [ 0.0105,  0.0621,  0.0503,  ..., -0.0361, -0.0437, -0.0014],\n",
      "        [-0.0288,  0.0578, -0.0069,  ...,  0.0402, -0.0169,  0.0829],\n",
      "        [-0.0231, -0.0491, -0.0412,  ...,  0.0293, -0.0226,  0.0673]])\n",
      "Shape: torch.Size([16, 128]), std: 0.0502, mean: 0.0003\n",
      "\n",
      "dynamics.reward_dense_layers.dense_layers.0.layer.bias:\n",
      "tensor([-0.0645, -0.0117, -0.0688,  0.0751, -0.0447,  0.0276, -0.0006, -0.0235,\n",
      "         0.0272,  0.0828, -0.0522,  0.0538,  0.0471,  0.0295,  0.0165,  0.0514])\n",
      "Shape: torch.Size([16]), std: 0.0489, mean: 0.0091\n",
      "\n",
      "dynamics.reward.layer.weight:\n",
      "tensor([[-0.0477,  0.1068,  0.1522,  0.1148, -0.0835,  0.1188, -0.0031, -0.1919,\n",
      "          0.1985,  0.2394, -0.0652,  0.1442,  0.1362,  0.0344, -0.2082, -0.2033]])\n",
      "Shape: torch.Size([1, 16]), std: 0.1465, mean: 0.0277\n",
      "\n",
      "dynamics.reward.layer.bias:\n",
      "tensor([0.0415])\n",
      "Shape: torch.Size([1]), std: nan, mean: 0.0415\n",
      "\n",
      "prediction.dense_layers.dense_layers.0.layer.weight:\n",
      "tensor([[ 0.0875,  0.0524, -0.0219,  ...,  0.0377, -0.0351, -0.0673],\n",
      "        [ 0.0219,  0.0062, -0.0697,  ..., -0.0716, -0.0366,  0.0364],\n",
      "        [ 0.0055, -0.0731, -0.0344,  ...,  0.0693, -0.0017,  0.0110],\n",
      "        ...,\n",
      "        [-0.0505,  0.0711,  0.0493,  ..., -0.0464,  0.0191, -0.0347],\n",
      "        [-0.0098, -0.0186,  0.0475,  ..., -0.0415,  0.0186, -0.0813],\n",
      "        [-0.0138, -0.0709,  0.0575,  ..., -0.0464,  0.0433, -0.0239]])\n",
      "Shape: torch.Size([128, 128]), std: 0.0510, mean: -0.0004\n",
      "\n",
      "prediction.dense_layers.dense_layers.0.layer.bias:\n",
      "tensor([ 0.0582, -0.0721, -0.0518, -0.0740, -0.0045,  0.0412,  0.0069, -0.0706,\n",
      "         0.0735,  0.0376, -0.0533, -0.0167,  0.0094, -0.0430, -0.0020, -0.0745,\n",
      "        -0.0375, -0.0336, -0.0363,  0.0833, -0.0236, -0.0625, -0.0034,  0.0429,\n",
      "         0.0550, -0.0461, -0.0748,  0.0686, -0.0125,  0.0646, -0.0638, -0.0412,\n",
      "         0.0548, -0.0863, -0.0457, -0.0024, -0.0220, -0.0429,  0.0670, -0.0666,\n",
      "         0.0116,  0.0165,  0.0849,  0.0543,  0.0589,  0.0231,  0.0695, -0.0797,\n",
      "         0.0527, -0.0783, -0.0440,  0.0810, -0.0179,  0.0346,  0.0154, -0.0846,\n",
      "         0.0109,  0.0659, -0.0476, -0.0134, -0.0530, -0.0081, -0.0710,  0.0307,\n",
      "        -0.0532, -0.0324,  0.0231, -0.0868,  0.0143, -0.0541, -0.0726,  0.0111,\n",
      "         0.0259, -0.0055,  0.0736,  0.0149,  0.0604,  0.0023, -0.0167,  0.0542,\n",
      "        -0.0365, -0.0793, -0.0867, -0.0713, -0.0184,  0.0090, -0.0883,  0.0536,\n",
      "         0.0305,  0.0435,  0.0054, -0.0145,  0.0190,  0.0175, -0.0188, -0.0102,\n",
      "         0.0815,  0.0168,  0.0070,  0.0728, -0.0306, -0.0828,  0.0460,  0.0168,\n",
      "        -0.0357, -0.0597, -0.0441, -0.0600,  0.0716, -0.0021,  0.0102,  0.0714,\n",
      "        -0.0276,  0.0742, -0.0452, -0.0120, -0.0775,  0.0876,  0.0346, -0.0671,\n",
      "         0.0460,  0.0516,  0.0475, -0.0342,  0.0853,  0.0517,  0.0744,  0.0399])\n",
      "Shape: torch.Size([128]), std: 0.0518, mean: -0.0021\n",
      "\n",
      "prediction.critic.dense_layers.dense_layers.0.layer.weight:\n",
      "tensor([[-0.0154, -0.0003, -0.0519,  ...,  0.0571,  0.0714,  0.0587],\n",
      "        [ 0.0435,  0.0388, -0.0238,  ..., -0.0129,  0.0225,  0.0019],\n",
      "        [ 0.0755,  0.0077, -0.0367,  ...,  0.0218, -0.0098,  0.0477],\n",
      "        ...,\n",
      "        [ 0.0802, -0.0637, -0.0253,  ...,  0.0751, -0.0530, -0.0638],\n",
      "        [ 0.0746,  0.0113,  0.0648,  ...,  0.0070,  0.0710, -0.0783],\n",
      "        [ 0.0175, -0.0333, -0.0342,  ...,  0.0090,  0.0471,  0.0070]])\n",
      "Shape: torch.Size([16, 128]), std: 0.0507, mean: 0.0006\n",
      "\n",
      "prediction.critic.dense_layers.dense_layers.0.layer.bias:\n",
      "tensor([-0.0482, -0.0839, -0.0417, -0.0227,  0.0271,  0.0124,  0.0345,  0.0372,\n",
      "        -0.0664,  0.0037,  0.0654, -0.0847, -0.0828,  0.0756, -0.0667, -0.0579])\n",
      "Shape: torch.Size([16]), std: 0.0552, mean: -0.0187\n",
      "\n",
      "prediction.critic.value.layer.weight:\n",
      "tensor([[ 0.0525,  0.1934,  0.2449,  0.0499,  0.1681,  0.0060, -0.1062, -0.1467,\n",
      "         -0.0989, -0.1214, -0.1272,  0.2326, -0.1943,  0.0368,  0.1709,  0.0370]])\n",
      "Shape: torch.Size([1, 16]), std: 0.1457, mean: 0.0248\n",
      "\n",
      "prediction.critic.value.layer.bias:\n",
      "tensor([0.1828])\n",
      "Shape: torch.Size([1]), std: nan, mean: 0.1828\n",
      "\n",
      "prediction.actor.dense_layers.dense_layers.0.layer.weight:\n",
      "tensor([[-0.0140, -0.0237, -0.0424,  ...,  0.0101,  0.0008, -0.0434],\n",
      "        [ 0.0352,  0.0279, -0.0767,  ...,  0.0811, -0.0119,  0.0475],\n",
      "        [-0.0839,  0.0842,  0.0336,  ...,  0.0104, -0.0477, -0.0760],\n",
      "        ...,\n",
      "        [ 0.0489, -0.0801, -0.0497,  ..., -0.0025, -0.0208, -0.0027],\n",
      "        [-0.0096, -0.0200,  0.0337,  ..., -0.0510, -0.0679,  0.0578],\n",
      "        [-0.0530, -0.0249,  0.0045,  ...,  0.0391, -0.0175, -0.0192]])\n",
      "Shape: torch.Size([16, 128]), std: 0.0505, mean: 0.0014\n",
      "\n",
      "prediction.actor.dense_layers.dense_layers.0.layer.bias:\n",
      "tensor([ 0.0646,  0.0731,  0.0634, -0.0817, -0.0233,  0.0504,  0.0545, -0.0656,\n",
      "         0.0318, -0.0161, -0.0346, -0.0062, -0.0169, -0.0117,  0.0735,  0.0572])\n",
      "Shape: torch.Size([16]), std: 0.0512, mean: 0.0133\n",
      "\n",
      "prediction.actor.actions.layer.weight:\n",
      "tensor([[-0.0317,  0.0490, -0.0568,  ..., -0.0339,  0.2254, -0.0197],\n",
      "        [ 0.0072,  0.0671, -0.1531,  ...,  0.1528, -0.0017,  0.2460],\n",
      "        [ 0.1033,  0.0057,  0.0169,  ...,  0.1905, -0.0003,  0.0973],\n",
      "        ...,\n",
      "        [-0.2145, -0.0348, -0.1655,  ..., -0.2112, -0.0175, -0.0796],\n",
      "        [ 0.1978, -0.0968, -0.0338,  ..., -0.2449, -0.2385,  0.2493],\n",
      "        [ 0.1661, -0.0470, -0.0282,  ..., -0.0831,  0.2110, -0.0733]])\n",
      "Shape: torch.Size([290, 16]), std: 0.1447, mean: -0.0005\n",
      "\n",
      "prediction.actor.actions.layer.bias:\n",
      "tensor([ 0.0166, -0.0544, -0.1919,  0.2239,  0.1147, -0.2431,  0.1936, -0.0282,\n",
      "         0.2380,  0.0149, -0.2471, -0.1636,  0.1954,  0.1299,  0.2099,  0.0024,\n",
      "        -0.0849,  0.1845, -0.2175, -0.0846,  0.2286,  0.2486, -0.1100,  0.0477,\n",
      "         0.1816, -0.0708, -0.0474,  0.1320, -0.1930,  0.1640,  0.0565, -0.0855,\n",
      "         0.1610,  0.1804,  0.1020,  0.0540, -0.1340, -0.0044,  0.1173, -0.1109,\n",
      "         0.2475, -0.0869, -0.0297, -0.0241, -0.1262,  0.1374, -0.1942,  0.1892,\n",
      "         0.1897,  0.1771, -0.0233,  0.0475,  0.1314,  0.1100, -0.1480, -0.2198,\n",
      "        -0.1132, -0.1086, -0.0345, -0.1776,  0.1503,  0.1569, -0.0116, -0.0519,\n",
      "         0.0061, -0.0633,  0.0231, -0.1858, -0.2434, -0.1494, -0.2164,  0.1523,\n",
      "         0.2338,  0.0452, -0.1419, -0.0373,  0.0070,  0.1447, -0.1591, -0.2391,\n",
      "        -0.0759,  0.1717, -0.2284, -0.2397, -0.1964, -0.0087, -0.1852,  0.0607,\n",
      "        -0.1247,  0.1977, -0.1819, -0.2083,  0.0797, -0.1535, -0.1035,  0.2444,\n",
      "        -0.1270, -0.0753,  0.0892,  0.1753,  0.1777,  0.0519,  0.0478,  0.0991,\n",
      "         0.0883, -0.0254, -0.1193,  0.0230,  0.0319, -0.1715,  0.0195, -0.2254,\n",
      "        -0.1000, -0.1204, -0.1043,  0.2248,  0.1450,  0.1705, -0.1931,  0.1416,\n",
      "         0.0501, -0.2269,  0.1103,  0.2203, -0.1750,  0.0429, -0.2121, -0.1760,\n",
      "         0.2101, -0.0673, -0.1194,  0.2450,  0.0674,  0.2384,  0.1197, -0.1623,\n",
      "         0.0768, -0.2275, -0.0460,  0.1854,  0.1650,  0.1994, -0.0936,  0.1286,\n",
      "        -0.0128,  0.0772,  0.0711, -0.2207, -0.1576, -0.1223, -0.1461, -0.0617,\n",
      "        -0.1180, -0.1350,  0.0193,  0.2223, -0.0804,  0.2219,  0.2017, -0.0371,\n",
      "        -0.0355, -0.0777, -0.0149, -0.1493, -0.0766,  0.0080, -0.1851, -0.2091,\n",
      "         0.0391, -0.1412,  0.1736,  0.0569,  0.0861, -0.1724, -0.2370,  0.1955,\n",
      "        -0.1939, -0.1128,  0.0411,  0.1625,  0.2003,  0.0859, -0.1275, -0.0701,\n",
      "        -0.0565, -0.1277, -0.0392,  0.1670,  0.2240, -0.1633,  0.1212, -0.1430,\n",
      "        -0.1616,  0.0370,  0.0060,  0.2303, -0.1054, -0.1348,  0.1691,  0.0885,\n",
      "         0.1626, -0.1718, -0.2211,  0.0458,  0.2404,  0.2177,  0.1510,  0.1079,\n",
      "         0.0448, -0.0417,  0.2058, -0.1194,  0.0043, -0.0755,  0.2433, -0.2323,\n",
      "         0.2035, -0.1016,  0.0254,  0.2114,  0.0060, -0.2174, -0.0293,  0.0887,\n",
      "        -0.1314, -0.1353, -0.0967, -0.1698, -0.2392,  0.0869,  0.2082, -0.0343,\n",
      "         0.0029, -0.0668,  0.1472,  0.1299,  0.2233,  0.1705, -0.2000, -0.0939,\n",
      "         0.1715,  0.1891,  0.1224,  0.1848,  0.2340,  0.0873,  0.1282,  0.0940,\n",
      "        -0.1223, -0.1969, -0.0704, -0.2006,  0.1445,  0.0721,  0.1678, -0.0014,\n",
      "        -0.0238, -0.2299,  0.2268, -0.1832,  0.1933, -0.1477,  0.1742,  0.1690,\n",
      "        -0.0430, -0.1538,  0.0848,  0.2003,  0.1806, -0.0824, -0.0264, -0.1333,\n",
      "        -0.1004, -0.1367, -0.0461, -0.1451, -0.1067,  0.1478, -0.2200, -0.1580,\n",
      "         0.1175, -0.0713,  0.0901, -0.0885, -0.1233,  0.0091, -0.0392,  0.0202,\n",
      "        -0.0828,  0.0686])\n",
      "Shape: torch.Size([290]), std: 0.1464, mean: 0.0010\n",
      "\n",
      "Warning: for board games it is recommnded to have n_step >= game length\n",
      "Max size: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py:101: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  f\"Shape: {param.shape}, std: {param.std():.4f}, mean: {param.mean():.4f}\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'test_score_vs_RandomPlayer' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_AlphaBetaPlayer' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 1] Starting self-play...\n",
      "Move 1Move\n",
      " 1\n",
      "Move 2\n",
      "Move 2\n",
      "Move 3\n",
      "Move 3\n",
      "Move 4\n",
      "Move 4\n",
      "Move 5\n",
      "Move 5\n",
      "Move 6\n",
      "Move 6\n",
      "Move 7\n",
      "Move 7\n",
      "Move 8\n",
      "Move 8\n",
      "Move 9\n",
      "Move 9\n",
      "Move 10\n",
      "Move 10\n",
      "Move 11\n",
      "Move 11\n",
      "Move 12\n",
      "Move 12\n",
      "Move 13\n",
      "Move 14\n",
      "Move 13\n",
      "Move 15\n",
      "Move 14\n",
      "Move 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 217, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 702, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 637, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 398, in monte_carlo_tree_search\n",
      "    node.expand(\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 217, in worker_fn\n",
      "    score, num_steps = self.play_game(env=worker_env)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 702, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 637, in predict\n",
      "    value, visit_counts = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 374, in monte_carlo_tree_search\n",
      "    action, node = node.select_child(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_mcts.py\", line 48, in select_child\n",
      "    child_ucbs = [\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_mcts.py\", line 49, in <listcomp>\n",
      "    self.child_ucb_score(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_mcts.py\", line 61, in child_ucb_score\n",
      "    def child_ucb_score(\n",
      "KeyboardInterrupt\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo saved Trials! Starting from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarl_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Objective Function to optimize\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Hyperparameter's Search Space\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optimization algorithm (representative TPE)\u001b[39;49;00m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of optimization attempts\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Record the results\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# early_stop_fn=no_progress_loss(5, 1),\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_trials.p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_best_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n\u001b[1;32m    102\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m space_eval(search_space, best)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../hyperparameter_optimization/hyperopt.py:193\u001b[0m, in \u001b[0;36mmarl_objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_buffer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_replay_buffer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# score = run_training([params, env, name])\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mmarl_run_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    195\u001b[0m     status \u001b[38;5;241m=\u001b[39m STATUS_FAIL\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../hyperparameter_optimization/hyperopt.py:105\u001b[0m, in \u001b[0;36mmarl_run_training\u001b[0;34m(params, agent_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtest_interval\n\u001b[1;32m    103\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtest_trials\n\u001b[0;32m--> 105\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39meval_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elo_evaulation(agent)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py:257\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tb))  \u001b[38;5;66;03m# optional: print worker traceback\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrain_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m training_game \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgames_per_generation)):\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../stats/stats.py:113\u001b[0m, in \u001b[0;36mStatTracker.drain_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_client:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrain_queue() can only be called on the host StatTracker.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mget_nowait()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:129\u001b[0m, in \u001b[0;36mQueue.empty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"catan_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=CatanPlayerWrapper(AlphaBetaPlayer, Color.WHITE),\n",
    "        make_env=CatanConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=CatanConfig,\n",
    "        games_per_pair=500,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=100,\n",
    "        test_interval=1000,\n",
    "        test_trials=10,\n",
    "        test_agents=[\n",
    "            CatanPlayerWrapper(RandomPlayer, Color.WHITE),\n",
    "            CatanPlayerWrapper(AlphaBetaPlayer, Color.WHITE),\n",
    "        ],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65445e1d",
   "metadata": {},
   "source": [
    "RAINBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"/content/rl-research\")\n",
    "sys.path.append(\"../..\")\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from wrappers import CatanatronWrapper\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs.catan_config import SinglePlayerCatanConfig\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [256, 256, 256, 256],\n",
    "    \"value_hidden_layers_widths\": [256],  #\n",
    "    \"advatage_hidden_layers_widths\": [256],  #\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 30000,\n",
    "    \"per_epsilon\": 1e-6,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"minibatch_size\": 16,\n",
    "    \"replay_buffer_size\": 500000,\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"n_step\": 9,\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 1.0,  # or 0.999 or even 0.9999 not 0.99 < this makes the start of the game possibly 0.05 after bootstrapping\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 512,\n",
    "}\n",
    "game_config = SinglePlayerCatanConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import catanatron.gym\n",
    "import gymnasium as gym\n",
    "\n",
    "env = CatanatronWrapper(\n",
    "    gym.make(\n",
    "        \"catanatron/Catanatron-v0\",\n",
    "        config={\n",
    "            \"enemies\": [RandomPlayer(Color.RED)],\n",
    "            \"invalid_action_reward\": -10,\n",
    "            \"map_type\": \"BASE\",\n",
    "            \"vps_to_win\": 10,\n",
    "            \"representation\": \"vector\",\n",
    "        },\n",
    "    )\n",
    ")\n",
    "agent = RainbowAgent(env, config, \"rainbow-catan-10vps\", device)\n",
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 100\n",
    "agent.test_trials = 25\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a1db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import CatanConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 1e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 100000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=CatanConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = False\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    ")\n",
    "\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "# env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"nfsp-catan\", device=\"cuda:0\")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 500\n",
    "agent.test_trials = 10\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import catanatron.gym\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"catanatron/Catanatron-v0\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    # your agent here (this takes random actions)\n",
    "    action = random.choice(info[\"valid_actions\"])\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "# Instantiate two random players\n",
    "player1 = RandomPlayer(Color.RED)\n",
    "player2 = RandomPlayer(Color.BLUE)\n",
    "\n",
    "# Create a 2-player game (you can fill remaining slots with random agents if needed)\n",
    "players = [player1, player2]\n",
    "game = Game(players)\n",
    "\n",
    "winner = game.play()\n",
    "print(f\"Winner: {winner}\")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "    player1 = player1(Color.RED)\n",
    "    player2 = player2(Color.BLUE)\n",
    "    game = Game([player1, player2])\n",
    "    winner = game.play()\n",
    "\n",
    "    if winner == Color.RED:\n",
    "        return 1\n",
    "    elif winner == Color.BLUE:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "players = [\n",
    "    RandomPlayer,\n",
    "    MCTSPlayer,\n",
    "    AlphaBetaPlayer,\n",
    "    # GreedyPlayoutsPlayer,\n",
    "    VictoryPointPlayer,\n",
    "    WeightedRandomPlayer,\n",
    "    # ValueFunctionPlayer,\n",
    "]\n",
    "games_per_pair = 10\n",
    "\n",
    "player_names = [p.__name__ for p in players]\n",
    "table = StandingsTable(player_names, start_elo=1000)\n",
    "\n",
    "\n",
    "def play_1v1_tournament(players, games_per_pair, play_game):\n",
    "    tournament_results = []\n",
    "    for player1 in players:\n",
    "        results = play_matches(player1, players, games_per_pair, play_game)\n",
    "        tournament_results.extend(results)\n",
    "    tournament_results = pd.DataFrame(\n",
    "        tournament_results, columns=[\"player1\", \"player2\", \"result\"]\n",
    "    )\n",
    "    return tournament_results\n",
    "\n",
    "\n",
    "def play_matches(player1, players, games_per_pair, play_game):\n",
    "    results = []\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {player1.__name__} vs {opponent.__name__} game {_+1}\")\n",
    "                result = play_game(player1, opponent)\n",
    "                results.append((player1.__name__, opponent.__name__, result))\n",
    "\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {opponent.__name__} vs {player1.__name__} game {_+1}\")\n",
    "                result = play_game(opponent, player1)\n",
    "                results.append(\n",
    "                    (\n",
    "                        player_names[players.index(opponent)],\n",
    "                        player_names[players.index(player1)],\n",
    "                        result,\n",
    "                    )\n",
    "                )\n",
    "    table.add_results_from_array(results)\n",
    "    print(table.bayes_elo())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(table.bayes_elo())\n",
    "print(table.get_win_table())\n",
    "print(table.get_draw_table())\n",
    "file = \"catan_1v1_tournament_results.pkl\"\n",
    "pickle.dump(table, open(file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "table.add_player(GreedyPlayoutsPlayer.__name__)\n",
    "players.append(GreedyPlayoutsPlayer) if GreedyPlayoutsPlayer not in players else None\n",
    "\n",
    "play_matches(GreedyPlayoutsPlayer, players, games_per_pair * 2, play_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play_1v1_tournament(players, games_per_pair, play_game)\n",
    "\n",
    "\n",
    "# table.add_results_from_dataframe(results)  # Adding multiple results\n",
    "print(table.bayes_elo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a petting zoo environment to see if it has all the functions and attributes needed\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-1,\n",
    ")\n",
    "\n",
    "\n",
    "# test reset\n",
    "env.reset()\n",
    "print(env.agent_selection)\n",
    "\n",
    "# # Get initial state for first agent\n",
    "# state, reward, termination, truncation, info = env.last()\n",
    "# print(state, info)\n",
    "\n",
    "\n",
    "# ab_player = CatanPlayerWrapper(AlphaBetaPlayer, Color.RED)\n",
    "# prediction = ab_player.predict(state, info, env)\n",
    "# action = ab_player.select_actions(prediction, info)\n",
    "# print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
