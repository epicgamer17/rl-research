{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5607c5be",
   "metadata": {},
   "source": [
    "*** TO DO FOR CATAN: ***\n",
    "RAINBOW: \n",
    "    1. vs Random\n",
    "    2. vs Weighted Random\n",
    "    3. vs MTCS\n",
    "    4. vs Victory Point\n",
    "    5. vs AlphaBeta\n",
    "Masked PPO the same \n",
    "NFSP \n",
    "MuZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa0589",
   "metadata": {},
   "source": [
    "MUZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d597228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=True,\n",
    ")\n",
    "env = record_video_wrapper(copy.deepcopy(env), \"./videos\", 1)\n",
    "params = {\n",
    "    \"num_simulations\": 25,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"n_step\": 750,\n",
    "    \"residual_layers\": [],\n",
    "    \"dense_layer_widths\": [128] * 3,\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 512,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    \"num_workers\": 5,\n",
    "    \"min_replay_buffer_size\": 512,\n",
    "    \"root_dirichlet_alpha\": 0.03,\n",
    "    \"learning_rate\": 0.001,  # 0.001 or sgd 0.1\n",
    "    \"optimizer\": Adam,  # SGD\n",
    "    \"temperature_updates\": [500],\n",
    "    \"to_play_loss_factor\": 1.0,\n",
    "    \"transfer_interval\": 1000,\n",
    "}\n",
    "game_config = CatanConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"catan_testing_2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[\n",
    "        CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "        CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "    ],\n",
    ")\n",
    "agent.checkpoint_interval = 1\n",
    "agent.test_interval = 100\n",
    "agent.test_trials = 10\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_onehot as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    # \"residual_filters\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    # ),\n",
    "    # \"residual_stacks\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    # ),\n",
    "    \"residual_layers\": hp.choice(\"residual_layers\", [[]]),\n",
    "    \"actor_conv_layers\": hp.choice(\"actor_conv_layers\", [[]]),\n",
    "    \"critic_conv_layers\": hp.choice(\"critic_conv_layers\", [[]]),\n",
    "    \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "    \"to_play_conv_layers\": hp.choice(\"to_play_conv_layers\", [[]]),\n",
    "    \"output_layer_widths\": scope.int(\n",
    "        hp.quniform(\"output_layer_widths\", 16, 16 + 1e-8, 16)\n",
    "    ),\n",
    "    \"dense_layer_width\": scope.int(2 ** hp.quniform(\"dense_layer_width\", 6, 9, 1)),\n",
    "    \"dense_layers\": scope.int(hp.quniform(\"dense_layers\", 1, 1 + 1e-8, 1)),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": 2 ** hp.quniform(\"value_loss_factor\", 0.0, 0.0 + 1e-8, 1.0),\n",
    "    \"to_play_loss_factor\": 2 ** hp.quniform(\"to_play_loss_factor\", 0.0, 1.0, 1.0),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -5, 0, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        800 * 2 ** hp.quniform(\"num_simulations\", -5, -4 + 1e-8, 1)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 32, 512, 32))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"to_play_loss_function\": hp.choice(\n",
    "        \"to_play_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(500), np.log(1000), 100)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 11, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        2 ** hp.quniform(\"min_replay_buffer_size\", 5, 10, 1)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [750]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\",\n",
    "        [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))],\n",
    "        # \"clipnorm\",\n",
    "        # [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.quniform(\"per_alpha\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta\": hp.quniform(\"per_beta\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta_final\": hp.quniform(\"per_beta_final\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 4, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\"), 0.1]),\n",
    "    \"transfer_interval\": scope.int(10 ** hp.quniform(\"transfer_interval\", 0, 1, 1)),\n",
    "    \"vps_to_win\": hp.quniform(\"vps_to_win\", 3, 3 + 1e-8, 1),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    params[\"dense_layer_widths\"] = [params[\"dense_layer_width\"]] * params[\n",
    "        \"dense_layers\"\n",
    "    ]\n",
    "    del params[\"dense_layer_width\"]\n",
    "    del params[\"dense_layers\"]\n",
    "    if params[\"output_layer_widths\"] != 0:\n",
    "        params[\"actor_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"critic_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"reward_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"to_play_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "    else:\n",
    "        params[\"actor_dense_layer_widths\"] = []\n",
    "        params[\"critic_dense_layer_widths\"] = []\n",
    "        params[\"reward_dense_layer_widths\"] = []\n",
    "        params[\"to_play_dense_layer_widths\"] = []\n",
    "    del params[\"output_layer_widths\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.catan_config import CatanConfig\n",
    "import torch\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = CatanConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "            episode_length += 1\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"catan_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)\n",
    "\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        make_env=CatanConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=CatanConfig,\n",
    "        games_per_pair=10,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=1,\n",
    "        test_interval=1000,\n",
    "        test_trials=100,\n",
    "        test_agents=[\n",
    "            CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "            CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        ],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65445e1d",
   "metadata": {},
   "source": [
    "RAINBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 30000\n",
      "Using         adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using         learning_rate                 : 0.001\n",
      "Using         clipnorm                      : 0.0\n",
      "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.KLDivergenceLoss object at 0x10575b3a0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : orthogonal\n",
      "Using         minibatch_size                : 64\n",
      "Using         replay_buffer_size            : 50000\n",
      "Using         min_replay_buffer_size        : 5000\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using default residual_layers               : []\n",
      "Using default conv_layers                   : []\n",
      "Using         dense_layer_widths            : [512]\n",
      "Using default value_hidden_layer_widths     : []\n",
      "Using default advantage_hidden_layer_widths : []\n",
      "Using default noisy_sigma                   : 0.5\n",
      "Using default eg_epsilon                    : 0.0\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using default eg_epsilon_decay_type         : linear\n",
      "Using default eg_epsilon_final_step         : 30000\n",
      "Using default dueling                       : True\n",
      "Using         discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 100\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 16\n",
      "Using         per_alpha                     : 0.5\n",
      "Using         per_beta                      : 0.5\n",
      "Using default per_beta_final                : 1.0\n",
      "Using         per_epsilon                   : 1e-06\n",
      "Using         n_step                        : 9\n",
      "Using         atom_size                     : 51\n",
      "Using device: cpu\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: <CatanatronWrapper<OrderEnforcing<PassiveEnvChecker<CatanatronEnv<catanatron/Catanatron-v0>>>>>\n",
      "<class 'gymnasium.spaces.box.Box'>\n",
      "Observation dimensions: (614,)\n",
      "Observation dtype: float64\n",
      "num_actions:  290 <class 'int'>\n",
      "float64\n",
      "Max size: 50000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "replay buffer size: 0\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 0 / (5000)\n",
      "filling replay buffer: 50 / (5000)\n",
      "filling replay buffer: 100 / (5000)\n",
      "filling replay buffer: 150 / (5000)\n",
      "filling replay buffer: 200 / (5000)\n",
      "filling replay buffer: 250 / (5000)\n",
      "filling replay buffer: 300 / (5000)\n",
      "filling replay buffer: 350 / (5000)\n",
      "filling replay buffer: 400 / (5000)\n",
      "filling replay buffer: 450 / (5000)\n",
      "filling replay buffer: 500 / (5000)\n",
      "filling replay buffer: 550 / (5000)\n",
      "filling replay buffer: 600 / (5000)\n",
      "filling replay buffer: 650 / (5000)\n",
      "filling replay buffer: 700 / (5000)\n",
      "filling replay buffer: 750 / (5000)\n",
      "filling replay buffer: 800 / (5000)\n",
      "filling replay buffer: 850 / (5000)\n",
      "filling replay buffer: 900 / (5000)\n",
      "filling replay buffer: 950 / (5000)\n",
      "filling replay buffer: 1000 / (5000)\n",
      "filling replay buffer: 1050 / (5000)\n",
      "filling replay buffer: 1100 / (5000)\n",
      "filling replay buffer: 1150 / (5000)\n",
      "filling replay buffer: 1200 / (5000)\n",
      "filling replay buffer: 1250 / (5000)\n",
      "filling replay buffer: 1300 / (5000)\n",
      "filling replay buffer: 1350 / (5000)\n",
      "filling replay buffer: 1400 / (5000)\n",
      "filling replay buffer: 1450 / (5000)\n",
      "filling replay buffer: 1500 / (5000)\n",
      "filling replay buffer: 1550 / (5000)\n",
      "filling replay buffer: 1600 / (5000)\n",
      "filling replay buffer: 1650 / (5000)\n",
      "filling replay buffer: 1700 / (5000)\n",
      "filling replay buffer: 1750 / (5000)\n",
      "filling replay buffer: 1800 / (5000)\n",
      "filling replay buffer: 1850 / (5000)\n",
      "filling replay buffer: 1900 / (5000)\n",
      "filling replay buffer: 1950 / (5000)\n",
      "filling replay buffer: 2000 / (5000)\n",
      "filling replay buffer: 2050 / (5000)\n",
      "filling replay buffer: 2100 / (5000)\n",
      "filling replay buffer: 2150 / (5000)\n",
      "filling replay buffer: 2200 / (5000)\n",
      "filling replay buffer: 2250 / (5000)\n",
      "filling replay buffer: 2300 / (5000)\n",
      "filling replay buffer: 2350 / (5000)\n",
      "filling replay buffer: 2400 / (5000)\n",
      "filling replay buffer: 2450 / (5000)\n",
      "filling replay buffer: 2500 / (5000)\n",
      "filling replay buffer: 2550 / (5000)\n",
      "filling replay buffer: 2600 / (5000)\n",
      "filling replay buffer: 2650 / (5000)\n",
      "filling replay buffer: 2700 / (5000)\n",
      "filling replay buffer: 2750 / (5000)\n",
      "filling replay buffer: 2800 / (5000)\n",
      "filling replay buffer: 2850 / (5000)\n",
      "filling replay buffer: 2900 / (5000)\n",
      "filling replay buffer: 2950 / (5000)\n",
      "filling replay buffer: 3000 / (5000)\n",
      "filling replay buffer: 3050 / (5000)\n",
      "filling replay buffer: 3100 / (5000)\n",
      "filling replay buffer: 3150 / (5000)\n",
      "filling replay buffer: 3200 / (5000)\n",
      "filling replay buffer: 3250 / (5000)\n",
      "filling replay buffer: 3300 / (5000)\n",
      "filling replay buffer: 3350 / (5000)\n",
      "filling replay buffer: 3400 / (5000)\n",
      "filling replay buffer: 3450 / (5000)\n",
      "filling replay buffer: 3500 / (5000)\n",
      "filling replay buffer: 3550 / (5000)\n",
      "filling replay buffer: 3600 / (5000)\n",
      "filling replay buffer: 3650 / (5000)\n",
      "filling replay buffer: 3700 / (5000)\n",
      "filling replay buffer: 3750 / (5000)\n",
      "filling replay buffer: 3800 / (5000)\n",
      "filling replay buffer: 3850 / (5000)\n",
      "filling replay buffer: 3900 / (5000)\n",
      "filling replay buffer: 3950 / (5000)\n",
      "filling replay buffer: 4000 / (5000)\n",
      "filling replay buffer: 4050 / (5000)\n",
      "filling replay buffer: 4100 / (5000)\n",
      "filling replay buffer: 4150 / (5000)\n",
      "filling replay buffer: 4200 / (5000)\n",
      "filling replay buffer: 4250 / (5000)\n",
      "filling replay buffer: 4300 / (5000)\n",
      "filling replay buffer: 4350 / (5000)\n",
      "filling replay buffer: 4400 / (5000)\n",
      "filling replay buffer: 4450 / (5000)\n",
      "filling replay buffer: 4500 / (5000)\n",
      "filling replay buffer: 4550 / (5000)\n",
      "filling replay buffer: 4600 / (5000)\n",
      "filling replay buffer: 4650 / (5000)\n",
      "filling replay buffer: 4700 / (5000)\n",
      "filling replay buffer: 4750 / (5000)\n",
      "filling replay buffer: 4800 / (5000)\n",
      "filling replay buffer: 4850 / (5000)\n",
      "filling replay buffer: 4900 / (5000)\n",
      "filling replay buffer: 4950 / (5000)\n",
      "score:  0\n",
      "score:  -996.1491555535343\n",
      "score:  0\n",
      "score:  -997.124547168294\n",
      "score:  -995.1180891182236\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -998.0301716233378\n",
      "score:  -998.0405846155867\n",
      "score:  -998.0573641713714\n",
      "score:  -996.1213280232319\n",
      "score:  -997.0731959681603\n",
      "score:  -997.0541088215597\n",
      "score:  0\n",
      "score:  -997.0591128321536\n",
      "score:  -998.0614396296589\n",
      "score:  0\n",
      "score:  -997.1305795846123\n",
      "score:  -998.0749628419576\n",
      "score:  -998.0421515993676\n",
      "score:  0\n",
      "score:  -997.1383169564563\n",
      "score:  0\n",
      "score:  -998.0435216821684\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -997.119078340727\n",
      "score:  -998.0248456335664\n",
      "score:  -997.0664562341719\n",
      "score:  -997.1291444438912\n",
      "score:  -996.0815611142502\n",
      "score:  -998.0608579452148\n",
      "score:  -998.0499677410783\n",
      "score:  0\n",
      "score:  -996.0536397987007\n",
      "score:  -997.0570533874405\n",
      "score:  -996.1038337921133\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -997.0517510477425\n",
      "score:  -997.1251222301058\n",
      "score:  -997.141749088027\n",
      "score:  -998.0946899420788\n",
      "score:  -998.0801536910312\n",
      "score:  0\n",
      "score:  -998.0876268831048\n",
      "Test score {'score': -638.2529232300585, 'max_score': 0, 'min_score': -998.0946899420788}\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -992.3520373544178\n",
      "score:  0\n",
      "score:  -997.1170608886654\n",
      "score:  -992.3665554128362\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -996.1209401172437\n",
      "score:  0\n",
      "score:  -997.0997103870069\n",
      "score:  -994.2011607737701\n",
      "score:  -995.1395234078162\n",
      "score:  0\n",
      "score:  -995.2429133988927\n",
      "score:  -997.0787518938209\n",
      "score:  1009.7297064182668\n",
      "score:  0\n",
      "score:  -995.1613475026606\n",
      "score:  -997.0337108604806\n",
      "score:  -994.2617330533077\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  1009.6309629235142\n",
      "score:  0\n",
      "score:  -996.1290780021285\n",
      "score:  1009.6242232716329\n",
      "score:  -995.2138050249071\n",
      "score:  -997.0860462570572\n",
      "score:  -993.320067993031\n",
      "score:  -993.1413602905824\n",
      "score:  -998.096023259071\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  1009.6792407460356\n",
      "score:  1009.6213362933685\n",
      "score:  0\n",
      "score:  -993.3153901696838\n",
      "score:  -996.1729581150583\n",
      "score:  0\n",
      "score:  -998.0874356266675\n",
      "score:  -996.0983750662649\n",
      "score:  -998.0309594365067\n",
      "score:  -996.1610752821892\n",
      "score:  -996.1038337921133\n",
      "score:  -996.1271419603557\n",
      "score:  0\n",
      "score:  -998.0569755859188\n",
      "Test score {'score': -436.7206100251926, 'max_score': 1009.7297064182668, 'min_score': -998.096023259071}\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "score:  -993.2515805700892\n",
      "score:  -993.208250009288\n",
      "score:  0\n",
      "score:  -995.0965603052315\n",
      "score:  0\n",
      "score:  -992.3833313694208\n",
      "score:  0\n",
      "score:  1009.6802087669123\n",
      "score:  -993.305355228859\n",
      "score:  0\n",
      "score:  1009.7024999867314\n",
      "score:  -997.1411773520857\n",
      "score:  1009.6850503236659\n",
      "score:  -993.2859116072015\n",
      "score:  0\n",
      "score:  1009.6811768846007\n",
      "score:  -996.0925179442601\n",
      "score:  1009.5829258996471\n",
      "score:  -994.1381907354637\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -997.1011602418134\n",
      "score:  -997.0458483539722\n",
      "score:  0\n",
      "score:  -997.1210943809957\n",
      "score:  0\n",
      "score:  1009.7267877982229\n",
      "score:  -998.066473591338\n",
      "score:  -998.0653130693439\n",
      "score:  0\n",
      "score:  1009.7238700536773\n",
      "score:  0\n",
      "score:  0\n",
      "score:  1009.7404155197169\n",
      "score:  0\n",
      "score:  -996.1387437032554\n",
      "score:  0\n",
      "score:  -998.0797696641664\n",
      "score:  -998.0608579452148\n",
      "score:  0\n",
      "score:  -998.0552259892645\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -992.320614764125\n",
      "score:  -993.2184305060259\n",
      "score:  -994.2548426550339\n",
      "score:  0\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -996.1679794762579\n",
      "Test score {'score': -276.48152588459067, 'max_score': 1009.7404155197169, 'min_score': -998.0797696641664}\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "score:  -994.234698365153\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -996.0713594065807\n",
      "score:  1009.5723899501859\n",
      "score:  0\n",
      "score:  -996.1201641888797\n",
      "score:  0\n",
      "score:  0\n",
      "score:  1009.7443126601611\n",
      "score:  -991.4303908296814\n",
      "score:  -998.0734221189867\n",
      "score:  -997.0904141293456\n",
      "score:  -993.2468547863639\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -998.0489924323757\n",
      "score:  0\n",
      "score:  -995.1869257974738\n",
      "score:  -996.1683626783102\n",
      "score:  1009.6802087669123\n",
      "score:  -997.0673362092983\n",
      "score:  -998.065506538037\n",
      "score:  0\n",
      "score:  -998.0958328423552\n",
      "score:  -998.0610518594202\n",
      "score:  -996.1721925918548\n",
      "score:  -998.0946899420788\n",
      "score:  -997.1302926138737\n",
      "score:  1009.7560134394654\n",
      "score:  -997.1457480393357\n",
      "score:  -998.0937370011867\n",
      "score:  -998.0774638892846\n",
      "score:  0\n",
      "score:  -998.0550314924138\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -998.031156340563\n",
      "score:  0\n",
      "score:  1009.7199810890492\n",
      "score:  -998.0563925619938\n",
      "score:  0\n",
      "score:  1009.8245485533878\n",
      "score:  -998.0927835836811\n",
      "score:  0\n",
      "score:  -997.1011602418134\n",
      "score:  -995.0911632838942\n",
      "score:  -994.1720925141144\n",
      "score:  -996.1073389394078\n",
      "score:  -998.0456727325777\n",
      "Test score {'score': -456.9226154698236, 'max_score': 1009.8245485533878, 'min_score': -998.0958328423552}\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "plotting score\n",
      "plotting loss\n",
      "plotting test_score\n",
      "  subkey score\n",
      "  subkey max_score\n",
      "  subkey min_score\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -991.4303908296814\n",
      "score:  -991.3139108107799\n",
      "score:  0\n",
      "score:  0\n",
      "score:  -997.0717321270214\n",
      "score:  -998.0362689404517\n",
      "score:  -991.2476417158621\n",
      "score:  -998.065506538037\n",
      "score:  1009.7462618150615\n",
      "score:  1009.7863055242285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     84\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 85\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../dqn/rainbow/rainbow_agent.py:542\u001b[0m, in \u001b[0;36mRainbowAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_model()\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mset_time_elapsed(time() \u001b[38;5;241m-\u001b[39m start_time)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../base_agent/agent.py:387\u001b[0m, in \u001b[0;36mBaseAgent.run_tests\u001b[0;34m(self, stats)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[1;32m    385\u001b[0m training_step_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mdir\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m test_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_step_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest score\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_score)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(test_score, \u001b[38;5;28mfloat\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../base_agent/agent.py:356\u001b[0m, in \u001b[0;36mBaseAgent.test\u001b[0;34m(self, num_trials, dir)\u001b[0m\n\u001b[1;32m    353\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 356\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_actions(prediction, info)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    361\u001b[0m     state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    362\u001b[0m         action\n\u001b[1;32m    363\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../dqn/rainbow/rainbow_agent.py:173\u001b[0m, in \u001b[0;36mRainbowAgent.predict\u001b[0;34m(self, states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# could change type later\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     state_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(states)\n\u001b[0;32m--> 173\u001b[0m     q_distribution: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_distribution\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../dqn/rainbow/rainbow_network.py:209\u001b[0m, in \u001b[0;36mRainbowNetwork.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     A \u001b[38;5;241m=\u001b[39m S\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# (B, adv_hidden_out || dense_features_out) -> (B, output_size * atom_size) -> (B, output_size, atom_size)\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m A: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvantage_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39matom_size\n\u001b[1;32m    211\u001b[0m )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# (B, output_size, atom_size) -[mean(1)]-> (B, 1, atom_size)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m a_mean \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../modules/dense.py:113\u001b[0m, in \u001b[0;36mNoisyDense.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functional\u001b[38;5;241m.\u001b[39mF\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../modules/dense.py:102\u001b[0m, in \u001b[0;36mNoisyDense.bias\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweight\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_w \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_w \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_w\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbias\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_b \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_b \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_b\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"/content/rl-research\")\n",
    "sys.path.append(\"../..\")\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from wrappers import CatanatronWrapper\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs.catan_config import SinglePlayerCatanConfig\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [512],\n",
    "    \"value_hidden_layers_widths\": [512],  #\n",
    "    \"advatage_hidden_layers_widths\": [512],  #\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 30000,\n",
    "    \"per_epsilon\": 1e-6,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"minibatch_size\": 64,\n",
    "    \"replay_buffer_size\": 50000,\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"n_step\": 9,\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 0.99,  # or 0.999 or even 0.9999 not 0.99 < this makes the start of the game possibly 0.05 after bootstrapping\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 16,\n",
    "}\n",
    "game_config = SinglePlayerCatanConfig()\n",
    "game_config.min_score = -1000\n",
    "game_config.max_score = 1010\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import catanatron.gym\n",
    "import gymnasium as gym\n",
    "\n",
    "from catanatron.gym.envs.catanatron_env import simple_reward\n",
    "from catanatron.gym.utils import get_tournament_total_return\n",
    "from catanatron.gym.utils import get_actual_victory_points\n",
    "\n",
    "\n",
    "def tournament_reward_function(game, p0_color):\n",
    "    winning_color = game.winning_color()\n",
    "    if p0_color == winning_color:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "    elif winning_color is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "\n",
    "\n",
    "env = CatanatronWrapper(\n",
    "    gym.make(\n",
    "        \"catanatron/Catanatron-v0\",\n",
    "        config={\n",
    "            \"enemies\": [RandomPlayer(Color.RED)],\n",
    "            \"invalid_action_reward\": -10,\n",
    "            \"map_type\": \"BASE\",\n",
    "            \"vps_to_win\": 3,\n",
    "            \"representation\": \"vector\",\n",
    "            \"reward_function\": tournament_reward_function,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "agent = RainbowAgent(env, config, \"rainbow-catan-3vps\", device)\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import CatanConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from wrappers import ActionMaskInInfoWrapper, FrameStackWrapper\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 500000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 2048,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"minibatch_size\": 1024,\n",
    "    \"replay_buffer_size\": 1e5,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [1024],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.12,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 5000,\n",
    "    \"sl_minibatch_size\": 1024,\n",
    "    \"sl_replay_buffer_size\": 100000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [1024, 1024, 1024],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 1.0,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 9,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=CatanConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = False\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    ")\n",
    "\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"nfsp-catan\", device=\"cpu\")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import catanatron.gym\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"catanatron/Catanatron-v0\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    # your agent here (this takes random actions)\n",
    "    action = random.choice(info[\"valid_actions\"])\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "# Instantiate two random players\n",
    "player1 = RandomPlayer(Color.RED)\n",
    "player2 = RandomPlayer(Color.BLUE)\n",
    "\n",
    "# Create a 2-player game (you can fill remaining slots with random agents if needed)\n",
    "players = [player1, player2]\n",
    "game = Game(players)\n",
    "\n",
    "winner = game.play()\n",
    "print(f\"Winner: {winner}\")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "    player1 = player1(Color.RED)\n",
    "    player2 = player2(Color.BLUE)\n",
    "    game = Game([player1, player2])\n",
    "    winner = game.play()\n",
    "\n",
    "    if winner == Color.RED:\n",
    "        return 1\n",
    "    elif winner == Color.BLUE:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "players = [\n",
    "    RandomPlayer,\n",
    "    MCTSPlayer,\n",
    "    AlphaBetaPlayer,\n",
    "    # GreedyPlayoutsPlayer,\n",
    "    VictoryPointPlayer,\n",
    "    WeightedRandomPlayer,\n",
    "    # ValueFunctionPlayer,\n",
    "]\n",
    "games_per_pair = 10\n",
    "\n",
    "player_names = [p.__name__ for p in players]\n",
    "table = StandingsTable(player_names, start_elo=1000)\n",
    "\n",
    "\n",
    "def play_1v1_tournament(players, games_per_pair, play_game):\n",
    "    tournament_results = []\n",
    "    for player1 in players:\n",
    "        results = play_matches(player1, players, games_per_pair, play_game)\n",
    "        tournament_results.extend(results)\n",
    "    tournament_results = pd.DataFrame(\n",
    "        tournament_results, columns=[\"player1\", \"player2\", \"result\"]\n",
    "    )\n",
    "    return tournament_results\n",
    "\n",
    "\n",
    "def play_matches(player1, players, games_per_pair, play_game):\n",
    "    results = []\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {player1.__name__} vs {opponent.__name__} game {_+1}\")\n",
    "                result = play_game(player1, opponent)\n",
    "                results.append((player1.__name__, opponent.__name__, result))\n",
    "\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {opponent.__name__} vs {player1.__name__} game {_+1}\")\n",
    "                result = play_game(opponent, player1)\n",
    "                results.append(\n",
    "                    (\n",
    "                        player_names[players.index(opponent)],\n",
    "                        player_names[players.index(player1)],\n",
    "                        result,\n",
    "                    )\n",
    "                )\n",
    "    table.add_results_from_array(results)\n",
    "    print(table.bayes_elo())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(table.bayes_elo())\n",
    "print(table.get_win_table())\n",
    "print(table.get_draw_table())\n",
    "file = \"catan_1v1_tournament_results.pkl\"\n",
    "pickle.dump(table, open(file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "table.add_player(GreedyPlayoutsPlayer.__name__)\n",
    "players.append(GreedyPlayoutsPlayer) if GreedyPlayoutsPlayer not in players else None\n",
    "\n",
    "play_matches(GreedyPlayoutsPlayer, players, games_per_pair * 2, play_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play_1v1_tournament(players, games_per_pair, play_game)\n",
    "\n",
    "\n",
    "# table.add_results_from_dataframe(results)  # Adding multiple results\n",
    "print(table.bayes_elo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a petting zoo environment to see if it has all the functions and attributes needed\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "# env = catan_env(\n",
    "#     render_mode=\"rgb_array\",\n",
    "#     num_players=2,\n",
    "#     map_type=\"BASE\",\n",
    "#     vps_to_win=10,\n",
    "#     representation=\"vector\",\n",
    "#     invalid_action_reward=-1,\n",
    "#     auto_play_single_action=False,\n",
    "# )\n",
    "\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=False,\n",
    ")\n",
    "env = record_video_wrapper(env, \"./videos\", 1)\n",
    "env.reset()\n",
    "\n",
    "ab_player = CatanPlayerWrapper(AlphaBetaPlayer, Color.RED)\n",
    "\n",
    "for i in range(100):\n",
    "    state, reward, termination, truncation, info = env.last()\n",
    "    prediction = ab_player.predict(state, info, env)\n",
    "    action = ab_player.select_actions(prediction, info).item()\n",
    "    print(action)\n",
    "    print(info)\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95931df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    render_mode=\"rgb_array\",\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-1,\n",
    ")\n",
    "\n",
    "# Set metadata fps for 'human' if desired (not required for rgb_array)\n",
    "env.metadata[\"render_fps\"] = 10\n",
    "\n",
    "# Reset the environment (seed optional)\n",
    "env.reset(seed=0)\n",
    "print(\"Starting agent selection:\", env.agent_selection)\n",
    "\n",
    "# # Render one frame as an RGB array, then plot it\n",
    "# frame = env.render()  # will return a HxWx3 numpy array if renderer implemented\n",
    "# if frame is None:\n",
    "#     print(\n",
    "#         \"Renderer returned None. Make sure env.render(mode='rgb_array') is implemented.\"\n",
    "#     )\n",
    "# else:\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(frame)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# Setup matplotlib figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "print(\"Starting Catan Game Visualization...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run game for a few steps\n",
    "step_count = 0\n",
    "max_steps = 20\n",
    "\n",
    "while step_count < max_steps:\n",
    "    # Render current state\n",
    "    rgb_array = env.render()\n",
    "\n",
    "    if rgb_array is not None:\n",
    "        # Display in notebook\n",
    "        # clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.imshow(rgb_array)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Catan Game - Step {step_count}\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "\n",
    "        # Print game info\n",
    "        current_agent = env.agent_selection\n",
    "        print(f\"\\nStep {step_count}\")\n",
    "        print(f\"Current Player: {current_agent}\")\n",
    "\n",
    "        if hasattr(env, \"game\") and hasattr(env.game, \"state\"):\n",
    "            print(f\"Turn Number: {env.game.state.num_turns}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Check if game is done\n",
    "    if env.terminations[env.agent_selection] or env.truncations[env.agent_selection]:\n",
    "        print(\"\\nGame Over!\")\n",
    "        break\n",
    "\n",
    "    # Get valid actions\n",
    "    obs = env.observe(env.agent_selection)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "    valid_actions = np.where(action_mask == 1)[0]\n",
    "\n",
    "    if len(valid_actions) > 0:\n",
    "        # Take a random valid action\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(action)\n",
    "    else:\n",
    "        print(\"No valid actions available!\")\n",
    "        break\n",
    "\n",
    "    step_count += 1\n",
    "    time.sleep(0.5)  # Pause to see each frame\n",
    "\n",
    "print(\"\\nTest completed!\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a11a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from custom_gym_envs.envs.catan import ActionType\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import ACTIONS_ARRAY\n",
    "\n",
    "n_steps = 25\n",
    "delay = 0.25  # seconds between frames\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # If the current agent is terminated/truncated, we still call env_instance.step per PettingZoo convention:\n",
    "    current_agent = env.agent_selection\n",
    "\n",
    "    # get observation for current agent\n",
    "    obs = env.observe(current_agent)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "\n",
    "    legal_indices = np.nonzero(action_mask)[0]\n",
    "    if len(legal_indices) == 0:\n",
    "        # no legal moves available (should rarely happen). Choose end-turn fallback if available:\n",
    "        try:\n",
    "            end_turn_index = ACTIONS_ARRAY.index((ActionType.END_TURN, None))\n",
    "            action_choice = end_turn_index\n",
    "        except Exception:\n",
    "            # fallback to 0\n",
    "            action_choice = 0\n",
    "    else:\n",
    "        action_choice = int(random.choice(legal_indices))\n",
    "\n",
    "    # Step the env with the chosen action\n",
    "    env.step(action_choice)\n",
    "\n",
    "    # Render and display frame\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(frame)\n",
    "        plt.title(f\"Step {step+1} - Agent: {current_agent} - Action: {action_choice}\")\n",
    "        plt.axis(\"off\")\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No frame returned (render returned None)\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# After rollout, print summary\n",
    "print(\"Done. Final agent selection:\", env.agent_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
