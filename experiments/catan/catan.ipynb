{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5607c5be",
   "metadata": {},
   "source": [
    "*** TO DO FOR CATAN: ***\n",
    "RAINBOW: \n",
    "    1. vs Random\n",
    "    2. vs Weighted Random\n",
    "    3. vs MTCS\n",
    "    4. vs Victory Point\n",
    "    5. vs AlphaBeta\n",
    "Masked PPO the same \n",
    "NFSP \n",
    "MuZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa0589",
   "metadata": {},
   "source": [
    "MUZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d2733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 200000\n",
      "Using default adam_epsilon                  : 1e-08\n",
      "Using default momentum                      : 0.9\n",
      "Using         learning_rate                 : 0.001\n",
      "Using         clipnorm                      : 10\n",
      "Using         optimizer                     : <class 'torch.optim.adam.Adam'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using default loss_function                 : <class 'utils.utils.MSELoss'>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 32\n",
      "Using         replay_buffer_size            : 100000\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using default num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "Using         known_bounds                  : [-1, 1]\n",
      "Using         residual_layers               : []\n",
      "Using default conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128, 128, 128]\n",
      "Using default representation_residual_layers: []\n",
      "Using default representation_conv_layers    : []\n",
      "Using default representation_dense_layer_widths: [128, 128, 128]\n",
      "Using default dynamics_residual_layers      : []\n",
      "Using default dynamics_conv_layers          : []\n",
      "Using default dynamics_dense_layer_widths   : [128, 128, 128]\n",
      "Using         reward_conv_layers            : []\n",
      "Using         reward_dense_layer_widths     : [32]\n",
      "Using         to_play_conv_layers           : []\n",
      "Using         to_play_dense_layer_widths    : [32]\n",
      "Using         critic_conv_layers            : []\n",
      "Using         critic_dense_layer_widths     : [32]\n",
      "Using         actor_conv_layers             : []\n",
      "Using         actor_dense_layer_widths      : [32]\n",
      "Using default noisy_sigma                   : 0.0\n",
      "Using default games_per_generation          : 100\n",
      "Using         value_loss_factor             : 1.0\n",
      "Using         to_play_loss_factor           : 1.0\n",
      "Using default weight_decay                  : 0.0001\n",
      "Using         root_dirichlet_alpha          : 0.03\n",
      "Using default root_exploration_fraction     : 0.25\n",
      "Using         num_simulations               : 50\n",
      "Using default temperatures                  : [1.0, 0.0]\n",
      "Using         temperature_updates           : [150]\n",
      "Using default temperature_with_training_steps: False\n",
      "Using default clip_low_prob                 : 0.0\n",
      "Using default pb_c_base                     : 19652\n",
      "Using default pb_c_init                     : 1.25\n",
      "Using default value_loss_function           : <utils.utils.MSELoss object at 0x31af07fd0>\n",
      "Using default reward_loss_function          : <utils.utils.MSELoss object at 0x31af07100>\n",
      "Using         policy_loss_function          : <utils.utils.KLDivergenceLoss object at 0x105d33160>\n",
      "Using default to_play_loss_function         : <utils.utils.CategoricalCrossentropyLoss object at 0x31af07040>\n",
      "Using         action_function               : <function action_as_onehot at 0x31af039a0>\n",
      "Using         n_step                        : 750\n",
      "Using default discount_factor               : 1.0\n",
      "Using default unroll_steps                  : 5\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using default per_epsilon                   : 1e-06\n",
      "Using default per_use_batch_weights         : False\n",
      "Using default per_initial_priority_max      : False\n",
      "Using         support_range                 : None\n",
      "Using default multi_process                 : True\n",
      "Using         num_workers                   : 4\n",
      "Using         lr_ratio                      : 0.1\n",
      "Using         transfer_interval             : 1\n",
      "Using         reanalyze_ratio               : 0.0\n",
      "Using         reanalyze_method              : mcts\n",
      "Using default reanalyze_tau                 : 0.3\n",
      "Using         injection_frac                : 0.25\n",
      "Using         reanalyze_noise               : True\n",
      "Using default reanalyze_update_priorities   : False\n",
      "Using         gumbel                        : False\n",
      "Using         gumbel_m                      : 16\n",
      "Using default gumbel_cvisit                 : 50\n",
      "Using default gumbel_cscale                 : 1.0\n",
      "Using         consistency_loss_factor       : 0.0\n",
      "Using         projector_output_dim          : 128\n",
      "Using         projector_hidden_dim          : 128\n",
      "Using         predictor_output_dim          : 128\n",
      "Using         predictor_hidden_dim          : 64\n",
      "Using default mask_absorbing                : True\n",
      "Using         value_prefix                  : False\n",
      "Using         lstm_horizon_len              : 5\n",
      "Using         lstm_hidden_size              : 64\n",
      "Using         q_estimation_method           : v_mix\n",
      "Using         stochastic                    : True\n",
      "Using default num_chance                    : 32\n",
      "Using default sigma_loss                    : <utils.utils.CategoricalCrossentropyLoss object at 0x31af06f80>\n",
      "Using default afterstate_residual_layers    : []\n",
      "Using default afterstate_conv_layers        : []\n",
      "Using default afterstate_dense_layer_widths : [128, 128, 128]\n",
      "Using         chance_conv_layers            : []\n",
      "Using         chance_dense_layer_widths     : [32]\n",
      "Using         vqvae_commitment_cost_factor  : 0.1\n",
      "Using device: cpu\n",
      "making test env\n",
      "Test env with record video\n",
      "env render mode rgb_array\n",
      "petting zoo env\n",
      "Test env: RecordVideo<RecordVideo<AppendAgentSelectionWrapper<FrameStackWrapper<ActionMaskInInfoWrapper<catanatron_v1>>>>>\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (2458,)\n",
      "Observation dtype: float32\n",
      "num_actions:  290 <class 'int'>\n",
      "Test agents: [<agents.catan_player_wrapper.CatanPlayerWrapper object at 0x31af07f40>, <agents.catan_player_wrapper.CatanPlayerWrapper object at 0x31af40310>]\n",
      "Started recording episode 0 to ./videos/episode_000000.mp4\n",
      "Hidden state shape: (32, 128)\n",
      "dynamics input shape (32, 128)\n",
      "Hidden state shape: (32, 128)\n",
      "dynamics input shape (32, 128)\n",
      "Warning: for board games it is recommnded to have n_step >= game length\n",
      "Max size: 100000\n",
      "Initializing stat 'score' with subkeys None\n",
      "Initializing stat 'policy_loss' with subkeys None\n",
      "Initializing stat 'value_loss' with subkeys None\n",
      "Initializing stat 'reward_loss' with subkeys None\n",
      "Initializing stat 'to_play_loss' with subkeys None\n",
      "Initializing stat 'cons_loss' with subkeys None\n",
      "Initializing stat 'q_loss' with subkeys None\n",
      "Initializing stat 'sigma_loss' with subkeys None\n",
      "Initializing stat 'vqvae_commitment_cost' with subkeys None\n",
      "Initializing stat 'loss' with subkeys None\n",
      "Initializing stat 'test_score' with subkeys ['score', 'max_score', 'min_score']\n",
      "Initializing stat 'episode_length' with subkeys None\n",
      "Initializing stat 'num_codes' with subkeys None\n",
      "Initializing stat 'test_score_vs_RandomPlayer' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "Initializing stat 'test_score_vs_AlphaBetaPlayer' with subkeys ['score', 'player_0_score', 'player_1_score', 'player_0_win%', 'player_1_win%']\n",
      "[Worker 0] Starting self-play...\n",
      "[Worker 1] Starting self-play...\n",
      "[Worker 2] Starting self-play...\n",
      "[Worker 3] Starting self-play...\n",
      "Started recording episode 0 to ./videos/catan_stochastic/0/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_stochastic/1/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_stochastic/2/episode_000000.mp4\n",
      "Started recording episode 0 to ./videos/catan_stochastic/3/episode_000000.mp4\n",
      "Stopped recording episode 0. Recorded 187 frames.\n",
      "Buffer size: 186\n",
      "Started recording episode 1 to ./videos/catan_stochastic/0/episode_000001.mp4\n",
      "Stopped recording episode 0. Recorded 217 frames.\n",
      "Buffer size: 402\n",
      "Started recording episode 1 to ./videos/catan_stochastic/3/episode_000001.mp4\n",
      "Stopped recording episode 0. Recorded 248 frames.\n",
      "Buffer size: 649\n",
      "Started recording episode 1 to ./videos/catan_stochastic/2/episode_000001.mp4\n",
      "Stopped recording episode 0. Recorded 268 frames.\n",
      "Buffer size: 916\n",
      "Started recording episode 1 to ./videos/catan_stochastic/1/episode_000001.mp4\n",
      "Stopped recording episode 1. Recorded 150 frames.\n",
      "Started recording episode 2 to ./videos/catan_stochastic/3/episode_000002.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 1. Recorded 200 frames.\n",
      "Started recording episode 2 to ./videos/catan_stochastic/0/episode_000002.mp4\n",
      "Stopped recording episode 1. Recorded 215 frames.\n",
      "Started recording episode 2 to ./videos/catan_stochastic/2/episode_000002.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 2. Recorded 202 frames.\n",
      "Started recording episode 3 to ./videos/catan_stochastic/3/episode_000003.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 2. Recorded 219 frames.\n",
      "Started recording episode 3 to ./videos/catan_stochastic/0/episode_000003.mp4\n",
      "Stopped recording episode 1. Recorded 359 frames.\n",
      "Started recording episode 2 to ./videos/catan_stochastic/1/episode_000002.mp4\n",
      "Stopped recording episode 2. Recorded 162 frames.\n",
      "Started recording episode 3 to ./videos/catan_stochastic/2/episode_000003.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 3. Recorded 190 frames.\n",
      "Started recording episode 4 to ./videos/catan_stochastic/3/episode_000004.mp4\n",
      "Stopped recording episode 3. Recorded 187 frames.\n",
      "Started recording episode 4 to ./videos/catan_stochastic/0/episode_000004.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 3. Recorded 231 frames.\n",
      "Started recording episode 4 to ./videos/catan_stochastic/2/episode_000004.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 2. Recorded 254 frames.\n",
      "Started recording episode 3 to ./videos/catan_stochastic/1/episode_000003.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 4. Recorded 254 frames.\n",
      "Started recording episode 5 to ./videos/catan_stochastic/0/episode_000005.mp4\n",
      "Stopped recording episode 4. Recorded 194 frames.\n",
      "Started recording episode 5 to ./videos/catan_stochastic/2/episode_000005.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 4. Recorded 320 frames.\n",
      "Started recording episode 5 to ./videos/catan_stochastic/3/episode_000005.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 3. Recorded 344 frames.\n",
      "Started recording episode 4 to ./videos/catan_stochastic/1/episode_000004.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 5. Recorded 230 frames.\n",
      "Started recording episode 6 to ./videos/catan_stochastic/0/episode_000006.mp4\n",
      "Stopped recording episode 5. Recorded 173 frames.\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Started recording episode 6 to ./videos/catan_stochastic/3/episode_000006.mp4\n",
      "Stopped recording episode 6. Recorded 233 frames.\n",
      "Started recording episode 7 to ./videos/catan_stochastic/0/episode_000007.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 5. Recorded 441 frames.\n",
      "Started recording episode 6 to ./videos/catan_stochastic/2/episode_000006.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 6. Recorded 410 frames.\n",
      "Started recording episode 7 to ./videos/catan_stochastic/3/episode_000007.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 6. Recorded 187 frames.\n",
      "Started recording episode 7 to ./videos/catan_stochastic/2/episode_000007.mp4\n",
      "Stopped recording episode 7. Recorded 242 frames.\n",
      "Started recording episode 8 to ./videos/catan_stochastic/0/episode_000008.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 4. Recorded 513 frames.\n",
      "Started recording episode 5 to ./videos/catan_stochastic/1/episode_000005.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 7. Recorded 233 frames.\n",
      "Started recording episode 8 to ./videos/catan_stochastic/3/episode_000008.mp4\n",
      "Stopped recording episode 7. Recorded 494 frames.\n",
      "Started recording episode 8 to ./videos/catan_stochastic/2/episode_000008.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 8. Recorded 499 frames.\n",
      "Started recording episode 9 to ./videos/catan_stochastic/0/episode_000009.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 5. Recorded 517 frames.\n",
      "Started recording episode 6 to ./videos/catan_stochastic/1/episode_000006.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 8. Recorded 412 frames.\n",
      "Started recording episode 9 to ./videos/catan_stochastic/3/episode_000009.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 9. Recorded 528 frames.\n",
      "Started recording episode 10 to ./videos/catan_stochastic/0/episode_000010.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 8. Recorded 564 frames.\n",
      "Stopped recording episode 6. Recorded 495 frames.\n",
      "Started recording episode 9 to ./videos/catan_stochastic/2/episode_000009.mp4\n",
      "Started recording episode 7 to ./videos/catan_stochastic/1/episode_000007.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 9. Recorded 535 frames.\n",
      "Started recording episode 10 to ./videos/catan_stochastic/3/episode_000010.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 9. Recorded 527 frames.\n",
      "Started recording episode 10 to ./videos/catan_stochastic/2/episode_000010.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 10. Recorded 549 frames.\n",
      "Started recording episode 11 to ./videos/catan_stochastic/0/episode_000011.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 10. Recorded 515 frames.\n",
      "Started recording episode 11 to ./videos/catan_stochastic/3/episode_000011.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 7. Recorded 632 frames.\n",
      "Started recording episode 8 to ./videos/catan_stochastic/1/episode_000008.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 11. Recorded 525 frames.\n",
      "Started recording episode 12 to ./videos/catan_stochastic/0/episode_000012.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 10. Recorded 578 frames.\n",
      "Started recording episode 11 to ./videos/catan_stochastic/2/episode_000011.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 11. Recorded 511 frames.\n",
      "Started recording episode 12 to ./videos/catan_stochastic/3/episode_000012.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 8. Recorded 561 frames.\n",
      "Started recording episode 9 to ./videos/catan_stochastic/1/episode_000009.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 12. Recorded 505 frames.\n",
      "Started recording episode 13 to ./videos/catan_stochastic/0/episode_000013.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 12. Recorded 476 frames.\n",
      "Started recording episode 13 to ./videos/catan_stochastic/3/episode_000013.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 9. Recorded 470 frames.\n",
      "Started recording episode 10 to ./videos/catan_stochastic/1/episode_000010.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 11. Recorded 561 frames.\n",
      "Started recording episode 12 to ./videos/catan_stochastic/2/episode_000012.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 13. Recorded 551 frames.\n",
      "Started recording episode 14 to ./videos/catan_stochastic/0/episode_000014.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 13. Recorded 507 frames.\n",
      "Started recording episode 14 to ./videos/catan_stochastic/3/episode_000014.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 10. Recorded 471 frames.\n",
      "Started recording episode 11 to ./videos/catan_stochastic/1/episode_000011.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 12. Recorded 495 frames.\n",
      "Started recording episode 13 to ./videos/catan_stochastic/2/episode_000013.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 14. Recorded 465 frames.\n",
      "Started recording episode 15 to ./videos/catan_stochastic/3/episode_000015.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 14. Recorded 527 frames.\n",
      "Started recording episode 15 to ./videos/catan_stochastic/0/episode_000015.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 11. Recorded 515 frames.\n",
      "Started recording episode 12 to ./videos/catan_stochastic/1/episode_000012.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 13. Recorded 549 frames.\n",
      "Started recording episode 14 to ./videos/catan_stochastic/2/episode_000014.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 15. Recorded 479 frames.\n",
      "Started recording episode 16 to ./videos/catan_stochastic/3/episode_000016.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 15. Recorded 473 frames.\n",
      "Started recording episode 16 to ./videos/catan_stochastic/0/episode_000016.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 14. Recorded 465 frames.\n",
      "Started recording episode 15 to ./videos/catan_stochastic/2/episode_000015.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 12. Recorded 595 frames.\n",
      "Started recording episode 13 to ./videos/catan_stochastic/1/episode_000013.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 16. Recorded 488 frames.\n",
      "Started recording episode 17 to ./videos/catan_stochastic/0/episode_000017.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 16. Recorded 520 frames.\n",
      "Started recording episode 17 to ./videos/catan_stochastic/3/episode_000017.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 15. Recorded 509 frames.\n",
      "Started recording episode 16 to ./videos/catan_stochastic/2/episode_000016.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 13. Recorded 774 frames.\n",
      "Started recording episode 14 to ./videos/catan_stochastic/1/episode_000014.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 17. Recorded 493 frames.\n",
      "Started recording episode 18 to ./videos/catan_stochastic/3/episode_000018.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 17. Recorded 549 frames.\n",
      "Started recording episode 18 to ./videos/catan_stochastic/0/episode_000018.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 16. Recorded 430 frames.\n",
      "Started recording episode 17 to ./videos/catan_stochastic/2/episode_000017.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 14. Recorded 387 frames.\n",
      "Started recording episode 15 to ./videos/catan_stochastic/1/episode_000015.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 18. Recorded 501 frames.\n",
      "Started recording episode 19 to ./videos/catan_stochastic/3/episode_000019.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 17. Recorded 481 frames.\n",
      "Started recording episode 18 to ./videos/catan_stochastic/2/episode_000018.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 18. Recorded 692 frames.\n",
      "Started recording episode 19 to ./videos/catan_stochastic/0/episode_000019.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 15. Recorded 504 frames.\n",
      "Started recording episode 16 to ./videos/catan_stochastic/1/episode_000016.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 19. Recorded 473 frames.\n",
      "Started recording episode 20 to ./videos/catan_stochastic/3/episode_000020.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 18. Recorded 545 frames.\n",
      "Started recording episode 19 to ./videos/catan_stochastic/2/episode_000019.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 19. Recorded 587 frames.\n",
      "Started recording episode 20 to ./videos/catan_stochastic/0/episode_000020.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 16. Recorded 526 frames.\n",
      "Started recording episode 17 to ./videos/catan_stochastic/1/episode_000017.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 20. Recorded 571 frames.\n",
      "Started recording episode 21 to ./videos/catan_stochastic/3/episode_000021.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 19. Recorded 562 frames.\n",
      "Started recording episode 20 to ./videos/catan_stochastic/2/episode_000020.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 17. Recorded 415 frames.\n",
      "Started recording episode 18 to ./videos/catan_stochastic/1/episode_000018.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 20. Recorded 552 frames.\n",
      "Started recording episode 21 to ./videos/catan_stochastic/0/episode_000021.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 21. Recorded 512 frames.\n",
      "Started recording episode 22 to ./videos/catan_stochastic/3/episode_000022.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 20. Recorded 541 frames.\n",
      "Started recording episode 21 to ./videos/catan_stochastic/2/episode_000021.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 18. Recorded 456 frames.\n",
      "Started recording episode 19 to ./videos/catan_stochastic/1/episode_000019.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 21. Recorded 487 frames.\n",
      "Started recording episode 22 to ./videos/catan_stochastic/0/episode_000022.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 22. Recorded 570 frames.\n",
      "Started recording episode 23 to ./videos/catan_stochastic/3/episode_000023.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 21. Recorded 509 frames.\n",
      "Started recording episode 22 to ./videos/catan_stochastic/2/episode_000022.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 22. Recorded 461 frames.\n",
      "Started recording episode 23 to ./videos/catan_stochastic/0/episode_000023.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 19. Recorded 648 frames.\n",
      "Started recording episode 20 to ./videos/catan_stochastic/1/episode_000020.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 23. Recorded 450 frames.\n",
      "Started recording episode 24 to ./videos/catan_stochastic/3/episode_000024.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 22. Recorded 459 frames.\n",
      "Started recording episode 23 to ./videos/catan_stochastic/2/episode_000023.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 23. Recorded 517 frames.\n",
      "Started recording episode 24 to ./videos/catan_stochastic/0/episode_000024.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 20. Recorded 509 frames.\n",
      "Started recording episode 21 to ./videos/catan_stochastic/1/episode_000021.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 24. Recorded 522 frames.\n",
      "Started recording episode 25 to ./videos/catan_stochastic/3/episode_000025.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 24. Recorded 523 frames.\n",
      "Started recording episode 25 to ./videos/catan_stochastic/0/episode_000025.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 23. Recorded 665 frames.\n",
      "Started recording episode 24 to ./videos/catan_stochastic/2/episode_000024.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 21. Recorded 534 frames.\n",
      "Started recording episode 22 to ./videos/catan_stochastic/1/episode_000022.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 25. Recorded 496 frames.\n",
      "Started recording episode 26 to ./videos/catan_stochastic/3/episode_000026.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 25. Recorded 538 frames.\n",
      "Started recording episode 26 to ./videos/catan_stochastic/0/episode_000026.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 24. Recorded 530 frames.\n",
      "Started recording episode 25 to ./videos/catan_stochastic/2/episode_000025.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Stopped recording episode 22. Recorded 494 frames.\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n",
      "Started recording episode 23 to ./videos/catan_stochastic/1/episode_000023.mp4\n",
      "plotting score\n",
      "plotting policy_loss\n",
      "plotting value_loss\n",
      "plotting reward_loss\n",
      "plotting to_play_loss\n",
      "plotting cons_loss\n",
      "plotting q_loss\n",
      "plotting sigma_loss\n",
      "plotting vqvae_commitment_cost\n",
      "plotting loss\n",
      "plotting episode_length\n",
      "plotting num_codes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 310, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1638, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1579, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 635, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 892, in _run_single_simulation\n",
      "    node.expand(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_mcts.py\", line 216, in expand\n",
      "    (p / (allowed_policy_sum + 1e-10)).item(),\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 310, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1638, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1579, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 635, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 892, in _run_single_simulation\n",
      "    node.expand(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_mcts.py\", line 211, in expand\n",
      "    allowed_policy_sum = sum(allowed_policy.values())\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 310, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1658, in play_game\n",
      "    env.step(action)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py\", line 1056, in step\n",
      "    self._capture_frame()\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py\", line 1156, in _capture_frame\n",
      "    self.video_writer.write(frame)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 310, in worker_fn\n",
      "    score, num_steps = self.play_game(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1638, in play_game\n",
      "    prediction = self.predict(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1579, in predict\n",
      "    value, policy, target_policy, best_action = self.monte_carlo_tree_search(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 635, in monte_carlo_tree_search\n",
      "    self._run_single_simulation(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 869, in _run_single_simulation\n",
      "    ) = self.predict_recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py\", line 1530, in predict_recurrent_inference\n",
      "    model.recurrent_inference(\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_network.py\", line 1605, in recurrent_inference\n",
      "    # action_plane_flat = linear(concat)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_network.py\", line 517, in forward\n",
      "    to_play = self.to_play(flattened_to_play_vector).softmax(dim=-1)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped recording episode 23. Recorded 172 frames.\n",
      "Stopped recording episode 26. Recorded 235 frames.\n",
      "Stopped recording episode 26. Recorded 444 frames.\n",
      "Stopped recording episode 25. Recorded 184 frames.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m\n\u001b[1;32m    112\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 114\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/experiments/catan/../../muzero/muzero_agent_torch.py:347\u001b[0m, in \u001b[0;36mMuZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_steps:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[0;32m--> 347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43merror_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    348\u001b[0m             err, tb \u001b[38;5;241m=\u001b[39m error_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;66;03m# Stop all workers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:129\u001b[0m, in \u001b[0;36mQueue.empty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from modules.utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "from wrappers import record_video_wrapper\n",
    "\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=True,\n",
    ")\n",
    "env = record_video_wrapper(copy.deepcopy(env), \"./videos\", 1)\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"n_step\": 750,\n",
    "    \"residual_layers\": [],\n",
    "    \"dense_layer_widths\": [128] * 3,\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"chance_dense_layer_widths\": [32],\n",
    "    \"chance_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    # \"lr_ratio\": float(\"inf\"),\n",
    "    \"lr_ratio\": 0.1,\n",
    "    \"num_workers\": 4,\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"root_dirichlet_alpha\": 0.03,\n",
    "    \"learning_rate\": 0.001,  # 0.001 or sgd 0.1\n",
    "    \"optimizer\": Adam,  # SGD\n",
    "    \"temperature_updates\": [150],\n",
    "    \"to_play_loss_factor\": 1.0,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 200000,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,  # might want to be zero for \"average state\" planning, as initial is known, but one step forward in dynamics has some randomness\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    \"value_prefix\": False,\n",
    "    \"lstm_horizon_len\": 5,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"q_estimation_method\": \"v_mix\",\n",
    "    \"clipnorm\": 10,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.1,\n",
    "}\n",
    "game_config = CatanConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"catan_stochastic\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[\n",
    "        CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "        CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "    ],\n",
    ")\n",
    "agent.checkpoint_interval = 1\n",
    "agent.test_interval = 2500\n",
    "agent.test_trials = 10\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d597228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=True,\n",
    ")\n",
    "env = record_video_wrapper(copy.deepcopy(env), \"./videos\", 1)\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"n_step\": 750,\n",
    "    \"residual_layers\": [],\n",
    "    \"dense_layer_widths\": [128] * 3,\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"chance_dense_layer_widths\": [32],\n",
    "    \"chance_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    \"num_workers\": 4,\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"root_dirichlet_alpha\": 0.03,\n",
    "    \"learning_rate\": 0.001,  # 0.001 or sgd 0.1\n",
    "    \"optimizer\": Adam,  # SGD\n",
    "    \"temperature_updates\": [150],\n",
    "    \"to_play_loss_factor\": 1.0,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 200000,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,  # might want to be zero for \"average state\" planning, as initial is known, but one step forward in dynamics has some randomness\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    \"value_prefix\": True,\n",
    "    \"lstm_horizon_len\": 5,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"q_estimation_method\": \"v_mix\",\n",
    "    \"clipnorm\": 0,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.5,\n",
    "}\n",
    "game_config = CatanConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"catan_gumbel_stochastic_value_prefix\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[\n",
    "        CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "        CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "    ],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 2500\n",
    "agent.test_trials = 10\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_tests(agent.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_onehot as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    # \"residual_filters\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    # ),\n",
    "    # \"residual_stacks\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    # ),\n",
    "    \"residual_layers\": hp.choice(\"residual_layers\", [[]]),\n",
    "    \"actor_conv_layers\": hp.choice(\"actor_conv_layers\", [[]]),\n",
    "    \"critic_conv_layers\": hp.choice(\"critic_conv_layers\", [[]]),\n",
    "    \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "    \"to_play_conv_layers\": hp.choice(\"to_play_conv_layers\", [[]]),\n",
    "    \"output_layer_widths\": scope.int(\n",
    "        hp.quniform(\"output_layer_widths\", 16, 16 + 1e-8, 16)\n",
    "    ),\n",
    "    \"dense_layer_width\": scope.int(2 ** hp.quniform(\"dense_layer_width\", 6, 9, 1)),\n",
    "    \"dense_layers\": scope.int(hp.quniform(\"dense_layers\", 1, 1 + 1e-8, 1)),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": 2 ** hp.quniform(\"value_loss_factor\", 0.0, 0.0 + 1e-8, 1.0),\n",
    "    \"to_play_loss_factor\": 2 ** hp.quniform(\"to_play_loss_factor\", 0.0, 1.0, 1.0),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -5, 0, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        800 * 2 ** hp.quniform(\"num_simulations\", -5, -4 + 1e-8, 1)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 32, 512, 32))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"to_play_loss_function\": hp.choice(\n",
    "        \"to_play_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(500), np.log(1000), 100)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 11, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        2 ** hp.quniform(\"min_replay_buffer_size\", 5, 10, 1)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [750]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\",\n",
    "        [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))],\n",
    "        # \"clipnorm\",\n",
    "        # [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.quniform(\"per_alpha\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta\": hp.quniform(\"per_beta\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta_final\": hp.quniform(\"per_beta_final\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 4, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\"), 0.1]),\n",
    "    \"transfer_interval\": scope.int(10 ** hp.quniform(\"transfer_interval\", 0, 1, 1)),\n",
    "    \"vps_to_win\": hp.quniform(\"vps_to_win\", 3, 3 + 1e-8, 1),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    params[\"dense_layer_widths\"] = [params[\"dense_layer_width\"]] * params[\n",
    "        \"dense_layers\"\n",
    "    ]\n",
    "    del params[\"dense_layer_width\"]\n",
    "    del params[\"dense_layers\"]\n",
    "    if params[\"output_layer_widths\"] != 0:\n",
    "        params[\"actor_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"critic_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"reward_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"to_play_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "    else:\n",
    "        params[\"actor_dense_layer_widths\"] = []\n",
    "        params[\"critic_dense_layer_widths\"] = []\n",
    "        params[\"reward_dense_layer_widths\"] = []\n",
    "        params[\"to_play_dense_layer_widths\"] = []\n",
    "    del params[\"output_layer_widths\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.catan_config import CatanConfig\n",
    "import torch\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = CatanConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "            episode_length += 1\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from muzero.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"catan_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)\n",
    "\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        make_env=CatanConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=CatanConfig,\n",
    "        games_per_pair=10,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=1,\n",
    "        test_interval=1000,\n",
    "        test_trials=100,\n",
    "        test_agents=[\n",
    "            CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "            CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        ],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65445e1d",
   "metadata": {},
   "source": [
    "RAINBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"/content/rl-research\")\n",
    "sys.path.append(\"../..\")\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from wrappers import CatanatronWrapper\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs.catan_config import SinglePlayerCatanConfig\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [512],\n",
    "    \"value_hidden_layers_widths\": [512],  #\n",
    "    \"advatage_hidden_layers_widths\": [512],  #\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 30000,\n",
    "    \"per_epsilon\": 1e-6,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"minibatch_size\": 64,\n",
    "    \"replay_buffer_size\": 50000,\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"n_step\": 9,\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 0.99,  # or 0.999 or even 0.9999 not 0.99 < this makes the start of the game possibly 0.05 after bootstrapping\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 16,\n",
    "}\n",
    "game_config = SinglePlayerCatanConfig()\n",
    "game_config.min_score = -1000\n",
    "game_config.max_score = 1010\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import catanatron.gym\n",
    "import gymnasium as gym\n",
    "\n",
    "from catanatron.gym.envs.catanatron_env import simple_reward\n",
    "from catanatron.gym.utils import get_tournament_total_return\n",
    "from catanatron.gym.utils import get_actual_victory_points\n",
    "\n",
    "\n",
    "def tournament_reward_function(game, p0_color):\n",
    "    winning_color = game.winning_color()\n",
    "    if p0_color == winning_color:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "    elif winning_color is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "\n",
    "\n",
    "env = CatanatronWrapper(\n",
    "    gym.make(\n",
    "        \"catanatron/Catanatron-v0\",\n",
    "        config={\n",
    "            \"enemies\": [RandomPlayer(Color.RED)],\n",
    "            \"invalid_action_reward\": -10,\n",
    "            \"map_type\": \"BASE\",\n",
    "            \"vps_to_win\": 3,\n",
    "            \"representation\": \"vector\",\n",
    "            \"reward_function\": tournament_reward_function,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "agent = RainbowAgent(env, config, \"rainbow-catan-3vps\", device)\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from dqn.NFSP.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import CatanConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from wrappers import ActionMaskInInfoWrapper, FrameStackWrapper\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 500000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 2048,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"minibatch_size\": 1024,\n",
    "    \"replay_buffer_size\": 1e5,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [1024],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.12,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 5000,\n",
    "    \"sl_minibatch_size\": 1024,\n",
    "    \"sl_replay_buffer_size\": 100000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [1024, 1024, 1024],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 1.0,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 9,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=CatanConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = False\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    ")\n",
    "\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"nfsp-catan\", device=\"cpu\")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import catanatron.gym\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"catanatron/Catanatron-v0\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    # your agent here (this takes random actions)\n",
    "    action = random.choice(info[\"valid_actions\"])\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "# Instantiate two random players\n",
    "player1 = RandomPlayer(Color.RED)\n",
    "player2 = RandomPlayer(Color.BLUE)\n",
    "\n",
    "# Create a 2-player game (you can fill remaining slots with random agents if needed)\n",
    "players = [player1, player2]\n",
    "game = Game(players)\n",
    "\n",
    "winner = game.play()\n",
    "print(f\"Winner: {winner}\")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "    player1 = player1(Color.RED)\n",
    "    player2 = player2(Color.BLUE)\n",
    "    game = Game([player1, player2])\n",
    "    winner = game.play()\n",
    "\n",
    "    if winner == Color.RED:\n",
    "        return 1\n",
    "    elif winner == Color.BLUE:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "players = [\n",
    "    RandomPlayer,\n",
    "    MCTSPlayer,\n",
    "    AlphaBetaPlayer,\n",
    "    # GreedyPlayoutsPlayer,\n",
    "    VictoryPointPlayer,\n",
    "    WeightedRandomPlayer,\n",
    "    # ValueFunctionPlayer,\n",
    "]\n",
    "games_per_pair = 10\n",
    "\n",
    "player_names = [p.__name__ for p in players]\n",
    "table = StandingsTable(player_names, start_elo=1000)\n",
    "\n",
    "\n",
    "def play_1v1_tournament(players, games_per_pair, play_game):\n",
    "    tournament_results = []\n",
    "    for player1 in players:\n",
    "        results = play_matches(player1, players, games_per_pair, play_game)\n",
    "        tournament_results.extend(results)\n",
    "    tournament_results = pd.DataFrame(\n",
    "        tournament_results, columns=[\"player1\", \"player2\", \"result\"]\n",
    "    )\n",
    "    return tournament_results\n",
    "\n",
    "\n",
    "def play_matches(player1, players, games_per_pair, play_game):\n",
    "    results = []\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {player1.__name__} vs {opponent.__name__} game {_+1}\")\n",
    "                result = play_game(player1, opponent)\n",
    "                results.append((player1.__name__, opponent.__name__, result))\n",
    "\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {opponent.__name__} vs {player1.__name__} game {_+1}\")\n",
    "                result = play_game(opponent, player1)\n",
    "                results.append(\n",
    "                    (\n",
    "                        player_names[players.index(opponent)],\n",
    "                        player_names[players.index(player1)],\n",
    "                        result,\n",
    "                    )\n",
    "                )\n",
    "    table.add_results_from_array(results)\n",
    "    print(table.bayes_elo())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(table.bayes_elo())\n",
    "print(table.get_win_table())\n",
    "print(table.get_draw_table())\n",
    "file = \"catan_1v1_tournament_results.pkl\"\n",
    "pickle.dump(table, open(file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "table.add_player(GreedyPlayoutsPlayer.__name__)\n",
    "players.append(GreedyPlayoutsPlayer) if GreedyPlayoutsPlayer not in players else None\n",
    "\n",
    "play_matches(GreedyPlayoutsPlayer, players, games_per_pair * 2, play_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play_1v1_tournament(players, games_per_pair, play_game)\n",
    "\n",
    "\n",
    "# table.add_results_from_dataframe(results)  # Adding multiple results\n",
    "print(table.bayes_elo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a petting zoo environment to see if it has all the functions and attributes needed\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "# env = catan_env(\n",
    "#     render_mode=\"rgb_array\",\n",
    "#     num_players=2,\n",
    "#     map_type=\"BASE\",\n",
    "#     vps_to_win=10,\n",
    "#     representation=\"vector\",\n",
    "#     invalid_action_reward=-1,\n",
    "#     auto_play_single_action=False,\n",
    "# )\n",
    "\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=False,\n",
    ")\n",
    "env = record_video_wrapper(env, \"./videos\", 1)\n",
    "env.reset()\n",
    "\n",
    "ab_player = CatanPlayerWrapper(AlphaBetaPlayer, Color.RED)\n",
    "\n",
    "for i in range(100):\n",
    "    state, reward, termination, truncation, info = env.last()\n",
    "    prediction = ab_player.predict(state, info, env)\n",
    "    action = ab_player.select_actions(prediction, info).item()\n",
    "    print(action)\n",
    "    print(info)\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95931df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    render_mode=\"rgb_array\",\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-1,\n",
    ")\n",
    "\n",
    "# Set metadata fps for 'human' if desired (not required for rgb_array)\n",
    "env.metadata[\"render_fps\"] = 10\n",
    "\n",
    "# Reset the environment (seed optional)\n",
    "env.reset(seed=0)\n",
    "print(\"Starting agent selection:\", env.agent_selection)\n",
    "\n",
    "# # Render one frame as an RGB array, then plot it\n",
    "# frame = env.render()  # will return a HxWx3 numpy array if renderer implemented\n",
    "# if frame is None:\n",
    "#     print(\n",
    "#         \"Renderer returned None. Make sure env.render(mode='rgb_array') is implemented.\"\n",
    "#     )\n",
    "# else:\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(frame)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# Setup matplotlib figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "print(\"Starting Catan Game Visualization...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run game for a few steps\n",
    "step_count = 0\n",
    "max_steps = 20\n",
    "\n",
    "while step_count < max_steps:\n",
    "    # Render current state\n",
    "    rgb_array = env.render()\n",
    "\n",
    "    if rgb_array is not None:\n",
    "        # Display in notebook\n",
    "        # clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.imshow(rgb_array)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Catan Game - Step {step_count}\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "\n",
    "        # Print game info\n",
    "        current_agent = env.agent_selection\n",
    "        print(f\"\\nStep {step_count}\")\n",
    "        print(f\"Current Player: {current_agent}\")\n",
    "\n",
    "        if hasattr(env, \"game\") and hasattr(env.game, \"state\"):\n",
    "            print(f\"Turn Number: {env.game.state.num_turns}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Check if game is done\n",
    "    if env.terminations[env.agent_selection] or env.truncations[env.agent_selection]:\n",
    "        print(\"\\nGame Over!\")\n",
    "        break\n",
    "\n",
    "    # Get valid actions\n",
    "    obs = env.observe(env.agent_selection)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "    valid_actions = np.where(action_mask == 1)[0]\n",
    "\n",
    "    if len(valid_actions) > 0:\n",
    "        # Take a random valid action\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(action)\n",
    "    else:\n",
    "        print(\"No valid actions available!\")\n",
    "        break\n",
    "\n",
    "    step_count += 1\n",
    "    time.sleep(0.5)  # Pause to see each frame\n",
    "\n",
    "print(\"\\nTest completed!\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a11a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from custom_gym_envs.envs.catan import ActionType\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import ACTIONS_ARRAY\n",
    "\n",
    "n_steps = 25\n",
    "delay = 0.25  # seconds between frames\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # If the current agent is terminated/truncated, we still call env_instance.step per PettingZoo convention:\n",
    "    current_agent = env.agent_selection\n",
    "\n",
    "    # get observation for current agent\n",
    "    obs = env.observe(current_agent)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "\n",
    "    legal_indices = np.nonzero(action_mask)[0]\n",
    "    if len(legal_indices) == 0:\n",
    "        # no legal moves available (should rarely happen). Choose end-turn fallback if available:\n",
    "        try:\n",
    "            end_turn_index = ACTIONS_ARRAY.index((ActionType.END_TURN, None))\n",
    "            action_choice = end_turn_index\n",
    "        except Exception:\n",
    "            # fallback to 0\n",
    "            action_choice = 0\n",
    "    else:\n",
    "        action_choice = int(random.choice(legal_indices))\n",
    "\n",
    "    # Step the env with the chosen action\n",
    "    env.step(action_choice)\n",
    "\n",
    "    # Render and display frame\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(frame)\n",
    "        plt.title(f\"Step {step+1} - Agent: {current_agent} - Action: {action_choice}\")\n",
    "        plt.axis(\"off\")\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No frame returned (render returned None)\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# After rollout, print summary\n",
    "print(\"Done. Final agent selection:\", env.agent_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
