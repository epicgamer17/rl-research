{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5607c5be",
   "metadata": {},
   "source": [
    "*** TO DO FOR CATAN: ***\n",
    "RAINBOW: \n",
    "    1. vs Random\n",
    "    2. vs Weighted Random\n",
    "    3. vs MTCS\n",
    "    4. vs Victory Point\n",
    "    5. vs AlphaBeta\n",
    "Masked PPO the same \n",
    "NFSP \n",
    "MuZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa0589",
   "metadata": {},
   "source": [
    "MUZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from modules.muzero_world_model import MuzeroWorldModel\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "\n",
    "from agents.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "from wrappers import record_video_wrapper\n",
    "\n",
    "from game_configs.game_2048_config import Game2048Config\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "import custom_gym_envs\n",
    "\n",
    "env = Game2048Config().make_env()\n",
    "params = {\n",
    "    \"num_simulations\": 100,\n",
    "    \"per_alpha\": 0.6,\n",
    "    \"per_beta\": 0.4,\n",
    "    \"per_beta_final\": 0.4,\n",
    "    \"n_step\": 10,\n",
    "    \"residual_layers\": [(64, 3, 1)],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"chance_dense_layer_widths\": [32],\n",
    "    \"chance_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"to_play_dense_layer_widths\": [],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    # \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": 300,\n",
    "    \"minibatch_size\": 512,  # 1024\n",
    "    \"replay_buffer_size\": 1e6,\n",
    "    # \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"learning_rate\": 0.003,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"num_workers\": 4,  # 1000\n",
    "    \"min_replay_buffer_size\": 1024,\n",
    "    \"root_dirichlet_alpha\": 0.3,\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"optimizer\": Adam,\n",
    "    # \"temperature_updates\": [1e4, 2e4, 3e4],  # [1e5, 2e5, 3e5],\n",
    "    # \"temperatures\": [1.0, 0.5, 0.1, 0.0],\n",
    "    \"temperature_updates\": [1e5],  # [1e5, 2e5, 3e5],\n",
    "    \"temperatures\": [0.25, 0.0],\n",
    "    \"temperature_with_training_steps\": True,\n",
    "    \"to_play_loss_factor\": 0.0,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"gumbel\": False,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"training_steps\": 20e6,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,\n",
    "    \"value_loss_factor\": 0.25,\n",
    "    \"injection_frac\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 2.0,  # might want to be zero for \"average state\" planning, as initial is known, but one step forward in dynamics has some randomness\n",
    "    \"projector_output_dim\": 1024,\n",
    "    \"projector_hidden_dim\": 1024,\n",
    "    \"predictor_output_dim\": 1024,\n",
    "    \"predictor_hidden_dim\": 512,\n",
    "    \"value_prefix\": True,\n",
    "    \"lstm_horizon_len\": 5,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"q_estimation_method\": \"v_mix\",\n",
    "    \"clipnorm\": 10,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 1.0,\n",
    "    \"discount_factor\": 0.999,\n",
    "    \"use_true_chance_codes\": True,\n",
    "    \"world_model_cls\": MuzeroWorldModel,\n",
    "}\n",
    "game_config = Game2048Config()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"2048_muzero_everything_2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 25\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = Game2048Config().make_env()\n",
    "print(isinstance(env.observation_space, gym.spaces.Box))\n",
    "# isinstance(obs_space, gym.spaces.Box)\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "\n",
    "from agents.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "from wrappers import record_video_wrapper\n",
    "\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    ")\n",
    "env = record_video_wrapper(copy.deepcopy(env), \"./videos\", 1)\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"n_step\": 2000,\n",
    "    \"residual_layers\": [],\n",
    "    \"dense_layer_widths\": [64] * 3,\n",
    "    \"reward_dense_layer_widths\": [16],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"chance_dense_layer_widths\": [16],\n",
    "    \"chance_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [16],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [16],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [16],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 256,\n",
    "    \"replay_buffer_size\": 200000,\n",
    "    # \"lr_ratio\": float(\"inf\"),\n",
    "    \"lr_ratio\": 0.1,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"num_workers\": 4,\n",
    "    \"min_replay_buffer_size\": 3000,\n",
    "    \"root_dirichlet_alpha\": 0.03,\n",
    "    \"optimizer\": Adam,  # SGD\n",
    "    \"temperature_updates\": [4],\n",
    "    \"to_play_loss_factor\": 2.0,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 8,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 200000,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,  # might want to be zero for \"average state\" planning, as initial is known, but one step forward in dynamics has some randomness\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    \"value_prefix\": True,\n",
    "    \"lstm_horizon_len\": 5,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"q_estimation_method\": \"v_mix\",\n",
    "    \"clipnorm\": 10,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.01,\n",
    "}\n",
    "game_config = CatanConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"catan_tournament_map_4\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[\n",
    "        CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "        CatanPlayerWrapper(WeightedRandomPlayer, Color.BLUE),\n",
    "        # CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "    ],\n",
    ")\n",
    "agent.checkpoint_interval = 1\n",
    "agent.test_interval = 100\n",
    "agent.test_trials = 15\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "t = torch.tensor(\n",
    "    [[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]\n",
    ").float()\n",
    "t1 = (\n",
    "    torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]]])\n",
    "    - 1\n",
    ").float()\n",
    "\n",
    "print(mse_loss(t, t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d597228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "from utils import KLDivergenceLoss\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "\n",
    "from agents.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from muzero.action_functions import action_as_onehot, action_as_plane\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=True,\n",
    ")\n",
    "env = record_video_wrapper(copy.deepcopy(env), \"./videos\", 1)\n",
    "params = {\n",
    "    \"num_simulations\": 50,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"n_step\": 750,\n",
    "    \"residual_layers\": [],\n",
    "    \"dense_layer_widths\": [128] * 3,\n",
    "    \"reward_dense_layer_widths\": [32],\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"chance_dense_layer_widths\": [32],\n",
    "    \"chance_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [32],\n",
    "    \"actor_conv_layers\": [],\n",
    "    \"critic_dense_layer_widths\": [32],\n",
    "    \"critic_conv_layers\": [],\n",
    "    \"to_play_dense_layer_widths\": [32],\n",
    "    \"to_play_conv_layers\": [],\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"support_range\": None,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 100000,\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    \"num_workers\": 4,\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"root_dirichlet_alpha\": 0.03,\n",
    "    \"learning_rate\": 0.001,  # 0.001 or sgd 0.1\n",
    "    \"optimizer\": Adam,  # SGD\n",
    "    \"temperature_updates\": [150],\n",
    "    \"to_play_loss_factor\": 1.0,\n",
    "    \"transfer_interval\": 1,\n",
    "    \"gumbel\": True,\n",
    "    \"gumbel_m\": 16,\n",
    "    \"policy_loss_function\": KLDivergenceLoss(),\n",
    "    \"training_steps\": 200000,\n",
    "    \"reanalyze_ratio\": 0.0,\n",
    "    \"reanalyze_noise\": True,  # for gumbel\n",
    "    \"value_loss_factor\": 1.0,  # for reanalyze\n",
    "    \"injection_frac\": 0.25,\n",
    "    \"reanalyze_method\": \"mcts\",\n",
    "    \"consistency_loss_factor\": 0.0,  # might want to be zero for \"average state\" planning, as initial is known, but one step forward in dynamics has some randomness\n",
    "    \"projector_output_dim\": 128,\n",
    "    \"projector_hidden_dim\": 128,\n",
    "    \"predictor_output_dim\": 128,\n",
    "    \"predictor_hidden_dim\": 64,\n",
    "    \"value_prefix\": True,\n",
    "    \"lstm_horizon_len\": 5,\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"q_estimation_method\": \"v_mix\",\n",
    "    \"clipnorm\": 0,\n",
    "    \"stochastic\": True,\n",
    "    \"vqvae_commitment_cost_factor\": 0.5,\n",
    "}\n",
    "game_config = CatanConfig()\n",
    "config = MuZeroConfig(config_dict=params, game_config=game_config)\n",
    "\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    name=\"catan_gumbel_stochastic_value_prefix\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[\n",
    "        CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "        CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "    ],\n",
    ")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 2500\n",
    "agent.test_trials = 10\n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_tests(agent.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SMALLEST SEARCH SPACE, IMPROVED\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from hyperparameter_optimization.hyperopt import save_search_space\n",
    "\n",
    "\n",
    "import dill as pickle\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from utils import CategoricalCrossentropyLoss, MSELoss, generate_layer_widths\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from muzero.action_functions import action_as_onehot as action_function\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# size = 5 * 1 * 1 * 4.0 * 3 * 2.0 * 5 * 1 * 1 = 600\n",
    "\n",
    "search_space = {\n",
    "    \"kernel_initializer\": hp.choice(\n",
    "        \"kernel_initializer\",\n",
    "        [\n",
    "            \"he_uniform\",\n",
    "            \"he_normal\",\n",
    "            \"glorot_uniform\",\n",
    "            \"glorot_normal\",\n",
    "            \"orthogonal\",\n",
    "        ],\n",
    "    ),\n",
    "    \"optimizer\": hp.choice(\n",
    "        \"optimizer\",\n",
    "        [\n",
    "            {\n",
    "                \"optimizer\": \"adam\",\n",
    "                # \"adam_epsilon\": 10 ** (-hp.quniform(\"adam_epsilon\", 8, 8 + 1e-8, 2)),\n",
    "                \"adam_epsilon\": hp.choice(\"adam_epsilon\", [1e-8]),\n",
    "                \"adam_learning_rate\": 10\n",
    "                ** (-hp.quniform(\"adam_learning_rate\", 3, 3 + 1e-8, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"optimizer\": \"sgd\",\n",
    "            #     \"momentum\": hp.choice(\"momentum\", [0.0, 0.9]),\n",
    "            #     \"sgd_learning_rate\": 10 ** (-hp.quniform(\"sgd_learning_rate\", 1, 3, 1)),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"conv_layers\": hp.choice(\"conv_layers\", [[]]),\n",
    "    \"known_bounds\": hp.choice(\"known_bounds\", [[-1, 1]]),\n",
    "    # \"residual_filters\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_filters\", np.log(24), np.log(24) + 1e-8, 8)\n",
    "    # ),\n",
    "    # \"residual_stacks\": scope.int(\n",
    "    #     hp.qloguniform(\"residual_stacks\", np.log(1), np.log(4), 1)\n",
    "    # ),\n",
    "    \"residual_layers\": hp.choice(\"residual_layers\", [[]]),\n",
    "    \"actor_conv_layers\": hp.choice(\"actor_conv_layers\", [[]]),\n",
    "    \"critic_conv_layers\": hp.choice(\"critic_conv_layers\", [[]]),\n",
    "    \"reward_conv_layers\": hp.choice(\"reward_conv_layers\", [[]]),\n",
    "    \"to_play_conv_layers\": hp.choice(\"to_play_conv_layers\", [[]]),\n",
    "    \"output_layer_widths\": scope.int(\n",
    "        hp.quniform(\"output_layer_widths\", 16, 16 + 1e-8, 16)\n",
    "    ),\n",
    "    \"dense_layer_width\": scope.int(2 ** hp.quniform(\"dense_layer_width\", 6, 9, 1)),\n",
    "    \"dense_layers\": scope.int(hp.quniform(\"dense_layers\", 1, 1 + 1e-8, 1)),\n",
    "    \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.0]),\n",
    "    \"value_loss_factor\": 2 ** hp.quniform(\"value_loss_factor\", 0.0, 0.0 + 1e-8, 1.0),\n",
    "    \"to_play_loss_factor\": 2 ** hp.quniform(\"to_play_loss_factor\", 0.0, 1.0, 1.0),\n",
    "    \"root_dirichlet_alpha\": 2 ** (hp.quniform(\"root_dirichlet_alpha\", -5, 0, 1.0)),\n",
    "    \"root_exploration_fraction\": hp.choice(\"root_exploration_fraction\", [0.25]),\n",
    "    \"num_simulations\": scope.int(\n",
    "        800 * 2 ** hp.quniform(\"num_simulations\", -5, -4 + 1e-8, 1)\n",
    "    ),\n",
    "    \"temperature_updates\": [scope.int(hp.quniform(\"temperature_updates\", 32, 512, 32))],\n",
    "    \"temperatures\": hp.choice(\"temperatures\", [[1.0, 0.1]]),\n",
    "    \"temperature_with_training_steps\": hp.choice(\n",
    "        \"temperature_with_training_steps\", [False]\n",
    "    ),\n",
    "    \"clip_low_prob\": hp.choice(\"clip_low_prob\", [0.0]),\n",
    "    \"pb_c_base\": hp.choice(\"pb_c_base\", [19652]),\n",
    "    \"pb_c_init\": hp.choice(\"pb_c_init\", [1.25]),\n",
    "    \"value_loss_function\": hp.choice(\"value_loss_function\", [MSELoss()]),\n",
    "    \"reward_loss_function\": hp.choice(\"reward_loss_function\", [MSELoss()]),\n",
    "    \"policy_loss_function\": hp.choice(\n",
    "        \"policy_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"to_play_loss_function\": hp.choice(\n",
    "        \"to_play_loss_function\", [CategoricalCrossentropyLoss()]\n",
    "    ),\n",
    "    \"training_steps\": scope.int(\n",
    "        hp.qloguniform(\"training_steps\", np.log(500), np.log(1000), 100)\n",
    "    ),\n",
    "    \"minibatch_size\": scope.int(2 ** (hp.quniform(\"minibatch_size\", 3, 11, 1))),\n",
    "    \"min_replay_buffer_size\": scope.int(\n",
    "        2 ** hp.quniform(\"min_replay_buffer_size\", 5, 10, 1)\n",
    "    ),\n",
    "    \"replay_buffer_size\": scope.int(\n",
    "        10 ** (hp.quniform(\"replay_buffer_size\", 5, 5 + 1e-8, 1))\n",
    "    ),\n",
    "    \"unroll_steps\": hp.choice(\"unroll_steps\", [5]),\n",
    "    \"n_step\": hp.choice(\"n_step\", [750]),\n",
    "    \"clipnorm\": hp.choice(\n",
    "        \"clipnorm\",\n",
    "        [0.0, scope.int(10 ** (hp.quniform(\"clip_val\", 0, 2, 1)))],\n",
    "        # \"clipnorm\",\n",
    "        # [0.0],\n",
    "    ),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4]),\n",
    "    \"per_alpha\": hp.quniform(\"per_alpha\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta\": hp.quniform(\"per_beta\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_beta_final\": hp.quniform(\"per_beta_final\", 0.0, 0.0 + 1e-8, 0.5),\n",
    "    \"per_epsilon\": hp.choice(\"per_epsilon\", [1e-4]),\n",
    "    \"action_function\": hp.choice(\"action_function\", [action_function]),\n",
    "    \"multi_process\": hp.choice(\n",
    "        \"multi_process\",\n",
    "        [\n",
    "            {\n",
    "                \"multi_process\": True,\n",
    "                \"num_workers\": scope.int(hp.quniform(\"num_workers\", 2, 4, 1)),\n",
    "            },\n",
    "            # {\n",
    "            #     \"multi_process\": False,\n",
    "            #     \"games_per_generation\": scope.int(\n",
    "            #         hp.qloguniform(\"games_per_generation\", np.log(8), np.log(32), 8)\n",
    "            #     ),\n",
    "            # },\n",
    "        ],\n",
    "    ),\n",
    "    \"lr_ratio\": hp.choice(\"lr_ratio\", [float(\"inf\"), 0.1]),\n",
    "    \"transfer_interval\": scope.int(10 ** hp.quniform(\"transfer_interval\", 0, 1, 1)),\n",
    "    \"vps_to_win\": hp.quniform(\"vps_to_win\", 3, 3 + 1e-8, 1),\n",
    "}\n",
    "\n",
    "initial_best_config = []\n",
    "\n",
    "search_space, initial_best_config = save_search_space(search_space, initial_best_config)\n",
    "\n",
    "\n",
    "def prep_params(params):\n",
    "    params[\"dense_layer_widths\"] = [params[\"dense_layer_width\"]] * params[\n",
    "        \"dense_layers\"\n",
    "    ]\n",
    "    del params[\"dense_layer_width\"]\n",
    "    del params[\"dense_layers\"]\n",
    "    if params[\"output_layer_widths\"] != 0:\n",
    "        params[\"actor_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"critic_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"reward_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "        params[\"to_play_dense_layer_widths\"] = [params[\"output_layer_widths\"]]\n",
    "    else:\n",
    "        params[\"actor_dense_layer_widths\"] = []\n",
    "        params[\"critic_dense_layer_widths\"] = []\n",
    "        params[\"reward_dense_layer_widths\"] = []\n",
    "        params[\"to_play_dense_layer_widths\"] = []\n",
    "    del params[\"output_layer_widths\"]\n",
    "\n",
    "    if params[\"multi_process\"][\"multi_process\"] == True:\n",
    "        params[\"num_workers\"] = params[\"multi_process\"][\"num_workers\"]\n",
    "        params[\"multi_process\"] = True\n",
    "    else:\n",
    "        params[\"games_per_generation\"] = params[\"multi_process\"][\"games_per_generation\"]\n",
    "        params[\"multi_process\"] = False\n",
    "\n",
    "    if params[\"optimizer\"][\"optimizer\"] == \"adam\":\n",
    "        params[\"adam_epsilon\"] = params[\"optimizer\"][\"adam_epsilon\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"adam_learning_rate\"]\n",
    "        params[\"optimizer\"] = Adam\n",
    "    elif params[\"optimizer\"][\"optimizer\"] == \"sgd\":\n",
    "        params[\"momentum\"] = params[\"optimizer\"][\"momentum\"]\n",
    "        params[\"learning_rate\"] = params[\"optimizer\"][\"sgd_learning_rate\"]\n",
    "        params[\"optimizer\"] = SGD\n",
    "\n",
    "    if isinstance(params[\"clipnorm\"], dict):\n",
    "        params[\"clipnorm\"] = params[\"clipnorm\"][\"clipval\"]\n",
    "    params[\"support_range\"] = None\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_configs.catan_config import CatanConfig\n",
    "import torch\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = CatanConfig().make_env()\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env, temperature=0.1)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            # state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "            episode_length += 1\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from hyperparameter_optimization.hyperopt import (\n",
    "    marl_objective,\n",
    "    set_marl_config,\n",
    "    MarlHyperoptConfig,\n",
    ")\n",
    "from hyperopt import atpe, tpe, fmin, space_eval\n",
    "from hyperopt.exceptions import AllTrialsFailed\n",
    "\n",
    "from agents.muzero_agent_torch import MuZeroAgent\n",
    "from agent_configs import MuZeroConfig\n",
    "from utils.utils import record_video_wrapper\n",
    "\n",
    "search_space_path, initial_best_config_path = (\n",
    "    \"search_space.pkl\",\n",
    "    \"best_config.pkl\",\n",
    ")\n",
    "# search_space = pickle.load(open(search_space_path, \"rb\"))\n",
    "# initial_best_config = pickle.load(open(initial_best_config_path, \"rb\"))\n",
    "file_name = \"catan_muzero\"\n",
    "max_trials = 64\n",
    "trials_step = 24  # how many additional trials to do after loading the last ones\n",
    "\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import dill as pickle\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "games_per_pair = 10\n",
    "try:\n",
    "    players = pickle.load(open(\"./tictactoe_players.pkl\", \"rb\"))\n",
    "    table = pickle.load(open(\"./tictactoe_table.pkl\", \"rb\"))\n",
    "    print(table.bayes_elo())\n",
    "    print(table.get_win_table())\n",
    "    print(table.get_draw_table())\n",
    "except:\n",
    "    players = []\n",
    "    table = StandingsTable([], start_elo=1000)\n",
    "\n",
    "\n",
    "set_marl_config(\n",
    "    MarlHyperoptConfig(\n",
    "        file_name=file_name,\n",
    "        eval_method=\"test_agents_elo\",\n",
    "        best_agent=CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        make_env=CatanConfig().make_env,\n",
    "        prep_params=prep_params,\n",
    "        agent_class=MuZeroAgent,\n",
    "        agent_config=MuZeroConfig,\n",
    "        game_config=CatanConfig,\n",
    "        games_per_pair=10,\n",
    "        num_opps=1,  # not used\n",
    "        table=table,  # not used\n",
    "        play_game=play_game,\n",
    "        checkpoint_interval=1,\n",
    "        test_interval=1000,\n",
    "        test_trials=100,\n",
    "        test_agents=[\n",
    "            CatanPlayerWrapper(RandomPlayer, Color.BLUE),\n",
    "            CatanPlayerWrapper(AlphaBetaPlayer, Color.BLUE),\n",
    "        ],\n",
    "        test_agent_weights=[1.0, 2.0],\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(f\"./{file_name}_trials.p\", \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\n",
    "        \"Rerunning from {} trials to {} (+{}) trials\".format(\n",
    "            len(trials.trials), max_trials, trials_step\n",
    "        )\n",
    "    )\n",
    "except:  # create a new trials object and start searching\n",
    "    print(\"No saved Trials! Starting from scratch.\")\n",
    "    trials = None\n",
    "\n",
    "best = fmin(\n",
    "    fn=marl_objective,  # Objective Function to optimize\n",
    "    space=search_space,  # Hyperparameter's Search Space\n",
    "    algo=atpe.suggest,  # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials,  # Number of optimization attempts\n",
    "    trials=trials,  # Record the results\n",
    "    # early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=f\"./{file_name}_trials.p\",\n",
    "    points_to_evaluate=initial_best_config,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65445e1d",
   "metadata": {},
   "source": [
    "RAINBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"/content/rl-research\")\n",
    "sys.path.append(\"../..\")\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from wrappers import CatanatronWrapper\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "\n",
    "from agents.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs.catan_config import SinglePlayerCatanConfig\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [512],\n",
    "    \"value_hidden_layers_widths\": [512],  #\n",
    "    \"advatage_hidden_layers_widths\": [512],  #\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 30000,\n",
    "    \"per_epsilon\": 1e-6,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.5,\n",
    "    \"minibatch_size\": 64,\n",
    "    \"replay_buffer_size\": 50000,\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"n_step\": 9,\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 0.99,  # or 0.999 or even 0.9999 not 0.99 < this makes the start of the game possibly 0.05 after bootstrapping\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 16,\n",
    "}\n",
    "game_config = SinglePlayerCatanConfig()\n",
    "game_config.min_score = -1000\n",
    "game_config.max_score = 1010\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import catanatron.gym\n",
    "import gymnasium as gym\n",
    "\n",
    "from catanatron.gym.envs.catanatron_env import simple_reward\n",
    "from catanatron.gym.utils import get_tournament_total_return\n",
    "from catanatron.gym.utils import get_actual_victory_points\n",
    "\n",
    "\n",
    "def tournament_reward_function(game, p0_color):\n",
    "    winning_color = game.winning_color()\n",
    "    if p0_color == winning_color:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "    elif winning_color is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return get_tournament_total_return(game, p0_color)\n",
    "\n",
    "\n",
    "env = CatanatronWrapper(\n",
    "    gym.make(\n",
    "        \"catanatron/Catanatron-v0\",\n",
    "        config={\n",
    "            \"enemies\": [RandomPlayer(Color.RED)],\n",
    "            \"invalid_action_reward\": -10,\n",
    "            \"map_type\": \"BASE\",\n",
    "            \"vps_to_win\": 3,\n",
    "            \"representation\": \"vector\",\n",
    "            \"reward_function\": tournament_reward_function,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "agent = RainbowAgent(env, config, \"rainbow-catan-3vps\", device)\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import CatanConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from wrappers import ActionMaskInInfoWrapper, FrameStackWrapper\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 500000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 2048,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 5000,\n",
    "    \"minibatch_size\": 1024,\n",
    "    \"replay_buffer_size\": 1e5,\n",
    "    \"transfer_interval\": 100,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [1024],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.12,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 5000,\n",
    "    \"sl_minibatch_size\": 1024,\n",
    "    \"sl_replay_buffer_size\": 100000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [1024, 1024, 1024],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 1.0,\n",
    "    \"per_beta_final\": 1.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 9,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": True,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=CatanConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = False\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    ")\n",
    "\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"nfsp-catan\", device=\"cpu\")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import catanatron.gym\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"catanatron/Catanatron-v0\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    # your agent here (this takes random actions)\n",
    "    action = random.choice(info[\"valid_actions\"])\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "# Instantiate two random players\n",
    "player1 = RandomPlayer(Color.RED)\n",
    "player2 = RandomPlayer(Color.BLUE)\n",
    "\n",
    "# Create a 2-player game (you can fill remaining slots with random agents if needed)\n",
    "players = [player1, player2]\n",
    "game = Game(players)\n",
    "\n",
    "winner = game.play()\n",
    "print(f\"Winner: {winner}\")\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "    player1 = player1(Color.RED)\n",
    "    player2 = player2(Color.BLUE)\n",
    "    game = Game([player1, player2])\n",
    "    winner = game.play()\n",
    "\n",
    "    if winner == Color.RED:\n",
    "        return 1\n",
    "    elif winner == Color.BLUE:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "players = [\n",
    "    RandomPlayer,\n",
    "    MCTSPlayer,\n",
    "    AlphaBetaPlayer,\n",
    "    # GreedyPlayoutsPlayer,\n",
    "    VictoryPointPlayer,\n",
    "    WeightedRandomPlayer,\n",
    "    # ValueFunctionPlayer,\n",
    "]\n",
    "games_per_pair = 10\n",
    "\n",
    "player_names = [p.__name__ for p in players]\n",
    "table = StandingsTable(player_names, start_elo=1000)\n",
    "\n",
    "\n",
    "def play_1v1_tournament(players, games_per_pair, play_game):\n",
    "    tournament_results = []\n",
    "    for player1 in players:\n",
    "        results = play_matches(player1, players, games_per_pair, play_game)\n",
    "        tournament_results.extend(results)\n",
    "    tournament_results = pd.DataFrame(\n",
    "        tournament_results, columns=[\"player1\", \"player2\", \"result\"]\n",
    "    )\n",
    "    return tournament_results\n",
    "\n",
    "\n",
    "def play_matches(player1, players, games_per_pair, play_game):\n",
    "    results = []\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {player1.__name__} vs {opponent.__name__} game {_+1}\")\n",
    "                result = play_game(player1, opponent)\n",
    "                results.append((player1.__name__, opponent.__name__, result))\n",
    "\n",
    "    for opponent in players:\n",
    "        if opponent != player1:\n",
    "            for _ in range(games_per_pair // 2):\n",
    "                print(f\"Playing {opponent.__name__} vs {player1.__name__} game {_+1}\")\n",
    "                result = play_game(opponent, player1)\n",
    "                results.append(\n",
    "                    (\n",
    "                        player_names[players.index(opponent)],\n",
    "                        player_names[players.index(player1)],\n",
    "                        result,\n",
    "                    )\n",
    "                )\n",
    "    table.add_results_from_array(results)\n",
    "    print(table.bayes_elo())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(table.bayes_elo())\n",
    "print(table.get_win_table())\n",
    "print(table.get_draw_table())\n",
    "file = \"catan_1v1_tournament_results.pkl\"\n",
    "pickle.dump(table, open(file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, RandomPlayer, Color\n",
    "from catanatron.players.mcts import MCTSPlayer\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from catanatron.players.playouts import GreedyPlayoutsPlayer\n",
    "from catanatron.players.search import VictoryPointPlayer\n",
    "from catanatron.players.weighted_random import WeightedRandomPlayer\n",
    "from catanatron.players.value import ValueFunctionPlayer\n",
    "\n",
    "table.add_player(GreedyPlayoutsPlayer.__name__)\n",
    "players.append(GreedyPlayoutsPlayer) if GreedyPlayoutsPlayer not in players else None\n",
    "\n",
    "play_matches(GreedyPlayoutsPlayer, players, games_per_pair * 2, play_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play_1v1_tournament(players, games_per_pair, play_game)\n",
    "\n",
    "\n",
    "# table.add_results_from_dataframe(results)  # Adding multiple results\n",
    "print(table.bayes_elo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abaa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a petting zoo environment to see if it has all the functions and attributes needed\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "from agents.catan_player_wrapper import CatanPlayerWrapper\n",
    "from catanatron.players.minimax import AlphaBetaPlayer\n",
    "from utils.utils import record_video_wrapper\n",
    "from catanatron import Game, RandomPlayer, Color\n",
    "from game_configs.catan_config import CatanConfig\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "# env = catan_env(\n",
    "#     render_mode=\"rgb_array\",\n",
    "#     num_players=2,\n",
    "#     map_type=\"BASE\",\n",
    "#     vps_to_win=10,\n",
    "#     representation=\"vector\",\n",
    "#     invalid_action_reward=-1,\n",
    "#     auto_play_single_action=False,\n",
    "# )\n",
    "\n",
    "\n",
    "env = CatanConfig().make_env(\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-10,\n",
    "    render_mode=\"rgb_array\",\n",
    "    auto_play_single_action=False,\n",
    ")\n",
    "env = record_video_wrapper(env, \"./videos\", 1)\n",
    "env.reset()\n",
    "\n",
    "ab_player = CatanPlayerWrapper(AlphaBetaPlayer, Color.RED)\n",
    "\n",
    "for i in range(100):\n",
    "    state, reward, termination, truncation, info = env.last()\n",
    "    prediction = ab_player.predict(state, info, env)\n",
    "    action = ab_player.select_actions(prediction, info).item()\n",
    "    print(action)\n",
    "    print(info)\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95931df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from custom_gym_envs.envs.catan import (\n",
    "    env as catan_env,\n",
    "    CatanAECEnv,\n",
    ")\n",
    "\n",
    "env = catan_env(\n",
    "    render_mode=\"rgb_array\",\n",
    "    num_players=2,\n",
    "    map_type=\"BASE\",\n",
    "    vps_to_win=10,\n",
    "    representation=\"vector\",\n",
    "    invalid_action_reward=-1,\n",
    ")\n",
    "\n",
    "# Set metadata fps for 'human' if desired (not required for rgb_array)\n",
    "env.metadata[\"render_fps\"] = 10\n",
    "\n",
    "# Reset the environment (seed optional)\n",
    "env.reset(seed=0)\n",
    "print(env.last())\n",
    "print(\"Starting agent selection:\", env.agent_selection)\n",
    "\n",
    "# # Render one frame as an RGB array, then plot it\n",
    "# frame = env.render()  # will return a HxWx3 numpy array if renderer implemented\n",
    "# if frame is None:\n",
    "#     print(\n",
    "#         \"Renderer returned None. Make sure env.render(mode='rgb_array') is implemented.\"\n",
    "#     )\n",
    "# else:\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(frame)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# Setup matplotlib figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "print(\"Starting Catan Game Visualization...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run game for a few steps\n",
    "step_count = 0\n",
    "max_steps = 20\n",
    "\n",
    "while step_count < max_steps:\n",
    "    # Render current state\n",
    "    rgb_array = env.render()\n",
    "    print(env.last())\n",
    "\n",
    "    if rgb_array is not None:\n",
    "        # Display in notebook\n",
    "        # clear_output(wait=True)\n",
    "        ax.clear()\n",
    "        ax.imshow(rgb_array)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Catan Game - Step {step_count}\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "\n",
    "        # Print game info\n",
    "        current_agent = env.agent_selection\n",
    "        print(f\"\\nStep {step_count}\")\n",
    "        print(f\"Current Player: {current_agent}\")\n",
    "\n",
    "        if hasattr(env, \"game\") and hasattr(env.game, \"state\"):\n",
    "            print(f\"Turn Number: {env.game.state.num_turns}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Check if game is done\n",
    "    if env.terminations[env.agent_selection] or env.truncations[env.agent_selection]:\n",
    "        print(\"\\nGame Over!\")\n",
    "        break\n",
    "\n",
    "    # Get valid actions\n",
    "    obs = env.observe(env.agent_selection)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "    valid_actions = np.where(action_mask == 1)[0]\n",
    "\n",
    "    if len(valid_actions) > 0:\n",
    "        # Take a random valid action\n",
    "        action = np.random.choice(valid_actions)\n",
    "        env.step(action)\n",
    "    else:\n",
    "        print(\"No valid actions available!\")\n",
    "        break\n",
    "\n",
    "    step_count += 1\n",
    "    time.sleep(0.5)  # Pause to see each frame\n",
    "\n",
    "print(\"\\nTest completed!\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a11a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from custom_gym_envs.envs.catan import ActionType\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from agents.catan_player_wrapper import ACTIONS_ARRAY\n",
    "\n",
    "n_steps = 25\n",
    "delay = 0.25  # seconds between frames\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # If the current agent is terminated/truncated, we still call env_instance.step per PettingZoo convention:\n",
    "    current_agent = env.agent_selection\n",
    "\n",
    "    # get observation for current agent\n",
    "    obs = env.observe(current_agent)\n",
    "    action_mask = obs[\"action_mask\"]\n",
    "\n",
    "    legal_indices = np.nonzero(action_mask)[0]\n",
    "    if len(legal_indices) == 0:\n",
    "        # no legal moves available (should rarely happen). Choose end-turn fallback if available:\n",
    "        try:\n",
    "            end_turn_index = ACTIONS_ARRAY.index((ActionType.END_TURN, None))\n",
    "            action_choice = end_turn_index\n",
    "        except Exception:\n",
    "            # fallback to 0\n",
    "            action_choice = 0\n",
    "    else:\n",
    "        action_choice = int(random.choice(legal_indices))\n",
    "\n",
    "    # Step the env with the chosen action\n",
    "    env.step(action_choice)\n",
    "\n",
    "    # Render and display frame\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(frame)\n",
    "        plt.title(f\"Step {step+1} - Agent: {current_agent} - Action: {action_choice}\")\n",
    "        plt.axis(\"off\")\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No frame returned (render returned None)\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# After rollout, print summary\n",
    "print(\"Done. Final agent selection:\", env.agent_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
