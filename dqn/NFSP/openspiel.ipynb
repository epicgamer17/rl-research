{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb9952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/homebrew/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# OpenSpiel imports\n",
    "import pyspiel\n",
    "import open_spiel\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms import nfsp\n",
    "from open_spiel.python.algorithms.nfsp import MODE\n",
    "\n",
    "# TensorFlow imports (OpenSpiel NFSP uses TensorFlow)\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4893ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: leduc_poker\n",
      "Number of players: 2\n",
      "Information state size: 30\n",
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Game Environment\n",
    "def setup_leduc_poker():\n",
    "    \"\"\"Setup Leduc Poker environment.\"\"\"\n",
    "    game_name = \"leduc_poker\"\n",
    "    game = pyspiel.load_game(game_name)\n",
    "    env = rl_environment.Environment(game)\n",
    "\n",
    "    # Get game information\n",
    "    info_state_size = game.information_state_tensor_shape()[0]\n",
    "    num_actions = game.num_distinct_actions()\n",
    "    num_players = 2\n",
    "\n",
    "    print(f\"Game: {game_name}\")\n",
    "    print(f\"Number of players: {num_players}\")\n",
    "    print(f\"Information state size: {info_state_size}\")\n",
    "    print(f\"Number of actions: {num_actions}\")\n",
    "\n",
    "    return game, env, info_state_size, num_actions, num_players\n",
    "\n",
    "\n",
    "game, env, info_state_size, num_actions, num_players = setup_leduc_poker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e95539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created NFSP agent for player 0\n",
      "Created NFSP agent for player 1\n",
      "Initialized 2 NFSP agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 14:26:09.120276: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_4/bias_1/Assign' id:540 op device:{requested: '', assigned: ''} def:{{{node mlp_4/bias_1/Assign}} = Assign[T=DT_FLOAT, _class=[\"loc:@mlp_4/bias_1\"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_4/bias_1, mlp_4/zeros_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: NFSP Agent Configuration\n",
    "def create_nfsp_agents(sess, info_state_size, num_actions, num_players):\n",
    "    \"\"\"Create NFSP agents with specified configuration.\"\"\"\n",
    "\n",
    "    # NFSP hyperparameters\n",
    "    config = {\n",
    "        \"hidden_layers_sizes\": [128],\n",
    "        \"replay_buffer_capacity\": 200000,\n",
    "        \"reservoir_buffer_capacity\": 2000000,\n",
    "        \"anticipatory_param\": 0.1,\n",
    "        \"batch_size\": 128,\n",
    "        \"rl_learning_rate\": 0.1,\n",
    "        \"sl_learning_rate\": 0.005,\n",
    "        \"min_buffer_size_to_learn\": 1000,\n",
    "        \"learn_every\": 64,\n",
    "        \"epsilon_start\": 0.06,\n",
    "        \"epsilon_end\": 0.001,\n",
    "        \"epsilon_decay_duration\": int(1e6),\n",
    "        \"update_target_network_every\": 300,\n",
    "        \"discount_factor\": 0.99,\n",
    "    }\n",
    "\n",
    "    agents = []\n",
    "    for player_id in range(num_players):\n",
    "        agent = nfsp.NFSP(\n",
    "            session=sess,\n",
    "            player_id=player_id,\n",
    "            state_representation_size=info_state_size,\n",
    "            num_actions=num_actions,\n",
    "            **config,\n",
    "        )\n",
    "        agents.append(agent)\n",
    "        print(f\"Created NFSP agent for player {player_id}\")\n",
    "\n",
    "    return agents, config\n",
    "\n",
    "\n",
    "# Initialize TensorFlow session\n",
    "sess = tf.Session()\n",
    "agents, nfsp_config = create_nfsp_agents(\n",
    "    sess, info_state_size, num_actions, num_players\n",
    ")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(f\"Initialized {len(agents)} NFSP agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RL-based exploitability computation...\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NFSP' object has no attribute 'action_probabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Test the improved exploitability computation\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting RL-based exploitability computation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m initial_exploitability \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_exploitability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial exploitability (RL vs Average): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_exploitability\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mcompute_exploitability\u001b[0;34m(game, agents)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Target players: Average strategy (evaluation mode)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Exploiting players: Best response (RL policy with low epsilon)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_exploitability_approximate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m, in \u001b[0;36mcompute_exploitability_approximate\u001b[0;34m(game, agents, num_episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m total_exploitability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# for target_player in range(len(agents)):\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     # Test how much the RL agent (best response) can exploit the target's average strategy\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     test_env = rl_environment.Environment(game)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# return total_exploitability / len(agents)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mexploitability\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexploitability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/open_spiel/python/algorithms/exploitability.py:153\u001b[0m, in \u001b[0;36mexploitability\u001b[0;34m(game, policy)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe game must be constant- or zero-sum, not \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    150\u001b[0m       game_info\u001b[38;5;241m.\u001b[39mutility))\n\u001b[1;32m    151\u001b[0m root_state \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mnew_initial_state()\n\u001b[1;32m    152\u001b[0m nash_conv_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyspiel_best_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCPPBestResponsePolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_responder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbest_responder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_players\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m game\u001b[38;5;241m.\u001b[39mutility_sum())\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nash_conv_value \u001b[38;5;241m/\u001b[39m game\u001b[38;5;241m.\u001b[39mnum_players()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/open_spiel/python/algorithms/exploitability.py:154\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe game must be constant- or zero-sum, not \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    150\u001b[0m       game_info\u001b[38;5;241m.\u001b[39mutility))\n\u001b[1;32m    151\u001b[0m root_state \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mnew_initial_state()\n\u001b[1;32m    152\u001b[0m nash_conv_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m--> 154\u001b[0m         \u001b[43mpyspiel_best_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCPPBestResponsePolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_responder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalue(root_state)\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m best_responder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(game\u001b[38;5;241m.\u001b[39mnum_players())) \u001b[38;5;241m-\u001b[39m game\u001b[38;5;241m.\u001b[39mutility_sum())\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nash_conv_value \u001b[38;5;241m/\u001b[39m game\u001b[38;5;241m.\u001b[39mnum_players()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/open_spiel/python/algorithms/best_response.py:271\u001b[0m, in \u001b[0;36mCPPBestResponsePolicy.__init__\u001b[0;34m(self, game, best_responder_id, policy, all_states, state_to_information_state, best_response_processor, cut_threshold)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructor.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    Increasing this value will trade off accuracy for speed.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_information_state) \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    268\u001b[0m     compute_states_and_info_states_if_none(game, all_states,\n\u001b[1;32m    269\u001b[0m                                            state_to_information_state))\n\u001b[0;32m--> 271\u001b[0m policy_to_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_to_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_to_information_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Cache TabularBestResponse for players, due to their costly construction\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# TODO(b/140426861): Use a single best-responder once the code supports\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# multiple player ids.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m best_response_processor:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/open_spiel/python/algorithms/policy_utils.py:60\u001b[0m, in \u001b[0;36mpolicy_to_dict\u001b[0;34m(player_policy, game, all_states, state_to_information_state)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m all_states:\n\u001b[1;32m     58\u001b[0m   information_state \u001b[38;5;241m=\u001b[39m state_to_information_state[state]\n\u001b[1;32m     59\u001b[0m   tabular_policy[information_state] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m---> 60\u001b[0m       \u001b[43mplayer_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_probabilities\u001b[49m(all_states[state])\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tabular_policy\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NFSP' object has no attribute 'action_probabilities'"
     ]
    }
   ],
   "source": [
    "# Cell 4: Fixed Exploitability Computation\n",
    "\n",
    "\n",
    "def compute_exploitability_approximate(game, agents, num_episodes=10000):\n",
    "    \"\"\"\n",
    "    Approximate exploitability by measuring how much a best response can exploit the average policy.\n",
    "    - Target player uses AVERAGE STRATEGY (is_evaluation=True)\n",
    "    - Exploiting player uses BEST RESPONSE (RL policy with low epsilon)\n",
    "    \"\"\"\n",
    "    total_exploitability = 0.0\n",
    "\n",
    "    # for target_player in range(len(agents)):\n",
    "    #     # Test how much the RL agent (best response) can exploit the target's average strategy\n",
    "    #     test_env = rl_environment.Environment(game)\n",
    "    #     exploiter_player = 1 - target_player  # The other player\n",
    "    #     for agent in agents:\n",
    "    #         agent._mode = MODE.best_response\n",
    "    #     agents[target_player]._mode = MODE.average_policy\n",
    "    #     rewards_vs_rl = []\n",
    "\n",
    "    #     for episode in range(num_episodes):\n",
    "    #         time_step = test_env.reset()\n",
    "    #         episode_rewards = [0.0, 0.0]\n",
    "\n",
    "    #         while not time_step.last():\n",
    "    #             current_player = time_step.observations[\"current_player\"]\n",
    "\n",
    "    #             # TARGET PLAYER: Use AVERAGE STRATEGY (supervised learning policy)\n",
    "    #             # This is the policy being evaluated for exploitability\n",
    "    #             agent_output = agents[current_player].step(\n",
    "    #                 time_step, is_evaluation=True\n",
    "    #             )\n",
    "    #             action = agent_output.action\n",
    "    #             time_step = test_env.step([action])\n",
    "\n",
    "    #             if time_step.rewards:\n",
    "    #                 for i, reward in enumerate(time_step.rewards):\n",
    "    #                     episode_rewards[i] += reward\n",
    "\n",
    "    #         # How much did the RL agent (best response) gain against the average strategy?\n",
    "    #         exploiter_reward = episode_rewards[exploiter_player]\n",
    "    #         rewards_vs_rl.append(exploiter_reward)\n",
    "\n",
    "    #     # Average exploitation value for this target player\n",
    "    #     avg_exploitation = np.mean(rewards_vs_rl)\n",
    "    #     total_exploitability += avg_exploitation\n",
    "    #     print(f\"Player {target_player} exploitability: {avg_exploitation:.6f}\")\n",
    "\n",
    "    # return total_exploitability / len(agents)\n",
    "\n",
    "\n",
    "def compute_exploitability(game, agents):\n",
    "    \"\"\"\n",
    "    Main exploitability function - measures how much best responses can exploit average strategies.\n",
    "    \"\"\"\n",
    "    print(\"Computing exploitability using:\")\n",
    "    print(\"- Target players: Average strategy (evaluation mode)\")\n",
    "    print(\"- Exploiting players: Best response (RL policy with low epsilon)\")\n",
    "    return compute_exploitability_approximate(game, agents, num_episodes=50)\n",
    "\n",
    "\n",
    "# Test the improved exploitability computation\n",
    "print(\"Testing RL-based exploitability computation...\")\n",
    "\n",
    "initial_exploitability = compute_exploitability(game, agents)\n",
    "print(f\"Initial exploitability (RL vs Average): {initial_exploitability:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode completed: {'rewards': [-3.0, 3.0], 'average_reward': 0.0, 'steps': 4}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Episode Function\n",
    "def train_episode(env, agents):\n",
    "    \"\"\"Train one episode and return episode statistics.\"\"\"\n",
    "    time_step = env.reset()\n",
    "    episode_rewards = [0] * len(agents)\n",
    "    steps = 0\n",
    "\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "\n",
    "        # Get action from NFSP agent\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action = agent_output.action\n",
    "\n",
    "        # Execute action\n",
    "        time_step = env.step([action])\n",
    "        steps += 1\n",
    "\n",
    "        # Accumulate rewards\n",
    "        if time_step.rewards:\n",
    "            for i, reward in enumerate(time_step.rewards):\n",
    "                episode_rewards[i] += reward\n",
    "\n",
    "    # Final step for agents\n",
    "    for agent in agents:\n",
    "        agent.step(time_step)\n",
    "\n",
    "    return {\n",
    "        \"rewards\": episode_rewards,\n",
    "        \"average_reward\": np.mean(episode_rewards),\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test one training episode\n",
    "episode_stats = train_episode(env, agents)\n",
    "print(f\"Test episode completed: {episode_stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2558dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main Training Loop with Periodic Evaluation\n",
    "def train_nfsp_with_evaluation(game, env, agents, num_episodes=100000, eval_every=1000):\n",
    "    \"\"\"Main training loop with periodic exploitability evaluation.\"\"\"\n",
    "\n",
    "    # Training metrics\n",
    "    training_metrics = {\n",
    "        \"episodes\": [],\n",
    "        \"exploitability\": [],\n",
    "        \"average_rewards\": [],\n",
    "        \"training_time\": [],\n",
    "        \"episode_lengths\": [],\n",
    "    }\n",
    "\n",
    "    print(f\"Starting NFSP training for {num_episodes} episodes...\")\n",
    "    print(f\"Evaluating exploitability every {eval_every} episodes\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Train one episode\n",
    "        episode_stats = train_episode(env, agents)\n",
    "\n",
    "        # Periodic evaluation\n",
    "        if (episode + 1) % eval_every == 0:\n",
    "            print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "\n",
    "            # Compute exploitability\n",
    "            current_exploitability = compute_exploitability(game, agents)\n",
    "\n",
    "            # Record metrics\n",
    "            training_metrics[\"episodes\"].append(episode + 1)\n",
    "            training_metrics[\"exploitability\"].append(current_exploitability)\n",
    "            training_metrics[\"average_rewards\"].append(episode_stats[\"average_reward\"])\n",
    "            training_metrics[\"training_time\"].append(time.time() - start_time)\n",
    "            training_metrics[\"episode_lengths\"].append(episode_stats[\"steps\"])\n",
    "\n",
    "            print(f\"Exploitability: {current_exploitability:.6f}\")\n",
    "            print(f\"Average episode reward: {episode_stats['average_reward']:.4f}\")\n",
    "            print(f\"Average episode length: {episode_stats['steps']:.1f}\")\n",
    "            print(f\"Time elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        # Progress indicator\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} completed\", end=\"\\r\")\n",
    "\n",
    "    print(f\"\\nTraining completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # Final evaluation\n",
    "    final_exploitability = compute_exploitability(game, agents)\n",
    "    print(f\"Final exploitability: {final_exploitability:.6f}\")\n",
    "\n",
    "    return training_metrics\n",
    "\n",
    "\n",
    "# # Run training - adjust num_episodes based on your computational resources\n",
    "# training_metrics = train_nfsp_with_evaluation(\n",
    "#     game,\n",
    "#     env,\n",
    "#     agents,\n",
    "#     num_episodes=20000 * 64,  # Start with smaller number for testing\n",
    "#     eval_every=128000 // 2,  # Evaluate more frequently for better tracking\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65504d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NFSP training for 2560000 steps...\n",
      "Evaluating exploitability every 128000 steps\n",
      "128000\n",
      "\n",
      "Step 6/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: -0.280000\n",
      "Player 1 exploitability: 0.880000\n",
      "Exploitability: 0.300000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 0.04s\n",
      "Step 126000/2560000 completed\n",
      "Step 128003/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.800000\n",
      "Player 1 exploitability: 2.840000\n",
      "Exploitability: 1.820000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 6.0\n",
      "Time elapsed: 19.26s\n",
      "Step 253000/2560000 completed\n",
      "Step 256004/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.180000\n",
      "Player 1 exploitability: 2.860000\n",
      "Exploitability: 1.520000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 38.47s\n",
      "Step 369000/2560000 completed\n",
      "Step 384004/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.420000\n",
      "Player 1 exploitability: -0.280000\n",
      "Exploitability: 0.570000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 58.35s\n",
      "Step 509000/2560000 completed\n",
      "Step 512004/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.940000\n",
      "Player 1 exploitability: 1.120000\n",
      "Exploitability: 1.030000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 6.0\n",
      "Time elapsed: 85.59s\n",
      "Step 637000/2560000 completed\n",
      "Step 640002/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 2.300000\n",
      "Player 1 exploitability: 1.320000\n",
      "Exploitability: 1.810000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 4.0\n",
      "Time elapsed: 106.36s\n",
      "Step 763000/2560000 completed\n",
      "Step 768007/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.940000\n",
      "Player 1 exploitability: 2.040000\n",
      "Exploitability: 1.490000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 6.0\n",
      "Time elapsed: 127.79s\n",
      "Step 895000/2560000 completed\n",
      "Step 896006/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.240000\n",
      "Player 1 exploitability: 1.260000\n",
      "Exploitability: 1.250000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 148.38s\n",
      "Step 1007000/2560000 completed\n",
      "Step 1024003/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.840000\n",
      "Player 1 exploitability: 1.700000\n",
      "Exploitability: 1.770000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 4.0\n",
      "Time elapsed: 170.36s\n",
      "Step 1144000/2560000 completed\n",
      "Step 1152005/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.140000\n",
      "Player 1 exploitability: 1.540000\n",
      "Exploitability: 1.340000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 6.0\n",
      "Time elapsed: 190.31s\n",
      "Step 1271000/2560000 completed\n",
      "Step 1280005/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.760000\n",
      "Player 1 exploitability: 1.780000\n",
      "Exploitability: 1.270000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 4.0\n",
      "Time elapsed: 210.60s\n",
      "Step 1403000/2560000 completed\n",
      "Step 1408002/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.680000\n",
      "Player 1 exploitability: 2.100000\n",
      "Exploitability: 1.390000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 236.25s\n",
      "Step 1535000/2560000 completed\n",
      "Step 1536003/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.000000\n",
      "Player 1 exploitability: 1.760000\n",
      "Exploitability: 1.380000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 6.0\n",
      "Time elapsed: 257.58s\n",
      "Step 1663000/2560000 completed\n",
      "Step 1664003/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.880000\n",
      "Player 1 exploitability: 1.460000\n",
      "Exploitability: 1.670000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 4.0\n",
      "Time elapsed: 278.56s\n",
      "Step 1787000/2560000 completed\n",
      "Step 1792003/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.160000\n",
      "Player 1 exploitability: -1.160000\n",
      "Exploitability: 0.000000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 302.81s\n",
      "Step 1920000/2560000 completed\n",
      "Step 1920005/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.780000\n",
      "Player 1 exploitability: 1.240000\n",
      "Exploitability: 1.510000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 322.87s\n",
      "Step 2048000/2560000 completed\n",
      "Step 2048005/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.540000\n",
      "Player 1 exploitability: 0.240000\n",
      "Exploitability: 0.890000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 5.0\n",
      "Time elapsed: 344.22s\n",
      "Step 2175000/2560000 completed\n",
      "Step 2176005/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.480000\n",
      "Player 1 exploitability: 1.600000\n",
      "Exploitability: 1.040000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 7.0\n",
      "Time elapsed: 364.27s\n",
      "Step 2297000/2560000 completed\n",
      "Step 2304005/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 2.120000\n",
      "Player 1 exploitability: 0.160000\n",
      "Exploitability: 1.140000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 4.0\n",
      "Time elapsed: 385.56s\n",
      "Step 2425000/2560000 completed\n",
      "Step 2432003/2560000\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 3.080000\n",
      "Player 1 exploitability: 3.140000\n",
      "Exploitability: 3.110000\n",
      "Average episode reward: 0.0000\n",
      "Average episode length: 4.0\n",
      "Time elapsed: 405.80s\n",
      "Step 2554000/2560000 completed\n",
      "Training completed in 428.39s\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 0.920000\n",
      "Player 1 exploitability: -0.240000\n",
      "Final exploitability: 0.340000\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Training Loop with Periodic Evaluation\n",
    "def train_nfsp_with_evaluation(game, env, agents, num_steps=20000, eval_every=1000):\n",
    "    \"\"\"Main training loop with periodic exploitability evaluation.\"\"\"\n",
    "\n",
    "    # Training metrics\n",
    "    training_metrics = {\n",
    "        \"steps\": [],\n",
    "        \"exploitability\": [],\n",
    "        \"average_rewards\": [],\n",
    "        \"training_time\": [],\n",
    "        \"episode_lengths\": [],\n",
    "    }\n",
    "\n",
    "    print(f\"Starting NFSP training for {num_steps} steps...\")\n",
    "    print(f\"Evaluating exploitability every {eval_every} steps\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    eval_points = []\n",
    "    i = 0\n",
    "    while i < num_steps:\n",
    "        eval_points.append(i)\n",
    "        i += eval_every\n",
    "    print(eval_every)\n",
    "    step = 0\n",
    "    while step < num_steps:\n",
    "        # Train one episode\n",
    "        episode_stats = train_episode(env, agents)\n",
    "\n",
    "        step += episode_stats[\"steps\"]\n",
    "\n",
    "        # Periodic evaluation\n",
    "\n",
    "        if len(eval_points) > 0 and step > eval_points[0]:\n",
    "            eval_points.pop(0)\n",
    "            print(f\"\\nStep {step + 1}/{num_steps}\")\n",
    "\n",
    "            # Compute exploitability\n",
    "            current_exploitability = compute_exploitability(game, agents)\n",
    "\n",
    "            # Record metrics\n",
    "            training_metrics[\"steps\"].append(step + 1)\n",
    "            training_metrics[\"exploitability\"].append(current_exploitability)\n",
    "            training_metrics[\"average_rewards\"].append(episode_stats[\"average_reward\"])\n",
    "            training_metrics[\"training_time\"].append(time.time() - start_time)\n",
    "            training_metrics[\"episode_lengths\"].append(episode_stats[\"steps\"])\n",
    "\n",
    "            print(f\"Exploitability: {current_exploitability:.6f}\")\n",
    "            print(f\"Average episode reward: {episode_stats['average_reward']:.4f}\")\n",
    "            print(f\"Average episode length: {episode_stats['steps']:.1f}\")\n",
    "            print(f\"Time elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        # Progress indicator\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print(f\"Step {step + 1}/{num_steps} completed\", end=\"\\r\")\n",
    "\n",
    "    print(f\"\\nTraining completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # Final evaluation\n",
    "    final_exploitability = compute_exploitability(game, agents)\n",
    "    print(f\"Final exploitability: {final_exploitability:.6f}\")\n",
    "\n",
    "    return training_metrics\n",
    "\n",
    "\n",
    "# Run training - adjust num_episodes based on your computational resources\n",
    "training_metrics = train_nfsp_with_evaluation(\n",
    "    game,\n",
    "    env,\n",
    "    agents,\n",
    "    num_steps=20000 * 128,  # Start with smaller number for testing\n",
    "    eval_every=1000 * 128,  # Evaluate more frequently for better tracking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bcf1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'episodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plot training results\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mplot_training_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnfsp_leduc_training_progress.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mplot_training_progress\u001b[0;34m(training_metrics, save_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot training progress including exploitability and rewards.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, ((ax1, ax2), (ax3, ax4)) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m exploitabilities \u001b[38;5;241m=\u001b[39m training_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexploitability\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m rewards \u001b[38;5;241m=\u001b[39m training_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episodes'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Visualize Training Progress\n",
    "def plot_training_progress(training_metrics, save_path=None):\n",
    "    \"\"\"Plot training progress including exploitability and rewards.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    episodes = training_metrics[\"episodes\"]\n",
    "    exploitabilities = training_metrics[\"exploitability\"]\n",
    "    rewards = training_metrics[\"average_rewards\"]\n",
    "    times = training_metrics[\"training_time\"]\n",
    "    lengths = training_metrics[\"episode_lengths\"]\n",
    "\n",
    "    # Plot exploitability\n",
    "    ax1.plot(episodes, exploitabilities, \"b-\", linewidth=2, label=\"Exploitability\")\n",
    "    ax1.set_xlabel(\"Episodes\")\n",
    "    ax1.set_ylabel(\"Exploitability\")\n",
    "    ax1.set_title(\"NFSP Training Progress: Exploitability\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale(\"log\")  # Log scale for better visualization\n",
    "\n",
    "    # Plot average rewards\n",
    "    ax2.plot(episodes, rewards, \"r-\", linewidth=2, label=\"Average Reward\")\n",
    "    ax2.set_xlabel(\"Episodes\")\n",
    "    ax2.set_ylabel(\"Average Reward\")\n",
    "    ax2.set_title(\"NFSP Training Progress: Average Rewards\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot training time\n",
    "    ax3.plot(\n",
    "        episodes,\n",
    "        np.array(times) / 60,\n",
    "        \"g-\",\n",
    "        linewidth=2,\n",
    "        label=\"Training Time (minutes)\",\n",
    "    )\n",
    "    ax3.set_xlabel(\"Episodes\")\n",
    "    ax3.set_ylabel(\"Training Time (minutes)\")\n",
    "    ax3.set_title(\"NFSP Training Progress: Time\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "\n",
    "    # Plot episode lengths\n",
    "    ax4.plot(episodes, lengths, \"m-\", linewidth=2, label=\"Episode Length\")\n",
    "    ax4.set_xlabel(\"Episodes\")\n",
    "    ax4.set_ylabel(\"Steps per Episode\")\n",
    "    ax4.set_title(\"NFSP Training Progress: Episode Length\")\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Training progress plot saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot training results\n",
    "plot_training_progress(training_metrics, \"nfsp_leduc_training_progress.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25af444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exploitability Analysis ===\n",
      "Initial exploitability: 2.050000\n",
      "Final exploitability: 1.490000\n",
      "Improvement factor: 1.38x\n",
      "Significant improvement at episode 4000: 49.0%\n",
      "Significant improvement at episode 5000: 15.2%\n",
      "Significant improvement at episode 9000: 116.1%\n",
      "Significant improvement at episode 10000: 380.6%\n",
      "Significant improvement at episode 12000: 49.5%\n",
      "Significant improvement at episode 13000: 10.6%\n",
      "Significant improvement at episode 14000: 70.2%\n",
      "Significant improvement at episode 16000: 91.4%\n",
      "Significant improvement at episode 19000: 88.2%\n",
      "Significant improvement at episode 24000: 27.2%\n",
      "Significant improvement at episode 25000: 14.2%\n",
      "Significant improvement at episode 27000: 14.4%\n",
      "Significant improvement at episode 28000: 31.3%\n",
      "Significant improvement at episode 32000: 34.0%\n",
      "Significant improvement at episode 35000: 22.0%\n",
      "Significant improvement at episode 36000: 52.5%\n",
      "Significant improvement at episode 39000: 28.9%\n",
      "Significant improvement at episode 41000: 38.2%\n",
      "Significant improvement at episode 43000: 16.6%\n",
      "Significant improvement at episode 46000: 68.5%\n",
      "Significant improvement at episode 48000: 51.3%\n",
      "Significant improvement at episode 51000: 28.8%\n",
      "Significant improvement at episode 54000: 34.0%\n",
      "Significant improvement at episode 55000: 62.1%\n",
      "Significant improvement at episode 58000: 22.0%\n",
      "Significant improvement at episode 59000: 48.7%\n",
      "Significant improvement at episode 60000: 59.1%\n",
      "Significant improvement at episode 62000: 39.0%\n",
      "Significant improvement at episode 64000: 17.7%\n",
      "Significant improvement at episode 67000: 38.3%\n",
      "Significant improvement at episode 68000: 36.1%\n",
      "Significant improvement at episode 69000: 37.9%\n",
      "Significant improvement at episode 71000: 48.6%\n",
      "Significant improvement at episode 73000: 52.9%\n",
      "Significant improvement at episode 74000: 35.8%\n",
      "Significant improvement at episode 77000: 21.7%\n",
      "Significant improvement at episode 80000: 33.9%\n",
      "Significant improvement at episode 85000: 10.4%\n",
      "Significant improvement at episode 89000: 31.4%\n",
      "Significant improvement at episode 92000: 19.8%\n",
      "Significant improvement at episode 94000: 42.9%\n",
      "Significant improvement at episode 98000: 52.8%\n",
      "Significant improvement at episode 100000: 31.8%\n",
      "Significant improvement at episode 102000: 22.0%\n",
      "Significant improvement at episode 104000: 23.6%\n",
      "Significant improvement at episode 107000: 37.6%\n",
      "Significant improvement at episode 110000: 43.7%\n",
      "Significant improvement at episode 112000: 56.7%\n",
      "Significant improvement at episode 114000: 49.1%\n",
      "Significant improvement at episode 117000: 42.6%\n",
      "Significant improvement at episode 119000: 115.5%\n",
      "Significant improvement at episode 120000: 353.8%\n",
      "Significant improvement at episode 122000: 48.6%\n",
      "Significant improvement at episode 125000: 33.3%\n",
      "Significant improvement at episode 127000: 43.4%\n",
      "Significant improvement at episode 130000: 23.8%\n",
      "Significant improvement at episode 132000: 55.3%\n",
      "Significant improvement at episode 135000: 15.4%\n",
      "Significant improvement at episode 136000: 26.3%\n",
      "Significant improvement at episode 137000: 96.8%\n",
      "Significant improvement at episode 139000: 34.5%\n",
      "Significant improvement at episode 140000: 25.0%\n",
      "Significant improvement at episode 145000: 45.9%\n",
      "Significant improvement at episode 148000: 13.0%\n",
      "Significant improvement at episode 149000: 14.4%\n",
      "Significant improvement at episode 150000: 66.9%\n",
      "Significant improvement at episode 152000: 75.0%\n",
      "Significant improvement at episode 154000: 23.2%\n",
      "Significant improvement at episode 155000: 15.3%\n",
      "Significant improvement at episode 156000: 11.7%\n",
      "Significant improvement at episode 160000: 44.7%\n",
      "Significant improvement at episode 161000: 72.0%\n",
      "Significant improvement at episode 164000: 11.3%\n",
      "Significant improvement at episode 165000: 49.7%\n",
      "Significant improvement at episode 167000: 57.3%\n",
      "Significant improvement at episode 171000: 36.8%\n",
      "Significant improvement at episode 174000: 21.7%\n",
      "Significant improvement at episode 175000: 18.9%\n",
      "Significant improvement at episode 176000: 30.7%\n",
      "Significant improvement at episode 177000: 13.9%\n",
      "Significant improvement at episode 179000: 17.3%\n",
      "Significant improvement at episode 183000: 52.7%\n",
      "Significant improvement at episode 187000: 50.5%\n",
      "Significant improvement at episode 188000: 55.7%\n",
      "Significant improvement at episode 190000: 32.2%\n",
      "Significant improvement at episode 192000: 18.5%\n",
      "Significant improvement at episode 193000: 110.9%\n",
      "Significant improvement at episode 194000: 1242.9%\n",
      "Significant improvement at episode 195000: 28.7%\n",
      "Significant improvement at episode 200000: 19.9%\n",
      "Final quarter mean exploitability: 1.622200\n",
      "Final quarter std deviation: 0.620797\n",
      "Coefficient of variation: 38.27%\n",
      " Training may need more episodes (high variance in final quarter)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Detailed Exploitability Analysis\n",
    "def analyze_exploitability_convergence(training_metrics):\n",
    "    \"\"\"Analyze exploitability convergence patterns.\"\"\"\n",
    "    episodes = np.array(training_metrics[\"episodes\"])\n",
    "    exploitabilities = np.array(training_metrics[\"exploitability\"])\n",
    "\n",
    "    print(\"=== Exploitability Analysis ===\")\n",
    "    print(f\"Initial exploitability: {exploitabilities[0]:.6f}\")\n",
    "    print(f\"Final exploitability: {exploitabilities[-1]:.6f}\")\n",
    "    print(f\"Improvement factor: {exploitabilities[0] / exploitabilities[-1]:.2f}x\")\n",
    "\n",
    "    # Find episodes where exploitability dropped significantly\n",
    "    for i in range(1, len(exploitabilities)):\n",
    "        improvement = (\n",
    "            exploitabilities[i - 1] - exploitabilities[i]\n",
    "        ) / exploitabilities[i - 1]\n",
    "        if improvement > 0.1:  # More than 10% improvement\n",
    "            print(\n",
    "                f\"Significant improvement at episode {episodes[i]}: {improvement:.1%}\"\n",
    "            )\n",
    "\n",
    "    # Convergence analysis\n",
    "    final_quarter = exploitabilities[-len(exploitabilities) // 4 :]\n",
    "    std_final = np.std(final_quarter)\n",
    "    mean_final = np.mean(final_quarter)\n",
    "\n",
    "    print(f\"Final quarter mean exploitability: {mean_final:.6f}\")\n",
    "    print(f\"Final quarter std deviation: {std_final:.6f}\")\n",
    "    print(f\"Coefficient of variation: {std_final/mean_final:.2%}\")\n",
    "\n",
    "    if std_final / mean_final < 0.1:\n",
    "        print(\" Training appears to have converged (low variance in final quarter)\")\n",
    "    else:\n",
    "        print(\" Training may need more episodes (high variance in final quarter)\")\n",
    "\n",
    "\n",
    "analyze_exploitability_convergence(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Trained Agents ===\n",
      "NFSP agent vs Random: 1.2940  4.4587\n",
      "Random vs NFSP agent: 1.1580\n",
      "Combined NFSP performance: 0.1360\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Test Trained Agents\n",
    "def test_trained_agents(game, agents, num_test_games=1000):\n",
    "    \"\"\"Test the trained agents' performance.\"\"\"\n",
    "    print(\"=== Testing Trained Agents ===\")\n",
    "\n",
    "    # Test against random policy\n",
    "    from open_spiel.python.algorithms import random_agent\n",
    "\n",
    "    test_env = rl_environment.Environment(game)\n",
    "    random_agents = [\n",
    "        random_agent.RandomAgent(player_id=i, num_actions=num_actions)\n",
    "        for i in range(num_players)\n",
    "    ]\n",
    "\n",
    "    nfsp_vs_random_rewards = []\n",
    "    random_vs_nfsp_rewards = []\n",
    "\n",
    "    # NFSP agents vs Random agents\n",
    "    for _ in range(num_test_games // 2):\n",
    "        time_step = test_env.reset()\n",
    "        episode_rewards = [0, 0]\n",
    "\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "\n",
    "            if player_id == 0:  # NFSP agent\n",
    "                # Use average policy for testing\n",
    "                agent_output = agents[player_id].step(time_step, is_evaluation=True)\n",
    "                action = agent_output.action\n",
    "            else:  # Random agent\n",
    "                action = random_agents[player_id].step(time_step).action\n",
    "\n",
    "            time_step = test_env.step([action])\n",
    "\n",
    "            if time_step.rewards:\n",
    "                for i, reward in enumerate(time_step.rewards):\n",
    "                    episode_rewards[i] += reward\n",
    "\n",
    "        nfsp_vs_random_rewards.append(episode_rewards[0])\n",
    "\n",
    "    # Random agents vs NFSP agents\n",
    "    for _ in range(num_test_games // 2):\n",
    "        time_step = test_env.reset()\n",
    "        episode_rewards = [0, 0]\n",
    "\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "\n",
    "            if player_id == 0:  # Random agent\n",
    "                action = random_agents[player_id].step(time_step).action\n",
    "            else:  # NFSP agent\n",
    "                agent_output = agents[player_id].step(time_step, is_evaluation=True)\n",
    "                action = agent_output.action\n",
    "\n",
    "            time_step = test_env.step([action])\n",
    "\n",
    "            if time_step.rewards:\n",
    "                for i, reward in enumerate(time_step.rewards):\n",
    "                    episode_rewards[i] += reward\n",
    "\n",
    "        random_vs_nfsp_rewards.append(episode_rewards[1])\n",
    "\n",
    "    nfsp_avg_vs_random = np.mean(nfsp_vs_random_rewards)\n",
    "    nfsp_avg_vs_random_std = np.std(nfsp_vs_random_rewards)\n",
    "    random_avg_vs_nfsp = np.mean(random_vs_nfsp_rewards)\n",
    "\n",
    "    print(\n",
    "        f\"NFSP agent vs Random: {nfsp_avg_vs_random:.4f}  {nfsp_avg_vs_random_std:.4f}\"\n",
    "    )\n",
    "    print(f\"Random vs NFSP agent: {random_avg_vs_nfsp:.4f}\")\n",
    "    print(f\"Combined NFSP performance: {(nfsp_avg_vs_random - random_avg_vs_nfsp):.4f}\")\n",
    "\n",
    "    return nfsp_vs_random_rewards, random_vs_nfsp_rewards\n",
    "\n",
    "\n",
    "# Test the trained agents\n",
    "nfsp_results, random_results = test_trained_agents(game, agents, num_test_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec166f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved agent 0 to ./nfsp_leduc_results/agent_0\n",
      "Saved agent 1 to ./nfsp_leduc_results/agent_1\n",
      "Saved training metrics to ./nfsp_leduc_results/training_metrics.pkl\n",
      "Saved configuration to ./nfsp_leduc_results/config.pkl\n",
      "All results saved to ./nfsp_leduc_results\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Save Results and Models\n",
    "def save_training_results(agents, training_metrics, save_dir=\"./nfsp_leduc_results\"):\n",
    "    \"\"\"Save trained agents and training results.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save agents\n",
    "    for i, agent in enumerate(agents):\n",
    "        agent_path = os.path.join(save_dir, f\"agent_{i}\")\n",
    "        agent.save(agent_path)\n",
    "        print(f\"Saved agent {i} to {agent_path}\")\n",
    "\n",
    "    # Save training metrics\n",
    "    metrics_path = os.path.join(save_dir, \"training_metrics.pkl\")\n",
    "    with open(metrics_path, \"wb\") as f:\n",
    "        pickle.dump(training_metrics, f)\n",
    "    print(f\"Saved training metrics to {metrics_path}\")\n",
    "\n",
    "    # Save configuration\n",
    "    config_path = os.path.join(save_dir, \"config.pkl\")\n",
    "    with open(config_path, \"wb\") as f:\n",
    "        pickle.dump(nfsp_config, f)\n",
    "    print(f\"Saved configuration to {config_path}\")\n",
    "\n",
    "    print(f\"All results saved to {save_dir}\")\n",
    "\n",
    "\n",
    "# Save everything\n",
    "save_training_results(agents, training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2c857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFSP training on Leduc Poker completed successfully!\n",
      "Final exploitability: 1.490000\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Cleanup\n",
    "def cleanup_session(sess):\n",
    "    \"\"\"Close TensorFlow session.\"\"\"\n",
    "    sess.close()\n",
    "    print(\"TensorFlow session closed\")\n",
    "\n",
    "\n",
    "# Uncomment the line below when you're done with training\n",
    "# cleanup_session(sess)\n",
    "\n",
    "print(\"NFSP training on Leduc Poker completed successfully!\")\n",
    "print(f\"Final exploitability: {training_metrics['exploitability'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10529337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing initial exploitability...\n",
      "Computing exploitability using:\n",
      "- Target players: Average strategy (evaluation mode)\n",
      "- Exploiting players: Best response (RL policy with low epsilon)\n",
      "Player 0 exploitability: 1.960000\n",
      "Player 1 exploitability: 1.600000\n",
      "Initial exploitability: 1.780000\n"
     ]
    }
   ],
   "source": [
    "# Test both methods\n",
    "print(\"Computing initial exploitability...\")\n",
    "\n",
    "try:\n",
    "    initial_exploitability = compute_exploitability(game, agents)\n",
    "    print(f\"Initial exploitability: {initial_exploitability:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exact exploitability failed: {e}\")\n",
    "    print(\"Using Nash convergence approximation instead...\")\n",
    "    initial_exploitability = compute_nash_conv_approximation(\n",
    "        game, agents, num_samples=100\n",
    "    )\n",
    "    print(f\"Initial Nash conv approximation: {initial_exploitability:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
