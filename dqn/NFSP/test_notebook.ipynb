{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noticing that it improves to a point and then get bad\n",
    "# try decreasing learning rate of sl (less overfitting slower learning more convergence?)\n",
    "#   try 0.000125 (defualt 0.0005)\n",
    "# try increasing rl learning rate (maybe to 0.005), and maybe try increasing sl learning rate a tiny bit now that i have decreased replay interval (and we are hopefully getting less repeated experiences)\n",
    "# try changing buffer size of sl (smaller more convergence? bigger less forgetting?)\n",
    "#   try 125000 (default 250000)\n",
    "#   try 500000\n",
    "# try changing number of filters of sl (not sure how this would help with convergence but maybe forgetting)\n",
    "#   try 128 (default 64)\n",
    "# try lowering replay interval? to decrease change between training steps and repeat experiences\n",
    "#   try 64 and 32 and 16? (default 128)\n",
    "# try increasing replay interval for faster convergence?\n",
    "#   try 256?\n",
    "# try decreasing minibatches to stop overfitting?\n",
    "#   try 2 and 1? (default 4)\n",
    "\n",
    "# noticing rl agent gets negative exploitability sometimes, means something is wrong with rl agent\n",
    "# maybe not long enough replay memory? so it tries something its already tried that is bad?\n",
    "# maybe not enough filters?\n",
    "# maybe overfitting? learning rate too high? batch size too small?\n",
    "\n",
    "\n",
    "# config 4 negative rl agent vs p1, may not have let it train long enough.\n",
    "# config 5 super super slow training time (12 minutes per 1000 iterations compared to usual 1.5 minutes)\n",
    "# config 6 seemingly very good but only plays 0 on first move (probably only plays 1 move on first move) and never changes, so doesnt seem to explore enough. RL agent does not have accurate q value (probably lack of playing and seeing those moves). SO OVERFIT TO ONE SET IT DOESNT DRAW WITH ANY OTHER MOVES, even after first same move\n",
    "# config 7 some negatives and same problems as 6 (i just decreased filters)\n",
    "# config 8 has negatives\n",
    "# config 16 starts good, noticing all the rl agents have very optimistic value predictions (which is really good). seems to have stopped improving after continuing training, may be a bug in loading/saving\n",
    "# config 16 with eg epsilon and 500k was good, maybe try higher eg? or higher noisy sigma? maybe  try using some stuff from 6\n",
    "\n",
    "# config 4 -> 5: changed to not shared_networks\n",
    "# config 5 -> 6: switched to 16 filters of conv layers from 64 3x3 and 64 2x2 res layers\n",
    "# config 6 -> 7: switched to 8 filters of conv layers and 2M sl memory\n",
    "# config 7 -> 8: switched to 16 filters of conv layers\n",
    "# config 8 -> 16: switched to 64 filters of conv layers, (still no eg_epsilon, even though i set it final step is 0), switched to lr 0.0025 and Adam optimizer for RL, noisy sigma 0.06, 4 minibatches, RL buffer 200000, replay interval down to 32, shared networks true, and added weight decay, only 100K training steps\n",
    "# *** all trials before 16 had non linear per beta annealing, 16 switched to linear after 100K steps were non linear\n",
    "# config 16 -> 31: switched to adam and adam, added eg_epsilon linear, and noisy sigma 0.5, 500K training steps, 2 minibatches instead of 4,\n",
    "# config 16 -> 32: switched to 0.5 noisy sigma, removed dense layers,\n",
    "# config 32 -> 33: thinking of removing clip norm, removed weight decay, maybe adding more eg with linear decay? so in later positions rl explores more? also would like to maybe for 34 try dense layers and see if they are better, also try double SGD and double Adam with both conv and dense so those would be 35 and 36\n",
    "\n",
    "\n",
    "# 32 was very good overall, needed more training time, and never trained on move 3 (though still preformed well)\n",
    "\n",
    "# try 16 with some low prob clipping\n",
    "# try with some both adam or both SGD\n",
    "\n",
    "# ALL ABOVE AGENTS HAD A DENSE LAYER OF 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 750000.0\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1054d9e10>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000\n",
      "Using         min_replay_buffer_size        : 128\n",
      "Using         num_minibatches               : 2\n",
      "Using default training_iterations           : 1\n",
      "NFSPDQNConfig\n",
      "Using         num_players                   : 2\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 750000.0\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1054d9e10>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000\n",
      "Using         min_replay_buffer_size        : 128\n",
      "Using         num_minibatches               : 2\n",
      "Using default training_iterations           : 1\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [64, 64, 64]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 750000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 750000.0\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x1054d9e10>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000\n",
      "Using         min_replay_buffer_size        : 128\n",
      "Using         num_minibatches               : 2\n",
      "Using default training_iterations           : 1\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [64, 64, 64]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 750000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1054d9e40>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 750000.0\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 128\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [64, 64, 64]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x1054d9e40>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 750000.0\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 128\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [64, 64, 64]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig, RainbowConfig\n",
    "from game_configs import MississippiMarblesConfig, LeducHoldemConfig, TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 7.5e5,  # like 2-5M in the paper (1M for initial test and see if rainbow is faster sooner)\n",
    "    \"num_players\": 2,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,\n",
    "    \"num_minibatches\": 2,  # 4 <- 16\n",
    "    \"learning_rate\": 0.1,  # 0.1 with SGD\n",
    "    # \"weight_decay\": 1e-9,\n",
    "    # \"clipnorm\": 1.0,\n",
    "    \"optimizer\": SGD,  # SGD\n",
    "    \"loss_function\": MSELoss(),\n",
    "    # \"per_epsilon\": 1e-3,\n",
    "    \"min_replay_buffer_size\": 128,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 200000,  # 50000\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],  # [(32, 32, 1)]\n",
    "    \"dense_layer_widths\": [64, 64, 64],  # [], [512]\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,  # 0.06\n",
    "    \"eg_epsilon\": 0.06,  # 0.1, 0 <- 16, 0.06\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",  # linear, inverse_sqrt\n",
    "    \"eg_epsilon_decay_final_step\": 500000,\n",
    "    \"sl_learning_rate\": 0.005,  # 0.005 with SGD <- 16, 0.0005 with Adam, 0.000125 with Adam,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,  # SGD <- 16\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 128,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,  # 250000\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [64, 64, 64],  # [128], [512]\n",
    "    \"sl_clip_low_prob\": 0.0,  # 0.0 <- 16\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "# config = NFSPConfig(config_dict=config_dict, game_config=MississippiMarblesConfig(), rl_config_type=RainbowConfig)\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig, RainbowConfig\n",
    "from game_configs import MississippiMarblesConfig, LeducHoldemConfig, TicTacToeConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# TODO: 8, 9, 10, 11, 12\n",
    "# DONE: 14, 13\n",
    "dir = \"./checkpoints/NFSP-TicTacToe-16\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "observation_dimensions:  (36,)\n",
      "num_actions:  4\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "observation_dimensions:  (36,)\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "int8\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "observation_dimensions:  (36,)\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "int8\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "observation_dimensions:  (36,)\n",
      "num_actions:  4\n",
      "(2000000, 36)\n",
      "int8\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "observation_dimensions:  (36,)\n",
      "num_actions:  4\n",
      "(2000000, 36)\n",
      "int8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.lz4_compress to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.lz4_compress` for environment variables or `env.get_wrapper_attr('lz4_compress')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "# env = gym.make('custom_gym_envs/MississippiMarbles-v0', render_mode=\"human\", players=2)\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=False)\n",
    "# env = FrameStack(env, 3)\n",
    "# env = gym.make('custom_gym_envs/TicTacToe-v0', render_mode=\"rgb_array\", player_turn_as_plane=False)\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./checkpoints/NFSP-LeducHoldem\"\n",
    "agent.load_from_checkpoint(dir, 44000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 1000\n",
    "agent.checkpoint_trials = 1000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_to_checkpoint(dir, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "b = np.array([[1, 0, 1, 0, 1], [0, 1, 0, 1, 0], [1, 2, 3, 4, 5]])\n",
    "print(np.sum(b == b.max(axis=1, keepdims=True), axis=1) > 1)\n",
    "for c in range(len(b)):\n",
    "    print(np.random.choice(np.where(b[c] == b[c].max())[0]))\n",
    "arr = np.apply_along_axis(lambda x: np.random.choice(np.where(x == x.max())[0]), 1, b)\n",
    "print(arr)\n",
    "arr = torch.stack(\n",
    "    [torch.tensor(np.random.choice(np.where(x == x.max())[0])) for x in b]\n",
    ")\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = agent.rl_agents[0].replay_buffer.action_buffer\n",
    "s = agent.rl_agents[0].replay_buffer.observation_buffer\n",
    "ns = agent.rl_agents[0].replay_buffer.next_observation_buffer\n",
    "d = agent.rl_agents[0].replay_buffer.done_buffer\n",
    "r = agent.rl_agents[0].replay_buffer.reward_buffer\n",
    "print(\"actions\", a)\n",
    "print(\"states\", s)\n",
    "print(\"next states\", ns)\n",
    "print(\"dones\", d)\n",
    "print(\"rewards\", r)\n",
    "\n",
    "samples = agent.rl_agents[0].replay_buffer.sample()\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = agent.sl_agents[0].replay_buffer.sample()\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "\n",
    "# TODO: 8, 9, 10, 11, 12\n",
    "# DONE: 14, 13\n",
    "dir = \"./checkpoints/NFSP-TicTacToe-32\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "\n",
    "env = gym.make(\n",
    "    \"custom_gym_envs/TicTacToe-v0\", render_mode=\"human\", encode_player_turn=True\n",
    ")\n",
    "# env = FrameStack(env, 3)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-TicTacToe\", device=\"cpu\")\n",
    "\n",
    "agent.load_from_checkpoint(\n",
    "    dir, 499999\n",
    ")  # test 104000 # 105000 # 87000 # 75000-83000 # 117000 # 118000 # 411000\n",
    "\n",
    "# 38000 for nfsp tictactoe 26 and 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import action_mask\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"custom_gym_envs/TicTacToe-v0\", render_mode=\"human\", encode_player_turn=True\n",
    ")\n",
    "# env = FrameStack(env, 3)\n",
    "state, info = env.reset()\n",
    "player = 0\n",
    "agent_player = 1\n",
    "done = False\n",
    "agent.policies = [\"average_strategy\", \"average_strategy\"]\n",
    "while not done:\n",
    "    print(f\"Player {player}\")\n",
    "    if player % 2 == agent_player:\n",
    "        prediction = agent.predict(state, info)\n",
    "        print(\n",
    "            \"Prediction\",\n",
    "            (\n",
    "                action_mask(\n",
    "                    (prediction * agent.rl_agents[0].support).sum(-1, keepdim=False),\n",
    "                    [info[\"legal_moves\"]],\n",
    "                    mask_value=float(\"-inf\"),\n",
    "                )\n",
    "                if agent.policies[agent_player] == \"best_response\"\n",
    "                else prediction\n",
    "            ),\n",
    "        )\n",
    "        action = agent.select_actions(prediction, info).item()\n",
    "    else:\n",
    "        action = int(input(\"Enter action: \"))\n",
    "        # prediction = agent.predict(state, info)\n",
    "        # print(\n",
    "        #     \"Prediction\",\n",
    "        #     (\n",
    "        #         action_mask(\n",
    "        #             (prediction * agent.rl_agents[0].support).sum(-1, keepdim=False),\n",
    "        #             [info[\"legal_moves\"]],\n",
    "        #             mask_value=float(\"-inf\"),\n",
    "        #         )\n",
    "        #         if agent.policies[agent_player] == \"best_response\"\n",
    "        #         else prediction\n",
    "        #     ),\n",
    "        # )\n",
    "        # action = agent.select_actions(prediction, info).item()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print(action)\n",
    "    player = (player + 1) % 2\n",
    "    env.render()\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rl_agents[0].replay_buffer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "probs = np.array([[[10, 20, 30], [40, 50, 60]], [[70, 80, 90], [100, 110, 120]]])\n",
    "dones = np.array([False, True])\n",
    "test[~dones] = probs[~dones]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "from utils import plot_comparisons\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "import gymnasium as gym\n",
    "import custom_gym_envs\n",
    "\n",
    "stats_list = []\n",
    "\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=False)\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-7\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 525000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=True)\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-8\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 250000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-9\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 250000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-10\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 275000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-11\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 525000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-12\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 250000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem-13\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 85000)\n",
    "stats_list.append(agent.stats)\n",
    "\n",
    "\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "agent.load_from_checkpoint(dir, 685000)\n",
    "stats_list.append(agent.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices:\n",
    "# 0: Default\n",
    "# 1: Default Shared\n",
    "# 2: PER + Shared\n",
    "# 3: Dueling + PER + Shared\n",
    "# 4: Distributional + Dueling + PER + Shared\n",
    "# 5: Distributional + Dueling + PER + Shared + LR 0.05\n",
    "plot_comparisons(stats_list, \"NFSPDQN-LeducHoldem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_configs import NFSPDQNConfig\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "import gymnasium as gym\n",
    "import custom_gym_envs\n",
    "from pathlib import Path\n",
    "\n",
    "# the test agent\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=True)\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "test_agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "test_agent.load_from_checkpoint(dir, 750000)\n",
    "\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=True)\n",
    "dir = \"./checkpoints/NFSPDQN-LeducHoldem\"\n",
    "config = NFSPDQNConfig.load(Path(dir, \"configs/config.yaml\"))\n",
    "challenger_agent = NFSPDQN(env, config, name=\"NFSPDQN-LeducHoldem\", device=\"cpu\")\n",
    "challenger_agent.load_from_checkpoint(dir, 750000)\n",
    "\n",
    "test_agent.policies = [\"average_strategy\", \"average_strategy\"]\n",
    "# the challenger agent\n",
    "challenger_agent.policies = [\"best_response\", \"best_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "test_player = 0\n",
    "score = 0\n",
    "test_score = 0\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=True)\n",
    "for _ in range(5000):\n",
    "    print(\"Trial \", _)\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        for player in range(2):\n",
    "            if player == 0:\n",
    "                prediction = test_agent.predict(state, info)\n",
    "                action = test_agent.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = challenger_agent.predict(state, info)\n",
    "                action = challenger_agent.select_actions(prediction, info).item()\n",
    "            print(\"Prediction\", prediction)\n",
    "            action_string = (\n",
    "                \"call\"\n",
    "                if action == 0\n",
    "                else (\"raise\" if action == 1 else \"fold\" if action == 2 else \"check\")\n",
    "            )\n",
    "            print(action_string)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            average_strategy_reward = reward[test_player]\n",
    "            total_reward = sum(reward)\n",
    "            test_score += total_reward - average_strategy_reward\n",
    "            if done:\n",
    "                break\n",
    "score = test_score / 5000  #\n",
    "\n",
    "\n",
    "test_player = 1\n",
    "test_score = 0\n",
    "\n",
    "env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=True)\n",
    "for _ in range(5000):\n",
    "    print(\"Trial \", _)\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        for player in range(2):\n",
    "            if player == 0:\n",
    "                prediction = challenger_agent.predict(state, info)\n",
    "                action = challenger_agent.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = test_agent.predict(state, info)\n",
    "                action = test_agent.select_actions(prediction, info).item()\n",
    "            print(\"Prediction\", prediction)\n",
    "            action_string = (\n",
    "                \"call\"\n",
    "                if action == 0\n",
    "                else (\"raise\" if action == 1 else \"fold\" if action == 2 else \"check\")\n",
    "            )\n",
    "            print(action_string)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            average_strategy_reward = reward[test_player]\n",
    "            total_reward = sum(reward)\n",
    "            test_score += total_reward - average_strategy_reward\n",
    "            if done:\n",
    "                break\n",
    "score += test_score / 5000  #\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = agent.nfsp_agents[0]\n",
    "player_2 = agent.nfsp_agents[1]\n",
    "player_1.policy = \"best_response\"\n",
    "state, info = env.reset()\n",
    "print(state)\n",
    "prediction = player_1.predict(state, info)\n",
    "action = player_1.select_actions(prediction, info).item()\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "print(action)\n",
    "print(state)\n",
    "\n",
    "\n",
    "state, reward, terminated, truncated, info = env.step(4)\n",
    "print(state)\n",
    "\n",
    "\n",
    "prediction = player_1.predict(state, info)\n",
    "action = player_1.select_actions(prediction, info).item()\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "print(action)\n",
    "print(state)\n",
    "\n",
    "\n",
    "state, reward, terminated, truncated, info = env.step(7)\n",
    "print(state)\n",
    "\n",
    "\n",
    "prediction = player_1.predict(state, info)\n",
    "action = player_1.select_actions(prediction, info).item()\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "print(action)\n",
    "print(state)\n",
    "\n",
    "\n",
    "state, reward, terminated, truncated, info = env.step(3)\n",
    "print(state)\n",
    "print(terminated)\n",
    "\n",
    "\n",
    "prediction = player_1.predict(state, info)\n",
    "action = player_1.select_actions(prediction, info).item()\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "print(action)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "print(state)\n",
    "state_2, reward, terminated, truncated, info = env.step(0)\n",
    "print(state_2)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = agent.nfsp_agents[0].rl_agent.replay_buffer.sample()\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "q_values = torch.tensor(\n",
    "    [\n",
    "        [1, 0, 0, 0.5, -1],\n",
    "        [-1, 1, 1, 1, -1],\n",
    "    ]\n",
    ")\n",
    "legal_moves = [[0, 1, 3, 4], [2, 3, 4]]\n",
    "mask = torch.zeros_like(q_values, dtype=torch.int8)\n",
    "for i, legal in enumerate(legal_moves):\n",
    "    mask[i, legal] = 1\n",
    "print(mask)\n",
    "q_values[mask == 0] = float(\"-inf\")\n",
    "selected_actions = q_values.argmax(1, keepdim=False)\n",
    "print(q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
