{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "## Replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        (\n",
    "            self.ptr,\n",
    "            self.size,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        print(idxs)\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "\n",
    "## Network\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "## Double DQN Agent\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "\n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        epsilon (float): parameter for epsilon greedy policy\n",
    "        epsilon_decay (float): step size to decrease epsilon\n",
    "        max_epsilon (float): max value of epsilon\n",
    "        min_epsilon (float): min value of epsilon\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including\n",
    "                           state, action, reward, next_state, done\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        epsilon_decay: float,\n",
    "        seed: int,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.0,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            epsilon_decay (float): step size to decrease epsilon\n",
    "            lr (float): learning rate\n",
    "            max_epsilon (float): max value of epsilon\n",
    "            min_epsilon (float): min value of epsilon\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = max_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.seed = seed\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # epsilon greedy policy\n",
    "        rand = np.random.rand()\n",
    "        print(rand)\n",
    "        # if rand < epsilon:\n",
    "        #     print(\"selecting a random move\")\n",
    "        #     if \"legal_moves\" in info:\n",
    "        #         # print(\"using legal moves\")\n",
    "        #         return random.choice(info[\"legal_moves\"])\n",
    "        #     else:\n",
    "        #         q_values = q_values.reshape(-1)\n",
    "        #         return random.choice(len(q_values))\n",
    "        # else:\n",
    "        #     # try:\n",
    "        #     # print(\"using provided wrapper to select action\")\n",
    "        #     return wrapper(q_values, info)\n",
    "        print(self.epsilon)\n",
    "        if self.epsilon > rand:\n",
    "            print(\"random action!\")\n",
    "            selected_action = random.choice(range(self.env.action_space.n))\n",
    "        else:\n",
    "            selected_action = self.dqn(\n",
    "                torch.FloatTensor(state).to(self.device)\n",
    "            ).argmax()\n",
    "            # selected_action = torch.stack(\n",
    "            #     [\n",
    "            #         torch.tensor(\n",
    "            #             np.random.choice(np.where(x.cpu() == x.cpu().max())[0])\n",
    "            #         )\n",
    "            #         for x in self.dqn(torch.FloatTensor(state).to(self.device))\n",
    "            #     ]\n",
    "            # )\n",
    "\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "\n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        samples = self.memory.sample_batch()\n",
    "\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(0, num_frames):\n",
    "            print(\"training step\", frame_idx)\n",
    "            action = self.select_action(state)\n",
    "            print(\"action\", action)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state, _ = self.env.reset(seed=self.seed)\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            self.epsilon = max(\n",
    "                self.min_epsilon,\n",
    "                self.epsilon\n",
    "                - (self.max_epsilon - self.min_epsilon) * self.epsilon_decay,\n",
    "            )\n",
    "            epsilons.append(self.epsilon)\n",
    "            if frame_idx % self.target_update == 0:\n",
    "                self._target_hard_update()\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "\n",
    "                # linearly decrease epsilon\n",
    "\n",
    "                # if hard update is needed\n",
    "\n",
    "            # plotting\n",
    "        #     if frame_idx % plotting_interval == 0:\n",
    "        #         self._plot(frame_idx, scores, losses, epsilons)\n",
    "        # self._plot(frame_idx, scores, losses, epsilons)\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, video_folder: str) -> None:\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "\n",
    "        # for recording a video\n",
    "        naive_env = self.env\n",
    "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)\n",
    "\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        # reset\n",
    "        self.env = naive_env\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Return dqn loss.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "        print(action)\n",
    "        print(state)\n",
    "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
    "        #       = r                       otherwise\n",
    "        # for param in self.dqn.parameters():\n",
    "        #     print(param)\n",
    "        curr_q_value = self.dqn(state).gather(1, action)\n",
    "        print(self.dqn(state))\n",
    "        print(curr_q_value)\n",
    "        print(self.dqn(next_state))\n",
    "        next_q_value = (\n",
    "            self.dqn_target(next_state)\n",
    "            .gather(1, self.dqn(next_state).argmax(dim=1, keepdim=True))  # Double DQN\n",
    "            .detach()\n",
    "        )\n",
    "        # print(next_q_value)\n",
    "        mask = 1 - done\n",
    "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
    "        # print(target)\n",
    "        # calculate dqn loss\n",
    "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
    "        print(loss)\n",
    "        return loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _plot(\n",
    "        self,\n",
    "        frame_idx: int,\n",
    "        scores: List[float],\n",
    "        losses: List[float],\n",
    "        epsilons: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"frame %s. score: %s\" % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"loss\")\n",
    "        plt.plot(losses)\n",
    "        # plt.subplot(133)\n",
    "        # plt.title(\"epsilons\")\n",
    "        # plt.plot(epsilons)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "## Environment\n",
    "# environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "## Set random seed\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 777\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "seed_torch(seed)\n",
    "## Initialize\n",
    "\n",
    "# parameters\n",
    "num_frames = 10000\n",
    "memory_size = 1000\n",
    "batch_size = 32\n",
    "target_update = 200\n",
    "epsilon_decay = 1 / 2000\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay, seed)\n",
    "\n",
    "# for i in range(75):\n",
    "#   print(i, random.choice(range(2)))\n",
    "\n",
    "## Train\n",
    "\n",
    "agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 777\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "seed_torch(seed)\n",
    "for i in range(4000):\n",
    "    print(i, np.random.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 777\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from math import e\n",
    "from time import time\n",
    "from pkg_resources import get_distribution\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.optim.adam import Adam\n",
    "import numpy as np\n",
    "from agent_configs import RainbowConfig\n",
    "from utils import (\n",
    "    update_per_beta,\n",
    "    get_legal_moves,\n",
    "    current_timestamp,\n",
    "    action_mask,\n",
    "    epsilon_greedy_policy,\n",
    "    CategoricalCrossentropyLoss,\n",
    "    HuberLoss,\n",
    "    KLDivergenceLoss,\n",
    "    MSELoss,\n",
    "    update_inverse_sqrt_schedule,\n",
    "    update_linear_schedule,\n",
    ")\n",
    "\n",
    "import sys\n",
    "\n",
    "from utils.utils import epsilon_greedy_policy\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from base_agent.agent import BaseAgent\n",
    "from replay_buffers.prioritized_n_step_replay_buffer import PrioritizedNStepReplayBuffer\n",
    "from dqn.rainbow.rainbow_network import RainbowNetwork\n",
    "\n",
    "\n",
    "class RainbowAgent(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        config: RainbowConfig,\n",
    "        name=f\"rainbow_{current_timestamp():.1f}\",\n",
    "        device: torch.device = (\n",
    "            torch.device(\"cuda\")\n",
    "            if torch.cuda.is_available()\n",
    "            # MPS is sometimes useful for M2 instances, but only for large models/matrix multiplications otherwise CPU is faster\n",
    "            else (\n",
    "                torch.device(\"mps\")\n",
    "                if torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "                else torch.device(\"cpu\")\n",
    "            )\n",
    "        ),\n",
    "        num_players: int = 1,\n",
    "    ):\n",
    "        super(RainbowAgent, self).__init__(env, config, name, device=device)\n",
    "        self.model = RainbowNetwork(\n",
    "            config=config,\n",
    "            output_size=self.num_actions,\n",
    "            input_shape=(self.config.minibatch_size,) + self.observation_dimensions,\n",
    "        )\n",
    "        self.target_model = RainbowNetwork(\n",
    "            config=config,\n",
    "            output_size=self.num_actions,\n",
    "            input_shape=(self.config.minibatch_size,) + self.observation_dimensions,\n",
    "        )\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.target_model.to(device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.optimizer: torch.optim.Optimizer = self.config.optimizer(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            eps=self.config.adam_epsilon,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "        )\n",
    "\n",
    "        self.replay_buffer = PrioritizedNStepReplayBuffer(\n",
    "            observation_dimensions=self.observation_dimensions,\n",
    "            observation_dtype=self.env.observation_space.dtype,\n",
    "            max_size=self.config.replay_buffer_size,\n",
    "            batch_size=self.config.minibatch_size,\n",
    "            max_priority=1.0,\n",
    "            alpha=self.config.per_alpha,\n",
    "            beta=self.config.per_beta,\n",
    "            # epsilon=config[\"per_epsilon\"],\n",
    "            n_step=self.config.n_step,\n",
    "            gamma=self.config.discount_factor,\n",
    "            compressed_observations=(\n",
    "                self.env.lz4_compress if hasattr(self.env, \"lz4_compress\") else False\n",
    "            ),\n",
    "            num_players=num_players,\n",
    "        )\n",
    "\n",
    "        self.eg_epsilon = self.config.eg_epsilon\n",
    "\n",
    "        self.stats = {\n",
    "            \"score\": [],\n",
    "            \"loss\": [],\n",
    "            \"test_score\": [],\n",
    "        }\n",
    "        self.targets = {\n",
    "            # \"score\": self.env.spec.reward_threshold,\n",
    "            # \"test_score\": self.env.spec.reward_threshold,\n",
    "        }\n",
    "\n",
    "    def predict(self, states) -> torch.Tensor:\n",
    "        # could change type later\n",
    "        state_input = self.preprocess(states)\n",
    "        q_distribution: torch.Tensor = self.model(state_input)\n",
    "        return q_distribution\n",
    "\n",
    "    def predict_target(self, states) -> torch.Tensor:\n",
    "        # could change type later\n",
    "        state_input = self.preprocess(states)\n",
    "        q_distribution: torch.Tensor = self.target_model(state_input)\n",
    "        return q_distribution\n",
    "\n",
    "    def select_actions(\n",
    "        self, distribution, info: dict = None, mask_actions: bool = False\n",
    "    ):\n",
    "        assert info is not None if mask_actions else True, \"Need info to mask actions\"\n",
    "        # print(info)\n",
    "        if self.config.atom_size > 1:\n",
    "            q_values = distribution * self.support\n",
    "            q_values = q_values.sum(2, keepdim=False)\n",
    "        else:\n",
    "            q_values = distribution\n",
    "        if mask_actions:\n",
    "            legal_moves = get_legal_moves(info)\n",
    "            q_values = action_mask(\n",
    "                q_values, legal_moves, mask_value=-float(\"inf\"), device=self.device\n",
    "            )\n",
    "        # print(\"Q Values\", q_values)\n",
    "        # q_values with argmax ties\n",
    "        selected_actions = torch.stack(\n",
    "            [\n",
    "                torch.tensor(np.random.choice(np.where(x.cpu() == x.cpu().max())[0]))\n",
    "                for x in q_values\n",
    "            ]\n",
    "        )\n",
    "        # print(selected_actions)\n",
    "        # selected_actions = q_values.argmax(1, keepdim=False)\n",
    "        return selected_actions\n",
    "\n",
    "    def learn(self) -> np.ndarray:\n",
    "        samples = self.replay_buffer.sample()\n",
    "        loss = self.learn_from_sample(samples)\n",
    "        return loss\n",
    "\n",
    "    def learn_from_sample(self, samples: dict):\n",
    "        observations, weights, actions = (\n",
    "            samples[\"observations\"],\n",
    "            samples[\"weights\"],\n",
    "            torch.from_numpy(samples[\"actions\"]).to(self.device).long(),\n",
    "        )\n",
    "        print(\"actions\", actions)\n",
    "\n",
    "        print(\"Observations\", observations)\n",
    "        online_predictions = self.predict(observations)[\n",
    "            range(self.config.minibatch_size), actions\n",
    "        ]\n",
    "        # for param in self.model.parameters():\n",
    "        #     print(param)\n",
    "        print(self.predict(observations))\n",
    "        print(online_predictions)\n",
    "        # (B, atom_size)\n",
    "        # print(\"using default dqn loss\")\n",
    "        next_observations, rewards, dones = (\n",
    "            torch.from_numpy(samples[\"next_observations\"]).to(self.device),\n",
    "            torch.from_numpy(samples[\"rewards\"]).to(self.device),\n",
    "            torch.from_numpy(samples[\"dones\"]).to(self.device),\n",
    "        )\n",
    "        target_predictions = self.predict_target(next_observations)  # next q values\n",
    "        # print(\"Next q values\", target_predictions)\n",
    "        # print(\"Current q values\", online_predictions)\n",
    "        print(self.predict(next_observations))\n",
    "        next_actions = self.select_actions(\n",
    "            self.predict(next_observations),  # current q values\n",
    "        )\n",
    "        print(\"Next actions\", next_actions)\n",
    "        target_predictions = target_predictions[\n",
    "            range(self.config.minibatch_size), next_actions\n",
    "        ]  # this might not work\n",
    "        print(target_predictions)\n",
    "        target_predictions = (\n",
    "            rewards + self.config.discount_factor * (~dones) * target_predictions\n",
    "        )\n",
    "        print(target_predictions)\n",
    "        # print(\"predicted\", online_distributions)\n",
    "        # print(\"target\", target_distributions)\n",
    "\n",
    "        loss = self.config.loss_function(online_predictions, target_predictions)\n",
    "        print(\"Loss\", loss.mean())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().to(\"cpu\").mean().item()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def update_eg_epsilon(self, training_step: int):\n",
    "        # print(\"decaying eg epsilon linearly\")\n",
    "        self.eg_epsilon = update_linear_schedule(\n",
    "            self.config.eg_epsilon_final,\n",
    "            self.config.eg_epsilon_final_step,\n",
    "            self.config.eg_epsilon,\n",
    "            training_step,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        start_time = time()\n",
    "        score = 0\n",
    "        target_model_updated = (False, False)  # (score, loss)\n",
    "\n",
    "        # self.fill_replay_buffer()\n",
    "        state, info = self.env.reset(seed=777)\n",
    "\n",
    "        # self.training_steps += self.start_training_step\n",
    "        for training_step in range(self.start_training_step, self.training_steps):\n",
    "            print(\"training step\", training_step)\n",
    "            with torch.no_grad():\n",
    "                values = self.predict(state)\n",
    "                # print(values)\n",
    "                action = epsilon_greedy_policy(\n",
    "                    values,\n",
    "                    info,\n",
    "                    self.eg_epsilon,\n",
    "                    wrapper=lambda values, info: self.select_actions(values).item(),\n",
    "                )\n",
    "                print(\"Action\", action)\n",
    "                self.update_eg_epsilon(training_step + 1)\n",
    "                next_state, reward, terminated, truncated, next_info = self.env.step(\n",
    "                    action\n",
    "                )\n",
    "                done = terminated or truncated\n",
    "                print(\"State\", state)\n",
    "                self.replay_buffer.store(\n",
    "                    state, info, action, reward, next_state, next_info, done\n",
    "                )\n",
    "                state = next_state\n",
    "                info = next_info\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    state, info = self.env.reset(seed=777)\n",
    "                    score_dict = {\n",
    "                        \"score\": score,\n",
    "                        \"target_model_updated\": target_model_updated[0],\n",
    "                    }\n",
    "                    self.stats[\"score\"].append(score_dict)\n",
    "                    target_model_updated = (False, target_model_updated[1])\n",
    "                    score = 0\n",
    "\n",
    "                if training_step % self.config.transfer_interval == 0:\n",
    "                    target_model_updated = (True, True)\n",
    "                    # stats[\"test_score\"].append(\n",
    "                    #     {\"target_model_weight_update\": training_step}\n",
    "                    # )\n",
    "                    self.update_target_model()\n",
    "\n",
    "            print(\"replay buffer size\", len(self.replay_buffer))\n",
    "            if len(self.replay_buffer) >= self.config.min_replay_buffer_size:\n",
    "                loss = self.learn()\n",
    "                # print(losses)\n",
    "                # could do things other than taking the mean here\n",
    "                self.stats[\"loss\"].append(\n",
    "                    {\"loss\": loss, \"target_model_updated\": target_model_updated[1]}\n",
    "                )\n",
    "                target_model_updated = (target_model_updated[0], False)\n",
    "\n",
    "            if (\n",
    "                training_step % self.checkpoint_interval == 0\n",
    "                and training_step > self.start_training_step\n",
    "            ):\n",
    "                # print(self.stats[\"score\"])\n",
    "                # print(len(self.replay_buffer))\n",
    "                self.save_checkpoint(\n",
    "                    training_step,\n",
    "                    training_step * self.config.replay_interval,\n",
    "                    time() - start_time,\n",
    "                )\n",
    "\n",
    "        self.save_checkpoint(\n",
    "            training_step,\n",
    "            training_step * self.config.replay_interval,\n",
    "            time() - start_time,\n",
    "        )\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from utils.utils import HuberLoss\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [128, 128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advatage_hidden_layer_widths\": [],\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 10000,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"per_alpha\": 0,\n",
    "    \"per_beta\": 0,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 1000,\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"transfer_interval\": 200,\n",
    "    \"n_step\": 1,\n",
    "    \"loss_function\": HuberLoss(),  # could do categorical cross entropy\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"atom_size\": 1,\n",
    "    \"replay_interval\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 1.0,\n",
    "    \"eg_epsilon_final\": 0.0,\n",
    "    \"eg_epsilon_final_step\": 2000,\n",
    "    \"eg_epsilon_decay_type\": \"linear\",\n",
    "}\n",
    "game_config = CartPoleConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_CartPole-v1-1\", device=device)\n",
    "agent.checkpoint_interval = 200\n",
    "\n",
    "for param in agent.model.parameters():\n",
    "    print(param)\n",
    "\n",
    "print(\"start\")\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
