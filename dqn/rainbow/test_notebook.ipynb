{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 777\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss, HuberLoss\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "config_dict = {\n",
    "    \"dense_layer_widths\": [128, 128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advatage_hidden_layer_widths\": [],\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_steps\": 10000,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"per_alpha\": 0,\n",
    "    \"per_beta\": 0,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 1000,\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"transfer_interval\": 200,\n",
    "    \"n_step\": 1,\n",
    "    \"loss_function\": HuberLoss(),  # could do categorical cross entropy\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"atom_size\": 1,\n",
    "    \"replay_interval\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 1.0,\n",
    "    \"eg_epsilon_final\": 0.0,\n",
    "    \"eg_epsilon_final_step\": 2000,\n",
    "    \"eg_epsilon_decay_type\": \"linear\",\n",
    "}\n",
    "\n",
    "config_dict = {\n",
    "    \"training_steps\": 10000,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"loss_function\": KLDivergenceLoss(),  # could do categorical cross entropy\n",
    "    \"discount_factor\": 0.99,\n",
    "}\n",
    "\n",
    "game_config = CartPoleConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_CartPole-v1\", device=device)\n",
    "agent.checkpoint_interval = 200\n",
    "\n",
    "for param in agent.model.parameters():\n",
    "    print(param)\n",
    "\n",
    "print(\"start\")\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_space():\n",
    "    search_space = {\n",
    "        \"activation\": hp.choice(\n",
    "            \"activation\",\n",
    "            [\n",
    "                \"linear\",\n",
    "                \"relu\",\n",
    "                # 'relu6',\n",
    "                \"sigmoid\",\n",
    "                \"softplus\",\n",
    "                \"soft_sign\",\n",
    "                \"silu\",\n",
    "                \"swish\",\n",
    "                \"log_sigmoid\",\n",
    "                \"hard_sigmoid\",\n",
    "                # 'hard_silu',\n",
    "                # 'hard_swish',\n",
    "                # 'hard_tanh',\n",
    "                \"elu\",\n",
    "                # 'celu',\n",
    "                \"selu\",\n",
    "                \"gelu\",\n",
    "                # 'glu'\n",
    "            ],\n",
    "        ),\n",
    "        \"kernel_initializer\": hp.choice(\n",
    "            \"kernel_initializer\",\n",
    "            [\n",
    "                \"he_uniform\",\n",
    "                \"he_normal\",\n",
    "                \"glorot_uniform\",\n",
    "                \"glorot_normal\",\n",
    "                \"lecun_uniform\",\n",
    "                \"lecun_normal\",\n",
    "                \"orthogonal\",\n",
    "                \"variance_baseline\",\n",
    "                \"variance_0.1\",\n",
    "                \"variance_0.3\",\n",
    "                \"variance_0.8\",\n",
    "                \"variance_3\",\n",
    "                \"variance_5\",\n",
    "                \"variance_10\",\n",
    "            ],\n",
    "        ),\n",
    "        \"optimizer\": hp.choice(\n",
    "            \"optimizer\", [tf.keras.optimizers.legacy.Adam]\n",
    "        ),  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "        \"learning_rate\": hp.choice(\n",
    "            \"learning_rate\", [10, 5, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "        ),  #\n",
    "        \"adam_epsilon\": hp.choice(\n",
    "            \"adam_epsilon\",\n",
    "            [1, 0.5, 0.3125, 0.03125, 0.003125, 0.0003125, 0.00003125, 0.000003125],\n",
    "        ),\n",
    "        \"clipnorm\": hp.choice(\"clipnorm\", [None]),\n",
    "        # NORMALIZATION?\n",
    "        \"soft_update\": hp.choice(\n",
    "            \"soft_update\", [False]\n",
    "        ),  # seems to always be false, we can try it with tru\n",
    "        \"ema_beta\": hp.uniform(\"ema_beta\", 0.95, 0.999),\n",
    "        \"transfer_interval\": hp.choice(\n",
    "            \"transfer_interval\", [10, 25, 50, 100, 200, 400, 800, 1600, 2000]\n",
    "        ),\n",
    "        \"replay_interval\": hp.choice(\n",
    "            \"replay_interval\", [1, 2, 3, 4, 5, 8, 10, 12, 350]\n",
    "        ),\n",
    "        \"minibatch_size\": hp.choice(\n",
    "            \"minibatch_size\", [2**i for i in range(0, 8)]\n",
    "        ),  ###########\n",
    "        \"replay_buffer_size\": hp.choice(\n",
    "            \"replay_buffer_size\",\n",
    "            [2000, 3000, 5000, 7500, 10000, 15000, 20000, 25000, 50000],\n",
    "        ),  #############\n",
    "        \"min_replay_buffer_size\": hp.choice(\n",
    "            \"min_replay_buffer_size\",\n",
    "            [0, 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000],\n",
    "        ),  # 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000\n",
    "        \"n_step\": hp.choice(\"n_step\", [1, 2, 3, 4, 5, 8, 10]),\n",
    "        \"discount_factor\": hp.choice(\n",
    "            \"discount_factor\", [0.1, 0.5, 0.9, 0.99, 0.995, 0.999]\n",
    "        ),\n",
    "        \"atom_size\": hp.choice(\"atom_size\", [11, 21, 31, 41, 51, 61, 71, 81]),  #\n",
    "        \"conv_layers\": hp.choice(\n",
    "            \"conv_layers\", [[], [(32, 8, 4), (64, 4, 2), (64, 3, 1)]]\n",
    "        ),\n",
    "        \"conv_layers_noisy\": hp.choice(\"conv_layers_noisy\", [False]),\n",
    "        \"width\": hp.choice(\"width\", [32, 64, 128, 256, 512, 1024]),\n",
    "        \"dense_layers\": hp.choice(\"dense_layers\", [0, 1, 2, 3, 4]),\n",
    "        \"dense_layers_noisy\": hp.choice(\n",
    "            \"dense_layers_noisy\", [True]\n",
    "        ),  # i think this is always true for rainbow\n",
    "        # REWARD CLIPPING\n",
    "        \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.5]),  #\n",
    "        \"loss_function\": hp.choice(\n",
    "            \"loss_function\",\n",
    "            [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()],\n",
    "        ),\n",
    "        \"dueling\": hp.choice(\"dueling\", [True]),\n",
    "        \"advantage_hidden_layers\": hp.choice(\n",
    "            \"advantage_hidden_layers\", [0, 1, 2, 3, 4]\n",
    "        ),  #\n",
    "        \"value_hidden_layers\": hp.choice(\"value_hidden_layers\", [0, 1, 2, 3, 4]),  #\n",
    "        \"training_steps\": hp.choice(\"training_steps\", [30000]),\n",
    "        \"per_epsilon\": hp.choice(\n",
    "            \"per_epsilon\", [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "        ),\n",
    "        \"per_alpha\": hp.choice(\"per_alpha\", [0.05 * i for i in range(0, 21)]),\n",
    "        \"per_beta\": hp.choice(\"per_beta\", [0.05 * i for i in range(1, 21)]),\n",
    "        # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        # 'search_max_depth': 5,\n",
    "        # 'search_max_time': 10,\n",
    "        \"training_iterations\": hp.choice(\"training_iterations\", [1, 2, 3, 4, 5]),\n",
    "        \"num_minibatches\": hp.choice(\"num_minibatches\", [1, 2, 3, 4, 5]),\n",
    "    }\n",
    "    initial_best_config = [\n",
    "        {\n",
    "            \"activation\": 1,\n",
    "            \"kernel_initializer\": 6,\n",
    "            \"optimizer\": 0,  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "            \"learning_rate\": 5,  #\n",
    "            \"adam_epsilon\": 5,\n",
    "            \"clipnorm\": 0,\n",
    "            # NORMALIZATION?\n",
    "            \"soft_update\": 0,  # seems to always be false, we can try it with tru\n",
    "            \"ema_beta\": 0.95,\n",
    "            \"transfer_interval\": 3,\n",
    "            \"replay_interval\": 1,\n",
    "            \"minibatch_size\": 7,\n",
    "            \"replay_buffer_size\": 8,\n",
    "            \"min_replay_buffer_size\": 4,\n",
    "            \"n_step\": 2,\n",
    "            \"discount_factor\": 3,\n",
    "            \"atom_size\": 4,  #\n",
    "            \"conv_layers\": 0,\n",
    "            \"conv_layers_noisy\": 0,\n",
    "            \"width\": 2,\n",
    "            \"dense_layers\": 2,\n",
    "            \"dense_layers_noisy\": 0,  # i think this is always true for rainbow\n",
    "            # REWARD CLIPPING\n",
    "            \"noisy_sigma\": 0,  #\n",
    "            \"loss_function\": 0,\n",
    "            \"dueling\": 0,\n",
    "            \"advantage_hidden_layers\": 1,  #\n",
    "            \"value_hidden_layers\": 1,  #\n",
    "            \"training_steps\": 0,\n",
    "            \"per_epsilon\": 3,\n",
    "            \"per_alpha\": 10,\n",
    "            \"per_beta\": 7,\n",
    "            # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "            # 'search_max_depth': 5,\n",
    "            # 'search_max_time': 10,\n",
    "            \"training_iterations\": 1,\n",
    "            \"num_minibatches\": 1,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return search_space, initial_best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import space_eval\n",
    "\n",
    "search_sapce, initial_best_config = create_search_space()\n",
    "config = space_eval(search_sapce, initial_best_config[0])\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "\n",
    "config = RainbowConfig(config, CartPoleConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.checkpoint_interval = 10\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.load_from_checkpoint(\"./checkpoints/RainbowDQN-CartPole-v1\", 100)\n",
    "agent.checkpoint_interval = 10\n",
    "# print(agent.stats)\n",
    "# print(agent.config)\n",
    "# print(agent.replay_buffer.sample())\n",
    "# print(agent.replay_buffer.beta)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_anytrading\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make(\"forex-v0\")\n",
    "# env = gym.make('stocks-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "custom_env = gym.make(\n",
    "    \"forex-v0\",\n",
    "    df=FOREX_EURUSD_1H_ASK,\n",
    "    window_size=10,\n",
    "    frame_bound=(10, 300),\n",
    "    unit_side=\"right\",\n",
    ")\n",
    "\n",
    "# custom_env = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=STOCKS_GOOGL,\n",
    "#     window_size=10,\n",
    "#     frame_bound=(10, 300)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env.unwrapped.shape)\n",
    "print(\"> df.shape:\", env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env.unwrapped.max_possible_profit())\n",
    "\n",
    "print()\n",
    "print(\"custom_env information:\")\n",
    "print(\"> shape:\", custom_env.unwrapped.shape)\n",
    "print(\"> df.shape:\", custom_env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", custom_env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", custom_env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", custom_env.unwrapped.max_possible_profit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "env = custom_env\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from modules.utils import CategoricalCrossentropyLoss, KLDivergenceLoss, HuberLoss\n",
    "\n",
    "config_dict = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"loss_function\": KLDivergenceLoss,\n",
    "    \"learning_rate\": 0.000001,\n",
    "}\n",
    "config = RainbowConfig(config_dict, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import AtariConfig\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "\n",
    "config_dict = {\n",
    "    \"conv_layers\": [\n",
    "        (32, 8, 4),\n",
    "        (64, 4, 2),\n",
    "        (64, 3, 1),\n",
    "    ],\n",
    "    \"dense_layers_widths\": [512],\n",
    "    \"value_hidden_layers_widths\": [],  #\n",
    "    \"advatage_hidden_layers_widths\": [],  #\n",
    "    \"adam_epsilon\": 1.5e-4,\n",
    "    \"learning_rate\": 0.00025 / 4,\n",
    "    \"training_steps\": 500000,  # 50000000 Agent saw 200,000,000 frames\n",
    "    \"per_epsilon\": 1e-6,  #\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.4,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 750000,  # 1000000\n",
    "    \"min_replay_buffer_size\": 80000,\n",
    "    \"transfer_interval\": 32000,\n",
    "    \"n_step\": 3,\n",
    "    \"kernel_initializer\": \"orthogonal\",  #\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,  #\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 4,\n",
    "}\n",
    "game_config = AtariConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "\n",
    "\n",
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\", max_episode_steps=108000\n",
    ")\n",
    "env = AtariPreprocessing(\n",
    "    env\n",
    ")  # terminal_on_life_loss=True we will need to change the resetting to check if lives is 0 instead of just checking if done\n",
    "env = FrameStack(env, 4)\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_Atari_MsPacmanNoFrameskip-v4\")\n",
    "agent.checkpoint_interval = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
