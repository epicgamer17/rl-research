{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import prepare_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_plots = list(map(list, zip(*[1, 2, 3, 4])))\n",
    "print(len(score_plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.empty((1, 1), dtype=np.int32)\n",
    "print(arr)  \n",
    "arr = np.append(arr, [[1, 1, 1],[1, 1, 1],[1, 1, 1]], axis=0)\n",
    "print(arr)\n",
    "arr = np.append(arr, [[1, 1, 1],[1, 1, 1],[1, 1, 1]], axis=0)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "import random\n",
    "env = gym.make('custom_gym_envs/MississippiMarbles-v0', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = random.choice(info['legal_moves'])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym_envs\n",
    "# import gymnasium as gym\n",
    "# env = gym.make('gym_envs/TicTacToe-v0')\n",
    "\n",
    "# state, info = env.reset()\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(0)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(4)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(3)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(6)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(2)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(1)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# env.render()\n",
    "# state, reward, terminated, truncated, info = env.step(7)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(8)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(5)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# print(\"Truncated:\", truncated)\n",
    "# env.render()\n",
    "\n",
    "\n",
    "# env.reset()\n",
    "# state, reward, terminated, truncated, info = env.step(0)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(3)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(7)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(4)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(2)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(6)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# state, reward, terminated, truncated, info = env.step(1)\n",
    "# print(state)\n",
    "# print(\"Turn: \", state[2][0][0])\n",
    "# print(\"Legal moves: \", info['legal_moves'])\n",
    "# print(\"Terminated:\", terminated)\n",
    "# print(\"Truncated:\", truncated)\n",
    "# print(\"Reward:\", reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeZeroToOne(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_high = self.env.observation_space.high\n",
    "        self.observation_low = self.env.observation_space.low\n",
    "\n",
    "    def observation(self, obs):\n",
    "        print(obs)\n",
    "        print((obs - self.observation_low) / (self.observation_high - self.observation_low))\n",
    "        return (obs - self.observation_low) / (self.observation_high - self.observation_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online with frame stackign though\n",
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online\n",
    "env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 00:40:44.933808: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-05-23 00:40:44.933833: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-05-23 00:40:44.933840: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-05-23 00:40:44.933878: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-23 00:40:44.933899: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_space():\n",
    "    search_space = {\n",
    "        \"activation\": hp.choice(\n",
    "            \"activation\",\n",
    "            [\n",
    "                \"linear\",\n",
    "                \"relu\",\n",
    "                # 'relu6',\n",
    "                \"sigmoid\",\n",
    "                \"softplus\",\n",
    "                \"soft_sign\",\n",
    "                \"silu\",\n",
    "                \"swish\",\n",
    "                \"log_sigmoid\",\n",
    "                \"hard_sigmoid\",\n",
    "                # 'hard_silu',\n",
    "                # 'hard_swish',\n",
    "                # 'hard_tanh',\n",
    "                \"elu\",\n",
    "                # 'celu',\n",
    "                \"selu\",\n",
    "                \"gelu\",\n",
    "                # 'glu'\n",
    "            ],\n",
    "        ),\n",
    "        \"kernel_initializer\": hp.choice(\n",
    "            \"kernel_initializer\",\n",
    "            [\n",
    "                \"he_uniform\",\n",
    "                \"he_normal\",\n",
    "                \"glorot_uniform\",\n",
    "                \"glorot_normal\",\n",
    "                \"lecun_uniform\",\n",
    "                \"lecun_normal\",\n",
    "                \"orthogonal\",\n",
    "                \"variance_baseline\",\n",
    "                \"variance_0.1\",\n",
    "                \"variance_0.3\",\n",
    "                \"variance_0.8\",\n",
    "                \"variance_3\",\n",
    "                \"variance_5\",\n",
    "                \"variance_10\",\n",
    "            ],\n",
    "        ),\n",
    "        \"optimizer\": hp.choice(\n",
    "            \"optimizer\", [tf.keras.optimizers.legacy.Adam]\n",
    "        ),  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "        \"learning_rate\": hp.choice(\n",
    "            \"learning_rate\", [10, 5, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "        ),  #\n",
    "        \"adam_epsilon\": hp.choice(\n",
    "            \"adam_epsilon\",\n",
    "            [1, 0.5, 0.3125, 0.03125, 0.003125, 0.0003125, 0.00003125, 0.000003125],\n",
    "        ),\n",
    "        \"clipnorm\": hp.choice(\"clipnorm\", [None]),\n",
    "        # NORMALIZATION?\n",
    "        \"soft_update\": hp.choice(\n",
    "            \"soft_update\", [False]\n",
    "        ),  # seems to always be false, we can try it with tru\n",
    "        \"ema_beta\": hp.uniform(\"ema_beta\", 0.95, 0.999),\n",
    "        \"transfer_interval\": hp.choice(\n",
    "            \"transfer_interval\", [10, 25, 50, 100, 200, 400, 800, 1600, 2000]\n",
    "        ),\n",
    "        \"replay_interval\": hp.choice(\"replay_interval\", [1, 2, 3, 4, 5, 8, 10, 12, 350]),\n",
    "        \"minibatch_size\": hp.choice(\n",
    "            \"minibatch_size\", [2**i for i in range(0, 8)]\n",
    "        ),  ###########\n",
    "        \"replay_buffer_size\": hp.choice(\n",
    "            \"replay_buffer_size\", [2000, 3000, 5000, 7500, 10000, 15000, 20000, 25000, 50000]\n",
    "        ),  #############\n",
    "        \"min_replay_buffer_size\": hp.choice(\n",
    "            \"min_replay_buffer_size\", [0, 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000]\n",
    "        ),  # 125, 250, 375, 500, 625, 750, 875, 1000, 1500, 2000\n",
    "        \"n_step\": hp.choice(\"n_step\", [1, 2, 3, 4, 5, 8, 10]),\n",
    "        \"discount_factor\": hp.choice(\n",
    "            \"discount_factor\", [0.1, 0.5, 0.9, 0.99, 0.995, 0.999]\n",
    "        ),\n",
    "        \"atom_size\": hp.choice(\"atom_size\", [11, 21, 31, 41, 51, 61, 71, 81]),  #\n",
    "        \"conv_layers\": hp.choice(\"conv_layers\", [[], [(32, 8, 4), (64, 4, 2), (64, 3, 1)]]),\n",
    "        \"conv_layers_noisy\": hp.choice(\"conv_layers_noisy\", [False]),\n",
    "        \"width\": hp.choice(\"width\", [32, 64, 128, 256, 512, 1024]),\n",
    "        \"dense_layers\": hp.choice(\"dense_layers\", [0, 1, 2, 3, 4]),\n",
    "        \"dense_layers_noisy\": hp.choice(\n",
    "            \"dense_layers_noisy\", [True]\n",
    "        ),  # i think this is always true for rainbow\n",
    "        # REWARD CLIPPING\n",
    "        \"noisy_sigma\": hp.choice(\"noisy_sigma\", [0.5]),  #\n",
    "        \"loss_function\": hp.choice(\n",
    "            \"loss_function\",\n",
    "            [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()],\n",
    "        ),\n",
    "        \"dueling\": hp.choice(\"dueling\", [True]),\n",
    "        \"advantage_hidden_layers\": hp.choice(\n",
    "            \"advantage_hidden_layers\", [0, 1, 2, 3, 4]\n",
    "        ),  #\n",
    "        \"value_hidden_layers\": hp.choice(\"value_hidden_layers\", [0, 1, 2, 3, 4]),  #\n",
    "        \"training_steps\": hp.choice(\"training_steps\", [30000]),\n",
    "        \"per_epsilon\": hp.choice(\n",
    "            \"per_epsilon\", [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "        ),\n",
    "        \"per_alpha\": hp.choice(\"per_alpha\", [0.05 * i for i in range(0, 21)]),\n",
    "        \"per_beta\": hp.choice(\"per_beta\", [0.05 * i for i in range(1, 21)]),\n",
    "        # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        # 'search_max_depth': 5,\n",
    "        # 'search_max_time': 10,\n",
    "        \"training_iterations\": hp.choice(\"training_iterations\", [1, 2, 3, 4, 5]),\n",
    "        \"num_minibatches\": hp.choice(\"num_minibatches\", [1, 2, 3, 4, 5]),\n",
    "    }\n",
    "    initial_best_config = [\n",
    "        {\n",
    "            \"activation\": 1,\n",
    "            \"kernel_initializer\": 6,\n",
    "            \"optimizer\": 0,  # NO SGD OR RMSPROP FOR NOW SINCE IT IS FOR RAINBOW DQN\n",
    "            \"learning_rate\": 5,  #\n",
    "            \"adam_epsilon\": 5,\n",
    "            \"clipnorm\": 0,\n",
    "            # NORMALIZATION?\n",
    "            \"soft_update\": 0,  # seems to always be false, we can try it with tru\n",
    "            \"ema_beta\": 0.95,\n",
    "            \"transfer_interval\": 3,\n",
    "            \"replay_interval\": 1,\n",
    "            \"minibatch_size\": 7,\n",
    "            \"replay_buffer_size\": 8,  \n",
    "            \"min_replay_buffer_size\": 4,\n",
    "            \"n_step\": 2,\n",
    "            \"discount_factor\": 3,\n",
    "            \"atom_size\": 4,  #\n",
    "            \"conv_layers\": 0,\n",
    "            \"conv_layers_noisy\": 0,\n",
    "            \"width\": 4,\n",
    "            \"dense_layers\": 2,\n",
    "            \"dense_layers_noisy\": 0,  # i think this is always true for rainbow\n",
    "            # REWARD CLIPPING\n",
    "            \"noisy_sigma\": 0,  #\n",
    "            \"loss_function\": 0,\n",
    "            \"dueling\": 0,\n",
    "            \"advantage_hidden_layers\": 0,  #\n",
    "            \"value_hidden_layers\": 0,  #\n",
    "            \"training_steps\": 0,\n",
    "            \"per_epsilon\": 3,\n",
    "            \"per_alpha\": 10,\n",
    "            \"per_beta\": 7,\n",
    "            # 'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "            # 'search_max_depth': 5,\n",
    "            # 'search_max_time': 10,\n",
    "            \"training_iterations\": 0,\n",
    "            \"num_minibatches\": 0,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return search_space, initial_best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'adam_epsilon': 0.0003125, 'advantage_hidden_layers': 0, 'atom_size': 51, 'clipnorm': None, 'conv_layers': (), 'conv_layers_noisy': False, 'dense_layers': 2, 'dense_layers_noisy': True, 'discount_factor': 0.99, 'dueling': True, 'ema_beta': 0.95, 'kernel_initializer': 'orthogonal', 'learning_rate': 0.01, 'loss_function': <keras.src.losses.CategoricalCrossentropy object at 0x29c309130>, 'min_replay_buffer_size': 500, 'minibatch_size': 128, 'n_step': 3, 'noisy_sigma': 0.5, 'num_minibatches': 1, 'optimizer': <class 'keras.src.optimizers.legacy.adam.Adam'>, 'per_alpha': 0.5, 'per_beta': 0.4, 'per_epsilon': 0.001, 'replay_buffer_size': 50000, 'replay_interval': 2, 'soft_update': False, 'training_iterations': 1, 'training_steps': 30000, 'transfer_interval': 100, 'value_hidden_layers': 0, 'width': 512}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import space_eval\n",
    "\n",
    "search_sapce, initial_best_config = create_search_space()\n",
    "config = space_eval(search_sapce, initial_best_config[0])\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights: True\n",
      "Using adam_epsilon: 0.0003125\n",
      "Using learning_rate: 0.01\n",
      "Using clipnorm: None\n",
      "Using optimizer: <class 'keras.src.optimizers.legacy.adam.Adam'>\n",
      "Using loss_function: <keras.src.losses.CategoricalCrossentropy object at 0x29c309130>\n",
      "Using training_iterations: 1\n",
      "Using num_minibatches: 1\n",
      "Using minibatch_size: 128\n",
      "Using replay_buffer_size: 50000\n",
      "Using min_replay_buffer_size: 500\n",
      "Using training_steps: 30000\n",
      "Using activation: relu\n",
      "Using kernel_initializer: orthogonal\n",
      "Using width: 512\n",
      "Using noisy_sigma: 0.5\n",
      "Using conv_layers: ()\n",
      "Using dense_layers: 2\n",
      "Using default deuling: False\n",
      "Using value_hidden_layers: 0\n",
      "Using advantage_hidden_layers: 0\n",
      "Using discount_factor: 0.99\n",
      "Using soft_update: False\n",
      "Using transfer_interval: 100\n",
      "Using ema_beta: 0.95\n",
      "Using replay_interval: 2\n",
      "Using per_alpha: 0.5\n",
      "Using per_beta: 0.4\n",
      "Using per_epsilon: 0.001\n",
      "Using n_step: 3\n",
      "Using atom_size: 51\n",
      "observation_dimensions:  (4,)\n",
      "num_actions:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/dqn/rainbow/videos/RainbowDQN-CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  10.0\n",
      "score:  11.0\n",
      "score:  12.0\n",
      "score:  11.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/10/RainbowDQN-CartPole-v1-episode-4.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/10/RainbowDQN-CartPole-v1-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/10/RainbowDQN-CartPole-v1-episode-4.mp4\n",
      "score:  9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  13.0\n",
      "score:  17.0\n",
      "score:  35.0\n",
      "score:  15.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/20/RainbowDQN-CartPole-v1-episode-9.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/20/RainbowDQN-CartPole-v1-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/20/RainbowDQN-CartPole-v1-episode-9.mp4\n",
      "score:  11.0\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  91.0\n",
      "score:  102.0\n",
      "score:  97.0\n",
      "score:  40.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/30/RainbowDQN-CartPole-v1-episode-14.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/30/RainbowDQN-CartPole-v1-episode-14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/30/RainbowDQN-CartPole-v1-episode-14.mp4\n",
      "score:  95.0\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  15.0\n",
      "score:  14.0\n",
      "score:  13.0\n",
      "score:  14.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/40/RainbowDQN-CartPole-v1-episode-19.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/40/RainbowDQN-CartPole-v1-episode-19.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/40/RainbowDQN-CartPole-v1-episode-19.mp4\n",
      "score:  11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  75.0\n",
      "score:  73.0\n",
      "score:  79.0\n",
      "score:  76.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/50/RainbowDQN-CartPole-v1-episode-24.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/50/RainbowDQN-CartPole-v1-episode-24.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/50/RainbowDQN-CartPole-v1-episode-24.mp4\n",
      "score:  82.0\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  68.0\n",
      "score:  61.0\n",
      "score:  65.0\n",
      "score:  65.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/60/RainbowDQN-CartPole-v1-episode-29.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/60/RainbowDQN-CartPole-v1-episode-29.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/60/RainbowDQN-CartPole-v1-episode-29.mp4\n",
      "score:  62.0\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "score:  48.0\n",
      "score:  11.0\n",
      "score:  11.0\n",
      "score:  13.0\n",
      "Moviepy - Building video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/70/RainbowDQN-CartPole-v1-episode-34.mp4.\n",
      "Moviepy - Writing video checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/70/RainbowDQN-CartPole-v1-episode-34.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready checkpoints/RainbowDQN-CartPole-v1/videos/RainbowDQN-CartPole-v1/70/RainbowDQN-CartPole-v1-episode-34.mp4\n",
      "score:  13.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m RainbowAgent(env, config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRainbowDQN-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mid))\n\u001b[1;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/dqn/rainbow/rainbow_agent.py:466\u001b[0m, in \u001b[0;36mRainbowAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_minibatches):\n\u001b[0;32m--> 466\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    468\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_model_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_model_updated[\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    470\u001b[0m     target_model_updated \u001b[38;5;241m=\u001b[39m (target_model_updated[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/dqn/rainbow/rainbow_agent.py:218\u001b[0m, in \u001b[0;36mRainbowAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m observations \u001b[38;5;241m=\u001b[39m n_step_observations\n\u001b[1;32m    217\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_states(observations)\n\u001b[0;32m--> 218\u001b[0m target_ditributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_target_distributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_step_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m initial_distributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[1;32m    222\u001b[0m distributions_to_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather_nd(\n\u001b[1;32m    223\u001b[0m     initial_distributions,\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(initial_distributions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), actions)),\n\u001b[1;32m    225\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/dqn/rainbow/rainbow_agent.py:315\u001b[0m, in \u001b[0;36mRainbowAgent.compute_target_distributions\u001b[0;34m(self, samples, discount_factor)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# print(\"Lower Distributions \", lower_distributions[0])\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# print(\"Upper Distributions \", upper_distributions[0])\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# print(\"B \", b)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# print(\"L \", l)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# print(\"U \", u)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# print(\"M \", m)\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mminibatch_size):\n\u001b[0;32m--> 315\u001b[0m     np\u001b[38;5;241m.\u001b[39madd\u001b[38;5;241m.\u001b[39mat(m[i], np\u001b[38;5;241m.\u001b[39masarray(l)[i], \u001b[43mlower_distributions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    316\u001b[0m     np\u001b[38;5;241m.\u001b[39madd\u001b[38;5;241m.\u001b[39mat(m[i], np\u001b[38;5;241m.\u001b[39masarray(u)[i], upper_distributions[i])\n\u001b[1;32m    317\u001b[0m target_distributions \u001b[38;5;241m=\u001b[39m m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/array_ops.py:1156\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1154\u001b[0m   var_empty \u001b[38;5;241m=\u001b[39m constant([], dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m   1155\u001b[0m   packed_begin \u001b[38;5;241m=\u001b[39m packed_end \u001b[38;5;241m=\u001b[39m packed_strides \u001b[38;5;241m=\u001b[39m var_empty\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstrided_slice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_begin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_strides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "config = RainbowConfig(config, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.checkpoint_interval = 10\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_anytrading\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('forex-v0')\n",
    "# env = gym.make('stocks-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "custom_env = gym.make(\n",
    "    'forex-v0',\n",
    "    df=FOREX_EURUSD_1H_ASK,\n",
    "    window_size=10,\n",
    "    frame_bound=(10, 300),\n",
    "    unit_side='right'\n",
    ")\n",
    "\n",
    "# custom_env = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=STOCKS_GOOGL,\n",
    "#     window_size=10,\n",
    "#     frame_bound=(10, 300)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"env information:\")\n",
    "print(\"> shape:\", env.unwrapped.shape)\n",
    "print(\"> df.shape:\", env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", env.unwrapped.max_possible_profit())\n",
    "\n",
    "print()\n",
    "print(\"custom_env information:\")\n",
    "print(\"> shape:\", custom_env.unwrapped.shape)\n",
    "print(\"> df.shape:\", custom_env.unwrapped.df.shape)\n",
    "print(\"> prices.shape:\", custom_env.unwrapped.prices.shape)\n",
    "print(\"> signal_features.shape:\", custom_env.unwrapped.signal_features.shape)\n",
    "print(\"> max_possible_profit:\", custom_env.unwrapped.max_possible_profit())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "env = custom_env\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow_agent import RainbowAgent\n",
    "from agent_configs import RainbowConfig\n",
    "from game_configs import CartPoleConfig\n",
    "config_dict = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_initializer\": \"orthogonal\",\n",
    "    \"min_replay_buffer_size\": 32,\n",
    "    \"loss_function\": tf.keras.losses.KLDivergence(),\n",
    "    \"learning_rate\": 0.000001,\n",
    "}\n",
    "config = RainbowConfig(config_dict, CartPoleConfig())\n",
    "# train\n",
    "agent = RainbowAgent(env, config, \"RainbowDQN-{}\".format(env.unwrapped.spec.id))\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
