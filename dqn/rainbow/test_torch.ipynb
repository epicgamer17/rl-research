{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "from game_configs import AtariConfig, CartPoleConfig\n",
    "from agent_configs import RainbowConfig\n",
    "import random\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ClipReward(AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "env = FrameStack(env, 4)\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "seed_torch(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "  \"conv_layers\": [[32, 8,4], [64, 4,2]],\n",
    "  \"dense_layers_widths\": [128],\n",
    "  \"value_hidden_layers_widths\": [128],\n",
    "  \"advantage_hidden_layers_widths\": [128],\n",
    "  \"adam_epsilon\": 1e-8,\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"training_steps\": 10000,\n",
    "  \"per_epsilon\": 1e-6,\n",
    "  \"per_alpha\": 0.2,\n",
    "  \"per_beta\": 0.6,\n",
    "  \"minibatch_size\": 32,\n",
    "  \"transfer_interval\": 100,\n",
    "  \"n_step\": 3,\n",
    "  \"noisy_sigma\": 0.5,\n",
    "  \"replay_interval\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = AtariConfig()\n",
    "game_config.max_score = 10000\n",
    "config = RainbowConfig(config_dict, game_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RainbowAgent(env, config)\n",
    "\n",
    "# for param in agent.model.parameters():\n",
    "#   print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start\")\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
