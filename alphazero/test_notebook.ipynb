{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Box(0.0, 1.0, (3, 6, 7), float64)\n",
                                    "Using default save_intermediate_weights     : False\n",
                                    "Using         training_steps                : 100\n",
                                    "Using         adam_epsilon                  : 1e-08\n",
                                    "Using default momentum                      : 0.9\n",
                                    "Using         learning_rate                 : 0.002\n",
                                    "Using         clipnorm                      : 0.5\n",
                                    "Using default optimizer                     : <class 'torch.optim.adam.Adam'>\n",
                                    "Using         weight_decay                  : 0.0001\n",
                                    "Using         loss_function                 : None\n",
                                    "Using default activation                    : relu\n",
                                    "Using         kernel_initializer            : None\n",
                                    "Using         minibatch_size                : 1024\n",
                                    "Using default replay_buffer_size            : 5000\n",
                                    "Using default min_replay_buffer_size        : 1024\n",
                                    "Using default num_minibatches               : 1\n",
                                    "Using default training_iterations           : 1\n",
                                    "Using         residual_layers               : [(128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1), (128, 3, 1)]\n",
                                    "Using default conv_layers                   : []\n",
                                    "Using default dense_layer_widths            : []\n",
                                    "Using default conv_layers                   : [(32, 3, 1)]\n",
                                    "Using default dense_layer_widths            : [256]\n",
                                    "Using default conv_layers                   : [(32, 3, 1)]\n",
                                    "Using default dense_layer_widths            : [256]\n",
                                    "Using default noisy_sigma                   : 0.0\n",
                                    "Using         games_per_generation          : 32\n",
                                    "Using default value_loss_factor             : 1.0\n",
                                    "Using         weight_decay                  : 0.0001\n",
                                    "Using         root_dirichlet_alpha          : 1.0\n",
                                    "Using         root_exploration_fraction     : 0.25\n",
                                    "Using         num_simulations               : 800\n",
                                    "Using         num_sampling_moves            : 30\n",
                                    "Using default exploration_temperature       : 1.0\n",
                                    "Using default exploitation_temperature      : 0.1\n",
                                    "Using default clip_low_prob                 : 0.0\n",
                                    "Using         pb_c_base                     : 19652\n",
                                    "Using         pb_c_init                     : 1.25\n",
                                    "observation_dimensions:  (3, 6, 7)\n",
                                    "num_actions:  7\n",
                                    "3\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n",
                                    "128\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/alphazero folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
                                    "  logger.warn(\n"
                              ]
                        }
                  ],
                  "source": [
                        "from agent_configs.alphazero_config import AlphaZeroConfig\n",
                        "from game_configs.tictactoe_config import TicTacToeConfig\n",
                        "from alphazero_agent import AlphaZeroAgent\n",
                        "import gymnasium as gym\n",
                        "import numpy as np\n",
                        "import custom_gym_envs\n",
                        "from torch.optim import Adam, SGD\n",
                        "\n",
                        "\n",
                        "class ClipReward(gym.RewardWrapper):\n",
                        "    def __init__(self, env, min_reward, max_reward):\n",
                        "        super().__init__(env)\n",
                        "        self.min_reward = min_reward\n",
                        "        self.max_reward = max_reward\n",
                        "        self.reward_range = (min_reward, max_reward)\n",
                        "\n",
                        "    def reward(self, reward):\n",
                        "        return np.clip(reward, self.min_reward, self.max_reward)\n",
                        "\n",
                        "\n",
                        "# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
                        "env = gym.make(\"custom_gym_envs/Connect4-v0\", render_mode=\"rgb_array\")\n",
                        "# env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
                        "# env = gym.wrappers.FrameStack(env, 4)\n",
                        "\n",
                        "\n",
                        "# self.games_per_generation: int = self.parse_field(\"games_per_generation\", 100)\n",
                        "# self.value_loss_factor: float = self.parse_field(\"value_loss_factor\", 1.0)\n",
                        "# self.weight_decay: float = self.parse_field(\"weight_decay\", 1e-4)\n",
                        "\n",
                        "# # MCTS\n",
                        "# self.root_dirichlet_alpha: float = self.parse_field(\n",
                        "#     \"root_dirichlet_alpha\", required=False\n",
                        "# )\n",
                        "# if self.root_dirichlet_alpha is None:\n",
                        "#     print(\"Root dirichlet alpha should be defined to a game specific value\")\n",
                        "# self.root_exploration_fraction: float = self.parse_field(\n",
                        "#     \"root_exploration_fraction\", 0.25\n",
                        "# )\n",
                        "# self.num_simulations: int = self.parse_field(\"num_simulations\", 800)\n",
                        "# self.num_sampling_moves: int = self.parse_field(\"num_sampling_moves\", 30)\n",
                        "# self.exploration_temperature: float = self.parse_field(\n",
                        "#     \"exploration_temperature\", 1.0\n",
                        "# )\n",
                        "# self.exploitation_temperature: float = self.parse_field(\n",
                        "#     \"exploitation_temperature\", 0.1\n",
                        "# )\n",
                        "# self.clip_low_prob: float = self.parse_field(\"clip_low_prob\", 0.0)\n",
                        "# self.pb_c_base: int = self.parse_field(\"pb_c_base\", 19652)\n",
                        "# self.pb_c_init: float = self.parse_field(\"pb_c_init\", 1.25)\n",
                        "\n",
                        "config = {\n",
                        "    \"optimizer_function\": Adam,\n",
                        "    \"learning_rate\": 0.002,  #\n",
                        "    \"adam_epsilon\": 1e-8,\n",
                        "    # \"momentum\": 0.9,\n",
                        "    \"clipnorm\": 0.5,\n",
                        "    # NORMALIZATION?\n",
                        "    # REWARD CLIPPING\n",
                        "    \"training_steps\": 100,  #\n",
                        "    \"residual_layers\": [(128, 3, 1)] * 20,  #\n",
                        "    \"critic_conv_layers\": [(32, 3, 1)],  #\n",
                        "    \"critic_widths\": [],  #\n",
                        "    \"actor_conv_layers\": [(32, 3, 1)],  #\n",
                        "    \"actor_widths\": [],  #\n",
                        "    \"memory_size\": 1600,  # 500,000 /  44,000,000 / 24,000,000 / 21,000,000\n",
                        "    \"minibatch_size\": 1024,  #\n",
                        "    \"root_dirichlet_alpha\": 1.0,  #\n",
                        "    \"root_exploration_fraction\": 0.25,\n",
                        "    \"pb_c_init\": 1.25,\n",
                        "    \"pb_c_base\": 19652,\n",
                        "    \"num_simulations\": 800,\n",
                        "    \"weight_decay\": 1e-4,\n",
                        "    \"num_sampling_moves\": 30,  #\n",
                        "    \"loss_function\": None,\n",
                        "    \"games_per_generation\": 32,  #\n",
                        "}\n",
                        "\n",
                        "config = AlphaZeroConfig(config, TicTacToeConfig())\n",
                        "\n",
                        "agent = AlphaZeroAgent(env, config, name=\"alphazero\", device=\"cpu\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Training Step  1\n",
                                    "Training Game  1\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/homebrew/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:246: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
                                    "  logger.warn(\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Target Policy [0.03375    0.03       0.04875    0.04       0.73874998 0.04\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.03375 0.03    0.04875 0.04    0.73875 0.04    0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07125    0.035      0.0525     0.05875    0.06125    0.04\n",
                                    " 0.68124998]\n",
                                    "Temperature Policy  [0.07125 0.035   0.0525  0.05875 0.06125 0.04    0.68125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1068, 0.0889, 0.1941, 0.1076, 0.1372, 0.1540, 0.2114],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1075, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04875    0.03375    0.04625    0.03625    0.04125    0.0425\n",
                                    " 0.75125003]\n",
                                    "Temperature Policy  [0.04875 0.03375 0.04625 0.03625 0.04125 0.0425  0.75125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1126, 0.1050, 0.1471, 0.1418, 0.1375, 0.1328, 0.2233],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2170, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.68875003 0.035      0.04       0.05375    0.0525     0.06375\n",
                                    " 0.06625   ]\n",
                                    "Temperature Policy  [0.68875 0.035   0.04    0.05375 0.0525  0.06375 0.06625]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0927, 0.0780, 0.1529, 0.1507, 0.1517, 0.1466, 0.2274],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2051, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0575     0.04375    0.04875    0.0475     0.0375     0.04\n",
                                    " 0.72500002]\n",
                                    "Temperature Policy  [0.0575  0.04375 0.04875 0.0475  0.0375  0.04    0.725  ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1273, 0.0860, 0.1601, 0.1362, 0.1161, 0.1439, 0.2304],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0238, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04375 0.0375  0.03625 0.04875 0.065   0.0475  0.72125]\n",
                                    "Temperature Policy  [0.04375 0.0375  0.03625 0.04875 0.065   0.0475  0.72125]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0965, 0.1132, 0.1396, 0.1238, 0.1420, 0.1439, 0.2410],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1035, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03125    0.74374998 0.04375    0.04875    0.04125    0.045\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.03125 0.74375 0.04375 0.04875 0.04125 0.045   0.04625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1207, 0.1037, 0.1439, 0.1244, 0.1657, 0.1225, 0.2190],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2213, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.055      0.0625     0.15625    0.06375    0.05875    0.0525\n",
                                    " 0.55124998]\n",
                                    "Temperature Policy  [0.055   0.0625  0.15625 0.06375 0.05875 0.0525  0.55125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0883, 0.1168, 0.1819, 0.1298, 0.1594, 0.1429, 0.1810],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2211, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0275     0.055      0.04125    0.02625    0.78250003 0.03625\n",
                                    " 0.03125   ]\n",
                                    "Temperature Policy  [0.0275  0.055   0.04125 0.02625 0.7825  0.03625 0.03125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0772, 0.0884, 0.2084, 0.1536, 0.1499, 0.1448, 0.1777],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0258, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175  0.0175  0.02125 0.02125 0.88    0.0225  0.02   ]\n",
                                    "Temperature Policy  [0.0175  0.0175  0.02125 0.02125 0.88    0.0225  0.02   ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0831, 0.0953, 0.1843, 0.1334, 0.1645, 0.1323, 0.2072],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2850, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.025      0.80124998 0.0325     0.0375     0.03875    0.03375\n",
                                    " 0.03125   ]\n",
                                    "Temperature Policy  [0.025   0.80125 0.0325  0.0375  0.03875 0.03375 0.03125]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0877, 0.0942, 0.1731, 0.1481, 0.1061, 0.1388, 0.2519],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1219, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.035      0.03875    0.04375    0.0525     0.75625002\n",
                                    " 0.04125   ]\n",
                                    "Temperature Policy  [0.0325  0.035   0.03875 0.04375 0.0525  0.75625 0.04125]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0922, 0.0838, 0.1393, 0.1690, 0.1679, 0.1647, 0.1831],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3216, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.03375    0.03625    0.03625    0.0375     0.78750002\n",
                                    " 0.03625   ]\n",
                                    "Temperature Policy  [0.0325  0.03375 0.03625 0.03625 0.0375  0.7875  0.03625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0846, 0.1090, 0.1626, 0.1500, 0.1209, 0.1636, 0.2093],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2589, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05    0.52625 0.04875 0.05125 0.145   0.0725  0.10625]\n",
                                    "Temperature Policy  [0.05    0.52625 0.04875 0.05125 0.145   0.0725  0.10625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1074, 0.0842, 0.1266, 0.1511, 0.1630, 0.1116, 0.2562],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4047, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0275     0.0475     0.28625    0.02625    0.045      0.51875001\n",
                                    " 0.04875   ]\n",
                                    "Temperature Policy  [0.0275  0.0475  0.28625 0.02625 0.045   0.51875 0.04875]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1117, 0.1008, 0.1982, 0.1250, 0.1294, 0.1340, 0.2009],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0775, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125    0.02       0.02375    0.0225     0.02125    0.87124997\n",
                                    " 0.02      ]\n",
                                    "Temperature Policy  [0.02125 0.02    0.02375 0.0225  0.02125 0.87125 0.02   ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0983, 0.0927, 0.1466, 0.1494, 0.1604, 0.1062, 0.2464],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3785, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625 0.1275  0.05    0.05125 0.1825  0.44    0.1125 ]\n",
                                    "Temperature Policy  [0.03625 0.1275  0.05    0.05125 0.1825  0.44    0.1125 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1030, 0.1038, 0.2017, 0.1200, 0.1706, 0.1150, 0.1859],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0687, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.05       0.56625003 0.245      0.0525     0.01875\n",
                                    " 0.035     ]\n",
                                    "Temperature Policy  [0.0325  0.05    0.56625 0.245   0.0525  0.01875 0.035  ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0896, 0.0929, 0.1375, 0.1450, 0.1679, 0.1445, 0.2226],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4263, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.02125    0.01875    0.88125002 0.02125    0.02\n",
                                    " 0.02      ]\n",
                                    "Temperature Policy  [0.0175  0.02125 0.01875 0.88125 0.02125 0.02    0.02   ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1241, 0.0985, 0.1362, 0.1638, 0.1878, 0.1044, 0.1852],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2582, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0425     0.28749999 0.0425     0.04125    0.51875001 0.01875\n",
                                    " 0.04875   ]\n",
                                    "Temperature Policy  [0.0425  0.2875  0.0425  0.04125 0.51875 0.01875 0.04875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1004, 0.0773, 0.1651, 0.1546, 0.1559, 0.1093, 0.2373],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0466, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04125    0.05625    0.035      0.42250001 0.2475     0.03125\n",
                                    " 0.16625001]\n",
                                    "Temperature Policy  [0.04125 0.05625 0.035   0.4225  0.2475  0.03125 0.16625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1303, 0.1090, 0.1936, 0.1817, 0.0000, 0.1484, 0.2369],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1408, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.27375001 0.47499999 0.035      0.0525     0.         0.01875\n",
                                    " 0.145     ]\n",
                                    "Temperature Policy  [0.27375 0.475   0.035   0.0525  0.01875 0.145  ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1145, 0.1076, 0.2114, 0.1655, 0.0000, 0.1334, 0.2676],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1834, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0275     0.03375    0.03875    0.77499998 0.         0.03375\n",
                                    " 0.09125   ]\n",
                                    "Temperature Policy  [0.0275  0.03375 0.03875 0.775   0.03375 0.09125]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1195, 0.1168, 0.1823, 0.1848, 0.0000, 0.1566, 0.2401],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1246, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.54374999 0.0825     0.08625    0.20874999 0.         0.02875\n",
                                    " 0.05      ]\n",
                                    "Temperature Policy  [0.54375 0.0825  0.08625 0.20875 0.02875 0.05   ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1428, 0.0944, 0.1745, 0.1950, 0.0000, 0.1429, 0.2504],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1004, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01375    0.01125    0.01375    0.01875    0.         0.92124999\n",
                                    " 0.02125   ]\n",
                                    "Temperature Policy  [0.01375 0.01125 0.01375 0.01875 0.92125 0.02125]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  2\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.025      0.05125    0.04       0.0675     0.03875\n",
                                    " 0.74250001]\n",
                                    "Temperature Policy  [0.035   0.025   0.05125 0.04    0.0675  0.03875 0.7425 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1108, 0.1008, 0.1482, 0.1454, 0.1616, 0.1125, 0.2205],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2469, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05125 0.03375 0.045   0.7525  0.0475  0.0325  0.0375 ]\n",
                                    "Temperature Policy  [0.05125 0.03375 0.045   0.7525  0.0475  0.0325  0.0375 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1217, 0.1017, 0.1785, 0.1211, 0.1321, 0.1297, 0.2151],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1409, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.06125    0.72624999 0.03125    0.04125    0.04125\n",
                                    " 0.05875   ]\n",
                                    "Temperature Policy  [0.04    0.06125 0.72625 0.03125 0.04125 0.04125 0.05875]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0903, 0.1116, 0.2207, 0.1259, 0.1755, 0.1237, 0.1522],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2061, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.0475     0.73374999 0.06       0.03875    0.04\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.03625 0.0475  0.73375 0.06    0.03875 0.04    0.04375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1041, 0.0951, 0.1543, 0.1357, 0.1752, 0.1094, 0.2262],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3381, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.75375003 0.04375    0.02875    0.04125    0.0475\n",
                                    " 0.0525    ]\n",
                                    "Temperature Policy  [0.0325  0.75375 0.04375 0.02875 0.04125 0.0475  0.0525 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1184, 0.0959, 0.1764, 0.1258, 0.1947, 0.1060, 0.1829],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2583, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.03625    0.75125003 0.035      0.05375    0.04125\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.03625 0.03625 0.75125 0.035   0.05375 0.04125 0.04625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0912, 0.0888, 0.1483, 0.1772, 0.1771, 0.1143, 0.2032],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0036, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.03       0.055      0.05375    0.70875001 0.055\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.05    0.03    0.055   0.05375 0.70875 0.055   0.0475 ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0932, 0.0891, 0.1625, 0.1742, 0.2149, 0.0938, 0.1725],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1576, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05625    0.0325     0.05125    0.04125    0.69749999 0.065\n",
                                    " 0.05625   ]\n",
                                    "Temperature Policy  [0.05625 0.0325  0.05125 0.04125 0.6975  0.065   0.05625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1004, 0.1137, 0.1449, 0.1834, 0.1531, 0.1364, 0.1681],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1059, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0475     0.0275     0.03625    0.03875    0.28       0.04\n",
                                    " 0.52999997]\n",
                                    "Temperature Policy  [0.0475  0.0275  0.03625 0.03875 0.28    0.04    0.53   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0880, 0.1086, 0.1526, 0.1441, 0.1846, 0.0869, 0.2352],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0315, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0525  0.035   0.0375  0.03875 0.0625  0.15375 0.62   ]\n",
                                    "Temperature Policy  [0.0525  0.035   0.0375  0.03875 0.0625  0.15375 0.62   ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0966, 0.0972, 0.1559, 0.1857, 0.1582, 0.1564, 0.1500],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1264, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.03375    0.0425     0.05       0.16500001 0.06125\n",
                                    " 0.61250001]\n",
                                    "Temperature Policy  [0.035   0.03375 0.0425  0.05    0.165   0.06125 0.6125 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0814, 0.1085, 0.1433, 0.1778, 0.1810, 0.1047, 0.2033],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0201, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.0225     0.02375    0.02125    0.0275     0.85000002\n",
                                    " 0.02625   ]\n",
                                    "Temperature Policy  [0.02875 0.0225  0.02375 0.02125 0.0275  0.85    0.02625]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1112, 0.0938, 0.1318, 0.1871, 0.1778, 0.1224, 0.1760],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3418, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06       0.14749999 0.04125    0.0375     0.12875    0.04125\n",
                                    " 0.54374999]\n",
                                    "Temperature Policy  [0.06    0.1475  0.04125 0.0375  0.12875 0.04125 0.54375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0859, 0.0950, 0.1960, 0.1646, 0.1584, 0.1055, 0.1945],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1345, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01625    0.01625    0.02       0.025      0.01875    0.0175\n",
                                    " 0.88625002]\n",
                                    "Temperature Policy  [0.01625 0.01625 0.02    0.025   0.01875 0.0175  0.88625]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1010, 0.0988, 0.1502, 0.1733, 0.1680, 0.1222, 0.1865],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1141, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0525     0.13124999 0.08625    0.42750001 0.13375001 0.02\n",
                                    " 0.14875001]\n",
                                    "Temperature Policy  [0.0525  0.13125 0.08625 0.4275  0.13375 0.02    0.14875]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1011, 0.1045, 0.1709, 0.1803, 0.1556, 0.0835, 0.2041],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2423, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01625    0.88249999 0.02125    0.02       0.01875    0.0175\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.01625 0.8825  0.02125 0.02    0.01875 0.0175  0.02375]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1067, 0.0969, 0.1614, 0.1608, 0.1604, 0.1381, 0.1757],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1886, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05875    0.055      0.65499997 0.04       0.09125    0.01875\n",
                                    " 0.08125   ]\n",
                                    "Temperature Policy  [0.05875 0.055   0.655   0.04    0.09125 0.01875 0.08125]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1279, 0.0968, 0.1651, 0.1937, 0.1673, 0.0798, 0.1695],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1616, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.035      0.61874998 0.0925     0.12875    0.04\n",
                                    " 0.04875   ]\n",
                                    "Temperature Policy  [0.03625 0.035   0.61875 0.0925  0.12875 0.04    0.04875]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1193, 0.1035, 0.1618, 0.1288, 0.1414, 0.1588, 0.1864],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0296, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02    0.0175  0.02375 0.0175  0.02    0.8775  0.02375]\n",
                                    "Temperature Policy  [0.02    0.0175  0.02375 0.0175  0.02    0.8775  0.02375]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1300, 0.0959, 0.1440, 0.1469, 0.1817, 0.1093, 0.1922],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2841, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06625    0.04125    0.08625    0.38249999 0.14       0.075\n",
                                    " 0.20874999]\n",
                                    "Temperature Policy  [0.06625 0.04125 0.08625 0.3825  0.14    0.075   0.20875]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1161, 0.1142, 0.1391, 0.1288, 0.1599, 0.1588, 0.1830],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1434, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.01875    0.01625    0.015      0.88499999 0.01875\n",
                                    " 0.02625   ]\n",
                                    "Temperature Policy  [0.02    0.01875 0.01625 0.015   0.885   0.01875 0.02625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1266, 0.0924, 0.1350, 0.1780, 0.1527, 0.0860, 0.2293],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0822, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.10625    0.12125    0.11125    0.22499999 0.16625001 0.14\n",
                                    " 0.13      ]\n",
                                    "Temperature Policy  [0.10625 0.12125 0.11125 0.225   0.16625 0.14    0.13   ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1181, 0.0923, 0.1635, 0.1210, 0.1719, 0.1413, 0.1919],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0519, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01    0.01    0.01125 0.00875 0.015   0.9325  0.0125 ]\n",
                                    "Temperature Policy  [0.01    0.01    0.01125 0.00875 0.015   0.9325  0.0125 ]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  3\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.02375    0.04375    0.0525     0.0675     0.035\n",
                                    " 0.74124998]\n",
                                    "Temperature Policy  [0.03625 0.02375 0.04375 0.0525  0.0675  0.035   0.74125]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1097, 0.1113, 0.1762, 0.1014, 0.1806, 0.1353, 0.1855],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2080, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0725     0.03125    0.05625    0.70749998 0.045      0.0375\n",
                                    " 0.05      ]\n",
                                    "Temperature Policy  [0.0725  0.03125 0.05625 0.7075  0.045   0.0375  0.05   ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0795, 0.0875, 0.1758, 0.1485, 0.1598, 0.1537, 0.1952],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1444, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04125    0.03125    0.045      0.0325     0.76125002 0.0425\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.04125 0.03125 0.045   0.0325  0.76125 0.0425  0.04625]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0973, 0.0977, 0.1599, 0.1661, 0.1361, 0.1367, 0.2062],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3951, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.06       0.04125    0.04625    0.055      0.0475\n",
                                    " 0.70999998]\n",
                                    "Temperature Policy  [0.04    0.06    0.04125 0.04625 0.055   0.0475  0.71   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0800, 0.0944, 0.1705, 0.1197, 0.1586, 0.1500, 0.2269],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0850, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625 0.04375 0.05125 0.02875 0.05875 0.72375 0.0575 ]\n",
                                    "Temperature Policy  [0.03625 0.04375 0.05125 0.02875 0.05875 0.72375 0.0575 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0862, 0.0762, 0.1733, 0.1772, 0.1733, 0.1383, 0.1754],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4599, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.05       0.05375    0.04375    0.45875001 0.30875\n",
                                    " 0.045     ]\n",
                                    "Temperature Policy  [0.04    0.05    0.05375 0.04375 0.45875 0.30875 0.045  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0824, 0.0755, 0.1570, 0.1505, 0.1643, 0.1201, 0.2502],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1847, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01625 0.02125 0.02    0.02    0.025   0.875   0.0225 ]\n",
                                    "Temperature Policy  [0.01625 0.02125 0.02    0.02    0.025   0.875   0.0225 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1063, 0.0855, 0.1983, 0.1472, 0.1647, 0.1497, 0.1483],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2463, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.31375 0.04125 0.08    0.03625 0.4325  0.0525  0.04375]\n",
                                    "Temperature Policy  [0.31375 0.04125 0.08    0.03625 0.4325  0.0525  0.04375]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0892, 0.0890, 0.1744, 0.1618, 0.1519, 0.1287, 0.2050],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0910, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.71499997 0.0475     0.04       0.03375    0.09125    0.025\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.715   0.0475  0.04    0.03375 0.09125 0.025   0.0475 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1279, 0.1112, 0.1303, 0.1537, 0.1234, 0.1390, 0.2146],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3035, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.88125002 0.0175     0.0175     0.02125    0.01875    0.0225\n",
                                    " 0.02125   ]\n",
                                    "Temperature Policy  [0.88125 0.0175  0.0175  0.02125 0.01875 0.0225  0.02125]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0918, 0.0910, 0.1759, 0.1656, 0.1245, 0.1248, 0.2263],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0175, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06       0.0775     0.06       0.03875    0.62875003 0.04375\n",
                                    " 0.09125   ]\n",
                                    "Temperature Policy  [0.06    0.0775  0.06    0.03875 0.62875 0.04375 0.09125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1069, 0.1060, 0.1644, 0.1509, 0.1339, 0.1529, 0.1849],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0313, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.25999999 0.17874999 0.055      0.0475     0.09125    0.1125\n",
                                    " 0.255     ]\n",
                                    "Temperature Policy  [0.26    0.17875 0.055   0.0475  0.09125 0.1125  0.255  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1022, 0.0724, 0.1398, 0.1692, 0.1651, 0.1229, 0.2284],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1792, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0675     0.045      0.05125    0.06375    0.56375003 0.0475\n",
                                    " 0.16125   ]\n",
                                    "Temperature Policy  [0.0675  0.045   0.05125 0.06375 0.56375 0.0475  0.16125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1105, 0.0935, 0.1333, 0.1711, 0.1284, 0.1814, 0.1818],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2400, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.15000001 0.3125     0.08       0.0775     0.0925     0.07625\n",
                                    " 0.21125001]\n",
                                    "Temperature Policy  [0.15    0.3125  0.08    0.0775  0.0925  0.07625 0.21125]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0887, 0.0913, 0.1302, 0.2133, 0.1570, 0.1135, 0.2060],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2143, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0875  0.0775  0.0925  0.0925  0.07625 0.10625 0.4675 ]\n",
                                    "Temperature Policy  [0.0875  0.0775  0.0925  0.0925  0.07625 0.10625 0.4675 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1193, 0.0918, 0.1709, 0.1434, 0.1300, 0.1313, 0.2133],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0719, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.04125    0.03875    0.1125     0.045      0.02625\n",
                                    " 0.70125002]\n",
                                    "Temperature Policy  [0.035   0.04125 0.03875 0.1125  0.045   0.02625 0.70125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1073, 0.0863, 0.1465, 0.2124, 0.1422, 0.0928, 0.2125],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0305, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.0175     0.02       0.87625003 0.02125    0.02375\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.0175  0.0175  0.02    0.87625 0.02125 0.02375 0.02375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1059, 0.0950, 0.1394, 0.1514, 0.1425, 0.1493, 0.2166],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1673, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03       0.035      0.04       0.09       0.03125    0.04625\n",
                                    " 0.72750002]\n",
                                    "Temperature Policy  [0.03    0.035   0.04    0.09    0.03125 0.04625 0.7275 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0901, 0.0818, 0.1423, 0.1936, 0.1528, 0.1114, 0.2279],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0305, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01625    0.01875    0.01875    0.02       0.02       0.01875\n",
                                    " 0.88749999]\n",
                                    "Temperature Policy  [0.01625 0.01875 0.01875 0.02    0.02    0.01875 0.8875 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0848, 0.0934, 0.1608, 0.1385, 0.1449, 0.1543, 0.2234],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0303, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.2        0.0375     0.04375    0.58749998 0.0375     0.0375\n",
                                    " 0.05625   ]\n",
                                    "Temperature Policy  [0.2     0.0375  0.04375 0.5875  0.0375  0.0375  0.05625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1138, 0.0825, 0.1548, 0.1751, 0.1501, 0.1243, 0.1995],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1344, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04875    0.03       0.05       0.04       0.04375    0.04375\n",
                                    " 0.74374998]\n",
                                    "Temperature Policy  [0.04875 0.03    0.05    0.04    0.04375 0.04375 0.74375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1082, 0.0933, 0.1175, 0.1451, 0.1631, 0.1313, 0.2415],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2279, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.03875    0.0425     0.26625001 0.04375    0.26374999\n",
                                    " 0.29499999]\n",
                                    "Temperature Policy  [0.05    0.03875 0.0425  0.26625 0.04375 0.26375 0.295  ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1068, 0.0870, 0.1660, 0.1715, 0.1384, 0.1013, 0.2290],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0103, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05125    0.04       0.17749999 0.53874999 0.0575     0.06375\n",
                                    " 0.07125   ]\n",
                                    "Temperature Policy  [0.05125 0.04    0.1775  0.53875 0.0575  0.06375 0.07125]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1072, 0.0959, 0.1268, 0.1621, 0.1549, 0.1266, 0.2263],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0766, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.24875    0.21250001 0.05       0.18125001 0.05       0.04375\n",
                                    " 0.21375   ]\n",
                                    "Temperature Policy  [0.24875 0.2125  0.05    0.18125 0.05    0.04375 0.21375]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1398, 0.0867, 0.1332, 0.1753, 0.1372, 0.1254, 0.2025],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0490, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04625 0.71875 0.05875 0.03625 0.04125 0.04125 0.0575 ]\n",
                                    "Temperature Policy  [0.04625 0.71875 0.05875 0.03625 0.04125 0.04125 0.0575 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1108, 0.0814, 0.1215, 0.1817, 0.1501, 0.1395, 0.2150],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1783, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.34125    0.0325     0.03875    0.30625001 0.04       0.09375\n",
                                    " 0.14749999]\n",
                                    "Temperature Policy  [0.34125 0.0325  0.03875 0.30625 0.04    0.09375 0.1475 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1188, 0.0902, 0.1099, 0.1862, 0.1752, 0.1012, 0.2185],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0974, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0675     0.085      0.055      0.065      0.05625    0.0375\n",
                                    " 0.63375002]\n",
                                    "Temperature Policy  [0.0675  0.085   0.055   0.065   0.05625 0.0375  0.63375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1623, 0.0914, 0.1441, 0.2171, 0.2132, 0.1719, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1105, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.11125    0.04       0.04875    0.61374998 0.06375    0.1225\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [0.11125 0.04    0.04875 0.61375 0.06375 0.1225 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1857, 0.1707, 0.1787, 0.0000, 0.2903, 0.1746, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0316, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.08875 0.755   0.045   0.      0.06125 0.05    0.     ]\n",
                                    "Temperature Policy  [0.08875 0.755   0.045   0.06125 0.05   ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1745, 0.1231, 0.1874, 0.0000, 0.2874, 0.2275, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2013, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.42750001 0.08625    0.075      0.         0.28625    0.125\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [0.4275  0.08625 0.075   0.28625 0.125  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1939, 0.2154, 0.3129, 0.0000, 0.0000, 0.2778, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0361, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.10875    0.5        0.05375    0.         0.         0.33750001\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [2.3235265e-07 9.8074251e-01 2.0213413e-10 1.9257182e-02]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.2591, 0.1796, 0.3086, 0.0000, 0.0000, 0.2527, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0502, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04625    0.2175     0.03       0.         0.         0.70625001\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [1.4505391e-12 7.6736978e-06 1.9126021e-14 9.9999237e-01]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.2406, 0.1933, 0.3043, 0.0000, 0.0000, 0.2618, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1153, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.20999999 0.46375    0.0925     0.         0.         0.23375\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [3.6202365e-04 9.9858081e-01 9.9531675e-08 1.0569640e-03]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.2631, 0.1887, 0.2870, 0.0000, 0.0000, 0.2611, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0199, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.015      0.95125002 0.01625    0.         0.         0.0175\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [9.5053010e-19 1.0000000e+00 2.1163474e-18 4.4405198e-18]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.2206, 0.0000, 0.4025, 0.0000, 0.0000, 0.3768, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0714, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.27250001 0.         0.435      0.         0.         0.29249999\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [0.00905095 0.9725715  0.01837755]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.2780, 0.0000, 0.3119, 0.0000, 0.0000, 0.4101, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0699, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175  0.      0.96375 0.      0.      0.01875 0.     ]\n",
                                    "Temperature Policy  [3.8970555e-18 1.0000000e+00 7.7690664e-18]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [-1, 1]]\n",
                                    "Updated Rewards [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  4\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325  0.02375 0.045   0.03875 0.7525  0.03875 0.06875]\n",
                                    "Temperature Policy  [0.0325  0.02375 0.045   0.03875 0.7525  0.03875 0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06375    0.03375    0.05       0.24375001 0.05375    0.0475\n",
                                    " 0.50749999]\n",
                                    "Temperature Policy  [0.06375 0.03375 0.05    0.24375 0.05375 0.0475  0.5075 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1114, 0.0914, 0.1924, 0.1304, 0.1148, 0.1543, 0.2053],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0861, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.75999999 0.0375     0.04625    0.03375    0.0325     0.04625\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.76    0.0375  0.04625 0.03375 0.0325  0.04625 0.04375]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0866, 0.1007, 0.1663, 0.1472, 0.1621, 0.1382, 0.1989],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0776, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.04375    0.06375    0.35874999 0.06375    0.04125\n",
                                    " 0.39250001]\n",
                                    "Temperature Policy  [0.03625 0.04375 0.06375 0.35875 0.06375 0.04125 0.3925 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0760, 0.0817, 0.1903, 0.1390, 0.1594, 0.1642, 0.1894],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1284, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05375    0.04125    0.0625     0.05375    0.15125    0.33250001\n",
                                    " 0.30500001]\n",
                                    "Temperature Policy  [0.05375 0.04125 0.0625  0.05375 0.15125 0.3325  0.305  ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0886, 0.0985, 0.1733, 0.1848, 0.1524, 0.1171, 0.1852],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2078, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.045      0.03375    0.04875    0.055      0.70125002 0.05\n",
                                    " 0.06625   ]\n",
                                    "Temperature Policy  [0.045   0.03375 0.04875 0.055   0.70125 0.05    0.06625]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0786, 0.0774, 0.1572, 0.1571, 0.1992, 0.1231, 0.2075],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2473, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0625  0.05625 0.045   0.0525  0.06    0.40625 0.3175 ]\n",
                                    "Temperature Policy  [0.0625  0.05625 0.045   0.0525  0.06    0.40625 0.3175 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0878, 0.0825, 0.1566, 0.1635, 0.1436, 0.1068, 0.2593],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0185, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03       0.04125    0.045      0.0425     0.75375003 0.04125\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.03    0.04125 0.045   0.0425  0.75375 0.04125 0.04625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0914, 0.0879, 0.1820, 0.1543, 0.1647, 0.1253, 0.1943],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1888, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.05375    0.04875    0.03625    0.48249999 0.0425\n",
                                    " 0.30000001]\n",
                                    "Temperature Policy  [0.03625 0.05375 0.04875 0.03625 0.4825  0.0425  0.3    ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1011, 0.1046, 0.1727, 0.1771, 0.1511, 0.1109, 0.1825],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1045, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.52749997 0.06       0.04       0.22750001 0.04375\n",
                                    " 0.06125   ]\n",
                                    "Temperature Policy  [0.04    0.5275  0.06    0.04    0.2275  0.04375 0.06125]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0817, 0.1022, 0.1640, 0.2292, 0.1273, 0.1173, 0.1783],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1791, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.40875    0.03125    0.075      0.04875    0.15000001 0.08875\n",
                                    " 0.19750001]\n",
                                    "Temperature Policy  [0.40875 0.03125 0.075   0.04875 0.15    0.08875 0.1975 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1019, 0.1161, 0.1667, 0.1571, 0.1782, 0.1004, 0.1796],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0349, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04375 0.31125 0.28375 0.24875 0.0425  0.03125 0.03875]\n",
                                    "Temperature Policy  [0.04375 0.31125 0.28375 0.24875 0.0425  0.03125 0.03875]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0942, 0.0899, 0.1487, 0.1683, 0.2180, 0.1271, 0.1538],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2296, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.02125    0.0225     0.86374998 0.0275     0.0225\n",
                                    " 0.0225    ]\n",
                                    "Temperature Policy  [0.02    0.02125 0.0225  0.86375 0.0275  0.0225  0.0225 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0913, 0.1006, 0.1552, 0.1396, 0.1751, 0.1129, 0.2253],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1155, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.47749999 0.21625    0.0475     0.07125    0.09125    0.03875\n",
                                    " 0.0575    ]\n",
                                    "Temperature Policy  [0.4775  0.21625 0.0475  0.07125 0.09125 0.03875 0.0575 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0892, 0.0730, 0.1608, 0.1649, 0.1541, 0.1506, 0.2075],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0116, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0125     0.89249998 0.01875    0.0175     0.02125    0.01875\n",
                                    " 0.01875   ]\n",
                                    "Temperature Policy  [0.0125  0.8925  0.01875 0.0175  0.02125 0.01875 0.01875]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0821, 0.0966, 0.1944, 0.1419, 0.1697, 0.1221, 0.1933],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0786, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.09375    0.10625    0.26875001 0.1125     0.16249999 0.11125\n",
                                    " 0.145     ]\n",
                                    "Temperature Policy  [0.09375 0.10625 0.26875 0.1125  0.1625  0.11125 0.145  ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0937, 0.0691, 0.1405, 0.1609, 0.1976, 0.1237, 0.2145],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1471, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03       0.0275     0.025      0.21250001 0.03125    0.02625\n",
                                    " 0.64749998]\n",
                                    "Temperature Policy  [0.03    0.0275  0.025   0.2125  0.03125 0.02625 0.6475 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0821, 0.1189, 0.1506, 0.1588, 0.1520, 0.1232, 0.2144],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0190, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125    0.02       0.02125    0.02375    0.02625    0.01875\n",
                                    " 0.86874998]\n",
                                    "Temperature Policy  [0.02125 0.02    0.02125 0.02375 0.02625 0.01875 0.86875]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0869, 0.0670, 0.1513, 0.1577, 0.2048, 0.1465, 0.1858],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2901, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.025      0.02125    0.03125    0.80124998 0.05       0.03\n",
                                    " 0.04125   ]\n",
                                    "Temperature Policy  [0.025   0.02125 0.03125 0.80125 0.05    0.03    0.04125]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0944, 0.1235, 0.1343, 0.1655, 0.1616, 0.1225, 0.1981],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1024, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.02       0.86750001 0.0225     0.0225     0.02125\n",
                                    " 0.02625   ]\n",
                                    "Temperature Policy  [0.02    0.02    0.8675  0.0225  0.0225  0.02125 0.02625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1174, 0.0950, 0.1539, 0.1699, 0.1621, 0.1252, 0.1765],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2687, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01875    0.01625    0.0175     0.88999999 0.02       0.0175\n",
                                    " 0.02      ]\n",
                                    "Temperature Policy  [0.01875 0.01625 0.0175  0.89    0.02    0.0175  0.02   ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0928, 0.0994, 0.1619, 0.1666, 0.1794, 0.1168, 0.1831],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1281, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.01875    0.02375    0.87124997 0.0225     0.02\n",
                                    " 0.02625   ]\n",
                                    "Temperature Policy  [0.0175  0.01875 0.02375 0.87125 0.0225  0.02    0.02625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1572, 0.0822, 0.1393, 0.1790, 0.1355, 0.1149, 0.1919],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3478, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06125    0.70999998 0.04625    0.04875    0.04       0.04\n",
                                    " 0.05375   ]\n",
                                    "Temperature Policy  [0.06125 0.71    0.04625 0.04875 0.04    0.04    0.05375]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1048, 0.0929, 0.1479, 0.1709, 0.1573, 0.1248, 0.2015],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1503, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04125    0.03875    0.30500001 0.0525     0.47749999 0.03375\n",
                                    " 0.05125   ]\n",
                                    "Temperature Policy  [0.04125 0.03875 0.305   0.0525  0.4775  0.03375 0.05125]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1054, 0.0877, 0.1789, 0.1743, 0.1233, 0.1393, 0.1912],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4074, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.0175     0.88999999 0.01875    0.01875    0.0175\n",
                                    " 0.02      ]\n",
                                    "Temperature Policy  [0.0175  0.0175  0.89    0.01875 0.01875 0.0175  0.02   ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1200, 0.0980, 0.1379, 0.1436, 0.1515, 0.1185, 0.2305],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1760, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04125    0.0375     0.49875    0.04875    0.25624999 0.05125\n",
                                    " 0.06625   ]\n",
                                    "Temperature Policy  [0.04125 0.0375  0.49875 0.04875 0.25625 0.05125 0.06625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1270, 0.0836, 0.0000, 0.2275, 0.1667, 0.1653, 0.2299],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2064, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06125    0.25749999 0.         0.0725     0.30625001 0.05\n",
                                    " 0.2525    ]\n",
                                    "Temperature Policy  [0.06125 0.2575  0.0725  0.30625 0.05    0.2525 ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1391, 0.1086, 0.0000, 0.1912, 0.2032, 0.1373, 0.2206],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0646, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04375    0.76249999 0.         0.04875    0.05125    0.0375\n",
                                    " 0.05625   ]\n",
                                    "Temperature Policy  [0.04375 0.7625  0.04875 0.05125 0.0375  0.05625]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1677, 0.0000, 0.0000, 0.2106, 0.1853, 0.1516, 0.2847],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1067, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07375    0.         0.         0.0225     0.02625    0.74874997\n",
                                    " 0.12875   ]\n",
                                    "Temperature Policy  [0.07375 0.0225  0.02625 0.74875 0.12875]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1438, 0.0000, 0.0000, 0.2117, 0.2350, 0.1715, 0.2381],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0686, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.18625    0.         0.         0.16249999 0.19374999 0.25749999\n",
                                    " 0.2       ]\n",
                                    "Temperature Policy  [0.18625 0.1625  0.19375 0.2575  0.2    ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1415, 0.0000, 0.0000, 0.2253, 0.1845, 0.1773, 0.2714],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2570, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01375    0.         0.         0.01625    0.01375    0.94125003\n",
                                    " 0.015     ]\n",
                                    "Temperature Policy  [4.4256500e-19 2.3522516e-18 4.4256500e-19 1.0000000e+00 1.0564835e-18]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  5\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.02875    0.04875    0.0425     0.74374998 0.035\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.0325  0.02875 0.04875 0.0425  0.74375 0.035   0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07125    0.0325     0.04875    0.26875001 0.04625    0.03875\n",
                                    " 0.49375001]\n",
                                    "Temperature Policy  [0.07125 0.0325  0.04875 0.26875 0.04625 0.03875 0.49375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1068, 0.0889, 0.1941, 0.1076, 0.1372, 0.1540, 0.2114],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1075, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.035      0.0425     0.02875    0.055      0.04875\n",
                                    " 0.74000001]\n",
                                    "Temperature Policy  [0.05    0.035   0.0425  0.02875 0.055   0.04875 0.74   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1126, 0.1050, 0.1471, 0.1418, 0.1375, 0.1328, 0.2233],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2170, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.065      0.0375     0.03875    0.0525     0.0475     0.0475\n",
                                    " 0.71125001]\n",
                                    "Temperature Policy  [0.065   0.0375  0.03875 0.0525  0.0475  0.0475  0.71125]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0917, 0.0970, 0.1386, 0.1375, 0.1325, 0.1942, 0.2086],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1132, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035   0.0275  0.0425  0.035   0.0375  0.03625 0.78625]\n",
                                    "Temperature Policy  [0.035   0.0275  0.0425  0.035   0.0375  0.03625 0.78625]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1087, 0.0981, 0.1358, 0.1311, 0.1725, 0.1140, 0.2399],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0006, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.505   0.05875 0.13875 0.105   0.09625 0.04625 0.05   ]\n",
                                    "Temperature Policy  [0.505   0.05875 0.13875 0.105   0.09625 0.04625 0.05   ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0998, 0.0808, 0.1972, 0.1567, 0.1230, 0.1680, 0.1746],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1307, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03375    0.0325     0.05125    0.0875     0.03625    0.04875\n",
                                    " 0.70999998]\n",
                                    "Temperature Policy  [0.03375 0.0325  0.05125 0.0875  0.03625 0.04875 0.71   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1129, 0.0803, 0.1726, 0.1332, 0.1828, 0.0851, 0.2331],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2190, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02375    0.01625    0.02       0.01625    0.02125    0.01625\n",
                                    " 0.88625002]\n",
                                    "Temperature Policy  [0.02375 0.01625 0.02    0.01625 0.02125 0.01625 0.88625]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0731, 0.1064, 0.1676, 0.1589, 0.1168, 0.1967, 0.1805],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0642, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01       0.01125    0.01625    0.0175     0.01375    0.02375\n",
                                    " 0.90750003]\n",
                                    "Temperature Policy  [0.01    0.01125 0.01625 0.0175  0.01375 0.02375 0.9075 ]\n",
                                    "Action  6\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  6\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.0275     0.0475     0.04       0.73750001 0.03875\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.04    0.0275  0.0475  0.04    0.7375  0.03875 0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07125    0.03       0.05       0.05875    0.23875    0.045\n",
                                    " 0.50625002]\n",
                                    "Temperature Policy  [0.07125 0.03    0.05    0.05875 0.23875 0.045   0.50625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0898, 0.1109, 0.2332, 0.1030, 0.1526, 0.1571, 0.1534],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0798, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.04875    0.0625     0.04375    0.70125002 0.05875\n",
                                    " 0.05      ]\n",
                                    "Temperature Policy  [0.035   0.04875 0.0625  0.04375 0.70125 0.05875 0.05   ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1120, 0.0838, 0.1665, 0.1376, 0.1687, 0.1558, 0.1756],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0129, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.15375    0.05375    0.04625    0.03875    0.57625002 0.05625\n",
                                    " 0.075     ]\n",
                                    "Temperature Policy  [0.15375 0.05375 0.04625 0.03875 0.57625 0.05625 0.075  ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0931, 0.1133, 0.1902, 0.1070, 0.1469, 0.1402, 0.2093],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2330, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.03375    0.0475     0.03625    0.76749998 0.045\n",
                                    " 0.04125   ]\n",
                                    "Temperature Policy  [0.02875 0.03375 0.0475  0.03625 0.7675  0.045   0.04125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1173, 0.0847, 0.1646, 0.1590, 0.1539, 0.1120, 0.2084],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1602, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02    0.02    0.02    0.02    0.875   0.02125 0.02375]\n",
                                    "Temperature Policy  [0.02    0.02    0.02    0.02    0.875   0.02125 0.02375]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1042, 0.0866, 0.1563, 0.1276, 0.1683, 0.1327, 0.2244],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4645, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.69625002 0.03875    0.06125    0.04375    0.05       0.0475\n",
                                    " 0.0625    ]\n",
                                    "Temperature Policy  [0.69625 0.03875 0.06125 0.04375 0.05    0.0475  0.0625 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0998, 0.0908, 0.1685, 0.1510, 0.1743, 0.0899, 0.2257],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1365, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.0375     0.04       0.0325     0.04625    0.76875001\n",
                                    " 0.03875   ]\n",
                                    "Temperature Policy  [0.03625 0.0375  0.04    0.0325  0.04625 0.76875 0.03875]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0909, 0.0808, 0.1763, 0.1355, 0.1498, 0.1283, 0.2385],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2462, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.71749997 0.0375     0.07125    0.03875    0.0475     0.03625\n",
                                    " 0.05125   ]\n",
                                    "Temperature Policy  [0.7175  0.0375  0.07125 0.03875 0.0475  0.03625 0.05125]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0850, 0.0977, 0.1631, 0.1816, 0.1618, 0.1020, 0.2088],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1332, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04375    0.0375     0.0425     0.05       0.73500001 0.04125\n",
                                    " 0.05      ]\n",
                                    "Temperature Policy  [0.04375 0.0375  0.0425  0.05    0.735   0.04125 0.05   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1131, 0.0796, 0.1558, 0.1171, 0.1251, 0.1427, 0.2666],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1122, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.79750001 0.025      0.035      0.02625    0.035      0.0325\n",
                                    " 0.04875   ]\n",
                                    "Temperature Policy  [0.7975  0.025   0.035   0.02625 0.035   0.0325  0.04875]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0965, 0.1000, 0.1771, 0.1418, 0.1656, 0.1051, 0.2141],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0181, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.86874998 0.0225     0.02125    0.02       0.0225     0.02125\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.86875 0.0225  0.02125 0.02    0.0225  0.02125 0.02375]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1146, 0.0853, 0.1890, 0.1396, 0.1388, 0.1286, 0.2040],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2061, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.90125 0.0125  0.01625 0.01375 0.02375 0.0125  0.02   ]\n",
                                    "Temperature Policy  [0.90125 0.0125  0.01625 0.01375 0.02375 0.0125  0.02   ]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  7\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.045      0.02625    0.04625    0.0425     0.73624998 0.035\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.045   0.02625 0.04625 0.0425  0.73625 0.035   0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07125 0.0425  0.05    0.0575  0.05375 0.04    0.685  ]\n",
                                    "Temperature Policy  [0.07125 0.0425  0.05    0.0575  0.05375 0.04    0.685  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1198, 0.1153, 0.2217, 0.1249, 0.1175, 0.1388, 0.1620],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1196, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.03625    0.04625    0.03       0.76249999 0.04625\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.0325  0.03625 0.04625 0.03    0.7625  0.04625 0.04625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1186, 0.1091, 0.1761, 0.1165, 0.1548, 0.1458, 0.1792],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1910, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06625    0.05375    0.09875    0.04375    0.065      0.1225\n",
                                    " 0.55000001]\n",
                                    "Temperature Policy  [0.06625 0.05375 0.09875 0.04375 0.065   0.1225  0.55   ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0879, 0.0938, 0.1841, 0.1657, 0.1551, 0.1358, 0.1775],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0352, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01125    0.0125     0.01625    0.01375    0.01625    0.91250002\n",
                                    " 0.0175    ]\n",
                                    "Temperature Policy  [0.01125 0.0125  0.01625 0.01375 0.01625 0.9125  0.0175 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0884, 0.0993, 0.1911, 0.1321, 0.1517, 0.1339, 0.2035],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1332, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.105   0.13625 0.185   0.10875 0.13875 0.14    0.18625]\n",
                                    "Temperature Policy  [0.105   0.13625 0.185   0.10875 0.13875 0.14    0.18625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1155, 0.1026, 0.1987, 0.1692, 0.1213, 0.1292, 0.1635],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0679, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0125     0.0125     0.45875001 0.01375    0.01625    0.03375\n",
                                    " 0.45249999]\n",
                                    "Temperature Policy  [0.0125  0.0125  0.45875 0.01375 0.01625 0.03375 0.4525 ]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  8\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.03625    0.04625    0.04       0.74124998 0.035\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.0325  0.03625 0.04625 0.04    0.74125 0.035   0.06875]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1020, 0.1106, 0.1513, 0.1341, 0.1357, 0.1515, 0.2148],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0364, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0575  0.04    0.04375 0.03375 0.04875 0.71625 0.06   ]\n",
                                    "Temperature Policy  [0.0575  0.04    0.04375 0.03375 0.04875 0.71625 0.06   ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1021, 0.1066, 0.1647, 0.1370, 0.1939, 0.1159, 0.1798],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2148, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0575     0.0375     0.1375     0.04625    0.61000001 0.04\n",
                                    " 0.07125   ]\n",
                                    "Temperature Policy  [0.0575  0.0375  0.1375  0.04625 0.61    0.04    0.07125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0954, 0.1089, 0.1827, 0.1239, 0.1476, 0.1745, 0.1670],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0122, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.03375    0.065      0.0275     0.74124998 0.04375\n",
                                    " 0.03875   ]\n",
                                    "Temperature Policy  [0.05    0.03375 0.065   0.0275  0.74125 0.04375 0.03875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0768, 0.1035, 0.1696, 0.1396, 0.1701, 0.1299, 0.2105],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1705, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03375    0.15125    0.05       0.055      0.22624999 0.06\n",
                                    " 0.42375001]\n",
                                    "Temperature Policy  [0.03375 0.15125 0.05    0.055   0.22625 0.06    0.42375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0976, 0.0866, 0.1921, 0.1799, 0.1352, 0.1631, 0.1456],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1202, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02625    0.04625    0.04875    0.03625    0.76125002 0.05125\n",
                                    " 0.03      ]\n",
                                    "Temperature Policy  [0.02625 0.04625 0.04875 0.03625 0.76125 0.05125 0.03   ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0878, 0.1153, 0.1800, 0.1304, 0.1541, 0.1307, 0.2016],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0827, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.1275     0.03625    0.045      0.05125    0.56875002 0.05375\n",
                                    " 0.1175    ]\n",
                                    "Temperature Policy  [0.1275  0.03625 0.045   0.05125 0.56875 0.05375 0.1175 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1009, 0.0855, 0.1828, 0.1731, 0.1359, 0.1154, 0.2064],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2350, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04    0.0275  0.03875 0.0375  0.77875 0.03375 0.04375]\n",
                                    "Temperature Policy  [0.04    0.0275  0.03875 0.0375  0.77875 0.03375 0.04375]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0857, 0.1219, 0.1001, 0.1545, 0.1762, 0.1497, 0.2118],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1515, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175  0.02    0.01625 0.0225  0.875   0.02375 0.025  ]\n",
                                    "Temperature Policy  [0.0175  0.02    0.01625 0.0225  0.875   0.02375 0.025  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1061, 0.0976, 0.2253, 0.2561, 0.0000, 0.1399, 0.1749],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3372, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.33625001 0.06125    0.23375    0.0575     0.         0.25749999\n",
                                    " 0.05375   ]\n",
                                    "Temperature Policy  [0.33625 0.06125 0.23375 0.0575  0.2575  0.05375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0844, 0.1196, 0.2130, 0.1587, 0.0000, 0.1840, 0.2403],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0734, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02375    0.33875    0.04625    0.50125003 0.         0.0425\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.02375 0.33875 0.04625 0.50125 0.0425  0.0475 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1031, 0.0880, 0.2924, 0.1907, 0.0000, 0.1504, 0.1753],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0043, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.44374999 0.03       0.20125    0.065      0.         0.2025\n",
                                    " 0.0575    ]\n",
                                    "Temperature Policy  [0.44375 0.03    0.20125 0.065   0.2025  0.0575 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1103, 0.0860, 0.2163, 0.1930, 0.0000, 0.1589, 0.2355],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0348, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.36125001 0.03875    0.44       0.0425     0.         0.04625\n",
                                    " 0.07125   ]\n",
                                    "Temperature Policy  [0.36125 0.03875 0.44    0.0425  0.04625 0.07125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0947, 0.0886, 0.2760, 0.2167, 0.0000, 0.1376, 0.1865],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1171, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07       0.03875    0.0525     0.06375    0.         0.06625\n",
                                    " 0.70875001]\n",
                                    "Temperature Policy  [0.07    0.03875 0.0525  0.06375 0.06625 0.70875]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1322, 0.0954, 0.1993, 0.1579, 0.0000, 0.1468, 0.2684],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0478, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.0325     0.03375    0.03625    0.         0.03\n",
                                    " 0.83249998]\n",
                                    "Temperature Policy  [0.035   0.0325  0.03375 0.03625 0.03    0.8325 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1236, 0.0945, 0.2059, 0.1988, 0.0000, 0.1467, 0.2305],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0232, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01875    0.01875    0.02125    0.02375    0.         0.01875\n",
                                    " 0.89875001]\n",
                                    "Temperature Policy  [0.01875 0.01875 0.02125 0.02375 0.01875 0.89875]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1229, 0.1048, 0.1670, 0.1478, 0.0000, 0.1783, 0.2792],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0202, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.20999999 0.04875    0.30875    0.19875    0.         0.04875\n",
                                    " 0.185     ]\n",
                                    "Temperature Policy  [0.21    0.04875 0.30875 0.19875 0.04875 0.185  ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1345, 0.1005, 0.2135, 0.1740, 0.0000, 0.1785, 0.1990],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0268, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04375 0.02875 0.08875 0.21875 0.      0.055   0.565  ]\n",
                                    "Temperature Policy  [0.04375 0.02875 0.08875 0.21875 0.055   0.565  ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1229, 0.0987, 0.1491, 0.1663, 0.0000, 0.1831, 0.2800],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0229, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.52999997 0.08125    0.065      0.13500001 0.         0.0975\n",
                                    " 0.09125   ]\n",
                                    "Temperature Policy  [0.53    0.08125 0.065   0.135   0.0975  0.09125]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1100, 0.1006, 0.2280, 0.1940, 0.0000, 0.1619, 0.2054],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1207, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.04875    0.73374999 0.04875    0.         0.045\n",
                                    " 0.08375   ]\n",
                                    "Temperature Policy  [0.04    0.04875 0.73375 0.04875 0.045   0.08375]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1093, 0.0952, 0.1903, 0.1394, 0.0000, 0.1335, 0.3323],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0343, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.09625    0.075      0.16125    0.155      0.         0.38249999\n",
                                    " 0.13      ]\n",
                                    "Temperature Policy  [0.09625 0.075   0.16125 0.155   0.3825  0.13   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1040, 0.0993, 0.2184, 0.1871, 0.0000, 0.1475, 0.2438],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1495, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04625 0.495   0.18625 0.065   0.      0.0775  0.13   ]\n",
                                    "Temperature Policy  [0.04625 0.495   0.18625 0.065   0.0775  0.13   ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1162, 0.1119, 0.1655, 0.1632, 0.0000, 0.1520, 0.2911],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1486, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03875    0.0175     0.75125003 0.06375    0.         0.06875\n",
                                    " 0.06      ]\n",
                                    "Temperature Policy  [0.03875 0.0175  0.75125 0.06375 0.06875 0.06   ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1186, 0.0984, 0.1973, 0.1766, 0.0000, 0.1622, 0.2469],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2191, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175  0.015   0.0225  0.0225  0.      0.90125 0.02125]\n",
                                    "Temperature Policy  [0.0175  0.015   0.0225  0.0225  0.90125 0.02125]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1670, 0.1245, 0.1757, 0.1300, 0.0000, 0.1620, 0.2409],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2594, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.075      0.02       0.61250001 0.0975     0.         0.13375001\n",
                                    " 0.06125   ]\n",
                                    "Temperature Policy  [0.075   0.02    0.6125  0.0975  0.13375 0.06125]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1305, 0.0903, 0.2327, 0.1659, 0.0000, 0.1667, 0.2139],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0473, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04       0.03       0.03875    0.07875    0.         0.75999999\n",
                                    " 0.0525    ]\n",
                                    "Temperature Policy  [0.04    0.03    0.03875 0.07875 0.76    0.0525 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1540, 0.1067, 0.1677, 0.1521, 0.0000, 0.1626, 0.2569],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1433, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02625    0.0225     0.03375    0.02875    0.         0.85374999\n",
                                    " 0.035     ]\n",
                                    "Temperature Policy  [0.02625 0.0225  0.03375 0.02875 0.85375 0.035  ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1625, 0.1473, 0.2595, 0.2161, 0.0000, 0.2146, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1203, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01625    0.01625    0.02125    0.02625    0.         0.92000002\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [0.01625 0.01625 0.02125 0.02625 0.92   ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1928, 0.1606, 0.2033, 0.1839, 0.0000, 0.2593, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.3117, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.11625    0.07375    0.085      0.13       0.         0.59500003\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [0.11625 0.07375 0.085   0.13    0.595  ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1823, 0.1340, 0.2231, 0.2231, 0.0000, 0.2376, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0096, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.015      0.02875    0.0375     0.         0.89875001\n",
                                    " 0.        ]\n",
                                    "Temperature Policy  [0.02    0.015   0.02875 0.0375  0.89875]\n",
                                    "Action  5\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [-1, 1]]\n",
                                    "Updated Rewards [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  9\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.02375    0.045      0.03875    0.75125003 0.0375\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.035   0.02375 0.045   0.03875 0.75125 0.0375  0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.26374999 0.03       0.05       0.05875    0.05625    0.03875\n",
                                    " 0.5025    ]\n",
                                    "Temperature Policy  [0.26375 0.03    0.05    0.05875 0.05625 0.03875 0.5025 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1114, 0.0914, 0.1924, 0.1304, 0.1148, 0.1543, 0.2053],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0861, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.76125002 0.04       0.04625    0.03375    0.0325     0.0425\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.76125 0.04    0.04625 0.03375 0.0325  0.0425  0.04375]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1206, 0.0867, 0.1418, 0.1197, 0.1532, 0.1496, 0.2283],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4051, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05    0.03625 0.20625 0.17375 0.1825  0.1675  0.18375]\n",
                                    "Temperature Policy  [0.05    0.03625 0.20625 0.17375 0.1825  0.1675  0.18375]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0907, 0.0807, 0.1782, 0.1678, 0.1491, 0.0996, 0.2340],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2880, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0275     0.02625    0.0475     0.03125    0.80374998 0.0275\n",
                                    " 0.03625   ]\n",
                                    "Temperature Policy  [0.0275  0.02625 0.0475  0.03125 0.80375 0.0275  0.03625]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1120, 0.0937, 0.1212, 0.1419, 0.1610, 0.1351, 0.2352],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.5109, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04875    0.0475     0.2375     0.06375    0.40000001 0.04625\n",
                                    " 0.15625   ]\n",
                                    "Temperature Policy  [0.04875 0.0475  0.2375  0.06375 0.4     0.04625 0.15625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1169, 0.1133, 0.1392, 0.1189, 0.1663, 0.1272, 0.2182],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2482, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03       0.26625001 0.57125002 0.03       0.03625    0.02625\n",
                                    " 0.04      ]\n",
                                    "Temperature Policy  [0.03    0.26625 0.57125 0.03    0.03625 0.02625 0.04   ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1030, 0.1039, 0.1687, 0.1220, 0.1750, 0.1458, 0.1815],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2663, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0525     0.055      0.10125    0.04375    0.43875    0.17375\n",
                                    " 0.13500001]\n",
                                    "Temperature Policy  [0.0525  0.055   0.10125 0.04375 0.43875 0.17375 0.135  ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1043, 0.1242, 0.1540, 0.1317, 0.1526, 0.1360, 0.1973],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0532, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125 0.84625 0.02625 0.0275  0.02875 0.025   0.025  ]\n",
                                    "Temperature Policy  [0.02125 0.84625 0.02625 0.0275  0.02875 0.025   0.025  ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0962, 0.0946, 0.1603, 0.1397, 0.1859, 0.1384, 0.1848],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2892, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.01875    0.86374998 0.02125    0.0275     0.025\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.02    0.01875 0.86375 0.02125 0.0275  0.025   0.02375]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1100, 0.1027, 0.1930, 0.1480, 0.1528, 0.1114, 0.1821],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1986, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125    0.0275     0.04       0.83625001 0.0275     0.0225\n",
                                    " 0.025     ]\n",
                                    "Temperature Policy  [0.02125 0.0275  0.04    0.83625 0.0275  0.0225  0.025  ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0925, 0.0758, 0.1763, 0.1274, 0.1802, 0.1415, 0.2063],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2131, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01875    0.01875    0.87374997 0.02625    0.02125    0.02\n",
                                    " 0.02125   ]\n",
                                    "Temperature Policy  [0.01875 0.01875 0.87375 0.02625 0.02125 0.02    0.02125]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1083, 0.0963, 0.2009, 0.1568, 0.1465, 0.1115, 0.1797],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0220, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.035      0.0375     0.64125001 0.03125    0.0325\n",
                                    " 0.1875    ]\n",
                                    "Temperature Policy  [0.035   0.035   0.0375  0.64125 0.03125 0.0325  0.1875 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0983, 0.0990, 0.1656, 0.1398, 0.1272, 0.1244, 0.2456],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1638, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0125  0.0175  0.90375 0.0175  0.01625 0.015   0.0175 ]\n",
                                    "Temperature Policy  [0.0125  0.0175  0.90375 0.0175  0.01625 0.015   0.0175 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1267, 0.1119, 0.1763, 0.1242, 0.1700, 0.1155, 0.1754],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0657, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.11125    0.13500001 0.1725     0.17125    0.12125    0.14125\n",
                                    " 0.14749999]\n",
                                    "Temperature Policy  [0.11125 0.135   0.1725  0.17125 0.12125 0.14125 0.1475 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1006, 0.1009, 0.1887, 0.1563, 0.1226, 0.1377, 0.1932],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1138, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01       0.01125    0.93000001 0.0125     0.01       0.01125\n",
                                    " 0.015     ]\n",
                                    "Temperature Policy  [0.01    0.01125 0.93    0.0125  0.01    0.01125 0.015  ]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [-1, 1]]\n",
                                    "Updated Rewards [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  10\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325  0.02625 0.04375 0.04    0.0675  0.04    0.75   ]\n",
                                    "Temperature Policy  [0.0325  0.02625 0.04375 0.04    0.0675  0.04    0.75   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1108, 0.1008, 0.1482, 0.1454, 0.1616, 0.1125, 0.2205],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2469, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05625 0.04    0.03125 0.755   0.04625 0.03375 0.0375 ]\n",
                                    "Temperature Policy  [0.05625 0.04    0.03125 0.755   0.04625 0.03375 0.0375 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0974, 0.0892, 0.1594, 0.1108, 0.1618, 0.1640, 0.2174],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1728, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.04875    0.73624998 0.035      0.05125    0.04\n",
                                    " 0.06      ]\n",
                                    "Temperature Policy  [0.02875 0.04875 0.73625 0.035   0.05125 0.04    0.06   ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0959, 0.1107, 0.1699, 0.1383, 0.1422, 0.1268, 0.2161],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1228, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0475     0.0475     0.0575     0.03375    0.70875001 0.0575\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.0475  0.0475  0.0575  0.03375 0.70875 0.0575  0.0475 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0847, 0.1059, 0.1850, 0.1121, 0.1465, 0.2065, 0.1592],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0243, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.04375    0.76499999 0.0375     0.0325     0.04\n",
                                    " 0.045     ]\n",
                                    "Temperature Policy  [0.03625 0.04375 0.765   0.0375  0.0325  0.04    0.045  ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0881, 0.1199, 0.1658, 0.1445, 0.2035, 0.1091, 0.1692],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1212, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.39375001 0.05875    0.10625    0.05       0.30000001 0.04\n",
                                    " 0.05125   ]\n",
                                    "Temperature Policy  [0.39375 0.05875 0.10625 0.05    0.3     0.04    0.05125]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0821, 0.0864, 0.1710, 0.1556, 0.1616, 0.1764, 0.1669],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1522, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625    0.49375001 0.26625001 0.04625    0.04625    0.06375\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.03625 0.49375 0.26625 0.04625 0.04625 0.06375 0.0475 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1099, 0.1031, 0.1521, 0.1529, 0.1968, 0.0965, 0.1888],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0899, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05375    0.74250001 0.04375    0.0325     0.04       0.04125\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.05375 0.7425  0.04375 0.0325  0.04    0.04125 0.04625]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0863, 0.0992, 0.1749, 0.1479, 0.1321, 0.1501, 0.2095],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1070, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.69875002 0.04375    0.04625    0.04625    0.075      0.0425\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.69875 0.04375 0.04625 0.04625 0.075   0.0425  0.0475 ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0839, 0.1022, 0.1738, 0.1460, 0.1725, 0.1161, 0.2053],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0973, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0525  0.06    0.71625 0.0375  0.055   0.04125 0.0375 ]\n",
                                    "Temperature Policy  [0.0525  0.06    0.71625 0.0375  0.055   0.04125 0.0375 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0923, 0.0730, 0.2148, 0.1688, 0.1371, 0.1348, 0.1793],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3049, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.14125    0.24375001 0.09375    0.10875    0.19499999 0.1125\n",
                                    " 0.105     ]\n",
                                    "Temperature Policy  [0.14125 0.24375 0.09375 0.10875 0.195   0.1125  0.105  ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0904, 0.1069, 0.1432, 0.1400, 0.2025, 0.1077, 0.2093],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1953, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.4425     0.0525     0.35749999 0.05       0.0275\n",
                                    " 0.0375    ]\n",
                                    "Temperature Policy  [0.0325 0.4425 0.0525 0.3575 0.05   0.0275 0.0375]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0964, 0.0951, 0.1555, 0.1933, 0.1474, 0.1594, 0.1528],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2860, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175  0.02125 0.02    0.02    0.88    0.02125 0.02   ]\n",
                                    "Temperature Policy  [0.0175  0.02125 0.02    0.02    0.88    0.02125 0.02   ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1461, 0.1206, 0.1289, 0.1404, 0.1656, 0.1028, 0.1956],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1972, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.04       0.0525     0.1425     0.61000001 0.05875\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.05    0.04    0.0525  0.1425  0.61    0.05875 0.04625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0928, 0.0871, 0.1517, 0.1784, 0.1392, 0.1764, 0.1744],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1169, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.185      0.32249999 0.205      0.09       0.08375    0.06125\n",
                                    " 0.0525    ]\n",
                                    "Temperature Policy  [0.185   0.3225  0.205   0.09    0.08375 0.06125 0.0525 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0751, 0.0979, 0.1329, 0.1632, 0.1977, 0.1248, 0.2083],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1149, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.39250001 0.04       0.05875    0.28999999 0.105      0.05875\n",
                                    " 0.055     ]\n",
                                    "Temperature Policy  [0.3925  0.04    0.05875 0.29    0.105   0.05875 0.055  ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0898, 0.0807, 0.1817, 0.1335, 0.1435, 0.1785, 0.1923],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0137, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04125 0.25    0.0625  0.05    0.2375  0.27625 0.0825 ]\n",
                                    "Temperature Policy  [0.04125 0.25    0.0625  0.05    0.2375  0.27625 0.0825 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0963, 0.1085, 0.1617, 0.1746, 0.1966, 0.1120, 0.1503],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0016, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.1825  0.04125 0.0675  0.40625 0.20125 0.05    0.05125]\n",
                                    "Temperature Policy  [0.1825  0.04125 0.0675  0.40625 0.20125 0.05    0.05125]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0921, 0.0820, 0.1595, 0.1728, 0.1418, 0.1471, 0.2046],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2074, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175  0.01625 0.02125 0.8775  0.02125 0.0225  0.02375]\n",
                                    "Temperature Policy  [0.0175  0.01625 0.02125 0.8775  0.02125 0.0225  0.02375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1046, 0.1005, 0.1967, 0.1337, 0.1805, 0.0922, 0.1919],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1463, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.68000001 0.02625    0.055      0.10125    0.05375    0.03625\n",
                                    " 0.0475    ]\n",
                                    "Temperature Policy  [0.68    0.02625 0.055   0.10125 0.05375 0.03625 0.0475 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0915, 0.0740, 0.1594, 0.1734, 0.1595, 0.1415, 0.2008],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0169, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.11625    0.05875    0.22       0.0525     0.06625    0.30375001\n",
                                    " 0.1825    ]\n",
                                    "Temperature Policy  [0.11625 0.05875 0.22    0.0525  0.06625 0.30375 0.1825 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1051, 0.0911, 0.1290, 0.1427, 0.2139, 0.1008, 0.2173],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1020, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.73374999 0.03625    0.0425     0.04125    0.045      0.055\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.73375 0.03625 0.0425  0.04125 0.045   0.055   0.04625]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0909, 0.1183, 0.1472, 0.1474, 0.1477, 0.1556, 0.1929],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1230, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01625    0.88375002 0.02       0.01875    0.02375    0.0175\n",
                                    " 0.02      ]\n",
                                    "Temperature Policy  [0.01625 0.88375 0.02    0.01875 0.02375 0.0175  0.02   ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1146, 0.1008, 0.1621, 0.1535, 0.1869, 0.1005, 0.1817],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0161, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03875    0.04625    0.64249998 0.05625    0.04375    0.1175\n",
                                    " 0.055     ]\n",
                                    "Temperature Policy  [0.03875 0.04625 0.6425  0.05625 0.04375 0.1175  0.055  ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0807, 0.0921, 0.1993, 0.1428, 0.1398, 0.1370, 0.2082],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2837, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0375  0.6275  0.04125 0.035   0.03375 0.16    0.065  ]\n",
                                    "Temperature Policy  [0.0375  0.6275  0.04125 0.035   0.03375 0.16    0.065  ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0979, 0.0918, 0.1723, 0.1543, 0.2099, 0.1064, 0.1674],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3775, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03375    0.70999998 0.05       0.06125    0.045      0.05375\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.03375 0.71    0.05    0.06125 0.045   0.05375 0.04625]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0783, 0.0887, 0.1907, 0.1418, 0.1612, 0.1701, 0.1692],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2955, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.04       0.05125    0.04375    0.33125001 0.46000001\n",
                                    " 0.05625   ]\n",
                                    "Temperature Policy  [0.0175  0.04    0.05125 0.04375 0.33125 0.46    0.05625]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1246, 0.1108, 0.1883, 0.1435, 0.2095, 0.0000, 0.2232],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1095, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.73124999 0.0375     0.11125    0.05375    0.02125    0.\n",
                                    " 0.045     ]\n",
                                    "Temperature Policy  [0.73125 0.0375  0.11125 0.05375 0.02125 0.045  ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0956, 0.1021, 0.2216, 0.2003, 0.1611, 0.0000, 0.2192],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2298, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01       0.01625    0.02375    0.0225     0.89875001 0.\n",
                                    " 0.02875   ]\n",
                                    "Temperature Policy  [0.01    0.01625 0.02375 0.0225  0.89875 0.02875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1311, 0.1196, 0.1889, 0.1452, 0.2036, 0.0000, 0.2117],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1922, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0775  0.0625  0.06125 0.08875 0.6225  0.      0.0875 ]\n",
                                    "Temperature Policy  [0.0775  0.0625  0.06125 0.08875 0.6225  0.0875 ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1103, 0.0942, 0.2189, 0.1958, 0.1846, 0.0000, 0.1962],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0471, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01125 0.01    0.0175  0.935   0.0125  0.      0.01375]\n",
                                    "Temperature Policy  [6.3592973e-20 1.9583210e-20 5.2755094e-18 1.0000000e+00 1.8238286e-19\n",
                                    " 4.7305415e-19]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1385, 0.1170, 0.1831, 0.1583, 0.2082, 0.0000, 0.1949],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1359, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.155   0.13    0.1825  0.11875 0.13625 0.      0.2775 ]\n",
                                    "Temperature Policy  [2.8990016e-03 4.9930398e-04 1.4844315e-02 2.0196107e-04 7.9853978e-04\n",
                                    " 9.8075688e-01]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0909, 0.0961, 0.2134, 0.2004, 0.1916, 0.0000, 0.2075],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0177, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0125     0.0125     0.92000002 0.0175     0.01875    0.\n",
                                    " 0.01875   ]\n",
                                    "Temperature Policy  [2.1439856e-19 2.1439856e-19 1.0000000e+00 6.2015782e-18 1.2363301e-17\n",
                                    " 1.2363301e-17]\n",
                                    "Action  2\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, -1]]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  11\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03625 0.025   0.04375 0.04125 0.745   0.04    0.06875]\n",
                                    "Temperature Policy  [0.03625 0.025   0.04375 0.04125 0.745   0.04    0.06875]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1108, 0.1008, 0.1482, 0.1454, 0.1616, 0.1125, 0.2205],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2469, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07    0.03    0.035   0.7475  0.04    0.03375 0.04375]\n",
                                    "Temperature Policy  [0.07    0.03    0.035   0.7475  0.04    0.03375 0.04375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0974, 0.0892, 0.1594, 0.1108, 0.1618, 0.1640, 0.2174],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1728, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325  0.0375  0.0475  0.0325  0.05125 0.04875 0.75   ]\n",
                                    "Temperature Policy  [0.0325  0.0375  0.0475  0.0325  0.05125 0.04875 0.75   ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1103, 0.0996, 0.1808, 0.1729, 0.1318, 0.1024, 0.2022],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3214, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06       0.04625    0.0475     0.04125    0.03       0.04375\n",
                                    " 0.73124999]\n",
                                    "Temperature Policy  [0.06    0.04625 0.0475  0.04125 0.03    0.04375 0.73125]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0974, 0.1281, 0.2083, 0.0928, 0.1433, 0.1733, 0.1567],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0973, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03       0.035      0.04625    0.03625    0.04125    0.03625\n",
                                    " 0.77499998]\n",
                                    "Temperature Policy  [0.03    0.035   0.04625 0.03625 0.04125 0.03625 0.775  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0950, 0.1094, 0.1550, 0.1390, 0.1437, 0.1363, 0.2217],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2511, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.045      0.03375    0.0475     0.03875    0.05375    0.04\n",
                                    " 0.74124998]\n",
                                    "Temperature Policy  [0.045   0.03375 0.0475  0.03875 0.05375 0.04    0.74125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0880, 0.1194, 0.1631, 0.1351, 0.1420, 0.1710, 0.1815],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0337, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.04375    0.73500001 0.045      0.03375    0.05125\n",
                                    " 0.05625   ]\n",
                                    "Temperature Policy  [0.035   0.04375 0.735   0.045   0.03375 0.05125 0.05625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1159, 0.1181, 0.1570, 0.1375, 0.1468, 0.1498, 0.1749],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1344, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.34375 0.03875 0.06625 0.045   0.41125 0.03875 0.05625]\n",
                                    "Temperature Policy  [0.34375 0.03875 0.06625 0.045   0.41125 0.03875 0.05625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1033, 0.0930, 0.1627, 0.1668, 0.1543, 0.1507, 0.1692],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0962, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0375     0.03375    0.07125    0.04       0.73124999 0.05\n",
                                    " 0.03625   ]\n",
                                    "Temperature Policy  [0.0375  0.03375 0.07125 0.04    0.73125 0.05    0.03625]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1246, 0.1133, 0.1781, 0.1323, 0.1533, 0.1289, 0.1695],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0977, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04625    0.0375     0.03625    0.03125    0.76999998 0.0375\n",
                                    " 0.04125   ]\n",
                                    "Temperature Policy  [0.04625 0.0375  0.03625 0.03125 0.77    0.0375  0.04125]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1226, 0.1014, 0.1798, 0.1341, 0.1170, 0.1642, 0.1808],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0768, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.07625    0.06125    0.0875     0.0475     0.05125    0.61750001\n",
                                    " 0.05875   ]\n",
                                    "Temperature Policy  [0.07625 0.06125 0.0875  0.0475  0.05125 0.6175  0.05875]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1048, 0.1011, 0.1615, 0.1151, 0.1649, 0.1253, 0.2272],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0330, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.025      0.02625    0.17874999 0.67624998 0.02875    0.03\n",
                                    " 0.035     ]\n",
                                    "Temperature Policy  [0.025   0.02625 0.17875 0.67625 0.02875 0.03    0.035  ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1295, 0.0984, 0.1444, 0.1196, 0.1578, 0.1529, 0.1975],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1243, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0225     0.02       0.87124997 0.02       0.0225     0.02125\n",
                                    " 0.0225    ]\n",
                                    "Temperature Policy  [0.0225  0.02    0.87125 0.02    0.0225  0.02125 0.0225 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1092, 0.0950, 0.1792, 0.1344, 0.1697, 0.1175, 0.1951],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.3365, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125 0.01875 0.0325  0.84125 0.0275  0.02625 0.0325 ]\n",
                                    "Temperature Policy  [0.02125 0.01875 0.0325  0.84125 0.0275  0.02625 0.0325 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1282, 0.0987, 0.1388, 0.1495, 0.1656, 0.1494, 0.1698],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0150, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.025   0.02    0.01875 0.87    0.02125 0.0225  0.0225 ]\n",
                                    "Temperature Policy  [0.025   0.02    0.01875 0.87    0.02125 0.0225  0.0225 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1134, 0.0878, 0.1627, 0.1243, 0.1680, 0.1095, 0.2342],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0274, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04625    0.085      0.24625    0.49125001 0.0475     0.035\n",
                                    " 0.04875   ]\n",
                                    "Temperature Policy  [0.04625 0.085   0.24625 0.49125 0.0475  0.035   0.04875]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1177, 0.1060, 0.1190, 0.1441, 0.1492, 0.1594, 0.2045],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0152, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.05625    0.01875    0.1725     0.05125    0.62625003\n",
                                    " 0.0425    ]\n",
                                    "Temperature Policy  [0.0325  0.05625 0.01875 0.1725  0.05125 0.62625 0.0425 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1036, 0.0922, 0.1648, 0.1802, 0.1603, 0.1246, 0.1742],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1033, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.035      0.2325     0.58749998 0.04125    0.035\n",
                                    " 0.03625   ]\n",
                                    "Temperature Policy  [0.0325  0.035   0.2325  0.5875  0.04125 0.035   0.03625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1074, 0.0971, 0.1216, 0.1447, 0.1474, 0.1617, 0.2201],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1212, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02375    0.0175     0.87875003 0.01875    0.01875    0.02\n",
                                    " 0.0225    ]\n",
                                    "Temperature Policy  [0.02375 0.0175  0.87875 0.01875 0.01875 0.02    0.0225 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1174, 0.0820, 0.1353, 0.1261, 0.1917, 0.1345, 0.2130],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0739, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.23999999 0.045      0.10625    0.05125    0.45249999\n",
                                    " 0.055     ]\n",
                                    "Temperature Policy  [0.05    0.24    0.045   0.10625 0.05125 0.4525  0.055  ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1202, 0.0949, 0.1362, 0.1490, 0.1563, 0.1628, 0.1807],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0711, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.02625    0.03       0.0325     0.65499997 0.19\n",
                                    " 0.03375   ]\n",
                                    "Temperature Policy  [0.0325  0.02625 0.03    0.0325  0.655   0.19    0.03375]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0889, 0.0810, 0.1403, 0.1481, 0.1821, 0.1330, 0.2266],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1592, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.13124999 0.17       0.12375    0.03875    0.36750001\n",
                                    " 0.11875   ]\n",
                                    "Temperature Policy  [0.05    0.13125 0.17    0.12375 0.03875 0.3675  0.11875]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1104, 0.1024, 0.1188, 0.1786, 0.1441, 0.1352, 0.2105],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2477, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.44499999 0.03       0.0325     0.23375    0.20375\n",
                                    " 0.0375    ]\n",
                                    "Temperature Policy  [0.0175  0.445   0.03    0.0325  0.23375 0.20375 0.0375 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0904, 0.0697, 0.1405, 0.1748, 0.1818, 0.1412, 0.2018],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1657, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.0175     0.01875    0.02125    0.88125002 0.02\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.0175  0.0175  0.01875 0.02125 0.88125 0.02    0.02375]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1149, 0.0943, 0.1128, 0.1916, 0.1336, 0.1378, 0.2150],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2410, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.04       0.03375    0.04375    0.23875    0.58249998\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.0175  0.04    0.03375 0.04375 0.23875 0.5825  0.04375]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0693, 0.0766, 0.1556, 0.1607, 0.1837, 0.1438, 0.2103],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1805, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.03125    0.77749997 0.03125    0.03875    0.04       0.03875\n",
                                    " 0.0425    ]\n",
                                    "Temperature Policy  [0.03125 0.7775  0.03125 0.03875 0.04    0.03875 0.0425 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1002, 0.0824, 0.1496, 0.1450, 0.1576, 0.1363, 0.2290],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2178, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0175     0.87625003 0.0225     0.01875    0.02       0.02125\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.0175  0.87625 0.0225  0.01875 0.02    0.02125 0.02375]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0858, 0.0923, 0.1944, 0.1355, 0.1620, 0.1162, 0.2137],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2239, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.015 0.88  0.02  0.02  0.02  0.02  0.025]\n",
                                    "Temperature Policy  [0.015 0.88  0.02  0.02  0.02  0.02  0.025]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1283, 0.0000, 0.1430, 0.1481, 0.1889, 0.1569, 0.2349],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0163, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.         0.06875    0.17625    0.04       0.64999998\n",
                                    " 0.045     ]\n",
                                    "Temperature Policy  [0.02    0.06875 0.17625 0.04    0.65    0.045  ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1104, 0.0000, 0.1977, 0.1445, 0.2235, 0.1370, 0.1868],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1364, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02       0.         0.0175     0.01875    0.02       0.90499997\n",
                                    " 0.01875   ]\n",
                                    "Temperature Policy  [0.02    0.0175  0.01875 0.02    0.905   0.01875]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1597, 0.0000, 0.1981, 0.1783, 0.2053, 0.0000, 0.2586],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0988, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125    0.         0.19499999 0.20999999 0.23125    0.\n",
                                    " 0.3425    ]\n",
                                    "Temperature Policy  [8.2000997e-13 3.4719694e-03 7.2848853e-03 1.9100608e-02 9.7014248e-01]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1297, 0.0000, 0.2212, 0.1420, 0.2518, 0.0000, 0.2555],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2860, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.         0.1025     0.06       0.05125    0.\n",
                                    " 0.75375003]\n",
                                    "Temperature Policy  [2.2210605e-14 2.1625464e-09 1.0215021e-11 2.1118617e-12 1.0000000e+00]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1404, 0.0000, 0.1892, 0.1668, 0.2007, 0.0000, 0.3029],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0162, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02125    0.         0.18625    0.36500001 0.27000001 0.\n",
                                    " 0.1575    ]\n",
                                    "Temperature Policy  [4.2586654e-13 1.1393233e-03 9.5194715e-01 4.6700530e-02 2.1305402e-04]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1468, 0.0000, 0.2598, 0.0000, 0.2535, 0.0000, 0.3399],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2411, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.16875    0.         0.16875    0.         0.41374999 0.\n",
                                    " 0.24875   ]\n",
                                    "Temperature Policy  [1.2655422e-04 1.2655422e-04 9.9361670e-01 6.1301300e-03]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1934, 0.0000, 0.1873, 0.0000, 0.2466, 0.0000, 0.3726],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1967, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02625    0.         0.80374998 0.         0.05875    0.\n",
                                    " 0.11125   ]\n",
                                    "Temperature Policy  [1.3806489e-15 1.0000000e+00 4.3538038e-12 2.5810141e-09]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.2184, 0.0000, 0.0000, 0.0000, 0.3738, 0.0000, 0.4078],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0779, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.75749999 0.         0.         0.         0.10625    0.\n",
                                    " 0.13625   ]\n",
                                    "Temperature Policy  [1.0000000e+00 2.9475622e-09 3.5443673e-08]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.2475, 0.0000, 0.0000, 0.0000, 0.2921, 0.0000, 0.4604],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2111, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.94499999 0.         0.         0.         0.025      0.\n",
                                    " 0.03      ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.6791211e-16 1.0396675e-15]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.2591, 0.0000, 0.0000, 0.0000, 0.3001, 0.0000, 0.4408],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0650, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.56375003 0.         0.         0.         0.2025     0.\n",
                                    " 0.23375   ]\n",
                                    "Temperature Policy  [9.9981415e-01 3.5752302e-05 1.5016543e-04]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.2213, 0.0000, 0.0000, 0.0000, 0.3707, 0.0000, 0.4079],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0657, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.94375002 0.         0.         0.         0.02375    0.\n",
                                    " 0.0325    ]\n",
                                    "Temperature Policy  [1.0000000e+00 1.0187474e-16 2.3456542e-15]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.2508, 0.0000, 0.0000, 0.0000, 0.3296, 0.0000, 0.4197],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0160, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.26750001 0.         0.         0.         0.32249999 0.\n",
                                    " 0.41      ]\n",
                                    "Temperature Policy  [0.0126525  0.08207969 0.90526783]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.4442, 0.0000, 0.0000, 0.0000, 0.5558, 0.0000, 0.0000],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0183, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.4325 0.     0.     0.     0.5675 0.     0.    ]\n",
                                    "Temperature Policy  [0.06200182 0.9379982 ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([1., 0., 0., 0., 0., 0., 0.], device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0990, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [1. 0. 0. 0. 0. 0. 0.]\n",
                                    "Temperature Policy  [1.]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
                                    "Updated Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                    "Training Game  12\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.02625    0.04625    0.03875    0.74874997 0.03875\n",
                                    " 0.06875   ]\n",
                                    "Temperature Policy  [0.0325  0.02625 0.04625 0.03875 0.74875 0.03875 0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06375    0.0325     0.0525     0.70249999 0.04625    0.03875\n",
                                    " 0.06375   ]\n",
                                    "Temperature Policy  [0.06375 0.0325  0.0525  0.7025  0.04625 0.03875 0.06375]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.0898, 0.1109, 0.2332, 0.1030, 0.1526, 0.1571, 0.1534],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0798, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0325     0.04       0.06875    0.04625    0.70375001 0.0575\n",
                                    " 0.05125   ]\n",
                                    "Temperature Policy  [0.0325  0.04    0.06875 0.04625 0.70375 0.0575  0.05125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1120, 0.0838, 0.1665, 0.1376, 0.1687, 0.1558, 0.1756],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0129, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.15375    0.06125    0.04625    0.03875    0.59249997 0.05625\n",
                                    " 0.05125   ]\n",
                                    "Temperature Policy  [0.15375 0.06125 0.04625 0.03875 0.5925  0.05625 0.05125]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1185, 0.1095, 0.1809, 0.1018, 0.1617, 0.1375, 0.1902],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1144, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.05       0.0825     0.0525     0.035      0.07       0.64999998\n",
                                    " 0.06      ]\n",
                                    "Temperature Policy  [0.05   0.0825 0.0525 0.035  0.07   0.65   0.06  ]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1264, 0.1099, 0.1923, 0.1363, 0.1333, 0.1447, 0.1571],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0436, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.12125    0.06       0.06125    0.03       0.0375     0.64499998\n",
                                    " 0.045     ]\n",
                                    "Temperature Policy  [0.12125 0.06    0.06125 0.03    0.0375  0.645   0.045  ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0919, 0.1003, 0.1889, 0.1060, 0.1965, 0.1311, 0.1853],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0374, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06       0.04125    0.54874998 0.03       0.24250001 0.03125\n",
                                    " 0.04625   ]\n",
                                    "Temperature Policy  [0.06    0.04125 0.54875 0.03    0.2425  0.03125 0.04625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1072, 0.0959, 0.1901, 0.1469, 0.1667, 0.1077, 0.1856],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1981, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0475     0.04125    0.75625002 0.03625    0.04125    0.03375\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.0475  0.04125 0.75625 0.03625 0.04125 0.03375 0.04375]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.0785, 0.0735, 0.2090, 0.1411, 0.1930, 0.1061, 0.1988],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0310, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.16       0.03875    0.58999997 0.04875    0.07125    0.0425\n",
                                    " 0.04875   ]\n",
                                    "Temperature Policy  [0.16    0.03875 0.59    0.04875 0.07125 0.0425  0.04875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1389, 0.0934, 0.1733, 0.1337, 0.1724, 0.1161, 0.1721],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1798, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.045   0.04125 0.07    0.035   0.6925  0.03875 0.0775 ]\n",
                                    "Temperature Policy  [0.045   0.04125 0.07    0.035   0.6925  0.03875 0.0775 ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0868, 0.0807, 0.1750, 0.1544, 0.1825, 0.1342, 0.1863],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.2330, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.035      0.0475     0.04625    0.75125003 0.03625\n",
                                    " 0.055     ]\n",
                                    "Temperature Policy  [0.02875 0.035   0.0475  0.04625 0.75125 0.03625 0.055  ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1371, 0.1177, 0.1931, 0.1714, 0.0000, 0.1566, 0.2241],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3159, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0525     0.28625    0.07375    0.03375    0.         0.24124999\n",
                                    " 0.3125    ]\n",
                                    "Temperature Policy  [0.0525  0.28625 0.07375 0.03375 0.24125 0.3125 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1105, 0.0985, 0.2028, 0.2299, 0.0000, 0.1413, 0.2170],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1190, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0475     0.30375001 0.0425     0.04625    0.         0.05625\n",
                                    " 0.50375003]\n",
                                    "Temperature Policy  [0.0475  0.30375 0.0425  0.04625 0.05625 0.50375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1360, 0.1271, 0.1883, 0.1481, 0.0000, 0.1728, 0.2277],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4392, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.06125    0.0625     0.13375001 0.04625    0.         0.61000001\n",
                                    " 0.08625   ]\n",
                                    "Temperature Policy  [0.06125 0.0625  0.13375 0.04625 0.61    0.08625]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1204, 0.1267, 0.1964, 0.1984, 0.0000, 0.1296, 0.2285],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0057, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0475     0.04375    0.03875    0.14375    0.         0.68624997\n",
                                    " 0.04      ]\n",
                                    "Temperature Policy  [0.0475  0.04375 0.03875 0.14375 0.68625 0.04   ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1465, 0.1209, 0.1670, 0.2003, 0.0000, 0.1355, 0.2299],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0533, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01875    0.02       0.01875    0.89999998 0.         0.02\n",
                                    " 0.0225    ]\n",
                                    "Temperature Policy  [0.01875 0.02    0.01875 0.9     0.02    0.0225 ]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1049, 0.1125, 0.2114, 0.2059, 0.0000, 0.1551, 0.2101],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0908, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.11875    0.25874999 0.0925     0.16500001 0.         0.31375\n",
                                    " 0.05125   ]\n",
                                    "Temperature Policy  [0.11875 0.25875 0.0925  0.165   0.31375 0.05125]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1276, 0.1259, 0.1828, 0.1649, 0.0000, 0.1344, 0.2643],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0676, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.01875    0.0225     0.89375001 0.0225     0.         0.01875\n",
                                    " 0.02375   ]\n",
                                    "Temperature Policy  [0.01875 0.0225  0.89375 0.0225  0.01875 0.02375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1234, 0.0970, 0.1923, 0.1871, 0.0000, 0.1163, 0.2838],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.3142, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.11125 0.11125 0.0975  0.12625 0.      0.44125 0.1125 ]\n",
                                    "Temperature Policy  [0.11125 0.11125 0.0975  0.12625 0.44125 0.1125 ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1284, 0.1137, 0.1479, 0.1748, 0.0000, 0.1487, 0.2865],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1175, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0225     0.61750001 0.21625    0.08375    0.         0.02375\n",
                                    " 0.03625   ]\n",
                                    "Temperature Policy  [0.0225  0.6175  0.21625 0.08375 0.02375 0.03625]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1322, 0.1113, 0.1887, 0.1715, 0.0000, 0.1234, 0.2729],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2422, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.65125 0.06875 0.07    0.05375 0.      0.075   0.08125]\n",
                                    "Temperature Policy  [0.65125 0.06875 0.07    0.05375 0.075   0.08125]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1406, 0.1210, 0.1777, 0.1683, 0.0000, 0.1284, 0.2639],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0059, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.91500002 0.01875    0.01625    0.01625    0.         0.01375\n",
                                    " 0.02      ]\n",
                                    "Temperature Policy  [0.915   0.01875 0.01625 0.01625 0.01375 0.02   ]\n",
                                    "Action  0\n",
                                    "Initial Rewards [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [-1, 1]]\n",
                                    "Updated Rewards [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Game  13\n",
                                    "Predicted Policy  tensor([0.0891, 0.0803, 0.1537, 0.1790, 0.1757, 0.1098, 0.2124],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2949, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035   0.03    0.045   0.04125 0.745   0.035   0.06875]\n",
                                    "Temperature Policy  [0.035   0.03    0.045   0.04125 0.745   0.035   0.06875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1220, 0.0948, 0.2011, 0.1220, 0.1437, 0.1371, 0.1794],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2475, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.065   0.03625 0.0575  0.05875 0.0575  0.04    0.685  ]\n",
                                    "Temperature Policy  [0.065   0.03625 0.0575  0.05875 0.0575  0.04    0.685  ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1068, 0.0889, 0.1941, 0.1076, 0.1372, 0.1540, 0.2114],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1075, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04875 0.03    0.0425  0.02875 0.04125 0.06125 0.7475 ]\n",
                                    "Temperature Policy  [0.04875 0.03    0.0425  0.02875 0.04125 0.06125 0.7475 ]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.1126, 0.1050, 0.1471, 0.1418, 0.1375, 0.1328, 0.2233],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2170, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04625    0.03       0.035      0.0425     0.04125    0.04125\n",
                                    " 0.76375002]\n",
                                    "Temperature Policy  [0.04625 0.03    0.035   0.0425  0.04125 0.04125 0.76375]\n",
                                    "Action  6\n",
                                    "Predicted Policy  tensor([0.0927, 0.0780, 0.1529, 0.1507, 0.1517, 0.1466, 0.2274],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2051, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0475     0.04       0.05375    0.045      0.035      0.04125\n",
                                    " 0.73750001]\n",
                                    "Temperature Policy  [0.0475  0.04    0.05375 0.045   0.035   0.04125 0.7375 ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  tensor([0.1292, 0.1108, 0.1781, 0.1101, 0.1200, 0.1499, 0.2019],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1397, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.04875    0.0375     0.03875    0.03875    0.22875001 0.56375003\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.04875 0.0375  0.03875 0.03875 0.22875 0.56375 0.04375]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.0806, 0.0929, 0.1550, 0.1442, 0.1567, 0.1339, 0.2367],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0407, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.80124998 0.03625    0.02625    0.0425     0.03125\n",
                                    " 0.03375   ]\n",
                                    "Temperature Policy  [0.02875 0.80125 0.03625 0.02625 0.0425  0.03125 0.03375]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.1319, 0.0981, 0.1389, 0.1439, 0.1575, 0.1035, 0.2262],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2668, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.73250002 0.03875    0.04875    0.04       0.055      0.03125\n",
                                    " 0.05375   ]\n",
                                    "Temperature Policy  [0.7325  0.03875 0.04875 0.04    0.055   0.03125 0.05375]\n",
                                    "Action  0\n",
                                    "Predicted Policy  tensor([0.0903, 0.0874, 0.1503, 0.1462, 0.1359, 0.1976, 0.1923],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.0223, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.0275     0.78500003 0.02375    0.055      0.03875\n",
                                    " 0.04125   ]\n",
                                    "Temperature Policy  [0.02875 0.0275  0.785   0.02375 0.055   0.03875 0.04125]\n",
                                    "Action  3\n",
                                    "Predicted Policy  tensor([0.1187, 0.1027, 0.1835, 0.1101, 0.1693, 0.1199, 0.1958],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.4034, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0225  0.87    0.0225  0.0225  0.02125 0.01875 0.0225 ]\n",
                                    "Temperature Policy  [0.0225  0.87    0.0225  0.0225  0.02125 0.01875 0.0225 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.0901, 0.1154, 0.1526, 0.1344, 0.1675, 0.1409, 0.1991],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(-0.1211, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02375 0.78375 0.04375 0.03375 0.0375  0.04    0.0375 ]\n",
                                    "Temperature Policy  [0.02375 0.78375 0.04375 0.03375 0.0375  0.04    0.0375 ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  tensor([0.1151, 0.0984, 0.1782, 0.1140, 0.1429, 0.1407, 0.2106],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2641, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.0425     0.03625    0.03875    0.035      0.74874997 0.04\n",
                                    " 0.05875   ]\n",
                                    "Temperature Policy  [0.0425  0.03625 0.03875 0.035   0.74875 0.04    0.05875]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.0835, 0.1274, 0.1694, 0.1462, 0.1500, 0.1208, 0.2028],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.0236, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.02875    0.11125    0.0475     0.03       0.70625001 0.0325\n",
                                    " 0.04375   ]\n",
                                    "Temperature Policy  [0.02875 0.11125 0.0475  0.03    0.70625 0.0325  0.04375]\n",
                                    "Action  4\n",
                                    "Predicted Policy  tensor([0.1191, 0.1039, 0.1973, 0.1330, 0.1342, 0.1471, 0.1655],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.1324, device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Target Policy [0.035      0.03       0.0425     0.03875    0.03125    0.77749997\n",
                                    " 0.045     ]\n",
                                    "Temperature Policy  [0.035   0.03    0.0425  0.03875 0.03125 0.7775  0.045  ]\n",
                                    "Action  5\n",
                                    "Predicted Policy  tensor([0.1186, 0.1259, 0.1390, 0.1394, 0.1500, 0.1160, 0.2112],\n",
                                    "       device='mps:0', grad_fn=<SelectBackward0>)\n",
                                    "Predicted Value  tensor(0.2245, device='mps:0', grad_fn=<SelectBackward0>)\n"
                              ]
                        },
                        {
                              "ename": "KeyboardInterrupt",
                              "evalue": "",
                              "output_type": "error",
                              "traceback": [
                                    "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                                    "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                                    "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:105\u001b[0m, in \u001b[0;36mAlphaZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m training_game \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgames_per_generation):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Game \u001b[39m\u001b[38;5;124m\"\u001b[39m, training_game \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     score, num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     total_environment_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_steps\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: score})  \u001b[38;5;66;03m# score for player one\u001b[39;00m\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:290\u001b[0m, in \u001b[0;36mAlphaZeroAgent.play_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexploitation_temperature\n\u001b[0;32m--> 290\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Policy\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemperature Policy \u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction[\u001b[38;5;241m0\u001b[39m])\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:258\u001b[0m, in \u001b[0;36mAlphaZeroAgent.predict\u001b[0;34m(self, state, info, env, temperature, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, info: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 258\u001b[0m     visit_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonte_carlo_tree_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     actions \u001b[38;5;241m=\u001b[39m [action \u001b[38;5;28;01mfor\u001b[39;00m _, action \u001b[38;5;129;01min\u001b[39;00m visit_counts]\n\u001b[1;32m    260\u001b[0m     visit_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([count \u001b[38;5;28;01mfor\u001b[39;00m count, _ \u001b[38;5;129;01min\u001b[39;00m visit_counts], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:159\u001b[0m, in \u001b[0;36mAlphaZeroAgent.monte_carlo_tree_search\u001b[0;34m(self, env, state, info)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# GO UNTIL A LEAF NODE IS REACHED\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m node\u001b[38;5;241m.\u001b[39mexpanded():\n\u001b[0;32m--> 159\u001b[0m     action, node \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_child\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpb_c_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpb_c_init\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     _, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m mcts_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    163\u001b[0m     search_path\u001b[38;5;241m.\u001b[39mappend(node)\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/../alphazero/alphazero_mcts.py:54\u001b[0m, in \u001b[0;36mNode.select_child\u001b[0;34m(self, pb_c_base, pb_c_init)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_child\u001b[39m(\u001b[38;5;28mself\u001b[39m, pb_c_base, pb_c_init):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Select the child with the highest UCB\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     _, action, child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_ucb_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpb_c_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpb_c_init\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# print(\"Selected Action\", action)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# print(\"Selected Child Visits\", child.visits)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# print(\"Selected Child Value\", child.value())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m#     print(\"Child Prior Policy\", c.prior_policy)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#     print(\"Child UCB\", self.child_ucb_score(c, pb_c_base, pb_c_init))\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action, child\n",
                                    "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                              ]
                        }
                  ],
                  "source": [
                        "agent.checkpoint_interval = 1\n",
                        "agent.train()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "array = np.zeros((3, 9))\n",
                        "array[0] = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                        "print(array)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import gymnasium as gym\n",
                        "\n",
                        "env = gym.make(\"custom_gym_envs/TicTacToe-v0\")\n",
                        "\n",
                        "state, info = env.reset()\n",
                        "agent.predict_no_mcts(state, info)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import gymnasium as gym\n",
                        "\n",
                        "env = gym.make(\"custom_gym_envs/TicTacToe-v0\")\n",
                        "\n",
                        "state, info = env.reset()\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(0)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(4)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(3)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(6)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(2)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(1)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(7)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(8)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(5)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "print(\"Truncated:\", truncated)\n",
                        "env.render()\n",
                        "\n",
                        "\n",
                        "env.reset()\n",
                        "state, reward, terminated, truncated, info = env.step(0)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(3)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(7)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(4)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(2)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(6)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(1)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info[\"legal_moves\"])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "print(\"Truncated:\", truncated)\n",
                        "print(\"Reward:\", reward)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from alphazero_agent import AlphaZeroAgent\n",
                        "from agent_configs import AlphaZeroConfig\n",
                        "from game_configs import TicTacToeConfig\n",
                        "\n",
                        "# from alphazero_agent import AlphaZeroAgent\n",
                        "import gymnasium as gym\n",
                        "import numpy as np\n",
                        "import custom_gym_envs\n",
                        "\n",
                        "\n",
                        "class ClipReward(gym.RewardWrapper):\n",
                        "    def __init__(self, env, min_reward, max_reward):\n",
                        "        super().__init__(env)\n",
                        "        self.min_reward = min_reward\n",
                        "        self.max_reward = max_reward\n",
                        "        self.reward_range = (min_reward, max_reward)\n",
                        "\n",
                        "    def reward(self, reward):\n",
                        "        return np.clip(reward, self.min_reward, self.max_reward)\n",
                        "\n",
                        "\n",
                        "# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
                        "# env = TicTacToeEnv(render_mode=\"rgb_array\")\n",
                        "# env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
                        "# env = gym.wrappers.FrameStack(env, 4)\n",
                        "env = gym.make(\"custom_gym_envs/Connect4-v0\", render_mode=\"rgb_array\")\n",
                        "\n",
                        "\n",
                        "# MODEL SEEMS TO BE UNDERFITTING SO TRY AND GET IT TO OVERFIT THEN FIND A HAPPY MEDIUM\n",
                        "# 1. INCREASE THE NUMBER OF RESIDUAL BLOCKS\n",
                        "# 2. INCREASE THE NUMBER OF FILTERS\n",
                        "# 3. DECREASE REGULARIZATION\n",
                        "# 4. TRY DECREASING LEARNING RATE (maybe its that whole thing where the policy goes to like 1 0 0 0 0... etc and then goes back on the third training step, so maybe the learning rate is too high)\n",
                        "# 5. TO OVERFIT USE LESS DATA (but that is probably just a bad idea)\n",
                        "# config = {\n",
                        "#         'activation': 'relu',\n",
                        "#         'kernel_initializer': 'glorot_uniform',\n",
                        "#         'optimizer': tf.keras.optimizers.legacy.Adam,\n",
                        "#         'learning_rate': 0.001, # 0.00001 could maybe increase by a factor of 10 or 100 and try to do some weights regularization\n",
                        "#         'adam_epsilon': 3.25e-6,\n",
                        "#         'clipnorm': None,\n",
                        "#         # NORMALIZATION?\n",
                        "#         # REWARD CLIPPING\n",
                        "#         'training_steps': 40,\n",
                        "#         'num_filters': 256,\n",
                        "#         'kernel_size': 3,\n",
                        "#         'stride': 1,\n",
                        "#         'num_res_blocks': 20,\n",
                        "#         'critic_conv_filters': 32, # 1\n",
                        "#         'critic_conv_layers': 1,\n",
                        "#         'critic_dense_size': 256,\n",
                        "#         'critic_dense_layers': 1,\n",
                        "#         'actor_conv_filters': 32, #\n",
                        "#         'actor_conv_layers': 1,\n",
                        "#         'actor_dense_size': 0,\n",
                        "#         'actor_dense_layers': 0,\n",
                        "#         'replay_buffer_size': 800, # IN GAMES\n",
                        "#         'replay_batch_size': 50, # IN MOVES\n",
                        "#         'root_dirichlet_alpha': 0.5, # 2 in theory?\n",
                        "#         'root_exploration_fraction': 0, # 0.25 in paper\n",
                        "#         'pb_c_base': 500,\n",
                        "#         'pb_c_init': 2,\n",
                        "#         'num_simulations': 200,\n",
                        "#         # 'two_player': True,\n",
                        "#         'weight_decay': 0.00, # could try setting this to something other than 0 and increasing learning rate\n",
                        "#         'num_sampling_moves': 0,\n",
                        "#         'initial_temperature': 1,\n",
                        "#         'exploitation_temperature': 0.1,\n",
                        "#         'value_loss_factor': 1, # could try setting this to something other than 1\n",
                        "#         'games_per_generation': 10, # times 8 from augmentation\n",
                        "#     }\n",
                        "\n",
                        "config = {\n",
                        "    \"activation\": \"relu\",\n",
                        "    \"kernel_initializer\": \"glorot_uniform\",\n",
                        "    \"optimizer\": tf.keras.optimizers.legacy.Adam,\n",
                        "    \"learning_rate\": 0.0005,  # 0.0001 # 0.00001 could maybe increase by a factor of 10 or 100 and try to do some weights regularization\n",
                        "    \"number_of_lr_cycles\": 1,  # this will determine the step size based on training steps\n",
                        "    # STILL ADD A SCHEDULE FOR BASE LEARNING RATE (MIN LEARNING RATE)\n",
                        "    \"adam_epsilon\": 3.25e-6,\n",
                        "    \"clipnorm\": None,\n",
                        "    # NORMALIZATION?\n",
                        "    # REWARD CLIPPING\n",
                        "    \"training_steps\": 100,  # alpha zero did 700,000, the lessons from alpha zero did 40 generations but 1000 batches per generation, so 40,000 batches (they just had a cyclical learning rate per generation (also they trained twice on the same data every generation))\n",
                        "    \"num_filters\": 256,\n",
                        "    \"kernel_size\": 3,\n",
                        "    \"stride\": 1,\n",
                        "    \"residual_blocks\": 20,\n",
                        "    \"critic_conv_filters\": 32,  # 1\n",
                        "    \"critic_conv_layers\": 1,\n",
                        "    \"critic_dense_size\": 256,\n",
                        "    \"critic_dense_layers\": 1,\n",
                        "    \"actor_conv_filters\": 32,  #\n",
                        "    \"actor_conv_layers\": 1,\n",
                        "    \"actor_dense_size\": 0,\n",
                        "    \"actor_dense_layers\": 0,\n",
                        "    \"replay_buffer_size\": 100,  # IN GAMES\n",
                        "    \"minibatch_size\": 24,  # SHOULD BE ROUGHLY SAME AS AVERAGE MOVE PER GENERATION (SO LIKE 7 TIMES NUMBER OF GAMES PLAYED PER GENERATION) <- what was used in the original paper (they played 44M games, 50 moves per game and sampled 700,000 minibatches of size 4096 (so thats like sampling 1 time per move roughly but this was also happening with parrallel data collection i believe))\n",
                        "    \"games_per_generation\": 1,  # times 8 from augmentation\n",
                        "    \"root_dirichlet_alpha\": 2.5,  # Less than 1 more random, greater than one more flat # 2 in theory? # 0.3 in alphazero for chess # TRY CHANGING (MAYBE LOWER? (IT SEEMS TO PLAY THE SAME LINE OVER AND OVER AGAIN <- so we want a lesss flat distribution maybe)\n",
                        "    \"root_exploration_fraction\": 0.25,  # 0.25 in paper\n",
                        "    \"pb_c_base\": 20000,  # Seems unimportant to be honest (increases puct the more simulations there are)\n",
                        "    \"pb_c_init\": 1.25,  # 1.25 in paper # MAYBE HIGHER? (IT SEEMS TO PLAY THE SAME LINE OVER AND OVER AGAIN)\n",
                        "    \"num_simulations\": 50,  # INCREASE THIS since the model is missing 1 move wins (and also 2 and 3 move wins (it wins by luck)))\n",
                        "    # 'two_player': True,\n",
                        "    \"weight_decay\": 0.00001,  # could try setting this to something other than 0 and increasing learning rate\n",
                        "    \"num_sampling_moves\": 30,\n",
                        "    \"exploration_temperature\": 1,\n",
                        "    \"exploitation_temperature\": 0.1,\n",
                        "    \"value_loss_factor\": 1,  # could try setting this to something other than 1\n",
                        "    \"loss_function\": None,\n",
                        "}\n",
                        "\n",
                        "config = AlphaZeroConfig(config, TicTacToeConfig())\n",
                        "\n",
                        "agent = AlphaZeroAgent(env, config, \"alphazero\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "agent.checkpoint_interval = 1\n",
                        "agent.train()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "agent.model.load_weights(\"./alphazero.keras\")\n",
                        "agent.train()"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.10.14"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
