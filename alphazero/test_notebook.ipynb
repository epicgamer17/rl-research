{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "indices = np.random.choice(10, 5, replace=True)\n",
    "arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(arr[indices])\n",
    "print(arr[-11:])\n",
    "reward_buffer = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1]\n",
    "size = 7\n",
    "game_start_index = 0\n",
    "reward = reward_buffer[size - 1]\n",
    "updated_rewards = np.empty((size - game_start_index,), int)\n",
    "updated_rewards[::2] = 1 * reward\n",
    "updated_rewards[1::2] = -1 * reward\n",
    "updated_rewards = np.flip(updated_rewards)\n",
    "print(updated_rewards)\n",
    "reward_buffer[game_start_index:size] = updated_rewards\n",
    "print(reward_buffer)\n",
    "game_start_index = 7\n",
    "size = 15\n",
    "print(len(reward_buffer))\n",
    "reward = reward_buffer[size - 1]\n",
    "updated_rewards = np.empty((size - game_start_index,), int)\n",
    "updated_rewards[::2] = 1 * reward\n",
    "updated_rewards[1::2] = -1 * reward\n",
    "updated_rewards = np.flip(updated_rewards)\n",
    "print(updated_rewards)\n",
    "reward_buffer[game_start_index:size] = updated_rewards\n",
    "print(reward_buffer)\n",
    "print(len(reward_buffer))\n",
    "overflow_reward = 1\n",
    "overflow_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_buffer = np.array([1, -1, 1, -1, 1, -1, 0])\n",
    "# reward_buffer[6] = [1, 1, 1, 1]\n",
    "# print(reward_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# from alphazero_agent import AlphaZeroAgent\n",
    "import gymnasium as gym \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import gym_envs\n",
    "\n",
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "\n",
    "\n",
    "# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "env = TicTacToeEnv()\n",
    "# env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "config = {\n",
    "        'activation': 'relu',\n",
    "        'kernel_initializer': 'orthogonal',\n",
    "        'optimizer_function': tf.keras.optimizers.legacy.Adam,\n",
    "        'learning_rate': 0.2,\n",
    "        'adam_epsilon': 1e-7,\n",
    "        'clipnorm': 0.5,\n",
    "        # NORMALIZATION?\n",
    "        # REWARD CLIPPING\n",
    "        'num_epochs': 30,\n",
    "        'num_filters': 32,\n",
    "        'kernel_size': 3,\n",
    "        'stride': 1,\n",
    "        'num_res_blocks': 5,\n",
    "        'critic_conv_filters': 32,\n",
    "        'critic_conv_layers': 1,\n",
    "        'critic_dense_size': 32,\n",
    "        'critic_dense_layers': 1,\n",
    "        'actor_conv_filters': 32,\n",
    "        'actor_conv_layers': 1,\n",
    "        'actor_dense_size': 32,\n",
    "        'actor_dense_layers': 1,\n",
    "        'memory_size': 180,\n",
    "        'max_game_length': 9,\n",
    "        'batch_size': 9,\n",
    "        'dirichlet_alpha': 0.3,\n",
    "        'dirichlet_epsilon': 0.25,\n",
    "        'c_puct': 1,\n",
    "        'monte_carlo_simulations': 18,\n",
    "        'two_player': True,\n",
    "        'weight_decay': 1e-4,\n",
    "    }    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "MCTS Simulation 0\n",
      "MCTS Simulation 1\n",
      "(1, 4, 210, 160, 3)\n",
      "(1, 4, 210, 160, 32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'batch_normalization' (type BatchNormalization).\n\n{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} input must be 4-dimensional[1,4,210,160,32] [Op:FusedBatchNormV3] name: \n\nCall arguments received by layer 'batch_normalization' (type BatchNormalization):\n  • inputs=tf.Tensor(shape=(1, 4, 210, 160, 32), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m AlphaZeroAgent(env, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:234\u001b[0m, in \u001b[0;36mAlphaZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep\u001b[39m\u001b[38;5;124m\"\u001b[39m, step)\n\u001b[1;32m    229\u001b[0m legal_moves \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m     info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegal_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegal_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions)\n\u001b[1;32m    233\u001b[0m )\n\u001b[0;32m--> 234\u001b[0m action_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonte_carlo_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegal_moves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonte_carlo_simulations\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition \u001b[38;5;241m=\u001b[39m [state, action_probabilities]\n\u001b[1;32m    238\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    239\u001b[0m     np\u001b[38;5;241m.\u001b[39margmax(action_probabilities)\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:163\u001b[0m, in \u001b[0;36mAlphaZeroAgent.monte_carlo_search\u001b[0;34m(self, env, observation, legal_moves, num_simulations)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_simulations):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCTS Simulation\u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m prob_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m9\u001b[39m))\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, node \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:194\u001b[0m, in \u001b[0;36mAlphaZeroAgent.explore\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m    191\u001b[0m     current_node \u001b[38;5;241m=\u001b[39m children[action_selected]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_node \u001b[38;5;241m!=\u001b[39m root:\n\u001b[0;32m--> 194\u001b[0m     probabilities, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     puct_score \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_puct \u001b[38;5;241m*\u001b[39m probabilities[\n\u001b[1;32m    196\u001b[0m         current_node\u001b[38;5;241m.\u001b[39mparent_action\n\u001b[1;32m    197\u001b[0m     ] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(current_node\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mvisits) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m current_node\u001b[38;5;241m.\u001b[39mvisits)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# print(\"MCTS PUCT\", puct_score)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:118\u001b[0m, in \u001b[0;36mAlphaZeroAgent.predict_single\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    116\u001b[0m state_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_states(state)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 118\u001b[0m value, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(probabilities\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m], value\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions), value\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_network.py:46\u001b[0m, in \u001b[0;36mNetwork.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs(inputs)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_batch_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m residual \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresiduals:\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual(x)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'batch_normalization' (type BatchNormalization).\n\n{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} input must be 4-dimensional[1,4,210,160,32] [Op:FusedBatchNormV3] name: \n\nCall arguments received by layer 'batch_normalization' (type BatchNormalization):\n  • inputs=tf.Tensor(shape=(1, 4, 210, 160, 32), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "agent = AlphaZeroAgent(env, config=config)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.zeros((3, 9))\n",
    "array[0] = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_envs\n",
    "import gymnasium as gym\n",
    "env = gym.make('gym_envs/TicTacToe-v0')\n",
    "\n",
    "state, info = env.reset()\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(0)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(4)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(3)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(6)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(2)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(1)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "env.render()\n",
    "state, reward, terminated, truncated, info = env.step(7)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(8)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(5)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "print(\"Truncated:\", truncated)\n",
    "env.render()\n",
    "\n",
    "\n",
    "env.reset()\n",
    "state, reward, terminated, truncated, info = env.step(0)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(3)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(7)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(4)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(2)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(6)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "state, reward, terminated, truncated, info = env.step(1)\n",
    "print(state)\n",
    "print(\"Turn: \", state[2][0][0])\n",
    "print(\"Legal moves: \", info['legal_moves'])\n",
    "print(\"Terminated:\", terminated)\n",
    "print(\"Truncated:\", truncated)\n",
    "print(\"Reward:\", reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_4_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [3,3,3,32] != values[1].shape = [32]\n\t [[{{node Pack}}]] [Op:Pack] name: packed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m regularizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39mL2(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mregularizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/regularizers.py:322\u001b[0m, in \u001b[0;36mL2.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# equivalent to \"self.l2 * tf.reduce_sum(tf.square(x))\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2 \u001b[38;5;241m*\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml2_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/gen_nn_ops.py:5749\u001b[0m, in \u001b[0;36ml2_loss\u001b[0;34m(t, name)\u001b[0m\n\u001b[1;32m   5747\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   5748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m-> 5749\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ml2_loss_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5750\u001b[0m \u001b[43m      \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   5752\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/gen_nn_ops.py:5790\u001b[0m, in \u001b[0;36ml2_loss_eager_fallback\u001b[0;34m(t, name, ctx)\u001b[0m\n\u001b[1;32m   5789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ml2_loss_eager_fallback\u001b[39m(t: Annotated[Any, TV_L2Loss_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_L2Loss_T]:\n\u001b[0;32m-> 5790\u001b[0m   _attr_T, (t,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5791\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [t]\n\u001b[1;32m   5792\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/array_ops.py:1585\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m inferred_dtype:\n\u001b[1;32m   1584\u001b[0m   v \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[0;32m-> 1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_autopacking_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpacked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/array_ops.py:1492\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   1489\u001b[0m   \u001b[38;5;66;03m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m   \u001b[38;5;66;03m# checking.\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, core\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m list_or_tuple):\n\u001b[0;32m-> 1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_or_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m must_pack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m converted_elems \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/gen_array_ops.py:6718\u001b[0m, in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6716\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   6717\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 6718\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   6720\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_4_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [3,3,3,32] != values[1].shape = [32]\n\t [[{{node Pack}}]] [Op:Pack] name: packed"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "regularizer = tf.keras.regularizers.L2(2)\n",
    "print(regularizer(agent.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02222222 0.04444444 0.06666667 0.08888889 0.11111111 0.13333333\n",
      " 0.15555556 0.17777778 0.2       ]\n",
      "tf.Tensor(\n",
      "[2.1207897e-04 5.7649042e-04 1.5670634e-03 4.2597200e-03 1.1579119e-02\n",
      " 3.1475309e-02 8.5558765e-02 2.3257284e-01 6.3219857e-01], shape=(9,), dtype=float32)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "arr = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "\n",
    "print(arr/np.sum(arr))\n",
    "# print(tf.nn.softmax(arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
