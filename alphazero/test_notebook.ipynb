{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
                                    "  warnings.warn(\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "1 Physical GPUs, 1 Logical GPUs\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "2024-03-29 21:43:59.793545: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
                                    "2024-03-29 21:43:59.793569: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
                                    "2024-03-29 21:43:59.793573: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
                                    "2024-03-29 21:43:59.793818: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
                                    "2024-03-29 21:43:59.794017: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
                              ]
                        },
                        {
                              "ename": "NameError",
                              "evalue": "name 'TicTacToeEnv' is not defined",
                              "output_type": "error",
                              "traceback": [
                                    "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                                    "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                                    "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mclip(reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_reward)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mTicTacToeEnv\u001b[49m()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mFrameStack(env, \u001b[38;5;241m4\u001b[39m)\n",
                                    "\u001b[0;31mNameError\u001b[0m: name 'TicTacToeEnv' is not defined"
                              ]
                        }
                  ],
                  "source": [
                        "from alphazero_agent import AlphaZeroAgent\n",
                        "import gymnasium as gym \n",
                        "import tensorflow as tf\n",
                        "import numpy as np\n",
                        "import gym_envs\n",
                        "\n",
                        "class ClipReward(gym.RewardWrapper):\n",
                        "    def __init__(self, env, min_reward, max_reward):\n",
                        "        super().__init__(env)\n",
                        "        self.min_reward = min_reward\n",
                        "        self.max_reward = max_reward\n",
                        "        self.reward_range = (min_reward, max_reward)\n",
                        "\n",
                        "    def reward(self, reward):\n",
                        "        return np.clip(reward, self.min_reward, self.max_reward)\n",
                        "\n",
                        "\n",
                        "# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
                        "env = TicTacToeEnv()\n",
                        "# env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
                        "env = gym.wrappers.FrameStack(env, 4)\n",
                        "\n",
                        "config = {\n",
                        "        'activation': 'relu',\n",
                        "        'kernel_initializer': 'orthogonal',\n",
                        "        'optimizer_function': tf.keras.optimizers.legacy.Adam,\n",
                        "        'learning_rate': 0.2,\n",
                        "        'adam_epsilon': 1e-7,\n",
                        "        'clipnorm': 0.5,\n",
                        "        # NORMALIZATION?\n",
                        "        # REWARD CLIPPING\n",
                        "        'num_epochs': 30,\n",
                        "        'num_filters': 32,\n",
                        "        'kernel_size': 3,\n",
                        "        'stride': 1,\n",
                        "        'num_res_blocks': 5,\n",
                        "        'critic_conv_filters': 32,\n",
                        "        'critic_conv_layers': 1,\n",
                        "        'critic_dense_size': 32,\n",
                        "        'critic_dense_layers': 1,\n",
                        "        'actor_conv_filters': 32,\n",
                        "        'actor_conv_layers': 1,\n",
                        "        'actor_dense_size': 32,\n",
                        "        'actor_dense_layers': 1,\n",
                        "        'memory_size': 180,\n",
                        "        'max_game_length': 9,\n",
                        "        'batch_size': 9,\n",
                        "        'dirichlet_alpha': 0.3,\n",
                        "        'dirichlet_epsilon': 0.25,\n",
                        "        'c_puct': 1,\n",
                        "        'monte_carlo_simulations': 18,\n",
                        "        'two_player': True,\n",
                        "        'weight_decay': 1e-4,\n",
                        "    }    \n",
                        "\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "array = np.zeros((3, 9))\n",
                        "array[0] = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                        "print(array)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import gym_envs\n",
                        "import gymnasium as gym\n",
                        "env = gym.make('gym_envs/TicTacToe-v0')\n",
                        "\n",
                        "state, info = env.reset()\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(0)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(4)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(3)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(6)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(2)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(1)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "env.render()\n",
                        "state, reward, terminated, truncated, info = env.step(7)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(8)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(5)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "print(\"Truncated:\", truncated)\n",
                        "env.render()\n",
                        "\n",
                        "\n",
                        "env.reset()\n",
                        "state, reward, terminated, truncated, info = env.step(0)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(3)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(7)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(4)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(2)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(6)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "state, reward, terminated, truncated, info = env.step(1)\n",
                        "print(state)\n",
                        "print(\"Turn: \", state[2][0][0])\n",
                        "print(\"Legal moves: \", info['legal_moves'])\n",
                        "print(\"Terminated:\", terminated)\n",
                        "print(\"Truncated:\", truncated)\n",
                        "print(\"Reward:\", reward)\n",
                        "\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
                                    "  warnings.warn(\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "1 Physical GPUs, 1 Logical GPUs\n",
                                    "Box(0.0, 1.0, (6, 7, 3), float64)\n",
                                    "No loss function set\n",
                                    "Using default training iterations: 1\n",
                                    "Using default number of minibatches: 1\n",
                                    "Using default min replay buffer size: 0\n",
                                    "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/alphazero/videos/alphazero\n",
                                    "alphazero\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "2024-03-29 21:46:45.587104: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
                                    "2024-03-29 21:46:45.587127: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
                                    "2024-03-29 21:46:45.587131: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
                                    "2024-03-29 21:46:45.587166: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
                                    "2024-03-29 21:46:45.587185: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
                                    "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/alphazero/videos/alphazero folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
                                    "  logger.warn(\n"
                              ]
                        }
                  ],
                  "source": [
                        "from alphazero_agent import AlphaZeroAgent\n",
                        "from configs.agent_configs.alphazero_config import AlphaZeroConfig\n",
                        "from configs.game_configs.tictactoe_config import TicTacToeConfig\n",
                        "\n",
                        "# from alphazero_agent import AlphaZeroAgent\n",
                        "import gymnasium as gym\n",
                        "import tensorflow as tf\n",
                        "import numpy as np\n",
                        "import gym_envs\n",
                        "\n",
                        "class ClipReward(gym.RewardWrapper):\n",
                        "    def __init__(self, env, min_reward, max_reward):\n",
                        "        super().__init__(env)\n",
                        "        self.min_reward = min_reward\n",
                        "        self.max_reward = max_reward\n",
                        "        self.reward_range = (min_reward, max_reward)\n",
                        "\n",
                        "    def reward(self, reward):\n",
                        "        return np.clip(reward, self.min_reward, self.max_reward)\n",
                        "\n",
                        "\n",
                        "# env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
                        "# env = TicTacToeEnv(render_mode=\"rgb_array\")\n",
                        "# env = gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
                        "# env = gym.wrappers.FrameStack(env, 4)\n",
                        "env = gym.make('gym_envs/Connect4-v0', render_mode=\"rgb_array\")\n",
                        "\n",
                        "\n",
                        "# MODEL SEEMS TO BE UNDERFITTING SO TRY AND GET IT TO OVERFIT THEN FIND A HAPPY MEDIUM\n",
                        "# 1. INCREASE THE NUMBER OF RESIDUAL BLOCKS\n",
                        "# 2. INCREASE THE NUMBER OF FILTERS\n",
                        "# 3. DECREASE REGULARIZATION\n",
                        "# 4. TRY DECREASING LEARNING RATE (maybe its that whole thing where the policy goes to like 1 0 0 0 0... etc and then goes back on the third training step, so maybe the learning rate is too high)\n",
                        "# 5. TO OVERFIT USE LESS DATA (but that is probably just a bad idea)\n",
                        "# config = {\n",
                        "#         'activation': 'relu',\n",
                        "#         'kernel_initializer': 'glorot_uniform',\n",
                        "#         'optimizer': tf.keras.optimizers.legacy.Adam,\n",
                        "#         'learning_rate': 0.001, # 0.00001 could maybe increase by a factor of 10 or 100 and try to do some weights regularization\n",
                        "#         'adam_epsilon': 3.25e-6,\n",
                        "#         'clipnorm': None,\n",
                        "#         # NORMALIZATION?\n",
                        "#         # REWARD CLIPPING\n",
                        "#         'training_steps': 40,\n",
                        "#         'num_filters': 256,\n",
                        "#         'kernel_size': 3,\n",
                        "#         'stride': 1,\n",
                        "#         'num_res_blocks': 20,\n",
                        "#         'critic_conv_filters': 32, # 1\n",
                        "#         'critic_conv_layers': 1,\n",
                        "#         'critic_dense_size': 256,\n",
                        "#         'critic_dense_layers': 1,\n",
                        "#         'actor_conv_filters': 32, # \n",
                        "#         'actor_conv_layers': 1,\n",
                        "#         'actor_dense_size': 0,\n",
                        "#         'actor_dense_layers': 0,\n",
                        "#         'replay_buffer_size': 800, # IN GAMES\n",
                        "#         'replay_batch_size': 50, # IN MOVES\n",
                        "#         'root_dirichlet_alpha': 0.5, # 2 in theory?\n",
                        "#         'root_exploration_fraction': 0, # 0.25 in paper\n",
                        "#         'pb_c_base': 500,\n",
                        "#         'pb_c_init': 2,\n",
                        "#         'num_simulations': 200,\n",
                        "#         # 'two_player': True,\n",
                        "#         'weight_decay': 0.00, # could try setting this to something other than 0 and increasing learning rate\n",
                        "#         'num_sampling_moves': 0, \n",
                        "#         'initial_temperature': 1,\n",
                        "#         'exploitation_temperature': 0.1,\n",
                        "#         'value_loss_factor': 1, # could try setting this to something other than 1\n",
                        "#         'games_per_generation': 10, # times 8 from augmentation\n",
                        "#     }\n",
                        "\n",
                        "config = {\n",
                        "        'activation': 'relu',\n",
                        "        'kernel_initializer': 'glorot_uniform',\n",
                        "        'optimizer': tf.keras.optimizers.legacy.Adam,\n",
                        "        'learning_rate': 0.0005, #0.0001 # 0.00001 could maybe increase by a factor of 10 or 100 and try to do some weights regularization\n",
                        "        'number_of_lr_cycles': 1, # this will determine the step size based on training steps\n",
                        "        # STILL ADD A SCHEDULE FOR BASE LEARNING RATE (MIN LEARNING RATE)\n",
                        "        'adam_epsilon': 3.25e-6,\n",
                        "        'clipnorm': None,\n",
                        "        # NORMALIZATION?\n",
                        "        # REWARD CLIPPING\n",
                        "        'training_steps': 100, # alpha zero did 700,000, the lessons from alpha zero did 40 generations but 1000 batches per generation, so 40,000 batches (they just had a cyclical learning rate per generation (also they trained twice on the same data every generation))\n",
                        "        'num_filters': 256,\n",
                        "        'kernel_size': 3,\n",
                        "        'stride': 1,\n",
                        "        'residual_blocks': 20,\n",
                        "        'critic_conv_filters': 32, # 1\n",
                        "        'critic_conv_layers': 1,\n",
                        "        'critic_dense_size': 256,\n",
                        "        'critic_dense_layers': 1,\n",
                        "        'actor_conv_filters': 32, # \n",
                        "        'actor_conv_layers': 1,\n",
                        "        'actor_dense_size': 0,\n",
                        "        'actor_dense_layers': 0,\n",
                        "        'replay_buffer_size': 100, # IN GAMES\n",
                        "        'minibatch_size': 24, # SHOULD BE ROUGHLY SAME AS AVERAGE MOVE PER GENERATION (SO LIKE 7 TIMES NUMBER OF GAMES PLAYED PER GENERATION) <- what was used in the original paper (they played 44M games, 50 moves per game and sampled 700,000 minibatches of size 4096 (so thats like sampling 1 time per move roughly but this was also happening with parrallel data collection i believe))\n",
                        "        'games_per_generation': 1, # times 8 from augmentation\n",
                        "        'root_dirichlet_alpha': 2.5, # Less than 1 more random, greater than one more flat # 2 in theory? # 0.3 in alphazero for chess # TRY CHANGING (MAYBE LOWER? (IT SEEMS TO PLAY THE SAME LINE OVER AND OVER AGAIN <- so we want a lesss flat distribution maybe)\n",
                        "        'root_exploration_fraction': 0.25, # 0.25 in paper\n",
                        "        'pb_c_base': 20000, # Seems unimportant to be honest (increases puct the more simulations there are)\n",
                        "        'pb_c_init': 1.25, # 1.25 in paper # MAYBE HIGHER? (IT SEEMS TO PLAY THE SAME LINE OVER AND OVER AGAIN)\n",
                        "        'num_simulations': 50, # INCREASE THIS since the model is missing 1 move wins (and also 2 and 3 move wins (it wins by luck)))\n",
                        "        # 'two_player': True,\n",
                        "        'weight_decay': 0.00001, # could try setting this to something other than 0 and increasing learning rate\n",
                        "        'num_sampling_moves': 30, \n",
                        "        'exploration_temperature': 1,\n",
                        "        'exploitation_temperature': 0.1,\n",
                        "        'value_loss_factor': 1, # could try setting this to something other than 1\n",
                        "    }\n",
                        "\n",
                        "config = AlphaZeroConfig(config, TicTacToeConfig())\n",
                        "\n",
                        "agent = AlphaZeroAgent(env, config, \"alphazero\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Training Step  1\n"
                              ]
                        },
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
                                    "  warnings.warn(\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Predicted Policy  [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
                                    " 0.14285715]\n",
                                    "Predicted Value  0.0\n",
                                    "Target Policy [0.14 0.16 0.14 0.14 0.14 0.14 0.14]\n",
                                    "Action  6\n",
                                    "Predicted Policy  [0.14296095 0.1436779  0.14256604 0.1419401  0.14312488 0.14308842\n",
                                    " 0.14264171]\n",
                                    "Predicted Value  0.0016447032103314996\n",
                                    "Target Policy [0.14 0.14 0.16 0.14 0.14 0.14 0.14]\n",
                                    "Action  6\n",
                                    "Predicted Policy  [0.14281788 0.14290401 0.14286394 0.14279015 0.14288084 0.1429141\n",
                                    " 0.14282909]\n",
                                    "Predicted Value  6.019706779625267e-05\n",
                                    "Target Policy [0.14 0.14 0.16 0.14 0.16 0.14 0.12]\n",
                                    "Action  5\n",
                                    "Predicted Policy  [0.14297362 0.14368972 0.14256611 0.1419139  0.1431023  0.1431554\n",
                                    " 0.14259891]\n",
                                    "Predicted Value  0.0014771424466744065\n",
                                    "Target Policy [0.16 0.14 0.14 0.14 0.14 0.14 0.14]\n",
                                    "Action  5\n",
                                    "Predicted Policy  [0.14282903 0.14295264 0.14290276 0.14271994 0.14288184 0.1429348\n",
                                    " 0.14277902]\n",
                                    "Predicted Value  4.619146420736797e-05\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.14 0.14 0.16]\n",
                                    "Action  3\n",
                                    "Predicted Policy  [0.14296483 0.14367625 0.14260474 0.14189614 0.14313576 0.14309613\n",
                                    " 0.14262606]\n",
                                    "Predicted Value  0.0016535550821572542\n",
                                    "Target Policy [0.1        0.1        0.1        0.08       0.41999999 0.1\n",
                                    " 0.1       ]\n",
                                    "Action  4\n",
                                    "Predicted Policy  [0.14283377 0.14303367 0.14284763 0.14268601 0.14294189 0.14295974\n",
                                    " 0.14269723]\n",
                                    "Predicted Value  3.0874252843204886e-05\n",
                                    "Target Policy [0.14 0.14 0.16 0.14 0.14 0.14 0.14]\n",
                                    "Action  4\n",
                                    "Predicted Policy  [0.14295858 0.14366403 0.14257854 0.14188983 0.14318272 0.14309189\n",
                                    " 0.14263438]\n",
                                    "Predicted Value  0.0016951982397586107\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.14 0.16 0.14]\n",
                                    "Action  0\n",
                                    "Predicted Policy  [0.14286414 0.14305459 0.14281395 0.14265463 0.14296955 0.1429638\n",
                                    " 0.14267936]\n",
                                    "Predicted Value  0.00022638676455244422\n",
                                    "Target Policy [0.14 0.16 0.16 0.12 0.14 0.14 0.14]\n",
                                    "Action  1\n",
                                    "Predicted Policy  [0.14295188 0.14371192 0.14256288 0.14187656 0.1431975  0.14310196\n",
                                    " 0.14259735]\n",
                                    "Predicted Value  0.0014317368622869253\n",
                                    "Target Policy [0.14 0.14 0.16 0.14 0.14 0.12 0.16]\n",
                                    "Action  1\n",
                                    "Predicted Policy  [0.14292966 0.1430727  0.1427393  0.14264692 0.14299774 0.14298919\n",
                                    " 0.14262448]\n",
                                    "Predicted Value  0.0004804805212188512\n",
                                    "Target Policy [0.18000001 0.14       0.14       0.14       0.12       0.14\n",
                                    " 0.14      ]\n",
                                    "Action  1\n",
                                    "Predicted Policy  [0.14295094 0.14375539 0.14250846 0.14194645 0.14325643 0.14305638\n",
                                    " 0.14252597]\n",
                                    "Predicted Value  0.0012910960940644145\n",
                                    "Target Policy [0.14 0.16 0.12 0.12 0.16 0.14 0.16]\n",
                                    "Action  2\n",
                                    "Predicted Policy  [0.14293809 0.14311649 0.14273492 0.14256401 0.14302595 0.1429921\n",
                                    " 0.14262846]\n",
                                    "Predicted Value  0.0005268061649985611\n",
                                    "Target Policy [0.14 0.14 0.16 0.12 0.16 0.14 0.14]\n",
                                    "Action  0\n",
                                    "Predicted Policy  [0.14295237 0.14374739 0.14248443 0.1419031  0.1432987  0.14304526\n",
                                    " 0.14256875]\n",
                                    "Predicted Value  0.0011504750000312924\n",
                                    "Target Policy [0.14 0.16 0.14 0.14 0.14 0.14 0.14]\n",
                                    "Action  6\n",
                                    "Predicted Policy  [0.14289176 0.14318945 0.14273834 0.14257869 0.1429829  0.14297038\n",
                                    " 0.14264849]\n",
                                    "Predicted Value  0.00036834017373621464\n",
                                    "Target Policy [0.14 0.14 0.16 0.12 0.14 0.16 0.14]\n",
                                    "Action  5\n",
                                    "Predicted Policy  [0.14291067 0.14371796 0.1424744  0.14192149 0.14339052 0.14307596\n",
                                    " 0.142509  ]\n",
                                    "Predicted Value  0.0011282337363809347\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.14 0.14 0.16]\n",
                                    "Action  1\n",
                                    "Predicted Policy  [0.14287338 0.14323552 0.14269954 0.1424999  0.143031   0.14302628\n",
                                    " 0.14263447]\n",
                                    "Predicted Value  0.00014809561253059655\n",
                                    "Target Policy [0.04       0.04       0.06       0.04       0.04       0.04\n",
                                    " 0.74000001]\n",
                                    "Action  5\n",
                                    "Predicted Policy  [0.1428625  0.14369537 0.14246319 0.14189291 0.14340433 0.14314018\n",
                                    " 0.14254159]\n",
                                    "Predicted Value  0.0011542809661477804\n",
                                    "Target Policy [0.14       0.14       0.14       0.14       0.14       0.12\n",
                                    " 0.18000001]\n",
                                    "Action  2\n",
                                    "Predicted Policy  [0.14285743 0.14330709 0.14268614 0.1424546  0.14304568 0.14304005\n",
                                    " 0.14260899]\n",
                                    "Predicted Value  0.00010399929306004196\n",
                                    "Target Policy [0.04       0.04       0.04       0.04       0.06       0.04\n",
                                    " 0.74000001]\n",
                                    "Action  6\n",
                                    "Initial Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "Training Step  2\n",
                                    "Predicted Policy  [0.13399336 0.14368737 0.16998874 0.10704032 0.17569561 0.14373182\n",
                                    " 0.12586284]\n",
                                    "Predicted Value  -0.3809987008571625\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.14 0.14 0.16]\n",
                                    "Action  0\n",
                                    "Predicted Policy  [0.13371675 0.14349395 0.17042659 0.10653017 0.17659342 0.14370324\n",
                                    " 0.12553585]\n",
                                    "Predicted Value  -0.38813936710357666\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.16 0.14 0.14]\n",
                                    "Action  2\n",
                                    "Predicted Policy  [0.1339994  0.14376281 0.16999206 0.10691705 0.17573236 0.14374624\n",
                                    " 0.12585004]\n",
                                    "Predicted Value  -0.3818255662918091\n",
                                    "Target Policy [0.14 0.14 0.16 0.12 0.16 0.14 0.14]\n",
                                    "Action  3\n",
                                    "Predicted Policy  [0.13362736 0.14349434 0.170517   0.10645602 0.17662616 0.14376007\n",
                                    " 0.12551902]\n",
                                    "Predicted Value  -0.3886733949184418\n",
                                    "Target Policy [0.14 0.14 0.16 0.14 0.14 0.14 0.14]\n",
                                    "Action  6\n",
                                    "Predicted Policy  [0.133943   0.14373478 0.17006509 0.10692614 0.17574312 0.14373681\n",
                                    " 0.12585099]\n",
                                    "Predicted Value  -0.38228023052215576\n",
                                    "Target Policy [0.16 0.14 0.14 0.14 0.14 0.14 0.14]\n",
                                    "Action  4\n",
                                    "Predicted Policy  [0.13362134 0.14350604 0.17056882 0.10641393 0.17664152 0.14376518\n",
                                    " 0.12548323]\n",
                                    "Predicted Value  -0.3886531591415405\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.16 0.14 0.14]\n",
                                    "Action  0\n",
                                    "Predicted Policy  [0.13390866 0.14373308 0.17006269 0.10688271 0.17579164 0.1437345\n",
                                    " 0.12588677]\n",
                                    "Predicted Value  -0.38239526748657227\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.14 0.16 0.14]\n",
                                    "Action  4\n",
                                    "Predicted Policy  [0.13361807 0.14348152 0.17062305 0.10634235 0.1767384  0.14371909\n",
                                    " 0.12547754]\n",
                                    "Predicted Value  -0.3895603120326996\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.16 0.14 0.14]\n",
                                    "Action  1\n",
                                    "Predicted Policy  [0.13386807 0.14369795 0.17016497 0.10678478 0.17595045 0.14373809\n",
                                    " 0.12579572]\n",
                                    "Predicted Value  -0.38379326462745667\n",
                                    "Target Policy [0.16 0.14 0.16 0.14 0.14 0.14 0.12]\n",
                                    "Action  6\n",
                                    "Predicted Policy  [0.13359432 0.14339383 0.17074837 0.10636938 0.17675282 0.14370109\n",
                                    " 0.12544023]\n",
                                    "Predicted Value  -0.3898918330669403\n",
                                    "Target Policy [0.14 0.14 0.14 0.12 0.16 0.16 0.14]\n",
                                    "Action  2\n",
                                    "Predicted Policy  [0.13376829 0.14360327 0.17031159 0.10671178 0.17600815 0.14382717\n",
                                    " 0.12576976]\n",
                                    "Predicted Value  -0.38529762625694275\n",
                                    "Target Policy [0.14 0.14 0.14 0.14 0.16 0.14 0.14]\n",
                                    "Action  4\n",
                                    "Predicted Policy  [0.1335393  0.1433274  0.1708446  0.10626578 0.17686704 0.14379154\n",
                                    " 0.12536436]\n",
                                    "Predicted Value  -0.39112865924835205\n",
                                    "Target Policy [0.08       0.06       0.08       0.06       0.60000002 0.06\n",
                                    " 0.06      ]\n",
                                    "Action  2\n",
                                    "Predicted Policy  [0.13372333 0.14355801 0.17031695 0.10670831 0.17596087 0.14392704\n",
                                    " 0.12580553]\n",
                                    "Predicted Value  -0.385500431060791\n",
                                    "Target Policy [0.04       0.06       0.06       0.04       0.69999999 0.06\n",
                                    " 0.04      ]\n",
                                    "Action  4\n",
                                    "Initial Rewards [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
                                    "Updated Rewards [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
                                    "1\n",
                                    "./videos/alphazero/1\n",
                                    "Predicted Policy  [0.14324479 0.14275873 0.14224787 0.14291851 0.14272606 0.14281027\n",
                                    " 0.14329378]\n",
                                    "Predicted Value  0.004697680938988924\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326344 0.14274454 0.14221331 0.14291729 0.14273122 0.1428529\n",
                                    " 0.14327736]\n",
                                    "Predicted Value  0.0048195370472967625\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324656 0.14275955 0.14224336 0.14291786 0.14272709 0.14281365\n",
                                    " 0.14329194]\n",
                                    "Predicted Value  0.004713623318821192\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326742 0.14274138 0.14221431 0.1429157  0.14272991 0.14285609\n",
                                    " 0.14327523]\n",
                                    "Predicted Value  0.004833173472434282\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324844 0.1427583  0.14224382 0.14291942 0.14272623 0.14281322\n",
                                    " 0.14329056]\n",
                                    "Predicted Value  0.004720117896795273\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.143267   0.14274214 0.14221339 0.14291555 0.14272945 0.14285652\n",
                                    " 0.14327602]\n",
                                    "Predicted Value  0.004838604014366865\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.16700393 0.1664318  0.         0.16662031 0.16639483 0.16649638\n",
                                    " 0.16705282]\n",
                                    "Predicted Value  0.004720944911241531\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702019 0.16640514 0.         0.16660829 0.16639319 0.16654111\n",
                                    " 0.16703197]\n",
                                    "Predicted Value  0.004847281146794558\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700539 0.16642624 0.         0.1666174  0.16639657 0.16650066\n",
                                    " 0.16705373]\n",
                                    "Predicted Value  0.004718701355159283\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701919 0.16640334 0.         0.16660735 0.16639212 0.16654478\n",
                                    " 0.16703328]\n",
                                    "Predicted Value  0.004856371320784092\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701052 0.1664275  0.         0.16661659 0.16639318 0.16649915\n",
                                    " 0.16705318]\n",
                                    "Predicted Value  0.0047192322090268135\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701849 0.16640218 0.         0.16661015 0.1663893  0.16654485\n",
                                    " 0.16703503]\n",
                                    "Predicted Value  0.004841968417167664\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670072  0.16642635 0.         0.1666145  0.16639934 0.1665006\n",
                                    " 0.16705203]\n",
                                    "Predicted Value  0.004706950858235359\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702257 0.16640459 0.         0.1666077  0.16638817 0.16654359\n",
                                    " 0.16703337]\n",
                                    "Predicted Value  0.004843916278332472\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700421 0.16642351 0.         0.16661593 0.1664025  0.16650209\n",
                                    " 0.16705179]\n",
                                    "Predicted Value  0.004739955998957157\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "score:  1\n",
                                    "Predicted Policy  [0.14324479 0.14275873 0.14224787 0.14291851 0.14272606 0.14281027\n",
                                    " 0.14329378]\n",
                                    "Predicted Value  0.004697680938988924\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326344 0.14274454 0.14221331 0.14291729 0.14273122 0.1428529\n",
                                    " 0.14327736]\n",
                                    "Predicted Value  0.0048195370472967625\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324656 0.14275955 0.14224336 0.14291786 0.14272709 0.14281365\n",
                                    " 0.14329194]\n",
                                    "Predicted Value  0.004713623318821192\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326742 0.14274138 0.14221431 0.1429157  0.14272991 0.14285609\n",
                                    " 0.14327523]\n",
                                    "Predicted Value  0.004833173472434282\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324844 0.1427583  0.14224382 0.14291942 0.14272623 0.14281322\n",
                                    " 0.14329056]\n",
                                    "Predicted Value  0.004720117896795273\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.143267   0.14274214 0.14221339 0.14291555 0.14272945 0.14285652\n",
                                    " 0.14327602]\n",
                                    "Predicted Value  0.004838604014366865\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.16700393 0.1664318  0.         0.16662031 0.16639483 0.16649638\n",
                                    " 0.16705282]\n",
                                    "Predicted Value  0.004720944911241531\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701694 0.16640997 0.         0.16660956 0.16639186 0.16654246\n",
                                    " 0.16702917]\n",
                                    "Predicted Value  0.004838308785110712\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700834 0.1664303  0.         0.16661608 0.16639604 0.16649452\n",
                                    " 0.16705473]\n",
                                    "Predicted Value  0.004706361331045628\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701552 0.16640908 0.         0.1666091  0.1663897  0.1665468\n",
                                    " 0.16702981]\n",
                                    "Predicted Value  0.004846985451877117\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701105 0.16642764 0.         0.16661566 0.16639437 0.16649531\n",
                                    " 0.16705592]\n",
                                    "Predicted Value  0.004707659594714642\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701643 0.16640787 0.         0.16660592 0.16639277 0.16654703\n",
                                    " 0.16702999]\n",
                                    "Predicted Value  0.004851649049669504\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701262 0.16642489 0.         0.16661742 0.16639234 0.16649814\n",
                                    " 0.16705464]\n",
                                    "Predicted Value  0.004709254018962383\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702157 0.16640568 0.         0.16660435 0.16639435 0.16654314\n",
                                    " 0.1670309 ]\n",
                                    "Predicted Value  0.004853715188801289\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670138  0.16642351 0.         0.16661844 0.16638763 0.16649902\n",
                                    " 0.16705762]\n",
                                    "Predicted Value  0.004719527903944254\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702102 0.16640413 0.         0.16660546 0.16639228 0.16654636\n",
                                    " 0.16703069]\n",
                                    "Predicted Value  0.0048532430082559586\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670113  0.16642305 0.         0.16661659 0.16639057 0.16650067\n",
                                    " 0.16705774]\n",
                                    "Predicted Value  0.0047319852747023106\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702229 0.16639933 0.         0.16660136 0.16639464 0.16655463\n",
                                    " 0.16702779]\n",
                                    "Predicted Value  0.004893261473625898\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701214 0.16641751 0.         0.16661866 0.16639468 0.1665006\n",
                                    " 0.16705644]\n",
                                    "Predicted Value  0.004726612940430641\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702396 0.16639918 0.         0.16660352 0.1663911  0.16655575\n",
                                    " 0.16702649]\n",
                                    "Predicted Value  0.004900757689028978\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700959 0.16641803 0.         0.16662203 0.16638935 0.16650526\n",
                                    " 0.16705574]\n",
                                    "Predicted Value  0.004734524525702\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702436 0.16639555 0.         0.16660121 0.16639419 0.16655588\n",
                                    " 0.16702884]\n",
                                    "Predicted Value  0.004909079521894455\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701034 0.16641696 0.         0.16662127 0.16639012 0.1665055\n",
                                    " 0.16705585]\n",
                                    "Predicted Value  0.004731808789074421\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "score:  1\n",
                                    "Predicted Policy  [0.14324479 0.14275873 0.14224787 0.14291851 0.14272606 0.14281027\n",
                                    " 0.14329378]\n",
                                    "Predicted Value  0.004697680938988924\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326344 0.14274454 0.14221331 0.14291729 0.14273122 0.1428529\n",
                                    " 0.14327736]\n",
                                    "Predicted Value  0.0048195370472967625\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324656 0.14275955 0.14224336 0.14291786 0.14272709 0.14281365\n",
                                    " 0.14329194]\n",
                                    "Predicted Value  0.004713623318821192\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326742 0.14274138 0.14221431 0.1429157  0.14272991 0.14285609\n",
                                    " 0.14327523]\n",
                                    "Predicted Value  0.004833173472434282\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324844 0.1427583  0.14224382 0.14291942 0.14272623 0.14281322\n",
                                    " 0.14329056]\n",
                                    "Predicted Value  0.004720117896795273\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.143267   0.14274214 0.14221339 0.14291555 0.14272945 0.14285652\n",
                                    " 0.14327602]\n",
                                    "Predicted Value  0.004838604014366865\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.16700393 0.1664318  0.         0.16662031 0.16639483 0.16649638\n",
                                    " 0.16705282]\n",
                                    "Predicted Value  0.004720944911241531\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701987 0.16640517 0.         0.16660903 0.16639267 0.16654256\n",
                                    " 0.16703072]\n",
                                    "Predicted Value  0.00484350323677063\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700758 0.16643275 0.         0.16662036 0.16639245 0.1664974\n",
                                    " 0.16704951]\n",
                                    "Predicted Value  0.004722184967249632\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702296 0.16640475 0.         0.16660969 0.16638997 0.16654015\n",
                                    " 0.16703251]\n",
                                    "Predicted Value  0.00482874596491456\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700831 0.16643329 0.         0.16662124 0.1663918  0.16649446\n",
                                    " 0.16705085]\n",
                                    "Predicted Value  0.004722657147794962\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702405 0.16640367 0.         0.16660757 0.16638866 0.16653964\n",
                                    " 0.16703635]\n",
                                    "Predicted Value  0.004830044694244862\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700952 0.16643186 0.         0.1666203  0.1663917  0.1664972\n",
                                    " 0.16704944]\n",
                                    "Predicted Value  0.004720944911241531\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702618 0.16640224 0.         0.16660972 0.1663853  0.16654335\n",
                                    " 0.16703328]\n",
                                    "Predicted Value  0.004843267146497965\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701087 0.16643044 0.         0.16662139 0.16639033 0.16649768\n",
                                    " 0.16704926]\n",
                                    "Predicted Value  0.00472366064786911\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702552 0.16639967 0.         0.16661114 0.16638544 0.16654333\n",
                                    " 0.16703492]\n",
                                    "Predicted Value  0.004837482236325741\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "score:  1\n",
                                    "Predicted Policy  [0.14324479 0.14275873 0.14224787 0.14291851 0.14272606 0.14281027\n",
                                    " 0.14329378]\n",
                                    "Predicted Value  0.004697680938988924\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326344 0.14274454 0.14221331 0.14291729 0.14273122 0.1428529\n",
                                    " 0.14327736]\n",
                                    "Predicted Value  0.0048195370472967625\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324656 0.14275955 0.14224336 0.14291786 0.14272709 0.14281365\n",
                                    " 0.14329194]\n",
                                    "Predicted Value  0.004713623318821192\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326742 0.14274138 0.14221431 0.1429157  0.14272991 0.14285609\n",
                                    " 0.14327523]\n",
                                    "Predicted Value  0.004833173472434282\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324844 0.1427583  0.14224382 0.14291942 0.14272623 0.14281322\n",
                                    " 0.14329056]\n",
                                    "Predicted Value  0.004720117896795273\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.143267   0.14274214 0.14221339 0.14291555 0.14272945 0.14285652\n",
                                    " 0.14327602]\n",
                                    "Predicted Value  0.004838604014366865\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.16700393 0.1664318  0.         0.16662031 0.16639483 0.16649638\n",
                                    " 0.16705282]\n",
                                    "Predicted Value  0.004720944911241531\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702019 0.16640514 0.         0.16660829 0.16639319 0.16654111\n",
                                    " 0.16703197]\n",
                                    "Predicted Value  0.004847281146794558\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700219 0.16642989 0.         0.16661665 0.16640024 0.16650084\n",
                                    " 0.16705015]\n",
                                    "Predicted Value  0.0047089592553675175\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670235  0.16640311 0.         0.16660984 0.16638932 0.1665397\n",
                                    " 0.16703449]\n",
                                    "Predicted Value  0.004850232508033514\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700317 0.16642705 0.         0.16661637 0.16640307 0.16650125\n",
                                    " 0.16704914]\n",
                                    "Predicted Value  0.004719940479844809\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702455 0.16640195 0.         0.16660753 0.16639157 0.16654125\n",
                                    " 0.16703318]\n",
                                    "Predicted Value  0.00486487103626132\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670058  0.16642265 0.         0.1666181  0.16640174 0.16650252\n",
                                    " 0.16704917]\n",
                                    "Predicted Value  0.004724900238215923\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670234  0.16640043 0.         0.16660152 0.16639812 0.16654545\n",
                                    " 0.16703114]\n",
                                    "Predicted Value  0.004863218404352665\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700281 0.16642205 0.         0.16661538 0.16640231 0.16650786\n",
                                    " 0.16704956]\n",
                                    "Predicted Value  0.004740546457469463\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702344 0.16639884 0.         0.16660161 0.1663981  0.16654694\n",
                                    " 0.1670311 ]\n",
                                    "Predicted Value  0.004868648946285248\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700353 0.16642392 0.         0.1666165  0.16639909 0.16650914\n",
                                    " 0.16704774]\n",
                                    "Predicted Value  0.004746037535369396\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.1670262  0.1663977  0.         0.16660061 0.16639894 0.16654794\n",
                                    " 0.16702864]\n",
                                    "Predicted Value  0.0048750825226306915\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700378 0.16642338 0.         0.16661482 0.16640098 0.16650942\n",
                                    " 0.16704763]\n",
                                    "Predicted Value  0.004742259159684181\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16702749 0.1664018  0.         0.16659859 0.16639635 0.16654873\n",
                                    " 0.16702704]\n",
                                    "Predicted Value  0.004890428390353918\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700275 0.16642444 0.         0.1666126  0.16640311 0.16650628\n",
                                    " 0.16705085]\n",
                                    "Predicted Value  0.00472413282841444\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "score:  1\n",
                                    "Predicted Policy  [0.14324479 0.14275873 0.14224787 0.14291851 0.14272606 0.14281027\n",
                                    " 0.14329378]\n",
                                    "Predicted Value  0.004697680938988924\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326344 0.14274454 0.14221331 0.14291729 0.14273122 0.1428529\n",
                                    " 0.14327736]\n",
                                    "Predicted Value  0.0048195370472967625\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324656 0.14275955 0.14224336 0.14291786 0.14272709 0.14281365\n",
                                    " 0.14329194]\n",
                                    "Predicted Value  0.004713623318821192\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14326742 0.14274138 0.14221431 0.1429157  0.14272991 0.14285609\n",
                                    " 0.14327523]\n",
                                    "Predicted Value  0.004833173472434282\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.14324844 0.1427583  0.14224382 0.14291942 0.14272623 0.14281322\n",
                                    " 0.14329056]\n",
                                    "Predicted Value  0.004720117896795273\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.143267   0.14274214 0.14221339 0.14291555 0.14272945 0.14285652\n",
                                    " 0.14327602]\n",
                                    "Predicted Value  0.004838604014366865\n",
                                    "Target Policy [0.04       0.04       0.75999999 0.04       0.04       0.04\n",
                                    " 0.04      ]\n",
                                    "Predicted Policy  [0.16700393 0.1664318  0.         0.16662031 0.16639483 0.16649638\n",
                                    " 0.16705282]\n",
                                    "Predicted Value  0.004720944911241531\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701987 0.16640517 0.         0.16660903 0.16639267 0.16654256\n",
                                    " 0.16703072]\n",
                                    "Predicted Value  0.00484350323677063\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700417 0.16642827 0.         0.16661906 0.16639417 0.1665025\n",
                                    " 0.16705185]\n",
                                    "Predicted Value  0.004730273503810167\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701828 0.16640201 0.         0.1666083  0.16639517 0.16654204\n",
                                    " 0.1670342 ]\n",
                                    "Predicted Value  0.004847517237067223\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700436 0.1664239  0.         0.16661604 0.16639523 0.166511\n",
                                    " 0.1670495 ]\n",
                                    "Predicted Value  0.004736708477139473\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701798 0.1664011  0.         0.16660722 0.16639929 0.16653892\n",
                                    " 0.16703549]\n",
                                    "Predicted Value  0.004849465563893318\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700748 0.16642514 0.         0.16661648 0.16639322 0.16650593\n",
                                    " 0.16705179]\n",
                                    "Predicted Value  0.004752177745103836\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701946 0.16640346 0.         0.1666095  0.16639604 0.16653816\n",
                                    " 0.16703339]\n",
                                    "Predicted Value  0.0048414962366223335\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700795 0.16642402 0.         0.16661596 0.166394   0.16650505\n",
                                    " 0.16705303]\n",
                                    "Predicted Value  0.004762863740324974\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16701832 0.16639909 0.         0.16660869 0.16639864 0.1665404\n",
                                    " 0.1670348 ]\n",
                                    "Predicted Value  0.0048534199595451355\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.16700825 0.16642354 0.         0.16661693 0.16639106 0.16650663\n",
                                    " 0.16705364]\n",
                                    "Predicted Value  0.004762450233101845\n",
                                    "Target Policy [0.18000001 0.16       0.         0.16       0.16       0.16\n",
                                    " 0.18000001]\n",
                                    "Predicted Policy  [0.         0.19976337 0.         0.20001476 0.19976316 0.19993241\n",
                                    " 0.20052625]\n",
                                    "Predicted Value  0.0048538921400904655\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19979344 0.         0.20002556 0.1997458  0.19989368\n",
                                    " 0.20054151]\n",
                                    "Predicted Value  0.004775084555149078\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19976486 0.         0.20000674 0.19976737 0.19993903\n",
                                    " 0.20052202]\n",
                                    "Predicted Value  0.004843149334192276\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19979167 0.         0.20003255 0.19974148 0.19990154\n",
                                    " 0.20053282]\n",
                                    "Predicted Value  0.004763572011142969\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19976453 0.         0.20000632 0.19976489 0.19994238\n",
                                    " 0.20052187]\n",
                                    "Predicted Value  0.004823020193725824\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19979614 0.         0.20003399 0.19973953 0.19989397\n",
                                    " 0.20053644]\n",
                                    "Predicted Value  0.004798521287739277\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19975573 0.         0.20000838 0.19977047 0.19994608\n",
                                    " 0.2005194 ]\n",
                                    "Predicted Value  0.004842441063374281\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19979435 0.         0.20003173 0.19974482 0.19989702\n",
                                    " 0.20053203]\n",
                                    "Predicted Value  0.004807612393051386\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n",
                                    "Predicted Policy  [0.         0.19974996 0.         0.20000982 0.19977473 0.19994476\n",
                                    " 0.2005207 ]\n",
                                    "Predicted Value  0.004862687550485134\n",
                                    "Target Policy [0.  0.2 0.  0.2 0.2 0.2 0.2]\n"
                              ]
                        },
                        {
                              "ename": "RuntimeError",
                              "evalue": "No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.",
                              "output_type": "error",
                              "traceback": [
                                    "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                                    "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                                    "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:109\u001b[0m, in \u001b[0;36mAlphaZeroAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# CHECKPOINTING\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m training_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtotal_environment_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_checkpoint(\n\u001b[1;32m    119\u001b[0m     stats,\n\u001b[1;32m    120\u001b[0m     targets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     time() \u001b[38;5;241m-\u001b[39m training_time,\n\u001b[1;32m    125\u001b[0m )\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/base_agent/agent.py:107\u001b[0m, in \u001b[0;36mBaseAgent.save_checkpoint\u001b[0;34m(self, stats, targets, num_trials, training_step, frames_seen, time_taken)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(path)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# save replay buffer\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# save optimizer\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# test model\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m test_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(test_score)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# plot the graphs\u001b[39;00m\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/base_agent/agent.py:184\u001b[0m, in \u001b[0;36mBaseAgent.test\u001b[0;34m(self, num_trials, step)\u001b[0m\n\u001b[1;32m    182\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state, legal_moves)\n\u001b[1;32m    183\u001b[0m test_game_moves\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m--> 184\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m    186\u001b[0m legal_moves \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    187\u001b[0m     info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegal_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mhas_legal_moves \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    188\u001b[0m )\n",
                                    "File \u001b[0;32m~/Documents/GitHub/rl-stuff/alphazero/alphazero_agent.py:67\u001b[0m, in \u001b[0;36mAlphaZeroAgent.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_state, reward, terminated, truncated, info\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/record_video.py:191\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminateds \u001b[38;5;129;01mor\u001b[39;00m truncateds:\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m terminateds[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m truncateds[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_video_recorder()\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/record_video.py:204\u001b[0m, in \u001b[0;36mRecordVideo.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:149\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageSequenceClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageSequenceClip\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoviepy is not installed, run `pip install moviepy`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/moviepy/video/io/ImageSequenceClip.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVideoClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoClip\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImageSequenceClip\u001b[39;00m(VideoClip):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    A VideoClip made from a series of images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/moviepy/video/VideoClip.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Clip\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEVNULL, string_types\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_setting\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (add_mask_if_none, apply_to_mask,\n\u001b[1;32m     20\u001b[0m                           convert_masks_to_RGB, convert_to_seconds, outplace,\n\u001b[1;32m     21\u001b[0m                           requires_duration, use_clip_fps_by_default)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (deprecated_version_of, extensions_dict, find_extension,\n\u001b[1;32m     23\u001b[0m                      is_string, subprocess_call)\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/moviepy/config.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FFMPEG_BINARY\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg-imageio\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mffmpeg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_exe\n\u001b[0;32m---> 36\u001b[0m     FFMPEG_BINARY \u001b[38;5;241m=\u001b[39m \u001b[43mget_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m FFMPEG_BINARY\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto-detect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m try_cmd([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]:\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/imageio/plugins/ffmpeg.py:173\u001b[0m, in \u001b[0;36mget_exe\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exe\u001b[39m():  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for imageio_ffmpeg.get_ffmpeg_exe()\"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimageio_ffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ffmpeg_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                                    "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/imageio_ffmpeg/_utils.py:34\u001b[0m, in \u001b[0;36mget_ffmpeg_exe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exe\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Nothing was found\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo ffmpeg exe could be found. Install ffmpeg on your system, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set the IMAGEIO_FFMPEG_EXE environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n",
                                    "\u001b[0;31mRuntimeError\u001b[0m: No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable."
                              ]
                        },
                        {
                              "ename": "",
                              "evalue": "",
                              "output_type": "error",
                              "traceback": [
                                    "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                                    "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                                    "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                                    "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                              ]
                        }
                  ],
                  "source": [
                        "agent.checkpoint_interval = 1\n",
                        "agent.train()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "agent.model.load_weights(\"./alphazero.keras\")\n",
                        "agent.train()"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.9.6"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
