{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Board Games\n",
    "\n",
    "RL for board games is a project under the McGill AI Lab, dedicated to developing and advancing reinforcement learning (RL) models aimed at mastering board games.\n",
    "By tackling state representation, action space modeling, and multi-agent RL, the team aims to create RL agents capable learning to play common board games.\n",
    "All the source code for the project can be found in the GitHub repository [epicgamer17/rl-research](https://github.com/epicgamer17/rl-research)\n",
    "\n",
    "## Project Focus\n",
    "\n",
    "The project's central goal is to create RL agents that can master both simple and complex board games.\n",
    "While early successes include solving classic control environments like Cartpole and Mountain Car,\n",
    "the primary focus now is on board game environments such as Tic-Tac-Toe, Checkers, Connect Four, with\n",
    "future ambitions include creating RL bots to pioneer solutions for more complex games like Scrabble, Risk, Monopoly, and Catan.\n",
    "\n",
    "Key challenges involve designing efficient state representations, modeling large action spaces,\n",
    "and training multi-agent systems. The team actively reads and recreates RL papers,\n",
    "encouraging collaboration and iterative learning, with an emphasis on adapting and\n",
    "expanding advanced models such as AlphaZero, MuZero, and NFSP to these board game challenges.\n",
    "\n",
    "## Implemented Algorithms\n",
    "\n",
    "The repository includes a range of RL algorithms, such as DQN-based architectures\n",
    "(Double DQN, Dueling DQN, Rainbow DQN) and Actor-Critic methods like A2C and PPO.\n",
    "More advanced models, including AlphaZero, MuZero, and NFSP, provide a foundation\n",
    "for tackling complex strategy games. The modular design allows for experimentation\n",
    "with different configurations, fostering innovation and learning.\n",
    "\n",
    "## Custom Environments\n",
    "\n",
    "To support RL agent development, the project provides custom OpenAI Gym environments tailored to board games:\n",
    "\n",
    "- **Tic Tac Toe**: A classic two-player game for testing basic RL models.\n",
    "- **Connect 4**: A strategic game that introduces complexity and planning depth.\n",
    "\n",
    "Furthermore, to investigate imperfect information and multi-agent settings, we have implementations of \n",
    "the following environments\n",
    "\n",
    "- **LeDuc Hold'em**: A simplified poker variant for studying imperfect information games.\n",
    "- **Mississippi Marbles**: A custom environment with unique dynamics and strategies.\n",
    "\n",
    "These environments allow for iterative training and evaluation, \n",
    "ensuring that agents can generalize across different strategic challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: RainbowDQN on CartPole, training episodes 4, 79, and 154\n",
    "\n",
    "![](./figs/Rainbow_ClassicControl_CartPole-v1-episode-4.mp4)\n",
    "![](./figs/Rainbow_ClassicControl_CartPole-v1-episode-79.mp4)\n",
    "![](./figs/Rainbow_ClassicControl_CartPole-v1-episode-154.mp4)\n",
    "\n",
    "# Example: RainbowDQN on Acrobot, training episodes 4, 79, and 154\n",
    "\n",
    "![](./figs/Rainbow_ClassicControl_Acrobot-v1-episode-4.mp4)\n",
    "![](./figs/Rainbow_ClassicControl_Acrobot-v1-episode-79.mp4)\n",
    "![](./figs/Rainbow_ClassicControl_Acrobot-v1-episode-154.mp4)\n",
    "\n",
    "# Example: RainbowDQN on LunarLander, training episodes 4, 79, and 154\n",
    "\n",
    "![](./figs/Rainbow_ClassicControl_LunarLander-v2-episode-4.mp4)\n",
    "![](./figs/Rainbow_ClassicControl_LunarLander-v2-episode-79.mp4)\n",
    "![](./figs/Rainbow_ClassicControl_LunarLander-v2-episode-154.mp4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Train RainbowDQN on Pac-Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from typing import Iterable, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "import itertools\n",
    "from hyperopt import space_eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# from ....replay_buffers.base_replay_buffer import Game\n",
    "# from replay_buffers.segment_tree import SumSegmentTree\n",
    "\n",
    "\n",
    "def normalize_policies(policies: torch.float32):\n",
    "    # print(policies)\n",
    "    policy_sums = policies.sum(axis=-1, keepdims=True)\n",
    "    # print(policy_sums)\n",
    "    policies = policies / policy_sums\n",
    "    return policies\n",
    "\n",
    "\n",
    "def action_mask(\n",
    "    actions: Tensor, legal_moves, mask_value: float = 0, device=\"cpu\"\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Mask actions that are not legal moves\n",
    "    actions: Tensor, probabilities of actions or q-values\n",
    "    \"\"\"\n",
    "    assert isinstance(\n",
    "        legal_moves, list\n",
    "    ), \"Legal moves should be a list got {} of type {}\".format(\n",
    "        legal_moves, type(legal_moves)\n",
    "    )\n",
    "\n",
    "    # add a dimension if the legal moves are not a list of lists\n",
    "    # if len(legal_moves) != actions.shape[0]:\n",
    "    #     legal_moves = [legal_moves]\n",
    "    assert (\n",
    "        len(legal_moves) == actions.shape[0]\n",
    "    ), \"Legal moves should be the same length as the batch size\"\n",
    "\n",
    "    mask = torch.zeros_like(actions, dtype=torch.bool).to(device)\n",
    "    for i, legal in enumerate(legal_moves):\n",
    "        mask[i, legal] = True\n",
    "    # print(mask)\n",
    "    # print(actions)\n",
    "    # actions[mask == 0] = mask_value\n",
    "    actions = torch.where(mask, actions, torch.tensor(mask_value).to(device)).to(device)\n",
    "    # print(mask)\n",
    "    return actions\n",
    "\n",
    "\n",
    "def clip_low_prob_actions(actions: Tensor, low_prob: float = 0.01) -> Tensor:\n",
    "    \"\"\"\n",
    "    Clip actions with probability lower than low_prob to 0\n",
    "    actions: Tensor, probabilities of actions\n",
    "    \"\"\"\n",
    "    # print(\"Actions in low prob func\", actions)\n",
    "    if low_prob == 0:\n",
    "        return actions\n",
    "    mask = actions < low_prob\n",
    "    # print(\"Mask\", mask)\n",
    "    actions = torch.where(mask, 0.0, actions)\n",
    "    # print(\"Actions after clipping\", actions)\n",
    "    return actions\n",
    "\n",
    "\n",
    "def get_legal_moves(info: dict | list[dict]):\n",
    "    # print(info)\n",
    "    if isinstance(info, dict):\n",
    "        return [info[\"legal_moves\"] if \"legal_moves\" in info else None]\n",
    "    else:\n",
    "        return [(i[\"legal_moves\"] if \"legal_moves\" in i else None) for i in info]\n",
    "\n",
    "\n",
    "def normalize_images(image: Tensor) -> Tensor:\n",
    "    \"\"\"Preprocessing step to normalize image with 8-bit (0-255) color inplace.\n",
    "    Modifys the original tensor\n",
    "\n",
    "    Args:\n",
    "        image (Tensor): An 8-bit color image\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The tensor divided by 255\n",
    "    \"\"\"\n",
    "    # Return a copy of the tensor divided by 255\n",
    "    normalized_image = image.div_(255)\n",
    "    return normalized_image\n",
    "\n",
    "\n",
    "def make_stack(item: Tensor) -> Tensor:\n",
    "    \"\"\"Convert a tensor of shape (*) to (1, *). Does not copy the data; instead,\n",
    "    returns a view of the original tensor.\n",
    "\n",
    "    Args:\n",
    "        item (Tensor):\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A view of the original tensor.\n",
    "    \"\"\"\n",
    "    #\n",
    "    return item.view(1, *item.shape)\n",
    "\n",
    "\n",
    "def update_per_beta(\n",
    "    per_beta: float, per_beta_final: float, per_beta_steps: int, initial_per_beta: int\n",
    "):\n",
    "    # could also use an initial per_beta instead of current (multiply below equation by current step)\n",
    "    if per_beta < per_beta_final:\n",
    "        clamp_func = min\n",
    "    else:\n",
    "        clamp_func = max\n",
    "    per_beta = clamp_func(\n",
    "        per_beta_final,\n",
    "        per_beta + (per_beta_final - initial_per_beta) / (per_beta_steps),\n",
    "    )\n",
    "\n",
    "    return per_beta\n",
    "\n",
    "\n",
    "def update_linear_schedule(\n",
    "    final_value: float,\n",
    "    total_steps: int,\n",
    "    initial_value: float,\n",
    "    current_step: int,\n",
    "):\n",
    "    # learning_rate = initial_value\n",
    "    if initial_value < final_value:\n",
    "        clamp_func = min\n",
    "    else:\n",
    "        clamp_func = max\n",
    "    value = clamp_func(\n",
    "        final_value,\n",
    "        initial_value + ((final_value - initial_value) * (current_step / total_steps)),\n",
    "    )\n",
    "    return value\n",
    "\n",
    "\n",
    "def update_inverse_sqrt_schedule(\n",
    "    initial_value: float = None,\n",
    "    current_step: int = None,\n",
    "):\n",
    "    return initial_value / math.sqrt(current_step + 1)\n",
    "\n",
    "\n",
    "def default_plot_func(\n",
    "    axs, key: str, values: list[dict], targets: dict, row: int, col: int\n",
    "):\n",
    "    axs[row][col].set_title(\"{} | rolling average: {}\".format(key, np.mean(values[-5:])))\n",
    "    x = np.arange(1, len(values) + 1)\n",
    "    axs[row][col].plot(x, values)\n",
    "    if key in targets and targets[key] is not None:\n",
    "        axs[row][col].axhline(y=targets[key], color=\"r\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "def plot_scores(axs, key: str, values: list[dict], targets: dict, row: int, col: int):\n",
    "    if len(values) == 0:\n",
    "        return\n",
    "    print(values)\n",
    "    scores = [value[\"score\"] for value in values]\n",
    "    x = np.arange(1, len(values) + 1)\n",
    "    axs[row][col].plot(x, scores)\n",
    "\n",
    "    has_max_scores = \"max_score\" in values[0]\n",
    "    has_min_scores = \"min_score\" in values[0]\n",
    "    assert (\n",
    "        has_max_scores == has_min_scores\n",
    "    ), \"Both max_scores and min_scores must be provided or not provided\"\n",
    "\n",
    "    if has_max_scores:\n",
    "        max_scores = [value[\"max_score\"] for value in values]\n",
    "        min_scores = [value[\"min_score\"] for value in values]\n",
    "        axs[row][col].fill_between(x, min_scores, max_scores, alpha=0.5)\n",
    "\n",
    "    has_target_model_updates = \"target_model_updated\" in values[0]\n",
    "    has_model_updates = \"model_updated\" in values[0]\n",
    "\n",
    "    if has_target_model_updates:\n",
    "        weight_updates = [value[\"target_model_updated\"] for value in values]\n",
    "        for i, weight_update in enumerate(weight_updates):\n",
    "            if weight_update:\n",
    "                axs[row][col].axvline(\n",
    "                    x=i,\n",
    "                    color=\"black\",\n",
    "                    linestyle=\"dotted\",\n",
    "                    # label=\"Target Model Weight Update\",\n",
    "                )\n",
    "\n",
    "    if has_model_updates:\n",
    "        weight_updates = [value[\"model_updated\"] for value in values]\n",
    "        for i, weight_update in enumerate(weight_updates):\n",
    "            if weight_update:\n",
    "                axs[row][col].axvline(\n",
    "                    x=i,\n",
    "                    color=\"gray\",\n",
    "                    linestyle=\"dotted\",\n",
    "                    # label=\"Model Weight Update\",\n",
    "                )\n",
    "\n",
    "    axs[row][col].set_title(\n",
    "        f\"{key} | rolling average: {np.mean(scores[-5:])} | latest: {scores[-1]}\"\n",
    "    )\n",
    "\n",
    "    axs[row][col].set_xlabel(\"Game\")\n",
    "    axs[row][col].set_ylabel(\"Score\")\n",
    "\n",
    "    axs[row][col].set_xlim(1, len(values))\n",
    "\n",
    "    if len(scores) > 1:\n",
    "        best_fit_x, best_fit_y = np.polyfit(x, scores, 1)\n",
    "        axs[row][col].plot(\n",
    "            x,\n",
    "            best_fit_x * x + best_fit_y,\n",
    "            color=\"g\",\n",
    "            label=\"Best Fit Line\",\n",
    "            linestyle=\"dotted\",\n",
    "        )\n",
    "\n",
    "    if key in targets and targets[key] is not None:\n",
    "        axs[row][col].axhline(\n",
    "            y=targets[key],\n",
    "            color=\"r\",\n",
    "            linestyle=\"dashed\",\n",
    "            label=\"Target Score: {}\".format(targets[key]),\n",
    "        )\n",
    "\n",
    "    axs[row][col].legend()\n",
    "\n",
    "\n",
    "def plot_loss(axs, key: str, values: list[dict], targets: dict, row: int, col: int):\n",
    "    loss = [value[\"loss\"] for value in values]\n",
    "    x = np.arange(1, len(values) + 1)\n",
    "    axs[row][col].plot(x, loss)\n",
    "\n",
    "    has_target_model_updates = \"target_model_updated\" in values[0]\n",
    "    has_model_updates = \"model_updated\" in values[0]\n",
    "\n",
    "    if has_target_model_updates:\n",
    "        weight_updates = [value[\"target_model_updated\"] for value in values]\n",
    "        for i, weight_update in enumerate(weight_updates):\n",
    "            if weight_update:\n",
    "                axs[row][col].axvline(\n",
    "                    x=i,\n",
    "                    color=\"black\",\n",
    "                    linestyle=\"dotted\",\n",
    "                    # label=\"Target Model Weight Update\",\n",
    "                )\n",
    "\n",
    "    if has_model_updates:\n",
    "        weight_updates = [value[\"model_updated\"] for value in values]\n",
    "        for i, weight_update in enumerate(weight_updates):\n",
    "            if weight_update:\n",
    "                axs[row][col].axvline(\n",
    "                    x=i,\n",
    "                    color=\"gray\",\n",
    "                    linestyle=\"dotted\",\n",
    "                    # label=\"Model Weight Update\",\n",
    "                )\n",
    "\n",
    "    axs[row][col].set_title(\n",
    "        f\"{key} | rolling average: {np.mean(loss[-5:])} | latest: {loss[-1]}\"\n",
    "    )\n",
    "\n",
    "    axs[row][col].set_xlabel(\"Time Step\")\n",
    "    axs[row][col].set_ylabel(\"Loss\")\n",
    "\n",
    "    axs[row][col].set_xlim(1, len(values))\n",
    "\n",
    "    if key in targets and targets[key] is not None:\n",
    "        axs[row][col].axhline(\n",
    "            y=targets[key],\n",
    "            color=\"r\",\n",
    "            linestyle=\"dashed\",\n",
    "            label=\"Target Score: {}\".format(targets[key]),\n",
    "        )\n",
    "\n",
    "    axs[row][col].legend()\n",
    "\n",
    "\n",
    "def plot_exploitability(\n",
    "    axs, key: str, values: list[dict], targets: dict, row: int, col: int\n",
    "):\n",
    "    if len(values) == 0:\n",
    "        return\n",
    "    exploitability = [abs(value[\"exploitability\"]) for value in values]\n",
    "    print(values)\n",
    "    rolling_averages = [\n",
    "        np.mean(exploitability[max(0, i - 5) : i])\n",
    "        for i in range(1, len(exploitability) + 1)\n",
    "    ]\n",
    "    # print(rolling_averages)\n",
    "    x = np.arange(1, len(values) + 1)\n",
    "    axs[row][col].plot(x, rolling_averages)\n",
    "    axs[row][col].plot(x, exploitability)\n",
    "\n",
    "    has_target_model_updates = \"target_model_updated\" in values[0]\n",
    "    has_model_updates = \"model_updated\" in values[0]\n",
    "\n",
    "    if has_target_model_updates:\n",
    "        weight_updates = [value[\"target_model_updated\"] for value in values]\n",
    "        for i, weight_update in enumerate(weight_updates):\n",
    "            if weight_update:\n",
    "                axs[row][col].axvline(\n",
    "                    x=i,\n",
    "                    color=\"black\",\n",
    "                    linestyle=\"dotted\",\n",
    "                    # label=\"Target Model Weight Update\",\n",
    "                )\n",
    "\n",
    "    if has_model_updates:\n",
    "        weight_updates = [value[\"model_updated\"] for value in values]\n",
    "        for i, weight_update in enumerate(weight_updates):\n",
    "            if weight_update:\n",
    "                axs[row][col].axvline(\n",
    "                    x=i,\n",
    "                    color=\"gray\",\n",
    "                    linestyle=\"dotted\",\n",
    "                    # label=\"Model Weight Update\",\n",
    "                )\n",
    "\n",
    "    if len(rolling_averages) > 1:\n",
    "        best_fit_x, best_fit_y = np.polyfit(x, rolling_averages, 1)\n",
    "        axs[row][col].plot(\n",
    "            x,\n",
    "            best_fit_x * x + best_fit_y,\n",
    "            color=\"g\",\n",
    "            label=\"Best Fit Line\",\n",
    "            linestyle=\"dotted\",\n",
    "        )\n",
    "\n",
    "    axs[row][col].set_title(\n",
    "        f\"{key} | rolling average: {np.mean(exploitability[-5:])} | latest: {exploitability[-1]}\"\n",
    "    )\n",
    "\n",
    "    axs[row][col].set_xlabel(\"Game\")\n",
    "    axs[row][col].set_ylabel(\"Exploitability (rolling average)\")\n",
    "\n",
    "    axs[row][col].set_xscale(\"log\")\n",
    "    axs[row][col].set_yscale(\"log\")\n",
    "\n",
    "    axs[row][col].set_xlim(1, len(values))\n",
    "    # axs[row][col].set_ylim(0.01, 10)\n",
    "    # axs[row][col].set_ylim(\n",
    "    #     -(10 ** math.ceil(math.log10(abs(min_exploitability)))),\n",
    "    #     10 ** math.ceil(math.log10(max_exploitability)),\n",
    "    # )\n",
    "\n",
    "    # axs[row][col].set_yticks(\n",
    "    #     [\n",
    "    #         -(10**i)\n",
    "    #         for i in range(\n",
    "    #             math.ceil(math.log10(abs(min_exploitability))),\n",
    "    #             math.floor(math.log10(abs(min_exploitability))) - 1,\n",
    "    #             -1,\n",
    "    #         )\n",
    "    #         if -(10**i) < min_exploitability\n",
    "    #     ]\n",
    "    #     + [0]\n",
    "    #     + [\n",
    "    #         10**i\n",
    "    #         for i in range(\n",
    "    #             math.ceil(math.log10(max_exploitability)),\n",
    "    #             math.floor(math.log10(max_exploitability)) + 1,\n",
    "    #         )\n",
    "    #         if 10**i > max_exploitability\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    if key in targets and targets[key] is not None:\n",
    "        axs[row][col].axhline(\n",
    "            y=targets[key],\n",
    "            color=\"r\",\n",
    "            linestyle=\"dashed\",\n",
    "            label=\"Target Exploitability: {}\".format(targets[key]),\n",
    "        )\n",
    "\n",
    "    axs[row][col].legend()\n",
    "\n",
    "\n",
    "def plot_trials(scores: list, file_name: str, final_trial: int = 0):\n",
    "    fig, axs = plt.subplots(\n",
    "        1,\n",
    "        1,\n",
    "        figsize=(10, 5),\n",
    "        squeeze=False,\n",
    "    )\n",
    "    if final_trial > 0:\n",
    "        x = np.arange(1, final_trial + 1)\n",
    "        scores = scores[:final_trial]\n",
    "    else:\n",
    "        x = np.arange(1, len(scores) + 1)\n",
    "    axs[0][0].scatter(x, scores)\n",
    "    best_fit_x, best_fit_y = np.polyfit(x, scores, 1)\n",
    "    axs[0][0].plot(\n",
    "        x,\n",
    "        best_fit_x * x + best_fit_y,\n",
    "        color=\"g\",\n",
    "        label=\"Best Fit Line\",\n",
    "        linestyle=\"dotted\",\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\"Score of Hyperopt trials over time for Rainbow DQN on CartPole-v1\")\n",
    "    axs[0][0].set_xlabel(\"Trial\")\n",
    "    axs[0][0].set_ylabel(\"Score\")\n",
    "    plt.savefig(f\"./graphs/{file_name}.png\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "stat_keys_to_plot_funcs = {\n",
    "    \"test_score\": plot_scores,\n",
    "    \"score\": plot_scores,\n",
    "    \"policy_loss\": plot_loss,\n",
    "    \"value_loss\": plot_loss,\n",
    "    \"l2_loss\": plot_loss,\n",
    "    \"loss\": plot_loss,\n",
    "    \"rl_loss\": plot_loss,\n",
    "    \"sl_loss\": plot_loss,\n",
    "    \"exploitability\": plot_exploitability,  # should this be plot_scores?\n",
    "}\n",
    "\n",
    "\n",
    "def plot_graphs(\n",
    "    stats: dict,\n",
    "    targets: dict,\n",
    "    step: int,\n",
    "    frames_seen: int,\n",
    "    time_taken: float,\n",
    "    model_name: str,\n",
    "    dir: str = \"./checkpoints/graphs\",\n",
    "):\n",
    "    num_plots = len(stats)\n",
    "    sqrt_num_plots = math.ceil(np.sqrt(num_plots))\n",
    "    fig, axs = plt.subplots(\n",
    "        sqrt_num_plots,\n",
    "        sqrt_num_plots,\n",
    "        figsize=(10 * sqrt_num_plots, 5 * sqrt_num_plots),\n",
    "        squeeze=False,\n",
    "    )\n",
    "\n",
    "    hours = int(time_taken // 3600)\n",
    "    minutes = int((time_taken % 3600) // 60)\n",
    "    seconds = int(time_taken % 60)\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"training stats | training step {} | frames seen {} | time taken {} hours {} minutes {} seconds\".format(\n",
    "            step, frames_seen, hours, minutes, seconds\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, (key, values) in enumerate(stats.items()):\n",
    "        row = i // sqrt_num_plots\n",
    "        col = i % sqrt_num_plots\n",
    "\n",
    "        if key in stat_keys_to_plot_funcs:\n",
    "            stat_keys_to_plot_funcs[key](axs, key, values, targets, row, col)\n",
    "        else:\n",
    "            default_plot_func(axs, key, values, targets, row, col)\n",
    "\n",
    "    for i in range(num_plots, sqrt_num_plots**2):\n",
    "        row = i // sqrt_num_plots\n",
    "        col = i % sqrt_num_plots\n",
    "        fig.delaxes(axs[row][col])\n",
    "\n",
    "    # plt.show()\n",
    "    assert os.path.exists(dir), f\"Directory {dir} does not exist\"\n",
    "    plt.savefig(\"{}/{}.png\".format(dir, model_name))\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_comparisons(\n",
    "    stats: list[dict],\n",
    "    model_name: str,\n",
    "    dir: str = \"./checkpoints/graphs\",\n",
    "):\n",
    "    num_plots = len(stats[0])\n",
    "    sqrt_num_plots = math.ceil(np.sqrt(num_plots))\n",
    "    fig, axs = plt.subplots(\n",
    "        sqrt_num_plots,\n",
    "        sqrt_num_plots,\n",
    "        figsize=(10 * sqrt_num_plots, 5 * sqrt_num_plots),\n",
    "        squeeze=False,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\"Comparison of training stats\")\n",
    "\n",
    "    for i, (key, _) in enumerate(stats[0].items()):\n",
    "        row = i // sqrt_num_plots\n",
    "        col = i % sqrt_num_plots\n",
    "        # max_value = float(\"-inf\")\n",
    "        # min_value = float(\"inf\")\n",
    "        max_len = 0\n",
    "        for s in stats:\n",
    "            values = s[key]\n",
    "            # print(values)\n",
    "            max_len = max(max_len, len(values))\n",
    "            print(max_len)\n",
    "            # max_value = max(max_value, max(values))\n",
    "            # min_value = min(min_value, min(values))\n",
    "            if key in stat_keys_to_plot_funcs:\n",
    "                stat_keys_to_plot_funcs[key](axs, key, values, {}, row, col)\n",
    "                axs[row][col].set_xlim(0, max_len)\n",
    "            else:\n",
    "                default_plot_func(axs, key, values, {}, row, col)\n",
    "\n",
    "        # axs[row][col].set_ylim(min_value, max_value)\n",
    "\n",
    "    for i in range(num_plots, sqrt_num_plots**2):\n",
    "        row = i // sqrt_num_plots\n",
    "        col = i % sqrt_num_plots\n",
    "        fig.delaxes(axs[row][col])\n",
    "\n",
    "    # plt.show()\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    plt.savefig(\"{}/{}.png\".format(dir, model_name))\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def prepare_kernel_initializers(kernel_initializer: str, output_layer: bool = False):\n",
    "    if kernel_initializer == \"pytorch_default\":\n",
    "        return None\n",
    "    if kernel_initializer == \"glorot_uniform\":\n",
    "        return nn.init.xavier_uniform_\n",
    "    elif kernel_initializer == \"glorot_normal\":\n",
    "        return nn.init.xavier_normal_\n",
    "    elif kernel_initializer == \"he_uniform\":\n",
    "        return nn.init.kaiming_uniform_\n",
    "    elif kernel_initializer == \"he_normal\":\n",
    "        return nn.init.kaiming_normal_\n",
    "    elif kernel_initializer == \"variance_baseline\":\n",
    "        return VarianceScaling()\n",
    "    elif kernel_initializer == \"variance_0.1\":\n",
    "        return VarianceScaling(scale=0.1)\n",
    "    elif kernel_initializer == \"variance_0.3\":\n",
    "        return VarianceScaling(scale=0.3)\n",
    "    elif kernel_initializer == \"variance_0.8\":\n",
    "        return VarianceScaling(scale=0.8)\n",
    "    elif kernel_initializer == \"variance_3\":\n",
    "        return VarianceScaling(scale=3)\n",
    "    elif kernel_initializer == \"variance_5\":\n",
    "        return VarianceScaling(scale=5)\n",
    "    elif kernel_initializer == \"variance_10\":\n",
    "        return VarianceScaling(scale=10)\n",
    "    # TODO\n",
    "    # elif kernel_initializer == \"lecun_uniform\":\n",
    "    #     return LecunUniform(seed=np.random.seed())\n",
    "    # elif kernel_initializer == \"lecun_normal\":\n",
    "    #     return LecunNormal(seed=np.random.seed())\n",
    "    elif kernel_initializer == \"orthogonal\":\n",
    "        return nn.init.orthogonal_\n",
    "\n",
    "    raise ValueError(f\"Invalid kernel initializer: {kernel_initializer}\")\n",
    "\n",
    "\n",
    "def prepare_activations(activation: str):\n",
    "    # print(\"Activation to prase: \", activation)\n",
    "    if activation == \"linear\":\n",
    "        return nn.Identity()\n",
    "    elif activation == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif activation == \"relu6\":\n",
    "        return nn.ReLU6()\n",
    "    elif activation == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif activation == \"softplus\":\n",
    "        return nn.Softplus()\n",
    "    elif activation == \"soft_sign\":\n",
    "        return nn.Softsign()\n",
    "    elif activation == \"silu\" or activation == \"swish\":\n",
    "        return nn.SiLU()\n",
    "    elif activation == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    # elif activation == \"log_sigmoid\":\n",
    "    #     return nn.LogSigmoid()\n",
    "    elif activation == \"hard_sigmoid\":\n",
    "        return nn.Hardsigmoid()\n",
    "    # elif activation == \"hard_silu\" or activation == \"hard_swish\":\n",
    "    #     return nn.Hardswish()\n",
    "    # elif activation == \"hard_tanh\":\n",
    "    #     return nn.Hardtanh()\n",
    "    elif activation == \"elu\":\n",
    "        return nn.ELU()\n",
    "    # elif activation == \"celu\":\n",
    "    #     return nn.CELU()\n",
    "    elif activation == \"selu\":\n",
    "        return nn.SELU()\n",
    "    elif activation == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    # elif activation == \"glu\":\n",
    "    #     return nn.GLU()\n",
    "\n",
    "    raise ValueError(f\"Activation {activation} not recognized\")\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(\n",
    "    q_values: list[float], info: dict, epsilon: float, wrapper=np.argmax\n",
    "):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # print(\"selecting a random move\")\n",
    "        if \"legal_moves\" in info:\n",
    "            # print(\"using legal moves\")\n",
    "            return random.choice(info[\"legal_moves\"])\n",
    "        else:\n",
    "            q_values = q_values.reshape(-1)\n",
    "            return random.choice(range(len(q_values)))\n",
    "    else:\n",
    "        # try:\n",
    "        # print(\"using provided wrapper to select action\")\n",
    "        return wrapper(q_values, info)\n",
    "    # except:\n",
    "    #     return wrapper(q_values)\n",
    "\n",
    "\n",
    "def add_dirichlet_noise(\n",
    "    policy: list[float], dirichlet_alpha: float, exploration_fraction: float\n",
    "):\n",
    "    # MAKE ALPHAZERO USE THIS\n",
    "    noise = np.random.dirichlet([dirichlet_alpha] * len(policy))\n",
    "    frac = exploration_fraction\n",
    "    for i, n in enumerate(noise):\n",
    "        policy[i] = (1 - frac) * policy[i] + frac * n\n",
    "    return policy\n",
    "\n",
    "\n",
    "def augment_game(game, flip_y: bool = False, flip_x: bool = False, rot90: bool = False):\n",
    "    # augmented_games[0] = rotate 90\n",
    "    # augmented_games[1] = rotate 180\n",
    "    # augmented_games[2] = rotate 270\n",
    "    # augmented_games[3] = flip y (rotate 180 and flip x)\n",
    "    # augmented_games[4] = rotate 90 and flip y (rotate 270 and flip x)\n",
    "    # augmented_games[5] = rotate 180 and flip y (flip x)\n",
    "    # augmented_games[6] = flip y and rotate 90 (rotate 270 and flip y) (rotate 90 and flip x)\n",
    "    # augmented_games[7] = normal\n",
    "\n",
    "    if (rot90 and flip_y) or (rot90 and flip_x):\n",
    "        augemented_games = [copy.deepcopy(game) for _ in range(7)]\n",
    "        for i in range(len(game.observation_history)):\n",
    "            board = game.observation_history[i]\n",
    "            policy = game.policy_history[i]\n",
    "            augemented_games[0].observation_history[i] = np.rot90(board)\n",
    "            augemented_games[0].policy_history[i] = np.rot90(policy)\n",
    "            augemented_games[1].observation_history[i] = np.rot90(np.rot90(board))\n",
    "            augemented_games[1].policy_history[i] = np.rot90(np.rot90(policy))\n",
    "            augemented_games[2].observation_history[i] = np.rot90(\n",
    "                np.rot90(np.rot90(board))\n",
    "            )\n",
    "            augemented_games[2].policy_history[i] = np.rot90(np.rot90(np.rot90(policy)))\n",
    "            augemented_games[3].observation_history[i] = np.flipud(board)\n",
    "            augemented_games[3].policy_history[i] = np.flipud(policy)\n",
    "            augemented_games[4].observation_history[i] = np.flipud(np.rot90(board))\n",
    "            augemented_games[4].policy_history[i] = np.flipud(np.rot90(policy))\n",
    "            augemented_games[5].observation_history[i] = np.flipud(\n",
    "                np.rot90(np.rot90(board))\n",
    "            )\n",
    "            augemented_games[5].policy_history[i] = np.flipud(np.rot90(np.rot90(policy)))\n",
    "            augemented_games[6].observation_history[i] = np.rot90(np.flipud(board))\n",
    "            augemented_games[6].policy_history[i] = np.rot90(np.flipud(policy))\n",
    "    elif rot90 and not flip_y and not flip_x:\n",
    "        augemented_games = [copy.deepcopy(game) for _ in range(3)]\n",
    "        augemented_games[0].observation_history = [\n",
    "            np.rot90(board) for board in game.observation_history\n",
    "        ]\n",
    "        augemented_games[0].policy_history = [\n",
    "            np.rot90(policy) for policy in game.policy_history\n",
    "        ]\n",
    "        augemented_games[1].observation_history = [\n",
    "            np.rot90(np.rot90(board)) for board in game.observation_history\n",
    "        ]\n",
    "        augemented_games[1].policy_history = [\n",
    "            np.rot90(np.rot90(policy)) for policy in game.policy_history\n",
    "        ]\n",
    "        augemented_games[2].observation_history = [\n",
    "            np.rot90(np.rot90(np.rot90(board))) for board in game.observation_history\n",
    "        ]\n",
    "        augemented_games[2].policy_history = [\n",
    "            np.rot90(np.rot90(np.rot90(policy)) for policy in game.policy_history)\n",
    "        ]\n",
    "    elif flip_y and not rot90 and not flip_x:\n",
    "        augemented_games = [copy.deepcopy(game)]\n",
    "        augemented_games[0].observation_history = [\n",
    "            np.flipud(board) for board in game.observation_history\n",
    "        ]\n",
    "        augemented_games[0].policy_history = [\n",
    "            np.flipud(policy) for policy in game.policy_history\n",
    "        ]\n",
    "\n",
    "    elif flip_x and not rot90 and not flip_y:\n",
    "        augemented_games = [copy.deepcopy(game) for _ in range(1)]\n",
    "        augemented_games[0].observation_history = [\n",
    "            np.fliplr(board) for board in game.observation_history\n",
    "        ]\n",
    "        augemented_games[0].policy_history = [\n",
    "            np.fliplr(policy) for policy in game.policy_history\n",
    "        ]\n",
    "\n",
    "    augemented_games.append(game)\n",
    "    return augemented_games\n",
    "\n",
    "\n",
    "def augment_board(\n",
    "    board, policy, flip_y: bool = False, flip_x: bool = False, rot90: bool = False\n",
    "):\n",
    "    if (rot90 and flip_y) or (rot90 and flip_x):\n",
    "        augemented_boards = [copy.deepcopy(board) for _ in range(7)]\n",
    "        augmented_policies = [copy.deepcopy(policy) for _ in range(7)]\n",
    "        augemented_boards[0] = np.rot90(board)\n",
    "        augmented_policies[0] = np.rot90(policy)\n",
    "        augemented_boards[1] = np.rot90(np.rot90(board))\n",
    "        augmented_policies[1] = np.rot90(np.rot90(policy))\n",
    "        augemented_boards[2] = np.rot90(np.rot90(np.rot90(board)))\n",
    "        augmented_policies[2] = np.rot90(np.rot90(np.rot90(policy)))\n",
    "        augemented_boards[3] = np.flipud(board)\n",
    "        augmented_policies[3] = np.flipud(policy)\n",
    "        augemented_boards[4] = np.flipud(np.rot90(board))\n",
    "        augmented_policies[4] = np.flipud(np.rot90(policy))\n",
    "        augemented_boards[5] = np.flipud(np.rot90(np.rot90(board)))\n",
    "        augmented_policies[5] = np.flipud(np.rot90(np.rot90(policy)))\n",
    "        augemented_boards[6] = np.rot90(np.flipud(board))\n",
    "        augmented_policies[6] = np.rot90(np.flipud(policy))\n",
    "    elif rot90 and not flip_y and not flip_x:\n",
    "        augemented_boards = [copy.deepcopy(board) for _ in range(3)]\n",
    "        augmented_policies = [copy.deepcopy(policy) for _ in range(3)]\n",
    "        augemented_boards[0] = np.rot90(board)\n",
    "        augmented_policies[0] = np.rot90(policy)\n",
    "        augemented_boards[1] = np.rot90(np.rot90(board))\n",
    "        augmented_policies[1] = np.rot90(np.rot90(policy))\n",
    "        augemented_boards[2] = np.rot90(np.rot90(np.rot90(board)))\n",
    "        augmented_policies[2] = np.rot90(np.rot90(np.rot90(policy)))\n",
    "    elif flip_y and not rot90 and not flip_x:\n",
    "        augemented_boards = [copy.deepcopy(board)]\n",
    "        augmented_policies = [copy.deepcopy(policy)]\n",
    "        augemented_boards[0] = np.flipud(board)\n",
    "        augmented_policies[0] = np.flipud(policy)\n",
    "    elif flip_x and not rot90 and not flip_y:\n",
    "        augemented_boards = [copy.deepcopy(board)]\n",
    "        augmented_policies = [copy.deepcopy(policy)]\n",
    "        augemented_boards[0] = np.fliplr(board)\n",
    "        augmented_policies[0] = np.fliplr(policy)\n",
    "    augemented_boards.append(board)\n",
    "    augmented_policies.append(policy)\n",
    "    return augemented_boards, augmented_policies\n",
    "\n",
    "\n",
    "def sample_by_random_indices(\n",
    "    max_index_or_1darray, batch_size: int, with_replacement=False\n",
    ") -> npt.NDArray[np.int64]:\n",
    "    \"\"\"\n",
    "    Sample from a numpy array using indices\n",
    "    \"\"\"\n",
    "    return np.random.choice(max_index_or_1darray, batch_size, replace=with_replacement)\n",
    "\n",
    "\n",
    "def sample_by_indices_probability(\n",
    "    max_index_or_1darray, batch_size: int, probabilities: npt.NDArray[np.float64]\n",
    ") -> npt.NDArray[np.int64]:\n",
    "    \"\"\"\n",
    "    Sample from a numpy array using indices\n",
    "    \"\"\"\n",
    "    return np.random.choice(max_index_or_1darray, batch_size, p=probabilities)\n",
    "\n",
    "\n",
    "def sample_tree_proportional(\n",
    "    tree, batch_size: int, max_size: int\n",
    ") -> npt.NDArray[np.int64]:\n",
    "    \"\"\"\n",
    "    tree: SumSegmentTree\n",
    "    Sample proportionally from a sum segment tree. Used in prioritized experience replay\n",
    "    \"\"\"\n",
    "    indices = np.zeros(batch_size, dtype=np.int64)\n",
    "    total_priority = tree.sum(0, max_size - 1)\n",
    "    priority_segment = total_priority / batch_size\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        l = priority_segment * i\n",
    "        h = priority_segment * (i + 1)\n",
    "        upperbound = np.random.uniform(l, h)\n",
    "        indices[i] = tree.retrieve(upperbound)\n",
    "        # print(tree[indices[i]])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def reward_clipping(reward: float, lower_bound: float = -1, upper_bound: float = 1):\n",
    "    if reward < lower_bound:\n",
    "        return lower_bound\n",
    "    elif reward > upper_bound:\n",
    "        return upper_bound\n",
    "    return reward\n",
    "\n",
    "\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def to_lists(l: list[Iterable]) -> list[Tuple]:\n",
    "    \"\"\"Convert a list of iterables to a zip of tuples\n",
    "\n",
    "    Args:\n",
    "        list (list[Iterable]): A list of iterables, e.g. [(1,1,1),(2,2,2),(3,3,3)]\n",
    "\n",
    "    Returns:\n",
    "        list[Tuple]: A list of tuples, i.e. [(1,2,3), (1,2,3), (1,2,3)]\n",
    "    \"\"\"\n",
    "\n",
    "    return list(zip(*l))\n",
    "\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now().timestamp()\n",
    "\n",
    "\n",
    "_epsilon = 1e-7\n",
    "\n",
    "\n",
    "def categorical_crossentropy(predicted: torch.Tensor, target: torch.Tensor, axis=-1):\n",
    "    # print(predicted)\n",
    "    predicted = predicted / torch.sum(predicted, dim=axis, keepdim=True)\n",
    "    # print(predicted)\n",
    "    predicted = torch.clamp(predicted, _epsilon, 1.0 - _epsilon)\n",
    "    # print(predicted)\n",
    "    log_prob = torch.log(predicted)\n",
    "    return -torch.sum(log_prob * target, axis=axis)\n",
    "\n",
    "\n",
    "class CategoricalCrossentropyLoss:\n",
    "    def __init__(self, from_logits=False, axis=-1):\n",
    "        self.from_logits = from_logits\n",
    "        self.axis = axis\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        return categorical_crossentropy(predicted, target, self.axis)\n",
    "\n",
    "\n",
    "def kl_divergence(predicted: torch.Tensor, target: torch.Tensor, axis=-1):\n",
    "    predicted = predicted / torch.sum(predicted, dim=axis, keepdim=True)\n",
    "    predicted = torch.clamp(predicted, _epsilon, 1.0)\n",
    "    target = torch.clamp(target, _epsilon, 1.0)\n",
    "    return torch.sum(target * torch.log(target / predicted), axis=axis)\n",
    "\n",
    "\n",
    "class KLDivergenceLoss:\n",
    "    def __init__(self, from_logits=False, axis=-1):\n",
    "        self.from_logits = from_logits\n",
    "        self.axis = axis\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        return kl_divergence(predicted, target, self.axis)\n",
    "\n",
    "\n",
    "def huber(predicted: torch.Tensor, target: torch.Tensor, axis=-1, delta: float = 1.0):\n",
    "    diff = torch.abs(predicted - target)\n",
    "    return torch.where(diff < delta, 0.5 * diff**2, delta * (diff - 0.5 * delta)).view(-1)\n",
    "\n",
    "\n",
    "class HuberLoss:\n",
    "    def __init__(self, axis=-1, delta: float = 1.0):\n",
    "        self.axis = axis\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        return huber(predicted, target, axis=self.axis, delta=self.delta)\n",
    "\n",
    "\n",
    "def mse(predicted: torch.Tensor, target: torch.Tensor):\n",
    "    # print(predicted)\n",
    "    # print(target)\n",
    "    return (predicted - target) ** 2\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        return mse(predicted, target)\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "Loss = Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
    "\n",
    "\n",
    "def calculate_padding(i: int, k: int, s: int) -> Tuple[int, int]:\n",
    "    \"\"\"Calculate both padding sizes along 1 dimension for a given input length, kernel length, and stride\n",
    "\n",
    "    Args:\n",
    "        i (int): input length\n",
    "        k (int): kernel length\n",
    "        s (int): stride\n",
    "\n",
    "    Returns:\n",
    "        (p_1, p_2): where p_1 = p_2 - 1 for uneven padding and p_1 == p_2 for even padding\n",
    "    \"\"\"\n",
    "\n",
    "    p = (i - 1) * s - i + k\n",
    "    p_1 = p // 2\n",
    "    p_2 = (p + 1) // 2\n",
    "    return (p_1, p_2)\n",
    "\n",
    "\n",
    "def generate_layer_widths(widths: list[int], max_num_layers: int) -> list[Tuple[int]]:\n",
    "    \"\"\"Create all possible combinations of widths for a given number of layers\"\"\"\n",
    "    width_combinations = []\n",
    "\n",
    "    for i in range(0, max_num_layers):\n",
    "        width_combinations.extend(itertools.combinations_with_replacement(widths, i))\n",
    "\n",
    "    return width_combinations\n",
    "\n",
    "\n",
    "def hyperopt_analysis(\n",
    "    data_dir: str,\n",
    "    file_name: str,\n",
    "    viable_trial_threshold: int,\n",
    "    step: int,\n",
    "    final_trial: int = 0,\n",
    "    eval_method: str = \"final_score\",\n",
    "):\n",
    "    trials = pickle.load(open(f\"{data_dir}/{file_name}.p\", \"rb\"))\n",
    "    if final_trial > 0:\n",
    "        print(\"Number of trials: {}\".format(final_trial))\n",
    "    else:\n",
    "        print(\"Number of trials: {}\".format(len(trials.trials)))\n",
    "    # losses.sort()\n",
    "    # print(len(os.listdir(f\"{data_dir}/checkpoints\")) - 1)\n",
    "    # print(len(trials.trials))\n",
    "\n",
    "    checkpoints = os.listdir(f\"{data_dir}/checkpoints\")\n",
    "    checkpoints.remove(\"videos\") if \"videos\" in checkpoints else None\n",
    "    checkpoints.remove(\".DS_Store\") if \".DS_Store\" in checkpoints else None\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
    "    if final_trial > 0:\n",
    "        checkpoints = checkpoints[:final_trial]\n",
    "\n",
    "    viable_throughout_trials = []\n",
    "    final_rolling_averages = []\n",
    "    final_std_devs = []\n",
    "    scores = []\n",
    "    losses = []\n",
    "    failed_trials = 0\n",
    "    for i, trial in enumerate(trials.trials):\n",
    "        losses.append(trial[\"result\"][\"loss\"])\n",
    "        if final_trial > 0 and i >= final_trial:\n",
    "            break\n",
    "        # print(trial[\"result\"][\"status\"])\n",
    "        if trial[\"result\"][\"status\"] == \"fail\":\n",
    "            failed_trials += 1\n",
    "            final_rolling_averages.append(trial[\"result\"][\"loss\"])\n",
    "            scores.append(trial[\"result\"][\"loss\"])\n",
    "            final_std_devs.append(trial[\"result\"][\"loss\"])\n",
    "        else:\n",
    "            # print(checkpoints[i - failed_trials])\n",
    "            # print(failed_trials)\n",
    "            # if os.path.exists(\n",
    "            #     f\"{data_dir}/checkpoints/{checkpoints[i - failed_trials]}/step_{step}/graphs_stats/stats.pkl\"\n",
    "            # ):\n",
    "            stats = pickle.load(\n",
    "                open(\n",
    "                    f\"{data_dir}/checkpoints/{checkpoints[i - failed_trials]}/step_{step}/graphs_stats/stats.pkl\",\n",
    "                    \"rb\",\n",
    "                )\n",
    "            )\n",
    "            max_score = 0\n",
    "\n",
    "            # print([stat_dict[\"score\"] for stat_dict in stats[\"test_score\"][-5:]])\n",
    "            final_rolling_averages.append(\n",
    "                np.around(\n",
    "                    np.mean(\n",
    "                        [stat_dict[\"score\"] for stat_dict in stats[\"test_score\"][-5:]]\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            final_std_devs.append(\n",
    "                np.around(\n",
    "                    np.std(\n",
    "                        [stat_dict[\"score\"] for stat_dict in stats[\"test_score\"][-5:]]\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for stat_dict in stats[\"test_score\"]:\n",
    "                if stat_dict[\"max_score\"] > max_score:\n",
    "                    max_score = stat_dict[\"max_score\"]\n",
    "\n",
    "            if max_score > viable_trial_threshold:\n",
    "                viable_throughout_trials.append(max_score)\n",
    "\n",
    "            if eval_method == \"final_score\":\n",
    "                score = -trial[\"result\"][\"loss\"]\n",
    "            elif (\n",
    "                eval_method == \"rolling_average\"\n",
    "                or eval_method == \"final_score_rolling_average\"\n",
    "            ):\n",
    "                score = stats[\"test_score\"][-1][\"score\"]\n",
    "            scores.append(score)\n",
    "\n",
    "    plot_trials(\n",
    "        scores,\n",
    "        file_name,\n",
    "        final_trial=final_trial,\n",
    "    )\n",
    "\n",
    "    res = [\n",
    "        list(x)\n",
    "        for x in zip(\n",
    "            *sorted(\n",
    "                zip(losses, scores, final_rolling_averages, final_std_devs),\n",
    "                key=itemgetter(0),\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    losses = res[0]\n",
    "    scores = res[1]\n",
    "    final_rolling_averages = res[2]\n",
    "    final_std_devs = res[3]\n",
    "    viable_trials = [score for score in scores if score > viable_trial_threshold]\n",
    "\n",
    "    print(\"Failed trials: ~{}%\".format(round(failed_trials / len(scores) * 100)))\n",
    "\n",
    "    print(\n",
    "        \"Viable trials (based on final score): ~{}%\".format(\n",
    "            round(len(viable_trials) / len(scores) * 100)\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Viable trials (throughout training): ~{}%\".format(\n",
    "            round(len(viable_throughout_trials) / len(scores) * 100)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Losses: {}\".format(losses))\n",
    "    print(\"Scores: {}\".format(scores))\n",
    "    print(\"Final rolling averages: {}\".format(final_rolling_averages))\n",
    "    print(\"Final standard deviations: {}\".format(final_std_devs))\n",
    "\n",
    "    print(\"Max loss: {}\".format(max(losses)))\n",
    "    print(\"Max score: {}\".format(max(scores)))\n",
    "    print(\"Max final rolling average: {}\".format(max(final_rolling_averages)))\n",
    "    print(\"Max final standard deviation: {}\".format(max(final_std_devs)))\n",
    "\n",
    "    print(\"Average loss: {}\".format(np.mean(losses)))\n",
    "    print(\"Average score: {}\".format(np.mean(scores)))\n",
    "    print(\"Average final rolling average: {}\".format(np.mean(final_rolling_averages)))\n",
    "    print(\"Average final standard deviation: {}\".format(np.mean(final_std_devs)))\n",
    "\n",
    "    viable_final_rolling_averages = [\n",
    "        final_rolling_averages[i]\n",
    "        for i, loss in enumerate(scores)\n",
    "        if loss > viable_trial_threshold\n",
    "    ]\n",
    "\n",
    "    viable_std_devs = [\n",
    "        final_std_devs[i]\n",
    "        for i, loss in enumerate(scores)\n",
    "        if loss > viable_trial_threshold\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        \"Average score of viable trials (based on final score): {}\".format(\n",
    "            np.mean(viable_trials)\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Average final rolling average of viable trials (based on final score): {}\".format(\n",
    "            np.mean(viable_final_rolling_averages)\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Average final standard deviation of viable trials (based on final score): {}\".format(\n",
    "            np.mean(viable_std_devs)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def graph_hyperparameter_importance(\n",
    "    data_dir: str, trials_file: str, search_space_file: str, viable_trial_threshold: int\n",
    "):\n",
    "    with open(f\"{data_dir}/{trials_file}\", \"rb\") as f:\n",
    "        trials = pickle.load(f)\n",
    "    print(trials)\n",
    "\n",
    "    search_space = pickle.load(open(f\"./search_spaces/{search_space_file}\", \"rb\"))\n",
    "\n",
    "    values_dict = defaultdict(list)\n",
    "    scores = []\n",
    "    for trial in trials.trials:\n",
    "        for key, value in space_eval(trial[\"misc\"][\"vals\"], search_space).items():\n",
    "            values_dict[key].append(value[0])\n",
    "        scores.append(-trial[\"result\"][\"loss\"])\n",
    "\n",
    "    df = pd.DataFrame(values_dict)\n",
    "    x_cols = df.columns\n",
    "    df[\"scores\"] = scores\n",
    "    # print(df)\n",
    "    df = df[df[\"scores\"] > viable_trial_threshold]\n",
    "\n",
    "    for col in x_cols:\n",
    "        if col == \"loss_function\":\n",
    "            continue\n",
    "        plt = df.plot(x=col, y=\"scores\", kind=\"scatter\")\n",
    "        grouped = df.groupby(col)[\"scores\"]\n",
    "        medians = grouped.median()\n",
    "        means = grouped.mean()\n",
    "        stddev = grouped.std()\n",
    "\n",
    "        if not (col == \"kernel_initializer\" or col == \"activation\"):\n",
    "            # plt.fill_between(medians.index, medians.values-stddev, medians.values+stddev, color=\"#00F0F0\")\n",
    "            plt.plot(means.index, means.values, color=\"#00FFFF\")\n",
    "        else:\n",
    "            plt.scatter(means.index, means.values, c=\"#00FFFF\")\n",
    "        # plt.add_line\n",
    "\n",
    "\n",
    "def calc_units(shape):\n",
    "    shape = tuple(shape)\n",
    "    if len(shape) == 1:\n",
    "        return shape + shape\n",
    "    if len(shape) == 2:\n",
    "        # dense layer -> (in_channels, out_channels)\n",
    "        return shape\n",
    "    else:\n",
    "        # conv_layer (Assuming convolution kernels (2D, 3D, or more).\n",
    "        # kernel shape: (input_depth, depth, ...)\n",
    "        in_units = shape[1]\n",
    "        out_units = shape[0]\n",
    "        c = 1\n",
    "        for dim in shape[2:]:\n",
    "            c *= dim\n",
    "        return (c * in_units, c * out_units)\n",
    "\n",
    "\n",
    "class VarianceScaling:\n",
    "    def __init__(self, scale=0.1, mode=\"fan_in\", distribution=\"uniform\"):\n",
    "        self.scale = scale\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "\n",
    "        assert mode == \"fan_in\" or mode == \"fan_out\" or mode == \"fan_avg\"\n",
    "        assert distribution == \"uniform\", \"only uniform distribution is supported\"\n",
    "\n",
    "    def __call__(self, tensor: Tensor) -> None:\n",
    "        with torch.no_grad():\n",
    "            scale = self.scale\n",
    "            shape = tensor.shape\n",
    "            in_units, out_units = calc_units(shape)\n",
    "            if self.mode == \"fan_in\":\n",
    "                scale /= in_units\n",
    "            elif self.mode == \"fan_out\":\n",
    "                scale /= out_units\n",
    "            else:\n",
    "                scale /= (in_units + out_units) / 2\n",
    "\n",
    "            limit = math.sqrt(3.0 * scale)\n",
    "            return tensor.uniform_(-limit, limit)\n",
    "\n",
    "\n",
    "def isiterable(o):\n",
    "    try:\n",
    "        it = iter(o)\n",
    "    except TypeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def tointlists(list):\n",
    "    ret = []\n",
    "    for x in list:\n",
    "        if isiterable(x):\n",
    "            ret.append(tointlists(x))\n",
    "        else:\n",
    "            ret.append(int(x))\n",
    "    return ret\n",
    "\n",
    "\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class StoppingCriteria:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def should_stop(self, details: dict) -> bool:\n",
    "        return False\n",
    "\n",
    "\n",
    "class TimeStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_runtime_sec=60 * 10):\n",
    "        self.stop_time = time.time() + max_runtime_sec\n",
    "\n",
    "    def should_stop(self, details: dict) -> bool:\n",
    "        return time.time() > self.stop_time\n",
    "\n",
    "\n",
    "class TrainingStepStoppingCritiera(StoppingCriteria):\n",
    "    def __init__(self, max_training_steps=100000):\n",
    "        self.max_training_steps = max_training_steps\n",
    "\n",
    "    def should_stop(self, details: dict) -> bool:\n",
    "        return details[\"training_step\"] > self.max_training_steps\n",
    "\n",
    "\n",
    "class EpisodesStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_episodes=100000):\n",
    "        self.max_episodes = max_episodes\n",
    "\n",
    "    def should_stop(self, details: dict) -> bool:\n",
    "        return details[\"max_episodes\"] > self.max_episodes\n",
    "\n",
    "\n",
    "class AverageScoreStoppingCritera(StoppingCriteria):\n",
    "    def __init__(self, min_avg_score: float, last_scores_length: int):\n",
    "        self.min_avg_score = min_avg_score\n",
    "        self.last_scores_length = last_scores_length\n",
    "        self.last_scores = deque(maxlen=last_scores_length)\n",
    "\n",
    "    def add_score(self, score: float):\n",
    "        self.last_scores.append(score)\n",
    "\n",
    "    def should_stop(self, details: dict) -> bool:\n",
    "        if len(self.last_scores) < self.last_scores_length:\n",
    "            return False\n",
    "\n",
    "        return np.average(self.last_scores) < self.min_avg_score\n",
    "\n",
    "\n",
    "class ApexLearnerStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self):\n",
    "        self.criterias: dict[str, StoppingCriteria] = {\n",
    "            \"time\": TimeStoppingCriteria(max_runtime_sec=1.5 * 60 * 60),\n",
    "            \"training_step\": TrainingStepStoppingCritiera(max_training_steps=10000),\n",
    "            \"avg_score\": AverageScoreStoppingCritera(\n",
    "                min_avg_score=15, last_scores_length=10\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def should_stop(self, details: dict) -> bool:\n",
    "        if self.criterias[\"time\"].should_stop(details):\n",
    "            return True\n",
    "\n",
    "        if details[\"training_step\"] < 10000:\n",
    "            return False\n",
    "\n",
    "        return self.criterias[\"training_step\"].should_stop(details) or self.criterias[\n",
    "            \"avg_score\"\n",
    "        ].should_stop(details)\n",
    "\n",
    "    def add_score(self, score: float):\n",
    "        tc: AverageScoreStoppingCritera = self.criterias[\"avg_score\"]\n",
    "        tc.add_score(score)\n",
    "\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import pickle\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import Module\n",
    "\n",
    "from utils import make_stack, plot_graphs\n",
    "\n",
    "# Every model should have:\n",
    "# 1. A network\n",
    "# 2. An optimizer\n",
    "# 3. A loss function\n",
    "# 4. A training method\n",
    "#       this method should have training iterations, minibatches, and training steps\n",
    "# 6. A select_action method\n",
    "# 7. A predict method\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "class ConfigBase:\n",
    "    def parse_field(\n",
    "        self, field_name, default=None, wrapper=None, required=True, dtype=None\n",
    "    ):\n",
    "        if field_name in self.config_dict:\n",
    "            val = self.config_dict[field_name]\n",
    "            # print(\"value: \", val)\n",
    "            print(f\"Using         {field_name:30}: {val}\")\n",
    "            if wrapper is not None:\n",
    "                return wrapper(val)\n",
    "            return self.config_dict[field_name]\n",
    "\n",
    "        if default is not None:\n",
    "            print(f\"Using default {field_name:30}: {default}\")\n",
    "            if wrapper is not None:\n",
    "                return wrapper(default)\n",
    "            return default\n",
    "\n",
    "        if required:\n",
    "            raise ValueError(\n",
    "                f\"Missing required field without default value: {field_name}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Using         {field_name:30}: {default}\")\n",
    "\n",
    "        if field_name in self._parsed_fields:\n",
    "            print(\"warning: duplicate field: \", field_name)\n",
    "        self._parsed_fields.add(field_name)\n",
    "\n",
    "    def __init__(self, config_dict: dict):\n",
    "        self.config_dict = config_dict\n",
    "        self._parsed_fields = set()\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            o = yaml.load(f, yaml.Loader)\n",
    "            print(o)\n",
    "            a = cls(config_dict=o[\"config_dict\"])\n",
    "\n",
    "        return a\n",
    "\n",
    "    def dump(self, filepath: str):\n",
    "        to_dump = dict(config_dict=self.config_dict)\n",
    "\n",
    "        with open(filepath, \"w\") as f:\n",
    "            yaml.dump(to_dump, f, yaml.Dumper)\n",
    "\n",
    "\n",
    "class GameConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_score,\n",
    "        min_score,\n",
    "        is_discrete,\n",
    "        is_image,\n",
    "        is_deterministic,\n",
    "        has_legal_moves,\n",
    "        perfect_information,\n",
    "        multi_agent,\n",
    "        num_players,\n",
    "    ):\n",
    "        self.max_score = max_score\n",
    "        self.min_score = min_score\n",
    "        self.is_discrete = is_discrete  # can just check the action space type instead of setting manually if the env is passed in (ALSO COULD DO THIS IN THE BASE GAME CONFIG)\n",
    "        # self.num_actions = num_actions\n",
    "        # self.observation_space = observation_space\n",
    "        self.is_image = is_image\n",
    "        self.is_deterministic = is_deterministic\n",
    "        # self.num_players = num_players (might not need this idk) <- it would likely be for muzero but could also be for rainbow and stuff when they play multiplayer games (like connect 4)\n",
    "        self.has_legal_moves = has_legal_moves\n",
    "        self.perfect_information = perfect_information\n",
    "        self.multi_agent = multi_agent\n",
    "        self.num_players = num_players\n",
    "\n",
    "    def __eq__(self, o: object) -> bool:\n",
    "        if not isinstance(o, GameConfig):\n",
    "            return False\n",
    "\n",
    "        return (\n",
    "            self.max_score == o.max_score\n",
    "            and self.min_score == o.min_score\n",
    "            and self.is_discrete == o.is_discrete\n",
    "            and self.is_image == o.is_image\n",
    "            and self.is_deterministic == o.is_deterministic\n",
    "            and self.has_legal_moves == o.has_legal_moves\n",
    "            and self.perfect_information == o.perfect_information\n",
    "            and self.multi_agent == o.multi_agent\n",
    "            and self.num_players == o.num_players\n",
    "        )\n",
    "\n",
    "\n",
    "class AtariConfig(GameConfig):\n",
    "    def __init__(self):\n",
    "        super(AtariConfig, self).__init__(\n",
    "            max_score=10,  # FROM CATEGORICAL DQN PAPER\n",
    "            min_score=-10,\n",
    "            is_discrete=True,\n",
    "            is_image=True,\n",
    "            is_deterministic=False,  # if no frameskip, then deterministic\n",
    "            has_legal_moves=False,\n",
    "            perfect_information=True,  # although it is not deterministic, it is so close to it that it is considered perfect information\n",
    "            multi_agent=False,\n",
    "            num_players=1,\n",
    "        )\n",
    "\n",
    "\n",
    "class Config(ConfigBase):\n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            o = yaml.load(f, yaml.Loader)\n",
    "            print(o)\n",
    "            a = cls(config_dict=o[\"config_dict\"], game_config=o[\"game\"])\n",
    "\n",
    "        return a\n",
    "\n",
    "    def dump(self, filepath: str):\n",
    "        to_dump = dict(config_dict=self.config_dict, game=self.game)\n",
    "\n",
    "        with open(filepath, \"w\") as f:\n",
    "            yaml.dump(to_dump, f, yaml.Dumper)\n",
    "\n",
    "    def __init__(self, config_dict: dict, game_config: GameConfig) -> None:\n",
    "        super().__init__(config_dict)\n",
    "        # could take in a game config and set an action space and observation shape here\n",
    "        # OR DO THAT IN BASE AGENT?\n",
    "        self.game = game_config\n",
    "\n",
    "        self._verify_game()\n",
    "\n",
    "        # not hyperparameters but utility things\n",
    "        self.save_intermediate_weights: bool = self.parse_field(\n",
    "            \"save_intermediate_weights\", False\n",
    "        )\n",
    "\n",
    "        # ADD LEARNING RATE SCHEDULES\n",
    "        self.training_steps: int = self.parse_field(\"training_steps\", 10000, wrapper=int)\n",
    "\n",
    "        self.adam_epsilon: float = self.parse_field(\"adam_epsilon\", 1e-6)\n",
    "        self.momentum = self.parse_field(\"momentum\", 0.9)\n",
    "        self.learning_rate: float = self.parse_field(\"learning_rate\", 0.001)\n",
    "        self.clipnorm: int = self.parse_field(\"clipnorm\", 0)\n",
    "        self.optimizer: torch.optim.Optimizer = self.parse_field(\n",
    "            \"optimizer\", torch.optim.Adam\n",
    "        )\n",
    "        self.weight_decay: float = self.parse_field(\"weight_decay\", 0.0)\n",
    "        self.loss_function: Loss = self.parse_field(\"loss_function\", required=True)\n",
    "        self.activation = self.parse_field(\n",
    "            \"activation\", \"relu\", wrapper=prepare_activations\n",
    "        )\n",
    "        self.kernel_initializer = self.parse_field(\n",
    "            \"kernel_initializer\",\n",
    "            None,\n",
    "            required=False,\n",
    "            wrapper=kernel_initializer_wrapper,\n",
    "        )\n",
    "\n",
    "        self.minibatch_size: int = self.parse_field(\"minibatch_size\", 64, wrapper=int)\n",
    "        self.replay_buffer_size: int = self.parse_field(\n",
    "            \"replay_buffer_size\", 5000, wrapper=int\n",
    "        )\n",
    "        self.min_replay_buffer_size: int = self.parse_field(\n",
    "            \"min_replay_buffer_size\", self.minibatch_size, wrapper=int\n",
    "        )\n",
    "        self.num_minibatches: int = self.parse_field(\"num_minibatches\", 1, wrapper=int)\n",
    "        self.training_iterations: int = self.parse_field(\n",
    "            \"training_iterations\", 1, wrapper=int\n",
    "        )\n",
    "        self.print_interval: int = self.parse_field(\"print_interval\", 100, wrapper=int)\n",
    "\n",
    "    def _verify_game(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        config: Config,\n",
    "        name,\n",
    "        device: torch.device = (\n",
    "            torch.device(\"cuda\")\n",
    "            if torch.cuda.is_available()\n",
    "            # MPS is sometimes useful for M2 instances, but only for large models/matrix multiplications otherwise CPU is faster\n",
    "            else (\n",
    "                torch.device(\"mps\")\n",
    "                if torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "                else torch.device(\"cpu\")\n",
    "            )\n",
    "        ),\n",
    "        from_checkpoint=False,\n",
    "    ):\n",
    "        if from_checkpoint:\n",
    "            self.from_checkpoint = True\n",
    "\n",
    "        self.model: Module = None\n",
    "        self.optimizer: Optimizer = None\n",
    "        self.model_name = name\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        self.training_time = 0\n",
    "        self.training_step = 0\n",
    "        self.total_environment_steps = 0\n",
    "        self.training_steps = self.config.training_steps\n",
    "        self.checkpoint_interval = max(self.training_steps // 30, 1)\n",
    "        self.checkpoint_trials = 5\n",
    "\n",
    "        self.env = env\n",
    "        self.test_env = self.make_test_env(env)\n",
    "        self.observation_dimensions = self.determine_observation_dimensions(env)\n",
    "\n",
    "        print(\"observation_dimensions: \", self.observation_dimensions)\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            self.num_actions = env.action_space.n\n",
    "            self.discrete_action_space = True\n",
    "        else:\n",
    "            self.num_actions = env.action_space.shape[0]\n",
    "            self.discrete_action_space = False\n",
    "\n",
    "        print(\"num_actions: \", self.num_actions)\n",
    "\n",
    "    def make_test_env(self, env: gym.Env):\n",
    "        # self.test_env = copy.deepcopy(env)\n",
    "        if hasattr(env, \"render_mode\") and env.render_mode == \"rgb_array\":\n",
    "            # assert (\n",
    "            #     self.env.render_mode == \"rgb_array\"\n",
    "            # ), \"Video recording for test_env requires render_mode to be 'rgb_array'\"\n",
    "            return gym.wrappers.RecordVideo(\n",
    "                copy.deepcopy(env),\n",
    "                \".\",\n",
    "                name_prefix=\"{}\".format(self.model_name),\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"Warning: test_env will not record videos as render_mode is not 'rgb_array'\"\n",
    "            )\n",
    "            return copy.deepcopy(env)\n",
    "\n",
    "    def determine_observation_dimensions(self, env: gym.Env):\n",
    "        if isinstance(env.observation_space, gym.spaces.Box):\n",
    "            return env.observation_space.shape\n",
    "        elif isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "            return (1,)\n",
    "        elif isinstance(env.observation_space, gym.spaces.Tuple):\n",
    "            return (len(env.observation_space.spaces),)  # for tuple of discretes\n",
    "        else:\n",
    "            raise ValueError(\"Observation space not supported\")\n",
    "\n",
    "    def train(self):\n",
    "        if self.training_steps != 0:\n",
    "            self.print_resume_training()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, states) -> torch.Tensor:\n",
    "        \"\"\"Applies necessary preprocessing steps to a batch of environment observations or a single environment observation\n",
    "        Does not alter the input state parameter, instead creating a new Tensor on the inputted device (default cpu)\n",
    "\n",
    "        Args:\n",
    "            state (Any): A or a list of state returned from self.env.step\n",
    "        Returns:\n",
    "            Tensor: The preprocessed state, a tensor of floats. If the input was a single environment step,\n",
    "                    the returned tensor is returned as outputed as if a batch of states with a length of a batch size of 1\n",
    "        \"\"\"\n",
    "\n",
    "        # always convert to np.array first for performance, recoommnded by pytorchx\n",
    "        # special case: list of compressed images (which are LazyFrames)\n",
    "        if isinstance(states[0], gym.wrappers.frame_stack.LazyFrames):\n",
    "            np_states = np.array([np.array(state) for state in states])\n",
    "        else:\n",
    "            # single observation, could be compressed or not compressed\n",
    "            # print(\"Single state\")\n",
    "            np_states = np.array(states)\n",
    "\n",
    "        # print(\"Numpyified States\", np_states)\n",
    "        prepared_state = (\n",
    "            torch.from_numpy(\n",
    "                np_states,\n",
    "            )\n",
    "            .to(torch.float32)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        # if self.config.game.is_image:\n",
    "        # normalize_images(prepared_state)\n",
    "\n",
    "        # if the state is a single number, add a dimension (not the batch dimension!, just wrapping it in []s basically)\n",
    "        if prepared_state.shape == torch.Size([]):\n",
    "            prepared_state = prepared_state.unsqueeze(0)\n",
    "\n",
    "        if prepared_state.shape == self.observation_dimensions:\n",
    "            prepared_state = make_stack(prepared_state)\n",
    "        return prepared_state\n",
    "\n",
    "    def predict(\n",
    "        self, state: torch.Tensor, *args\n",
    "    ) -> torch.Tensor:  # args is for info for player counts or legal move masks\n",
    "        \"\"\"Run inference on 1 or a batch of environment states, applying necessary preprocessing steps\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The predicted values, e.g. Q values for DQN or Q distributions for Categorical DQN\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_actions(self, predicted, info, mask_actions=False) -> torch.Tensor:\n",
    "        \"\"\"Return actions determined from the model output, appling postprocessing steps such as masking beforehand\n",
    "\n",
    "        Args:\n",
    "            state (_type_): _description_\n",
    "            legal_moves (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            Tensor: _description_\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def learn(self):\n",
    "        # raise NotImplementedError, \"Every agent should have a learn method. (Previously experience_replay)\"\n",
    "        pass\n",
    "\n",
    "    def load_optimizer_state(self, checkpoint):\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    def load_replay_buffers(self, checkpoint):\n",
    "        self.replay_buffer = checkpoint[\"replay_buffer\"]\n",
    "\n",
    "    def load_model_weights(self, checkpoint):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    def checkpoint_base(self, checkpoint):\n",
    "        checkpoint[\"training_time\"] = self.training_time\n",
    "        checkpoint[\"training_step\"] = self.training_step\n",
    "        checkpoint[\"total_environment_steps\"] = self.total_environment_steps\n",
    "        return checkpoint\n",
    "\n",
    "    def checkpoint_environment(self, checkpoint):\n",
    "        checkpoint[\"enviroment\"] = self.env\n",
    "        return checkpoint\n",
    "\n",
    "    def checkpoint_optimizer_state(self, checkpoint):\n",
    "        checkpoint[\"optimizer\"] = self.optimizer.state_dict()\n",
    "        return checkpoint\n",
    "\n",
    "    def checkpoint_replay_buffers(self, checkpoint):\n",
    "        checkpoint[\"replay_buffer\"] = self.replay_buffer\n",
    "        return checkpoint\n",
    "\n",
    "    def checkpoint_model_weights(self, checkpoint):\n",
    "        checkpoint[\"model\"] = self.model.state_dict()\n",
    "        return checkpoint\n",
    "\n",
    "    def checkpoint_extra(self, checkpoint) -> dict:\n",
    "        return checkpoint\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, *args, **kwargs):\n",
    "        cls.loaded_from_checkpoint = True\n",
    "        return cls.load_from_checkpoint(*args, **kwargs)\n",
    "\n",
    "    def load_from_checkpoint(agent_class, config_class, dir: str, training_step):\n",
    "        # load the config and checkpoint\n",
    "        training_step_dir = Path(dir, f\"step_{training_step}\")\n",
    "        weights_dir = Path(training_step_dir, \"model_weights\")\n",
    "        weights_path = str(Path(training_step_dir, f\"model_weights/weights.keras\"))\n",
    "        config = config_class.load(Path(dir, \"configs/config.yaml\"))\n",
    "        checkpoint = torch.load(weights_path)\n",
    "        env = checkpoint[\"enviroment\"]\n",
    "        model_name = checkpoint[\"model_name\"]\n",
    "\n",
    "        # construct the agent\n",
    "        agent = agent_class(env, config, model_name, from_checkpoint=True)\n",
    "\n",
    "        # load the model state (weights, optimizer, replay buffer, training time, training step, total environment steps)\n",
    "        os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "        agent.training_time = checkpoint[\"training_time\"]\n",
    "        agent.training_step = checkpoint[\"training_step\"]\n",
    "        agent.total_environment_steps = checkpoint[\"total_environment_steps\"]\n",
    "\n",
    "        agent.load_model_weights(checkpoint)\n",
    "        agent.load_optimizer_state(checkpoint)\n",
    "        agent.load_replay_buffers(checkpoint)\n",
    "\n",
    "        # load the graph stats and targets\n",
    "        with open(Path(training_step_dir, f\"graphs_stats/stats.pkl\"), \"rb\") as f:\n",
    "            agent.stats = pickle.load(f)\n",
    "        with open(Path(training_step_dir, f\"graphs_stats/targets.pkl\"), \"rb\") as f:\n",
    "            agent.targets = pickle.load(f)\n",
    "\n",
    "        return agent\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        frames_seen=None,\n",
    "        training_step=None,\n",
    "        time_taken=None,\n",
    "    ):\n",
    "        if not frames_seen is None:\n",
    "            print(\n",
    "                \"warning: frames_seen option is deprecated, update self.total_environment_steps instead\"\n",
    "            )\n",
    "\n",
    "        if not time_taken is None:\n",
    "            print(\n",
    "                \"warning: time_taken option is deprecated, update self.training_time instead\"\n",
    "            )\n",
    "\n",
    "        if not training_step is None:\n",
    "            print(\n",
    "                \"warning: training_step option is deprecated, update self.training_step instead\"\n",
    "            )\n",
    "\n",
    "        dir = Path(\"checkpoints\", self.model_name)\n",
    "        training_step_dir = Path(dir, f\"step_{self.training_step}\")\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "        # save the model state\n",
    "        if self.config.save_intermediate_weights:\n",
    "            weights_path = str(Path(training_step_dir, f\"model_weights/weights.keras\"))\n",
    "            os.makedirs(Path(training_step_dir, \"model_weights\"), exist_ok=True)\n",
    "            checkpoint = self.make_checkpoint_dict(checkpoint)\n",
    "            torch.save(checkpoint, weights_path)\n",
    "\n",
    "        if self.env.render_mode == \"rgb_array\":\n",
    "            os.makedirs(Path(training_step_dir, \"videos\"), exist_ok=True)\n",
    "\n",
    "        # save config\n",
    "        os.makedirs(Path(dir, \"configs\"), exist_ok=True)\n",
    "        self.config.dump(f\"{dir}/configs/config.yaml\")\n",
    "\n",
    "        # test model\n",
    "        test_score = self.test(\n",
    "            self.checkpoint_trials, self.training_step, training_step_dir\n",
    "        )\n",
    "        self.stats[\"test_score\"].append(test_score)\n",
    "        # save the graph stats and targets\n",
    "        os.makedirs(\n",
    "            Path(training_step_dir, f\"graphs_stats\", exist_ok=True), exist_ok=True\n",
    "        )\n",
    "        with open(Path(training_step_dir, f\"graphs_stats/stats.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.stats, f)\n",
    "        with open(Path(training_step_dir, f\"graphs_stats/targets.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.targets, f)\n",
    "\n",
    "        # to periodically clear uneeded memory, if it is drastically slowing down training you can comment this out, checkpoint less often, or do less trials\n",
    "        gc.collect()\n",
    "\n",
    "        # plot the graphs (and save the graph)\n",
    "        print(self.stats)\n",
    "        print(self.targets)\n",
    "\n",
    "        os.makedirs(Path(dir, \"graphs\"), exist_ok=True)\n",
    "        plot_graphs(\n",
    "            self.stats,\n",
    "            self.targets,\n",
    "            self.training_step if training_step is None else training_step,\n",
    "            self.total_environment_steps if frames_seen is None else frames_seen,\n",
    "            self.training_time if time_taken is None else time_taken,\n",
    "            self.model_name,\n",
    "            f\"{dir}/graphs\",\n",
    "        )\n",
    "\n",
    "    def make_checkpoint_dict(self):\n",
    "        checkpoint = self.checkpoint_base({})\n",
    "        checkpoint = self.checkpoint_environment(checkpoint)\n",
    "        checkpoint = self.checkpoint_optimizer_state(checkpoint)\n",
    "        checkpoint = self.checkpoint_replay_buffers(checkpoint)\n",
    "        checkpoint = self.checkpoint_model_weights(checkpoint)\n",
    "        checkpoint = self.checkpoint_extra(checkpoint)\n",
    "        return checkpoint\n",
    "\n",
    "    def test(self, num_trials, step, dir=\"./checkpoints\") -> None:\n",
    "        if num_trials == 0:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            \"\"\"Test the agent.\"\"\"\n",
    "            average_score = 0\n",
    "            max_score = float(\"-inf\")\n",
    "            min_score = float(\"inf\")\n",
    "            # self.test_env.reset()\n",
    "            if self.test_env.render_mode == \"rgb_array\":\n",
    "                self.test_env.episode_trigger = lambda x: (x + 1) % num_trials == 0\n",
    "                self.test_env.video_folder = \"{}/videos/{}/{}\".format(\n",
    "                    dir, self.model_name, step\n",
    "                )\n",
    "                if not os.path.exists(self.test_env.video_folder):\n",
    "                    os.makedirs(self.test_env.video_folder)\n",
    "            for trials in range(num_trials):\n",
    "                state, info = self.test_env.reset()\n",
    "\n",
    "                done = False\n",
    "                score = 0\n",
    "\n",
    "                while not done:\n",
    "                    prediction = self.predict(\n",
    "                        state, info, env=self.test_env\n",
    "                    )  # env = self.test_env is there for alpha_zero which needs to use the test env here instead of the normal env for the tree search (might be able to just use the regular env still)\n",
    "                    action = self.select_actions(\n",
    "                        prediction, info, self.config.game.has_legal_moves\n",
    "                    ).item()\n",
    "                    next_state, reward, terminated, truncated, info = self.test_env.step(\n",
    "                        action\n",
    "                    )\n",
    "                    # self.test_env.render()\n",
    "                    done = terminated or truncated\n",
    "                    state = next_state\n",
    "                    score += reward[0] if isinstance(reward, list) else reward\n",
    "                average_score += score\n",
    "                max_score = max(max_score, score)\n",
    "                min_score = min(min_score, score)\n",
    "                print(\"score: \", score)\n",
    "\n",
    "            # reset\n",
    "            # if self.test_env.render_mode != \"rgb_array\":\n",
    "            #     self.test_env.render()\n",
    "            # self.test_env.close()\n",
    "            average_score /= num_trials\n",
    "            return {\n",
    "                \"score\": average_score,\n",
    "                \"max_score\": max_score,\n",
    "                \"min_score\": min_score,\n",
    "            }\n",
    "\n",
    "    def print_training_progress(self):\n",
    "        print(f\"Training step: {self.training_step + 1}/{self.training_steps}\")\n",
    "\n",
    "    def print_resume_training(self):\n",
    "        print(\n",
    "            f\"Resuming training at step {self.training_step + 1} / {self.training_steps}\"\n",
    "        )\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(f\"\")\n",
    "\n",
    "\n",
    "def unpack(x: int | Tuple):\n",
    "    if isinstance(x, Tuple):\n",
    "        assert len(x) == 2\n",
    "        return x\n",
    "    else:\n",
    "        try:\n",
    "            x = int(x)\n",
    "            return x, x\n",
    "        except Exception as e:\n",
    "            print(f\"error converting {x} to int: \", e)\n",
    "\n",
    "\n",
    "class Conv2dStack(nn.Module):\n",
    "    @staticmethod\n",
    "    def calculate_same_padding(i, k, s) -> Tuple[None | Tuple[int], None | str | Tuple]:\n",
    "        \"\"\"Calculate pytorch inputs for same padding\n",
    "        Args:\n",
    "            i (int, int) or int: (h, w) or (w, w)\n",
    "            k (int, int) or int: (k_h, k_w) or (k, k)\n",
    "            s (int, int) or int: (s_h, s_w) or (s, s)\n",
    "        Returns:\n",
    "            Tuple[manual_pad_padding, torch_conv2d_padding_input]: Either the manual padding that must be applied (first element of tuple) or the input to the torch padding argument of the Conv2d layer\n",
    "        \"\"\"\n",
    "\n",
    "        if s == 1:\n",
    "            return None, \"same\"\n",
    "        h, w = unpack(i)\n",
    "        k_h, k_w = unpack(k)\n",
    "        s_h, s_w = unpack(s)\n",
    "        p_h = calculate_padding(h, k_h, s_h)\n",
    "        p_w = calculate_padding(w, k_w, s_w)\n",
    "        if p_h[0] == p_h[1] and p_w[0] == p_w[1]:\n",
    "            return None, (p_h[0], p_w[0])\n",
    "        else:\n",
    "            # not torch compatiable, manually pad with torch.nn.functional.pad\n",
    "            return (*p_w, *p_h), None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple[int],\n",
    "        filters: list[int],\n",
    "        kernel_sizes: list[int | Tuple[int, int]],\n",
    "        strides: list[int | Tuple[int, int]],\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        noisy_sigma: float = 0,\n",
    "    ):\n",
    "        \"\"\"A sequence of convolution layers with the activation function applied after each layer.\n",
    "        Always applies the minimum zero-padding that ensures the output shape is equal to the input shape.\n",
    "        Input shape in \"BCHW\" form, i.e. (batch_size, input_channels, height, width)\n",
    "        \"\"\"\n",
    "        super(Conv2dStack, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        # [B, C_in, H, W]\n",
    "        assert len(input_shape) == 4\n",
    "        assert len(filters) == len(kernel_sizes) == len(strides)\n",
    "        assert len(filters) > 0\n",
    "\n",
    "        self.noisy = noisy_sigma != 0\n",
    "        if self.noisy:\n",
    "            print(\"warning: Noisy convolutions not implemented yet\")\n",
    "            # raise NotImplementedError(\"\")\n",
    "\n",
    "        current_input_channels = input_shape[1]\n",
    "        for i in range(len(filters)):\n",
    "\n",
    "            h, w = input_shape[2], input_shape[3]\n",
    "            manual_padding, torch_padding = self.calculate_same_padding(\n",
    "                (h, w), kernel_sizes[i], strides[i]\n",
    "            )\n",
    "\n",
    "            if not torch_padding is None:\n",
    "                layer = nn.Conv2d(\n",
    "                    in_channels=current_input_channels,\n",
    "                    out_channels=filters[i],\n",
    "                    kernel_size=kernel_sizes[i],\n",
    "                    stride=strides[i],\n",
    "                    padding=torch_padding,\n",
    "                )\n",
    "            else:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.ZeroPad2d(manual_padding),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=current_input_channels,\n",
    "                        out_channels=filters[i],\n",
    "                        kernel_size=kernel_sizes[i],\n",
    "                        stride=strides[i],\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            self.conv_layers.append(layer)\n",
    "            current_input_channels = filters[i]\n",
    "\n",
    "        self._output_len = current_input_channels\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        def initialize_if_conv(m: nn.Module):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                initializer(m.weight)\n",
    "\n",
    "        self.apply(initialize_if_conv)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.conv_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        assert self.noisy\n",
    "\n",
    "        # noisy not implemented\n",
    "\n",
    "        # for layer in self.conv_layers:\n",
    "        #     # layer.reset_noise()\n",
    "        # return\n",
    "\n",
    "    def remove_noise(self):\n",
    "        assert self.noisy\n",
    "\n",
    "        # noisy not implemented\n",
    "\n",
    "        # for layer in self.conv_layers:\n",
    "        #     # layer.reset_noise()\n",
    "        # return\n",
    "\n",
    "    @property\n",
    "    def output_channels(self):\n",
    "        return self._output_len\n",
    "\n",
    "\n",
    "from torch import nn, Tensor, functional\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, bias: bool = True, *args, **kwargs\n",
    "    ):\n",
    "        super(Dense, self).__init__(*args, **kwargs)\n",
    "        self.layer = nn.Linear(\n",
    "            in_features=in_features, out_features=out_features, bias=bias\n",
    "        )\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        initializer(self.layer.weight)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.layer(inputs)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return self.layer.extra_repr()\n",
    "\n",
    "\n",
    "class NoisyDense(nn.Module):\n",
    "    \"\"\"See https://arxiv.org/pdf/1706.10295.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def f(x: Tensor):\n",
    "        return x.sgn() * (x.abs().sqrt())\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        initial_sigma: float = 0.5,\n",
    "        use_factorized: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.initial_sigma = initial_sigma\n",
    "        self.use_factorized = use_factorized\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.mu_w = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.sigma_w = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.eps_w = self.register_buffer(\"eps_w\", torch.empty(out_features, in_features))\n",
    "        if self.use_bias:\n",
    "            self.mu_b = nn.Parameter(torch.empty(out_features))\n",
    "            self.sigma_b = nn.Parameter(torch.empty(out_features))\n",
    "            self.eps_b = self.register_buffer(\"eps_b\", torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"mu_b\", None)\n",
    "            self.register_parameter(\"sigma_b\", None)\n",
    "            self.eps_b = self.register_buffer(\"eps_b\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        if self.use_factorized:\n",
    "            eps_i = torch.randn(1, self.in_features).to(self.mu_w.device)\n",
    "            eps_j = torch.randn(self.out_features, 1).to(self.mu_w.device)\n",
    "            self.eps_w = self.f(eps_j) @ self.f(eps_i)\n",
    "            self.eps_b = self.f(eps_j).reshape(self.out_features)\n",
    "        else:\n",
    "            self.eps_w = self.f(torch.randn(self.mu_w.shape)).to(self.mu_w.device)\n",
    "            if self.use_bias:\n",
    "                self.eps_b = self.f(torch.randn(size=self.mu_b.shape)).to(\n",
    "                    self.mu_w.device\n",
    "                )\n",
    "\n",
    "    def remove_noise(self) -> None:\n",
    "        self.eps_w = torch.zeros_like(self.mu_w).to(self.mu_w.device)\n",
    "        if self.use_bias:\n",
    "            self.eps_b = torch.zeros_like(self.mu_b).to(self.mu_w.device)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        p = self.in_features\n",
    "        if self.use_factorized:\n",
    "            mu_init = 1.0 / (p**0.5)\n",
    "            sigma_init = self.initial_sigma / (p**0.5)\n",
    "        else:\n",
    "            mu_init = (3.0 / p) ** 0.5\n",
    "            sigma_init = 0.017\n",
    "\n",
    "        nn.init.constant_(self.sigma_w, sigma_init)\n",
    "        nn.init.uniform_(self.mu_w, -mu_init, mu_init)\n",
    "        if self.use_bias:\n",
    "            nn.init.constant_(self.sigma_b, sigma_init)\n",
    "            nn.init.uniform_(self.mu_b, -mu_init, mu_init)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.mu_w + self.sigma_w * self.eps_w\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        if self.use_bias:\n",
    "            return self.mu_b + self.sigma_b * self.eps_b\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return functional.F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}, initial_sigma={self.initial_sigma}, use_factorized={self.use_factorized}\"\n",
    "\n",
    "\n",
    "def build_dense(in_features: int, out_features: int, sigma: float = 0):\n",
    "    if sigma == 0:\n",
    "        return Dense(in_features, out_features)\n",
    "    else:\n",
    "        return NoisyDense(in_features, out_features)\n",
    "\n",
    "\n",
    "class DenseStack(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_width: int,\n",
    "        widths: list[int],\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        noisy_sigma: float = 0,\n",
    "    ):\n",
    "        super(DenseStack, self).__init__()\n",
    "        self.dense_layers: nn.ModuleList = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "\n",
    "        assert len(widths) > 0\n",
    "        self.noisy = noisy_sigma != 0\n",
    "\n",
    "        current_input_width = initial_width\n",
    "        for i in range(len(widths)):\n",
    "            layer = build_dense(\n",
    "                in_features=current_input_width,\n",
    "                out_features=widths[i],\n",
    "                sigma=noisy_sigma,\n",
    "            )\n",
    "            self.dense_layers.append(layer)\n",
    "            current_input_width = widths[i]\n",
    "\n",
    "        self.initial_width = initial_width\n",
    "        self._output_len = current_input_width\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        for layer in self.dense_layers:\n",
    "            layer.initialize(initializer)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        x = inputs\n",
    "        for layer in self.dense_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        assert self.noisy\n",
    "\n",
    "        for layer in self.dense_layers:\n",
    "            layer.reset_noise()\n",
    "        return\n",
    "\n",
    "    def remove_noise(self) -> None:\n",
    "        assert self.noisy\n",
    "\n",
    "        for layer in self.dense_layers:\n",
    "            layer.remove_noise()\n",
    "        return\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.initial_width}, out_width={self.output_width}, noisy={self.noisy}\"\n",
    "\n",
    "    @property\n",
    "    def output_width(self):\n",
    "        return self._output_len\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple[int],\n",
    "        filters: list[int],\n",
    "        kernel_sizes: list[int | Tuple[int, int]],\n",
    "        strides: list[int | Tuple[int, int]],\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        noisy_sigma: float = 0,\n",
    "    ):\n",
    "        \"\"\"A sequence of residual layers with the activation function applied after each layer.\n",
    "        Always applies the minimum zero-padding that ensures the output shape is equal to the input shape.\n",
    "        Input shape in \"BCHW\" form, i.e. (batch_size, input_channels, height, width)\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.residual_layers = nn.ModuleList()\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        # [B, C_in, H, W]\n",
    "        assert (\n",
    "            len(input_shape) == 4\n",
    "            and len(filters) == len(kernel_sizes) == len(strides)\n",
    "            and len(filters) > 0\n",
    "        )\n",
    "\n",
    "        self.noisy = noisy_sigma != 0\n",
    "        if self.noisy:\n",
    "            print(\"warning: Noisy convolutions not implemented yet\")\n",
    "            # raise NotImplementedError(\"\")\n",
    "\n",
    "        current_input_channels = input_shape[1]\n",
    "\n",
    "        for i in range(len(filters)):\n",
    "            print(current_input_channels)\n",
    "            layer = Residual(\n",
    "                in_channels=current_input_channels,\n",
    "                out_channels=filters[i],\n",
    "                kernel_size=kernel_sizes[i],\n",
    "                stride=strides[i],\n",
    "            )\n",
    "            self.residual_layers.append(layer)\n",
    "            current_input_channels = filters[i]\n",
    "\n",
    "        self._output_len = current_input_channels\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        def initialize_if_conv(m: nn.Module):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                initializer(m.weight)\n",
    "\n",
    "        self.apply(initialize_if_conv)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.residual_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        assert self.noisy\n",
    "\n",
    "        # noisy not implemented\n",
    "\n",
    "        # for layer in self.conv_layers:\n",
    "        #     # layer.reset_noise()\n",
    "        # return\n",
    "\n",
    "    def remove_noise(self):\n",
    "        assert self.noisy\n",
    "\n",
    "        # noisy not implemented\n",
    "\n",
    "        # for layer in self.conv_layers:\n",
    "        #     # layer.reset_noise()\n",
    "        # return\n",
    "\n",
    "    @property\n",
    "    def output_channels(self):\n",
    "        return self._output_len\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "    ):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "\n",
    "        # REGULARIZATION?\n",
    "        self.bn1 = nn.BatchNorm2d(\n",
    "            num_features=out_channels,\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "\n",
    "        # REGULARIZATION?\n",
    "        self.bn2 = nn.BatchNorm2d(\n",
    "            num_features=out_channels,\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=\"same\",\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        def initialize_if_conv(m: nn.Module):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                initializer(m.weight)\n",
    "\n",
    "        self.apply(initialize_if_conv)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = self.downsample(inputs) if self.downsample else inputs\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x + residual)\n",
    "        return x\n",
    "\n",
    "\n",
    "def kernel_initializer_wrapper(x):\n",
    "    if x is None:\n",
    "        return x\n",
    "    elif isinstance(x, str):\n",
    "        return prepare_kernel_initializers(x)\n",
    "    else:\n",
    "        assert callable(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RainbowConfig(Config):\n",
    "    def __init__(self, config_dict: dict, game_config):\n",
    "        super(RainbowConfig, self).__init__(config_dict, game_config)\n",
    "        print(\"RainbowConfig\")\n",
    "        self.residual_layers: list = self.parse_field(\"residual_layers\", [])\n",
    "        self.conv_layers: list = self.parse_field(\"conv_layers\", [])\n",
    "        self.dense_layer_widths: int = self.parse_field(\n",
    "            \"dense_layer_widths\", [128], tointlists\n",
    "        )\n",
    "        self.value_hidden_layer_widths = self.parse_field(\n",
    "            \"value_hidden_layer_widths\", [], tointlists\n",
    "        )\n",
    "        self.advantage_hidden_layer_widths: int = self.parse_field(\n",
    "            \"advantage_hidden_layer_widths\", [], tointlists\n",
    "        )\n",
    "\n",
    "        self.noisy_sigma: float = self.parse_field(\"noisy_sigma\", 0.5)\n",
    "        self.eg_epsilon: float = self.parse_field(\"eg_epsilon\", 0.00)\n",
    "        self.eg_epsilon_final: float = self.parse_field(\"eg_epsilon_final\", 0.00)\n",
    "        self.eg_epsilon_decay_type: str = self.parse_field(\n",
    "            \"eg_epsilon_decay_type\", \"linear\"\n",
    "        )\n",
    "        self.eg_epsilon_final_step: int = self.parse_field(\n",
    "            \"eg_epsilon_final_step\", self.training_steps\n",
    "        )\n",
    "\n",
    "        self.dueling: bool = self.parse_field(\"dueling\", True)\n",
    "        self.discount_factor: float = self.parse_field(\"discount_factor\", 0.99)\n",
    "        self.soft_update: bool = self.parse_field(\"soft_update\", False)\n",
    "        self.transfer_interval: int = self.parse_field(\n",
    "            \"transfer_interval\", 512, wrapper=int\n",
    "        )\n",
    "        self.ema_beta: float = self.parse_field(\"ema_beta\", 0.99)\n",
    "        self.replay_interval: int = self.parse_field(\"replay_interval\", 1, wrapper=int)\n",
    "        self.per_alpha: float = self.parse_field(\"per_alpha\", 0.6)\n",
    "        self.per_beta: float = self.parse_field(\"per_beta\", 0.5)\n",
    "        self.per_beta_final: float = self.parse_field(\"per_beta_final\", 1.0)\n",
    "        self.per_epsilon: float = self.parse_field(\"per_epsilon\", 1e-6)\n",
    "        self.n_step: int = self.parse_field(\"n_step\", 3)\n",
    "        self.atom_size: int = self.parse_field(\"atom_size\", 51, wrapper=int)\n",
    "        # assert (\n",
    "        #     self.atom_size > 1\n",
    "        # ), \"Atom size must be greater than 1, as softmax and Q distribution to Q value calculation requires more than 1 atom\"\n",
    "\n",
    "        # assert not (\n",
    "        #     self.game.is_image\n",
    "        #     and len(self.conv_layers) == 0\n",
    "        #     and len(self.residual_layers) == 0\n",
    "        # ), \"Convolutional layers must be defined for image based games\"\n",
    "\n",
    "        if len(self.conv_layers) > 0:\n",
    "            assert len(self.conv_layers[0]) == 3\n",
    "\n",
    "        # maybe don't use a game config, since if tuning for multiple games this should be the same regardless of the game <- (it is really a hyper parameter if you are tuning for multiple games or a game with unknown bounds)\n",
    "\n",
    "        # could use a MuZero min-max config and just constantly update the suport size (would this break the model?) <- might mean this is not in the config but just a part of the model\n",
    "\n",
    "        self.v_min = game_config.min_score\n",
    "        self.v_max = game_config.max_score\n",
    "\n",
    "        if self.atom_size != 1:\n",
    "            assert self.v_min != None and self.v_max != None\n",
    "\n",
    "    def _verify_game(self):\n",
    "        assert self.game.is_discrete, \"Rainbow only supports discrete action spaces\"\n",
    "\n",
    "\n",
    "class RainbowNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: RainbowConfig,\n",
    "        output_size: int,\n",
    "        input_shape: Tuple[int],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "        self.has_residual_layers = len(config.residual_layers) > 0\n",
    "        self.has_conv_layers = len(config.conv_layers) > 0\n",
    "        self.has_dense_layers = len(config.dense_layer_widths) > 0\n",
    "        assert (\n",
    "            self.has_conv_layers or self.has_dense_layers or self.has_residual_layers\n",
    "        ), \"At least one of the layers should be present.\"\n",
    "\n",
    "        self.has_value_hidden_layers = len(config.value_hidden_layer_widths) > 0\n",
    "        self.has_advantage_hidden_layers = len(config.advantage_hidden_layer_widths) > 0\n",
    "        if not self.config.dueling:\n",
    "            assert not (\n",
    "                self.has_value_hidden_layers or self.has_advantage_hidden_layers\n",
    "            ), \"Value or Advantage hidden layers are only used in dueling networks\"\n",
    "\n",
    "        self.output_size = output_size\n",
    "\n",
    "        current_shape = input_shape\n",
    "        B = current_shape[0]\n",
    "\n",
    "        if self.has_residual_layers:\n",
    "            assert (\n",
    "                len(input_shape) == 4\n",
    "            ), \"Input shape should be (B, C, H, W), got {}\".format(input_shape)\n",
    "            filters, kernel_sizes, strides = to_lists(config.residual_layers)\n",
    "\n",
    "            # (B, C_in, H, W) -> (B, C_out H, W)\n",
    "            self.residual_layers = ResidualStack(\n",
    "                input_shape=input_shape,\n",
    "                filters=filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                strides=strides,\n",
    "                activation=self.config.activation,\n",
    "                noisy_sigma=config.noisy_sigma,\n",
    "            )\n",
    "            current_shape = (\n",
    "                B,\n",
    "                self.residual_layers.output_channels,\n",
    "                current_shape[2],\n",
    "                current_shape[3],\n",
    "            )\n",
    "\n",
    "        if self.has_conv_layers:\n",
    "            assert (\n",
    "                len(input_shape) == 4\n",
    "            ), \"Input shape should be (B, C, H, W), got {}\".format(input_shape)\n",
    "            filters, kernel_sizes, strides = to_lists(config.conv_layers)\n",
    "\n",
    "            # (B, C_in, H, W) -> (B, C_out H, W)\n",
    "            self.conv_layers = Conv2dStack(\n",
    "                input_shape=input_shape,\n",
    "                filters=filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                strides=strides,\n",
    "                activation=self.config.activation,\n",
    "                noisy_sigma=config.noisy_sigma,\n",
    "            )\n",
    "            current_shape = (\n",
    "                B,\n",
    "                self.conv_layers.output_channels,\n",
    "                current_shape[2],\n",
    "                current_shape[3],\n",
    "            )\n",
    "\n",
    "        if self.has_dense_layers:\n",
    "            if len(current_shape) == 4:\n",
    "                initial_width = current_shape[1] * current_shape[2] * current_shape[3]\n",
    "            else:\n",
    "                assert len(current_shape) == 2\n",
    "                initial_width = current_shape[1]\n",
    "\n",
    "            # (B, width_in) -> (B, width_out)\n",
    "            self.dense_layers = DenseStack(\n",
    "                initial_width=initial_width,\n",
    "                widths=self.config.dense_layer_widths,\n",
    "                activation=self.config.activation,\n",
    "                noisy_sigma=self.config.noisy_sigma,\n",
    "            )\n",
    "            current_shape = (\n",
    "                B,\n",
    "                self.dense_layers.output_width,\n",
    "            )\n",
    "\n",
    "        if len(current_shape) == 4:\n",
    "            initial_width = current_shape[1] * current_shape[2] * current_shape[3]\n",
    "        else:\n",
    "            assert (\n",
    "                len(current_shape) == 2\n",
    "            ), \"Input shape should be (B, width), got {}\".format(current_shape)\n",
    "            initial_width = current_shape[1]\n",
    "\n",
    "        if self.config.dueling:\n",
    "            if self.has_value_hidden_layers:\n",
    "                # (B, width_in) -> (B, value_in_features) -> (B, atom_size)\n",
    "                self.value_hidden_layers = DenseStack(\n",
    "                    initial_width=initial_width,\n",
    "                    widths=self.config.value_hidden_layer_widths,\n",
    "                    activation=self.config.activation,\n",
    "                    noisy_sigma=self.config.noisy_sigma,\n",
    "                )\n",
    "                value_in_features = self.value_hidden_layers.output_width\n",
    "            else:\n",
    "                value_in_features = initial_width\n",
    "            # (B, value_in_features) -> (B, atom_size)\n",
    "            self.value_layer = build_dense(\n",
    "                in_features=value_in_features,\n",
    "                out_features=config.atom_size,\n",
    "                sigma=config.noisy_sigma,\n",
    "            )\n",
    "\n",
    "            if self.has_advantage_hidden_layers:\n",
    "                # (B, width_in) -> (B, advantage_in_features)\n",
    "                self.advantage_hidden_layers = DenseStack(\n",
    "                    initial_width=initial_width,\n",
    "                    widths=self.config.advantage_hidden_layer_widths,\n",
    "                    activation=self.config.activation,\n",
    "                    noisy_sigma=self.config.noisy_sigma,\n",
    "                )\n",
    "                advantage_in_features = self.advantage_hidden_layers.output_width\n",
    "            else:\n",
    "                advantage_in_features = initial_width\n",
    "            # (B, advantage_in_features) -> (B, output_size * atom_size)\n",
    "            self.advantage_layer = build_dense(\n",
    "                in_features=advantage_in_features,\n",
    "                out_features=output_size * config.atom_size,\n",
    "                sigma=self.config.noisy_sigma,\n",
    "            )\n",
    "        else:\n",
    "            self.distribution_layer = build_dense(\n",
    "                in_features=initial_width,\n",
    "                out_features=self.output_size * self.config.atom_size,\n",
    "                sigma=self.config.noisy_sigma,\n",
    "            )\n",
    "\n",
    "    def initialize(self, initializer: Callable[[Tensor], None]) -> None:\n",
    "        if self.has_residual_layers:\n",
    "            self.residual_layers.initialize(initializer)\n",
    "        if self.has_conv_layers:\n",
    "            self.conv_layers.initialize(initializer)\n",
    "        if self.has_dense_layers:\n",
    "            self.dense_layers.initialize(initializer)\n",
    "        if self.has_value_hidden_layers:\n",
    "            self.value_hidden_layers.initialize(initializer)\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            self.advantage_hidden_layers.initialize(initializer)\n",
    "        if self.config.dueling:\n",
    "            self.value_layer.initialize(initializer)\n",
    "            self.advantage_layer.initialize(initializer)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        if self.has_conv_layers:\n",
    "            assert inputs.dim() == 4\n",
    "\n",
    "        # (B, *)\n",
    "        S = inputs\n",
    "        # (B, C_in, H, W) -> (B, C_out, H, W)\n",
    "        if self.has_residual_layers:\n",
    "            S = self.residual_layers(S)\n",
    "\n",
    "        # (B, C_in, H, W) -> (B, C_out, H, W)\n",
    "        if self.has_conv_layers:\n",
    "            S = self.conv_layers(S)\n",
    "\n",
    "        # (B, *) -> (B, dense_features_in)\n",
    "        S = S.flatten(1, -1)\n",
    "\n",
    "        # (B, dense_features_in) -> (B, dense_features_out)\n",
    "        if self.has_dense_layers:\n",
    "            S = self.dense_layers(S)\n",
    "\n",
    "        if self.config.dueling:\n",
    "            # (B, value_hidden_in) -> (B, value_hidden_out)\n",
    "            if self.has_value_hidden_layers:\n",
    "                v = self.value_hidden_layers(S)\n",
    "            else:\n",
    "                v = S\n",
    "\n",
    "            # (B, value_hidden_in || dense_features_out) -> (B, atom_size) -> (B, 1, atom_size)\n",
    "            v: Tensor = self.value_layer(v).view(-1, 1, self.config.atom_size)\n",
    "\n",
    "            # (B, adv_hidden_in) -> (B, adv_hidden_out)\n",
    "            if self.has_advantage_hidden_layers:\n",
    "                A = self.advantage_hidden_layers(S)\n",
    "            else:\n",
    "                A = S\n",
    "\n",
    "            # (B, adv_hidden_out || dense_features_out) -> (B, output_size * atom_size) -> (B, output_size, atom_size)\n",
    "            A: Tensor = self.advantage_layer(A).view(\n",
    "                -1, self.output_size, self.config.atom_size\n",
    "            )\n",
    "\n",
    "            # (B, output_size, atom_size) -[mean(1)]-> (B, 1, atom_size)\n",
    "            a_mean = A.mean(1, keepdim=True)\n",
    "\n",
    "            # (B, 1, atom_size) +\n",
    "            # (B, output_size, atom_size) +\n",
    "            # (B, 1, atom_size)\n",
    "            # is valid broadcasting operation\n",
    "            Q = v + A - a_mean\n",
    "\n",
    "            # -[softmax(2)]-> turns the atom dimension into a valid p.d.f.\n",
    "            # ONLY CLIP FOR CATEGORICAL CROSS ENTROPY LOSS TO PREVENT NAN\n",
    "            # MIGHT BE ABLE TO REMOVE CLIPPING ENTIRELY SINCE I DONT THINK THE TENSORFLOW LOSSES CAN RETURN NaN\n",
    "            # q.clip(1e-3, 1)\n",
    "        else:\n",
    "            # (B, dense_features_out) -> (B, output_size, atom_size)\n",
    "            Q = self.distribution_layer(S).view(\n",
    "                -1, self.output_size, self.config.atom_size\n",
    "            )\n",
    "\n",
    "        if self.config.atom_size == 1:\n",
    "            return Q.squeeze(-1)\n",
    "        else:\n",
    "            return Q.softmax(dim=-1)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        if self.config.noisy_sigma != 0:\n",
    "            if self.has_residual_layers:\n",
    "                self.residual_layers.reset_noise()\n",
    "            if self.has_conv_layers:\n",
    "                self.conv_layers.reset_noise()\n",
    "            if self.has_dense_layers:\n",
    "                self.dense_layers.reset_noise()\n",
    "            if self.has_value_hidden_layers:\n",
    "                self.value_hidden_layers.reset_noise()\n",
    "            if self.has_advantage_hidden_layers:\n",
    "                self.advantage_hidden_layers.reset_noise()\n",
    "            if self.config.dueling:\n",
    "                self.value_layer.reset_noise()\n",
    "                self.advantage_layer.reset_noise()\n",
    "\n",
    "    def remove_noise(self):\n",
    "        if self.config.noisy_sigma != 0:\n",
    "            if self.has_residual_layers:\n",
    "                self.residual_layers.remove_noise()\n",
    "            if self.has_conv_layers:\n",
    "                self.conv_layers.remove_noise()\n",
    "            if self.has_dense_layers:\n",
    "                self.dense_layers.remove_noise()\n",
    "            if self.has_value_hidden_layers:\n",
    "                self.value_hidden_layers.remove_noise()\n",
    "            if self.has_advantage_hidden_layers:\n",
    "                self.advantage_hidden_layers.remove_noise()\n",
    "            if self.config.dueling:\n",
    "                self.value_layer.remove_noise()\n",
    "                self.advantage_layer.remove_noise()\n",
    "\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import operator\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class SegmentTree:\n",
    "    \"\"\"Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int)\n",
    "        tree (list)\n",
    "        operation (function)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, operation: Callable, init_value: float):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "            operation (function)\n",
    "            init_value (float)\n",
    "\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            capacity > 0 and capacity & (capacity - 1) == 0\n",
    "        ), \"capacity must be positive and a power of 2.\"\n",
    "        self.capacity = capacity\n",
    "        self.tree = [init_value for _ in range(2 * capacity)]\n",
    "        self.operation = operation\n",
    "\n",
    "    def _operate_helper(\n",
    "        self, start: int, end: int, node: int, node_start: int, node_end: int\n",
    "    ) -> float:\n",
    "        \"\"\"Returns result of operation in segment.\"\"\"\n",
    "        if start == node_start and end == node_end:\n",
    "            return self.tree[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._operate_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self.operation(\n",
    "                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n",
    "                )\n",
    "\n",
    "    def operate(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns result of applying `self.operation`.\"\"\"\n",
    "        if end <= 0:\n",
    "            end += self.capacity\n",
    "        end -= 1\n",
    "\n",
    "        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        idx += self.capacity\n",
    "        self.tree[idx] = val\n",
    "\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    \"\"\"Create SumSegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=operator.add, init_value=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n",
    "        return super(SumSegmentTree, self).operate(start, end)\n",
    "\n",
    "    def retrieve(self, upperbound: float) -> int:\n",
    "        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n",
    "        # TODO: Check assert case and fix bug\n",
    "        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n",
    "\n",
    "        idx = 1\n",
    "\n",
    "        while idx < self.capacity:  # while non-leaf\n",
    "            left = 2 * idx\n",
    "            right = left + 1\n",
    "            if self.tree[left] > upperbound:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                upperbound -= self.tree[left]\n",
    "                idx = right\n",
    "        return idx - self.capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    \"\"\"Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=min, init_value=float(\"inf\")\n",
    "        )\n",
    "\n",
    "    def min(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n",
    "        return super(MinSegmentTree, self).operate(start, end)\n",
    "\n",
    "\n",
    "class FastSumTree(object):\n",
    "    # https://medium.com/free-code-camp/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = (\n",
    "            capacity  # number of leaf nodes (final nodes) that contains experiences\n",
    "        )\n",
    "\n",
    "        self.tree = np.zeros(2 * self.capacity - 1)  # sub tree\n",
    "        # self.data = np.zeros(self.capacity, object)  # contains the experiences\n",
    "\n",
    "    def add(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        tree_index = idx + self.capacity - 1\n",
    "        # self.data[self.data_pointer] = data\n",
    "        self.update(tree_index, val)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "    def update(self, tree_index: int, val: float):\n",
    "        change = val - self.tree[tree_index]\n",
    "        # print(\"change\", change)\n",
    "        self.tree[tree_index] = val\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "            # print(\"new value\", self.tree[tree_index])\n",
    "\n",
    "    def retrieve(self, v: float):\n",
    "        parent_index = 0\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "\n",
    "class BaseReplayBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        batch_size: int = None,\n",
    "        compressed_observations: bool = False,\n",
    "    ):\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size if batch_size is not None else max_size\n",
    "        self.compressed_observations = compressed_observations\n",
    "\n",
    "        self.clear()\n",
    "        assert self.size == 0, \"Replay buffer should be empty at initialization\"\n",
    "        assert self.max_size > 0, \"Replay buffer should have a maximum size\"\n",
    "        assert self.batch_size > 0, \"Replay buffer batch size should be greater than 0\"\n",
    "\n",
    "    def store(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_from_indices(self, indices: list[int]):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clear(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load(self, path):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(\n",
    "        self, num_players: int\n",
    "    ):  # num_actions, discount=1.0, n_step=1, gamma=0.99\n",
    "        self.length = 0\n",
    "        self.observation_history = []\n",
    "        self.rewards = []\n",
    "        self.policy_history = []\n",
    "        self.value_history = []\n",
    "        self.action_history = []\n",
    "        self.info_history = []\n",
    "\n",
    "        self.num_players = num_players\n",
    "\n",
    "    def append(\n",
    "        self,\n",
    "        observation,\n",
    "        reward: int,\n",
    "        policy,\n",
    "        value=None,\n",
    "        action=None,\n",
    "        info=None,\n",
    "    ):\n",
    "        self.observation_history.append(copy.deepcopy(observation))\n",
    "        self.rewards.append(reward)\n",
    "        self.policy_history.append(policy)\n",
    "        self.value_history.append(value)\n",
    "        self.action_history.append(action)\n",
    "        self.info_history.append(info)\n",
    "        self.length += 1\n",
    "\n",
    "    def set_rewards(self):\n",
    "        print(\"Initial Rewards\", self.rewards)\n",
    "        final_reward = self.rewards[-1]\n",
    "        for i in reversed(range(self.length)):\n",
    "            self.rewards[i] = (\n",
    "                final_reward[i % self.num_players]\n",
    "                # if i % self.num_players == (self.length - 1) % self.num_players\n",
    "                # else -final_reward\n",
    "            )\n",
    "        print(\"Updated Rewards\", self.rewards)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class BaseGameReplayBuffer(BaseReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__(max_size=max_size, batch_size=batch_size)\n",
    "\n",
    "    def store(self, game: Game):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "        self.size += 1\n",
    "\n",
    "    def sample(self):\n",
    "        move_sum = float(sum([len(game) for game in self.buffer]))\n",
    "        games: list[Game] = np.random.choice(\n",
    "            self.buffer,\n",
    "            self.batch_size,\n",
    "            p=[len(game) / move_sum for game in self.buffer],\n",
    "        )\n",
    "\n",
    "        return [(game, np.random.randint(len(game))) for game in games]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer: list[Game] = []\n",
    "        self.size = 0\n",
    "\n",
    "\n",
    "class BaseDQNReplayBuffer(BaseReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_dimensions: tuple,\n",
    "        observation_dtype: np.dtype,\n",
    "        max_size: int,\n",
    "        batch_size: int = 32,\n",
    "        compressed_observations: bool = False,\n",
    "    ):\n",
    "        self.observation_dimensions = observation_dimensions\n",
    "        self.observation_dtype = observation_dtype\n",
    "        print(observation_dtype)\n",
    "        super().__init__(\n",
    "            max_size=max_size,\n",
    "            batch_size=batch_size,\n",
    "            compressed_observations=compressed_observations,\n",
    "        )\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        observation,\n",
    "        info: dict,\n",
    "        action,\n",
    "        reward: float,\n",
    "        next_observation,\n",
    "        next_info: dict,\n",
    "        done: bool,\n",
    "        id=None,\n",
    "    ):\n",
    "        # compute n-step return and store\n",
    "        self.id_buffer[self.pointer] = id\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.next_observation_buffer[self.pointer] = next_observation\n",
    "        self.done_buffer[self.pointer] = done\n",
    "        self.info_buffer[self.pointer] = info\n",
    "        self.next_info_buffer[self.pointer] = next_info\n",
    "\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def clear(self):\n",
    "        if self.compressed_observations:\n",
    "            self.observation_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "            self.next_observation_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "        else:\n",
    "            observation_buffer_shape = (self.max_size,) + self.observation_dimensions\n",
    "            self.observation_buffer = np.zeros(\n",
    "                observation_buffer_shape, self.observation_dtype\n",
    "            )\n",
    "            self.next_observation_buffer = np.zeros(\n",
    "                observation_buffer_shape, dtype=self.observation_dtype\n",
    "            )\n",
    "\n",
    "        self.id_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "        self.action_buffer = np.zeros(self.max_size, dtype=np.uint8)\n",
    "        self.reward_buffer = np.zeros(self.max_size, dtype=np.float16)\n",
    "        self.done_buffer = np.zeros(self.max_size, dtype=np.bool_)\n",
    "        self.info_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "        self.next_info_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "        self.pointer = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def sample(self):\n",
    "        indices = np.random.choice(self.size, self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            observations=self.observation_buffer[indices],\n",
    "            next_observations=self.next_observation_buffer[indices],\n",
    "            actions=self.action_buffer[indices],\n",
    "            rewards=self.reward_buffer[indices],\n",
    "            dones=self.done_buffer[indices],\n",
    "            ids=self.id_buffer[indices],\n",
    "            info=self.info_buffer[indices],\n",
    "            next_info=self.next_info_buffer[indices],\n",
    "        )\n",
    "\n",
    "    def sample_from_indices(self, indices: list[int]):\n",
    "        return dict(\n",
    "            observations=self.observation_buffer[indices],\n",
    "            next_observations=self.next_observation_buffer[indices],\n",
    "            actions=self.action_buffer[indices],\n",
    "            rewards=self.reward_buffer[indices],\n",
    "            dones=self.done_buffer[indices],\n",
    "            ids=self.id_buffer[indices],\n",
    "            infos=self.info_buffer[indices],\n",
    "            next_infos=self.next_info_buffer[indices],\n",
    "        )\n",
    "\n",
    "    def __check_id__(self, index: int, id: str) -> bool:\n",
    "        return self.id_buffer[index] == id\n",
    "\n",
    "\n",
    "class BasePPOReplayBuffer(BaseReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_dimensions,\n",
    "        observation_dtype: np.dtype,\n",
    "        max_size: int,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        compressed_observations: bool = False,\n",
    "    ):\n",
    "        self.observation_dimensions = observation_dimensions\n",
    "        self.observation_dtype = observation_dtype\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        super().__init__(\n",
    "            max_size=max_size, compressed_observations=compressed_observations\n",
    "        )\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        observation,\n",
    "        info: dict,\n",
    "        action,\n",
    "        value: float,\n",
    "        log_probability: float,\n",
    "        reward: float,\n",
    "        id=None,\n",
    "    ):\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.log_probability_buffer[self.pointer] = log_probability\n",
    "        self.info_buffer[self.pointer] = info\n",
    "\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self):\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean = np.mean(self.advantage_buffer)\n",
    "        advantage_std = np.std(self.advantage_buffer)\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / (\n",
    "            advantage_std + 1e-10\n",
    "        )  # avoid division by zero\n",
    "        return dict(\n",
    "            observations=self.observation_buffer,\n",
    "            actions=self.action_buffer,\n",
    "            advantages=self.advantage_buffer,\n",
    "            returns=self.return_buffer,\n",
    "            log_probabilities=self.log_probability_buffer,\n",
    "            infos=self.info_buffer,\n",
    "        )\n",
    "\n",
    "    def clear(self):\n",
    "        if self.compressed_observations:\n",
    "            self.observation_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "            self.next_observation_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "        else:\n",
    "            observation_buffer_shape = (self.max_size,) + self.observation_dimensions\n",
    "            self.observation_buffer = np.zeros(\n",
    "                observation_buffer_shape, self.observation_dtype\n",
    "            )\n",
    "            self.next_observation_buffer = np.zeros(\n",
    "                observation_buffer_shape, dtype=self.observation_dtype\n",
    "            )\n",
    "        self.action_buffer = np.zeros(self.max_size, dtype=np.int8)\n",
    "        self.reward_buffer = np.zeros(self.max_size, dtype=np.float16)\n",
    "        self.advantage_buffer = np.zeros(self.max_size, dtype=np.float16)\n",
    "        self.return_buffer = np.zeros(self.max_size, dtype=np.float16)\n",
    "        self.value_buffer = np.zeros(self.max_size, dtype=np.float16)\n",
    "        self.log_probability_buffer = np.zeros(self.max_size, dtype=np.float16)\n",
    "        self.info_buffer = np.zeros(self.max_size, dtype=np.object_)\n",
    "\n",
    "        self.pointer = 0\n",
    "        self.trajectory_start_index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def finish_trajectory(self, last_value: float = 0):\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.gae_lambda\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[\n",
    "            :-1\n",
    "        ]\n",
    "        # print(discounted_cumulative_sums(deltas, self.gamma * self.gae_lambda))\n",
    "        # print(discounted_cumulative_sums(deltas, self.gamma * self.gae_lambda)[:-1])\n",
    "        # print(self.advantage_buffer)\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "\n",
    "class NStepReplayBuffer(BaseDQNReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_dimensions: tuple,\n",
    "        observation_dtype: np.dtype,\n",
    "        max_size: int,\n",
    "        batch_size: int = 32,\n",
    "        n_step: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "        compressed_observations: bool = False,\n",
    "        num_players: int = 1,\n",
    "    ):\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.num_players = num_players\n",
    "        super().__init__(\n",
    "            observation_dimensions=observation_dimensions,\n",
    "            observation_dtype=observation_dtype,\n",
    "            max_size=max_size,\n",
    "            batch_size=batch_size,\n",
    "            compressed_observations=compressed_observations,\n",
    "        )\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        observation,\n",
    "        info: dict,\n",
    "        action,\n",
    "        reward: float,\n",
    "        next_observation,\n",
    "        next_info: dict,\n",
    "        done: bool,\n",
    "        id=None,\n",
    "        player: int = 0,\n",
    "    ):\n",
    "        \"\"\"Store a (s_t, a, r, s_t+1) transtion to the replay buffer.\n",
    "           Returns a valid generated n-step transition (s_t-n, a, r, s_t) with the\n",
    "           inputted observation as the next_observation (s_t)\n",
    "\n",
    "        Returns:\n",
    "            (s_t-n, a, r, s_t): where r is the n-step return calculated with the replay buffer's gamma\n",
    "        \"\"\"\n",
    "        transition = (\n",
    "            observation,\n",
    "            info,\n",
    "            action,\n",
    "            reward,\n",
    "            next_observation,\n",
    "            next_info,\n",
    "            done,\n",
    "        )\n",
    "        # print(\"store t:\", transition)\n",
    "        self.n_step_buffers[player].append(transition)\n",
    "        if len(self.n_step_buffers[player]) < self.n_step:\n",
    "            return None\n",
    "\n",
    "        # compute n-step return and store\n",
    "        reward, next_observation, next_info, done = self._get_n_step_info(player)\n",
    "        observation, info, action = self.n_step_buffers[player][0][:3]\n",
    "        n_step_transition = (\n",
    "            observation,\n",
    "            info,\n",
    "            action,\n",
    "            reward,\n",
    "            next_observation,\n",
    "            next_info,\n",
    "            done,\n",
    "        )\n",
    "        super().store(*n_step_transition, id=id)\n",
    "        return n_step_transition\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        self.n_step_buffers = [deque(maxlen=self.n_step) for q in range(self.num_players)]\n",
    "\n",
    "    def _get_n_step_info(self, player: int = 0):\n",
    "        reward, next_observation, next_info, done = self.n_step_buffers[player][-1][-4:]\n",
    "\n",
    "        for transition in reversed(list(self.n_step_buffers[player])[:-1]):\n",
    "            r, n_o, n_i, d = transition[-4:]\n",
    "            reward = r + self.gamma * reward * (1 - d)\n",
    "            next_observation, next_info, done = (\n",
    "                (n_o, n_i, d) if d else (next_observation, next_info, done)\n",
    "            )\n",
    "\n",
    "        return reward, next_observation, next_info, done\n",
    "\n",
    "\n",
    "class PrioritizedNStepReplayBuffer(NStepReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_dimensions,\n",
    "        observation_dtype: np.dtype,\n",
    "        max_size: int,\n",
    "        batch_size: int = 32,\n",
    "        max_priority: float = 1.0,\n",
    "        alpha: float = 0.6,\n",
    "        beta: float = 0.4,\n",
    "        # epsilon=0.01,\n",
    "        n_step: float = 1,\n",
    "        gamma: float = 0.99,\n",
    "        compressed_observations: bool = False,\n",
    "        num_players: int = 1,\n",
    "    ):\n",
    "        assert alpha >= 0 and alpha <= 1\n",
    "        assert beta >= 0 and beta <= 1\n",
    "        assert n_step >= 1\n",
    "        assert gamma > 0 and gamma <= 1\n",
    "\n",
    "        self.initial_max_priority = max_priority\n",
    "        super(PrioritizedNStepReplayBuffer, self).__init__(\n",
    "            observation_dimensions,\n",
    "            observation_dtype,\n",
    "            max_size,\n",
    "            batch_size,\n",
    "            n_step=n_step,\n",
    "            gamma=gamma,\n",
    "            compressed_observations=compressed_observations,\n",
    "            num_players=num_players,\n",
    "        )\n",
    "\n",
    "        self.alpha = alpha  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "        self.beta = beta\n",
    "        # self.epsilon = epsilon\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        observation,\n",
    "        info: dict,\n",
    "        action,\n",
    "        reward: float,\n",
    "        next_observation,\n",
    "        next_info: dict,\n",
    "        done: bool,\n",
    "        id=None,\n",
    "        priority: float = None,\n",
    "        player: int = 0,\n",
    "    ):\n",
    "        transition = super().store(\n",
    "            observation,\n",
    "            info,\n",
    "            action,\n",
    "            reward,\n",
    "            next_observation,\n",
    "            next_info,\n",
    "            done,\n",
    "            id,\n",
    "            player=player,\n",
    "        )\n",
    "\n",
    "        if priority is None:\n",
    "            priority = self.max_priority**self.alpha\n",
    "            self.max_priority = max(\n",
    "                self.max_priority, priority\n",
    "            )  # could remove and clip priorities in experience replay isntead\n",
    "\n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_pointer] = priority**self.alpha\n",
    "            self.min_tree[self.tree_pointer] = priority**self.alpha\n",
    "            self.tree_pointer = (self.tree_pointer + 1) % self.max_size\n",
    "\n",
    "        return transition\n",
    "\n",
    "    def set_beta(self, beta: float):\n",
    "        self.beta = beta\n",
    "\n",
    "    def store_batch(self, batch):\n",
    "        (\n",
    "            observations,\n",
    "            infos,\n",
    "            actions,\n",
    "            rewards,\n",
    "            next_observations,\n",
    "            next_infos,\n",
    "            dones,\n",
    "            ids,\n",
    "            priorities,\n",
    "        ) = batch\n",
    "        for i in range(len(observations)):\n",
    "            self.store(\n",
    "                observations[i],\n",
    "                infos[i],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_observations[i],\n",
    "                next_infos[i],\n",
    "                dones[i],\n",
    "                ids[i],\n",
    "                priorities[i],\n",
    "            )\n",
    "\n",
    "    def sample(self, throw_exception=True) -> dict:\n",
    "        if len(self) < self.batch_size:\n",
    "            if throw_exception:\n",
    "                raise \"Only {} elements in buffer expected at least {}\".format(\n",
    "                    len(self), self.batch_size\n",
    "                )\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        if self.alpha != 0.0:\n",
    "            indices = self._sample_proportional()\n",
    "        else:\n",
    "            indices = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "            # print(indices)\n",
    "        weights = np.array([self._calculate_weight(i) for i in indices])\n",
    "\n",
    "        n_step_samples = self.sample_from_indices(indices)\n",
    "        # print(n_step_samples)\n",
    "        n_step_samples.update(dict(weights=weights, indices=indices))\n",
    "        # print(n_step_samples)\n",
    "\n",
    "        return n_step_samples\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        self.max_priority = self.initial_max_priority  # (initial) priority\n",
    "        self.tree_pointer = 0\n",
    "\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "\n",
    "    def update_priorities(self, indices: list[int], priorities: list[float], ids=None):\n",
    "        # necessary for shared replay buffer\n",
    "        if ids is not None:\n",
    "            assert len(priorities) == len(ids) == len(indices)\n",
    "            assert priorities.shape == ids.shape == indices.shape\n",
    "\n",
    "            for index, id, priority in zip(indices, ids, priorities):\n",
    "                assert priority > 0, \"Negative priority: {} \\n All priorities {}\".format(\n",
    "                    priority, priorities\n",
    "                )\n",
    "                assert 0 <= index < len(self)\n",
    "\n",
    "                if self.id_buffer[index] != id:\n",
    "                    continue\n",
    "\n",
    "                self.sum_tree[index] = priority**self.alpha\n",
    "                self.min_tree[index] = priority**self.alpha\n",
    "                self.max_priority = max(self.max_priority, priority)\n",
    "        else:\n",
    "            assert len(indices) == len(priorities)\n",
    "            for index, priority in zip(indices, priorities):\n",
    "                assert priority > 0, \"Negative priority: {}\".format(priority)\n",
    "                assert 0 <= index < len(self)\n",
    "\n",
    "                self.sum_tree[index] = priority**self.alpha\n",
    "                self.min_tree[index] = priority**self.alpha\n",
    "                self.max_priority = max(\n",
    "                    self.max_priority, priority\n",
    "                )  # could remove and clip priorities in experience replay isntead\n",
    "\n",
    "        return priorities**self.alpha\n",
    "\n",
    "    def _sample_proportional(self):\n",
    "        indices = []\n",
    "        total_priority = self.sum_tree.sum(0, len(self) - 1)\n",
    "        priority_segment = total_priority / self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = priority_segment * i\n",
    "            b = priority_segment * (i + 1)\n",
    "            upperbound = np.random.uniform(a, b)\n",
    "            index = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(index)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def _calculate_weight(self, index: int):\n",
    "        min_priority = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (min_priority * len(self)) ** (-self.beta)\n",
    "        priority_sample = self.sum_tree[index] / self.sum_tree.sum()\n",
    "        weight = (priority_sample * len(self)) ** (-self.beta)\n",
    "        weight = weight / max_weight\n",
    "\n",
    "        return weight\n",
    "\n",
    "\n",
    "class FastPrioritizedReplayBuffer(NStepReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_dimensions,\n",
    "        max_size: int,\n",
    "        batch_size: int = 32,\n",
    "        max_priority: float = 1.0,\n",
    "        alpha: float = 0.6,\n",
    "        beta: float = 0.4,\n",
    "        # epsilon=0.01,\n",
    "        n_step: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        assert alpha >= 0 and alpha <= 1\n",
    "        assert beta >= 0 and beta <= 1\n",
    "        assert n_step >= 1\n",
    "        assert gamma > 0 and gamma <= 1\n",
    "\n",
    "        super(FastPrioritizedReplayBuffer, self).__init__(\n",
    "            observation_dimensions, max_size, batch_size, n_step=n_step, gamma=gamma\n",
    "        )\n",
    "\n",
    "        self.max_priority = max_priority  # (initial) priority\n",
    "        self.min_priority = max_priority\n",
    "        self.tree_pointer = 0\n",
    "\n",
    "        self.alpha = alpha  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "        self.beta = beta\n",
    "        # self.epsilon = epsilon\n",
    "\n",
    "        self.tree = FastSumTree(self.max_size)\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        observation,\n",
    "        action,\n",
    "        reward: float,\n",
    "        next_observation,\n",
    "        done: bool,\n",
    "    ):\n",
    "        transition = super().store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        # max_priority = np.max(self.tree.tree[-self.tree.capacity :])\n",
    "        # if max_priority == 0:\n",
    "        #     max_priority = self.max_priority\n",
    "\n",
    "        if transition:\n",
    "            self.tree.add(self.tree_pointer, self.max_priority)\n",
    "            self.tree_pointer = (self.tree_pointer + 1) % self.max_size\n",
    "\n",
    "        return transition\n",
    "\n",
    "    def sample(self):\n",
    "        assert len(self) >= self.batch_size\n",
    "\n",
    "        priority_segment = self.tree.total_priority / self.batch_size\n",
    "        indices, weights = np.empty((self.batch_size,), dtype=np.int32), np.empty(\n",
    "            (self.batch_size, 1), dtype=np.float32\n",
    "        )\n",
    "        for i in range(self.batch_size):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority = self.tree.retrieve(value)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            # weights[i, 0] = (self.batch_size * sampling_probabilities) ** -beta\n",
    "            weights[i, 0] = (len(self) * sampling_probabilities) ** -self.beta\n",
    "            indices[i] = index - self.tree.capacity + 1\n",
    "            indices[i] = index - self.tree.capacity + 1\n",
    "\n",
    "        # max_weight = max(weights)\n",
    "        max_weight = (\n",
    "            len(self) * self.min_priority / self.tree.total_priority\n",
    "        ) ** -self.beta\n",
    "        weights = weights / max_weight\n",
    "\n",
    "        # print(weights)\n",
    "        # print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree Time \", time() - time1)\n",
    "        # print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays\")\n",
    "        # time2 = 0\n",
    "        # time2 = time()\n",
    "        observations = self.observation_buffer[indices]\n",
    "        next_observations = self.next_observation_buffer[indices]\n",
    "        actions = self.action_buffer[indices]\n",
    "        rewards = self.reward_buffer[indices]\n",
    "        dones = self.done_buffer[indices]\n",
    "        # weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        # print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays Time \", time() - time2)\n",
    "\n",
    "        # print(\"Sampling from PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return dict(\n",
    "            observations=observations,\n",
    "            next_observations=next_observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            dones=dones,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices: list[int], priorities: list[float]):\n",
    "        assert len(indices) == len(priorities)\n",
    "        # priorities += self.epsilon\n",
    "\n",
    "        for index, priority in zip(indices, priorities):\n",
    "            assert priority > 0, \"Negative priority: {}\".format(priority)\n",
    "            # assert 0 <= index < len(self)\n",
    "            # self.tree[index] = priority ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority**self.alpha)\n",
    "            self.min_priority = min(self.min_priority, priority**self.alpha)\n",
    "            # priority = np.clip(priority, self.epsilon, self.max_priority)\n",
    "            self.tree.update(index + self.tree.capacity - 1, priority**self.alpha)\n",
    "\n",
    "\n",
    "class RainbowAgent(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        config: RainbowConfig,\n",
    "        name=f\"rainbow_{current_timestamp():.1f}\",\n",
    "        device: torch.device = (\n",
    "            torch.device(\"cuda\")\n",
    "            if torch.cuda.is_available()\n",
    "            # MPS is sometimes useful for M2 instances, but only for large models/matrix multiplications otherwise CPU is faster\n",
    "            else (\n",
    "                torch.device(\"mps\")\n",
    "                if torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "                else torch.device(\"cpu\")\n",
    "            )\n",
    "        ),\n",
    "        from_checkpoint=False,\n",
    "    ):\n",
    "        super(RainbowAgent, self).__init__(env, config, name, device=device)\n",
    "        self.model = RainbowNetwork(\n",
    "            config=config,\n",
    "            output_size=self.num_actions,\n",
    "            input_shape=(self.config.minibatch_size,) + self.observation_dimensions,\n",
    "        )\n",
    "        self.target_model = RainbowNetwork(\n",
    "            config=config,\n",
    "            output_size=self.num_actions,\n",
    "            input_shape=(self.config.minibatch_size,) + self.observation_dimensions,\n",
    "        )\n",
    "\n",
    "        if not self.config.kernel_initializer == None:\n",
    "            self.model.initialize(self.config.kernel_initializer)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.target_model.to(device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        if self.config.optimizer == Adam:\n",
    "            self.optimizer: torch.optim.Optimizer = self.config.optimizer(\n",
    "                params=self.model.parameters(),\n",
    "                lr=self.config.learning_rate,\n",
    "                eps=self.config.adam_epsilon,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "        elif self.config.optimizer == SGD:\n",
    "            print(\"Warning: SGD does not use adam_epsilon param\")\n",
    "            self.optimizer: torch.optim.Optimizer = self.config.optimizer(\n",
    "                params=self.model.parameters(),\n",
    "                lr=self.config.learning_rate,\n",
    "                momentum=self.config.momentum,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "\n",
    "        self.replay_buffer = PrioritizedNStepReplayBuffer(\n",
    "            observation_dimensions=self.observation_dimensions,\n",
    "            observation_dtype=self.env.observation_space.dtype,\n",
    "            max_size=self.config.replay_buffer_size,\n",
    "            batch_size=self.config.minibatch_size,\n",
    "            max_priority=1.0,\n",
    "            alpha=self.config.per_alpha,\n",
    "            beta=self.config.per_beta,\n",
    "            # epsilon=config[\"per_epsilon\"],\n",
    "            n_step=self.config.n_step,\n",
    "            gamma=self.config.discount_factor,\n",
    "            compressed_observations=(\n",
    "                self.env.lz4_compress if hasattr(self.env, \"lz4_compress\") else False\n",
    "            ),\n",
    "            num_players=self.config.game.num_players,\n",
    "        )\n",
    "\n",
    "        # could use a MuZero min-max config and just constantly update the suport size (would this break the model?)\n",
    "        # self.v_min = self.config.v_min\n",
    "        # self.v_max = self.config.v_max\n",
    "\n",
    "        self.support = torch.linspace(\n",
    "            self.config.v_min,\n",
    "            self.config.v_max,\n",
    "            self.config.atom_size,\n",
    "            device=device,\n",
    "        ).to(device)\n",
    "        \"\"\"row vector Tensor(atom_size)\n",
    "        \"\"\"\n",
    "\n",
    "        self.eg_epsilon = self.config.eg_epsilon\n",
    "\n",
    "        self.stats = {\n",
    "            \"score\": [],\n",
    "            \"loss\": [],\n",
    "            \"test_score\": [],\n",
    "        }\n",
    "        self.targets = {\n",
    "            \"score\": self.env.spec.reward_threshold,\n",
    "            \"test_score\": self.env.spec.reward_threshold,\n",
    "        }\n",
    "\n",
    "    def checkpoint_model_weights(self, checkpoint):\n",
    "        checkpoint = super().checkpoint_model_weights(checkpoint)\n",
    "        checkpoint[\"target_model\"] = self.target_model.state_dict()\n",
    "\n",
    "    def load_model_weights(self, checkpoint):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.target_model.load_state_dict(checkpoint[\"target_model\"])\n",
    "        self.target_model.eval()\n",
    "\n",
    "    def predict(self, states, *args, **kwargs) -> torch.Tensor:\n",
    "        # could change type later\n",
    "        state_input = self.preprocess(states)\n",
    "        q_distribution: torch.Tensor = self.model(state_input)\n",
    "        return q_distribution\n",
    "\n",
    "    def predict_target(self, states) -> torch.Tensor:\n",
    "        # could change type later\n",
    "        state_input = self.preprocess(states)\n",
    "        q_distribution: torch.Tensor = self.target_model(state_input)\n",
    "        return q_distribution\n",
    "\n",
    "    def select_actions(self, distribution, info: dict = None, mask_actions: bool = True):\n",
    "        assert info is not None if mask_actions else True, \"Need info to mask actions\"\n",
    "        # print(info)\n",
    "        if self.config.atom_size > 1:\n",
    "            q_values = distribution * self.support\n",
    "            q_values = q_values.sum(2, keepdim=False)\n",
    "        else:\n",
    "            q_values = distribution\n",
    "        if mask_actions:\n",
    "            legal_moves = get_legal_moves(info)\n",
    "            q_values = action_mask(\n",
    "                q_values, legal_moves, mask_value=-float(\"inf\"), device=self.device\n",
    "            )\n",
    "        # print(\"Q Values\", q_values)\n",
    "        # q_values with argmax ties\n",
    "        # selected_actions = torch.stack(\n",
    "        #     [\n",
    "        #         torch.tensor(np.random.choice(np.where(x.cpu() == x.cpu().max())[0]))\n",
    "        #         for x in q_values\n",
    "        #     ]\n",
    "        # )\n",
    "        # print(selected_actions)\n",
    "        selected_actions = q_values.argmax(1, keepdim=False)\n",
    "        return selected_actions\n",
    "\n",
    "    def learn(self) -> np.ndarray:\n",
    "        losses = np.zeros(self.config.training_iterations)\n",
    "        for i in range(self.config.training_iterations):\n",
    "            samples = self.replay_buffer.sample()\n",
    "            loss = self.learn_from_sample(samples)\n",
    "            losses[i] = loss\n",
    "        return losses\n",
    "\n",
    "    def learn_from_sample(self, samples: dict):\n",
    "        observations, weights, actions = (\n",
    "            samples[\"observations\"],\n",
    "            samples[\"weights\"],\n",
    "            torch.from_numpy(samples[\"actions\"]).to(self.device).long(),\n",
    "        )\n",
    "        # print(\"actions\", actions)\n",
    "\n",
    "        # print(\"Observations\", observations)\n",
    "        # (B, outputs, atom_size) -[index action dimension by actions]> (B, atom_size)\n",
    "        online_predictions = self.predict(observations)[\n",
    "            range(self.config.minibatch_size), actions\n",
    "        ]\n",
    "        # for param in self.model.parameters():\n",
    "        #     print(param)\n",
    "        # print(self.predict(observations))\n",
    "        # print(online_predictions)\n",
    "        # (B, atom_size)\n",
    "        if self.config.atom_size > 1:\n",
    "            assert isinstance(self.config.loss_function, KLDivergenceLoss) or isinstance(\n",
    "                self.config.loss_function, CategoricalCrossentropyLoss\n",
    "            ), \"Only KLDivergenceLoss and CategoricalCrossentropyLoss are supported for atom_size > 1, recieved {}\".format(\n",
    "                self.config.loss_function\n",
    "            )\n",
    "            target_predictions = self.compute_target_distributions(samples)\n",
    "        else:\n",
    "            # print(\"using default dqn loss\")\n",
    "            assert isinstance(self.config.loss_function, HuberLoss) or isinstance(\n",
    "                self.config.loss_function, MSELoss\n",
    "            ), \"Only HuberLoss or MSELoss are supported for atom_size = 1, recieved {}\".format(\n",
    "                self.config.loss_function\n",
    "            )\n",
    "            next_observations, rewards, dones = (\n",
    "                torch.from_numpy(samples[\"next_observations\"]).to(self.device),\n",
    "                torch.from_numpy(samples[\"rewards\"]).to(self.device),\n",
    "                torch.from_numpy(samples[\"dones\"]).to(self.device),\n",
    "            )\n",
    "            next_infos = samples[\"next_infos\"]\n",
    "            target_predictions = self.predict_target(next_observations)  # next q values\n",
    "            # print(\"Next q values\", target_predictions)\n",
    "            # print(\"Current q values\", online_predictions)\n",
    "            # print(self.predict(next_observations))\n",
    "            next_actions = self.select_actions(\n",
    "                self.predict(next_observations),  # current q values\n",
    "                info=next_infos,\n",
    "                mask_actions=self.config.game.has_legal_moves,\n",
    "            )\n",
    "            # print(\"Next actions\", next_actions)\n",
    "            target_predictions = target_predictions[\n",
    "                range(self.config.minibatch_size), next_actions\n",
    "            ]  # this might not work\n",
    "            # print(target_predictions)\n",
    "            target_predictions = (\n",
    "                rewards + self.config.discount_factor * (~dones) * target_predictions\n",
    "            )\n",
    "            # print(target_predictions)\n",
    "\n",
    "        # print(\"predicted\", online_distributions)\n",
    "        # print(\"target\", target_distributions)\n",
    "\n",
    "        weights_cuda = torch.from_numpy(weights).to(torch.float32).to(self.device)\n",
    "        # (B)\n",
    "        elementwise_loss = self.config.loss_function(\n",
    "            online_predictions, target_predictions\n",
    "        )\n",
    "        # print(\"Loss\", elementwise_loss.mean())\n",
    "        assert torch.all(elementwise_loss) >= 0, \"Elementwise Loss: {}\".format(\n",
    "            elementwise_loss\n",
    "        )\n",
    "        assert (\n",
    "            elementwise_loss.shape == weights_cuda.shape\n",
    "        ), \"Loss Shape: {}, Weights Shape: {}\".format(\n",
    "            elementwise_loss.shape, weights_cuda.shape\n",
    "        )\n",
    "        loss = elementwise_loss * weights_cuda\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        if self.config.clipnorm > 0:\n",
    "            # print(\"clipnorm\", self.config.clipnorm)\n",
    "            clip_grad_norm_(self.model.parameters(), self.config.clipnorm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.update_replay_priorities(\n",
    "            samples=samples,\n",
    "            priorities=elementwise_loss.detach().to(\"cpu\").numpy()\n",
    "            + self.config.per_epsilon,\n",
    "        )\n",
    "        self.model.reset_noise()\n",
    "        self.target_model.reset_noise()\n",
    "        return loss.detach().to(\"cpu\").mean().item()\n",
    "\n",
    "    def update_replay_priorities(self, samples, priorities):\n",
    "        self.replay_buffer.update_priorities(samples[\"indices\"], priorities)\n",
    "\n",
    "    def compute_target_distributions(self, samples):\n",
    "        # print(\"computing target distributions\")\n",
    "        with torch.no_grad():\n",
    "            discount_factor = self.config.discount_factor**self.config.n_step\n",
    "            delta_z = (self.config.v_max - self.config.v_min) / (\n",
    "                self.config.atom_size - 1\n",
    "            )\n",
    "            next_observations, rewards, dones = (\n",
    "                samples[\"next_observations\"],\n",
    "                torch.from_numpy(samples[\"rewards\"]).to(self.device).view(-1, 1),\n",
    "                torch.from_numpy(samples[\"dones\"]).to(self.device).view(-1, 1),\n",
    "            )\n",
    "            online_distributions = self.predict(next_observations)\n",
    "            target_distributions = self.predict_target(next_observations)\n",
    "\n",
    "            # print(samples[\"next_infos\"])\n",
    "            next_actions = self.select_actions(\n",
    "                online_distributions,\n",
    "                info=samples[\"next_infos\"],\n",
    "                mask_actions=self.config.game.has_legal_moves,\n",
    "            )  # {} is the info but we are not doing action masking yet\n",
    "            # (B, outputs, atom_size) -[index by [0..B-1, a_0..a_B-1]]> (B, atom_size)\n",
    "            probabilities = target_distributions[\n",
    "                range(self.config.minibatch_size), next_actions\n",
    "            ]\n",
    "            # print(probabilities)\n",
    "\n",
    "            # (B, 1) + k(B, atom_size) * (B, atom_size) -> (B, atom_size)\n",
    "            Tz = (rewards + discount_factor * (~dones) * self.support).clamp(\n",
    "                self.config.v_min, self.config.v_max\n",
    "            )\n",
    "            # print(\"Tz\", Tz)\n",
    "\n",
    "            # all elementwise\n",
    "            b: torch.Tensor = (Tz - self.config.v_min) / delta_z\n",
    "            l, u = (\n",
    "                torch.clamp(b.floor().long(), 0, self.config.atom_size - 1),\n",
    "                torch.clamp(b.ceil().long(), 0, self.config.atom_size - 1),\n",
    "            )\n",
    "            # print(\"b\", b)\n",
    "            # print(\"l\", l)\n",
    "            # print(\"u\", u)\n",
    "\n",
    "            # Fix disappearing probability mass when l = b = u (b is int)\n",
    "            l[(u > 0) * (l == u)] -= 1\n",
    "            u[(l < (self.config.atom_size - 1)) * (l == u)] += 1\n",
    "            # print(\"fixed l\", l)\n",
    "            # print(\"fixed u\", u)\n",
    "            # dones = dones.squeeze()\n",
    "            # masked_probs = torch.ones_like(probabilities) / self.config.atom_size\n",
    "            # masked_probs[~dones] = probabilities[~dones]\n",
    "\n",
    "            m = torch.zeros_like(probabilities)\n",
    "            m.scatter_add_(dim=1, index=l, src=probabilities * ((u.float()) - b))\n",
    "            m.scatter_add_(dim=1, index=u, src=probabilities * ((b - l.float())))\n",
    "            # print(\"old_m\", (m * self.support).sum(-1))\n",
    "\n",
    "            # projected_distribution = torch.zeros_like(probabilities)\n",
    "            # projected_distribution.scatter_add_(\n",
    "            #     dim=1, index=l, src=masked_probs * (u.float() - b)\n",
    "            # )\n",
    "            # projected_distribution.scatter_add_(\n",
    "            #     dim=1, index=u, src=masked_probs * (b - l.float())\n",
    "            # )\n",
    "            # print(\"m\", (projected_distribution * self.support).sum(-1))\n",
    "            return m\n",
    "\n",
    "    def fill_replay_buffer(self):\n",
    "        print(\"replay buffer size:\", self.replay_buffer.size)\n",
    "        with torch.no_grad():\n",
    "            state, info = self.env.reset()\n",
    "            target_size = self.config.min_replay_buffer_size\n",
    "            while self.replay_buffer.size < target_size:\n",
    "                if (self.replay_buffer.size % (target_size // 100)) == 0:\n",
    "                    print(\n",
    "                        f\"filling replay buffer: {self.replay_buffer.size} / ({target_size})\"\n",
    "                    )\n",
    "                # dist = self.predict(state)\n",
    "                # action = self.select_actions(dist).item()\n",
    "                action = self.env.action_space.sample()\n",
    "                next_state, reward, terminated, truncated, next_info = self.env.step(\n",
    "                    action\n",
    "                )\n",
    "                done = terminated or truncated\n",
    "                # print(state)\n",
    "                self.replay_buffer.store(\n",
    "                    state, info, action, reward, next_state, next_info, done\n",
    "                )\n",
    "                # print(self.replay_buffer.observation_buffer[0])\n",
    "                state = next_state\n",
    "                info = next_info\n",
    "                if done:\n",
    "                    state, info = self.env.reset()\n",
    "                # gc.collect()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        if self.config.soft_update:\n",
    "            for wt, wp in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "                wt.copy_(self.config.ema_beta * wt + (1 - self.config.ema_beta) * wp)\n",
    "        else:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def update_eg_epsilon(self, training_step):\n",
    "        if self.config.eg_epsilon_decay_type == \"linear\":\n",
    "            # print(\"decaying eg epsilon linearly\")\n",
    "            self.eg_epsilon = update_linear_schedule(\n",
    "                self.config.eg_epsilon_final,\n",
    "                self.config.eg_epsilon_final_step,\n",
    "                self.config.eg_epsilon,\n",
    "                training_step,\n",
    "            )\n",
    "        elif self.config.eg_epsilon_decay_type == \"inverse_sqrt\":\n",
    "            self.eg_epsilon = update_inverse_sqrt_schedule(\n",
    "                self.config.eg_epsilon,\n",
    "                training_step,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid epsilon decay type: {}\".format(self.config.eg_epsilon_decay_type)\n",
    "            )\n",
    "\n",
    "    def train(self):\n",
    "        super().train()\n",
    "        start_time = time() - self.training_time\n",
    "        score = 0\n",
    "        target_model_updated = (False, False)  # (score, loss)\n",
    "        self.fill_replay_buffer()\n",
    "\n",
    "        state, info = self.env.reset()\n",
    "\n",
    "        while self.training_step < self.config.training_steps:\n",
    "            if self.training_step % self.config.print_interval == 0:\n",
    "                self.print_training_progress()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _ in range(self.config.replay_interval):\n",
    "                    values = self.predict(state)\n",
    "                    # print(values)\n",
    "                    action = epsilon_greedy_policy(\n",
    "                        values,\n",
    "                        info,\n",
    "                        self.eg_epsilon,\n",
    "                        wrapper=lambda values, info: self.select_actions(\n",
    "                            values, info\n",
    "                        ).item(),\n",
    "                    )\n",
    "                    # print(\"Action\", action)\n",
    "                    # print(\"Epislon Greedy Epsilon\", self.eg_epsilon)\n",
    "                    next_state, reward, terminated, truncated, next_info = self.env.step(\n",
    "                        action\n",
    "                    )\n",
    "                    done = terminated or truncated\n",
    "                    # print(\"State\", state)\n",
    "                    self.replay_buffer.store(\n",
    "                        state, info, action, reward, next_state, next_info, done\n",
    "                    )\n",
    "                    state = next_state\n",
    "                    info = next_info\n",
    "                    score += reward\n",
    "                    self.replay_buffer.set_beta(\n",
    "                        update_per_beta(\n",
    "                            self.replay_buffer.beta,\n",
    "                            self.config.per_beta_final,\n",
    "                            self.training_steps,\n",
    "                            self.config.per_beta,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    if done:\n",
    "                        state, info = self.env.reset()\n",
    "                        score_dict = {\n",
    "                            \"score\": score,\n",
    "                            \"target_model_updated\": target_model_updated[0],\n",
    "                        }\n",
    "                        self.stats[\"score\"].append(score_dict)\n",
    "                        target_model_updated = (False, target_model_updated[1])\n",
    "                        score = 0\n",
    "\n",
    "            self.update_eg_epsilon(self.training_step + 1)\n",
    "            # print(\"replay buffer size\", len(self.replay_buffer))\n",
    "            for minibatch in range(self.config.num_minibatches):\n",
    "                if len(self.replay_buffer) < self.config.min_replay_buffer_size:\n",
    "                    break\n",
    "                losses = self.learn()\n",
    "                # print(losses)\n",
    "                loss_mean = losses.mean()\n",
    "                # could do things other than taking the mean here\n",
    "                self.stats[\"loss\"].append(\n",
    "                    {\"loss\": loss_mean, \"target_model_updated\": target_model_updated[1]}\n",
    "                )\n",
    "                target_model_updated = (target_model_updated[0], False)\n",
    "\n",
    "            if self.training_step % self.config.transfer_interval == 0:\n",
    "                target_model_updated = (True, True)\n",
    "                # stats[\"test_score\"].append(\n",
    "                #     {\"target_model_weight_update\": training_step}\n",
    "                # )\n",
    "                self.update_target_model()\n",
    "\n",
    "            if self.training_step % self.checkpoint_interval == 0:\n",
    "                # print(self.stats[\"score\"])\n",
    "                # print(len(self.replay_buffer))\n",
    "                self.training_time = time() - start_time\n",
    "                self.total_environment_steps = (\n",
    "                    self.training_step * self.config.replay_interval\n",
    "                )\n",
    "                self.save_checkpoint()\n",
    "            # gc.collect()\n",
    "            self.training_step += 1\n",
    "\n",
    "        self.training_time = time() - start_time\n",
    "        self.total_environment_steps = self.training_step * self.config.replay_interval\n",
    "        self.save_checkpoint()\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "\n",
    "config_dict = {\n",
    "    \"conv_layers\": [\n",
    "        (32, 8, 4),\n",
    "        (64, 4, 2),\n",
    "        (64, 3, 1),\n",
    "    ],\n",
    "    \"dense_layers_widths\": [512],\n",
    "    \"value_hidden_layers_widths\": [],  #\n",
    "    \"advatage_hidden_layers_widths\": [],  #\n",
    "    \"adam_epsilon\": 1.5e-4,\n",
    "    \"learning_rate\": 0.00025 / 4,\n",
    "    \"training_steps\": 50000000,  # Agent saw 200,000,000 frames\n",
    "    \"per_epsilon\": 1e-6,  #\n",
    "    \"per_alpha\": 0.5,\n",
    "    \"per_beta\": 0.4,\n",
    "    \"minibatch_size\": 32,\n",
    "    \"replay_buffer_size\": 1000000,\n",
    "    \"min_replay_buffer_size\": 80000,  # 80000\n",
    "    \"transfer_interval\": 32000,\n",
    "    \"n_step\": 3,\n",
    "    \"kernel_initializer\": \"orthogonal\",  #\n",
    "    \"loss_function\": KLDivergenceLoss(),\n",
    "    \"clipnorm\": 0.0,  #\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"atom_size\": 51,\n",
    "    \"replay_interval\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "game_config = AtariConfig()\n",
    "config = RainbowConfig(config_dict, game_config)\n",
    "\n",
    "\n",
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\", max_episode_steps=108000\n",
    ")\n",
    "env = AtariPreprocessing(env, terminal_on_life_loss=True)\n",
    "env = FrameStack(env, 4, lz4_compress=True)\n",
    "agent = RainbowAgent(env, config, name=\"Rainbow_Atari_MsPacmanNoFrameskip-v4\")\n",
    "agent.checkpoint_interval = 1000\n",
    "agent.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
