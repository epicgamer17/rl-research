{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c86fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import open_spiel\n",
    "import numpy as np\n",
    "game = pyspiel.load_game(\"universal_poker\", {\"numPlayers\":2, \"numSuits\": 4, \"numRanks\":13, \"numHoleCards\": 2, \"numBoardCards\": \"0 3\", \"bettingAbstraction\": \"fcpa\", \"numRounds\":2})\n",
    "import copy\n",
    "class WrapperEnv:\n",
    "    def __init__(self,game):\n",
    "        self.game= game\n",
    "        self.state = game.new_initial_state()\n",
    "        self.agent_selection = str(self.state.current_player())\n",
    "        self.traverser = None\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.state = self.game.new_initial_state()\n",
    "        while self.state.is_chance_node():\n",
    "            self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "        self.agent_selection =  str(self.state.current_player())\n",
    "        return self.obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.state.is_chance_node():\n",
    "            while self.state.is_chance_node():\n",
    "                self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "\n",
    "        else:\n",
    "            self.state.apply_action(action)\n",
    "            if self.state.is_chance_node():\n",
    "                while self.state.is_chance_node():\n",
    "                    self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "        \n",
    "        if self.state.is_terminal():\n",
    "            return self.obs()\n",
    "        else:\n",
    "            # store = copy.deepcopy(self.state)\n",
    "            while self.state.is_chance_node():\n",
    "                self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "            # print(\"3\")\n",
    "            # print(self.state.is_terminal())\n",
    "            # if self.state.is_terminal():\n",
    "            #     print(\"store:\", store)\n",
    "            # print(self.state)\n",
    "            # print(self.state.legal_actions_mask(int(self.agent_selection)))\n",
    "            # print(\"3\")\n",
    "            self.agent_selection =  str(self.state.current_player())\n",
    "\n",
    "            return self.obs()\n",
    "    \n",
    "    def last(self):\n",
    "        return self.obs()\n",
    "    \n",
    "    def obs(self):\n",
    "        return {\"observation\":self.state.observation_tensor(int(self.agent_selection)), \"action_mask\":np.stack(self.state.legal_actions_mask(int(self.agent_selection)))}, self.state.player_reward(self.traverser) if self.traverser is not None else self.state.player_return(int(self.agent_selection)), self.state.is_terminal(), False, \"OPENSPIEL\"\n",
    "\n",
    "env = WrapperEnv(game)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9ed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ile-Maurice/Library/Python/3.10/lib/python/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRConfig\n",
      "Iteration 0 done\n",
      "Nodes touched 27592\n",
      "Checkpointing at 20.0% of training steps, i.e 0 iterations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sampling:\n\u001b[1;32m     37\u001b[0m     model \u001b[38;5;241m=\u001b[39m CFRAgent(env\u001b[38;5;241m=\u001b[39menv,config\u001b[38;5;241m=\u001b[39mconfig, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFR_FHP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m i, max_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000000\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/rl-research-main/rl-research/DeepCFR/cfr_agent.py:218\u001b[0m, in \u001b[0;36mCFRAgent.train\u001b[0;34m(self, sampling)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpointing at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_interval\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of training steps, i.e \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# CHECKPOINT EVERY 10% OF TRAINING STEPS\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     checkpoint_interval \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/rl-research-main/rl-research/DeepCFR/cfr_agent.py:265\u001b[0m, in \u001b[0;36mCFRAgent.save_checkpoint\u001b[0;34m(self, iteration)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_steps\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/policy/notlinear/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes_touched\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Desktop/rl-research-main/rl-research/DeepCFR/cfr_agent.py:155\u001b[0m, in \u001b[0;36mCFRAgent.policy_learn\u001b[0;34m(self, linear)\u001b[0m\n\u001b[1;32m    153\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_buffer\u001b[38;5;241m.\u001b[39msample(num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msteps_per_epoch)\n\u001b[0;32m--> 155\u001b[0m observations \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m target_policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    157\u001b[0m iteration \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfos\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import texas_holdem_v4\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "import torch\n",
    "from cfr_network import CFRNetwork\n",
    "\n",
    "hidden_dim = 128\n",
    "input_dim = 108\n",
    "output_dim = 4\n",
    "num_players = 2\n",
    "replay_buffer_size = 4000000\n",
    "minibatch_size = 10000\n",
    "steps_per_epoch = 3000\n",
    "traversals = 3000\n",
    "training_steps = 200\n",
    "lr = 0.0001\n",
    "optimizer = None\n",
    "p_v_networks = {'input_shape':input_dim, 'output_shape':output_dim, 'hidden_size':hidden_dim, 'learning_rate':lr, 'optimizer':optimizer}\n",
    "active_player_obj = ActivePlayer(num_players)\n",
    "config = CFRConfig(\n",
    "    config_dict={'network': {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players},\n",
    "                 'replay_buffer_size':replay_buffer_size,\n",
    "                 'minibatch_size':minibatch_size,\n",
    "                 'steps_per_epoch':steps_per_epoch,\n",
    "                 'traversals': traversals,\n",
    "                 'training_steps': training_steps,\n",
    "                 'active_player_obj': active_player_obj,\n",
    "                 },\n",
    "    game_config={'num_players':num_players,\n",
    "                 'observation_space':108,\n",
    "                 'action_space':4,},\n",
    ")\n",
    "sampling = [\"Full\"]\n",
    "for i in sampling:\n",
    "    model = CFRAgent(env=env,config=config, name=\"CFR_FHP\" + i, max_nodes=4000000)\n",
    "    model.train(sampling=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
