{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d31d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_configs import RainbowConfig\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import CategoricalCrossentropyLoss, KLDivergenceLoss\n",
    "from utils.utils import HuberLoss\n",
    "from cfr_utils import EvalWrapper, evaluatebots, WrapperEnv, load_agents, EmptyConf\n",
    "import pyspiel\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "from cfr_network import CFRNetwork\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from dqn.rainbow.rainbow_agent import RainbowAgent\n",
    "\n",
    "fhp = pyspiel.load_game(\n",
    "    \"universal_poker\",\n",
    "    {\n",
    "        \"numPlayers\": 2,\n",
    "        \"numSuits\": 4,\n",
    "        \"numRanks\": 13,\n",
    "        \"numHoleCards\": 2,\n",
    "        \"numBoardCards\": \"0 3\",\n",
    "        \"bettingAbstraction\": \"fcpa\",\n",
    "        \"numRounds\": 2,\n",
    "        \"blind\": \"50 100\",\n",
    "    },\n",
    ")\n",
    "leduc = pyspiel.load_game(\n",
    "    \"universal_poker\",\n",
    "    {\n",
    "        \"numPlayers\": 2,\n",
    "        \"numSuits\": 2,\n",
    "        \"numRanks\": 3,\n",
    "        \"numHoleCards\": 1,\n",
    "        \"numBoardCards\": \"0 1\",\n",
    "        \"bettingAbstraction\": \"fcpa\",\n",
    "        \"numRounds\": 2,\n",
    "        \"blind\": \"50 100\",\n",
    "    },\n",
    ")\n",
    "leducconfig = {\"state_representation_size\": 16}\n",
    "fhpconfig = {\"state_representation_size\": 108}\n",
    "leducgame = WrapperEnv(leduc)\n",
    "fhpgame = WrapperEnv(fhp)\n",
    "\n",
    "\n",
    "# path1 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/linear/1032012/CFR_0.001LR_TEST_Full_FHP_38.pt' # 10%\n",
    "# path2 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/linear/4000020/CFR_0.001LR_TEST_Full_FHP_144.pt' #40%\n",
    "# path3 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/linear/6015876/CFR_0.001LR_TEST_Full_FHP_219.pt' # 60%\n",
    "# path4 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/linear/8014283/CFR_0.001LR_TEST_Full_FHP_297.pt' #80%\n",
    "# path5 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/linear/10014831/CFR_0.001LR_TEST_Full_FHP_371.pt' #100%\n",
    "# path6 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/notlinear/1032012/CFR_0.001LR_TEST_Full_FHP_38.pt' # 10%\n",
    "# path7 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/notlinear/4000020/CFR_0.001LR_TEST_Full_FHP_144.pt' #40%\n",
    "# path8 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/notlinear/6015876/CFR_0.001LR_TEST_Full_FHP_219.pt' # 60%\n",
    "# path9 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/notlinear/8014283/CFR_0.001LR_TEST_Full_FHP_297.pt' #80%\n",
    "# path10 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/notlinear/10014831/CFR_0.001LR_TEST_Full_FHP_371.pt' #100%\n",
    "\n",
    "\n",
    "# path1 = 'checkpoints/CFR_0.001LR_TEST_MC_FHP/policy/linear/1005275/CFR_0.001LR_TEST_MC_FHP_56.pt'\n",
    "# path2 = 'checkpoints/CFR_0.001LR_TEST_MC_FHP/policy/linear/4014284/CFR_0.001LR_TEST_MC_FHP_224.pt'\n",
    "# path3 = 'checkpoints/CFR_0.001LR_TEST_MC_FHP/policy/linear/6001547/CFR_0.001LR_TEST_MC_FHP_332.pt'\n",
    "# path4 = 'checkpoints/CFR_0.001LR_TEST_MC_FHP/policy/linear/8016903/CFR_0.001LR_TEST_MC_FHP_439.pt'\n",
    "# path5 = 'checkpoints/CFR_0.001LR_TEST_MC_FHP/policy/linear/10015290/CFR_0.001LR_TEST_MC_FHP_546.pt'\n",
    "\n",
    "\n",
    "path6 = \"checkpoints_1/notlinear/1005275/CFR_0.001LR_TEST_MC_FHP_56.pt\"\n",
    "path7 = \"checkpoints_1/notlinear/4014284/CFR_0.001LR_TEST_MC_FHP_224.pt\"\n",
    "path8 = \"checkpoints_1/notlinear/6001547/CFR_0.001LR_TEST_MC_FHP_332.pt\"\n",
    "path9 = \"checkpoints_1/notlinear/8016903/CFR_0.001LR_TEST_MC_FHP_439.pt\"\n",
    "path10 = \"checkpoints_1/notlinear/10015290/CFR_0.001LR_TEST_MC_FHP_546.pt\"\n",
    "\n",
    "# path11 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path12 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path13 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path14 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path15 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path16 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path17 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path18 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path19 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "# path20 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt'\n",
    "\n",
    "# path11 = 'checkpoints/CFR_0.001LR_TEST_Full_FHP/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt' # 10%\n",
    "# path12 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt' # 40%\n",
    "# path13 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt' # 60%\n",
    "# path14 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt' # 80%\n",
    "# path15 = 'checkpoints/CFR_0.001LR_TEST_Full_Leduc/policy/linear/10011696/CFR_0.001LR_TEST_Full_Leduc_490.pt' # 100% 37.2\n",
    "\n",
    "# linear_paths = [path1, path2, path3, path4, path5]\n",
    "notlinear_paths = [path6, path7, path8, path9, path10]\n",
    "# note numbers are wrong 5 is first before\n",
    "games = [fhpgame]\n",
    "for game in games:\n",
    "    if game == leducgame:\n",
    "        chosen_game = \"leduc\"\n",
    "        game_string = \"Leduc\"\n",
    "        # path = path1\n",
    "    elif game == fhpgame:\n",
    "        chosen_game = \"fhp\"\n",
    "        game_string = \"FHP\"\n",
    "        # path = path2\n",
    "\n",
    "    hidden_dim = 128\n",
    "    input_dim = 16 if chosen_game == \"leduc\" else 108\n",
    "    output_dim = 4\n",
    "    num_players = 2\n",
    "    replay_buffer_size = 4000000\n",
    "    minibatch_size = 10000\n",
    "    steps_per_epoch = 3000\n",
    "    traversals = 3000\n",
    "    training_steps = 20000\n",
    "    lr = 0.0001\n",
    "    optimizer = None\n",
    "\n",
    "    p_v_networks = {\n",
    "        \"input_shape\": input_dim,\n",
    "        \"output_shape\": output_dim,\n",
    "        \"hidden_size\": hidden_dim,\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": optimizer,\n",
    "    }\n",
    "    active_player_obj = ActivePlayer(num_players)\n",
    "    config = CFRConfig(\n",
    "        config_dict={\n",
    "            \"network\": {\n",
    "                \"policy\": p_v_networks,\n",
    "                \"value\": p_v_networks,\n",
    "                \"num_players\": num_players,\n",
    "            },\n",
    "            \"replay_buffer_size\": replay_buffer_size,\n",
    "            \"minibatch_size\": minibatch_size,\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"traversals\": traversals,\n",
    "            \"training_steps\": training_steps,\n",
    "            \"active_player_obj\": active_player_obj,\n",
    "        },\n",
    "        game_config={\n",
    "            \"num_players\": num_players,\n",
    "            \"observation_space\": input_dim,\n",
    "            \"action_space\": 4,\n",
    "        },\n",
    "    )\n",
    "    # for i in range(len(linear_paths)):\n",
    "    #     path = linear_paths[i]\n",
    "    #     number = i\n",
    "    #     print(\"Linear Path: \", path)\n",
    "    #     print(\"Number: \", number)\n",
    "\n",
    "    #     agent1, agent2 = load_agents(path, path, p_v_networks, num_players)\n",
    "    #     modelselect = CFRAgent(env=game, config=config)\n",
    "\n",
    "    #     evaled_game = EvalWrapper(\n",
    "    #         game=game,\n",
    "    #         agent=agent1,\n",
    "    #         in_size=input_dim,\n",
    "    #         out_size=output_dim,\n",
    "    #         select_model=modelselect,\n",
    "    #     )\n",
    "\n",
    "    #     config_dict = {\n",
    "    #         \"dense_layer_widths\": [128, 256, 256, 128],\n",
    "    #         \"value_hidden_layer_widths\": [128, 128],\n",
    "    #         \"advantage_hidden_layer_widths\": [128, 128],\n",
    "    #         \"adam_epsilon\": 1e-8,\n",
    "    #         \"learning_rate\": 0.002,\n",
    "    #         \"training_steps\": 20000,\n",
    "    #         \"per_epsilon\": 0.001,\n",
    "    #         \"per_alpha\": 0,\n",
    "    #         \"per_beta\": 0,\n",
    "    #         \"per_beta_final\": 0.5,\n",
    "    #         \"minibatch_size\": 256,\n",
    "    #         \"replay_buffer_size\": 1000000,\n",
    "    #         \"min_replay_buffer_size\": 256,\n",
    "    #         \"transfer_interval\": 1024,\n",
    "    #         \"loss_function\": KLDivergenceLoss(),\n",
    "    #         \"clipnorm\": 0.0,\n",
    "    #         \"discount_factor\": 0.99,\n",
    "    #         \"replay_interval\": 64,\n",
    "    #         \"eg_epsilon\": 1,\n",
    "    #         \"eg_epsilon_final\": 0.0,\n",
    "    #         \"eg_epsilon_final_step\": 5000,\n",
    "    #         \"eg_epsilon_decay_type\": \"linear\",\n",
    "    #         \"num_minibatches\": 4,\n",
    "    #     }\n",
    "    #     gameconfig = EmptyConf()\n",
    "    #     rconfig = RainbowConfig(config_dict, gameconfig)\n",
    "    #     device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #     model_name = (\n",
    "    #         \"Rainbow_\" + game_string + \"_agent_\" + str(number) + \"_linear\" + \"_MC\"\n",
    "    #     )\n",
    "    #     print(\"Model Name: \", model_name)\n",
    "    #     agent = RainbowAgent(evaled_game, rconfig, name=model_name, device=device)\n",
    "    #     agent.checkpoint_interval = 500\n",
    "    #     agent.train()\n",
    "    #     agent.save_checkpoint()\n",
    "    #     print(\"Checkpoint saved for agent: \", model_name)\n",
    "\n",
    "    for i in range(len(notlinear_paths)):\n",
    "        path = notlinear_paths[i]\n",
    "        number = i\n",
    "        print(\"Not Linear Path: \", path)\n",
    "        print(\"Number: \", number)\n",
    "\n",
    "        agent1, agent2 = load_agents(path, path, p_v_networks, num_players)\n",
    "        modelselect = CFRAgent(env=game, config=config)\n",
    "\n",
    "        evaled_game = EvalWrapper(\n",
    "            game=game,\n",
    "            agent=agent1,\n",
    "            in_size=input_dim,\n",
    "            out_size=output_dim,\n",
    "            select_model=modelselect,\n",
    "        )\n",
    "\n",
    "        config_dict = {\n",
    "            \"dense_layer_widths\": [128, 256, 256, 128],\n",
    "            \"value_hidden_layer_widths\": [128, 128],\n",
    "            \"advantage_hidden_layer_widths\": [128, 128],\n",
    "            \"adam_epsilon\": 1e-8,\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"training_steps\": 20000,\n",
    "            \"per_epsilon\": 0.001,\n",
    "            \"per_alpha\": 0,\n",
    "            \"per_beta\": 0,\n",
    "            \"per_beta_final\": 0.5,\n",
    "            \"minibatch_size\": 256,\n",
    "            \"replay_buffer_size\": 1000000,\n",
    "            \"min_replay_buffer_size\": 256,\n",
    "            \"transfer_interval\": 1024,\n",
    "            \"loss_function\": KLDivergenceLoss(),\n",
    "            \"clipnorm\": 0.0,\n",
    "            \"discount_factor\": 0.99,\n",
    "            \"replay_interval\": 64,\n",
    "            \"eg_epsilon\": 1,\n",
    "            \"eg_epsilon_final\": 0.0,\n",
    "            \"eg_epsilon_final_step\": 5000,\n",
    "            \"eg_epsilon_decay_type\": \"linear\",\n",
    "            \"num_minibatches\": 4,\n",
    "        }\n",
    "        gameconfig = EmptyConf()\n",
    "        rconfig = RainbowConfig(config_dict, gameconfig)\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_name = (\n",
    "            \"Rainbow_\" + game_string + \"_agent_\" + str(number) + \"_notlinear\" + \"_MC\"\n",
    "        )\n",
    "        print(\"Model Name: \", model_name)\n",
    "        agent = RainbowAgent(evaled_game, rconfig, name=model_name, device=device)\n",
    "        agent.checkpoint_interval = 500\n",
    "        agent.train()\n",
    "        agent.save_checkpoint()\n",
    "        print(\"Checkpoint saved for agent: \", model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
