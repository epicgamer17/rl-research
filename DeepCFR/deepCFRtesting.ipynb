{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import texas_holdem_v4\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "import torch\n",
    "from cfr_network import CFRNetwork\n",
    "game = texas_holdem_v4.env(num_players=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 72\n",
    "output_dim = 4\n",
    "num_players = 2\n",
    "replay_buffer_size = 4000000\n",
    "minibatch_size = 5000\n",
    "steps_per_epoch = 2000\n",
    "traversals = 1500\n",
    "training_steps = 200\n",
    "lr = 0.001\n",
    "optimizer = None\n",
    "p_v_networks = {'input_shape':input_dim, 'output_shape':output_dim, 'hidden_size':hidden_dim, 'learning_rate':lr, 'optimizer':optimizer}\n",
    "active_player_obj = ActivePlayer(num_players)\n",
    "config = CFRConfig(\n",
    "    config_dict={'network': {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players},\n",
    "                 'replay_buffer_size':replay_buffer_size,\n",
    "                 'minibatch_size':minibatch_size,\n",
    "                 'steps_per_epoch':steps_per_epoch,\n",
    "                 'traversals': traversals,\n",
    "                 'training_steps': training_steps,\n",
    "                 'active_player_obj': active_player_obj,\n",
    "                 },\n",
    "    game_config={'num_players':num_players,\n",
    "                 'observation_space':72,\n",
    "                 'action_space':4,},\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelselect = CFRAgent(\n",
    "    env=game,\n",
    "    config=config,\n",
    ")\n",
    "# modelselect.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c2844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 5.649683475494385\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1849\n",
      "LOSS 5.6683783531188965\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1850\n",
      "LOSS 5.506097793579102\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1851\n",
      "LOSS 5.875100612640381\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1852\n",
      "LOSS 5.72903299331665\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1853\n",
      "LOSS 5.659924030303955\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1854\n",
      "LOSS 5.870151519775391\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1855\n",
      "LOSS 5.8641743659973145\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1856\n",
      "LOSS 5.799927711486816\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1857\n",
      "LOSS 5.886231422424316\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1858\n",
      "LOSS 5.7255377769470215\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1859\n",
      "LOSS 5.787532329559326\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1860\n",
      "LOSS 5.663789749145508\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1861\n",
      "LOSS 5.957200050354004\n",
      "PLAYER ID 0\n",
      "LEARNING ITERATION 1862\n"
     ]
    }
   ],
   "source": [
    "modelselect.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/1744571971.415963.pt')\n",
    "agent2 = torch.load('checkpoints/1744755949.354019.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b945aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "model2 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "model.load_state_dict(agent)\n",
    "model2.load_state_dict(agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23415bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games = 100000\n",
    "import numpy as np\n",
    "rewards_player_1 = []\n",
    "rewards_player_2  = []\n",
    "for i in range(eval_games):\n",
    "    # FOR EACH EVAL GAME, RESET ENVIRONEMENT (DEBATABLE STEP) BUT RESET WITH SET SEED FOR RECREATION\n",
    "    random_seed = np.random.randint(0, 2**32 - 1)\n",
    "    observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "\n",
    "    modelselect.env.reset(seed=random_seed)\n",
    "    active_player =  modelselect.env.agent_selection[-1]\n",
    "    modelselect.active_player_obj.set_active_player(int(active_player))\n",
    "    while not termination and not truncation:\n",
    "        # GET CURRENT STATE\n",
    "        observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "        if termination or truncation:\n",
    "            break\n",
    "        active_player =  modelselect.active_player_obj.get_active_player()\n",
    "        if active_player == 0:\n",
    "            predictions = model.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,72)).detach().numpy()[0]\n",
    "\n",
    "            sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "        else:\n",
    "            # predictions = np.ones(4) / 4\n",
    "            # sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            predictions = model2.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,72)).detach().numpy()[0]\n",
    "            sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "        # if active player, branch off and traverse\n",
    "        modelselect.env.step(sample)\n",
    "        modelselect.active_player_obj.next()\n",
    "    final_rewards_p_1 = modelselect.env.rewards[\"player_1\"]  # dict of {agent_0: r0, agent_1: r1}\n",
    "    final_rewards_p_2 = modelselect.env.rewards[\"player_0\"]\n",
    "    rewards_player_1.append(final_rewards_p_1)\n",
    "    rewards_player_2.append(final_rewards_p_2)\n",
    "    modelselect.env.close()\n",
    "\n",
    "print(\"PLAYER 1 REW MEAN: \", np.mean(rewards_player_1))\n",
    "print(\"PLAYER 1 REW STD: \", np.std(rewards_player_1))\n",
    "print(\"PLAYER 2 REW MEAN: \", np.mean(rewards_player_2))\n",
    "print(\"PLAYER 2 REW STD: \", np.std(rewards_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4780ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelselect.env.last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelselect.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
