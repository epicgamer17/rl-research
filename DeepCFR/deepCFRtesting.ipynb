{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "import torch\n",
    "from cfr_network import CFRNetwork\n",
    "game = leduc_holdem_v4.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 72\n",
    "input_dim = 36\n",
    "output_dim = 4\n",
    "num_players = 2\n",
    "replay_buffer_size = 4000000\n",
    "minibatch_size = 512\n",
    "steps_per_epoch = 200\n",
    "traversals = 200\n",
    "training_steps = 100\n",
    "lr = 0.001\n",
    "optimizer = None\n",
    "p_v_networks = {'input_shape':input_dim, 'output_shape':output_dim, 'hidden_size':hidden_dim, 'learning_rate':lr, 'optimizer':optimizer}\n",
    "active_player_obj = ActivePlayer(num_players)\n",
    "config = CFRConfig(\n",
    "    config_dict={'network': {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players},\n",
    "                 'replay_buffer_size':replay_buffer_size,\n",
    "                 'minibatch_size':minibatch_size,\n",
    "                 'steps_per_epoch':steps_per_epoch,\n",
    "                 'traversals': traversals,\n",
    "                 'training_steps': training_steps,\n",
    "                 'active_player_obj': active_player_obj,\n",
    "                 },\n",
    "    game_config={'num_players':num_players,\n",
    "                 'observation_space':72,\n",
    "                 'action_space':4,},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = [\"MC\", \"Full\"]\n",
    "for sampling_method in sampling:\n",
    "    game=leduc_holdem_v4.env()\n",
    "    model = CFRAgent(env=game,config=config)\n",
    "    model.train(sampling=sampling_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1_state = torch.load('checkpoints/policy/linear/38063/1745086660.608601.pt')\n",
    "agent2_state = torch.load('checkpoints/policy/linear/233694/1745086660.608601.pt')\n",
    "agent3_state = torch.load('checkpoints/policy/notlinear/38063/1745086660.608601.pt')\n",
    "agent4_state = torch.load('checkpoints/policy/notlinear/233694/1745086660.608601.pt')\n",
    "\n",
    "agent1 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent1.policy.load_state_dict(agent1_state)\n",
    "agent2 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent2.policy.load_state_dict(agent2_state)\n",
    "agent3 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent3.policy.load_state_dict(agent3_state)\n",
    "agent4 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent4.policy.load_state_dict(agent4_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23415bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.policy.eval()\n",
    "agent2.policy.eval()\n",
    "agent3.policy.eval()\n",
    "agent4.policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent1, agent2, agent3, agent4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatebots(agent1, agent2, num_of_eval_games):\n",
    "    modelselect = CFRAgent(env=game, config=config)\n",
    "    eval_games = num_of_eval_games\n",
    "    import numpy as np\n",
    "    rewards_player_1 = []\n",
    "    rewards_player_2  = []\n",
    "    for i in range(eval_games):\n",
    "        # FOR EACH EVAL GAME, RESET ENVIRONEMENT (DEBATABLE STEP) BUT RESET WITH SET SEED FOR RECREATION\n",
    "        random_seed = np.random.randint(0, 2**32 - 1)\n",
    "        observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "\n",
    "        modelselect.env.reset(seed=random_seed)\n",
    "        active_player =  modelselect.env.agent_selection[-1]\n",
    "        modelselect.active_player_obj.set_active_player(int(active_player))\n",
    "        while not termination and not truncation:\n",
    "            # GET CURRENT STATE\n",
    "            observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            active_player =  modelselect.active_player_obj.get_active_player()\n",
    "            if active_player == 0:\n",
    "                predictions = agent1.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,36)).detach().numpy()[0]\n",
    "\n",
    "                sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            else:\n",
    "                # predictions = np.ones(4) / 4\n",
    "                # sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "                predictions = agent2.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,36)).detach().numpy()[0]\n",
    "                sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            # if active player, branch off and traverse\n",
    "            modelselect.env.step(sample)\n",
    "            modelselect.active_player_obj.next()\n",
    "        final_rewards_p_1 = modelselect.env.rewards[\"player_0\"]  # dict of {agent_0: r0, agent_1: r1}\n",
    "        final_rewards_p_2 = modelselect.env.rewards[\"player_1\"]\n",
    "        rewards_player_1.append(final_rewards_p_1)\n",
    "        rewards_player_2.append(final_rewards_p_2)\n",
    "        modelselect.env.close()\n",
    "    return rewards_player_1, rewards_player_2\n",
    "    print(\"PLAYER 1 REW MEAN: \", np.mean(rewards_player_1))\n",
    "    print(\"PLAYER 1 REW STD: \", np.std(rewards_player_1))\n",
    "    print(\"PLAYER 2 REW MEAN: \", np.mean(rewards_player_2))\n",
    "    print(\"PLAYER 2 REW STD: \", np.std(rewards_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4780ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = np.zeros((4,4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        rewards_player_1, rewards_player_2 = evaluatebots(agents[i], agents[j], 100000)\n",
    "        results[i][j] = np.mean(rewards_player_1)\n",
    "\n",
    "print(\"RESULTS: \")\n",
    "# print results as matplotlib matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(results, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], yticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "plt.title(\"Results\")\n",
    "plt.xlabel(\"Agent 2\")\n",
    "plt.ylabel(\"Agent 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelselect.env.close()\n",
    "modelselect.env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
