{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import texas_holdem_v4\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "import torch\n",
    "from cfr_network import CFRNetwork\n",
    "game = texas_holdem_v4.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "input_dim = 72\n",
    "output_dim = 4\n",
    "num_players = 2\n",
    "replay_buffer_size = 4000000\n",
    "minibatch_size = 10000\n",
    "steps_per_epoch = 3000\n",
    "traversals = 3000\n",
    "training_steps = 200\n",
    "lr = 0.001\n",
    "optimizer = None\n",
    "p_v_networks = {'input_shape':input_dim, 'output_shape':output_dim, 'hidden_size':hidden_dim, 'learning_rate':lr, 'optimizer':optimizer}\n",
    "active_player_obj = ActivePlayer(num_players)\n",
    "config = CFRConfig(\n",
    "    config_dict={'network': {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players},\n",
    "                 'replay_buffer_size':replay_buffer_size,\n",
    "                 'minibatch_size':minibatch_size,\n",
    "                 'steps_per_epoch':steps_per_epoch,\n",
    "                 'traversals': traversals,\n",
    "                 'training_steps': training_steps,\n",
    "                 'active_player_obj': active_player_obj,\n",
    "                 },\n",
    "    game_config={'num_players':num_players,\n",
    "                 'observation_space':72,\n",
    "                 'action_space':4,},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = [\"MC\", \"Full\"]\n",
    "for sampling_method in sampling:\n",
    "    game=texas_holdem_v4.env()\n",
    "    model = CFRAgent(env=game,config=config, name=\"CFR_TXAS_HOLD_EM\")\n",
    "    model.train(sampling=sampling_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1_state = torch.load('checkpoints/policy/linear/38063/1745086660.608601.pt')\n",
    "agent2_state = torch.load('checkpoints/policy/linear/233694/1745086660.608601.pt')\n",
    "agent3_state = torch.load('checkpoints/policy/notlinear/38063/1745086660.608601.pt')\n",
    "agent4_state = torch.load('checkpoints/policy/notlinear/233694/1745086660.608601.pt')\n",
    "\n",
    "agent1 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent1.policy.load_state_dict(agent1_state)\n",
    "agent2 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent2.policy.load_state_dict(agent2_state)\n",
    "agent3 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent3.policy.load_state_dict(agent3_state)\n",
    "agent4 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent4.policy.load_state_dict(agent4_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23415bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.policy.eval()\n",
    "agent2.policy.eval()\n",
    "agent3.policy.eval()\n",
    "agent4.policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent1, agent2, agent3, agent4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatebots(agent1, agent2, num_of_eval_games):\n",
    "    modelselect = CFRAgent(env=game, config=config)\n",
    "    eval_games = num_of_eval_games\n",
    "    import numpy as np\n",
    "    rewards_player_1 = []\n",
    "    rewards_player_2  = []\n",
    "    for i in range(eval_games):\n",
    "        # FOR EACH EVAL GAME, RESET ENVIRONEMENT (DEBATABLE STEP) BUT RESET WITH SET SEED FOR RECREATION\n",
    "        random_seed = np.random.randint(0, 2**32 - 1)\n",
    "        observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "\n",
    "        modelselect.env.reset(seed=random_seed)\n",
    "        active_player =  modelselect.env.agent_selection[-1]\n",
    "        modelselect.active_player_obj.set_active_player(int(active_player))\n",
    "        while not termination and not truncation:\n",
    "            # GET CURRENT STATE\n",
    "            observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            active_player =  modelselect.active_player_obj.get_active_player()\n",
    "            if active_player == 0:\n",
    "                predictions = agent1.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,36)).detach().numpy()[0]\n",
    "\n",
    "                sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            else:\n",
    "                # predictions = np.ones(4) / 4\n",
    "                # sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "                predictions = agent2.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,36)).detach().numpy()[0]\n",
    "                sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            # if active player, branch off and traverse\n",
    "            modelselect.env.step(sample)\n",
    "            modelselect.active_player_obj.next()\n",
    "        final_rewards_p_1 = modelselect.env.rewards[\"player_0\"]  # dict of {agent_0: r0, agent_1: r1}\n",
    "        final_rewards_p_2 = modelselect.env.rewards[\"player_1\"]\n",
    "        rewards_player_1.append(final_rewards_p_1)\n",
    "        rewards_player_2.append(final_rewards_p_2)\n",
    "        modelselect.env.close()\n",
    "    return rewards_player_1, rewards_player_2\n",
    "    print(\"PLAYER 1 REW MEAN: \", np.mean(rewards_player_1))\n",
    "    print(\"PLAYER 1 REW STD: \", np.std(rewards_player_1))\n",
    "    print(\"PLAYER 2 REW MEAN: \", np.mean(rewards_player_2))\n",
    "    print(\"PLAYER 2 REW STD: \", np.std(rewards_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4780ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = np.zeros((4,4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        rewards_player_1, rewards_player_2 = evaluatebots(agents[i], agents[j], 100000)\n",
    "        results[i][j] = np.mean(rewards_player_1)\n",
    "\n",
    "print(\"RESULTS: \")\n",
    "# print results as matplotlib matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(results, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], yticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "plt.title(\"Results\")\n",
    "plt.xlabel(\"Agent 2\")\n",
    "plt.ylabel(\"Agent 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results, columns=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], index=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "df.to_csv(\"results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c05412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"results.csv\", index_col=0)\n",
    "print(df)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], yticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "plt.title(\"Results\")\n",
    "plt.xlabel(\"Agent 2\")\n",
    "plt.ylabel(\"Agent 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aacd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import open_spiel\n",
    "import numpy as np\n",
    "\n",
    "import open_spiel.python\n",
    "import open_spiel.python.algorithms\n",
    "import open_spiel.python.algorithms.nfsp\n",
    "import tensorflow as tf\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    game = pyspiel.load_game(\"universal_poker\", {\"numPlayers\":2, \"numSuits\": 4, \"numRanks\":13, \"numHoleCards\": 2, \"numBoardCards\": \"0 3\", \"bettingAbstraction\": \"fcpa\", \"numRounds\":2})\n",
    "\n",
    "    agent = open_spiel.python.algorithms.nfsp.NFSP(\n",
    "    session=sess,\n",
    "    player_id=0,\n",
    "    state_representation_size=108,\n",
    "    num_actions=4,\n",
    "    hidden_layers_sizes=[128,128,128],\n",
    "    reservoir_buffer_capacity=4000000,\n",
    "    anticipatory_param=0.2,\n",
    "    batch_size=10000, rl_learning_rate=0.001, sl_learning_rate=0.001, min_buffer_size_to_learn=1000, learn_every=64, optimizer_str=\"sgd\"\n",
    "    )\n",
    "    total_parameters = 0\n",
    "    for variable in tf.compat.v1.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print(\"Total parameters:\", total_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import open_spiel\n",
    "import numpy as np\n",
    "fhp = pyspiel.load_game(\"universal_poker\", {\"numPlayers\":2, \"numSuits\": 4, \"numRanks\":13, \"numHoleCards\": 2, \"numBoardCards\": \"0 3\", \"bettingAbstraction\": \"fcpa\", \"numRounds\":2, \"blind\": \"50 100\"})\n",
    "\n",
    "leduc = pyspiel.load_game(\"universal_poker\", {\"numPlayers\":2, \"numSuits\": 2, \"numRanks\":3, \"numHoleCards\": 1, \"numBoardCards\": \"0 1\", \"bettingAbstraction\": \"fcpa\", \"numRounds\":2, \"blind\": \"50 100\"})\n",
    "import copy\n",
    "class WrapperEnv:\n",
    "    def __init__(self,game):\n",
    "        self.game= game\n",
    "        self.state = game.new_initial_state()\n",
    "        self.agent_selection = str(self.state.current_player())\n",
    "        self.traverser = None\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.state = self.game.new_initial_state()\n",
    "        while self.state.is_chance_node():\n",
    "            self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "        self.agent_selection =  str(self.state.current_player())\n",
    "        return self.obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.state.is_chance_node():\n",
    "            while self.state.is_chance_node():\n",
    "                self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "\n",
    "        else:\n",
    "            self.state.apply_action(action)\n",
    "            if self.state.is_chance_node():\n",
    "                while self.state.is_chance_node():\n",
    "                    self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "        \n",
    "        if self.state.is_terminal():\n",
    "            return self.obs()\n",
    "        else:\n",
    "            # store = copy.deepcopy(self.state)\n",
    "            while self.state.is_chance_node():\n",
    "                self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "            # print(\"3\")\n",
    "            # print(self.state.is_terminal())\n",
    "            # if self.state.is_terminal():\n",
    "            #     print(\"store:\", store)\n",
    "            # print(self.state)\n",
    "            # print(self.state.legal_actions_mask(int(self.agent_selection)))\n",
    "            # print(\"3\")\n",
    "            self.agent_selection =  str(self.state.current_player())\n",
    "\n",
    "            return self.obs()\n",
    "    \n",
    "    def last(self):\n",
    "        return self.obs()\n",
    "    \n",
    "    def obs(self):\n",
    "        return {\"observation\":self.state.observation_tensor(int(self.agent_selection)), \"action_mask\":np.stack(self.state.legal_actions_mask(int(self.agent_selection)))}, self.state.player_reward(self.traverser) if self.traverser is not None else self.state.player_return(int(self.agent_selection)), self.state.is_terminal(), False, \"OPENSPIEL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ebe9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFSPWrapper:\n",
    "    def __init__(self,env):\n",
    "        self.game = env\n",
    "        self.state = self.game.new_initial_state()\n",
    "        self.observations = {\"info_state\":[0 for _ in range(self.state.num_players())], \"legal_actions\":[0 for _ in range(self.state.num_players())]}\n",
    "        self.agent_selection = str(self.state.current_player())\n",
    "        self.traverser = None\n",
    "        self.rewards = [0 for _ in range(self.state.num_players())]\n",
    "    \n",
    "    def is_simultaneous_move(self):\n",
    "        return self.state.is_simultaneous_node()\n",
    "\n",
    "\n",
    "    def last(self):\n",
    "        return self.state.is_terminal()\n",
    "\n",
    "    def current_player(self):\n",
    "        return self.state.current_player()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.state.is_chance_node():\n",
    "            while self.state.is_chance_node():\n",
    "                self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "\n",
    "        else:\n",
    "            self.state.apply_action(action)\n",
    "            if self.state.is_chance_node():\n",
    "                while self.state.is_chance_node():\n",
    "                    self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "        \n",
    "        if not self.state.is_terminal():\n",
    "            while self.state.is_chance_node():\n",
    "                self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "            self.agent_selection =  str(self.state.current_player())\n",
    "\n",
    "            return self.obs()\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.state = self.game.new_initial_state()\n",
    "        while self.state.is_chance_node():\n",
    "            self.state.apply_action(np.random.choice(self.state.legal_actions()))\n",
    "        self.agent_selection =  str(self.state.current_player())\n",
    "        return self.obs()\n",
    "    \n",
    "    \n",
    "    def obs(self):\n",
    "        if not self.state.is_terminal():\n",
    "            self.observations[\"info_state\"][self.state.current_player()] = self.state.observation_tensor(self.state.current_player())\n",
    "            self.observations[\"legal_actions\"][self.state.current_player()] = np.stack(self.state.legal_actions(self.state.current_player()))\n",
    "        if not self.state.is_chance_node():\n",
    "            self.rewards = self.state.rewards()\n",
    "        else:\n",
    "            self.rewards = [0 for _ in range(self.state.num_players())]\n",
    "        return {\"observation\":self.state.observation_tensor(int(self.agent_selection)), \"action_mask\":np.stack(self.state.legal_actions(int(self.agent_selection)))}, self.state.player_reward(self.traverser) if self.traverser is not None else self.state.player_return(int(self.agent_selection)), self.state.is_terminal(), False, \"OPENSPIEL\"\n",
    "\n",
    "leducgame = NFSPWrapper(leduc)\n",
    "fhpgame = NFSPWrapper(fhp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "def train(agents, env, max_nodes, game_string):\n",
    "    nodes = 0\n",
    "    checkpoint = 0.1\n",
    "    while nodes<=max_nodes:\n",
    "        env.reset()\n",
    "        while not env.last():\n",
    "            currrent_player = env.current_player()\n",
    "            action, probs = agents[currrent_player].step(copy.deepcopy(env))\n",
    "            env.step(action)\n",
    "            nodes += 1\n",
    "        print(\"Nodes:\", nodes)\n",
    "        if nodes >= checkpoint * max_nodes:\n",
    "            for i in range(len(agents)):\n",
    "                print(\"Checkpoint reached: \", checkpoint)\n",
    "                if not os.path.exists(\"checkpoints/\" + game_string + \"/\"):\n",
    "                    os.makedirs(\"checkpoints/\" + game_string + \"/\")\n",
    "                if not os.path.exists(\"checkpoints/\"+ game_string + \"/nfsp/\"):\n",
    "                    os.makedirs(\"checkpoints/\"+ game_string + \"/nfsp/\")\n",
    "                if not os.path.exists(\"checkpoints/\"+ game_string + \"/nfsp/\" + str(i)):\n",
    "                    os.makedirs(\"checkpoints/\"+ game_string + \"/nfsp/\" + str(i))\n",
    "                if not os.path.exists(\"checkpoints/\"+ game_string + \"/nfsp/\"+str(i)+ \"/\" + str(nodes)):\n",
    "                    os.makedirs(\"checkpoints/\"+ game_string + \"/nfsp/\"+str(i)+ \"/\" + str(nodes))\n",
    "                agents[i].save(\"checkpoints/\"+ game_string + \"/nfsp/\"+str(i)+ \"/\" + str(nodes))\n",
    "            checkpoint += 0.1\n",
    "    for i in range(len(agents)):\n",
    "        print(\"Checkpoint reached: \", checkpoint)\n",
    "        if not os.path.exists(\"checkpoints/\" + game_string + \"/\"):\n",
    "            os.makedirs(\"checkpoints/\" + game_string + \"/\")\n",
    "        if not os.path.exists(\"checkpoints/\"+ game_string + \"/nfsp/\"):\n",
    "            os.makedirs(\"checkpoints/\"+ game_string + \"/nfsp/\")\n",
    "        if not os.path.exists(\"checkpoints/\"+ game_string + \"/nfsp/\" + str(i)):\n",
    "            os.makedirs(\"checkpoints/\"+ game_string + \"/nfsp/\" + str(i))\n",
    "        if not os.path.exists(\"checkpoints/\"+ game_string + \"/nfsp/\"+str(i)+ \"/\" + str(nodes)):\n",
    "            os.makedirs(\"checkpoints/\"+ game_string + \"/nfsp/\"+str(i)+ \"/\" + str(nodes))\n",
    "        agents[i].save(\"checkpoints/\"+ game_string + \"/nfsp/\"+str(i)+ \"/\" + str(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 16:51:24.909188: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_16/bias_4/Assign' id:3955 op device:{requested: '', assigned: ''} def:{{{node mlp_16/bias_4/Assign}} = Assign[T=DT_FLOAT, _class=[\"loc:@mlp_16/bias_4\"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_16/bias_4, mlp_16/zeros_4)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 16:51:26.432410: W tensorflow/c/c_api.cc:305] Operation '{name:'mlp_22/bias_4/Assign' id:5474 op device:{requested: '', assigned: ''} def:{{{node mlp_22/bias_4/Assign}} = Assign[T=DT_FLOAT, _class=[\"loc:@mlp_22/bias_4\"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_22/bias_4, mlp_22/zeros_4)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "# import tensorflow.python.compiler.mlcompute as mlcompute\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# mlcompute.set_mlc_device(device_name='gpu')\n",
    "# print(\"is_apple_mlc_enabled %s\" % mlcompute.is_apple_mlc_enabled())\n",
    "# print(\"is_tf_compiled_with_apple_mlc %s\" % mlcompute.is_tf_compiled_with_apple_mlc())\n",
    "# print(f\"eagerly? {tf.executing_eagerly()}\")\n",
    "# print(tf.config.list_logical_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import pyspiel\n",
    "import open_spiel\n",
    "import numpy as np\n",
    "leducconfig = {\"state_representation_size\": 16}\n",
    "fhpconfig = {\"state_representation_size\": 108}\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "import open_spiel.python\n",
    "import open_spiel.python.algorithms\n",
    "import open_spiel.python.algorithms.nfsp\n",
    "num_players = 2\n",
    "max_nodes = 10000000\n",
    "nodes = 0\n",
    "games = [leducgame, fhpgame]\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    for i in games:\n",
    "        i.reset()\n",
    "        if i == leducgame:\n",
    "            game_string = \"leduc\"\n",
    "        else:\n",
    "            game_string = \"fhp\"\n",
    "        agents = [open_spiel.python.algorithms.nfsp.NFSP(\n",
    "        session=sess,\n",
    "        player_id=_,\n",
    "        state_representation_size=leducconfig[\"state_representation_size\"] if i == leducgame else fhpconfig[\"state_representation_size\"],\n",
    "        num_actions=4,\n",
    "        hidden_layers_sizes=[1024,512,1024,512],\n",
    "        reservoir_buffer_capacity=30000000,\n",
    "        anticipatory_param=0.1,\n",
    "        batch_size=256, rl_learning_rate=0.1, sl_learning_rate=0.01, min_buffer_size_to_learn=1000, learn_every=256, optimizer_str=\"sgd\", replay_buffer_capacity= 600000, epsilon_start=0.08, epsilon_end=0,\n",
    "        ) for _ in range(num_players)]\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        train(agents, i,max_nodes, game_string)\n",
    "        print(\"done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
