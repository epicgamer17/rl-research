{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import texas_holdem_v4\n",
    "import copy\n",
    "from agent_configs.cfr_config import CFRConfig\n",
    "from active_player import ActivePlayer\n",
    "from cfr_agent import CFRAgent\n",
    "import torch\n",
    "from cfr_network import CFRNetwork\n",
    "game = texas_holdem_v4.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "input_dim = 72\n",
    "output_dim = 4\n",
    "num_players = 2\n",
    "replay_buffer_size = 4000000\n",
    "minibatch_size = 10000\n",
    "steps_per_epoch = 3000\n",
    "traversals = 3000\n",
    "training_steps = 200\n",
    "lr = 0.001\n",
    "optimizer = None\n",
    "p_v_networks = {'input_shape':input_dim, 'output_shape':output_dim, 'hidden_size':hidden_dim, 'learning_rate':lr, 'optimizer':optimizer}\n",
    "active_player_obj = ActivePlayer(num_players)\n",
    "config = CFRConfig(\n",
    "    config_dict={'network': {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players},\n",
    "                 'replay_buffer_size':replay_buffer_size,\n",
    "                 'minibatch_size':minibatch_size,\n",
    "                 'steps_per_epoch':steps_per_epoch,\n",
    "                 'traversals': traversals,\n",
    "                 'training_steps': training_steps,\n",
    "                 'active_player_obj': active_player_obj,\n",
    "                 },\n",
    "    game_config={'num_players':num_players,\n",
    "                 'observation_space':72,\n",
    "                 'action_space':4,},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d452a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 123 done\n",
      "Iteration 124 done\n",
      "Iteration 125 done\n",
      "Iteration 126 done\n",
      "Iteration 127 done\n",
      "Iteration 128 done\n",
      "Iteration 129 done\n",
      "Iteration 130 done\n",
      "Iteration 131 done\n",
      "Iteration 132 done\n",
      "Iteration 133 done\n",
      "Iteration 134 done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m game\u001b[38;5;241m=\u001b[39mtexas_holdem_v4\u001b[38;5;241m.\u001b[39menv()\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CFRAgent(env\u001b[38;5;241m=\u001b[39mgame,config\u001b[38;5;241m=\u001b[39mconfig, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFR_TXAS_HOLD_EM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/rl-research-main/rl-research/DeepCFR/cfr_agent.py:167\u001b[0m, in \u001b[0;36mCFRAgent.train\u001b[0;34m(self, sampling)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverse(history\u001b[38;5;241m=\u001b[39mtraverse_history, iteration_T\u001b[38;5;241m=\u001b[39mi, seed\u001b[38;5;241m=\u001b[39mrandom_seed, game\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, active_player\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_player_obj\u001b[38;5;241m.\u001b[39mget_active_player(), traverser\u001b[38;5;241m=\u001b[39mp, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# NOW LEARN FROM THE PLAYER'S VALUE BUFFER\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_steps \u001b[38;5;241m*\u001b[39m checkpoint_interval:\n",
      "File \u001b[0;32m~/Desktop/rl-research-main/rl-research/DeepCFR/cfr_agent.py:125\u001b[0m, in \u001b[0;36mCFRAgent.learn\u001b[0;34m(self, player_id)\u001b[0m\n\u001b[1;32m    123\u001b[0m observations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]))\n\u001b[1;32m    124\u001b[0m target_policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]))\n\u001b[0;32m--> 125\u001b[0m iteration \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfos\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)])\n\u001b[1;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mvalues[player_id]\u001b[38;5;241m.\u001b[39mlearn(batch\u001b[38;5;241m=\u001b[39m[iteration, observations, target_policy])\n",
      "File \u001b[0;32m~/Desktop/rl-research-main/rl-research/DeepCFR/cfr_agent.py:125\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m observations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]))\n\u001b[1;32m    124\u001b[0m target_policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]))\n\u001b[0;32m--> 125\u001b[0m iteration \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfos\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)])\n\u001b[1;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mvalues[player_id]\u001b[38;5;241m.\u001b[39mlearn(batch\u001b[38;5;241m=\u001b[39m[iteration, observations, target_policy])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling = [\"MC\", \"Full\"]\n",
    "for sampling_method in sampling:\n",
    "    game=texas_holdem_v4.env()\n",
    "    model = CFRAgent(env=game,config=config, name=\"CFR_TXAS_HOLD_EM\")\n",
    "    model.train(sampling=sampling_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1_state = torch.load('checkpoints/policy/linear/38063/1745086660.608601.pt')\n",
    "agent2_state = torch.load('checkpoints/policy/linear/233694/1745086660.608601.pt')\n",
    "agent3_state = torch.load('checkpoints/policy/notlinear/38063/1745086660.608601.pt')\n",
    "agent4_state = torch.load('checkpoints/policy/notlinear/233694/1745086660.608601.pt')\n",
    "\n",
    "agent1 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent1.policy.load_state_dict(agent1_state)\n",
    "agent2 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent2.policy.load_state_dict(agent2_state)\n",
    "agent3 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent3.policy.load_state_dict(agent3_state)\n",
    "agent4 = CFRNetwork(\n",
    "     config = {'policy': p_v_networks, 'value': p_v_networks, 'num_players':num_players}\n",
    ")\n",
    "agent4.policy.load_state_dict(agent4_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23415bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.policy.eval()\n",
    "agent2.policy.eval()\n",
    "agent3.policy.eval()\n",
    "agent4.policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent1, agent2, agent3, agent4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatebots(agent1, agent2, num_of_eval_games):\n",
    "    modelselect = CFRAgent(env=game, config=config)\n",
    "    eval_games = num_of_eval_games\n",
    "    import numpy as np\n",
    "    rewards_player_1 = []\n",
    "    rewards_player_2  = []\n",
    "    for i in range(eval_games):\n",
    "        # FOR EACH EVAL GAME, RESET ENVIRONEMENT (DEBATABLE STEP) BUT RESET WITH SET SEED FOR RECREATION\n",
    "        random_seed = np.random.randint(0, 2**32 - 1)\n",
    "        observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "\n",
    "        modelselect.env.reset(seed=random_seed)\n",
    "        active_player =  modelselect.env.agent_selection[-1]\n",
    "        modelselect.active_player_obj.set_active_player(int(active_player))\n",
    "        while not termination and not truncation:\n",
    "            # GET CURRENT STATE\n",
    "            observation, reward, termination, truncation, infos =  modelselect.env.last()\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            active_player =  modelselect.active_player_obj.get_active_player()\n",
    "            if active_player == 0:\n",
    "                predictions = agent1.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,36)).detach().numpy()[0]\n",
    "\n",
    "                sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            else:\n",
    "                # predictions = np.ones(4) / 4\n",
    "                # sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "                predictions = agent2.policy(torch.tensor(observation['observation'], dtype=torch.float32).reshape(1,36)).detach().numpy()[0]\n",
    "                sample, policy = modelselect.select_actions(predictions, info=torch.from_numpy(observation[\"action_mask\"]).type(torch.float), mask_actions=True)\n",
    "            # if active player, branch off and traverse\n",
    "            modelselect.env.step(sample)\n",
    "            modelselect.active_player_obj.next()\n",
    "        final_rewards_p_1 = modelselect.env.rewards[\"player_0\"]  # dict of {agent_0: r0, agent_1: r1}\n",
    "        final_rewards_p_2 = modelselect.env.rewards[\"player_1\"]\n",
    "        rewards_player_1.append(final_rewards_p_1)\n",
    "        rewards_player_2.append(final_rewards_p_2)\n",
    "        modelselect.env.close()\n",
    "    return rewards_player_1, rewards_player_2\n",
    "    print(\"PLAYER 1 REW MEAN: \", np.mean(rewards_player_1))\n",
    "    print(\"PLAYER 1 REW STD: \", np.std(rewards_player_1))\n",
    "    print(\"PLAYER 2 REW MEAN: \", np.mean(rewards_player_2))\n",
    "    print(\"PLAYER 2 REW STD: \", np.std(rewards_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4780ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = np.zeros((4,4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        rewards_player_1, rewards_player_2 = evaluatebots(agents[i], agents[j], 100000)\n",
    "        results[i][j] = np.mean(rewards_player_1)\n",
    "\n",
    "print(\"RESULTS: \")\n",
    "# print results as matplotlib matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(results, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], yticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "plt.title(\"Results\")\n",
    "plt.xlabel(\"Agent 2\")\n",
    "plt.ylabel(\"Agent 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results, columns=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], index=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "df.to_csv(\"results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c05412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"results.csv\", index_col=0)\n",
    "print(df)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"], yticklabels=[\"agent1\", \"agent2\", \"agent3\", \"agent4\"])\n",
    "plt.title(\"Results\")\n",
    "plt.xlabel(\"Agent 2\")\n",
    "plt.ylabel(\"Agent 1\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
