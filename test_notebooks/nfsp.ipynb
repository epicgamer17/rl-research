{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 1000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.HuberLoss object at 0x16bd3a860>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 1000\n",
      "Using         min_replay_buffer_size        : 500\n",
      "Using         num_minibatches               : 2\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 1000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.HuberLoss object at 0x16bd3a860>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 1000\n",
      "Using         min_replay_buffer_size        : 500\n",
      "Using         num_minibatches               : 2\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 1000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 1000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.HuberLoss object at 0x16bd3a860>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 1000\n",
      "Using         min_replay_buffer_size        : 500\n",
      "Using         num_minibatches               : 2\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 1000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x16bd3b520>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 1000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 500\n",
      "Using         sl_replay_buffer_size         : 20000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x16bd3b520>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 1000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 500\n",
      "Using         sl_replay_buffer_size         : 20000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig, RainbowConfig\n",
    "from game_configs import MatchingPenniesConfig\n",
    "from modules.utils import (\n",
    "    KLDivergenceLoss,\n",
    "    CategoricalCrossentropyLoss,\n",
    "    HuberLoss,\n",
    "    MSELoss,\n",
    ")\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 1000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 2,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": HuberLoss(),\n",
    "    \"min_replay_buffer_size\": 500,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 1000,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 500,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 20000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=MatchingPenniesConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: matching_pennies_v0\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (6,)\n",
      "Observation dtype: float32\n",
      "num_actions:  2\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: matching_pennies_v0\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (6,)\n",
      "Observation dtype: float32\n",
      "num_actions:  2\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 1000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: matching_pennies_v0\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (6,)\n",
      "Observation dtype: float32\n",
      "num_actions:  2\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 1000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: matching_pennies_v0\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (6,)\n",
      "Observation dtype: float32\n",
      "num_actions:  2\n",
      "Max size: 20000\n",
      "(20000, 6)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: matching_pennies_v0\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (6,)\n",
      "Observation dtype: float32\n",
      "num_actions:  2\n",
      "Max size: 20000\n",
      "(20000, 6)\n"
     ]
    }
   ],
   "source": [
    "import custom_gym_envs\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "# env = gym.make(\"custom_gym_envs/LeducHoldem-v0\", encode_player_turn=False)\n",
    "\n",
    "env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-MatchingPennies\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:00<00:22, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 64/1000\n",
      "   Player 0 SL buffer: 5/20000\n",
      "   Player 1 RL buffer: 64/1000\n",
      "   Player 1 SL buffer: 5/20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 96/1000 [00:02<00:21, 42.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  645\n",
      "P1 SL buffer distribution [179. 466.]\n",
      "P1 actions distribution [0.27751938 0.72248062]\n",
      "P2 SL Buffer Size:  628\n",
      "P2 SL buffer distribution [442. 186.]\n",
      "P2 actions distribution [0.70382166 0.29617834]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.3419, 0.6581]])\n",
      "average score: -0.235\n",
      "Player 1 Prediction: tensor([[0.6145, 0.3855]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:316: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:380: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:246: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:266: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      " 20%|â–ˆâ–‰        | 198/1000 [00:10<00:21, 37.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  1306\n",
      "P1 SL buffer distribution [839. 467.]\n",
      "P1 actions distribution [0.6424196 0.3575804]\n",
      "P2 SL Buffer Size:  1265\n",
      "P2 SL buffer distribution [841. 424.]\n",
      "P2 actions distribution [0.66482213 0.33517787]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.5887, 0.4113]])\n",
      "average score: -0.4746\n",
      "Player 1 Prediction: tensor([[0.7315, 0.2685]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–‰       | 299/1000 [00:18<00:18, 37.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  1955\n",
      "P1 SL buffer distribution [1252.  703.]\n",
      "P1 actions distribution [0.64040921 0.35959079]\n",
      "P2 SL Buffer Size:  1905\n",
      "P2 SL buffer distribution [ 842. 1063.]\n",
      "P2 actions distribution [0.44199475 0.55800525]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.6751, 0.3249]])\n",
      "average score: -0.0664\n",
      "Player 1 Prediction: tensor([[0.4652, 0.5348]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 397/1000 [00:26<00:17, 35.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  2585\n",
      "P1 SL buffer distribution [1253. 1332.]\n",
      "P1 actions distribution [0.48471954 0.51528046]\n",
      "P2 SL Buffer Size:  2555\n",
      "P2 SL buffer distribution [ 968. 1587.]\n",
      "P2 actions distribution [0.37886497 0.62113503]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.5088, 0.4912]])\n",
      "average score: -0.2624\n",
      "Player 1 Prediction: tensor([[0.3673, 0.6327]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [00:35<00:14, 33.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  3255\n",
      "P1 SL buffer distribution [1365. 1890.]\n",
      "P1 actions distribution [0.41935484 0.58064516]\n",
      "P2 SL Buffer Size:  3207\n",
      "P2 SL buffer distribution [1620. 1587.]\n",
      "P2 actions distribution [0.505145 0.494855]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.4178, 0.5822]])\n",
      "average score: 0.0274\n",
      "Player 1 Prediction: tensor([[0.4795, 0.5205]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}, {'exploitability': -0.0137}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [00:44<00:12, 32.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  3906\n",
      "P1 SL buffer distribution [1948. 1958.]\n",
      "P1 actions distribution [0.49871992 0.50128008]\n",
      "P2 SL Buffer Size:  3841\n",
      "P2 SL buffer distribution [2036. 1805.]\n",
      "P2 actions distribution [0.53007029 0.46992971]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.4893, 0.5107]])\n",
      "average score: -0.088\n",
      "Player 1 Prediction: tensor([[0.5386, 0.4614]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}, {'exploitability': -0.0137}, {'exploitability': 0.044}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 697/1000 [00:52<00:09, 32.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  4501\n",
      "P1 SL buffer distribution [2280. 2221.]\n",
      "P1 actions distribution [0.5065541 0.4934459]\n",
      "P2 SL Buffer Size:  4443\n",
      "P2 SL buffer distribution [2264. 2179.]\n",
      "P2 actions distribution [0.50956561 0.49043439]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.5015, 0.4985]])\n",
      "average score: -0.0136\n",
      "Player 1 Prediction: tensor([[0.5109, 0.4891]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}, {'exploitability': -0.0137}, {'exploitability': 0.044}, {'exploitability': 0.0068}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [01:01<00:05, 35.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  5182\n",
      "P1 SL buffer distribution [2622. 2560.]\n",
      "P1 actions distribution [0.50598225 0.49401775]\n",
      "P2 SL Buffer Size:  5080\n",
      "P2 SL buffer distribution [2549. 2531.]\n",
      "P2 actions distribution [0.50177165 0.49822835]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.5035, 0.4965]])\n",
      "average score: -0.0178\n",
      "Player 1 Prediction: tensor([[0.5073, 0.4927]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}, {'exploitability': -0.0137}, {'exploitability': 0.044}, {'exploitability': 0.0068}, {'exploitability': 0.0089}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [01:09<00:03, 31.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 SL Buffer Size:  5792\n",
      "P1 SL buffer distribution [2892. 2900.]\n",
      "P1 actions distribution [0.49930939 0.50069061]\n",
      "P2 SL Buffer Size:  5746\n",
      "P2 SL buffer distribution [2902. 2844.]\n",
      "P2 actions distribution [0.50504699 0.49495301]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.4962, 0.5038]])\n",
      "average score: 0.0344\n",
      "Player 1 Prediction: tensor([[0.5139, 0.4861]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}, {'exploitability': -0.0137}, {'exploitability': 0.044}, {'exploitability': 0.0068}, {'exploitability': 0.0089}, {'exploitability': -0.0172}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:18<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: 0.0\n",
      "Player 0 Prediction: tensor([[0.5032, 0.4968]])\n",
      "average score: -0.0246\n",
      "Player 1 Prediction: tensor([[0.5016, 0.4984]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.1175}, {'exploitability': 0.2373}, {'exploitability': 0.0332}, {'exploitability': 0.1312}, {'exploitability': -0.0137}, {'exploitability': 0.044}, {'exploitability': 0.0068}, {'exploitability': 0.0089}, {'exploitability': -0.0172}, {'exploitability': 0.0123}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x104ea1ea0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "NFSPDQNConfig\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x104ea1ea0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "Using default save_intermediate_weights     : False\n",
      "Using         training_steps                : 50000\n",
      "Using default adam_epsilon                  : 1e-06\n",
      "Using         momentum                      : 0.0\n",
      "Using         learning_rate                 : 0.1\n",
      "Using         clipnorm                      : 10.0\n",
      "Using         optimizer                     : <class 'torch.optim.sgd.SGD'>\n",
      "Using default weight_decay                  : 0.0\n",
      "Using         loss_function                 : <utils.utils.MSELoss object at 0x104ea1ea0>\n",
      "Using default activation                    : relu\n",
      "Using         kernel_initializer            : None\n",
      "Using         minibatch_size                : 128\n",
      "Using         replay_buffer_size            : 200000.0\n",
      "Using         min_replay_buffer_size        : 1000\n",
      "Using         num_minibatches               : 1\n",
      "Using default training_iterations           : 1\n",
      "Using default print_interval                : 100\n",
      "RainbowConfig\n",
      "Using         residual_layers               : []\n",
      "Using         conv_layers                   : []\n",
      "Using         dense_layer_widths            : [128]\n",
      "Using         value_hidden_layer_widths     : []\n",
      "Using         advantage_hidden_layer_widths : []\n",
      "Using         noisy_sigma                   : 0.0\n",
      "Using         eg_epsilon                    : 0.06\n",
      "Using default eg_epsilon_final              : 0.0\n",
      "Using         eg_epsilon_decay_type         : inverse_sqrt\n",
      "Using default eg_epsilon_final_step         : 50000\n",
      "Using         dueling                       : False\n",
      "Using default discount_factor               : 0.99\n",
      "Using default soft_update                   : False\n",
      "Using         transfer_interval             : 300\n",
      "Using default ema_beta                      : 0.99\n",
      "Using         replay_interval               : 128\n",
      "Using         per_alpha                     : 0.0\n",
      "Using         per_beta                      : 0.0\n",
      "Using         per_beta_final                : 0.0\n",
      "Using         per_epsilon                   : 1e-05\n",
      "Using         n_step                        : 1\n",
      "Using         atom_size                     : 1\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x104ea02b0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "SupervisedConfig\n",
      "Using default sl_adam_epsilon               : 1e-07\n",
      "Using         sl_learning_rate              : 0.005\n",
      "Using         sl_momentum                   : 0.0\n",
      "Using         sl_loss_function              : <utils.utils.CategoricalCrossentropyLoss object at 0x104ea02b0>\n",
      "Using         sl_clipnorm                   : 10.0\n",
      "Using         sl_optimizer                  : <class 'torch.optim.sgd.SGD'>\n",
      "Using default sl_weight_decay               : 0.0\n",
      "Using         training_steps                : 50000\n",
      "Using default sl_training_iterations        : 1\n",
      "Using default sl_num_minibatches            : 1\n",
      "Using         sl_minibatch_size             : 128\n",
      "Using         sl_min_replay_buffer_size     : 1000\n",
      "Using         sl_replay_buffer_size         : 2000000\n",
      "Using default sl_activation                 : relu\n",
      "Using         sl_kernel_initializer         : None\n",
      "Using         sl_clip_low_prob              : 0.0\n",
      "Using default sl_noisy_sigma                : 0\n",
      "Using         sl_residual_layers            : []\n",
      "Using         sl_conv_layers                : []\n",
      "Using         sl_dense_layer_widths         : [128]\n",
      "Using         replay_interval               : 128\n",
      "Using         anticipatory_param            : 0.1\n",
      "Using         shared_networks_and_buffers   : False\n"
     ]
    }
   ],
   "source": [
    "# shared network but not shared buffer?\n",
    "# 1 vs 2 minibatches\n",
    "\n",
    "from nfsp_agent_clean import NFSPDQN\n",
    "from agent_configs import NFSPDQNConfig\n",
    "from game_configs import LeducHoldemConfig, MatchingPenniesConfig\n",
    "from utils import KLDivergenceLoss, CategoricalCrossentropyLoss, HuberLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "config_dict = {\n",
    "    \"shared_networks_and_buffers\": False,\n",
    "    \"training_steps\": 50000,\n",
    "    \"anticipatory_param\": 0.1,\n",
    "    \"replay_interval\": 128,  #\n",
    "    \"num_minibatches\": 1,  # or 2, could be 2 minibatches per network, or 2 minibatches (1 for each network/player)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.0,\n",
    "    \"optimizer\": SGD,\n",
    "    \"loss_function\": MSELoss(),\n",
    "    \"min_replay_buffer_size\": 1000,\n",
    "    \"minibatch_size\": 128,\n",
    "    \"replay_buffer_size\": 2e5,\n",
    "    \"transfer_interval\": 300,\n",
    "    \"residual_layers\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [128],\n",
    "    \"value_hidden_layer_widths\": [],\n",
    "    \"advantage_hidden_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"eg_epsilon\": 0.06,\n",
    "    # \"eg_epsilon_final\": 0.06,\n",
    "    \"eg_epsilon_decay_type\": \"inverse_sqrt\",\n",
    "    \"eg_epsilon_decay_final_step\": 0,\n",
    "    \"sl_learning_rate\": 0.005,\n",
    "    \"sl_momentum\": 0.0,\n",
    "    # \"sl_weight_decay\": 1e-9,\n",
    "    # \"sl_clipnorm\": 1.0,\n",
    "    \"sl_optimizer\": SGD,\n",
    "    \"sl_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"sl_min_replay_buffer_size\": 1000,\n",
    "    \"sl_minibatch_size\": 128,\n",
    "    \"sl_replay_buffer_size\": 2000000,\n",
    "    \"sl_residual_layers\": [],\n",
    "    \"sl_conv_layers\": [],\n",
    "    \"sl_dense_layer_widths\": [128],\n",
    "    \"sl_clip_low_prob\": 0.0,\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_epsilon\": 0.00001,\n",
    "    \"n_step\": 1,\n",
    "    \"atom_size\": 1,\n",
    "    \"dueling\": False,\n",
    "    \"clipnorm\": 10.0,\n",
    "    \"sl_clipnorm\": 10.0,\n",
    "}\n",
    "config = NFSPDQNConfig(\n",
    "    config_dict=config_dict,\n",
    "    game_config=LeducHoldemConfig(),\n",
    ")\n",
    "config.save_intermediate_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (36,), float32))\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Warning: SGD does not use adam_epsilon param\n",
      "float32\n",
      "Max size: 200000\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n",
      "making test env\n",
      "Warning: test_env will not record videos as render_mode is not 'rgb_array'\n",
      "Test env: leduc_holdem_v4\n",
      "<class 'method'>\n",
      "petting zoo\n",
      "Observation dimensions: (36,)\n",
      "Observation dtype: float32\n",
      "num_actions:  4\n",
      "Max size: 2000000\n",
      "(2000000, 36)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "from custom_gym_envs.envs.matching_pennies import (\n",
    "    env as matching_pennies_env,\n",
    "    MatchingPenniesGymEnv,\n",
    ")\n",
    "\n",
    "\n",
    "env = leduc_holdem_v4.env()\n",
    "# env = matching_pennies_env(render_mode=\"human\", max_cycles=1)\n",
    "\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "agent = NFSPDQN(env, config, name=\"NFSP-LeducHoldem-Standard\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Initial policies: ['average_strategy', 'average_strategy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/50000 [00:00<43:03, 19.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0600 â†’ 0.0600\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 0:\n",
      "   Player 0 RL buffer: 60/200000\n",
      "   Player 0 SL buffer: 11/2000000\n",
      "   Player 1 RL buffer: 68/200000\n",
      "   Player 1 SL buffer: 8/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1003/50000 [01:36<1:08:23, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0019 â†’ 0.0019\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 1000:\n",
      "   Player 0 RL buffer: 64282/200000\n",
      "   Player 0 SL buffer: 7059/2000000\n",
      "   Player 1 RL buffer: 63844/200000\n",
      "   Player 1 SL buffer: 6758/2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 2000/50000 [03:03<1:34:32,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Player 0 Îµ: 0.0013 â†’ 0.0013\n",
      "\n",
      "ðŸ“Š Buffer sizes at step 2000:\n",
      "   Player 0 RL buffer: 128144/200000\n",
      "   Player 0 SL buffer: 14000/2000000\n",
      "   Player 1 RL buffer: 127982/200000\n",
      "   Player 1 SL buffer: 13611/2000000\n",
      "P1 SL Buffer Size:  14000\n",
      "P1 SL buffer distribution [4229. 7531.  680. 1560.]\n",
      "P1 actions distribution [0.30207143 0.53792857 0.04857143 0.11142857]\n",
      "P2 SL Buffer Size:  13611\n",
      "P2 SL buffer distribution [4520. 7048.  772. 1271.]\n",
      "P2 actions distribution [0.33208434 0.51781647 0.05671883 0.09338035]\n",
      "warning: frames_seen option is deprecated, update self.total_environment_steps instead\n",
      "warning: time_taken option is deprecated, update self.training_time instead\n",
      "warning: training_step option is deprecated, update self.training_step instead\n",
      "WARNING: NFSP does not checkpoint environments, as RL card environments are not pickleable\n",
      "average score: -1.0087\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7928, 0.0460, 0.1612]])\n",
      "Player 0 Prediction: tensor([[0.0000, 0.7660, 0.0612, 0.1728]])\n",
      "average score: -0.97385\n",
      "Player 1 Prediction: tensor([[0.0000, 0.8649, 0.0416, 0.0935]])\n",
      "Player 1 Prediction: tensor([[0.0000, 0.7908, 0.0686, 0.1406]])\n",
      "Plotting rl_loss...\n",
      "Plotting sl_loss...\n",
      "Plotting exploitability...\n",
      "[{'exploitability': 0.9912749999999999}]\n",
      "Plotting test_score_vs_random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:316: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:380: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:246: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  axs[row][col].set_xlim(1, len(values))\n",
      "/Users/jonathanlamontange-kratz/Documents/GitHub/rl-stuff/packages/utils/utils/utils.py:266: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[row][col].legend()\n",
      "  4%|â–         | 2224/50000 [04:30<1:36:45,  8.23it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent\u001b[38;5;241m.\u001b[39mcheckpoint_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m      2\u001b[0m agent\u001b[38;5;241m.\u001b[39mcheckpoint_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/dqn/NFSP/nfsp_agent_clean.py:315\u001b[0m, in \u001b[0;36mNFSPDQN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    309\u001b[0m (\n\u001b[1;32m    310\u001b[0m     next_state,\n\u001b[1;32m    311\u001b[0m     reward,\n\u001b[1;32m    312\u001b[0m     termination,\n\u001b[1;32m    313\u001b[0m     truncation,\n\u001b[1;32m    314\u001b[0m     next_info,\n\u001b[0;32m--> 315\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m done \u001b[38;5;241m=\u001b[39m termination \u001b[38;5;129;01mor\u001b[39;00m truncation\n\u001b[1;32m    317\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/utils/env.py:186\u001b[0m, in \u001b[0;36mAECEnv.last\u001b[0;34m(self, observe)\u001b[0m\n\u001b[1;32m    184\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m observe \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    188\u001b[0m     observation,\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cumulative_rewards[agent],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfos[agent],\n\u001b[1;32m    193\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:101\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.observe\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    100\u001b[0m     EnvLogger\u001b[38;5;241m.\u001b[39merror_observe_before_reset()\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:41\u001b[0m, in \u001b[0;36mBaseWrapper.observe\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ObsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:41\u001b[0m, in \u001b[0;36mBaseWrapper.observe\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ObsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/utils/wrappers/terminate_illegal.py:31\u001b[0m, in \u001b[0;36mTerminateIllegalWrapper.observe\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ObsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:41\u001b[0m, in \u001b[0;36mBaseWrapper.observe\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ObsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pettingzoo/classic/rlcard_envs/rlcard_base.py:79\u001b[0m, in \u001b[0;36mRLCardBase.observe\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[0;32m---> 79\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name_to_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     observation \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m     82\u001b[0m     legal_moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_legal_moves\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/rlcard/envs/env.py:197\u001b[0m, in \u001b[0;36mEnv.get_state\u001b[0;34m(self, player_id)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, player_id):\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Get the state given player id\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m        (numpy.array): The observed state of the player\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/rlcard/envs/leducholdem.py:54\u001b[0m, in \u001b[0;36mLeducholdemEnv._extract_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Extract the state representation from state dictionary for agent\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mNote: Currently the use the hand cards and the public cards. TODO: encode the states\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    observation (list): combine the player's score and dealer's observable score for observation\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     52\u001b[0m extracted_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 54\u001b[0m legal_actions \u001b[38;5;241m=\u001b[39m OrderedDict({\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mindex(a): \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     55\u001b[0m extracted_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m legal_actions\n\u001b[1;32m     57\u001b[0m public_card \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic_card\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/rlcard/envs/leducholdem.py:54\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Extract the state representation from state dictionary for agent\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mNote: Currently the use the hand cards and the public cards. TODO: encode the states\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    observation (list): combine the player's score and dealer's observable score for observation\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     52\u001b[0m extracted_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 54\u001b[0m legal_actions \u001b[38;5;241m=\u001b[39m OrderedDict({\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     55\u001b[0m extracted_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m legal_actions\n\u001b[1;32m     57\u001b[0m public_card \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic_card\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agent.checkpoint_interval = 2000\n",
    "agent.checkpoint_trials = 10000\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
