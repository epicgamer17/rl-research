{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [0, 500],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [512, 64],\n",
    "    \"dynamics_dense_layer_widths\": [512, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [512],  # ???\n",
    "    \"critic_dense_layer_widths\": [512],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 50,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.5, 0.25],\n",
    "    \"temperature_updates\": [30000, 60000],\n",
    "    \"temperature_with_training_steps\": True,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.005,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"discount_factor\": 0.997,\n",
    "    \"value_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"reward_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 50000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 10,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    # \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": 31,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = CartPoleConfig().make_env()\n",
    "game_config = CartPoleConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_cartpole\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 10\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 50\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(24, 3, 1)] * 1,  # increase num layers\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layers\": [],\n",
    "    \"actor_conv_layers\": [(16, 1, 1)],\n",
    "    \"critic_conv_layers\": [(16, 1, 1)],\n",
    "    \"reward_conv_layers\": [(16, 1, 1)],\n",
    "    \"actor_dense_layer_widths\": [],\n",
    "    \"critic_dense_layer_widths\": [],\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # try larger\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [8],  # change this\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # slightly increase this, maybe 0.002 or 0.003\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,  # try lower\n",
    "    \"value_loss_function\": MSELoss(),  #  MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),  # MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 8,\n",
    "    \"min_replay_buffer_size\": 4000,  # try lower (or just different)\n",
    "    \"replay_buffer_size\": 100000,  # try lower, 50k or 20k or 10k\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"glorot_normal\",  # try different\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,  # 0.0 was original\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,  # None\n",
    "}\n",
    "\n",
    "# REANALYZE NOT IMPLEMENTED BUT NEVER USED IN THING I SAW ONLINE (like it is computed but never ends up used since bootstrap index always > than len of game history)\n",
    "\n",
    "# add a max depth to the tree\n",
    "# game priority is max of position priorities of the game\n",
    "# with these two changes should be an exact match with online github implementation\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-hyperopt-best\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 50\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 300\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [],  # ??? more depth better? up to depth 16, need at most 16 filters\n",
    "    \"representation_dense_layer_widths\": [256, 64],\n",
    "    \"dynamics_dense_layer_widths\": [256, 64],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [256],  # ???\n",
    "    \"critic_dense_layer_widths\": [256],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 0.25,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.002,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_onehot,\n",
    "    \"training_steps\": 100000,\n",
    "    \"minibatch_size\": 128,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 5000,  # ???\n",
    "    \"replay_buffer_size\": 20000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"he_normal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": True,\n",
    "    \"per_initial_priority_max\": True,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 6,  # ???\n",
    "    # \"lr_ratio\": float(\"inf\"),  # 0.1\n",
    "    \"lr_ratio\": 0.1,  # 0.1\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# # run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe-dense-5moves_1\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tested the ones i found on the github and they worked good? but i think i am gonna do less workers because they did 480k episodes after 32k training steps, i got 250k after like 5k steps, so i want to change that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"critic_conv_layers\": [(8, 1, 1)],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 1.8,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [2],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": SGD,\n",
    "    \"learning_rate\": 0.1,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.9,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 4000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 10000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 1.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 2 moves left one losing one drawing, it finds the drawing move, but evaluates the winning position after tree search as 0 instead of 1 (though predicts it as 1). shown in paper 2m-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 250\n",
    "agent.test_trials = 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from packages.utils.utils.utils import process_petting_zoo_obs\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def play_game(player1, player2):\n",
    "\n",
    "    env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "    with torch.no_grad():  # No gradient computation during testing\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        state, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        agent_id = env.agent_selection\n",
    "        current_player = env.agents.index(agent_id)\n",
    "        state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "        agent_names = env.agents.copy()\n",
    "\n",
    "        episode_length = 0\n",
    "        while not done and episode_length < 1000:  # Safety limit\n",
    "            # Get current agent and player\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get action from average strategy\n",
    "            if current_player == 0:\n",
    "                prediction = player1.predict(state, info, env=env)\n",
    "                action = player1.select_actions(prediction, info).item()\n",
    "            else:\n",
    "                prediction = player2.predict(state, info, env=env)\n",
    "                action = player2.select_actions(prediction, info).item()\n",
    "\n",
    "            # Step environment\n",
    "            env.step(action)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "            agent_id = env.agent_selection\n",
    "            current_player = env.agents.index(agent_id)\n",
    "            state, info = process_petting_zoo_obs(state, info, current_player)\n",
    "            done = termination or truncation\n",
    "        print(env.rewards)\n",
    "        return env.rewards[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from elo.elo import StandingsTable\n",
    "\n",
    "\n",
    "random_vs_expert_table = StandingsTable([agent, TicTacToeBestAgent()], start_elo=1400)\n",
    "random_vs_expert_table.play_1v1_tournament(1000, play_game)\n",
    "print(random_vs_expert_table.bayes_elo())\n",
    "print(random_vs_expert_table.get_win_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToeBestAgent:\n",
    "    def __init__(self, model_name=\"tictactoe_expert\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def predict(self, observation, info, env=None):\n",
    "        return observation, info\n",
    "\n",
    "    def select_actions(self, prediction, info):\n",
    "        # Reconstruct board: +1 for current player, -1 for opponent, 0 otherwise\n",
    "        board = prediction[0][0] - prediction[0][1]\n",
    "        print(board)\n",
    "        # Default: random legal move\n",
    "        action = np.random.choice(info[\"legal_moves\"])\n",
    "\n",
    "        # Horizontal and vertical checks\n",
    "        for i in range(3):\n",
    "            # Row\n",
    "            if np.sum(board[i, :]) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                return np.ravel_multi_index((i, ind), (3, 3))\n",
    "            elif abs(np.sum(board[i, :])) == 2 and 0 in board[i, :]:\n",
    "                ind = np.where(board[i, :] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((i, ind), (3, 3))\n",
    "\n",
    "            # Column\n",
    "            if np.sum(board[:, i]) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                return np.ravel_multi_index((ind, i), (3, 3))\n",
    "            elif abs(np.sum(board[:, i])) == 2 and 0 in board[:, i]:\n",
    "                ind = np.where(board[:, i] == 0)[0][0]\n",
    "                action = np.ravel_multi_index((ind, i), (3, 3))\n",
    "\n",
    "        # Diagonals\n",
    "        diag = board.diagonal()\n",
    "        if np.sum(diag) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, ind), (3, 3))\n",
    "        elif abs(np.sum(diag)) == 2 and 0 in diag:\n",
    "            ind = np.where(diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, ind), (3, 3))\n",
    "\n",
    "        anti_diag = np.fliplr(board).diagonal()\n",
    "        if np.sum(anti_diag) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            return np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "        elif abs(np.sum(anti_diag)) == 2 and 0 in anti_diag:\n",
    "            ind = np.where(anti_diag == 0)[0][0]\n",
    "            action = np.ravel_multi_index((ind, 2 - ind), (3, 3))\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    FrameStackWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from agent_configs import MuZeroConfig\n",
    "import gymnasium as gym\n",
    "from utils.utils import CategoricalCrossentropyLoss\n",
    "from action_functions import action_as_plane, action_as_onehot\n",
    "from muzero_agent_torch import MuZeroAgent\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig, CartPoleConfig\n",
    "from utils import MSELoss\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import Adam, SGD\n",
    "from agents.random import RandomAgent\n",
    "from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"known_bounds\": [-1, 1],\n",
    "    \"residual_layers\": [(16, 3, 1)],\n",
    "    \"representation_dense_layer_widths\": [],\n",
    "    \"dynamics_dense_layer_widths\": [],\n",
    "    \"actor_conv_layers\": [],  # ???\n",
    "    \"critic_conv_layers\": [],  # ???\n",
    "    \"reward_conv_layers\": [],\n",
    "    \"actor_dense_layer_widths\": [],  # ???\n",
    "    \"critic_dense_layer_widths\": [],  # ???\n",
    "    \"reward_dense_layer_widths\": [],\n",
    "    \"conv_layers\": [],\n",
    "    \"dense_layer_widths\": [],\n",
    "    \"noisy_sigma\": 0.0,\n",
    "    \"value_loss_factor\": 1.0,\n",
    "    \"root_dirichlet_alpha\": 2.0,  # ???\n",
    "    \"root_exploration_fraction\": 0.25,\n",
    "    \"num_simulations\": 25,  # ??? goal is to increase this and see if it learns faster\n",
    "    \"temperatures\": [1.0, 0.1],\n",
    "    \"temperature_updates\": [5],\n",
    "    \"temperature_with_training_steps\": False,\n",
    "    \"clip_low_prob\": 0.0,\n",
    "    \"pb_c_base\": 19652,\n",
    "    \"pb_c_init\": 1.25,\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,  # ??? find a learning rate that works okay (no exploding, but not too small) # 0.1 to 0.01 decrease to 10% of the init value after 400k steps in pseudocode, but 0.2 in alphazero paper (and decreased 3 times)\n",
    "    \"momentum\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"value_loss_function\": MSELoss(),\n",
    "    \"reward_loss_function\": MSELoss(),\n",
    "    \"policy_loss_function\": CategoricalCrossentropyLoss(),\n",
    "    \"action_function\": action_as_plane,\n",
    "    \"training_steps\": 33000,\n",
    "    \"minibatch_size\": 32,  # ??? this should be about 0.1 of the number of positions collected... or is it in the replay buffer? AlphaZero did a batch size of 4096 muzero 2048, and they said this was about 0.1.\n",
    "    \"min_replay_buffer_size\": 1000,  # 9000 # ???\n",
    "    \"replay_buffer_size\": 40000,  # ??? paper used a buffer size of 1M games\n",
    "    \"unroll_steps\": 5,\n",
    "    \"n_step\": 9,\n",
    "    \"clipnorm\": 0.0,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"kernel_initializer\": \"orthogonal\",  # ???\n",
    "    \"per_alpha\": 0.0,\n",
    "    \"per_beta\": 0.0,\n",
    "    \"per_beta_final\": 0.0,\n",
    "    \"per_use_batch_weights\": False,\n",
    "    \"per_initial_priority_max\": False,\n",
    "    \"per_epsilon\": 0.0001,\n",
    "    \"multi_process\": True,\n",
    "    \"num_workers\": 2,  # ???\n",
    "    \"lr_ratio\": float(\"inf\"),\n",
    "    # \"lr_ratio\": 0.1,\n",
    "    \"games_per_generation\": 8,  # ??? AlphaZero did ~64 games per generation\n",
    "    \"reanalyze\": True,  # TODO\n",
    "    \"support_range\": None,\n",
    "}\n",
    "\n",
    "# DO AN ABALATION ON NUM SIMULATIONS, THE OG PAPER FOUND MORE SIMULATIONS MEANS BETTER LEARNING SIGNAL\n",
    "# CHECK MY MCTS STUFF, IS THE SIGN CORRECT? IS IT CORRECT WITH REWARDS? IS IT CORRECT FOR TERMINAL STATES?\n",
    "\n",
    "# steps, run tictactoe on fast settings for at least 200k steps, see if it learns to play okay\n",
    "# add only updating mcts network every x steps\n",
    "# add a ratio for learning steps to self play steps\n",
    "# add frame stacking\n",
    "# run it without multiprocessing\n",
    "# increase num simulations to 50 or 100 and see if it learns faster\n",
    "\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "\n",
    "game_config = TicTacToeConfig()\n",
    "config = MuZeroConfig(config, game_config)\n",
    "\n",
    "agent = MuZeroAgent(\n",
    "    env,\n",
    "    config,\n",
    "    name=\"muzero_tictactoe_hyperopt-2\",\n",
    "    device=\"cpu\",\n",
    "    test_agents=[RandomAgent(), TicTacToeBestAgent()],  # RandomAgent(),\n",
    ")\n",
    "\n",
    "agent.checkpoint_interval = 100\n",
    "agent.test_interval = 1000\n",
    "agent.test_trials = 500\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.training_steps += 100\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from wrappers import (\n",
    "    ActionMaskInInfoWrapper,\n",
    "    ChannelLastToFirstWrapper,\n",
    "    TwoPlayerPlayerPlaneWrapper,\n",
    "    FrameStackWrapper,\n",
    ")\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from game_configs import TicTacToeConfig\n",
    "\n",
    "# from agents.tictactoe_expert import TicTacToeBestAgent\n",
    "\n",
    "# from agents.random import RandomAgent\n",
    "from supersuit import frame_stack_v1, agent_indicator_v0\n",
    "\n",
    "best_agent = TicTacToeBestAgent()\n",
    "env = tictactoe_v3.env(render_mode=\"rgb_array\")\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ActionMaskInInfoWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = FrameStackWrapper(env, 4, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = TwoPlayerPlayerPlaneWrapper(env, channel_first=False)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "env = ChannelLastToFirstWrapper(env)\n",
    "print(env.observation_space(\"player_0\"))\n",
    "\n",
    "env = TicTacToeConfig().make_env()\n",
    "env.reset()\n",
    "env.step(0)\n",
    "env.step(6)\n",
    "env.step(2)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "\n",
    "selected_actions = {i: 0 for i in range(agent.num_actions)}\n",
    "for i in range(100):\n",
    "    selected_actions[agent.select_actions(prediction).item()] += 1\n",
    "print(selected_actions)\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "env.step(action)\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "prediction = agent.predict(state, info, env)\n",
    "print(\"MCTS Prediction\", prediction)\n",
    "initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "print(\"Initial Value\", initial_inference[0])\n",
    "print(\"Initial Policy\", initial_inference[1])\n",
    "\n",
    "for move in info[\"legal_moves\"]:\n",
    "    reccurent_inference = agent.predict_single_recurrent_inference(\n",
    "        initial_inference[2], move\n",
    "    )\n",
    "    print(\"Move\", move)\n",
    "    print(\"Reccurent Value\", reccurent_inference[2])\n",
    "    print(\"Reccurent Reward\", reccurent_inference[0])\n",
    "    print(\"Reccurent Policy\", reccurent_inference[3])\n",
    "\n",
    "action = agent.select_actions(prediction).item()\n",
    "print(\"Action\", action)\n",
    "# env.step(action)\n",
    "# state, reward, terminated, truncated, info = env.last()\n",
    "# prediction = agent.predict(state, info, env)\n",
    "# print(\"MCTS Prediction\", prediction)\n",
    "# initial_inference = agent.predict_single_initial_inference(state, info)\n",
    "# print(\"Initial Value\", initial_inference[0])\n",
    "# print(\"Initial Policy\", initial_inference[1])\n",
    "# action = agent.select_actions(prediction).item()\n",
    "# print(\"Action\", action)\n",
    "# env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from math import log, sqrt, inf\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior_policy):\n",
    "        self.visits = 0\n",
    "        self.to_play = -1\n",
    "        self.prior_policy = prior_policy\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expand(self, legal_moves, to_play, policy, hidden_state, reward):\n",
    "        self.to_play = to_play\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        # print(legal_moves)\n",
    "        policy = {a: policy[a] for a in legal_moves}\n",
    "        policy_sum = sum(policy.values())\n",
    "\n",
    "        for action, p in policy.items():\n",
    "            self.children[action] = Node((p / (policy_sum + 1e-10)).item())\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visits == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visits\n",
    "\n",
    "    def add_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior_policy = (1 - frac) * self.children[\n",
    "                a\n",
    "            ].prior_policy + frac * n\n",
    "\n",
    "    def select_child(self, min_max_stats, pb_c_base, pb_c_init, discount, num_players):\n",
    "        # Select the child with the highest UCB\n",
    "        child_ucbs = [\n",
    "            self.child_ucb_score(\n",
    "                child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "            )\n",
    "            for action, child in self.children.items()\n",
    "        ]\n",
    "        print(\"Child UCBs\", child_ucbs)\n",
    "        action_index = np.random.choice(\n",
    "            np.where(np.isclose(child_ucbs, max(child_ucbs)))[0]\n",
    "        )\n",
    "        action = list(self.children.keys())[action_index]\n",
    "        return action, self.children[action]\n",
    "\n",
    "    def child_ucb_score(\n",
    "        self, child, min_max_stats, pb_c_base, pb_c_init, discount, num_players\n",
    "    ):\n",
    "        pb_c = log((self.visits + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
    "        pb_c *= sqrt(self.visits) / (child.visits + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior_policy\n",
    "        if child.visits > 0:\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward\n",
    "                + discount\n",
    "                * (\n",
    "                    child.value() if num_players == 1 else -child.value()\n",
    "                )  # (or if on the same team)\n",
    "            )\n",
    "        else:\n",
    "            value_score = 0.0\n",
    "\n",
    "        # check if value_score is nan\n",
    "        assert (\n",
    "            value_score == value_score\n",
    "        ), \"value_score is nan, child value is {}, and reward is {},\".format(\n",
    "            child.value(),\n",
    "            child.reward,\n",
    "        )\n",
    "        assert prior_score == prior_score, \"prior_score is nan\"\n",
    "        print(\"Prior Score\", prior_score)\n",
    "        print(\"Value Score\", value_score)\n",
    "        return prior_score + value_score\n",
    "        # return value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.config.num_simulations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Optional\n",
    "\n",
    "from pyparsing import List\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float(\"inf\")\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "    def __init__(\n",
    "        self, known_bounds: Optional[List[float]]\n",
    "    ):  # might need to say known_bounds=None\n",
    "        self.max = known_bounds[1] if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "        self.min = known_bounds[0] if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.max = max(self.max, value)\n",
    "        self.min = min(self.min, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        print(\"Initial value\", value)\n",
    "        if self.max > self.min:\n",
    "            # We normalize only when we have at a max and min value\n",
    "            print(\"normalized value\", (value - self.min) / (self.max - self.min))\n",
    "            return (value - self.min) / (self.max - self.min)\n",
    "        return value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"min: {self.min}, max: {self.max}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_legal_moves\n",
    "\n",
    "# from muzero.muzero_mcts import Node\n",
    "# from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "root = Node(0.0)\n",
    "_, policy, hidden_state = agent.predict_single_initial_inference(\n",
    "    state,\n",
    "    info,\n",
    ")\n",
    "print(\"root policy\", policy)\n",
    "legal_moves = get_legal_moves(info)[0]\n",
    "to_play = env.agents.index(env.agent_selection)\n",
    "root.expand(legal_moves, to_play, policy, hidden_state, 0.0)\n",
    "print(\"expanded root\")\n",
    "min_max_stats = MinMaxStats(agent.config.known_bounds)\n",
    "\n",
    "for _ in range(agent.config.num_simulations):\n",
    "    print(\"at root\")\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "    to_play = env.agents.index(env.agent_selection)\n",
    "\n",
    "    # GO UNTIL A LEAF NODE IS REACHED\n",
    "    while node.expanded():\n",
    "        print(\"selecting child\")\n",
    "        action, node = node.select_child(\n",
    "            min_max_stats,\n",
    "            agent.config.pb_c_base,\n",
    "            agent.config.pb_c_init,\n",
    "            agent.config.discount_factor,\n",
    "            agent.config.game.num_players,\n",
    "        )\n",
    "        print(\"Selected action\", action)\n",
    "        # THIS NEEDS TO BE CHANGED FOR GAMES WHERE PLAYER COUNT DECREASES AS PLAYERS GET ELIMINATED, USE agent_selector.next() (clone of the current one)\n",
    "        to_play = (to_play + 1) % agent.config.game.num_players\n",
    "        search_path.append(node)\n",
    "    parent = search_path[-2]\n",
    "    reward, hidden_state, value, policy = agent.predict_single_recurrent_inference(\n",
    "        parent.hidden_state,\n",
    "        action,  # model=model\n",
    "    )\n",
    "    reward = reward.item()\n",
    "    value = value.item()\n",
    "    print(\"leaf value\", value)\n",
    "    print(\"leaf reward\", reward)\n",
    "\n",
    "    node.expand(\n",
    "        list(range(agent.num_actions)),\n",
    "        to_play,\n",
    "        policy,\n",
    "        hidden_state,\n",
    "        (\n",
    "            reward  # if self.config.game.has_intermediate_rewards else 0.0\n",
    "        ),  # for board games and games with no intermediate rewards\n",
    "    )\n",
    "\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "        min_max_stats.update(\n",
    "            node.reward\n",
    "            + agent.config.discount_factor\n",
    "            * (node.value() if agent.config.game.num_players == 1 else -node.value())\n",
    "        )\n",
    "        value = (\n",
    "            -node.reward\n",
    "            if node.to_play == to_play and agent.config.game.num_players > 1\n",
    "            else node.reward\n",
    "        ) + agent.config.discount_factor * value\n",
    "\n",
    "    visit_counts = [(child.visits, action) for action, child in root.children.items()]\n",
    "\n",
    "print(visit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "import torch\n",
    "\n",
    "num_players = 2\n",
    "min_max_stats = MinMaxStats([-1, 1])\n",
    "\n",
    "\n",
    "def make_search_path():\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "    root.expand(legal_moves, 0, policy, hidden_state, 0.0)\n",
    "\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "    search_path.append(node)\n",
    "    node.expand(legal_moves, 1, policy, hidden_state, 0.0)\n",
    "    node = node.children[0]\n",
    "    search_path.append(node)\n",
    "    node.expand(legal_moves, 1, policy, hidden_state, 1.0)\n",
    "    node = node.children[0]\n",
    "    search_path.append(node)\n",
    "\n",
    "    value = 0.0\n",
    "    reward = 0.0\n",
    "    to_play = 0\n",
    "    node.expand(legal_moves, to_play, policy, hidden_state, reward)\n",
    "    print(\"leaf value\", value)\n",
    "    print(\"leaf reward\", reward)\n",
    "    print(\"leaf to_play\", to_play)\n",
    "\n",
    "    return search_path, to_play, value\n",
    "\n",
    "\n",
    "search_path_1, to_play_1, value_1 = make_search_path()\n",
    "search_path_2, to_play_2, value_2 = make_search_path()\n",
    "\n",
    "for _ in range(1):\n",
    "    for node_1, node_2 in zip(reversed(search_path_1), reversed(search_path_2)):\n",
    "        print(node_1)\n",
    "        print(\"node 1 to_play\", node_1.to_play)\n",
    "        print(\"init value sum 1\", node_1.value_sum)\n",
    "        node_1.value_sum += value_1 if node_1.to_play == to_play_1 else -value_1\n",
    "        print(\"new value sum 1\", node_1.value_sum)\n",
    "        node_1.visits += 1\n",
    "        min_max_update_1 = node_1.reward + 1.0 * (\n",
    "            node_1.value() if node_1.to_play == to_play_1 else -node_1.value()\n",
    "        )\n",
    "        print(\"min max update 1\", min_max_update_1)\n",
    "        min_max_stats.update(min_max_update_1)\n",
    "\n",
    "        print(node_2)\n",
    "        print(\"node to_play\", node_2.to_play)\n",
    "        print(\"init value sum\", node_2.value_sum)\n",
    "        node_2.value_sum += value_2 if node_2.to_play == to_play_2 else -value_2\n",
    "        print(\"new value sum\", node_2.value_sum)\n",
    "        node_2.visits += 1\n",
    "        if num_players == 1:\n",
    "            min_max_update_2 = node_2.reward + 1.0 * node_2.value()\n",
    "        elif node_2.to_play == to_play_2:\n",
    "            min_max_update_2 = node_2.reward + 1.0 * node_2.value()\n",
    "        else:\n",
    "            min_max_update_2 = node_2.reward + 1.0 * (-node_2.value())\n",
    "        print(\"min max update 2\", min_max_update_2)\n",
    "        min_max_stats.update(min_max_update_2)\n",
    "\n",
    "        print(node_1.value_sum == node_2.value_sum)\n",
    "        print(node_1.visits == node_2.visits)\n",
    "        print(min_max_update_1 == min_max_update_2)\n",
    "\n",
    "        #  METHOD 1 (baseline)\n",
    "        if node_1.to_play == to_play_1 and num_players > 1:\n",
    "            value_1 = -node_1.reward + 1.0 * value_1\n",
    "        else:\n",
    "            value_1 = node_1.reward + 1.0 * value_1\n",
    "\n",
    "        print(\"next value to be added\", value_1)\n",
    "        print(\"method 1 value\", node_1.value())\n",
    "\n",
    "        #  METHOD 2 (generalized version)\n",
    "        if num_players == 1:\n",
    "            value_2 = node_2.reward + 1.0 * value_2\n",
    "        elif node_2.to_play == to_play_2:\n",
    "            value_2 = -node_2.reward + 1.0 * value_2\n",
    "        else:\n",
    "            value_2 = node_2.reward + 1.0 * value_2\n",
    "        print(\"next value to be added\", value_2)\n",
    "        print(\"method 2 value\", node_2.value())\n",
    "\n",
    "        print(value_1 == value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "def make_search_path(path_config):\n",
    "    \"\"\"\n",
    "    path_config is a list of tuples: (to_play, reward)\n",
    "    Last element has the leaf value and to_play\n",
    "    \"\"\"\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "\n",
    "    # Initialize root (player 0)\n",
    "    root.expand(legal_moves, 0, policy, hidden_state, 0.0)\n",
    "\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "\n",
    "    # Build path according to config\n",
    "    for i, (to_play, reward) in enumerate(path_config[:-1]):\n",
    "        search_path.append(node)\n",
    "        node.expand(legal_moves, to_play, policy, hidden_state, reward)\n",
    "        node = node.children[0]\n",
    "\n",
    "    # Last node\n",
    "    search_path.append(node)\n",
    "    last_config = path_config[-1]\n",
    "    leaf_to_play = last_config[0]\n",
    "    leaf_reward = last_config[1]\n",
    "    leaf_value = last_config[2] if len(last_config) > 2 else 0.0\n",
    "    node.expand(legal_moves, leaf_to_play, policy, hidden_state, leaf_reward)\n",
    "\n",
    "    return search_path, leaf_to_play, leaf_value\n",
    "\n",
    "\n",
    "def backpropagate_method1(search_path, to_play, value, num_players):\n",
    "    \"\"\"Original Method 1\"\"\"\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visits += 1\n",
    "\n",
    "        if node.to_play == to_play and num_players > 1:\n",
    "            value = -node.reward + 1.0 * value\n",
    "        else:\n",
    "            value = node.reward + 1.0 * value\n",
    "\n",
    "    return [n.value() for n in search_path]\n",
    "\n",
    "\n",
    "def backpropagate_method2(search_path, to_play, value, num_players):\n",
    "    \"\"\"\n",
    "    Robust method 2: compute, for each node in the search_path, the total return\n",
    "    from that node according to that node's player's perspective, then add it\n",
    "    to node.value_sum and increment visits.\n",
    "\n",
    "    - `to_play` is the leaf player (owner of `value`).\n",
    "    - `value` is the leaf value (scalar).\n",
    "    - reward is stored on nodes so that search_path[j].reward is the reward\n",
    "      for the parent at j-1 (i.e. it's the reward received when moving into j).\n",
    "    \"\"\"\n",
    "    leaf_to_play = to_play\n",
    "    leaf_value = value\n",
    "\n",
    "    # For each node in the path compute the exact return from that node's perspective.\n",
    "    # Complexity O(n^2) in path length  fine for typical MCTS path lengths.\n",
    "    for i, node in enumerate(search_path):\n",
    "        total = 0.0\n",
    "        # sum future rewards (reward at search_path[j] belongs to acting_player = search_path[j-1].to_play)\n",
    "        for j in range(i + 1, len(search_path)):\n",
    "            acting_player = search_path[j - 1].to_play\n",
    "            r = search_path[j].reward\n",
    "            if acting_player == node.to_play:\n",
    "                total += r\n",
    "            else:\n",
    "                total -= r\n",
    "\n",
    "        # add the leaf value (belongs to leaf_to_play)\n",
    "        if leaf_to_play == node.to_play:\n",
    "            total += leaf_value\n",
    "        else:\n",
    "            total -= leaf_value\n",
    "\n",
    "        # update node stats (value_sum / visits semantics preserved)\n",
    "        node.value_sum += total\n",
    "        node.visits += 1\n",
    "\n",
    "    return [n.value() for n in search_path]\n",
    "\n",
    "\n",
    "# Test cases: (path_config, expected_root_value, num_players, description)\n",
    "test_cases = [\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 1.0, 0.0)],\n",
    "        [1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)],\n",
    "        [1.0, -1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 1.0), (0, 1.0, 0.0)],\n",
    "        [3.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)],\n",
    "        [4.0, 4.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)],\n",
    "        [-5.0, 5.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    # Single player test cases\n",
    "    (\n",
    "        [(0, 1.0), (0, 2.0), (0, 3.0, 0.0)],\n",
    "        [6.0, 5.0, 3.0, 0.0],\n",
    "        1,\n",
    "        \"1-player: All rewards sum up\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 0.0), (0, 0.0, 5.0)],\n",
    "        [6.0, 5.0, 5.0, 5.0],\n",
    "        1,\n",
    "        \"1-player: Rewards + leaf value\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Testing MuZero Value Backpropagation\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def are_lists_roughly_equal(list1, list2):\n",
    "    \"\"\"\n",
    "    Checks if two lists of floats are roughly equal in Python 2.\n",
    "\n",
    "    Args:\n",
    "        list1: The first list of floats.\n",
    "        list2: The second list of floats.\n",
    "        tolerance: The maximum allowed absolute difference between corresponding\n",
    "                   elements for them to be considered roughly equal.\n",
    "\n",
    "    Returns:\n",
    "        True if the lists are roughly equal, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        # Compare elements using absolute tolerance\n",
    "        if not math.isclose(list1[i], list2[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "all_passed = True\n",
    "method1_correct = 0\n",
    "method2_correct = 0\n",
    "total_tests = 0\n",
    "for i, (path_config, expected, num_players, description) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest Case {i}: {description}\")\n",
    "    print(f\"Path config: {path_config}\")\n",
    "    print(f\"Num players: {num_players}\")\n",
    "    print(f\"Expected node values: {expected}\")\n",
    "\n",
    "    # Test Method 1\n",
    "    search_path_1, to_play_1, value_1 = make_search_path(path_config)\n",
    "    result_1 = backpropagate_method1(search_path_1, to_play_1, value_1, num_players)\n",
    "\n",
    "    # Test Method 2\n",
    "    search_path_2, to_play_2, value_2 = make_search_path(path_config)\n",
    "    result_2 = backpropagate_method2(search_path_2, to_play_2, value_2, num_players)\n",
    "\n",
    "    # Check results\n",
    "    method1_pass = are_lists_roughly_equal(result_1, expected)\n",
    "    method2_pass = are_lists_roughly_equal(result_2, expected)\n",
    "    match = are_lists_roughly_equal(result_1, result_2)\n",
    "\n",
    "    print(f\"Method 1 result: {result_1} {'' if method1_pass else ''}\")\n",
    "    print(f\"Method 2 result: {result_2} {'' if method2_pass else ''}\")\n",
    "    print(f\"Methods match: {'' if match else ''}\")\n",
    "    if method1_pass:\n",
    "        method1_correct += 1\n",
    "    if method2_pass:\n",
    "        method2_correct += 1\n",
    "    if not method1_pass:\n",
    "        print(\" METHOD 1 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 1 PASSED\")\n",
    "    if not method2_pass:\n",
    "        print(\" METHOD 2 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 2 PASSED\")\n",
    "\n",
    "    # if not (method1_pass and method2_pass and match):\n",
    "    #     all_passed = False\n",
    "    #     print(\" FAILED\")\n",
    "    # else:\n",
    "    #     print(\" PASSED\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_passed:\n",
    "    print(\" All tests passed!\")\n",
    "else:\n",
    "    print(\" Some tests failed\")\n",
    "\n",
    "print(\"Method 1 got\", method1_correct)\n",
    "print(\"Method 2 got\", method2_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "\n",
    "\n",
    "def make_search_path(path_config):\n",
    "    \"\"\"\n",
    "    path_config is a list of tuples: (to_play, reward)\n",
    "    Last element has the leaf value and to_play\n",
    "    \"\"\"\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "\n",
    "    # Initialize root (player 0)\n",
    "    root.expand(legal_moves, 0, policy, hidden_state, 0.0)\n",
    "\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "\n",
    "    # Build path according to config\n",
    "    for i, (to_play, reward) in enumerate(path_config[:-1]):\n",
    "        search_path.append(node)\n",
    "        node.expand(legal_moves, to_play, policy, hidden_state, reward)\n",
    "        node = node.children[0]\n",
    "\n",
    "    # Last node\n",
    "    search_path.append(node)\n",
    "    last_config = path_config[-1]\n",
    "    leaf_to_play = last_config[0]\n",
    "    leaf_reward = last_config[1]\n",
    "    leaf_value = last_config[2] if len(last_config) > 2 else 0.0\n",
    "    node.expand(legal_moves, leaf_to_play, policy, hidden_state, leaf_reward)\n",
    "\n",
    "    return search_path, leaf_to_play, leaf_value\n",
    "\n",
    "\n",
    "def backpropagate_method1(search_path, to_play, value, num_players, min_max_stats):\n",
    "    Node.value_prefix = False\n",
    "    Node.estimation_method = \"zero\"\n",
    "    Node.discount = 1.0\n",
    "    #     \"\"\"Original Method 1\"\"\"\n",
    "    #     for node in reversed(search_path):\n",
    "    #         node.value_sum += value if node.to_play == to_play else -value\n",
    "    #         node.visits += 1\n",
    "    #         min_max_stats.update(\n",
    "    #             node.reward\n",
    "    #             + 1.0\n",
    "    #             * (node.value(False, 1.0) if num_players == 1 else -node.value(False, 1.0))\n",
    "    #         )\n",
    "\n",
    "    #         if node.to_play == to_play and num_players > 1:\n",
    "    #             value = -node.reward + 1.0 * value\n",
    "    #         else:\n",
    "    #             value = node.reward + 1.0 * value\n",
    "\n",
    "    #     return [n.value(False, 1.0) for n in search_path], min_max_stats\n",
    "    n = len(search_path)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    # --- 1) Build per-player accumulator array acc[p] = Acc_p(i) for current i (starting from i = n-1) ---\n",
    "    # Acc_p(i) definition: discounted return from node i for a node whose player is p:\n",
    "    # Acc_p(i) = sum_{j=i+1..n-1} discount^{j-i-1} * sign(p, j) * reward_j\n",
    "    #            + discount^{n-1-i} * sign(p, leaf) * leaf_value\n",
    "    # Where sign(p, j) = +1 if acting_player_at_j (which is search_path[j-1].to_play) == p else -1.\n",
    "    #\n",
    "    # We compute Acc_p(n-1) = sign(p, leaf) * leaf_value as base, then iterate backward:\n",
    "    # Acc_p(i-1) = s(p, i) * reward_i + discount * Acc_p(i)\n",
    "\n",
    "    # Initialize acc for i = n-1 (base: discounted exponent 0 for leaf value)\n",
    "    # acc is a Python list of floats length num_players\n",
    "    acc = [0.0] * num_players\n",
    "    for p in range(num_players):\n",
    "        acc[p] = value if to_play == p else -value\n",
    "\n",
    "    # totals[i] will hold Acc_{node_player}(i)\n",
    "    totals = [0.0] * n\n",
    "    # Iterate from i = n-1 down to 0\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        node = search_path[i]\n",
    "        node_player = node.to_play\n",
    "        # totals for this node = acc[node_player] (current Acc_p(i))\n",
    "        # print(totals[i])\n",
    "        # print(acc[node_player])\n",
    "        totals[i] = acc[node_player]\n",
    "\n",
    "        node.value_sum += totals[i]\n",
    "        node.visits += 1\n",
    "\n",
    "        # Prepare acc for i-1 (if any)\n",
    "        if i > 0:\n",
    "            # reward at index i belongs to acting_player = search_path[i-1].to_play\n",
    "            r_i = search_path[i - 1].child_reward(search_path[i])\n",
    "            acting_player = search_path[i - 1].to_play\n",
    "\n",
    "            # # Update per-player accumulators in O(num_players)\n",
    "            # # Acc_p(i-1) = sign(p, i) * r_i + discount * Acc_p(i)\n",
    "            # # sign(p, i) = +1 if acting_player == p else -1\n",
    "            # # We overwrite acc[p] in-place to be Acc_p(i-1)\n",
    "            for p in range(num_players):\n",
    "                sign = 1.0 if acting_player == p else -1.0\n",
    "                acc[p] = sign * r_i + Node.discount * acc[p]\n",
    "                # child_q = search_path[i - 1].get_child_q_from_parent(search_path[i])\n",
    "                # acc[p] = min_max_stats.normalize(child_q)\n",
    "\n",
    "            child_q = search_path[i - 1].get_child_q_from_parent(search_path[i])\n",
    "            min_max_stats.update(child_q)\n",
    "        else:\n",
    "            min_max_stats.update(search_path[i].value())\n",
    "    return [n.value() for n in search_path], min_max_stats\n",
    "\n",
    "\n",
    "def backpropagate_method2(\n",
    "    search_path, leaf_to_play, leaf_value, num_players, min_max_stats\n",
    "):\n",
    "\n",
    "    n = len(search_path)\n",
    "\n",
    "    # 1) Compute exact total return for each node from that node's player perspective.\n",
    "    #    totals[i] is the scalar to add to search_path[i].value_sum.\n",
    "    totals = [0.0] * n\n",
    "    for i, node in enumerate(search_path):\n",
    "        total = 0.0\n",
    "        # Sum future rewards: reward at search_path[j] belongs to acting_player = search_path[j-1].to_play\n",
    "        for j in range(i + 1, n):\n",
    "            acting_player = search_path[j - 1].to_play\n",
    "            r = search_path[j].reward\n",
    "            total += r if acting_player == node.to_play else -r\n",
    "\n",
    "        # Add leaf value (owned by leaf_to_play)\n",
    "        total += leaf_value if leaf_to_play == node.to_play else -leaf_value\n",
    "        totals[i] = total\n",
    "\n",
    "    # 2) Apply updates in reverse order and update MinMaxStats using parent-perspective value.\n",
    "    #    For node at index i, its parent is search_path[i-1] (if i>0).\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        node = search_path[i]\n",
    "\n",
    "        # Update node stats (so node.value() reflects totals after update)\n",
    "        node.value_sum += totals[i]\n",
    "        node.visits += 1\n",
    "\n",
    "        # Compute the scalar used to update MinMaxStats: it must be the value of this child\n",
    "        # from its parent's perspective:\n",
    "        # parent_value_contrib = child.reward + discount * (sign * child.value())\n",
    "        # sign = +1 if child.to_play == parent.to_play (same player acts again), else -1.\n",
    "        if i > 0:\n",
    "            parent_to_play = search_path[i - 1].to_play\n",
    "            # For single-player games, child.value() is always added (no sign flip).\n",
    "            if num_players == 1:\n",
    "                sign = 1.0\n",
    "            else:\n",
    "                sign = 1.0 if node.to_play == parent_to_play else -1.0\n",
    "        else:\n",
    "            # For root (no parent) we must still pass something to MinMaxStats.\n",
    "            # Use sign = +1 (treat root as its own parent's perspective) for a consistent convention.\n",
    "            # This is harmless because root's parent doesn't exist  MinMaxStats is just tracking\n",
    "            # global min/max of these scalars for normalization.\n",
    "            sign = 1.0 if num_players == 1 else 1.0\n",
    "\n",
    "        parent_value_contrib = node.reward + 1.0 * (sign * node.value())\n",
    "        min_max_stats.update(parent_value_contrib)\n",
    "\n",
    "    return [n.value() for n in search_path], min_max_stats\n",
    "\n",
    "\n",
    "def backpropagate_method3(\n",
    "    search_path,  # list of nodes from root .. leaf\n",
    "    leaf_to_play,  # player id that owns `leaf_value`\n",
    "    leaf_value,  # scalar leaf value\n",
    "    num_players,\n",
    "    min_max_stats,\n",
    "):\n",
    "    \"\"\"\n",
    "    O(n) discounted backpropagation with correct sign handling for repeated same-player turns.\n",
    "    - search_path: list of Nodes, index 0 is root, last is leaf.\n",
    "      Each Node must expose: .reward (reward stored on the node),\n",
    "      .to_play (player id who acts at that node),\n",
    "      .value_sum, .visits, and .value() == value_sum/visits.\n",
    "    - leaf_to_play, leaf_value: ownership and scalar of the leaf bootstrap.\n",
    "    - num_players: number of players (1 for single-player).\n",
    "    - discount: gamma\n",
    "    - min_max_stats: MinMaxStats instance with .update(x) and .normalize(x)\n",
    "    Returns: list of node.value() after updates.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(search_path)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    # --- 1) Build per-player accumulator array acc[p] = Acc_p(i) for current i (starting from i = n-1) ---\n",
    "    # Acc_p(i) definition: discounted return from node i for a node whose player is p:\n",
    "    # Acc_p(i) = sum_{j=i+1..n-1} discount^{j-i-1} * sign(p, j) * reward_j\n",
    "    #            + discount^{n-1-i} * sign(p, leaf) * leaf_value\n",
    "    # Where sign(p, j) = +1 if acting_player_at_j (which is search_path[j-1].to_play) == p else -1.\n",
    "    #\n",
    "    # We compute Acc_p(n-1) = sign(p, leaf) * leaf_value as base, then iterate backward:\n",
    "    # Acc_p(i-1) = s(p, i) * reward_i + discount * Acc_p(i)\n",
    "\n",
    "    # Initialize acc for i = n-1 (base: discounted exponent 0 for leaf value)\n",
    "    # acc is a Python list of floats length num_players\n",
    "    acc = [0.0] * num_players\n",
    "    for p in range(num_players):\n",
    "        acc[p] = leaf_value if leaf_to_play == p else -leaf_value\n",
    "\n",
    "    # totals[i] will hold Acc_{node_player}(i)\n",
    "    totals = [0.0] * n\n",
    "\n",
    "    # Iterate from i = n-1 down to 0\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        node = search_path[i]\n",
    "        node_player = node.to_play\n",
    "\n",
    "        # totals for this node = acc[node_player] (current Acc_p(i))\n",
    "        totals[i] = acc[node_player]\n",
    "\n",
    "        node.value_sum += totals[i]\n",
    "        node.visits += 1\n",
    "\n",
    "        # Prepare acc for i-1 (if any)\n",
    "        if i > 0:\n",
    "            # reward at index i belongs to acting_player = search_path[i-1].to_play\n",
    "            r_i = search_path[i].reward\n",
    "            acting_player = search_path[i - 1].to_play\n",
    "\n",
    "            # Update per-player accumulators in O(num_players)\n",
    "            # Acc_p(i-1) = sign(p, i) * r_i + discount * Acc_p(i)\n",
    "            # sign(p, i) = +1 if acting_player == p else -1\n",
    "            # We overwrite acc[p] in-place to be Acc_p(i-1)\n",
    "            for p in range(num_players):\n",
    "                sign = 1.0 if acting_player == p else -1.0\n",
    "                acc[p] = sign * r_i + 1.0 * acc[p]\n",
    "\n",
    "        # apply computed discounted total for this node's player\n",
    "\n",
    "        # compute scalar that MinMaxStats expects for this child from its parent's perspective:\n",
    "        # parent_value_contrib = child.reward + discount * (sign * child.value())\n",
    "        # sign = +1 if single-player OR child.to_play == parent.to_play else -1\n",
    "        if i > 0:\n",
    "            parent = search_path[i - 1]\n",
    "            if num_players == 1:\n",
    "                sign = 1.0\n",
    "            else:\n",
    "                sign = 1.0 if node.to_play == parent.to_play else -1.0\n",
    "        else:\n",
    "            # root: choose sign = +1 convention (root has no parent)\n",
    "            sign = 1.0\n",
    "\n",
    "        parent_value_contrib = node.reward + 1.0 * (sign * node.value())\n",
    "        min_max_stats.update(parent_value_contrib)\n",
    "\n",
    "    # Return updated node values\n",
    "    return [node.value() for node in search_path], min_max_stats\n",
    "\n",
    "\n",
    "# Test cases: (path_config, expected_root_value, num_players, description)\n",
    "test_cases = [\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)],\n",
    "        [-2.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 1.0, 0.0)],\n",
    "        [1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: alternating game, player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        [-1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)],\n",
    "        [1.0, -1.0, 1.0, -1.0, 1.0],\n",
    "        2,\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 1.0), (0, 1.0, 0.0)],\n",
    "        [3.0, 2.0, 1.0, 0.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)],\n",
    "        [4.0, 4.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: All player 0 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)],\n",
    "        [-5.0, 5.0, 4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)],\n",
    "        [3.0, -3.0, -4.0, -4.0, 4.0],\n",
    "        2,\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    # Single player test cases\n",
    "    (\n",
    "        [(0, 1.0), (0, 2.0), (0, 3.0, 0.0)],\n",
    "        [6.0, 5.0, 3.0, 0.0],\n",
    "        1,\n",
    "        \"1-player: All rewards sum up\",\n",
    "    ),\n",
    "    (\n",
    "        [(0, 1.0), (0, 0.0), (0, 0.0, 5.0)],\n",
    "        [6.0, 5.0, 5.0, 5.0],\n",
    "        1,\n",
    "        \"1-player: Rewards + leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 1.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, -1.0, 1.0, 0.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 2 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (2, 0.0), (0, 0.0), (0, 0.0, 0.0)],\n",
    "        [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 1.0), (0, 0.0), (0, 0.0, 0.0)],\n",
    "        [-1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 0.0), (0, 1.0, 0.0)],\n",
    "        [1.0, -1.0, -1.0, 1.0, 0.0],\n",
    "        3,\n",
    "        \"3-player: Player 0 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        [-1.0, 1.0, -1.0, -1.0, 1.0],\n",
    "        3,\n",
    "        \"3-player: player 1 ends with a value prediction\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Testing MuZero Value Backpropagation\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def are_lists_roughly_equal(list1, list2):\n",
    "    \"\"\"\n",
    "    Checks if two lists of floats are roughly equal in Python 2.\n",
    "\n",
    "    Args:\n",
    "        list1: The first list of floats.\n",
    "        list2: The second list of floats.\n",
    "        tolerance: The maximum allowed absolute difference between corresponding\n",
    "                   elements for them to be considered roughly equal.\n",
    "\n",
    "    Returns:\n",
    "        True if the lists are roughly equal, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        # Compare elements using absolute tolerance\n",
    "        if not math.isclose(list1[i], list2[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "all_passed = True\n",
    "method1_correct = 0\n",
    "method2_correct = 0\n",
    "method3_correct = 0\n",
    "for i, (path_config, expected, num_players, description) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest Case {i}: {description}\")\n",
    "    print(f\"Path config: {path_config}\")\n",
    "    print(f\"Num players: {num_players}\")\n",
    "    print(f\"Expected node values: {expected}\")\n",
    "\n",
    "    # Test Method 1\n",
    "    search_path_1, to_play_1, value_1 = make_search_path(path_config)\n",
    "    result_1, min_max_stats_1 = backpropagate_method1(\n",
    "        search_path_1,\n",
    "        to_play_1,\n",
    "        value_1,\n",
    "        num_players,\n",
    "        min_max_stats=MinMaxStats(known_bounds=[-1, 1]),\n",
    "    )\n",
    "\n",
    "    # Test Method 2\n",
    "    search_path_2, to_play_2, value_2 = make_search_path(path_config)\n",
    "    result_2, min_max_stats_2 = backpropagate_method2(\n",
    "        search_path_2,\n",
    "        to_play_2,\n",
    "        value_2,\n",
    "        num_players,\n",
    "        min_max_stats=MinMaxStats(known_bounds=[-1, 1]),\n",
    "    )\n",
    "\n",
    "    # Test Method 3\n",
    "    search_path_3, to_play_3, value_3 = make_search_path(path_config)\n",
    "    result_3, min_max_stats_3 = backpropagate_method3(\n",
    "        search_path_3,\n",
    "        to_play_3,\n",
    "        value_3,\n",
    "        num_players,\n",
    "        min_max_stats=MinMaxStats(known_bounds=[-1, 1]),\n",
    "    )\n",
    "\n",
    "    # Check results\n",
    "    method1_pass = are_lists_roughly_equal(result_1, expected)\n",
    "    method2_pass = are_lists_roughly_equal(result_2, expected)\n",
    "    method3_pass = are_lists_roughly_equal(result_3, expected)\n",
    "    match1 = are_lists_roughly_equal(result_1, result_2)\n",
    "    match2 = are_lists_roughly_equal(result_2, result_3)\n",
    "    match3 = are_lists_roughly_equal(result_1, result_3)\n",
    "\n",
    "    print(f\"Method 1 result: {result_1} {'' if method1_pass else ''}\")\n",
    "    print(f\"Method 2 result: {result_2} {'' if method2_pass else ''}\")\n",
    "    print(f\"Method 3 result: {result_3} {'' if method3_pass else ''}\")\n",
    "    print(f\"Methods 1 and 2 match: {'' if match1 else ''}\")\n",
    "    print(f\"Methods 2 and 3 match: {'' if match2 else ''}\")\n",
    "    print(f\"Methods 1 and 3 match: {'' if match3 else ''}\")\n",
    "    print(\n",
    "        f\"MinMaxStats maxes match:  {'' if min_max_stats_1.max == min_max_stats_2.max else ''} {min_max_stats_1.max} = {min_max_stats_2.max}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats mins match:  {'' if min_max_stats_1.min == min_max_stats_2.min else ''} {min_max_stats_1.min} = {min_max_stats_2.min}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats maxes match:  {'' if min_max_stats_2.max == min_max_stats_3.max else ''} {min_max_stats_2.max} = {min_max_stats_3.max}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats mins match:  {'' if min_max_stats_2.min == min_max_stats_3.min else ''} {min_max_stats_2.min} = {min_max_stats_3.min}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats maxes match:  {'' if min_max_stats_1.max == min_max_stats_3.max else ''} {min_max_stats_1.max} = {min_max_stats_3.max}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MinMaxStats mins match:  {'' if min_max_stats_1.min == min_max_stats_3.min else ''} {min_max_stats_1.min} = {min_max_stats_3.min}\"\n",
    "    )\n",
    "    if method1_pass:\n",
    "        method1_correct += 1\n",
    "    if method2_pass:\n",
    "        method2_correct += 1\n",
    "    if method3_pass:\n",
    "        method3_correct += 1\n",
    "    if not method1_pass:\n",
    "        print(\" METHOD 1 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 1 PASSED\")\n",
    "    if not method2_pass:\n",
    "        print(\" METHOD 2 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 2 PASSED\")\n",
    "    if not method3_pass:\n",
    "        print(\" METHOD 3 FAILED\")\n",
    "    else:\n",
    "        print(\" METHOD 3 PASSED\")\n",
    "\n",
    "    # if not (method1_pass and method2_pass and match):\n",
    "    #     all_passed = False\n",
    "    #     print(\" FAILED\")\n",
    "    # else:\n",
    "    #     print(\" PASSED\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_passed:\n",
    "    print(\" All tests passed!\")\n",
    "else:\n",
    "    print(\" Some tests failed\")\n",
    "\n",
    "print(f\"Method 1 got {method1_correct}/{len(test_cases)}\")\n",
    "print(f\"Method 2 got {method2_correct}/{len(test_cases)}\")\n",
    "print(f\"Method 3 got {method3_correct}/{len(test_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_fn(\n",
    "    index: int,\n",
    "    values: list,\n",
    "    policies: list,\n",
    "    rewards: list,\n",
    "    actions: list,\n",
    "    infos: list,\n",
    "    num_unroll_steps: int,\n",
    "    n_step: int,\n",
    "):\n",
    "    n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_policies = torch.zeros(\n",
    "        (num_unroll_steps + 1, num_actions), dtype=torch.float32\n",
    "    )\n",
    "    n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.int16)\n",
    "    for current_index in range(index, index + num_unroll_steps + 1):\n",
    "        unroll_step = current_index - index\n",
    "        bootstrap_index = current_index + n_step\n",
    "        # print(\"bootstrapping\")\n",
    "        # value of current position is the value at the position n_steps away + rewards to get to the n_step position\n",
    "        if bootstrap_index < len(values):\n",
    "            if (\n",
    "                \"player\" not in infos[current_index]\n",
    "                or infos[current_index][\"player\"] == infos[bootstrap_index][\"player\"]\n",
    "            ):\n",
    "                value = values[bootstrap_index] * gamma**n_step\n",
    "            else:\n",
    "                value = -values[bootstrap_index] * gamma**n_step\n",
    "        else:\n",
    "            value = 0\n",
    "\n",
    "        # the rewards at this index to the bootstrap index should be added to the value\n",
    "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
    "            # WHAT IS current_index + i + 1 when current index is the last frame?? IS THIS AN ERROR?\n",
    "            if (\n",
    "                \"player\" not in infos[current_index]\n",
    "                or infos[current_index][\"player\"]\n",
    "                == infos[current_index + i][\n",
    "                    \"player\"\n",
    "                ]  # + 1 if doing my og thing and i want to go back\n",
    "            ):\n",
    "                value += reward * gamma**i\n",
    "            else:\n",
    "                value -= reward * gamma**i\n",
    "\n",
    "        # target reward is the reward before the ones added to the value\n",
    "        if current_index > 0 and current_index <= len(rewards):\n",
    "            last_reward = rewards[current_index - 1]\n",
    "            # if self.has_intermediate_rewards:\n",
    "            #     last_reward = rewards[current_index - 1]\n",
    "            # else:\n",
    "            #     value += (\n",
    "            #         rewards[current_index - 1]\n",
    "            #         if infos[current_index][\"player\"]\n",
    "            #         == infos[current_index - 1][\"player\"]\n",
    "            #         else -rewards[current_index - 1]\n",
    "            #     )\n",
    "            #     last_reward = rewards[current_index - 1]  # reward not used\n",
    "        else:\n",
    "            last_reward = 0  # self absorbing state 0 reward\n",
    "\n",
    "        if current_index < len(values):\n",
    "            n_step_values[unroll_step] = value\n",
    "            n_step_rewards[unroll_step] = last_reward\n",
    "            n_step_policies[unroll_step] = policies[current_index]\n",
    "            if unroll_step < num_unroll_steps:\n",
    "                # no action for last unroll step (since you dont act on that state)\n",
    "                n_step_actions[unroll_step] = actions[current_index]\n",
    "        else:\n",
    "            n_step_values[unroll_step] = (\n",
    "                value  # should be value or 0, maybe broken for single player\n",
    "            )\n",
    "            n_step_rewards[unroll_step] = last_reward\n",
    "            n_step_policies[unroll_step] = (\n",
    "                torch.ones(num_actions) / num_actions\n",
    "            )  # self absorbing state\n",
    "            if unroll_step < num_unroll_steps:\n",
    "                # no action for last unroll step (since you dont act on that state)\n",
    "                n_step_actions[unroll_step] = -1  # self absorbing state\n",
    "\n",
    "    return (\n",
    "        n_step_values,  # [initial value, recurrent values]\n",
    "        n_step_policies,  # [initial policy, recurrent policies]\n",
    "        n_step_rewards,  # [initial reward (0), recurrent rewards] initial reward is useless like the first last action, but we ignore it in the learn function\n",
    "        n_step_actions,  # [recurrent actions, extra action]\n",
    "    )  # remove the last actions, as there should be one less action than other stuff\n",
    "\n",
    "\n",
    "def new_fn(\n",
    "    index: int,\n",
    "    values: list,\n",
    "    policies: list,\n",
    "    rewards: list,\n",
    "    actions: list,\n",
    "    infos: list,\n",
    "    num_unroll_steps: int,\n",
    "    n_step: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        n_step_values: tensor shape (num_unroll_steps+1,)\n",
    "        n_step_policies: tensor shape (num_unroll_steps+1, num_actions)\n",
    "        n_step_rewards: tensor shape (num_unroll_steps+1,)\n",
    "        n_step_actions: tensor shape (num_unroll_steps,)\n",
    "    Conventions:\n",
    "        - rewards[t] is the reward from taking action at state t (transition t  t+1)\n",
    "        - infos[t][\"player\"] is the player who acted at state t\n",
    "        - n_step_rewards[0] = 0 (no reward leading into root)\n",
    "    \"\"\"\n",
    "    n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    n_step_policies = torch.zeros(\n",
    "        (num_unroll_steps + 1, num_actions), dtype=torch.float32\n",
    "    )\n",
    "    n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.int16)\n",
    "\n",
    "    max_index = len(values)\n",
    "\n",
    "    for u in range(0, num_unroll_steps + 1):\n",
    "        current_index = index + u\n",
    "\n",
    "        # 1. discounted n-step value from current_index\n",
    "        value = 0.0\n",
    "        for k in range(n_step):\n",
    "            r_idx = current_index + k\n",
    "            if r_idx < len(rewards):\n",
    "                r = rewards[r_idx]\n",
    "                node_player = (\n",
    "                    infos[current_index].get(\"player\", None)\n",
    "                    if current_index < len(infos)\n",
    "                    else None\n",
    "                )\n",
    "                acting_player = (\n",
    "                    infos[r_idx].get(\"player\", None) if r_idx < len(infos) else None\n",
    "                )\n",
    "                sign = (\n",
    "                    1.0\n",
    "                    if (\n",
    "                        node_player is None\n",
    "                        or acting_player is None\n",
    "                        or node_player == acting_player\n",
    "                    )\n",
    "                    else -1.0\n",
    "                )\n",
    "                value += (gamma**k) * (sign * r)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        boot_idx = current_index + n_step\n",
    "        if boot_idx < len(values):\n",
    "            v_boot = values[boot_idx]\n",
    "            node_player = (\n",
    "                infos[current_index].get(\"player\", None)\n",
    "                if current_index < len(infos)\n",
    "                else None\n",
    "            )\n",
    "            boot_player = (\n",
    "                infos[boot_idx].get(\"player\", None) if boot_idx < len(infos) else None\n",
    "            )\n",
    "            sign_leaf = (\n",
    "                1.0\n",
    "                if (\n",
    "                    node_player is None\n",
    "                    or boot_player is None\n",
    "                    or node_player == boot_player\n",
    "                )\n",
    "                else -1.0\n",
    "            )\n",
    "            value += (gamma**n_step) * (sign_leaf * v_boot)\n",
    "\n",
    "        n_step_values[u] = value\n",
    "\n",
    "        # 2. reward target\n",
    "        if u == 0:\n",
    "            n_step_rewards[u] = 0.0  # root has no preceding reward\n",
    "        else:\n",
    "            reward_idx = current_index - 1\n",
    "            n_step_rewards[u] = (\n",
    "                rewards[reward_idx] if reward_idx < len(rewards) else 0.0\n",
    "            )\n",
    "\n",
    "        # 3. policy\n",
    "        if current_index < len(policies):\n",
    "            n_step_policies[u] = policies[current_index]\n",
    "        else:\n",
    "            n_step_policies[u] = torch.ones(num_actions) / num_actions\n",
    "\n",
    "        # 4. action\n",
    "        if u < num_unroll_steps:\n",
    "            n_step_actions[u] = (\n",
    "                actions[current_index] if current_index < len(actions) else -1\n",
    "            )\n",
    "\n",
    "    return n_step_values, n_step_policies, n_step_rewards, n_step_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "def compare_get_n_step_info(\n",
    "    old_fn,\n",
    "    new_fn,\n",
    "    *,\n",
    "    index,\n",
    "    values,\n",
    "    policies,\n",
    "    rewards,\n",
    "    actions,\n",
    "    infos,\n",
    "    num_unroll_steps,\n",
    "    n_step,\n",
    "    num_actions,\n",
    "    gamma=0.997,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares old and new _get_n_step_info outputs and checks correctness.\n",
    "    Arguments:\n",
    "        old_fn: callable implementing the old logic\n",
    "        new_fn: callable implementing the new logic\n",
    "        (both should take arguments in same order as MuZeroReplayBuffer._get_n_step_info)\n",
    "    \"\"\"\n",
    "\n",
    "    # Run both implementations\n",
    "    old_vals, old_pols, old_rews, old_acts = old_fn(\n",
    "        index, values, policies, rewards, actions, infos, num_unroll_steps, n_step\n",
    "    )\n",
    "\n",
    "    new_vals, new_pols, new_rews, new_acts = new_fn(\n",
    "        index, values, policies, rewards, actions, infos, num_unroll_steps, n_step\n",
    "    )\n",
    "\n",
    "    # --- compute expected mathematically correct discounted values ---\n",
    "    def expected_nstep_value(t):\n",
    "        \"\"\"Return correct discounted n-step bootstrap value from index t\"\"\"\n",
    "        v = 0.0\n",
    "        for k in range(n_step):\n",
    "            idx = t + k\n",
    "            if idx >= len(rewards):\n",
    "                break\n",
    "            # who acted for reward idx\n",
    "            r_player = infos[idx][\"player\"]\n",
    "            node_player = infos[t][\"player\"]\n",
    "            sign = 1 if r_player == node_player else -1\n",
    "            v += (gamma**k) * (sign * rewards[idx])\n",
    "        boot_idx = t + n_step\n",
    "        if boot_idx < len(values):\n",
    "            node_player = infos[t][\"player\"]\n",
    "            leaf_player = infos[boot_idx][\"player\"]\n",
    "            sign_leaf = 1 if leaf_player == node_player else -1\n",
    "            v += (gamma**n_step) * (sign_leaf * values[boot_idx])\n",
    "        return v\n",
    "\n",
    "    expected_vals = torch.tensor(\n",
    "        [expected_nstep_value(index + u) for u in range(num_unroll_steps + 1)],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    # --- compare ---\n",
    "    def close(a, b, tol=1e-5):\n",
    "        return torch.allclose(a, b, atol=tol, rtol=tol)\n",
    "\n",
    "    ok_vals = close(new_vals, expected_vals)\n",
    "    ok_rews = close(\n",
    "        new_rews,\n",
    "        torch.tensor(\n",
    "            [\n",
    "                rewards[index + u] if index + u < len(rewards) else 0\n",
    "                for u in range(num_unroll_steps + 1)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    ok_acts = close(\n",
    "        new_acts,\n",
    "        torch.tensor(\n",
    "            [\n",
    "                actions[index + u] if index + u < len(actions) else -1\n",
    "                for u in range(num_unroll_steps)\n",
    "            ],\n",
    "            dtype=torch.int16,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # --- print diagnostics ---\n",
    "    if verbose:\n",
    "        print(\"====== N-STEP COMPARISON ======\")\n",
    "        for name, old, new in zip(\n",
    "            [\"values\", \"rewards\", \"actions\"],\n",
    "            [old_vals, old_rews, old_acts],\n",
    "            [new_vals, new_rews, new_acts],\n",
    "        ):\n",
    "            print(f\"\\n{name.upper()}:\")\n",
    "            print(f\" old: {old.tolist()}\")\n",
    "            print(f\" new: {new.tolist()}\")\n",
    "\n",
    "        print(\"\\nEXPECTED CORRECT VALUES:\", expected_vals.tolist())\n",
    "        print(\"\\nChecks:\")\n",
    "        print(f\"  New matches expected values: {ok_vals}\")\n",
    "        print(f\"  Immediate rewards correct:   {ok_rews}\")\n",
    "        print(f\"  Actions aligned:             {ok_acts}\")\n",
    "\n",
    "        # Highlight mismatches\n",
    "        if not ok_vals:\n",
    "            diffs = (new_vals - expected_vals).abs()\n",
    "            print(\"  Value diffs:\", diffs.tolist())\n",
    "        if not close(new_vals, old_vals):\n",
    "            print(\n",
    "                \"  Old and new disagree on n-step values (expected if old lacked proper discounting or sign logic).\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"ok_values\": ok_vals,\n",
    "        \"ok_rewards\": ok_rews,\n",
    "        \"ok_actions\": ok_acts,\n",
    "        \"old_vals\": old_vals,\n",
    "        \"new_vals\": new_vals,\n",
    "        \"expected_vals\": expected_vals,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example episode\n",
    "values = [0.2, -0.1, 0.4, 0.5]\n",
    "rewards = [1.0, -2.0, 3.0]\n",
    "actions = [0, 1, 2]\n",
    "policies = [\n",
    "    torch.tensor([0.7, 0.3]),\n",
    "    torch.tensor([0.2, 0.8]),\n",
    "    torch.tensor([0.5, 0.5]),\n",
    "    torch.tensor([0.5, 0.5]),\n",
    "]\n",
    "infos = [\n",
    "    {\"player\": 0},\n",
    "    {\"player\": 1},\n",
    "    {\"player\": 0},\n",
    "    {\"player\": 1},\n",
    "]\n",
    "index = 0\n",
    "num_unroll_steps = 2\n",
    "n_step = 2\n",
    "num_actions = 2\n",
    "gamma = 0.99\n",
    "\n",
    "results = compare_get_n_step_info(\n",
    "    old_fn=old_fn,\n",
    "    new_fn=new_fn,\n",
    "    index=index,\n",
    "    values=values,\n",
    "    policies=policies,\n",
    "    rewards=rewards,\n",
    "    actions=actions,\n",
    "    infos=infos,\n",
    "    num_unroll_steps=num_unroll_steps,\n",
    "    n_step=n_step,\n",
    "    num_actions=num_actions,\n",
    "    gamma=0.997,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.muzero_mcts import Node\n",
    "from muzero.muzero_minmax_stats import MinMaxStats\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_v_mix_with_some_visited_children():\n",
    "    \"\"\"\n",
    "    Tree:\n",
    "      root (visits > 0)\n",
    "        children actions 0,1,2\n",
    "        - action 0: visits=10, value_sum -> value = vs0/10\n",
    "        - action 1: visits=5,  value_sum -> value = vs1/5\n",
    "        - action 2: visits=0  (unvisited)\n",
    "\n",
    "    Compute expected v_mix manually and compare to get_v_mix.\n",
    "    \"\"\"\n",
    "    # network policy for 3 actions (sums to 1.0)\n",
    "    net_pol = [0.6, 0.3, 0.1]\n",
    "\n",
    "    # create root\n",
    "    root = Node(visits=4, value_sum=4.0, network_policy=net_pol, discount=0.9)\n",
    "\n",
    "    # create children and attach to root\n",
    "    # child 0: visited\n",
    "    child0 = Node(\n",
    "        action_idx=0,\n",
    "        visits=10,\n",
    "        value_sum=30.0,\n",
    "        reward=1.0,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "    # child 1: visited\n",
    "    child1 = Node(\n",
    "        visits=5,\n",
    "        value_sum=10.0,\n",
    "        reward=0.5,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "    # child 2: unvisited\n",
    "    child2 = Node(\n",
    "        visits=0,\n",
    "        value_sum=0.0,\n",
    "        reward=0.0,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "\n",
    "    root.children = {0: child0, 1: child1, 2: child2}\n",
    "\n",
    "    # Sanity: child values\n",
    "    v0 = child0.value()  # 30 / 10 = 3.0\n",
    "    v1 = child1.value()  # 10 / 5 = 2.0\n",
    "    assert pytest.approx(v0) == 3.0\n",
    "    assert pytest.approx(v1) == 2.0\n",
    "\n",
    "    # compute q(a) for visited actions (r + discount * v)\n",
    "    q0 = child0.reward + root.discount * v0  # 1.0 + 0.9*3.0 = 3.7\n",
    "    q1 = child1.reward + root.discount * v1  # 0.5 + 0.9*2.0 = 2.3\n",
    "\n",
    "    # expected_q_vis = sum(network_policy[a] * q(a)) but q(a)=0 for unvisited actions\n",
    "    expected_q_vis = net_pol[0] * q0 + net_pol[1] * q1 + net_pol[2] * 0.0\n",
    "\n",
    "    # p_vis_sum is the pi mass on visited actions\n",
    "    p_vis_sum = net_pol[0] + net_pol[1]  # 0.6 + 0.3 = 0.9\n",
    "\n",
    "    sum_N = float(child0.visits + child1.visits + child2.visits)  # 10 + 5 + 0 = 15.0\n",
    "\n",
    "    term = sum_N * (expected_q_vis / p_vis_sum)\n",
    "\n",
    "    # root.value() = value_sum / visits = 4.0 / 4 = 1.0\n",
    "    expected_vmix = (root.value() + term) / (1.0 + sum_N)\n",
    "\n",
    "    # compare to implementation\n",
    "    computed_vmix = root.get_v_mix()\n",
    "\n",
    "    assert pytest.approx(computed_vmix, rel=1e-6) == expected_vmix\n",
    "\n",
    "\n",
    "def test_v_mix_with_no_visits_returns_root_value():\n",
    "    \"\"\"\n",
    "    When sum_N == 0 the code should set term = 0 and v_mix == self.value()\n",
    "    Make a root with no visited children so sum_N == 0.\n",
    "    \"\"\"\n",
    "    net_pol = [0.5, 0.5]\n",
    "    root = Node(\n",
    "        action_idx=None, visits=2, value_sum=6.0, network_policy=net_pol, discount=0.9\n",
    "    )\n",
    "\n",
    "    child0 = Node(\n",
    "        action_idx=0,\n",
    "        visits=0,\n",
    "        value_sum=0.0,\n",
    "        reward=0.0,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "    child1 = Node(\n",
    "        action_idx=1,\n",
    "        visits=0,\n",
    "        value_sum=0.0,\n",
    "        reward=0.0,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "    root.children = {0: child0, 1: child1}\n",
    "\n",
    "    # sum_N == 0, so v_mix should equal root.value()\n",
    "    assert float(root.get_v_mix()) == pytest.approx(root.value())\n",
    "\n",
    "\n",
    "def test_v_mix_edge_case_single_visited_action():\n",
    "    \"\"\"\n",
    "    If only one child visited, ensure p_vis_sum uses only that visited action and math is correct.\n",
    "    \"\"\"\n",
    "    net_pol = [0.2, 0.8, 0.0]\n",
    "    root = Node(\n",
    "        action_idx=None, visits=1, value_sum=2.0, network_policy=net_pol, discount=0.5\n",
    "    )\n",
    "\n",
    "    # child 0 unvisited\n",
    "    c0 = Node(\n",
    "        action_idx=0,\n",
    "        visits=0,\n",
    "        value_sum=0.0,\n",
    "        reward=0.0,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "    # child 1 visited\n",
    "    c1 = Node(\n",
    "        action_idx=1,\n",
    "        visits=4,\n",
    "        value_sum=8.0,\n",
    "        reward=1.0,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "    # child 2 unvisited\n",
    "    c2 = Node(\n",
    "        action_idx=2,\n",
    "        visits=0,\n",
    "        value_sum=0.0,\n",
    "        reward=0.3,\n",
    "        to_play=root.to_play,\n",
    "        parent=root,\n",
    "    )\n",
    "\n",
    "    root.children = {0: c0, 1: c1, 2: c2}\n",
    "\n",
    "    # compute by hand\n",
    "    v1 = c1.value()  # 8/4 = 2.0\n",
    "    q1 = c1.reward + root.discount * v1  # 1.0 + 0.5*2.0 = 2.0\n",
    "    expected_q_vis = net_pol[1] * q1  # only visited action contributes\n",
    "    p_vis_sum = net_pol[1]  # 0.8\n",
    "    sum_N = float(c0.visits + c1.visits + c2.visits)  # 4.0\n",
    "\n",
    "    term = sum_N * (expected_q_vis / p_vis_sum)\n",
    "    expected_vmix = (root.value() + term) / (1.0 + sum_N)\n",
    "\n",
    "    assert pytest.approx(root.get_v_mix(), rel=1e-6) == expected_vmix\n",
    "\n",
    "\n",
    "test_v_mix_with_some_visited_children()\n",
    "test_v_mix_with_no_visits_returns_root_value()\n",
    "test_v_mix_edge_case_single_visited_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/wwwbwc016yxfhlbgfph7_xmc0000gn/T/ipykernel_74022/1391708388.py:442: RuntimeWarning: divide by zero encountered in log\n",
      "  logits = np.log(np.asarray(net_policy))\n"
     ]
    }
   ],
   "source": [
    "# tests/test_vmix_completedq_improved_policy_sigma_b.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytest\n",
    "from math import log, sqrt\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# NodeA: original Torch-based Node (kept faithful to your provided class)\n",
    "# -------------------------\n",
    "class NodeA:\n",
    "    estimation_method = None\n",
    "    discount = None\n",
    "    value_prefix = None\n",
    "    pb_c_init = None\n",
    "    pb_c_base = None\n",
    "    gumbel = None\n",
    "    cvisit = None\n",
    "    cscale = None\n",
    "\n",
    "    def __init__(self, prior_policy, parent=None):\n",
    "        self.visits = 0\n",
    "        self.to_play = -1\n",
    "        self.prior_policy = prior_policy\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward_h_state = None\n",
    "        self.reward_c_state = None\n",
    "\n",
    "        self.reward = 0\n",
    "        self.parent = parent\n",
    "\n",
    "        self.root_score = None\n",
    "        self.network_policy = None  # dense policy vector (numpy or torch)\n",
    "        self.network_value = None  # network scalar value estimate (float)\n",
    "        self.is_reset = True\n",
    "\n",
    "    def expand(\n",
    "        self,\n",
    "        allowed_actions,\n",
    "        to_play,\n",
    "        policy,\n",
    "        hidden_state,\n",
    "        reward,\n",
    "        value=None,\n",
    "        reward_h_state=None,\n",
    "        reward_c_state=None,\n",
    "        is_reset=True,\n",
    "    ):\n",
    "        self.to_play = to_play\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        self.reward_h_state = reward_h_state\n",
    "        self.reward_c_state = reward_c_state\n",
    "        self.is_reset = is_reset\n",
    "\n",
    "        self.network_policy = (\n",
    "            policy.detach().cpu()\n",
    "            if isinstance(policy, torch.Tensor)\n",
    "            else torch.tensor(policy)\n",
    "        )\n",
    "        self.network_value = value\n",
    "        allowed_policy = {a: policy[a] for a in allowed_actions}\n",
    "        allowed_policy_sum = sum(allowed_policy.values())\n",
    "\n",
    "        for action, p in allowed_policy.items():\n",
    "            self.children[action] = NodeA(\n",
    "                (p / (allowed_policy_sum + 1e-10)).item(), self\n",
    "            )\n",
    "\n",
    "    def expanded(self):\n",
    "        assert (len(self.children) > 0) == (self.visits > 0)\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visits == 0:\n",
    "            if self.estimation_method == \"v_mix\":\n",
    "                value = self.parent.get_v_mix()\n",
    "            elif self.estimation_method == \"mcts_value\":\n",
    "                value = self.parent.value()\n",
    "            elif self.estimation_method == \"network_value\":\n",
    "                value = self.parent.network_value\n",
    "            else:\n",
    "                value = 0.0\n",
    "        else:\n",
    "            value = self.value_sum / self.visits\n",
    "        assert value is not None\n",
    "        return value\n",
    "\n",
    "    def child_reward(self, child):\n",
    "        if self.value_prefix:\n",
    "            if child.is_reset:\n",
    "                return child.reward\n",
    "            else:\n",
    "                return child.reward - self.reward\n",
    "        else:\n",
    "            true_reward = child.reward\n",
    "\n",
    "        assert true_reward is not None\n",
    "        return true_reward\n",
    "\n",
    "    def add_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior_policy = (1 - frac) * self.children[\n",
    "                a\n",
    "            ].prior_policy + frac * n\n",
    "\n",
    "    def select_child(\n",
    "        self,\n",
    "        min_max_stats,\n",
    "        allowed_actions=None,\n",
    "    ):\n",
    "        assert self.expanded(), \"node must be expanded to select a child\"\n",
    "        actions = list(self.children.keys())\n",
    "        if allowed_actions is not None and self.parent is None:\n",
    "            actions = [a for a in allowed_actions]\n",
    "\n",
    "        if len(actions) == 1:\n",
    "            return actions[0], self.children[actions[0]]\n",
    "\n",
    "        if self.gumbel and self.parent != None:\n",
    "            pi0 = self.get_gumbel_improved_policy(\n",
    "                min_max_stats=min_max_stats,\n",
    "            )\n",
    "            visits = torch.tensor([float(self.children[a].visits) for a in actions])\n",
    "            sum_N = float(visits.sum())\n",
    "            denom = 1.0 + sum_N\n",
    "            selection_scores = pi0[actions] - (visits / denom)\n",
    "            max_score = float(selection_scores.max())\n",
    "            candidate_indices = np.where(np.isclose(selection_scores, max_score))[0]\n",
    "            chosen_idx = np.random.choice(candidate_indices)\n",
    "            action = actions[int(chosen_idx)]\n",
    "        else:\n",
    "            child_ucbs = [\n",
    "                self.child_uct_score(\n",
    "                    self.children[action],\n",
    "                    min_max_stats,\n",
    "                )\n",
    "                for action in actions\n",
    "            ]\n",
    "            action_index = np.random.choice(\n",
    "                np.where(np.isclose(child_ucbs, max(child_ucbs)))[0]\n",
    "            )\n",
    "            action = list(actions)[action_index]\n",
    "        return action, self.children[action]\n",
    "\n",
    "    def child_uct_score(\n",
    "        self,\n",
    "        child,\n",
    "        min_max_stats,\n",
    "    ):\n",
    "\n",
    "        pb_c = log((self.visits + self.pb_c_base + 1) / self.pb_c_base) + self.pb_c_init\n",
    "        pb_c *= sqrt(self.visits) / (child.visits + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior_policy\n",
    "        if child.expanded():\n",
    "            value_score = min_max_stats.normalize(self.get_child_q_from_parent(child))\n",
    "        else:\n",
    "            value_score = min_max_stats.normalize(self.value())\n",
    "\n",
    "        assert value_score == value_score, \"value_score is nan\"\n",
    "        assert prior_score == prior_score, \"prior_score is nan\"\n",
    "        return prior_score + value_score\n",
    "\n",
    "    def get_v_mix(self):\n",
    "        visits = torch.tensor(\n",
    "            [float(child.visits) for (action, child) in self.children.items()]\n",
    "        )\n",
    "        visited_actions = [\n",
    "            action for action, child in self.children.items() if child.expanded()\n",
    "        ]\n",
    "        sum_N = float(visits.sum())\n",
    "\n",
    "        q_vals = torch.zeros(len(self.network_policy))\n",
    "        if sum_N > 0:\n",
    "            for action, child in self.children.items():\n",
    "                if child.expanded():\n",
    "                    q_vals[action] = self.get_child_q_from_parent(child)\n",
    "\n",
    "            p_vis_sum = float(self.network_policy[visited_actions].sum())\n",
    "            expected_q_vis = float((self.network_policy * q_vals).sum())\n",
    "            term = sum_N * (expected_q_vis / p_vis_sum)\n",
    "        else:\n",
    "            term = 0.0\n",
    "        v_mix = (self.value() + term) / (1.0 + sum_N)\n",
    "        assert v_mix is not None\n",
    "        return v_mix\n",
    "\n",
    "    def get_completed_q(self, min_max_stats):\n",
    "        v_mix = self.get_v_mix()\n",
    "        completedQ = torch.full(\n",
    "            (len(self.network_policy),), min_max_stats.normalize(v_mix)\n",
    "        )\n",
    "        for action, child in self.children.items():\n",
    "            if child.expanded():\n",
    "                completedQ[action] = min_max_stats.normalize(\n",
    "                    self.get_child_q_from_parent(child)\n",
    "                )\n",
    "\n",
    "        return completedQ\n",
    "\n",
    "    def get_gumbel_improved_policy(\n",
    "        self,\n",
    "        min_max_stats,\n",
    "    ):\n",
    "        completedQ = self.get_completed_q(min_max_stats)\n",
    "\n",
    "        max_visits = (\n",
    "            max([ch.visits for ch in self.children.values()])\n",
    "            if len(self.children) > 0\n",
    "            else 0\n",
    "        )\n",
    "        sigma = (self.cvisit + max_visits) * self.cscale * completedQ\n",
    "\n",
    "        eps = 1e-12\n",
    "        logits = torch.log(self.network_policy + eps)\n",
    "\n",
    "        pi0_logits = logits + sigma\n",
    "        pi0 = torch.softmax(pi0_logits, dim=0)\n",
    "        return pi0\n",
    "\n",
    "    def get_gumbel_root_child_score(self, child, min_max_stats):\n",
    "        normalized_q = min_max_stats.normalize(self.get_child_q_from_parent(child))\n",
    "\n",
    "        max_visits = (\n",
    "            max([ch.visits for ch in self.children.values()])\n",
    "            if len(self.children) > 0\n",
    "            else 0\n",
    "        )\n",
    "        sigma = (self.cvisit + max_visits) * self.cscale * normalized_q\n",
    "        return float(child.root_score + sigma)\n",
    "\n",
    "    def get_child_q_from_parent(self, child):\n",
    "        r = float(self.child_reward(child))\n",
    "        v = float(child.value())\n",
    "        sign = 1.0 if child.to_play == self.to_play else -1.0\n",
    "        q_from_parent = r + self.discount * (sign * v)\n",
    "        return q_from_parent\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# NodeB (numpy)\n",
    "# -------------------------\n",
    "def softmax(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "\n",
    "class NodeB:\n",
    "    discount = 0\n",
    "    num_actions = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def set_static_attributes(discount, num_actions):\n",
    "        NodeB.discount = discount\n",
    "        NodeB.num_actions = num_actions\n",
    "\n",
    "    def __init__(self, prior, action=None, parent=None):\n",
    "        self.prior = float(prior)\n",
    "        self.action = action\n",
    "        self.parent = parent\n",
    "\n",
    "        self.depth = parent.depth + 1 if parent else 0\n",
    "        self.visit_count = 0\n",
    "        self.value_prefix = 0.0\n",
    "\n",
    "        self.state = None\n",
    "        self.reward_hidden = None\n",
    "        self.estimated_value_lst = []\n",
    "        self.children = []\n",
    "        self.selected_children_idx = []\n",
    "        self.reset_value_prefix = True\n",
    "\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        assert NodeB.num_actions > 1\n",
    "        assert 0 < NodeB.discount <= 1.0\n",
    "\n",
    "    def expand(\n",
    "        self,\n",
    "        state,\n",
    "        value_prefix,\n",
    "        policy_logits,\n",
    "        reward_hidden=None,\n",
    "        reset_value_prefix=True,\n",
    "    ):\n",
    "        self.state = state\n",
    "        self.reward_hidden = reward_hidden\n",
    "        self.value_prefix = value_prefix\n",
    "        self.reset_value_prefix = reset_value_prefix\n",
    "\n",
    "        for action in range(NodeB.num_actions):\n",
    "            prior = policy_logits[action]\n",
    "            child = NodeB(prior, action, self)\n",
    "            self.children.append(child)\n",
    "\n",
    "    def get_policy(self):\n",
    "        logits = np.asarray([child.prior for child in self.children])\n",
    "        return softmax(logits)\n",
    "\n",
    "    def get_v_mix(self):\n",
    "        pi_lst = self.get_policy()\n",
    "        pi_sum = 0.0\n",
    "        pi_qsa_sum = 0.0\n",
    "\n",
    "        for action, child in enumerate(self.children):\n",
    "            if child.is_expanded():\n",
    "                pi_sum += pi_lst[action]\n",
    "                pi_qsa_sum += pi_lst[action] * self.get_qsa(action)\n",
    "\n",
    "        if pi_sum < self.epsilon:\n",
    "            v_mix = self.get_value()\n",
    "        else:\n",
    "            visit_sum = self.get_children_visit_sum()\n",
    "            v_mix = (1.0 / (1.0 + visit_sum)) * (\n",
    "                self.get_value() + visit_sum * pi_qsa_sum / pi_sum\n",
    "            )\n",
    "\n",
    "        return float(v_mix)\n",
    "\n",
    "    def get_completed_Q(self, normalize_func):\n",
    "        completed_Qs = []\n",
    "        v_mix = self.get_v_mix()\n",
    "        for action, child in enumerate(self.children):\n",
    "            if child.is_expanded():\n",
    "                completed_Q = self.get_qsa(action)\n",
    "            else:\n",
    "                completed_Q = v_mix\n",
    "            completed_Qs.append(normalize_func(completed_Q))\n",
    "        return np.asarray(completed_Qs)\n",
    "\n",
    "    def get_children_priors(self):\n",
    "        return np.asarray([child.prior for child in self.children])\n",
    "\n",
    "    def get_children_visits(self):\n",
    "        return np.asarray([child.visit_count for child in self.children])\n",
    "\n",
    "    def get_children_visit_sum(self):\n",
    "        visit_lst = self.get_children_visits()\n",
    "        visit_sum = np.sum(visit_lst)\n",
    "        assert visit_sum == self.visit_count - 1\n",
    "        return float(visit_sum)\n",
    "\n",
    "    def get_value(self):\n",
    "        if self.is_expanded():\n",
    "            return float(np.mean(self.estimated_value_lst))\n",
    "        else:\n",
    "            return float(self.parent.get_v_mix())\n",
    "\n",
    "    def get_qsa(self, action):\n",
    "        child = self.children[action]\n",
    "        assert child.is_expanded()\n",
    "        qsa = child.get_reward() + NodeB.discount * child.get_value()\n",
    "        return float(qsa)\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.reset_value_prefix:\n",
    "            return float(self.value_prefix)\n",
    "        else:\n",
    "            assert self.parent is not None\n",
    "            return float(self.value_prefix - self.parent.value_prefix)\n",
    "\n",
    "    def get_expanded_children(self):\n",
    "        assert self.is_expanded()\n",
    "        children = []\n",
    "        for _, child in enumerate(self.children):\n",
    "            if child.is_expanded():\n",
    "                children.append(child)\n",
    "        return children\n",
    "\n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        assert self.is_expanded()\n",
    "        return len(self.get_expanded_children()) == 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        assert (len(self.children) > 0) == (self.visit_count > 0)\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def get_improved_policy(self, transformed_completed_Qs):\n",
    "        logits = np.asarray([child.prior for child in self.children])\n",
    "        return softmax(logits + np.asarray(transformed_completed_Qs))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers + builder\n",
    "# -------------------------\n",
    "class IdentityMinMax:\n",
    "    def normalize(self, x):\n",
    "        return float(x)\n",
    "\n",
    "\n",
    "def build_equivalent_trees(net_policy, root_value, children_info, discount=0.9):\n",
    "    \"\"\"\n",
    "    Build matching trees for NodeA and NodeB.\n",
    "    children_info: list of dicts with keys 'visits', 'child_value', 'reward'\n",
    "    \"\"\"\n",
    "    num_actions = len(net_policy)\n",
    "\n",
    "    # --- NodeA root ---\n",
    "    rootA = NodeA(prior_policy=0.0, parent=None)\n",
    "    rootA.network_policy = torch.tensor(net_policy, dtype=torch.float32)\n",
    "    rootA.visits = 1\n",
    "    rootA.value_sum = float(root_value * rootA.visits)\n",
    "    rootA.to_play = 0\n",
    "    rootA.reward = 0.0\n",
    "    rootA.value_prefix = False\n",
    "    rootA.cvisit = 0.0\n",
    "    rootA.cscale = 1.0\n",
    "\n",
    "    for a, info in enumerate(children_info):\n",
    "        child = NodeA(prior_policy=float(net_policy[a]), parent=rootA)\n",
    "        child.visits = int(info[\"visits\"])\n",
    "        child.value_sum = (\n",
    "            float(info[\"child_value\"] * child.visits) if child.visits > 0 else 0.0\n",
    "        )\n",
    "        child.reward = float(info[\"reward\"])\n",
    "        child.to_play = rootA.to_play\n",
    "\n",
    "        if child.visits > 0:\n",
    "            grandchild = NodeA(prior_policy=0.0, parent=child)\n",
    "            grandchild.visits = 0\n",
    "            grandchild.value_sum = 0.0\n",
    "            grandchild.reward = 0.0\n",
    "            grandchild.to_play = child.to_play\n",
    "            child.children[0] = grandchild\n",
    "\n",
    "        rootA.children[a] = child\n",
    "\n",
    "    # --- NodeB root ---\n",
    "    NodeB.set_static_attributes(discount=discount, num_actions=num_actions)\n",
    "    rootB = NodeB(prior=0.0, action=None, parent=None)\n",
    "    logits = np.log(np.asarray(net_policy))\n",
    "    rootB.expand(\n",
    "        state=None,\n",
    "        value_prefix=root_value,\n",
    "        policy_logits=logits,\n",
    "        reward_hidden=None,\n",
    "        reset_value_prefix=True,\n",
    "    )\n",
    "    rootB.estimated_value_lst = [float(root_value)]\n",
    "\n",
    "    total_child_visits = 0\n",
    "    for a, info in enumerate(children_info):\n",
    "        child = rootB.children[a]\n",
    "        child.visit_count = int(info[\"visits\"])\n",
    "        total_child_visits += child.visit_count\n",
    "        if child.visit_count > 0:\n",
    "            child.expand(\n",
    "                state=None,\n",
    "                value_prefix=float(info[\"reward\"]),\n",
    "                policy_logits=logits,\n",
    "                reward_hidden=None,\n",
    "                reset_value_prefix=True,\n",
    "            )\n",
    "            child.estimated_value_lst = [float(info[\"child_value\"])]\n",
    "            child.value_prefix = float(info[\"reward\"])\n",
    "            child.reset_value_prefix = True\n",
    "            for gc in child.children:\n",
    "                gc.visit_count = 0\n",
    "                gc.estimated_value_lst = []\n",
    "                gc.value_prefix = 0.0\n",
    "                gc.reset_value_prefix = True\n",
    "        else:\n",
    "            child.estimated_value_lst = []\n",
    "            child.value_prefix = float(info[\"reward\"])\n",
    "            child.reset_value_prefix = True\n",
    "\n",
    "    rootB.visit_count = 1 + total_child_visits\n",
    "\n",
    "    NodeA.discount = discount\n",
    "\n",
    "    return rootA, rootB\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Tests\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def test_compare_some_visited_children_vmix():\n",
    "    net_pol = [0.6, 0.3, 0.1]\n",
    "    root_value = 1.0\n",
    "    children_info = [\n",
    "        {\"visits\": 10, \"child_value\": 3.0, \"reward\": 1.0},\n",
    "        {\"visits\": 5, \"child_value\": 2.0, \"reward\": 0.5},\n",
    "        {\"visits\": 0, \"child_value\": 0.0, \"reward\": 0.0},\n",
    "    ]\n",
    "    rootA, rootB = build_equivalent_trees(\n",
    "        net_pol, root_value, children_info, discount=0.9\n",
    "    )\n",
    "\n",
    "    vmix_A = float(rootA.get_v_mix())\n",
    "    vmix_B = float(rootB.get_v_mix())\n",
    "\n",
    "    assert pytest.approx(vmix_A, rel=1e-6) == vmix_B\n",
    "\n",
    "\n",
    "def test_compare_completed_q():\n",
    "    net_pol = [0.6, 0.3, 0.1]\n",
    "    root_value = 1.0\n",
    "    children_info = [\n",
    "        {\"visits\": 10, \"child_value\": 3.0, \"reward\": 1.0},\n",
    "        {\"visits\": 5, \"child_value\": 2.0, \"reward\": 0.5},\n",
    "        {\"visits\": 0, \"child_value\": 0.0, \"reward\": 0.0},\n",
    "    ]\n",
    "    rootA, rootB = build_equivalent_trees(\n",
    "        net_pol, root_value, children_info, discount=0.9\n",
    "    )\n",
    "\n",
    "    minmax = IdentityMinMax()\n",
    "    completedA = rootA.get_completed_q(minmax).numpy()\n",
    "    completedB = rootB.get_completed_Q(lambda x: float(x))\n",
    "\n",
    "    assert completedA.shape == completedB.shape\n",
    "    assert np.allclose(completedA, completedB, rtol=1e-6, atol=1e-8)\n",
    "\n",
    "\n",
    "def test_compare_improved_policy_using_respective_completedQs():\n",
    "    net_pol = [0.6, 0.3, 0.1]\n",
    "    root_value = 1.0\n",
    "    children_info = [\n",
    "        {\"visits\": 10, \"child_value\": 3.0, \"reward\": 1.0},\n",
    "        {\"visits\": 5, \"child_value\": 2.0, \"reward\": 0.5},\n",
    "        {\"visits\": 0, \"child_value\": 0.0, \"reward\": 0.0},\n",
    "    ]\n",
    "    rootA, rootB = build_equivalent_trees(\n",
    "        net_pol, root_value, children_info, discount=0.9\n",
    "    )\n",
    "\n",
    "    minmax = IdentityMinMax()\n",
    "\n",
    "    # set cvisit and cscale as requested\n",
    "    rootA.cvisit = 50.0\n",
    "    rootA.cscale = 1.0\n",
    "\n",
    "    # NodeA's improved policy (internal completedQ_A used by NodeA)\n",
    "    pi0_A = rootA.get_gumbel_improved_policy(minmax).numpy()\n",
    "\n",
    "    # compute completedQ_B using NodeB API (normalized via identity -> same scale)\n",
    "    completedQ_B = rootB.get_completed_Q(lambda x: float(x))  # numpy array\n",
    "\n",
    "    max_visits = (\n",
    "        max([ch.visits for ch in rootA.children.values()])\n",
    "        if len(rootA.children) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # sigma built from NodeB's completedQ\n",
    "    sigma_B = (\n",
    "        (rootA.cvisit + max_visits)\n",
    "        * rootA.cscale\n",
    "        * np.asarray(completedQ_B, dtype=np.float64)\n",
    "    )\n",
    "\n",
    "    # NodeB.improved_policy expects transformed_completed_Qs to add to logits\n",
    "    pi0_B = rootB.get_improved_policy(sigma_B)\n",
    "\n",
    "    # debug assertion: if completedQs differ very slightly, let test show them on failure\n",
    "    assert np.allclose(\n",
    "        pi0_A, pi0_B, rtol=1e-6, atol=1e-8\n",
    "    ), f\"pi mismatch\\npi0_A={pi0_A}\\npi0_B={pi0_B}\\ncompletedQ_A={rootA.get_completed_q(minmax).numpy()}\\ncompletedQ_B={completedQ_B}\\n\"\n",
    "\n",
    "\n",
    "def test_compare_single_visited_action_improved_policy_using_respective_completedQs():\n",
    "    net_pol = [0.2, 0.8, 0.0]\n",
    "    root_value = 2.0\n",
    "    children_info = [\n",
    "        {\"visits\": 0, \"child_value\": 0.0, \"reward\": 0.0},\n",
    "        {\"visits\": 4, \"child_value\": 2.0, \"reward\": 1.0},\n",
    "        {\"visits\": 0, \"child_value\": 0.0, \"reward\": 0.3},\n",
    "    ]\n",
    "    rootA, rootB = build_equivalent_trees(\n",
    "        net_pol, root_value, children_info, discount=0.5\n",
    "    )\n",
    "\n",
    "    minmax = IdentityMinMax()\n",
    "\n",
    "    rootA.cvisit = 50.0\n",
    "    rootA.cscale = 1.0\n",
    "\n",
    "    pi0_A = rootA.get_gumbel_improved_policy(minmax).numpy()\n",
    "    completedQ_B = rootB.get_completed_Q(lambda x: float(x))\n",
    "    max_visits = (\n",
    "        max([ch.visits for ch in rootA.children.values()])\n",
    "        if len(rootA.children) > 0\n",
    "        else 0\n",
    "    )\n",
    "    sigma_B = (\n",
    "        (rootA.cvisit + max_visits)\n",
    "        * rootA.cscale\n",
    "        * np.asarray(completedQ_B, dtype=np.float64)\n",
    "    )\n",
    "    pi0_B = rootB.get_improved_policy(sigma_B)\n",
    "\n",
    "    assert np.allclose(pi0_A, pi0_B, rtol=1e-6, atol=1e-6), f\"{pi0_A, pi0_B}\"\n",
    "\n",
    "\n",
    "test_compare_some_visited_children_vmix()\n",
    "test_compare_completed_q()\n",
    "test_compare_improved_policy_using_respective_completedQs()\n",
    "test_compare_single_visited_action_improved_policy_using_respective_completedQs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Seed 12385 improved policy mismatch\npiA=[0.03834926 0.9616507 ]\npiB=[0.03834905 0.96165095]\ncompletedA=[-1.3553375 -1.2624274]\ncompletedB=[-1.35533755 -1.26242743]\nnet_pol=[0.8458368 0.1541632]\nchildren=[{'visits': 2, 'child_value': -4.277887577304448, 'reward': 0.659149858961192}, {'visits': 3, 'child_value': -3.7459272666552144, 'reward': 0.5015561227860718}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 178\u001b[0m\n\u001b[1;32m    174\u001b[0m         run_single_random_case(s)\n\u001b[1;32m    177\u001b[0m test_edge_cases_fixed()\n\u001b[0;32m--> 178\u001b[0m \u001b[43mtest_randomized_sweep_many_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 174\u001b[0m, in \u001b[0;36mtest_randomized_sweep_many_cases\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m seeds \u001b[38;5;241m=\u001b[39m [RANDOM_SEED \u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_RANDOM)]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[0;32m--> 174\u001b[0m     \u001b[43mrun_single_random_case\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 165\u001b[0m, in \u001b[0;36mrun_single_random_case\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    161\u001b[0m piB \u001b[38;5;241m=\u001b[39m rootB\u001b[38;5;241m.\u001b[39mget_improved_policy(sigmaB)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(piA, piB, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# include arrays for debugging\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m improved policy mismatch\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mpiA=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpiA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mpiB=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpiB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcompletedA=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcompletedB=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mnet_pol=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet_pol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mchildren=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchildren_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Seed 12385 improved policy mismatch\npiA=[0.03834926 0.9616507 ]\npiB=[0.03834905 0.96165095]\ncompletedA=[-1.3553375 -1.2624274]\ncompletedB=[-1.35533755 -1.26242743]\nnet_pol=[0.8458368 0.1541632]\nchildren=[{'visits': 2, 'child_value': -4.277887577304448, 'reward': 0.659149858961192}, {'visits': 3, 'child_value': -3.7459272666552144, 'reward': 0.5015561227860718}]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def test_edge_cases_fixed():\n",
    "    # several edge-case scenarios\n",
    "    cases = [\n",
    "        # all unvisited\n",
    "        {\n",
    "            \"net_pol\": [0.5, 0.5],\n",
    "            \"root_value\": 2.0,\n",
    "            \"children_info\": [\n",
    "                {\"visits\": 0, \"child_value\": 0, \"reward\": 0},\n",
    "                {\"visits\": 0, \"child_value\": 0, \"reward\": 0},\n",
    "            ],\n",
    "            \"discount\": 0.9,\n",
    "        },\n",
    "        # all visited\n",
    "        {\n",
    "            \"net_pol\": [0.4, 0.3, 0.3],\n",
    "            \"root_value\": 1.0,\n",
    "            \"children_info\": [\n",
    "                {\"visits\": 3, \"child_value\": 1.0, \"reward\": 0.2},\n",
    "                {\"visits\": 2, \"child_value\": 0.5, \"reward\": -0.1},\n",
    "                {\"visits\": 1, \"child_value\": 2.0, \"reward\": 0.0},\n",
    "            ],\n",
    "            \"discount\": 0.95,\n",
    "        },\n",
    "        # skewed policy, single visited\n",
    "        {\n",
    "            \"net_pol\": [0.99, 0.01],\n",
    "            \"root_value\": 0.5,\n",
    "            \"children_info\": [\n",
    "                {\"visits\": 0, \"child_value\": 0, \"reward\": 0},\n",
    "                {\"visits\": 5, \"child_value\": 1.0, \"reward\": 0.3},\n",
    "            ],\n",
    "            \"discount\": 0.5,\n",
    "        },\n",
    "        # small p_vis_sum scenario\n",
    "        {\n",
    "            \"net_pol\": [1e-6, 0.999999],\n",
    "            \"root_value\": 1.2,\n",
    "            \"children_info\": [\n",
    "                {\"visits\": 5, \"child_value\": 0.1, \"reward\": 0.0},\n",
    "                {\"visits\": 0, \"child_value\": 0, \"reward\": 0},\n",
    "            ],\n",
    "            \"discount\": 0.9,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    minmax = IdentityMinMax()\n",
    "    for i, c in enumerate(cases):\n",
    "        rootA, rootB = build_equivalent_trees(\n",
    "            c[\"net_pol\"], c[\"root_value\"], c[\"children_info\"], discount=c[\"discount\"]\n",
    "        )\n",
    "        vmixA = float(rootA.get_v_mix())\n",
    "        vmixB = float(rootB.get_v_mix())\n",
    "        assert pytest.approx(vmixA, rel=1e-5) == vmixB\n",
    "\n",
    "        # completed Q\n",
    "        compA = rootA.get_completed_q(minmax).numpy()\n",
    "        compB = rootB.get_completed_Q(lambda x: float(x))\n",
    "        assert compA.shape == compB.shape\n",
    "        assert np.allclose(compA, compB, rtol=1e-5, atol=1e-5)\n",
    "\n",
    "        # improved policy: compute sigma from each node's own completed Qs\n",
    "        rootA.cvisit = 50.0\n",
    "        rootA.cscale = 1.0\n",
    "        piA = rootA.get_gumbel_improved_policy(minmax).numpy()\n",
    "        compB_for_sigma = compB\n",
    "        max_visits = (\n",
    "            max([ch.visits for ch in rootA.children.values()])\n",
    "            if len(rootA.children) > 0\n",
    "            else 0\n",
    "        )\n",
    "        sigmaB = (\n",
    "            (rootA.cvisit + max_visits)\n",
    "            * rootA.cscale\n",
    "            * np.asarray(compB_for_sigma, dtype=np.float64)\n",
    "        )\n",
    "        piB = rootB.get_improved_policy(sigmaB)\n",
    "        assert np.allclose(\n",
    "            piA, piB, rtol=1e-6, atol=1e-5\n",
    "        ), f\"Edge case {i} failure\\npiA={piA}\\npiB={piB}\\ncompA={compA}\\ncompB={compB}\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Randomized property tests\n",
    "# -------------------------\n",
    "NUM_RANDOM = 200  # increase if you want a larger sweep\n",
    "RANDOM_SEED = 12345\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def random_net_policy(num_actions):\n",
    "    # sample Dirichlet for a well-formed probability vector\n",
    "    a = np.random.dirichlet([1.0] * num_actions)\n",
    "    # ensure strictly positive entries (dirichlet already does)\n",
    "    return a\n",
    "\n",
    "\n",
    "def random_children_info(num_actions, max_visits=20):\n",
    "    info = []\n",
    "    for _ in range(num_actions):\n",
    "        visits = int(np.random.choice(range(0, max_visits + 1)))\n",
    "        child_value = float(np.random.normal(loc=0.0, scale=3.0))\n",
    "        reward = float(np.random.normal(loc=0.0, scale=1.0))\n",
    "        info.append({\"visits\": visits, \"child_value\": child_value, \"reward\": reward})\n",
    "    return info\n",
    "\n",
    "\n",
    "def run_single_random_case(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    num_actions = np.random.choice([2, 3, 4, 5, 6])\n",
    "    net_pol = random_net_policy(num_actions)\n",
    "    root_value = float(np.random.normal(loc=0.0, scale=2.0))\n",
    "    children_info = random_children_info(num_actions, max_visits=12)\n",
    "    discount = float(np.random.uniform(0.1, 0.99))\n",
    "\n",
    "    rootA, rootB = build_equivalent_trees(\n",
    "        net_pol.tolist(), root_value, children_info, discount=discount\n",
    "    )\n",
    "\n",
    "    minmax = IdentityMinMax()\n",
    "\n",
    "    # compare v_mix\n",
    "    vmixA = float(rootA.get_v_mix())\n",
    "    vmixB = float(rootB.get_v_mix())\n",
    "    if not np.isclose(vmixA, vmixB, rtol=1e-6, atol=1e-7):\n",
    "        raise AssertionError(\n",
    "            f\"Seed {seed} vmix mismatch: vmixA={vmixA}, vmixB={vmixB}\\nnet_pol={net_pol}\\nchildren={children_info}\"\n",
    "        )\n",
    "\n",
    "    # compare completed Q\n",
    "    compA = rootA.get_completed_q(minmax).numpy()\n",
    "    compB = rootB.get_completed_Q(lambda x: float(x))\n",
    "    if compA.shape != compB.shape or (\n",
    "        not np.allclose(compA, compB, rtol=1e-6, atol=1e-7)\n",
    "    ):\n",
    "        raise AssertionError(\n",
    "            f\"Seed {seed} completedQ mismatch\\ncompA={compA}\\ncompB={compB}\\nnet_pol={net_pol}\\nchildren={children_info}\"\n",
    "        )\n",
    "\n",
    "    # improved policy: NodeA uses its completedQ; NodeB uses its own completedQ to build sigma\n",
    "    rootA.cvisit = 50.0\n",
    "    rootA.cscale = 1.0\n",
    "    piA = rootA.get_gumbel_improved_policy(minmax).numpy()\n",
    "\n",
    "    compB_for_sigma = compB  # NodeB's completedQ\n",
    "    max_visits = (\n",
    "        max([ch.visits for ch in rootA.children.values()])\n",
    "        if len(rootA.children) > 0\n",
    "        else 0\n",
    "    )\n",
    "    sigmaB = (\n",
    "        (rootA.cvisit + max_visits)\n",
    "        * rootA.cscale\n",
    "        * np.asarray(compB_for_sigma, dtype=np.float64)\n",
    "    )\n",
    "    piB = rootB.get_improved_policy(sigmaB)\n",
    "\n",
    "    if not np.allclose(piA, piB, rtol=1e-6, atol=1e-7):\n",
    "        # include arrays for debugging\n",
    "        raise AssertionError(\n",
    "            f\"Seed {seed} improved policy mismatch\\npiA={piA}\\npiB={piB}\\ncompletedA={compA}\\ncompletedB={compB}\\nnet_pol={net_pol}\\nchildren={children_info}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def test_randomized_sweep_many_cases():\n",
    "    # run many randomized cases; on first failure pytest will report\n",
    "    seeds = [RANDOM_SEED + i for i in range(NUM_RANDOM)]\n",
    "    for s in seeds:\n",
    "        run_single_random_case(s)\n",
    "\n",
    "\n",
    "test_edge_cases_fixed()\n",
    "test_randomized_sweep_many_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_n_step_info.py\n",
    "import torch\n",
    "import pytest\n",
    "\n",
    "\n",
    "# ---- Minimal test harness object ----\n",
    "# We copy the _get_n_step_info implementation into a small test class so\n",
    "# we can call it exactly as your real object would.\n",
    "class DummyMuZero:\n",
    "    def __init__(self, num_actions, num_players, gamma=1.0, value_prefix=True):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_players = num_players\n",
    "        self.gamma = gamma\n",
    "        self.value_prefix = value_prefix\n",
    "\n",
    "    def _get_n_step_info(\n",
    "        self,\n",
    "        index: int,\n",
    "        values: list,\n",
    "        policies: list,\n",
    "        rewards: list,\n",
    "        actions: list,\n",
    "        infos: list,\n",
    "        num_unroll_steps: int,\n",
    "        n_step: int,\n",
    "        lstm_horizon_len: int = 5,\n",
    "    ):\n",
    "        import torch\n",
    "\n",
    "        n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "        n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "        n_step_policies = torch.zeros(\n",
    "            (num_unroll_steps + 1, self.num_actions), dtype=torch.float32\n",
    "        )\n",
    "        n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.float16)\n",
    "        n_step_to_plays = torch.zeros(\n",
    "            (num_unroll_steps + 1, self.num_players), dtype=torch.int16\n",
    "        )\n",
    "        value_prefix = 0.0\n",
    "        horizon_id = 0\n",
    "        root_player = (\n",
    "            infos[index].get(\"player\", None)\n",
    "            if index < len(infos) and \"player\" in infos[index]\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        for u in range(0, num_unroll_steps + 1):\n",
    "            current_index = index + u\n",
    "\n",
    "            # 1. discounted n-step value from current_index (same logic as before)\n",
    "            value = 0.0\n",
    "            for k in range(n_step):\n",
    "                r_idx = current_index + k\n",
    "                if r_idx < len(rewards):\n",
    "                    r = rewards[r_idx]\n",
    "                    node_player = (\n",
    "                        infos[current_index].get(\"player\", None)\n",
    "                        if current_index < len(infos)\n",
    "                        else None\n",
    "                    )\n",
    "                    acting_player = (\n",
    "                        infos[r_idx].get(\"player\", None) if r_idx < len(infos) else None\n",
    "                    )\n",
    "                    sign = (\n",
    "                        1.0\n",
    "                        if (\n",
    "                            node_player is None\n",
    "                            or acting_player is None\n",
    "                            or node_player == acting_player\n",
    "                        )\n",
    "                        else -1.0\n",
    "                    )\n",
    "                    value += (self.gamma**k) * (sign * r)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            boot_idx = current_index + n_step\n",
    "            if boot_idx < len(values):\n",
    "                v_boot = values[boot_idx]\n",
    "                node_player = (\n",
    "                    infos[current_index].get(\"player\", None)\n",
    "                    if current_index < len(infos)\n",
    "                    else None\n",
    "                )\n",
    "                boot_player = (\n",
    "                    infos[boot_idx].get(\"player\", None)\n",
    "                    if boot_idx < len(infos)\n",
    "                    else None\n",
    "                )\n",
    "                sign_leaf = (\n",
    "                    1.0\n",
    "                    if (\n",
    "                        node_player is None\n",
    "                        or boot_player is None\n",
    "                        or node_player == boot_player\n",
    "                    )\n",
    "                    else -1.0\n",
    "                )\n",
    "                value += (self.gamma**n_step) * (sign_leaf * v_boot)\n",
    "\n",
    "            n_step_values[u] = value\n",
    "\n",
    "            # 2. reward target with first cell zeroed\n",
    "            if self.value_prefix:\n",
    "                # 2. Value Prefix (EfficientZero Logic)\n",
    "                # Reset accumulation periodically based on LSTM horizon\n",
    "                if horizon_id % lstm_horizon_len == 0:\n",
    "                    value_prefix = 0.0\n",
    "                horizon_id += 1\n",
    "\n",
    "                # Get immediate reward (root u=0 has 0 reward)\n",
    "                current_reward = 0.0\n",
    "                if u > 0:\n",
    "                    reward_idx = current_index - 1\n",
    "                    if reward_idx < len(rewards):\n",
    "                        r = rewards[reward_idx]\n",
    "                        acting_player = (\n",
    "                            infos[reward_idx].get(\"player\", None)\n",
    "                            if reward_idx < len(infos) and \"player\" in infos[reward_idx]\n",
    "                            else None\n",
    "                        )\n",
    "                        sign = (\n",
    "                            1.0\n",
    "                            if (\n",
    "                                root_player is None\n",
    "                                or acting_player is None\n",
    "                                or root_player == acting_player\n",
    "                            )\n",
    "                            else -1.0\n",
    "                        )\n",
    "                        current_reward = sign * r\n",
    "\n",
    "                # Accumulate into prefix\n",
    "                value_prefix += current_reward\n",
    "                n_step_rewards[u] = value_prefix\n",
    "            else:\n",
    "                if u == 0:\n",
    "                    n_step_rewards[u] = 0.0  # root has no preceding reward\n",
    "                else:\n",
    "                    reward_idx = current_index - 1\n",
    "                    n_step_rewards[u] = (\n",
    "                        rewards[reward_idx] if reward_idx < len(rewards) else 0.0\n",
    "                    )\n",
    "\n",
    "            # 3. policy\n",
    "            if current_index < len(policies):\n",
    "                n_step_policies[u] = policies[current_index]\n",
    "            else:\n",
    "                n_step_policies[u] = torch.ones(self.num_actions) / self.num_actions\n",
    "\n",
    "            # 4. action\n",
    "            if u < num_unroll_steps:\n",
    "                if current_index < len(actions):\n",
    "                    n_step_actions[u] = actions[current_index]\n",
    "                else:\n",
    "                    n_step_actions[u] = int(torch.randint(0, self.num_actions, (1,)))\n",
    "            # 5. to_play (NEW): store the player id for this state (or -1 if OOB)\n",
    "            if current_index < len(infos):\n",
    "                if \"player\" in infos[current_index]:\n",
    "                    n_step_to_plays[u][infos[current_index][\"player\"]] = 1\n",
    "\n",
    "        return (\n",
    "            n_step_values,\n",
    "            n_step_policies,\n",
    "            n_step_rewards,\n",
    "            n_step_actions,\n",
    "            n_step_to_plays,\n",
    "        )\n",
    "\n",
    "\n",
    "# ---- Tests ----\n",
    "\n",
    "\n",
    "def test_value_prefix_single_player_accumulates():\n",
    "    \"\"\"Single-player: value_prefix should be running sum of previous rewards,\n",
    "    with element 0 == 0. (no sign flips)\"\"\"\n",
    "    dummy = DummyMuZero(num_actions=2, num_players=1, gamma=1.0, value_prefix=True)\n",
    "\n",
    "    rewards = [1.0, 2.0, 3.0, 4.0]\n",
    "    values = [0.0] * 10\n",
    "    policies = [torch.tensor([0.5, 0.5])] * 10\n",
    "    actions = [0, 1, 0, 1]\n",
    "    infos = [{\"player\": 0} for _ in rewards]  # all player 0\n",
    "\n",
    "    _, _, n_step_rewards, _, n_step_to_plays = dummy._get_n_step_info(\n",
    "        index=0,\n",
    "        values=values,\n",
    "        policies=policies,\n",
    "        rewards=rewards,\n",
    "        actions=actions,\n",
    "        infos=infos,\n",
    "        num_unroll_steps=3,\n",
    "        n_step=3,\n",
    "        lstm_horizon_len=5,\n",
    "    )\n",
    "\n",
    "    # Expect accumulation: [0, 1, 1+2=3, 1+2+3=6]\n",
    "    assert n_step_rewards.tolist() == [0.0, 1.0, 3.0, 6.0]\n",
    "\n",
    "    # Check to_play mask shape and content\n",
    "    assert n_step_to_plays.shape == (4, 1)\n",
    "    # every row should have a 1 at player 0\n",
    "    assert all(int(row[0]) == 1 for row in n_step_to_plays)\n",
    "\n",
    "\n",
    "def test_value_prefix_two_player_sign_flips():\n",
    "    \"\"\"Two-player alternating: value_prefix should flip sign for rewards\n",
    "    from a player different than the root player.\"\"\"\n",
    "    dummy = DummyMuZero(num_actions=2, num_players=2, gamma=1.0, value_prefix=True)\n",
    "\n",
    "    # players: root (index=0) is player 0, then player 1 acts, then player 0, etc.\n",
    "    infos = [{\"player\": 0}, {\"player\": 1}, {\"player\": 0}, {\"player\": 1}]\n",
    "    rewards = [1.0, 2.0, 3.0, 4.0]  # reward[t] is reward for transition t->t+1\n",
    "    values = [0.0] * 10\n",
    "    policies = [torch.tensor([0.5, 0.5])] * 10\n",
    "    actions = [0, 1, 0, 1]\n",
    "\n",
    "    # index=0, root_player = infos[0][\"player\"] == 0\n",
    "    _, _, n_step_rewards, _, n_step_to_plays = dummy._get_n_step_info(\n",
    "        index=0,\n",
    "        values=values,\n",
    "        policies=policies,\n",
    "        rewards=rewards,\n",
    "        actions=actions,\n",
    "        infos=infos,\n",
    "        num_unroll_steps=3,\n",
    "        n_step=3,\n",
    "        lstm_horizon_len=5,\n",
    "    )\n",
    "\n",
    "    # Computation by hand:\n",
    "    # u=0 -> 0\n",
    "    # u=1 -> reward_idx=0 acting_player=0 sign=+1 => +1 => prefix=1\n",
    "    # u=2 -> reward_idx=1 acting_player=1 sign=-1 => -2 => prefix=1 + (-2) = -1\n",
    "    # u=3 -> reward_idx=2 acting_player=0 sign=+1 => +3 => prefix = -1 + 3 = 2\n",
    "    assert n_step_rewards.tolist() == [0.0, 1.0, -1.0, 2.0]\n",
    "\n",
    "    # confirm to_play rows\n",
    "    assert n_step_to_plays.shape == (4, 2)\n",
    "    # row 0 -> player 0\n",
    "    assert int(n_step_to_plays[0][0]) == 1 and int(n_step_to_plays[0][1]) == 0\n",
    "    # row 1 -> player 1\n",
    "    assert int(n_step_to_plays[1][1]) == 1\n",
    "\n",
    "\n",
    "def test_lstm_horizon_reset_of_prefix():\n",
    "    \"\"\"When horizon_id hits a multiple of lstm_horizon_len, the prefix resets to 0.\"\"\"\n",
    "    dummy = DummyMuZero(num_actions=2, num_players=1, gamma=1.0, value_prefix=True)\n",
    "\n",
    "    # single player so sign is always +1\n",
    "    infos = [{\"player\": 0} for _ in range(6)]\n",
    "    rewards = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    values = [0.0] * 10\n",
    "    policies = [torch.tensor([0.5, 0.5])] * 10\n",
    "    actions = [0] * 6\n",
    "\n",
    "    # Use a small lstm_horizon_len to force a reset mid-unroll\n",
    "    _, _, n_step_rewards, _, _ = dummy._get_n_step_info(\n",
    "        index=0,\n",
    "        values=values,\n",
    "        policies=policies,\n",
    "        rewards=rewards,\n",
    "        actions=actions,\n",
    "        infos=infos,\n",
    "        num_unroll_steps=5,\n",
    "        n_step=3,\n",
    "        lstm_horizon_len=2,  # reset every 2 steps (horizon_id-based)\n",
    "    )\n",
    "\n",
    "    # Let's compute expected:\n",
    "    # horizon_id starts 0 -> u=0 reset -> prefix=0 -> n[0]=0\n",
    "    # u=1 horizon_id=1 -> prefix += rewards[0] (=1) -> n[1]=1\n",
    "    # u=2 horizon_id=2 -> since 2%2==0 reset prefix=0 -> then prefix += rewards[1] (=1) -> n[2]=1\n",
    "    # u=3 horizon_id=3 -> prefix += rewards[2] (=1) -> n[3]=2\n",
    "    # u=4 horizon_id=4 -> reset -> prefix=0 -> prefix += rewards[3] (=1) -> n[4]=1\n",
    "    # u=5 horizon_id=5 -> prefix += rewards[4] (=1) -> n[5]=2\n",
    "    assert n_step_rewards.tolist() == [0.0, 1.0, 1.0, 2.0, 1.0, 2.0]\n",
    "\n",
    "\n",
    "def test_value_prefix_off_immediate_rewards_only():\n",
    "    \"\"\"When value_prefix is False the n_step_rewards are just immediate rewards\n",
    "    (u=0 -> 0, u>0 -> reward at current_index-1).\"\"\"\n",
    "    dummy = DummyMuZero(num_actions=2, num_players=1, gamma=1.0, value_prefix=False)\n",
    "\n",
    "    infos = [{\"player\": 0}, {\"player\": 0}, {\"player\": 0}]\n",
    "    rewards = [10.0, 20.0, 30.0]\n",
    "    values = [0.0] * 10\n",
    "    policies = [torch.tensor([0.5, 0.5])] * 10\n",
    "    actions = [0, 1, 0]\n",
    "\n",
    "    _, _, n_step_rewards, _, _ = dummy._get_n_step_info(\n",
    "        index=0,\n",
    "        values=values,\n",
    "        policies=policies,\n",
    "        rewards=rewards,\n",
    "        actions=actions,\n",
    "        infos=infos,\n",
    "        num_unroll_steps=2,\n",
    "        n_step=1,\n",
    "        lstm_horizon_len=5,\n",
    "    )\n",
    "\n",
    "    # u=0 -> 0\n",
    "    # u=1 -> rewards[0] == 10\n",
    "    # u=2 -> rewards[1] == 20\n",
    "    assert n_step_rewards.tolist() == [0.0, 10.0, 20.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value_prefix_off_immediate_rewards_only()\n",
    "test_lstm_horizon_reset_of_prefix()\n",
    "test_value_prefix_two_player_sign_flips()\n",
    "test_value_prefix_single_player_accumulates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prefix_assigned_to_nodes.py\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import pytest\n",
    "\n",
    "sys.path.append(\"..\")  # adjust if your package root is different\n",
    "from muzero.muzero_mcts import Node\n",
    "\n",
    "\n",
    "# Re-implement the exact _get_n_step_info you provided inside a dummy object\n",
    "class DummyMuZero:\n",
    "    def __init__(self, num_actions=2, num_players=2, gamma=1.0, value_prefix=True):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_players = num_players\n",
    "        self.gamma = gamma\n",
    "        self.value_prefix = value_prefix\n",
    "\n",
    "    # def _get_n_step_info(\n",
    "    #     self,\n",
    "    #     index: int,\n",
    "    #     values: list,\n",
    "    #     policies: list,\n",
    "    #     rewards: list,\n",
    "    #     actions: list,\n",
    "    #     infos: list,\n",
    "    #     num_unroll_steps: int,\n",
    "    #     n_step: int,\n",
    "    #     lstm_horizon_len: int = 5,\n",
    "    # ):\n",
    "    #     import torch\n",
    "\n",
    "    #     n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    #     n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "    #     n_step_policies = torch.zeros(\n",
    "    #         (num_unroll_steps + 1, self.num_actions), dtype=torch.float32\n",
    "    #     )\n",
    "    #     n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.float16)\n",
    "    #     n_step_to_plays = torch.zeros(\n",
    "    #         (num_unroll_steps + 1, self.num_players), dtype=torch.int16\n",
    "    #     )\n",
    "    #     value_prefix = 0.0\n",
    "    #     horizon_id = 0\n",
    "    #     root_player = (\n",
    "    #         infos[index].get(\"player\", None)\n",
    "    #         if index < len(infos) and \"player\" in infos[index]\n",
    "    #         else None\n",
    "    #     )\n",
    "\n",
    "    #     for u in range(0, num_unroll_steps + 1):\n",
    "    #         current_index = index + u\n",
    "\n",
    "    #         # 1. discounted n-step value from current_index (same logic as before)\n",
    "    #         value = 0.0\n",
    "    #         for k in range(n_step):\n",
    "    #             r_idx = current_index + k\n",
    "    #             if r_idx < len(rewards):\n",
    "    #                 r = rewards[r_idx]\n",
    "    #                 node_player = (\n",
    "    #                     infos[current_index].get(\"player\", None)\n",
    "    #                     if current_index < len(infos)\n",
    "    #                     else None\n",
    "    #                 )\n",
    "    #                 acting_player = (\n",
    "    #                     infos[r_idx].get(\"player\", None) if r_idx < len(infos) else None\n",
    "    #                 )\n",
    "    #                 sign = (\n",
    "    #                     1.0\n",
    "    #                     if (\n",
    "    #                         node_player is None\n",
    "    #                         or acting_player is None\n",
    "    #                         or node_player == acting_player\n",
    "    #                     )\n",
    "    #                     else -1.0\n",
    "    #                 )\n",
    "    #                 value += (self.gamma**k) * (sign * r)\n",
    "    #             else:\n",
    "    #                 break\n",
    "\n",
    "    #         boot_idx = current_index + n_step\n",
    "    #         if boot_idx < len(values):\n",
    "    #             v_boot = values[boot_idx]\n",
    "    #             node_player = (\n",
    "    #                 infos[current_index].get(\"player\", None)\n",
    "    #                 if current_index < len(infos)\n",
    "    #                 else None\n",
    "    #             )\n",
    "    #             boot_player = (\n",
    "    #                 infos[boot_idx].get(\"player\", None)\n",
    "    #                 if boot_idx < len(infos)\n",
    "    #                 else None\n",
    "    #             )\n",
    "    #             sign_leaf = (\n",
    "    #                 1.0\n",
    "    #                 if (\n",
    "    #                     node_player is None\n",
    "    #                     or boot_player is None\n",
    "    #                     or node_player == boot_player\n",
    "    #                 )\n",
    "    #                 else -1.0\n",
    "    #             )\n",
    "    #             value += (self.gamma**n_step) * (sign_leaf * v_boot)\n",
    "\n",
    "    #         n_step_values[u] = value\n",
    "\n",
    "    #         # 2. reward target with first cell zeroed\n",
    "    #         if self.value_prefix:\n",
    "    #             # 2. Value Prefix (EfficientZero Logic)\n",
    "    #             # Reset accumulation periodically based on LSTM horizon\n",
    "    #             if horizon_id % lstm_horizon_len == 0:\n",
    "    #                 value_prefix = 0.0\n",
    "    #             horizon_id += 1\n",
    "\n",
    "    #             # Get immediate reward (root u=0 has 0 reward)\n",
    "    #             current_reward = 0.0\n",
    "    #             if u > 0:\n",
    "    #                 reward_idx = current_index - 1\n",
    "    #                 if reward_idx < len(rewards):\n",
    "    #                     r = rewards[reward_idx]\n",
    "    #                     acting_player = (\n",
    "    #                         infos[reward_idx].get(\"player\", None)\n",
    "    #                         if reward_idx < len(infos) and \"player\" in infos[reward_idx]\n",
    "    #                         else None\n",
    "    #                     )\n",
    "    #                     sign = (\n",
    "    #                         1.0\n",
    "    #                         if (\n",
    "    #                             root_player is None\n",
    "    #                             or acting_player is None\n",
    "    #                             or root_player == acting_player\n",
    "    #                         )\n",
    "    #                         else -1.0\n",
    "    #                     )\n",
    "    #                     current_reward = sign * r\n",
    "\n",
    "    #             # Accumulate into prefix\n",
    "    #             value_prefix += current_reward\n",
    "    #             n_step_rewards[u] = value_prefix\n",
    "    #         else:\n",
    "    #             if u == 0:\n",
    "    #                 n_step_rewards[u] = 0.0  # root has no preceding reward\n",
    "    #             else:\n",
    "    #                 reward_idx = current_index - 1\n",
    "    #                 n_step_rewards[u] = (\n",
    "    #                     rewards[reward_idx] if reward_idx < len(rewards) else 0.0\n",
    "    #                 )\n",
    "\n",
    "    #         # 3. policy\n",
    "    #         if current_index < len(policies):\n",
    "    #             n_step_policies[u] = policies[current_index]\n",
    "    #         else:\n",
    "    #             n_step_policies[u] = torch.ones(self.num_actions) / self.num_actions\n",
    "\n",
    "    #         # 4. action\n",
    "    #         if u < num_unroll_steps:\n",
    "    #             if current_index < len(actions):\n",
    "    #                 n_step_actions[u] = actions[current_index]\n",
    "    #             else:\n",
    "    #                 n_step_actions[u] = int(torch.randint(0, self.num_actions, (1,)))\n",
    "    #         # 5. to_play (NEW): store the player id for this state (or -1 if OOB)\n",
    "    #         if current_index < len(infos):\n",
    "    #             if \"player\" in infos[current_index]:\n",
    "    #                 n_step_to_plays[u][infos[current_index][\"player\"]] = 1\n",
    "\n",
    "    #     return (\n",
    "    #         n_step_values,\n",
    "    #         n_step_policies,\n",
    "    #         n_step_rewards,\n",
    "    #         n_step_actions,\n",
    "    #         n_step_to_plays,\n",
    "    #     )\n",
    "\n",
    "    def _get_n_step_info(\n",
    "        self,\n",
    "        index: int,\n",
    "        values: list,\n",
    "        policies: list,\n",
    "        rewards: list,\n",
    "        actions: list,\n",
    "        infos: list,\n",
    "        num_unroll_steps: int,\n",
    "        n_step: int,\n",
    "        lstm_horizon_len: int = 5,\n",
    "    ):\n",
    "        import torch\n",
    "\n",
    "        n_step_values = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "        n_step_rewards = torch.zeros(num_unroll_steps + 1, dtype=torch.float32)\n",
    "        n_step_policies = torch.zeros(\n",
    "            (num_unroll_steps + 1, self.num_actions), dtype=torch.float32\n",
    "        )\n",
    "        n_step_actions = torch.zeros(num_unroll_steps, dtype=torch.float16)\n",
    "        n_step_to_plays = torch.zeros(\n",
    "            (num_unroll_steps + 1, self.num_players), dtype=torch.int16\n",
    "        )\n",
    "        value_prefix = 0.0\n",
    "        horizon_id = 0\n",
    "        root_player = (\n",
    "            infos[index].get(\"player\", None)\n",
    "            if index < len(infos) and \"player\" in infos[index]\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        for u in range(0, num_unroll_steps + 1):\n",
    "            current_index = index + u\n",
    "\n",
    "            # 1. discounted n-step value from current_index (same logic as before)\n",
    "            value = 0.0\n",
    "            for k in range(n_step):\n",
    "                r_idx = current_index + k\n",
    "                if r_idx < len(rewards):\n",
    "                    r = rewards[r_idx]\n",
    "                    node_player = (\n",
    "                        infos[current_index].get(\"player\", None)\n",
    "                        if current_index < len(infos)\n",
    "                        else None\n",
    "                    )\n",
    "                    acting_player = (\n",
    "                        infos[r_idx].get(\"player\", None) if r_idx < len(infos) else None\n",
    "                    )\n",
    "                    # keep sign logic for n-step value bootstrapping (unchanged)\n",
    "                    sign = (\n",
    "                        1.0\n",
    "                        if (\n",
    "                            node_player is None\n",
    "                            or acting_player is None\n",
    "                            or node_player == acting_player\n",
    "                        )\n",
    "                        else -1.0\n",
    "                    )\n",
    "                    value += (self.gamma**k) * (sign * r)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            boot_idx = current_index + n_step\n",
    "            if boot_idx < len(values):\n",
    "                v_boot = values[boot_idx]\n",
    "                node_player = (\n",
    "                    infos[current_index].get(\"player\", None)\n",
    "                    if current_index < len(infos)\n",
    "                    else None\n",
    "                )\n",
    "                boot_player = (\n",
    "                    infos[boot_idx].get(\"player\", None)\n",
    "                    if boot_idx < len(infos)\n",
    "                    else None\n",
    "                )\n",
    "                sign_leaf = (\n",
    "                    1.0\n",
    "                    if (\n",
    "                        node_player is None\n",
    "                        or boot_player is None\n",
    "                        or node_player == boot_player\n",
    "                    )\n",
    "                    else -1.0\n",
    "                )\n",
    "                value += (self.gamma**n_step) * (sign_leaf * v_boot)\n",
    "\n",
    "            n_step_values[u] = value\n",
    "\n",
    "            # 2. reward target with first cell zeroed\n",
    "            if self.value_prefix:\n",
    "                # EfficientZero-style value-prefix, but accumulate RAW rewards (no per-reward sign flip)\n",
    "                # Reset accumulation periodically based on LSTM horizon\n",
    "                if horizon_id % lstm_horizon_len == 0:\n",
    "                    value_prefix = 0.0\n",
    "                horizon_id += 1\n",
    "\n",
    "                # Get immediate reward (root u=0 has 0 reward)\n",
    "                current_reward = 0.0\n",
    "                if u > 0:\n",
    "                    reward_idx = current_index - 1\n",
    "                    if reward_idx < len(rewards):\n",
    "                        # <-- **use raw reward directly** (no sign flipping)\n",
    "                        current_reward = rewards[reward_idx]\n",
    "\n",
    "                # Accumulate into prefix\n",
    "                value_prefix += current_reward\n",
    "                n_step_rewards[u] = value_prefix\n",
    "            else:\n",
    "                if u == 0:\n",
    "                    n_step_rewards[u] = 0.0  # root has no preceding reward\n",
    "                else:\n",
    "                    reward_idx = current_index - 1\n",
    "                    n_step_rewards[u] = (\n",
    "                        rewards[reward_idx] if reward_idx < len(rewards) else 0.0\n",
    "                    )\n",
    "\n",
    "            # 3. policy\n",
    "            if current_index < len(policies):\n",
    "                n_step_policies[u] = policies[current_index]\n",
    "            else:\n",
    "                n_step_policies[u] = torch.ones(self.num_actions) / self.num_actions\n",
    "\n",
    "            # 4. action\n",
    "            if u < num_unroll_steps:\n",
    "                if current_index < len(actions):\n",
    "                    n_step_actions[u] = actions[current_index]\n",
    "                else:\n",
    "                    n_step_actions[u] = int(torch.randint(0, self.num_actions, (1,)))\n",
    "\n",
    "            # 5. to_play (NEW): store the player id for this state (or -1 if OOB)\n",
    "            if current_index < len(infos):\n",
    "                if \"player\" in infos[current_index]:\n",
    "                    n_step_to_plays[u][infos[current_index][\"player\"]] = 1\n",
    "\n",
    "        return (\n",
    "            n_step_values,\n",
    "            n_step_policies,\n",
    "            n_step_rewards,\n",
    "            n_step_actions,\n",
    "            n_step_to_plays,\n",
    "        )\n",
    "\n",
    "\n",
    "# --- test cases copied from your original message (only the path_config portion used) ---\n",
    "test_cases = [\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        \"2-player: two player 1s with a reward for player 1 on a normally player 0 turn, ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 1.0, 0.0)],\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 1.0, 0.0)],\n",
    "        \"2-player: two player 1s both actions getting a reward (they should dont cancel), ending on a root for player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (0, 0.0, 0.0)],\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 0\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 1.0), (1, 1.0), (1, 0.0, 0.0)],\n",
    "        \"2-player: Two player 1 turns (but player 0 got a reward), ending on player 1\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0, 0.0)],\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 1.0), (1, 0.0), (0, 0.0, 0.0)],\n",
    "        \"2-player: alternating game, player 1 wins on there first move\",\n",
    "    ),\n",
    "    ([(1, 0.0), (0, 0.0), (1, 1.0, 0.0)], \"2-player: alternating game, player 0 wins\"),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 1.0, 0.0)],\n",
    "        \"2-player: alternating game, player 1 wins\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (0, 0.0), (1, 0.0), (0, 0.0, 1.0)],\n",
    "        \"2-player: alternating game with a leaf value\",\n",
    "    ),\n",
    "    ([(0, 1.0), (0, 1.0), (0, 1.0, 0.0)], \"2-player: All player 0 turns\"),\n",
    "    (\n",
    "        [(0, 0.0), (0, 0.0), (0, 0.0, 4.0)],\n",
    "        \"2-player: All player 0 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (0, 0.0, 4.0)],\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0, 4.0)],\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    (\n",
    "        [(1, 0.0), (1, 1.0), (1, 0.0), (0, 0.0, 4.0)],\n",
    "        \"2-player: Two player 1 turns with leaf value\",\n",
    "    ),\n",
    "    # Single player test cases\n",
    "    ([(0, 1.0), (0, 2.0), (0, 3.0, 0.0)], \"1-player: All rewards sum up\"),\n",
    "    ([(0, 1.0), (0, 0.0), (0, 0.0, 5.0)], \"1-player: Rewards + leaf value\"),\n",
    "    ([(1, 0.0), (2, 0.0), (0, 1.0), (0, 0.0, 0.0)], \"3-player: Player 2 wins\"),\n",
    "    ([(1, 1.0), (2, 0.0), (0, 0.0), (0, 0.0, 0.0)], \"3-player: Player 0 wins\"),\n",
    "    ([(1, 0.0), (2, 1.0), (0, 0.0), (0, 0.0, 0.0)], \"3-player: Player 1 wins\"),\n",
    "    ([(1, 0.0), (2, 0.0), (0, 0.0), (0, 1.0, 0.0)], \"3-player: Player 0 wins\"),\n",
    "    (\n",
    "        [(1, 0.0), (2, 0.0), (0, 0.0), (1, 0.0, 1.0)],\n",
    "        \"3-player: player 1 ends with a value prediction\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def build_infos_and_rewards_from_path_config(path_config):\n",
    "    \"\"\"\n",
    "    Build infos list (one entry per node/state) and reward list (one per transition)\n",
    "    We make infos length = len(path_config) + 1 (nodes), and rewards length = len(path_config) (transitions).\n",
    "    Each infos[i] has 'player' set to the to_play of the i-th tuple in path_config for the first len(path_config) entries,\n",
    "    and the last state's player set to the last tuple's to_play as well.\n",
    "    \"\"\"\n",
    "    # rewards per transition (length = len(path_config))\n",
    "    rewards = [t[1] for t in path_config]\n",
    "    # infos per state (length = len(path_config) + 1)\n",
    "    infos = [{\"player\": t[0]} for t in path_config]\n",
    "    # append final leaf state's player (use the last tuple's to_play)\n",
    "    infos.append({\"player\": path_config[-1][0]})\n",
    "    return infos, rewards\n",
    "\n",
    "\n",
    "def are_lists_roughly_equal(list1, list2):\n",
    "    \"\"\"\n",
    "    Checks if two lists of floats are roughly equal in Python 2.\n",
    "\n",
    "    Args:\n",
    "        list1: The first list of floats.\n",
    "        list2: The second list of floats.\n",
    "        tolerance: The maximum allowed absolute difference between corresponding\n",
    "                   elements for them to be considered roughly equal.\n",
    "\n",
    "    Returns:\n",
    "        True if the lists are roughly equal, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        # Compare elements using absolute tolerance\n",
    "        if not math.isclose(list1[i], list2[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"path_config, description\", test_cases)\n",
    "def test_assign_prefix_as_node_reward_and_check_child_rewards(path_config, description):\n",
    "    \"\"\"\n",
    "    For each path_config:\n",
    "      - compute n_step_rewards (value-prefix targets) using DummyMuZero._get_n_step_info\n",
    "      - expand Nodes using those prefixes as the node reward\n",
    "      - assert parent.child_reward(child) equals the prefix that we assigned to that child node\n",
    "    \"\"\"\n",
    "    Node.value_prefix = True\n",
    "    # build infos and rewards from path_config\n",
    "    infos, rewards = build_infos_and_rewards_from_path_config(path_config)\n",
    "\n",
    "    num_unroll_steps = len(path_config)  # number of transitions (edges)\n",
    "    # prepare other arrays needed by _get_n_step_info\n",
    "    values = [0.0] * (num_unroll_steps + 10)\n",
    "    policies = [torch.tensor([0.5, 0.5])] * (num_unroll_steps + 10)\n",
    "    actions = [0] * (num_unroll_steps + 10)\n",
    "\n",
    "    dummy = DummyMuZero(\n",
    "        num_actions=2,\n",
    "        num_players=max(inf[\"player\"] for inf in infos) + 1,\n",
    "        gamma=1.0,\n",
    "        value_prefix=True,\n",
    "    )\n",
    "\n",
    "    # compute n_step_rewards (prefix) for index=0\n",
    "    _, _, n_step_rewards, _, _ = dummy._get_n_step_info(\n",
    "        index=0,\n",
    "        values=values,\n",
    "        policies=policies,\n",
    "        rewards=rewards,\n",
    "        actions=actions,\n",
    "        infos=infos,\n",
    "        num_unroll_steps=num_unroll_steps,\n",
    "        n_step=num_unroll_steps,\n",
    "        lstm_horizon_len=2,\n",
    "    )\n",
    "\n",
    "    # convert to python floats for assignment\n",
    "    prefix_list = [\n",
    "        float(x) for x in n_step_rewards.tolist()\n",
    "    ]  # length = num_unroll_steps + 1\n",
    "    print(\"prefix list\", prefix_list)\n",
    "    print(\"rewards\", rewards)\n",
    "    # Build a search path where we set each node's stored reward to the corresponding prefix:\n",
    "    # - assign root the prefix_list[0], then each successive child gets prefix_list[1], prefix_list[2], ...\n",
    "    root = Node(0.0)\n",
    "    policy = torch.tensor([0.0, 1.0])\n",
    "    hidden_state = torch.tensor([1])\n",
    "    legal_moves = [0, 1]\n",
    "\n",
    "    # Expand root with root player = infos[0][\"player\"] and reward = prefix_list[0]\n",
    "    root.expand(\n",
    "        legal_moves,\n",
    "        infos[0][\"player\"],\n",
    "        policy,\n",
    "        hidden_state,\n",
    "        float(prefix_list[0]),\n",
    "        is_reset=True,\n",
    "    )\n",
    "    search_path = [root]\n",
    "    node = root.children[0]\n",
    "    horizon_len = 0\n",
    "    # expand subsequent nodes using prefix_list entries\n",
    "    for u in range(1, len(prefix_list)):\n",
    "        horizon_len = (horizon_len + 1) % 2\n",
    "        to_play = infos[u][\"player\"] if u < len(infos) else infos[-1][\"player\"]\n",
    "        node.expand(\n",
    "            legal_moves,\n",
    "            to_play,\n",
    "            policy,\n",
    "            hidden_state,\n",
    "            float(prefix_list[u]),\n",
    "            is_reset=horizon_len == 0,\n",
    "        )\n",
    "        search_path.append(node)\n",
    "        node = node.children[0]\n",
    "    # append the final leaf node (node now refers to leaf after loop)\n",
    "    # search_path.append(node)\n",
    "\n",
    "    # Now check edges: for each parent -> child, the child_reward(child) should equal the prefix we assigned to that child\n",
    "    n_edges = len(search_path) - 1\n",
    "    assert (\n",
    "        n_edges == num_unroll_steps + 1\n",
    "        or n_edges == len(prefix_list) - 1\n",
    "        or n_edges >= 1\n",
    "    )\n",
    "    # Loop edges and assert\n",
    "    predicted_reward_list = []\n",
    "    for i in range(n_edges):\n",
    "        parent = search_path[i]\n",
    "        child = search_path[i + 1]\n",
    "        pr = parent.reward\n",
    "        cr = child.reward\n",
    "        raw = parent.child_reward(child)\n",
    "        # handle tensor returns gracefully\n",
    "        if isinstance(raw, torch.Tensor):\n",
    "            child_reward_val = float(raw.item())\n",
    "        else:\n",
    "            child_reward_val = float(raw)\n",
    "        predicted_reward_list.append(child_reward_val)\n",
    "\n",
    "    print(\"predicted\", predicted_reward_list)\n",
    "    match = are_lists_roughly_equal(predicted_reward_list, rewards)\n",
    "    if match:\n",
    "        print(\" Match!\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\" Mismatch\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix list [0.0, 0.0, 1.0, 1.0]\n",
      "rewards [0.0, 1.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0]\n",
      "rewards [0.0, 1.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 2.0]\n",
      "rewards [0.0, 1.0, 1.0]\n",
      "predicted [0.0, 1.0, 1.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 2.0]\n",
      "rewards [0.0, 1.0, 1.0]\n",
      "predicted [0.0, 1.0, 1.0]\n",
      " Match!\n",
      "prefix list [0.0, 1.0, 1.0, 1.0]\n",
      "rewards [1.0, 1.0, 0.0]\n",
      "predicted [1.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 1.0, 1.0, 1.0]\n",
      "rewards [1.0, 1.0, 0.0]\n",
      "predicted [1.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0]\n",
      "rewards [0.0, 1.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "rewards [0.0, 1.0, 0.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 1.0]\n",
      "rewards [0.0, 0.0, 1.0]\n",
      "predicted [0.0, 0.0, 1.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "rewards [0.0, 0.0, 0.0, 1.0]\n",
      "predicted [0.0, 0.0, 0.0, 1.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 0.0]\n",
      "rewards [0.0, 0.0, 0.0]\n",
      "predicted [0.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "rewards [0.0, 0.0, 0.0, 0.0]\n",
      "predicted [0.0, 0.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 1.0, 1.0, 2.0]\n",
      "rewards [1.0, 1.0, 1.0]\n",
      "predicted [1.0, 1.0, 1.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 0.0]\n",
      "rewards [0.0, 0.0, 0.0]\n",
      "predicted [0.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0]\n",
      "rewards [0.0, 1.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0]\n",
      "rewards [0.0, 1.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "rewards [0.0, 1.0, 0.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 1.0, 2.0, 5.0]\n",
      "rewards [1.0, 2.0, 3.0]\n",
      "predicted [1.0, 2.0, 3.0]\n",
      " Match!\n",
      "prefix list [0.0, 1.0, 0.0, 0.0]\n",
      "rewards [1.0, 0.0, 0.0]\n",
      "predicted [1.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "rewards [0.0, 0.0, 1.0, 0.0]\n",
      "predicted [0.0, 0.0, 1.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "rewards [1.0, 0.0, 0.0, 0.0]\n",
      "predicted [1.0, 0.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "rewards [0.0, 1.0, 0.0, 0.0]\n",
      "predicted [0.0, 1.0, 0.0, 0.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "rewards [0.0, 0.0, 0.0, 1.0]\n",
      "predicted [0.0, 0.0, 0.0, 1.0]\n",
      " Match!\n",
      "prefix list [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "rewards [0.0, 0.0, 0.0, 0.0]\n",
      "predicted [0.0, 0.0, 0.0, 0.0]\n",
      " Match!\n",
      "24/24 correct\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, (path_config, description) in enumerate(test_cases, 0):\n",
    "    correct += test_assign_prefix_as_node_reward_and_check_child_rewards(\n",
    "        path_config, description\n",
    "    )\n",
    "    total += 1\n",
    "\n",
    "print(f\"{correct}/{total} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
