{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nd_scaffold import GridModule\n",
    "\n",
    "mod = GridModule(shape=(3, 3, 5), device=\"cpu:0\", T=0.1)\n",
    "\n",
    "print(\"state\", mod.state)\n",
    "print(\"one hot\", mod.onehot())\n",
    "\n",
    "mod.denoise_self()\n",
    "\n",
    "print(\"state\", mod.state)\n",
    "print(\"one hot\", mod.onehot())\n",
    "\n",
    "mod.shift(torch.tensor([1, 0, 0], device=\"cpu:0\"))\n",
    "\n",
    "print(\"state\", mod.state)\n",
    "print(\"one hot\", mod.onehot())\n",
    "\n",
    "mod.denoise_self()\n",
    "\n",
    "print(\"state\", mod.state)\n",
    "print(\"one hot\", mod.onehot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from nd_scaffold import GridScaffold, SparseMatrixBySparsityInitializer\n",
    "from graph_utils import graph_scaffold, print_imgs_side_by_side\n",
    "import os\n",
    "\n",
    "\n",
    "def test_mnist(\n",
    "    num_imgs=1,\n",
    "    prefix=\"\",\n",
    "    relu_theta=0.5,\n",
    "    sparsity=0.1,\n",
    "    N_h=400,\n",
    "    T=0.01,\n",
    "    plot_figs=False,\n",
    "):\n",
    "    import torchvision\n",
    "    from torchvision import transforms\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten())]\n",
    "    )\n",
    "\n",
    "    mnist = torchvision.datasets.MNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    mnist_data = mnist.data.flatten(1).float().to(\"cpu\")[:num_imgs]\n",
    "    mnist_data = (mnist_data - mnist_data.mean()) / mnist_data.std()\n",
    "    l = mnist_data.shape[0]\n",
    "\n",
    "    shapes = [(3, 3, 5), (4, 4, 7)]\n",
    "    velocities = torch.tile(torch.tensor([[1, 1, 1]]), (l, 1)).to(\"cpu\")\n",
    "\n",
    "    if False and os.path.exists(\"checkpoint.pt\"):\n",
    "        GS = GridScaffold.load(\"checkpoint.pt\", device=\"cpu\")\n",
    "    else:\n",
    "        GS = GridScaffold(\n",
    "            shapes=shapes,\n",
    "            N_h=N_h,\n",
    "            input_size=784,\n",
    "            device=\"cpu\",\n",
    "            sparse_matrix_initializer=SparseMatrixBySparsityInitializer(\n",
    "                sparsity=sparsity,\n",
    "                device=\"cpu\",\n",
    "            ),\n",
    "            relu_theta=relu_theta,\n",
    "            T=T,\n",
    "        )\n",
    "        # GS.checkpoint(\"checkpoint.pt\")\n",
    "\n",
    "    # graph_scaffold(GS)\n",
    "\n",
    "    # random_noise = torch.zeros_like(mnist_data).uniform_(-128, 128)\n",
    "    noisy_mnist = mnist_data  # + random_noise\n",
    "    # recalled_imgs = GS.recall(noisy_mnist)\n",
    "\n",
    "    # for i in range(1):\n",
    "    #     original_img = mnist_data[i].reshape(28, 28).cpu().numpy()\n",
    "    #     noisy_img = noisy_mnist[i].reshape(28, 28).cpu().numpy()\n",
    "    #     recalled_img = recalled_imgs[i].reshape(28, 28).cpu().numpy()\n",
    "    #     print_imgs_side_by_side(\n",
    "    #         original_img,\n",
    "    #         noisy_img,\n",
    "    #         recalled_img,\n",
    "    #         out=f\"mnist_unlearned_{i}.png\",\n",
    "    #         captions=[\"original\", \"noisy\", \"recalled\"],\n",
    "    #         title=\"Unlearned\",\n",
    "    #     )\n",
    "\n",
    "    GS.learn_path(observations=mnist_data, velocities=velocities)\n",
    "    recalled_imgs = GS.recall(noisy_mnist)\n",
    "\n",
    "    if plot_figs:\n",
    "        for i in range(1):\n",
    "            original_img = mnist_data[i].reshape(28, 28).cpu().numpy()\n",
    "            noisy_img = noisy_mnist[i].reshape(28, 28).cpu().numpy()\n",
    "            recalled_img = recalled_imgs[i].reshape(28, 28).cpu().numpy()\n",
    "            print_imgs_side_by_side(\n",
    "                original_img,\n",
    "                noisy_img,\n",
    "                recalled_img,\n",
    "                out=f\"{prefix}mnist_learned_{i}.png\",\n",
    "                captions=[\"original\", \"noisy\", \"recalled\"],\n",
    "                title=\"Learned\",\n",
    "            )\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(\n",
    "        mnist_data, GS.recall(noisy_mnist)\n",
    "    )\n",
    "    return similarity\n",
    "\n",
    "\n",
    "test_mnist(11, sparsity=0.99, N_h=1000, prefix=\"\", relu_theta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temperatures = [0.001, 0.03, 0.01, 0.1, 0.3, 1]\n",
    "N_h = [200, 400, 600, 800, 1000]\n",
    "\n",
    "scores = np.zeros((len(temperatures), len(N_h)))\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    for j, N in enumerate(N_h):\n",
    "        scores[i, j] = (\n",
    "            test_mnist(11, sparsity=0.99, N_h=N, prefix=f\"T_{T}_N_{N}_\").mean().item()\n",
    "        )\n",
    "\n",
    "print(scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(scores, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"scores.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i, T in enumerate(temperatures):\n",
    "    ax.plot(N_h, scores[i], label=f\"T={T}\")\n",
    "\n",
    "ax.set_xlabel(\"N_h\")\n",
    "ax.set_ylabel(\"Similarity\")\n",
    "ax.legend()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import spacefillingcurve\n",
    "\n",
    "modules = [(2, 3), (3, 4)]\n",
    "v = spacefillingcurve(modules)\n",
    "\n",
    "# graph walk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_walk(v, modules):\n",
    "    l2 = modules[0][0] * modules[1][0]\n",
    "    l1 = modules[0][1] * modules[1][1]\n",
    "    x = np.cumsum([0] + [i[0] for i in v]) % l1\n",
    "    y = np.cumsum([0] + [i[1] for i in v]) % l2\n",
    "\n",
    "    # plot arrows\n",
    "\n",
    "    c = plt.scatter(x, y, c=range(len(x)), cmap=\"viridis\", s=20)\n",
    "    for i in range(len(x) - 1):\n",
    "        plt.arrow(\n",
    "            x[i],\n",
    "            y[i],\n",
    "            x[i + 1] - x[i],\n",
    "            y[i + 1] - y[i],\n",
    "            head_width=0.2,\n",
    "            head_length=0.2,\n",
    "            length_includes_head=True,\n",
    "        )\n",
    "    plt.colorbar(c)\n",
    "\n",
    "\n",
    "print(len(v))\n",
    "plot_walk(v, modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [0.01, 0.03, 0.1, 0.3, 1]\n",
    "N_h = [500, 600, 700, 800, 900]\n",
    "\n",
    "scores = np.zeros((len(T), len(N_h)))\n",
    "\n",
    "for i, t in enumerate(T):\n",
    "    for j, dim in enumerate(N_h):\n",
    "        scores[i, j] = (\n",
    "            test_mnist(\n",
    "                100, sparsity=0.99, N_h=dim, prefix=f\"T_{t}_N_{dim}_\", plot_figs=False\n",
    "            )\n",
    "            .mean()\n",
    "            .item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [0.01, 0.03, 0.1, 0.3, 1]\n",
    "N_h = [500, 600, 700, 800, 900]\n",
    "fig, ax = plt.subplots()\n",
    "for i, t in enumerate(T):\n",
    "    ax.plot(N_h, scores[i], label=f\"T={t}\")\n",
    "\n",
    "ax.set_xlabel(\"N_h\")\n",
    "ax.set_ylabel(\"Similarity\")\n",
    "ax.legend()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nd_scaffold import GridScaffold, SparseMatrixBySparsityInitializer\n",
    "from matrix_initializers import SparseMatrixByScalingInitializer\n",
    "import torchvision\n",
    "from vectorhash_functions import calculate_big_theta, calculate_relu_theta, solve_mean\n",
    "from torchvision import transforms\n",
    "from graph_utils import print_imgs_side_by_side, graph_scaffold\n",
    "from vectorhash_functions import spacefillingcurve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def test_mnist2(\n",
    "    percent=1,\n",
    "    prefix=\"\",\n",
    "    relu_theta=0.5,\n",
    "    sparsity=0.1,\n",
    "    N_h=400,\n",
    "    T=0.01,\n",
    "    plot_figs=False,\n",
    "    plot_scaffold=False,\n",
    "    continualupdate_=False,\n",
    "    ratshift_=True,\n",
    "    sparsitymethod=0,\n",
    "    ratio=0.1,\n",
    "):\n",
    "    os.makedirs(\"mnist_test_2\", exist_ok=True)\n",
    "\n",
    "    shapes = [(3, 3), (4, 4)]\n",
    "    velocities = spacefillingcurve(shapes)\n",
    "    velocities = velocities[:133]\n",
    "    print(\"imgs:    \", int(len(velocities) * percent))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten())]\n",
    "    )\n",
    "\n",
    "    mnist = torchvision.datasets.MNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    mnist_data = mnist.data.flatten(1).float()[: int(len(velocities) * percent)]\n",
    "    mnist_data = (mnist_data - mnist_data.mean()) / mnist_data.std()\n",
    "    l = mnist_data.shape[0]\n",
    "\n",
    "    relu_theta_ = calculate_relu_theta(\n",
    "        modules=len(shapes), sparsity=1 - sparsity, target_prob=0.1\n",
    "    )\n",
    "    print(\"relu_theta\", relu_theta_)\n",
    "    relu_big_theta = calculate_big_theta(len(shapes), sparsity=sparsity, targetp=0.2)\n",
    "    print(\"relu_big_theta\", relu_big_theta)\n",
    "\n",
    "    # plot normal distribution with mean relu_theta and std 1 and plot relu big theta\n",
    "    x = torch.linspace(-5, 5, 100)\n",
    "    plt.plot(x, norm.pdf(x, relu_theta_, math.sqrt(len(shapes) * (1 - sparsity))))\n",
    "    plt.plot(x, norm.pdf(x, -relu_big_theta, math.sqrt(len(shapes) * (sparsity))))\n",
    "    print(\"what are 10 percent of numbers going to be below? 1: ezra, 2: johnny\")\n",
    "    print(norm(-relu_theta_, math.sqrt(len(shapes) * (sparsity))).cdf(0))\n",
    "    print(norm(-relu_big_theta, math.sqrt(len(shapes) * (1 - sparsity))).cdf(0))\n",
    "    relu_new = solve_mean(p=ratio, var=len(shapes) * (1 - sparsity))\n",
    "    print(norm(-relu_new, math.sqrt(len(shapes) * (1 - sparsity))).cdf(0))\n",
    "    ####\n",
    "    ### TESTING OTHER GROUP MATRIX INITIALIZERS\n",
    "    ####\n",
    "    k = 2 * len(shapes)\n",
    "    c = 0.8\n",
    "    var = 1.0\n",
    "    print(\"SPARSITY VARSITY BANANA LAME \")\n",
    "    a = SparseMatrixBySparsityInitializer(sparsity=sparsity)\n",
    "    print((a((N_h, sum([module[0] * module[1] for module in shapes])))))\n",
    "    if False and os.path.exists(\"checkpoint.pt\"):\n",
    "        GS = GridScaffold.load(\"checkpoint.pt\", device=\"cpu\")\n",
    "    else:\n",
    "        GS = GridScaffold(\n",
    "            shapes=shapes,\n",
    "            N_h=N_h,\n",
    "            input_size=784,\n",
    "            device=None,\n",
    "            sparse_matrix_initializer=(\n",
    "                SparseMatrixByScalingInitializer(\n",
    "                    mean=relu_theta_, scale=var / math.sqrt(k)\n",
    "                )\n",
    "                if sparsitymethod == 1\n",
    "                else SparseMatrixBySparsityInitializer(sparsity=sparsity)\n",
    "            ),\n",
    "            relu_theta=(-relu_new) if sparsitymethod == 0 else relu_theta,\n",
    "            T=T,\n",
    "            continualupdate=continualupdate_,\n",
    "            ratshift=ratshift_,\n",
    "        )\n",
    "\n",
    "    if plot_scaffold:\n",
    "        graph_scaffold(GS, dir=\"mnist_test_2\")\n",
    "        # GS.checkpoint(\"checkpoint.pt\")\n",
    "\n",
    "    # graph_scaffold(GS)\n",
    "\n",
    "    # random_noise = torch.zeros_like(mnist_data).uniform_(-128, 128)\n",
    "    noisy_mnist = mnist_data  # + random_noise\n",
    "    # recalled_imgs = GS.recall(noisy_mnist)\n",
    "\n",
    "    # for i in range(1):\n",
    "    #     original_img = mnist_data[i].reshape(28, 28).cpu().numpy()\n",
    "    #     noisy_img = noisy_mnist[i].reshape(28, 28).cpu().numpy()\n",
    "    #     recalled_img = recalled_imgs[i].reshape(28, 28).cpu().numpy()\n",
    "    #     print_imgs_side_by_side(\n",
    "    #         original_img,\n",
    "    #         noisy_img,\n",
    "    #         recalled_img,\n",
    "    #         out=f\"mnist_unlearned_{i}.png\",\n",
    "    #         captions=[\"original\", \"noisy\", \"recalled\"],\n",
    "    #         title=\"Unlearned\",\n",
    "    #     )\n",
    "\n",
    "    GS.learn_path(observations=mnist_data, velocities=velocities)\n",
    "    recalled_imgs = GS.recall(noisy_mnist)\n",
    "\n",
    "    if plot_figs:\n",
    "        for i in range(3):\n",
    "            original_img = mnist_data[i].reshape(28, 28).cpu().numpy()\n",
    "            noisy_img = noisy_mnist[i].reshape(28, 28).cpu().numpy()\n",
    "            recalled_img = recalled_imgs[i].reshape(28, 28).cpu().numpy()\n",
    "            print_imgs_side_by_side(\n",
    "                original_img,\n",
    "                noisy_img,\n",
    "                recalled_img,\n",
    "                out=f\"mnist_test_2/{prefix}mnist2_LRND_{i}.png\",\n",
    "                captions=[\"original\", \"noisy\", \"recalled\"],\n",
    "                title=\"Learned\",\n",
    "            )\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(\n",
    "        mnist_data, GS.recall(noisy_mnist)\n",
    "    )\n",
    "    return similarity, GS\n",
    "\n",
    "\n",
    "scores, GS = test_mnist2(\n",
    "    percent=1,\n",
    "    sparsity=0,\n",
    "    N_h=1000,\n",
    "    prefix=\"\",\n",
    "    relu_theta=0,\n",
    "    T=0.0000001,\n",
    "    plot_figs=True,\n",
    "    plot_scaffold=True,\n",
    "    continualupdate_=True,\n",
    "    ratshift_=False,\n",
    "    sparsitymethod=0,\n",
    "    ratio=0.005,\n",
    ")\n",
    "print(scores.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import solve_mean, calculate_big_theta, calculate_relu_theta\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "sparsity = 0.9\n",
    "relu_theta_ = calculate_relu_theta(\n",
    "    modules=len(shapes), sparsity=1 - sparsity, target_prob=0.1\n",
    ")\n",
    "print(\"relu_theta\", relu_theta_)\n",
    "relu_big_theta = calculate_big_theta(len(shapes), sparsity=sparsity, targetp=0.1)\n",
    "print(\"relu_big_theta\", relu_big_theta)\n",
    "relu_new = solve_mean(p=0.2, var=len(shapes) * (1 - sparsity))\n",
    "print(\"relu_new\", relu_new)\n",
    "# plot normal distribution with mean relu_theta and std 1 and plot relu big theta\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "plt.plot(x, norm.pdf(x, relu_theta_, math.sqrt(len(shapes) * (1 - sparsity))))\n",
    "plt.plot(x, norm.pdf(x, -relu_big_theta, math.sqrt(len(shapes) * (sparsity))))\n",
    "print(\"what are 10 percent of numbers going to be below? 1: ezra, 2: johnny\")\n",
    "print(norm(-relu_theta_, math.sqrt(len(shapes) * (sparsity))).cdf(0))\n",
    "print(norm(-relu_big_theta, math.sqrt(len(shapes) * (1 - sparsity))).cdf(0))\n",
    "print(norm(relu_new, math.sqrt(len(shapes) * (1 - sparsity))).cdf(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
