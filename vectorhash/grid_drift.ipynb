{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 0.18819222895762647 0.31622776601683794 0.37638445791525293 0.447213595499958 -1.1291533737457589\n",
      "module shapes:  [(2, 2), (3, 3)]\n",
      "N_g     :  13\n",
      "N_patts :  36\n",
      "N_h     :  1200\n"
     ]
    }
   ],
   "source": [
    "from vectorhash_imported import *\n",
    "from vectorhash_convered import *\n",
    "from nd_scaffold import GridScaffold\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "lambdas = [2,3]\n",
    "shapes = [(i, i) for i in lambdas]\n",
    "percent_nonzero_relu = 0.8\n",
    "W_gh_var = 0.1\n",
    "sparse_initialization = 0.1\n",
    "T = 0.01 \n",
    "W_hg_std = math.sqrt(W_gh_var)\n",
    "W_hg_mean = -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(lambdas))\n",
    "h_normal_mean = len(lambdas) * W_hg_mean\n",
    "h_normal_std = math.sqrt(len(lambdas)) * W_hg_std\n",
    "relu_theta = math.sqrt((1 - sparse_initialization) * len(lambdas)) * norm.ppf(\n",
    "    1 - percent_nonzero_relu\n",
    ")\n",
    "num_imgs = 5\n",
    "\n",
    "print(percent_nonzero_relu, W_hg_mean, W_hg_std, h_normal_mean, h_normal_std, relu_theta)\n",
    "\n",
    "GS = GridScaffold(\n",
    "    shapes=shapes,\n",
    "    N_h=1200,\n",
    "    input_size=5,\n",
    "    h_normal_mean=h_normal_mean,\n",
    "    h_normal_std=h_normal_std,\n",
    "    device=None,\n",
    "    sparse_matrix_initializer=SparseMatrixBySparsityInitializer(\n",
    "        sparsity=sparse_initialization, device=\"cpu\"\n",
    "    ),\n",
    "    relu_theta=relu_theta, ######\n",
    "    from_checkpoint=False,\n",
    "    T=T,\n",
    "    continualupdate=False,\n",
    "    ratshift=False,\n",
    "    initialize_W_gh_with_zeroes=False,\n",
    "    pseudo_inverse=False,\n",
    "    batch_update=False,\n",
    "    use_h_fix=False,\n",
    "    learned_pseudo=True,\n",
    "    epsilon=0.01,\n",
    "    calculate_update_scaling_method=\"n_h\",\n",
    "    MagicMath=False,\n",
    "    sanity_check=True,\n",
    "    calculate_g_method=\"fast\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=5,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"medium\",\n",
    "    use_fix=False,\n",
    "):\n",
    "    import torch\n",
    "    import random\n",
    "\n",
    "    data = dataset.data\n",
    "    # print(num_imgs)\n",
    "    # print(data.shape)\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    # print(data.shape)\n",
    "    data = torch.tensor(data[:num_imgs]).float().to(\"cpu\")\n",
    "    # print(data.shape)\n",
    "\n",
    "    # data = random.sample(dataset.data.flatten(1).float().to(\"cpu\"), num_imgs)\n",
    "    if preprocess_sensory:\n",
    "        if use_fix:\n",
    "            for i in range(len(data)):\n",
    "                data[i] = (data[i] - data[i].mean()) / data[i].std()\n",
    "\n",
    "        data = (data - data.mean()) / data.std()\n",
    "    # noissing the data\n",
    "    if noise_level == \"none\":\n",
    "        return data, data\n",
    "    elif noise_level == \"low\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1, 1)\n",
    "    elif noise_level == \"medium\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.25, 1.25)\n",
    "    elif noise_level == \"high\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.5, 1.5)\n",
    "    noisy_data = data + random_noise\n",
    "\n",
    "    return data, noisy_data\n",
    "\n",
    "def prepare_data_random(noise_scale=0.1):\n",
    "    data = torch.randn((num_imgs, 5))\n",
    "    noise = noise_scale * torch.randn((num_imgs, 5))\n",
    "    return data, data + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ezrahuang/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/nd_scaffold.py:548: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
      "  1 + input.T @ self.inhibition_matrix_hs @ input\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Whs should be the pseudo-inverse of Wsh. Got tensor([0.0000, 0.0000, 0.0155,  ..., 0.0060, 0.0072, 0.0071]) and expected tensor([0.0000, 0.0000, 3.4812,  ..., 1.3494, 1.6256, 1.6081])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m data, noisy_data \u001b[38;5;241m=\u001b[39mprepare_data_random(noise_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \n\u001b[1;32m     23\u001b[0m v \u001b[38;5;241m=\u001b[39m spacefillingcurve(shapes)\n\u001b[0;32m---> 25\u001b[0m g_points, g_points2 \u001b[38;5;241m=\u001b[39m \u001b[43mGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m recalled_imgs \u001b[38;5;241m=\u001b[39m GS\u001b[38;5;241m.\u001b[39mrecall(noisy_data)\n\u001b[1;32m     27\u001b[0m similarity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcosine_similarity(data, recalled_imgs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rl-research/vectorhash/nd_scaffold.py:792\u001b[0m, in \u001b[0;36mGridScaffold.learn_path\u001b[0;34m(self, observations, velocities)\u001b[0m\n\u001b[1;32m    790\u001b[0m seen_gs\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m    791\u001b[0m seen_hs\u001b[38;5;241m.\u001b[39madd(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hg \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu_theta))\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# testing code\u001b[39;00m\n\u001b[1;32m    795\u001b[0m g_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_position(first_obs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rl-research/vectorhash/nd_scaffold.py:829\u001b[0m, in \u001b[0;36mGridScaffold.learn\u001b[0;34m(self, observation, velocity)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation, velocity):\n\u001b[1;32m    824\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add a memory to the memory scaffold and shift the grid coding state by a given velocity.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m    observation shape: `(input_size)`\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    velocity shape: `(D)` where `D` is the dimensionality of the grid modules.\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift(velocity)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rl-research/vectorhash/nd_scaffold.py:686\u001b[0m, in \u001b[0;36mGridScaffold.store_memory\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_update(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39ms, output\u001b[38;5;241m=\u001b[39mh)\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_sh \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_update_Wsh_fix(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mh, output\u001b[38;5;241m=\u001b[39ms)\n\u001b[0;32m--> 686\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hs \u001b[38;5;241m@\u001b[39m s, h\n\u001b[1;32m    688\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhs should be the pseudo-inverse of Wsh. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hs\u001b[38;5;250m \u001b[39m\u001b[38;5;241m@\u001b[39m\u001b[38;5;250m \u001b[39ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_sh \u001b[38;5;241m@\u001b[39m h, s\n\u001b[1;32m    692\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWsh should be the pseudo-inverse of Whs. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_sh\u001b[38;5;250m \u001b[39m\u001b[38;5;241m@\u001b[39m\u001b[38;5;250m \u001b[39m((h\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_h)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_h_fix\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mh)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Whs should be the pseudo-inverse of Wsh. Got tensor([0.0000, 0.0000, 0.0155,  ..., 0.0060, 0.0072, 0.0071]) and expected tensor([0.0000, 0.0000, 3.4812,  ..., 1.3494, 1.6256, 1.6081])"
     ]
    }
   ],
   "source": [
    "from vectorhash_functions import solve_mean, spacefillingcurve\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten())]\n",
    ")\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# data, noisy_data = prepare_data(\n",
    "#     dataset,\n",
    "#     num_imgs=num_imgs,\n",
    "#     preprocess_sensory=True,\n",
    "#     noise_level=\"none\",\n",
    "#     use_fix=True\n",
    "# )\n",
    "data, noisy_data =prepare_data_random(noise_scale=0.1) \n",
    "\n",
    "v = spacefillingcurve(shapes)\n",
    "\n",
    "g_points, g_points2 = GS.learn_path(observations=data, velocities=v[: len(data)])\n",
    "recalled_imgs = GS.recall(noisy_data)\n",
    "similarity = torch.cosine_similarity(data, recalled_imgs, dim=1)\n",
    "print(similarity.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import *\n",
    "\n",
    "mod_n_positions = []\n",
    "mod_n_positions_2 = []\n",
    "\n",
    "# assume shapes are squares\n",
    "\n",
    "for _ in lambdas:\n",
    "    mod_n_positions.append(list())\n",
    "    mod_n_positions_2.append(list())\n",
    "\n",
    "for g in g_points:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos:pos + l**2]\n",
    "        mod_n_positions[i].append(ConvertToXYNew(mod_n, (l,l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_points2:\n",
    "    pos = 0\n",
    "    for i, l in  enumerate(lambdas):\n",
    "        mod_n = g[pos:pos + l**2]\n",
    "        mod_n_positions_2[i].append(ConvertToXYNew(mod_n, (l,l)))\n",
    "        pos += l**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "  GraphGrid((l,l), mod_n_positions[i], first_point=mod_n_positions[i][0], title=f\"estimated {l}x{l} grid position of first image over time. \\nN_h={GS.N_h}, sparsity={sparse_initialization}, relu_theta={relu_theta}, W_hg_mean={W_hg_mean}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\")\n",
    "  GraphGrid((l,l), mod_n_positions_2[i], first_point=mod_n_positions_2[i][0], title=f\"estimated {l}x{l} grid position of second image over time. \\nN_h={GS.N_h}, sparsity={sparse_initialization}, relu_theta={relu_theta}, W_hg_mean={W_hg_mean}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph estimated position of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_positions_n = []\n",
    "for l in lambdas:\n",
    "    point_positions_n.append([])\n",
    "\n",
    "for img in data:\n",
    "    pos = GS.estimate_position(img)\n",
    "\n",
    "    p=0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = pos[p:p+l**2]\n",
    "        point_positions_n[i].append(ConvertToXYNew(mod_n, (l,l)))\n",
    "        p += l**2\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrid((l,l), point_positions_n[i], title=f\"estimated {l}x{l} grid position of each image. \\nN_h={GS.N_h}, sparsity={sparse_initialization}, relu_theta={relu_theta}, W_hg_mean={W_hg_mean}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data[1]\n",
    "recalled = GS.recall(img)\n",
    "print(torch.cosine_similarity(img, recalled, dim=0).mean())\n",
    "\n",
    "plt.imshow(img.reshape(5,1))\n",
    "plt.show()\n",
    "plt.imshow(recalled.reshape(5,1))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
