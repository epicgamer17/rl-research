{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_imported import *\n",
    "from vectorhash_convered import *\n",
    "from nd_scaffold import GridScaffold\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "lambdas = [3, 4, 5]\n",
    "shapes = [(i, i) for i in lambdas]\n",
    "percent_nonzero_relu = 0.5  #\n",
    "W_gh_var = 1\n",
    "sparse_initialization = 0.1\n",
    "T = 0.1\n",
    "W_hg_std = math.sqrt(W_gh_var)\n",
    "W_hg_mean = -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(lambdas))\n",
    "h_normal_mean = len(lambdas) * W_hg_mean\n",
    "h_normal_std = math.sqrt(len(lambdas)) * W_hg_std\n",
    "relu_theta = math.sqrt((1 - sparse_initialization) * len(lambdas)) * norm.ppf(\n",
    "    1 - percent_nonzero_relu\n",
    ")\n",
    "num_imgs = 200\n",
    "\n",
    "print(\n",
    "    percent_nonzero_relu, W_hg_mean, W_hg_std, h_normal_mean, h_normal_std, relu_theta\n",
    ")\n",
    "\n",
    "GS = GridScaffold(\n",
    "    shapes=shapes,\n",
    "    N_h=1000,\n",
    "    input_size=784,\n",
    "    device=None,\n",
    "    learned_pseudo=\"bidirectional\",\n",
    "    hidden_layer_factor=3,\n",
    "    stationary=True,\n",
    "    # sparse_matrix_initializer=SparseMatrixBySparsityInitializer(\n",
    "    #     sparsity=sparse_initialization, device=\"cpu\"\n",
    "    # ),\n",
    "    # relu_theta=0,\n",
    "    sparse_matrix_initializer=SparseMatrixByScalingInitializer(\n",
    "        scale=W_hg_std, mean=W_hg_mean, device=\"cpu\"\n",
    "    ),\n",
    "    relu_theta=relu_theta,\n",
    "    T=T,\n",
    "    # h fix\n",
    "    calculate_update_scaling_method=\"n_h\",\n",
    "    use_h_fix=False,  # true if norm scaling, false 25/32, true 21/32 11/32\n",
    "    h_normal_mean=h_normal_mean,\n",
    "    h_normal_std=h_normal_std,\n",
    "    # epsilon=0.01,\n",
    "    scaling_updates=False,  # only relevant when using hebbian false 21/32, true 15/32 7/32\n",
    "    sanity_check=False,\n",
    "    # dream_fix=1,\n",
    "    # zero_tol=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import spacefillingcurve\n",
    "from data_utils import prepare_data, load_mnist_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_mnist_dataset()\n",
    "data, noisy_data = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=num_imgs,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"none\",\n",
    "    across_dataset=True,\n",
    ")\n",
    "\n",
    "# REMOVE THESE LINES\n",
    "# data = data.float() * input_scale_factor_optimal\n",
    "# noisy_data = noisy_data.float() * input_scale_factor_optimal\n",
    "\n",
    "# data, noisy_data = prepare_data_random(noise_scale=0)\n",
    "v = spacefillingcurve(shapes)\n",
    "\n",
    "g_positions, g_positions2, g_points, g_points_2 = GS.learn_path(\n",
    "    observations=data, velocities=v[: len(data)]\n",
    ")\n",
    "recalled_imgs = GS.recall(noisy_data)\n",
    "similarity = torch.cosine_similarity(data, recalled_imgs, dim=1)\n",
    "mse = torch.nn.functional.mse_loss(data, recalled_imgs, reduction=\"none\")\n",
    "print(mse.mean())\n",
    "print(similarity.mean())\n",
    "for i in range(min(num_imgs, 5)):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    im1 = ax[0].imshow(data[(-i)].reshape(28, 28), cmap=\"gray\")\n",
    "    ax[0].set_title(\"Original\")\n",
    "    im2 = ax[1].imshow((recalled_imgs[(-i)]).reshape(28, 28), cmap=\"gray\")\n",
    "    title = f\"Recalled similarity: {similarity[(-i)].item()}\"\n",
    "    ax[1].set_title(title)\n",
    "\n",
    "    fig.colorbar(im1, ax=ax[0])\n",
    "    fig.colorbar(im2, ax=ax[1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# BARCHART\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "# put legend outside of the plot\n",
    "labels = dataset.train_labels[:num_imgs]\n",
    "unique_labels = np.unique(labels)\n",
    "similarity_per_label = []\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    similarity_per_label.append(similarity[idx].mean())\n",
    "# make bars not overlap\n",
    "label = f\"percent_nonzero_relu={percent_nonzero_relu} sparsity={sparse_initialization}\"\n",
    "ax.bar(unique_labels, similarity_per_label, label=label)\n",
    "ax.set_title(\"Similarity per label\")\n",
    "ax.set_xlabel(\"Label\")\n",
    "ax.set_ylabel(\"Similarity\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LINEPLOT\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "\n",
    "label = f\"percent_nonzero_relu={percent_nonzero_relu} sparsity={sparse_initialization}\"\n",
    "ax.plot(similarity, label=label)\n",
    "ax.set_title(\"Similarity per image\")\n",
    "ax.set_xlabel(\"Image\")\n",
    "ax.set_ylabel(\"Similarity\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data.mean(dim=0).reshape(28, 28), cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "print(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import *\n",
    "\n",
    "mod_n_positions = []\n",
    "mod_n_positions_2 = []\n",
    "mod_n_states = []\n",
    "mod_n_states_2 = []\n",
    "\n",
    "# assume shapes are squares\n",
    "\n",
    "for _ in lambdas:\n",
    "    mod_n_positions.append(list())\n",
    "    mod_n_positions_2.append(list())\n",
    "    mod_n_states.append(list())\n",
    "    mod_n_states_2.append(list())\n",
    "\n",
    "for g in g_positions:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_positions[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_positions2:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_positions_2[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_points:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_states[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_points_2:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_states_2[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrids(\n",
    "        shapes_lists=[(l, l), (l, l), (l, l), (l, l)],\n",
    "        points_lists=[\n",
    "            mod_n_positions[i],\n",
    "            mod_n_positions_2[i],\n",
    "            mod_n_states[i],\n",
    "            mod_n_states_2[i],\n",
    "        ],\n",
    "        first_points=[\n",
    "            mod_n_positions[i][0],\n",
    "            mod_n_positions_2[i][0],\n",
    "            mod_n_states[i][0],\n",
    "            mod_n_states_2[i][0],\n",
    "        ],\n",
    "        titles=[\n",
    "            f\"estimated grid position of first image\",\n",
    "            f\"estimated grid position of second image\",\n",
    "            f\"denoised grid state of first image\",\n",
    "            f\"denoised grid state of second image\",\n",
    "        ],\n",
    "        main_title=f\"estimated grid positions and denoised grid states of first and second images as the number of images learned increases. \\n{l}x{l} module, N_h={GS.N_h}, sparsity={round(sparse_initialization, 2)}, relu_theta={round(relu_theta, 2)}, W_hg_mean={round(W_hg_mean, 2)}, W_hg_std={W_hg_std} shapes={shapes}, num_imgs={num_imgs}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_positions_n = []\n",
    "point_states_n = []\n",
    "for l in lambdas:\n",
    "    point_positions_n.append([])\n",
    "    point_states_n.append([])\n",
    "\n",
    "for img in data:\n",
    "    pos = GS.estimate_position(img)\n",
    "    state = GS.denoise(\n",
    "        GS.grid_from_hippocampal(GS.hippocampal_from_sensory(img))\n",
    "    ).flatten()\n",
    "\n",
    "    p = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        point_positions_n[i].append(ConvertToXYNew(pos[p : p + l**2], (l, l)))\n",
    "        point_states_n[i].append(ConvertToXYNew(state[p : p + l**2], (l, l)))\n",
    "        p += l**2\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrids(\n",
    "        shapes_lists=[(l, l), (l, l)],\n",
    "        points_lists=[point_positions_n[i], point_states_n[i]],\n",
    "        first_points=[point_positions_n[i][0], point_states_n[i][0]],\n",
    "        titles=[\n",
    "            f\"estimated {l}x{l} grid position of each image.\",\n",
    "            f\"denoised {l}x{l} grid state of each image.\",\n",
    "        ],\n",
    "        main_title=f\"estimated positions and denoised grid states of each image.\\n {l}x{l} module, N_h={GS.N_h}, sparsity={round(sparse_initialization, 2)}, relu_theta={round(relu_theta,2 )}, W_hg_mean={round(W_hg_mean, 2)}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data[0]\n",
    "recalled = GS.recall(img)\n",
    "print(torch.cosine_similarity(img, recalled).mean())\n",
    "\n",
    "plt.imshow(img.reshape(28, 28))\n",
    "print(img.mean())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow((recalled).reshape(28, 28))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    GS.sensory_from_hippocampal(GS.hippocampal_from_grid(GS.G[3200])).reshape(28, 28)\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = (dataset.train_labels == 0).nonzero().flatten()\n",
    "\n",
    "data_by_class = dataset.data[zero_indices][:200].reshape(200, 784)\n",
    "data_by_class = torch.tensor(data).float().to(\"cpu\")\n",
    "data_by_class = (data - data.mean(dim=0)) / (data.std(dim=0) + 1e-8)\n",
    "\n",
    "plt.imshow(data_by_class[0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(data_by_class[1].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(data_by_class.mean(dim=0).reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "point_positions_n = []\n",
    "point_states_n = []\n",
    "for l in lambdas:\n",
    "    point_positions_n.append([])\n",
    "    point_states_n.append([])\n",
    "\n",
    "for img in data_by_class:\n",
    "    pos = GS.estimate_position(img)\n",
    "    state = GS.denoise(\n",
    "        GS.grid_from_hippocampal(GS.hippocampal_from_sensory(img))\n",
    "    ).flatten()\n",
    "\n",
    "    p = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        point_positions_n[i].append(ConvertToXYNew(pos[p : p + l**2], (l, l)))\n",
    "        point_states_n[i].append(ConvertToXYNew(state[p : p + l**2], (l, l)))\n",
    "        p += l**2\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrids(\n",
    "        shapes_lists=[(l, l), (l, l)],\n",
    "        points_lists=[point_positions_n[i], point_states_n[i]],\n",
    "        first_points=[point_positions_n[i][0], point_states_n[i][0]],\n",
    "        titles=[\n",
    "            f\"estimated {l}x{l} grid position of each image.\",\n",
    "            f\"denoised {l}x{l} grid state of each image.\",\n",
    "        ],\n",
    "        main_title=f\"estimated positions and denoised grid states of each image.\\n {l}x{l} module, N_h={GS.N_h}, sparsity={round(sparse_initialization, 2)}, relu_theta={round(relu_theta,2 )}, W_hg_mean={round(W_hg_mean, 2)}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import spacefillingcurve\n",
    "from data_utils import prepare_data, load_mnist_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_mnist_dataset()\n",
    "data, noisy_data = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=num_imgs,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"none\",\n",
    "    across_dataset=True,\n",
    ")\n",
    "# data, noisy_data = prepare_data_random(noise_scale=0)\n",
    "params = [\n",
    "    [0.01, 0.8],\n",
    "    [0.02, 0.8],\n",
    "    [0.03, 0.8],\n",
    "]  # (percent_nonzero_relu, sparse_initialization)\n",
    "sims = []\n",
    "v = spacefillingcurve(shapes)\n",
    "\n",
    "lambdas = [3, 4, 5]\n",
    "shapes = [(i, i) for i in lambdas]\n",
    "for percent_nonzero_relu, sparse_initialization in params:\n",
    "    W_gh_var = 1\n",
    "    T = 0.01\n",
    "    W_hg_std = math.sqrt(W_gh_var)\n",
    "    W_hg_mean = -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(lambdas))\n",
    "    h_normal_mean = len(lambdas) * W_hg_mean\n",
    "    h_normal_std = math.sqrt(len(lambdas)) * W_hg_std\n",
    "    relu_theta = math.sqrt((1 - sparse_initialization) * len(lambdas)) * norm.ppf(\n",
    "        1 - percent_nonzero_relu\n",
    "    )\n",
    "    num_imgs = 201\n",
    "\n",
    "    print(\n",
    "        percent_nonzero_relu,\n",
    "        W_hg_mean,\n",
    "        W_hg_std,\n",
    "        h_normal_mean,\n",
    "        h_normal_std,\n",
    "        relu_theta,\n",
    "    )\n",
    "\n",
    "    GS = GridScaffold(\n",
    "        shapes=shapes,\n",
    "        N_h=1700,\n",
    "        input_size=784,\n",
    "        h_normal_mean=h_normal_mean,\n",
    "        h_normal_std=h_normal_std,\n",
    "        device=None,\n",
    "        sparse_matrix_initializer=SparseMatrixBySparsityInitializer(\n",
    "            sparsity=sparse_initialization, device=\"cpu\"\n",
    "        ),\n",
    "        relu_theta=relu_theta,  ######\n",
    "        from_checkpoint=False,\n",
    "        T=T,\n",
    "        ratshift=False,\n",
    "        pseudo_inverse=False,\n",
    "        batch_update=False,\n",
    "        use_h_fix=False,\n",
    "        learned_pseudo=True,\n",
    "        epsilon=0.01,\n",
    "        calculate_update_scaling_method=\"norm\",\n",
    "        MagicMath=False,\n",
    "        sanity_check=False,\n",
    "        calculate_g_method=\"fast\",\n",
    "        scaling_updates=False,\n",
    "        dream_fix=None,\n",
    "        ZeroTol=1,\n",
    "    )\n",
    "\n",
    "    g_positions, g_positions2, g_points, g_points_2 = GS.learn_path(\n",
    "        observations=data, velocities=v[: len(data)]\n",
    "    )\n",
    "    recalled_imgs = GS.recall(noisy_data)\n",
    "    similarity = torch.cosine_similarity(data, recalled_imgs, dim=1)\n",
    "    sims.append(similarity)\n",
    "    print(similarity.mean())\n",
    "    for i in range(20):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        im1 = ax[0].imshow(data[(-i)].reshape(28, 28), cmap=\"gray\")\n",
    "        ax[0].set_title(\"Original\")\n",
    "        im2 = ax[1].imshow(recalled_imgs[(-i)].reshape(28, 28), cmap=\"gray\")\n",
    "        title = f\"Recalled; percent_nonzero_relu={percent_nonzero_relu}; sparsity={sparse_initialization}\"\n",
    "        ax[1].set_title(title)\n",
    "\n",
    "        fig.colorbar(im1, ax=ax[0])\n",
    "        fig.colorbar(im2, ax=ax[1])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# BARCHART\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "# put legend outside of the plot\n",
    "for i in range(len(sims)):\n",
    "    similarity = sims[i]\n",
    "    percent_nonzero_relu, sparse_initialization = params[i]\n",
    "\n",
    "    labels = dataset.train_labels[:num_imgs]\n",
    "    unique_labels = np.unique(labels)\n",
    "    similarity_per_label = []\n",
    "    for label in unique_labels:\n",
    "        idx = labels == label\n",
    "        similarity_per_label.append(similarity[idx].mean())\n",
    "    # make bars not overlap\n",
    "    label = (\n",
    "        f\"percent_nonzero_relu={percent_nonzero_relu} sparsity={sparse_initialization}\"\n",
    "    )\n",
    "    ax.bar(unique_labels, similarity_per_label, label=label)\n",
    "    ax.set_title(\"Similarity per label\")\n",
    "    ax.set_xlabel(\"Label\")\n",
    "    ax.set_ylabel(\"Similarity\")\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LINEPLOT\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "for i in range(len(sims)):\n",
    "    similarity = sims[i]\n",
    "    percent_nonzero_relu, sparse_initialization = params[i]\n",
    "    label = (\n",
    "        f\"percent_nonzero_relu={percent_nonzero_relu} sparsity={sparse_initialization}\"\n",
    "    )\n",
    "    ax.plot(similarity, label=label)\n",
    "    ax.set_title(\"Similarity per image\")\n",
    "    ax.set_xlabel(\"Image\")\n",
    "    ax.set_ylabel(\"Similarity\")\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
