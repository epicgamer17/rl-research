{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_imported import *\n",
    "from vectorhash_convered import *\n",
    "from nd_scaffold import GridScaffold\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "lambdas = [3,4,5]\n",
    "shapes = [(i, i) for i in lambdas]\n",
    "percent_nonzero_relu = 0.99\n",
    "W_gh_var = 1\n",
    "sparse_initialization = 0.1\n",
    "T = 0.00005\n",
    "W_hg_std = math.sqrt(W_gh_var)\n",
    "W_hg_mean = -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(lambdas))\n",
    "h_normal_mean = len(lambdas) * W_hg_mean\n",
    "h_normal_std = math.sqrt(len(lambdas)) * W_hg_std\n",
    "relu_theta = math.sqrt((1 - sparse_initialization) * len(lambdas)) * norm.ppf(\n",
    "    1 - percent_nonzero_relu\n",
    ")\n",
    "num_imgs = 201\n",
    "\n",
    "print(percent_nonzero_relu, W_hg_mean, W_hg_std, h_normal_mean, h_normal_std, relu_theta)\n",
    "\n",
    "GS = GridScaffold(\n",
    "    shapes=shapes,\n",
    "    N_h=1200,\n",
    "    input_size=784,\n",
    "    h_normal_mean=h_normal_mean,\n",
    "    h_normal_std=h_normal_std,\n",
    "    device=None,\n",
    "    sparse_matrix_initializer=SparseMatrixBySparsityInitializer(\n",
    "        sparsity=sparse_initialization, device=\"cpu\"\n",
    "    ),\n",
    "    relu_theta=relu_theta, ######\n",
    "    from_checkpoint=False,\n",
    "    T=T,\n",
    "    continualupdate=False,\n",
    "    ratshift=False,\n",
    "    initialize_W_gh_with_zeroes=False,\n",
    "    pseudo_inverse=False,\n",
    "    batch_update=False,\n",
    "    use_h_fix=False,\n",
    "    learned_pseudo=False,\n",
    "    epsilon=0.01,\n",
    "    calculate_update_scaling_method=\"norm\",\n",
    "    MagicMath=False,\n",
    "    sanity_check=False,\n",
    "    calculate_g_method=\"fast\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=5,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"medium\",\n",
    "    use_fix=False,\n",
    "):\n",
    "    import torch\n",
    "    import random\n",
    "\n",
    "    data = dataset.train_data.flatten(1)[:num_imgs].float().to(\"cpu\")\n",
    "    # data = random.sample(dataset.data.flatten(1).float().to(\"cpu\"), num_imgs)\n",
    "    if preprocess_sensory:\n",
    "    #     # if use_fix:\n",
    "    #     #     for i in range(len(data)):\n",
    "    #     #         data[i] = (data[i] - data[i].mean()) / data[i].std()\n",
    "\n",
    "        data = (data - data.mean(dim=0)) / (data.std(dim=0) + 1e-8)\n",
    "    # noissing the data\n",
    "    # data = data / 255.0\n",
    "    if noise_level == \"none\":\n",
    "        return data, data\n",
    "    elif noise_level == \"low\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1, 1)\n",
    "    elif noise_level == \"medium\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.25, 1.25)\n",
    "    elif noise_level == \"high\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.5, 1.5)\n",
    "    noisy_data = data + random_noise\n",
    "\n",
    "    return data, noisy_data\n",
    "\n",
    "def prepare_data_random(noise_scale=0.1):\n",
    "    data = torch.randn((num_imgs, 784))\n",
    "    noise = noise_scale * torch.randn((num_imgs, 784))\n",
    "    return data, data + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import solve_mean, spacefillingcurve\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten())]\n",
    ")\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "data, noisy_data = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=num_imgs,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"none\",\n",
    "    use_fix=True\n",
    ")\n",
    "# data, noisy_data = prepare_data_random(noise_scale=0) \n",
    "params = [[0.03, 0.8], [0.07, 0.8], [0.1, 0.8], [0.15, 0.8]]\n",
    "sims = []\n",
    "v = spacefillingcurve(shapes)\n",
    "\n",
    "lambdas = [3,4,5]\n",
    "shapes = [(i, i) for i in lambdas]\n",
    "for i in params:\n",
    "    percent_nonzero_relu = i[0]\n",
    "    W_gh_var = 1\n",
    "    sparse_initialization = i[1]\n",
    "    T = 0.00005\n",
    "    W_hg_std = math.sqrt(W_gh_var)\n",
    "    W_hg_mean = -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(lambdas))\n",
    "    h_normal_mean = len(lambdas) * W_hg_mean\n",
    "    h_normal_std = math.sqrt(len(lambdas)) * W_hg_std\n",
    "    relu_theta = math.sqrt((1 - sparse_initialization) * len(lambdas)) * norm.ppf(\n",
    "        1 - percent_nonzero_relu\n",
    "    )\n",
    "    num_imgs = 201\n",
    "\n",
    "    print(percent_nonzero_relu, W_hg_mean, W_hg_std, h_normal_mean, h_normal_std, relu_theta)\n",
    "\n",
    "    GS = GridScaffold(\n",
    "        shapes=shapes,\n",
    "        N_h=1200,\n",
    "        input_size=784,\n",
    "        h_normal_mean=h_normal_mean,\n",
    "        h_normal_std=h_normal_std,\n",
    "        device=None,\n",
    "        sparse_matrix_initializer=SparseMatrixBySparsityInitializer(\n",
    "            sparsity=sparse_initialization, device=\"cpu\"\n",
    "        ),\n",
    "        relu_theta=relu_theta, ######\n",
    "        from_checkpoint=False,\n",
    "        T=T,\n",
    "        continualupdate=False,\n",
    "        ratshift=False,\n",
    "        initialize_W_gh_with_zeroes=False,\n",
    "        pseudo_inverse=False,\n",
    "        batch_update=False,\n",
    "        use_h_fix=False,\n",
    "        learned_pseudo=False,\n",
    "        epsilon=0.01,\n",
    "        calculate_update_scaling_method=\"norm\",\n",
    "        MagicMath=False,\n",
    "        sanity_check=False,\n",
    "        calculate_g_method=\"fast\",\n",
    "        scaling_updates=True,\n",
    "        dream_fix=5,\n",
    "        ZeroTol=1,\n",
    "    )\n",
    "\n",
    "    g_positions, g_positions2, g_points, g_points_2 = GS.learn_path(observations=data, velocities=v[: len(data)])\n",
    "    recalled_imgs = GS.recall(noisy_data)\n",
    "    similarity = torch.cosine_similarity(data, recalled_imgs, dim=1)\n",
    "    sims.append(similarity)\n",
    "    print(similarity.mean())\n",
    "    for o in range(20):\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(data[(-o)].reshape(28, 28), cmap=\"gray\")\n",
    "        ax[0].set_title(\"Original\")\n",
    "        ax[1].imshow(recalled_imgs[(-o)].reshape(28, 28), cmap=\"gray\")\n",
    "        ax[1].set_title(\"Recalled\" + \"percent_nonzero_relu=\" + str(i[0]) + \" sparsity=\" + str(i[1]))\n",
    "        plt.colorbar( ax[0].imshow(data[(-o)].reshape(28, 28), cmap=\"gray\"), ax=ax[0])\n",
    "        plt.colorbar( ax[1].imshow(recalled_imgs[(-o)].reshape(28, 28), cmap=\"gray\"), ax=ax[1])\n",
    "        plt.show()\n",
    "\n",
    "# please make a plot with x the number of images and y the similarity\n",
    "# between the original and the recalled images\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# BARCHART\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "# put legend outside of the plot\n",
    "for i in range(len(sims)):\n",
    "    similarity = sims[i]\n",
    "\n",
    "    labels = dataset.train_labels[:num_imgs]\n",
    "    unique_labels = np.unique(labels)\n",
    "    similarity_per_label = []\n",
    "    for label in unique_labels:\n",
    "        idx = labels == label\n",
    "        similarity_per_label.append(similarity[idx].mean())\n",
    "    # make bars not overlap\n",
    "    ax.bar(unique_labels, similarity_per_label, label=\"percent_nonzero_relu=\" + str(params[i][0]) + \" sparsity=\" + str(params[i][1]))\n",
    "    ax.set_title(\"Similarity per label\")\n",
    "    ax.set_xlabel(\"Label\")\n",
    "    ax.set_ylabel(\"Similarity\")\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LINEPLOT\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "for i in range(len(sims)):\n",
    "    similarity = sims[i]\n",
    "    ax.plot(similarity, label=\"percent_nonzero_relu=\" + str(params[i][0]) + \" sparsity=\" + str(params[i][1]))\n",
    "    ax.set_title(\"Similarity per image\")\n",
    "    ax.set_xlabel(\"Image\")\n",
    "    ax.set_ylabel(\"Similarity\")\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data.mean(dim=0).reshape(28, 28), cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "print(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorhash_functions import *\n",
    "\n",
    "mod_n_positions = []\n",
    "mod_n_positions_2 = []\n",
    "mod_n_states = []\n",
    "mod_n_states_2 = []\n",
    "\n",
    "# assume shapes are squares\n",
    "\n",
    "for _ in lambdas:\n",
    "    mod_n_positions.append(list())\n",
    "    mod_n_positions_2.append(list())\n",
    "    mod_n_states.append(list())\n",
    "    mod_n_states_2.append(list())\n",
    "\n",
    "for g in g_positions:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_positions[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_positions2:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_positions_2[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_points:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_states[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2\n",
    "\n",
    "for g in g_points_2:\n",
    "    pos = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        mod_n = g[pos : pos + l**2]\n",
    "        mod_n_states_2[i].append(ConvertToXYNew(mod_n, (l, l)))\n",
    "        pos += l**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrids(\n",
    "        shapes_lists=[(l, l), (l, l), (l, l), (l, l)],\n",
    "        points_lists=[\n",
    "            mod_n_positions[i],\n",
    "            mod_n_positions_2[i],\n",
    "            mod_n_states[i],\n",
    "            mod_n_states_2[i],\n",
    "        ],\n",
    "        first_points=[\n",
    "            mod_n_positions[i][0],\n",
    "            mod_n_positions_2[i][0],\n",
    "            mod_n_states[i][0],\n",
    "            mod_n_states_2[i][0],\n",
    "        ],\n",
    "        titles=[\n",
    "            f\"estimated grid position of first image\",\n",
    "            f\"estimated grid position of second image\",\n",
    "            f\"denoised grid state of first image\",\n",
    "            f\"denoised grid state of second image\",\n",
    "        ],\n",
    "        main_title=f\"estimated grid positions and denoised grid states of first and second images as the number of images learned increases. \\n{l}x{l} module, N_h={GS.N_h}, sparsity={round(sparse_initialization, 2)}, relu_theta={round(relu_theta, 2)}, W_hg_mean={round(W_hg_mean, 2)}, W_hg_std={W_hg_std} shapes={shapes}, num_imgs={num_imgs}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph estimated position of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_positions_n = []\n",
    "point_states_n = []\n",
    "for l in lambdas:\n",
    "    point_positions_n.append([])\n",
    "    point_states_n.append([])\n",
    "\n",
    "for img in data:\n",
    "    pos = GS.estimate_position(img)\n",
    "    state = GS.denoise(\n",
    "        GS.grid_from_hippocampal(GS.hippocampal_from_sensory(img))\n",
    "    ).flatten()\n",
    "\n",
    "    p = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        point_positions_n[i].append(ConvertToXYNew(pos[p : p + l**2], (l, l)))\n",
    "        point_states_n[i].append(ConvertToXYNew(state[p : p + l**2], (l, l)))\n",
    "        p += l**2\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrids(\n",
    "        shapes_lists=[(l, l), (l, l)],\n",
    "        points_lists=[point_positions_n[i], point_states_n[i]],\n",
    "        first_points=[point_positions_n[i][0], point_states_n[i][0]],\n",
    "        titles=[\n",
    "            f\"estimated {l}x{l} grid position of each image.\",\n",
    "            f\"denoised {l}x{l} grid state of each image.\",\n",
    "        ],\n",
    "        main_title=f\"estimated positions and denoised grid states of each image.\\n {l}x{l} module, N_h={GS.N_h}, sparsity={round(sparse_initialization, 2)}, relu_theta={round(relu_theta,2 )}, W_hg_mean={round(W_hg_mean, 2)}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data[1]\n",
    "recalled = GS.recall(img)\n",
    "print(torch.cosine_similarity(img, recalled, dim=0).mean())\n",
    "\n",
    "plt.imshow(img.reshape(28,28))\n",
    "print(img.mean())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow((recalled).reshape(28,28))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(GS.sensory_from_hippocampal(GS.hippocampal_from_grid(GS.G[3200])).reshape(28,28))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = (dataset.train_labels == 0).nonzero().flatten()\n",
    "\n",
    "data_by_class = dataset.data[zero_indices][:200].reshape(200, 784) \n",
    "data_by_class = torch.tensor(data).float().to(\"cpu\")\n",
    "data_by_class = (data - data.mean(dim=0)) / (data.std(dim=0)+1e-8)\n",
    "\n",
    "plt.imshow(data_by_class[0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(data_by_class[1].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(data_by_class.mean(dim=0).reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "point_positions_n = []\n",
    "point_states_n = []\n",
    "for l in lambdas:\n",
    "    point_positions_n.append([])\n",
    "    point_states_n.append([])\n",
    "\n",
    "for img in data_by_class:\n",
    "    pos = GS.estimate_position(img)\n",
    "    state = GS.denoise(\n",
    "        GS.grid_from_hippocampal(GS.hippocampal_from_sensory(img))\n",
    "    ).flatten()\n",
    "\n",
    "    p = 0\n",
    "    for i, l in enumerate(lambdas):\n",
    "        point_positions_n[i].append(ConvertToXYNew(pos[p : p + l**2], (l, l)))\n",
    "        point_states_n[i].append(ConvertToXYNew(state[p : p + l**2], (l, l)))\n",
    "        p += l**2\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    GraphGrids(\n",
    "        shapes_lists=[(l, l), (l, l)],\n",
    "        points_lists=[point_positions_n[i], point_states_n[i]],\n",
    "        first_points=[point_positions_n[i][0], point_states_n[i][0]],\n",
    "        titles=[\n",
    "            f\"estimated {l}x{l} grid position of each image.\",\n",
    "            f\"denoised {l}x{l} grid state of each image.\",\n",
    "        ],\n",
    "        main_title=f\"estimated positions and denoised grid states of each image.\\n {l}x{l} module, N_h={GS.N_h}, sparsity={round(sparse_initialization, 2)}, relu_theta={round(relu_theta,2 )}, W_hg_mean={round(W_hg_mean, 2)}, W_hg_std={W_hg_std}\\n shapes={shapes}, num_imgs={num_imgs}\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
