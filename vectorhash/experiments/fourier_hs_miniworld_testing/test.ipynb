{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4691ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import torch\n",
    "from hippocampal_sensory_layers import (\n",
    "    ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayerComplexScalars,\n",
    "    ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer,\n",
    "    ComplexExactPseudoInverseHippocampalSensoryLayerComplexScalars,\n",
    "    ComplexExactPseudoInverseHippocampalSensoryLayer,\n",
    "    RegularizedComplexExactPseudoInverseHippocampalSensoryLayerComplexScalars,\n",
    "    HippocampalSensoryLayer,\n",
    ")\n",
    "from fourier_scaffold import FourierScaffold, HadamardShiftMatrixRat\n",
    "from preprocessing_cnn import (\n",
    "    Preprocessor,\n",
    "    RescalePreprocessing,\n",
    "    SequentialPreprocessing,\n",
    "    GrayscaleAndFlattenPreprocessing,\n",
    ")\n",
    "from experiments.fourier_miniworld_gridsearch.room_env import RoomExperiment\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from graph_utils import plot_imgs_side_by_side\n",
    "from agent import TrueData\n",
    "from tqdm import tqdm\n",
    "\n",
    "forward_20 = [2] * 20\n",
    "right_60_deg = [1] * 20\n",
    "loop_path = (forward_20 + right_60_deg) * 6 + forward_20\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer_iterative_no_hidden(sbook, D, device, gbook):\n",
    "    layer = (\n",
    "        ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayerComplexScalars(\n",
    "            input_size=sbook.shape[1],\n",
    "            N_h=D,\n",
    "            hidden_layer_factor=0,\n",
    "            epsilon_sh=0.1,\n",
    "            epsilon_hs=0.1,\n",
    "            device=device,\n",
    "        )\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "def make_layer_iterative(sbook, D, device, gbook):\n",
    "    layer = (\n",
    "        ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayerComplexScalars(\n",
    "            input_size=sbook.shape[1],\n",
    "            N_h=D,\n",
    "            hidden_layer_factor=1,\n",
    "            epsilon_sh=0.1,\n",
    "            epsilon_hs=0.1,\n",
    "            device=device,\n",
    "        )\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "def make_layer_iterative_noncomplex(sbook, D, device, gbook):\n",
    "    layer = (\n",
    "        ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "            input_size=sbook.shape[1],\n",
    "            N_h=D,\n",
    "            hidden_layer_factor=1,\n",
    "            epsilon_sh=0.1,\n",
    "            epsilon_hs=0.1,\n",
    "            device=device,\n",
    "        )\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "def make_layer_iterative_no_hidden_noncomplex(sbook, D, device, gbook):\n",
    "    layer = (\n",
    "        ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "            input_size=sbook.shape[1],\n",
    "            N_h=D,\n",
    "            hidden_layer_factor=0,\n",
    "            epsilon_sh=0.1,\n",
    "            epsilon_hs=0.1,\n",
    "            device=device,\n",
    "        )\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_layer_analytic(sbook, D, device, gbook):\n",
    "    layer = ComplexExactPseudoInverseHippocampalSensoryLayerComplexScalars(\n",
    "        input_size=sbook.shape[1],\n",
    "        N_patts=len(gbook),\n",
    "        hbook=gbook,\n",
    "        N_h=D,\n",
    "        device=device,\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "def make_layer_analytic_noncomplex(sbook, D, device, gbook):\n",
    "    layer = ComplexExactPseudoInverseHippocampalSensoryLayer(\n",
    "        input_size=sbook.shape[1],\n",
    "        N_patts=len(gbook),\n",
    "        hbook=gbook,\n",
    "        N_h=D,\n",
    "        device=device,\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "def make_layer_complex_regularized(sbook, D, device, gbook):\n",
    "    layer = RegularizedComplexExactPseudoInverseHippocampalSensoryLayerComplexScalars(\n",
    "        input_size=sbook.shape[1],\n",
    "        N_patts=len(gbook),\n",
    "        hbook=gbook,\n",
    "        N_h=D,\n",
    "        device=device,\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "\n",
    "shapes_list = [\n",
    "    [(3, 3, 3), (5, 5, 5)],\n",
    "    # [(3, 3, 3), (7, 7, 7)],\n",
    "    # [(5, 5, 5), (7, 7, 7)],\n",
    "    # [(7, 7, 7), (9, 9, 9)],\n",
    "]\n",
    "\n",
    "D_list = [300, 600, 800]\n",
    "D_reshape_size_map = {\n",
    "    4000: (50, 80),\n",
    "    2000: (40, 50),\n",
    "    800: (20, 40),\n",
    "    600: (20, 30),\n",
    "    300: (15, 20),\n",
    "}\n",
    "\n",
    "layers_list = [\n",
    "    (\"iterative no hidden\", make_layer_iterative_no_hidden),\n",
    "    (\"iterative hidden\", make_layer_iterative_no_hidden),\n",
    "    (\"iterative no hidden noncomplex \", make_layer_iterative_no_hidden_noncomplex),\n",
    "    (\"iterative hidden noncomplex\", make_layer_iterative_no_hidden_noncomplex),\n",
    "    (\"analytic\", make_layer_analytic),\n",
    "    (\"analytic_noncomplex\", make_layer_analytic_noncomplex),\n",
    "    # (\"analytic_regularized\", make_layer_complex_regularized),\n",
    "]\n",
    "\n",
    "preprocessing_list = [\n",
    "    SequentialPreprocessing(\n",
    "        transforms=[\n",
    "            RescalePreprocessing(0.25),\n",
    "            GrayscaleAndFlattenPreprocessing(device),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "g_rescaling_list = [False]\n",
    "sbook_rand_list = [False]\n",
    "seeds = [44]\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return RoomExperiment([3, 0, 3], 0)\n",
    "\n",
    "\n",
    "def make_data(\n",
    "    path: list[int],\n",
    "    scaffold: FourierScaffold,\n",
    "    preprocessing: Preprocessor,\n",
    "    noise_dist=None,\n",
    "):\n",
    "    env = make_env()\n",
    "\n",
    "    def get_true_pos():\n",
    "        p_x, p_y, p_z = env.get_wrapper_attr(\"agent\").pos\n",
    "        angle = env.get_wrapper_attr(\"agent\").dir\n",
    "        p = torch.tensor([p_x, p_z, angle]).float().to(device)\n",
    "        return p\n",
    "\n",
    "    def _env_reset():\n",
    "        obs, info = env.reset()\n",
    "        img = obs\n",
    "        processed_img = preprocessing.encode(img)\n",
    "        p = get_true_pos()\n",
    "        return processed_img, p\n",
    "\n",
    "    def _obs_postpreprocess(step_tuple, action):\n",
    "        obs, reward, terminated, truncated, info = step_tuple\n",
    "        img = obs\n",
    "        processed_img = preprocessing.encode(img)\n",
    "        p = get_true_pos()\n",
    "        return processed_img, p\n",
    "\n",
    "    start_img, start_pos = _env_reset()\n",
    "    v_cumulative = torch.zeros(3, device=device)\n",
    "\n",
    "    true_data = TrueData(start_pos)\n",
    "\n",
    "    true_positions = [true_data.true_position.clone()]\n",
    "    Pbook = [scaffold.P.clone().unsqueeze(0)]\n",
    "    gbook = [(scaffold.P @ scaffold.g_s).clone()]\n",
    "    sbook = [start_img]\n",
    "\n",
    "    print(\"gbook 0 shape:\", gbook[0].shape)\n",
    "    print(\"sbook 0 shape:\", sbook[0].shape)\n",
    "    for i, action in enumerate(path):\n",
    "        ### env-specific observation processing\n",
    "        step_tuple = env.step(action)\n",
    "\n",
    "        ### this is the sensory input not flattened yet\n",
    "        new_img, new_pos = _obs_postpreprocess(step_tuple, action)\n",
    "\n",
    "        ### calculation of noisy input\n",
    "        dp = new_pos - true_data.true_position\n",
    "        true_data.true_position = new_pos\n",
    "        noisy_dp = new_pos\n",
    "        if noise_dist != None:\n",
    "            noisy_dp += noise_dist.sample(3)\n",
    "\n",
    "        dt = 1\n",
    "        v = (dp / dt) * scaffold.scale_factor\n",
    "        v_cumulative += v\n",
    "\n",
    "        if v_cumulative.norm(p=float(\"inf\")) < 1:\n",
    "            continue\n",
    "\n",
    "        scaffold.velocity_shift(v_cumulative)\n",
    "        print(\"entropy(P): \", scaffold.entropy(scaffold.P))\n",
    "        scaffold.smooth()\n",
    "        # scaffold.sharpen()\n",
    "        g_avg = scaffold.P @ scaffold.g_s\n",
    "        print(\"||g_avg||₂² :\", g_avg.norm() ** 2)\n",
    "        true_positions += [true_data.true_position.clone()]\n",
    "        Pbook += [scaffold.P.clone().unsqueeze(0)]\n",
    "        gbook += [g_avg.clone()]\n",
    "        sbook += [new_img]\n",
    "        v_cumulative = torch.zeros(3, device=device)\n",
    "\n",
    "    return (\n",
    "        torch.vstack(true_positions),\n",
    "        torch.vstack(gbook),\n",
    "        torch.vstack(sbook),\n",
    "        torch.concat(Pbook, dim=0),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_xy_distributions(scaffold: FourierScaffold, Pbook: torch.Tensor):\n",
    "    c_x, c_y, c_th = 3, 0, 3\n",
    "    r_x, r_y, r_th = 7, 7, 7\n",
    "    l_x, l_y, l_th = 2 * r_x + 1, 2 * r_y + 1, 2 * r_th + 1\n",
    "    omega = torch.cartesian_prod(\n",
    "        torch.arange(c_x - r_x, c_x + r_x + 1, 1, device=device),\n",
    "        torch.arange(c_y - r_y, c_y + r_y + 1, 1, device=device),\n",
    "        torch.arange(c_th - r_th, c_th + r_th + 1, 1, device=device),\n",
    "    )\n",
    "    xy_distributions = torch.empty(len(Pbook), l_x, l_y)\n",
    "    for i in range(len(Pbook)):\n",
    "        dist = scaffold.get_probability_abs_batched(omega, P=Pbook[i])\n",
    "        xy_distributions[i] = dist.reshape(l_x, l_y, l_th).sum(-1)\n",
    "\n",
    "    return xy_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d37558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_layer(\n",
    "    layer: HippocampalSensoryLayer,\n",
    "    hbook: torch.Tensor,\n",
    "    sbook: torch.Tensor,\n",
    "    large: bool,\n",
    "):\n",
    "    err_l1_first_img_s_h_s = -torch.ones(len(sbook))\n",
    "    err_l1_last_img_s_h_s = -torch.ones(len(sbook))\n",
    "    avg_accumulated_err_l2 = -torch.ones(len(sbook))\n",
    "    first_img = sbook[0]\n",
    "\n",
    "    gbook_recovered = torch.zeros_like(gbook_)\n",
    "    recovered_first_imgs_shs = torch.zeros_like(sbook)\n",
    "    recovered_last_imgs_shs = torch.zeros_like(sbook)\n",
    "    recovered_last_imgs_hs = torch.zeros_like(sbook)\n",
    "\n",
    "    for i in tqdm(range(len(sbook))):\n",
    "        h_ = hbook[i]\n",
    "        if large:\n",
    "            h = torch.einsum(\"i,j->ij\", h_, h_.conj()).flatten()\n",
    "        else:\n",
    "            h = h_\n",
    "\n",
    "        s = sbook[i]\n",
    "        layer.learn(h, s)\n",
    "\n",
    "        gbook_recovered[i] = layer.hippocampal_from_sensory(s)\n",
    "        recovered_first_imgs_shs[i] = layer.sensory_from_hippocampal(\n",
    "            layer.hippocampal_from_sensory(first_img)\n",
    "        )[0]\n",
    "        recovered_last_imgs_shs[i] = layer.sensory_from_hippocampal(\n",
    "            layer.hippocampal_from_sensory(s)\n",
    "        )[0]\n",
    "        recovered_last_imgs_hs[i] = layer.sensory_from_hippocampal(\n",
    "            h\n",
    "        )[0]\n",
    "\n",
    "        err_l1_first_img_s_h_s[i] = torch.mean(\n",
    "            torch.abs(\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(first_img)\n",
    "                )[0]\n",
    "                - first_img\n",
    "            )\n",
    "        )\n",
    "\n",
    "        err_l1_last_img_s_h_s[i] = torch.mean(\n",
    "            torch.abs(\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(sbook[i])\n",
    "                )[0]\n",
    "                - sbook[i]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        avg_accumulated_err_l2[i] = torch.mean(\n",
    "            (\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(sbook[: i + 1])\n",
    "                )\n",
    "                - sbook[: i + 1]\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        if (\n",
    "            # err_l1_first_img_s_h_s[i] > 10e5\n",
    "            # or avg_accumulated_err_l2[i] > 10e5\n",
    "            torch.any(torch.isnan(err_l1_first_img_s_h_s[i]))\n",
    "            or torch.any(torch.isnan(avg_accumulated_err_l2[i]))\n",
    "        ):\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        err_l1_first_img_s_h_s,\n",
    "        err_l1_last_img_s_h_s,\n",
    "        avg_accumulated_err_l2,\n",
    "        recovered_first_imgs_shs,\n",
    "        recovered_last_imgs_shs,\n",
    "        gbook_recovered,\n",
    "        recovered_last_imgs_hs\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eab3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/Projects/rl-research/vectorhash/experiments/fourier_hs_miniworld_testing/../../fourier_scaffold.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.shapes = torch.tensor(shapes).int()\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    if seed == 42:\n",
    "        continue\n",
    "    torch.manual_seed(seed)\n",
    "    for i, D in enumerate(D_list):\n",
    "        for j, shapes in enumerate(shapes_list):\n",
    "            for k, preprocessing in enumerate(preprocessing_list):\n",
    "                scaffold = FourierScaffold(\n",
    "                    shapes=torch.tensor(shapes),\n",
    "                    D=D,\n",
    "                    shift=HadamardShiftMatrixRat(shapes=torch.tensor(shapes)),\n",
    "                    device=device,\n",
    "                )\n",
    "                true_positions, gbook, sbook, Pbook = make_data(\n",
    "                    loop_path, scaffold, preprocessing\n",
    "                )\n",
    "                xy_dists_true = get_xy_distributions(scaffold=scaffold, Pbook=Pbook)\n",
    "                for l, g_rescaling in enumerate(g_rescaling_list):\n",
    "                    if g_rescaling:\n",
    "                        gbook_ = gbook * 1e6\n",
    "                    else:\n",
    "                        gbook_ = gbook\n",
    "                    for m, sbook_rand in enumerate(sbook_rand_list):\n",
    "                        if sbook_rand:\n",
    "                            sbook_ = torch.sign(torch.randn_like(sbook))\n",
    "                        else:\n",
    "                            sbook_ = sbook\n",
    "\n",
    "                        for n, [layer_name, layer_constr] in enumerate(layers_list):\n",
    "                            layer = layer_constr(sbook_, D, device, gbook_)\n",
    "                            (\n",
    "                                err_l1_first_img_s_h_s,\n",
    "                                err_l1_last_img_s_h_s,\n",
    "                                avg_accumulated_err_l2,\n",
    "                                recovered_first_imgs_shs,\n",
    "                                recovered_last_imgs_shs,\n",
    "                                gbook_recovered,\n",
    "                                recovered_last_imgs_hs,\n",
    "                            ) = test_layer(layer, gbook_, sbook_, False)\n",
    "\n",
    "                            fig, axs = plt.subplots(\n",
    "                                nrows=len(recovered_first_imgs_shs),\n",
    "                                ncols=6,\n",
    "                                figsize=(28, 48),\n",
    "                            )\n",
    "\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[\n",
    "                                    recovered_first_imgs_shs[i].cpu().reshape(15, 20)\n",
    "                                    for i in range(len(recovered_first_imgs_shs))\n",
    "                                ],\n",
    "                                titles=[f\"recovered sbook[0]\" for j in range(len(sbook_))],\n",
    "                                axs=axs[:, 0],\n",
    "                                fig=fig,\n",
    "                                use_first_img_scale=True,\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[\n",
    "                                    sbook_[i].cpu().reshape(15, 20)\n",
    "                                    for i in range(len(sbook_))\n",
    "                                ],\n",
    "                                titles=[f\"sbook[{j}]\" for j in range(len(sbook_))],\n",
    "                                axs=axs[:, 1],\n",
    "                                fig=fig,\n",
    "                                use_first_img_scale=True,\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[\n",
    "                                    recovered_last_imgs_shs[i].cpu().reshape(15, 20)\n",
    "                                    for i in range(len(recovered_last_imgs_shs))\n",
    "                                ],\n",
    "                                titles=[f\"s->h->s sbook[{j}]\" for j in range(len(sbook_))],\n",
    "                                axs=axs[:, 2],\n",
    "                                fig=fig,\n",
    "                                use_first_img_scale=True,\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[\n",
    "                                    recovered_last_imgs_shs[i].cpu().reshape(15, 20)\n",
    "                                    for i in range(len(recovered_last_imgs_hs))\n",
    "                                ],\n",
    "                                titles=[f\"h-> s sbook[{j}]\" for j in range(len(sbook_))],\n",
    "                                axs=axs[:, 3],\n",
    "                                fig=fig,\n",
    "                                use_first_img_scale=True,\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[\n",
    "                                    gbook_[i].cpu().reshape(D_reshape_size_map[D]).real\n",
    "                                    for i in range(len(gbook_))\n",
    "                                ],\n",
    "                                titles=[\n",
    "                                    f\"Re(gbook[{j}]). ||g||²={gbook_[k].norm() **2:.1e}\"\n",
    "                                    for j in range(len(sbook_))\n",
    "                                ],\n",
    "                                axs=axs[:, 4],\n",
    "                                fig=fig,\n",
    "                                use_first_img_scale=True,\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[\n",
    "                                    gbook_recovered[i]\n",
    "                                    .cpu()\n",
    "                                    .reshape(D_reshape_size_map[D])\n",
    "                                    .real\n",
    "                                    for i in range(len(gbook_recovered))\n",
    "                                ],\n",
    "                                titles=[\n",
    "                                    f\"Re(gbook_rec[{j}]), ||g||²={gbook_recovered[k].norm()**2:.1e}\"\n",
    "                                    for j in range(len(sbook_))\n",
    "                                ],\n",
    "                                axs=axs[:, 5],\n",
    "                                fig=fig,\n",
    "                                use_first_img_scale=True,\n",
    "                            )\n",
    "                            fig.suptitle(\n",
    "                                f\"layer_type={layer_name}, D={D}, downscaling=0.25, g_rescaling={g_rescaling}, rand_sbook={sbook_rand}, shapes={shapes}\"\n",
    "                            )\n",
    "                            fig.savefig(\n",
    "                                f\"D={D}-layer={layer_name}-rescaling={g_rescaling}-rand_sbook={sbook_rand}-seed={seed}.png\"\n",
    "                            )\n",
    "                            plt.close(fig)\n",
    "                            fig, axs = plt.subplots(\n",
    "                                nrows=len(recovered_first_imgs_shs),\n",
    "                                ncols=3,\n",
    "                                figsize=(14, 48),\n",
    "                            )\n",
    "                            xy_dists_from_gbook = get_xy_distributions(\n",
    "                                scaffold,\n",
    "                                Pbook=torch.einsum(\"bi,bj->bij\", gbook_, gbook_.conj()),\n",
    "                            )\n",
    "                            xy_dists_from_g_rec = get_xy_distributions(\n",
    "                                scaffold,\n",
    "                                Pbook=torch.einsum(\n",
    "                                    \"bi,bj->bij\", gbook_recovered, gbook_recovered.conj()\n",
    "                                ),\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[xy_dists_true[k].cpu() for k in range(len(xy_dists_true))], \n",
    "                                titles=[f'true_xy_dist[{k}]' for k in range(len(xy_dists_true))], \n",
    "                                axs = axs[:, 0], \n",
    "                                fig=fig, \n",
    "                                use_first_img_scale=False\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[xy_dists_from_gbook[k].cpu() for k in range(len(xy_dists_true))], \n",
    "                                titles=[f'gbook_xy_dist[{k}]' for k in range(len(xy_dists_true))], \n",
    "                                axs = axs[:, 1], \n",
    "                                fig=fig, \n",
    "                                use_first_img_scale=False\n",
    "                            )\n",
    "                            plot_imgs_side_by_side(\n",
    "                                imgs=[xy_dists_from_g_rec[k].cpu() for k in range(len(xy_dists_true))], \n",
    "                                titles=[f'g_rec_xy_dist[{k}]' for k in range(len(xy_dists_true))], \n",
    "                                axs = axs[:, 2], \n",
    "                                fig=fig, \n",
    "                                use_first_img_scale=False\n",
    "                            )\n",
    "                            fig.suptitle(\n",
    "                                f\"dists-layer_type={layer_name}, D={D}, downscaling=0.25, g_rescaling={g_rescaling}, rand_sbook={sbook_rand}, shapes={shapes}\"\n",
    "                            )\n",
    "                            fig.savefig(\n",
    "                                f\"dists-D={D}-layer={layer_name}-rescaling={g_rescaling}-rand_sbook={sbook_rand}-seed={seed}.png\"\n",
    "                            )\n",
    "                            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04b6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
