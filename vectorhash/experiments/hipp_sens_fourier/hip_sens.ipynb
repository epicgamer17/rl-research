{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import load_mnist_dataset, prepare_data\n",
    "from hippocampal_sensory_layers import (\n",
    "    HippocampalSensoryLayer,\n",
    "    ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer,\n",
    "    ComplexExactPseudoInverseHippocampalSensoryLayer,\n",
    "    ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayerComplexScalars\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from data_utils import load_mnist_dataset\n",
    "from matplotlib.axes import Axes\n",
    "from fourier_scaffold import FourierScaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc470b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_layer(\n",
    "    layer: HippocampalSensoryLayer, hbook: torch.Tensor, sbook: torch.Tensor\n",
    "):\n",
    "    err_l1_first_img_s_h_s = -torch.ones(len(sbook))\n",
    "    err_l1_last_img_s_h_s = -torch.ones(len(sbook))\n",
    "    avg_accumulated_err_l2 = -torch.ones(len(sbook))\n",
    "    first_img = sbook[0]\n",
    "\n",
    "    for i in tqdm(range(len(sbook))):\n",
    "        h = hbook[i]\n",
    "        s = sbook[i]\n",
    "        layer.learn(h, s)\n",
    "\n",
    "        err_l1_first_img_s_h_s[i] = torch.mean(\n",
    "            torch.abs(\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(first_img)\n",
    "                )[0]\n",
    "                - first_img\n",
    "            )\n",
    "        )\n",
    "\n",
    "        err_l1_last_img_s_h_s[i] = torch.mean(\n",
    "            torch.abs(\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(sbook[i])\n",
    "                )[0]\n",
    "                - sbook[i]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        avg_accumulated_err_l2[i] = torch.mean(\n",
    "            (\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(sbook[: i + 1])\n",
    "                )\n",
    "                - sbook[: i + 1]\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        if (\n",
    "            # err_l1_first_img_s_h_s[i] > 10e5\n",
    "            # or avg_accumulated_err_l2[i] > 10e5\n",
    "            torch.any(torch.isnan(err_l1_first_img_s_h_s[i]))\n",
    "            or torch.any(torch.isnan(avg_accumulated_err_l2[i]))\n",
    "        ):\n",
    "            break\n",
    "\n",
    "    return err_l1_first_img_s_h_s, err_l1_last_img_s_h_s, avg_accumulated_err_l2\n",
    "\n",
    "\n",
    "def plot_avg_acc_l2_err_on_ax(ax: Axes, avg_accumulated_err_l2: torch.Tensor, label):\n",
    "    x = torch.arange(0, len(avg_accumulated_err_l2[0]))\n",
    "    mean = avg_accumulated_err_l2.mean(dim=0)\n",
    "    std = avg_accumulated_err_l2.std(dim=0)\n",
    "    ax.plot(x, mean, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_first_img_l1_err_on_ax(ax: Axes, err_l1_first_img_s_h_s: torch.Tensor, label):\n",
    "    x = torch.arange(0, len(err_l1_first_img_s_h_s[0]))\n",
    "    mean = err_l1_first_img_s_h_s.mean(dim=0)\n",
    "    std = err_l1_first_img_s_h_s.std(dim=0)\n",
    "    ax.plot(x, mean, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_last_img_l1_err_on_ax(ax: Axes, err_l1_last_img_s_h_s: torch.Tensor, label):\n",
    "    x = torch.arange(0, len(err_l1_last_img_s_h_s[0]))\n",
    "    mean = err_l1_last_img_s_h_s.mean(dim=0)\n",
    "    std = err_l1_last_img_s_h_s.std(dim=0)\n",
    "    ax.plot(x, mean, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def set_ax_titles(ax: Axes, title, xtitle, ytitle):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xtitle)\n",
    "    ax.set_ylabel(ytitle)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def add_vertical_bar_on_ax(ax: Axes, x):\n",
    "    ax.axvline(x=x, color=\"b\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "def add_horizontal_bar_on_ax(ax: Axes, y, label):\n",
    "    ax.axhline(y=y, color=\"k\", linestyle=\"--\", label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc1bb5",
   "metadata": {},
   "source": [
    "Analytic vs. Iterative pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_mnist_dataset()\n",
    "N_patts = 600\n",
    "N_s = 784\n",
    "# data = torch.randn(N_patts, N_s)\n",
    "data, noisy_data = prepare_data(dataset, N_patts, noise_level=\"none\", device=device)\n",
    "runs = 5\n",
    "\n",
    "N_h = 400\n",
    "shapes = [(3, 3, 3), (5, 5, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb326ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"analytic\",\n",
    "    \"iterative\",\n",
    "    \"iterative_complex\",\n",
    "    \"iterative_no_sigmoid\",\n",
    "    \"iterative_no_sigmoid_complex_scalars\",\n",
    "]\n",
    "err_l1_first_img_s_h_s = -torch.ones(len(names), runs, N_patts)\n",
    "err_l1_last_img_s_h_s = -torch.ones(len(names), runs, N_patts)\n",
    "avg_accumulated_err_l2 = -torch.ones(len(names), runs, N_patts)\n",
    "\n",
    "scaffold = FourierScaffold(shapes=torch.tensor(shapes), D=N_h, _skip_K_calc=True)\n",
    "gbook = scaffold.gbook().T[:N_patts]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    for run in range(runs):\n",
    "        if name == \"analytic\":\n",
    "            layer = ComplexExactPseudoInverseHippocampalSensoryLayer(\n",
    "                N_s, N_h, N_patts, gbook, device=device\n",
    "            )\n",
    "        elif name == \"iterative\":\n",
    "            layer = ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "                N_s, N_h, 1, True, 0.1, 0.1, device=device\n",
    "            )\n",
    "        elif name == 'iterative_complex':\n",
    "            layer = ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayerComplexScalars(\n",
    "                N_s, N_h, 1, True, 0.1, 0.1, device=device\n",
    "            )\n",
    "        elif name == 'iterative_no_sigmoid':\n",
    "            layer = ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "                N_s, N_h, 0, True, 0.1, 0.1, device=device\n",
    "            )\n",
    "        elif name == 'iterative_no_sigmoid_complex_scalars':\n",
    "            layer = ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayerComplexScalars(\n",
    "                N_s, N_h, 0, True, 0.1, 0.1, device=device\n",
    "            )\n",
    "        (\n",
    "            err_l1_first_img_s_h_s[i, run],\n",
    "            err_l1_last_img_s_h_s[i, run],\n",
    "            avg_accumulated_err_l2[i, run],\n",
    "        ) = test_layer(layer, gbook, data)\n",
    "        # data[torch.randperm(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(err_l1_first_img_s_h_s)\n",
    "print(err_l1_last_img_s_h_s)\n",
    "print(avg_accumulated_err_l2)\n",
    "\n",
    "import pickle\n",
    "\n",
    "data = {\n",
    "    \"err_l1_first_img_s_h_s\": err_l1_first_img_s_h_s,\n",
    "    \"err_l1_last_img_s_h_s\": err_l1_last_img_s_h_s,\n",
    "    \"avg_accumulated_err_l2\": avg_accumulated_err_l2,\n",
    "}\n",
    "\n",
    "with open('results.pkl', 'wb') as f:\n",
    "  pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ced10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_of_dataset = torch.mean(data, dim=0)\n",
    "rand = torch.rand_like(data)\n",
    "err_mean_l2 = torch.mean((mean_of_dataset - data) ** 2).cpu()\n",
    "err_mean_l1 = torch.mean(torch.abs(mean_of_dataset - data[0])).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 9))\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    plot_avg_acc_l2_err_on_ax(axs, avg_accumulated_err_l2[i], label=name)\n",
    "  \n",
    "add_vertical_bar_on_ax(axs, N_h)\n",
    "add_horizontal_bar_on_ax(axs, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    axs,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Average L2 error over all patterns\",\n",
    ")\n",
    "axs.set_ylim(0, 3)\n",
    "fig.savefig(\"hipp_sens_result_analytic_vs_iterative_dataset_err\")\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 9))\n",
    "for i, name in enumerate(names):\n",
    "    plot_first_img_l1_err_on_ax(axs, err_l1_first_img_s_h_s[i], label=name)\n",
    "    # label=f\"iterative hidden_layer_factor={1}, stationary={True}, epsilon_W_sh={0.1}, epsilon_W_hs={0.1}\",\n",
    "    \n",
    "add_vertical_bar_on_ax(axs, N_h)\n",
    "add_horizontal_bar_on_ax(axs, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    axs,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Error when recovering first pattern\",\n",
    ")\n",
    "axs.set_ylim(0, 3)\n",
    "fig.savefig(\"hipp_sens_result_analytic_vs_iterative_first_img_err\")\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(15, 9))\n",
    "for i, name in enumerate(names):\n",
    "    plot_last_img_l1_err_on_ax(axs, err_l1_last_img_s_h_s[i], label=name)\n",
    "    # label=f\"iterative hidden_layer_factor={1}, stationary={True}, epsilon_W_sh={0.1}, epsilon_W_hs={0.1}\",\n",
    "    \n",
    "add_vertical_bar_on_ax(axs, N_h)\n",
    "add_horizontal_bar_on_ax(axs, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    axs,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Error when recovering last pattern\",\n",
    ")\n",
    "axs.set_ylim(0, 3)\n",
    "fig.savefig(\"hipp_sens_result_analytic_vs_iterative_last_img_err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [(3, 3, 3), (5, 5, 5)]\n",
    "N_h=400\n",
    "scaffold = FourierScaffold(shapes=torch.tensor(shapes), D=N_h, _skip_K_calc=True)\n",
    "gbook = scaffold.gbook().T[:N_patts]\n",
    "N_patts=600\n",
    "# N_s=100\n",
    "# data = torch.sign(\n",
    "#     torch.randn(N_patts, N_s)\n",
    "# )\n",
    "N_s = 784\n",
    "data, noisy_data = prepare_data(dataset, N_patts, noise_level=\"none\", device=device)\n",
    "# layer = ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "#     784, N_h, 1, True, 0.1, 0.1, device=device\n",
    "# )\n",
    "# layer = ComplexExactPseudoInverseHippocampalSensoryLayer(\n",
    "#     N_s, N_h, N_patts, gbook\n",
    "# )\n",
    "layer = ComplexIterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "    N_s, N_h, 1, True, 0.01, 0.01, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(400):\n",
    "  layer.learn(gbook[j], data[j])\n",
    "# layer.learn_batch(data[:100], gbook[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce64c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_utils import plot_imgs_side_by_side\n",
    "\n",
    "j = 99\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 9))\n",
    "image_shape = (28, 28)\n",
    "\n",
    "def denoise(g):\n",
    "  scaffold.P =  layer.hbook[:N_patts] @ layer.hbook[:N_patts].T \n",
    "plot_imgs_side_by_side(\n",
    "    imgs=[\n",
    "        data[j].reshape(28, 28),\n",
    "        layer.sensory_from_hippocampal(gbook[j]).reshape(28, 28),\n",
    "        layer.sensory_from_hippocampal(\n",
    "            layer.hippocampal_from_sensory(data[j])\n",
    "        ).real.reshape(28, 28),\n",
    "    ],\n",
    "    axs=axs,\n",
    "    titles=[\"original\", \"g->s\", \"s->g->s\", \"s->g- (denoise) -> g -> s\"],\n",
    "    fig=fig,\n",
    "    use_first_img_scale=False\n",
    ")\n",
    "print((data[j] - layer.sensory_from_hippocampal(gbook[j])).abs().sum())\n",
    "print(\n",
    "    (data[j] - layer.sensory_from_hippocampal(layer.hippocampal_from_sensory(data[j])))\n",
    "    .abs()\n",
    "    .sum()\n",
    ")\n",
    "print(layer.W_hs.norm())\n",
    "print(layer.W_sh.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfee12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
