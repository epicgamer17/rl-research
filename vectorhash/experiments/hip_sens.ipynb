{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import load_mnist_dataset, prepare_data\n",
    "from vectorhash import build_scaffold\n",
    "from hippocampal_sensory_layers import (\n",
    "    ExactPseudoInverseHippocampalSensoryLayer,\n",
    "    IterativeBidirectionalPseudoInverseHippocampalSensoryLayer,\n",
    "    HippocampalSensoryLayer,\n",
    "    HebbianHippocampalSensoryLayer,\n",
    "    HSPseudoInverseSHHebbieanHippocampalSensoryLayer,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from data_utils import load_mnist_dataset\n",
    "from matplotlib.axes import Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc470b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_layer(\n",
    "    layer: HippocampalSensoryLayer, hbook: torch.Tensor, sbook: torch.Tensor\n",
    "):\n",
    "    err_l1_first_img_s_h_s = -torch.ones(len(sbook))\n",
    "    err_l1_last_img_s_h_s = -torch.ones(len(sbook))\n",
    "    avg_accumulated_err_l2 = -torch.ones(len(sbook))\n",
    "    first_img = sbook[0]\n",
    "\n",
    "    for i in tqdm(range(len(sbook))):\n",
    "        h = hbook[i]\n",
    "        s = sbook[i]\n",
    "        layer.learn(h, s)\n",
    "\n",
    "        err_l1_first_img_s_h_s[i] = torch.mean(\n",
    "            torch.abs(\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(first_img)\n",
    "                )[0]\n",
    "                - first_img\n",
    "            )\n",
    "        )\n",
    "\n",
    "        err_l1_last_img_s_h_s[i] = torch.mean(\n",
    "            torch.abs(\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(sbook[i])\n",
    "                )[0]\n",
    "                - sbook[i]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        avg_accumulated_err_l2[i] = torch.mean(\n",
    "            (\n",
    "                layer.sensory_from_hippocampal(\n",
    "                    layer.hippocampal_from_sensory(sbook[:i+1])\n",
    "                )\n",
    "                - sbook[:i+1]\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        if (\n",
    "            err_l1_first_img_s_h_s[i] > 10e5\n",
    "            or avg_accumulated_err_l2[i] > 10e5\n",
    "            or torch.any(torch.isnan(err_l1_first_img_s_h_s[i]))\n",
    "            or torch.any(torch.isnan(avg_accumulated_err_l2[i]))\n",
    "        ):\n",
    "            break\n",
    "\n",
    "    return err_l1_first_img_s_h_s, err_l1_last_img_s_h_s, avg_accumulated_err_l2\n",
    "\n",
    "\n",
    "def plot_avg_acc_l2_err_on_ax(ax: Axes, avg_accumulated_err_l2: torch.Tensor, label):\n",
    "    x = torch.arange(0, len(avg_accumulated_err_l2[0]))\n",
    "    mean = avg_accumulated_err_l2.mean(dim=0)\n",
    "    std = avg_accumulated_err_l2.std(dim=0)\n",
    "    ax.plot(x, mean, label=label)\n",
    "    ax.fill_between(x, mean-std, mean+std, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_first_img_l1_err_on_ax(ax: Axes, err_l1_first_img_s_h_s: torch.Tensor, label):\n",
    "    x = torch.arange(0, len(err_l1_first_img_s_h_s[0]))\n",
    "    mean = err_l1_first_img_s_h_s.mean(dim=0)\n",
    "    std = err_l1_first_img_s_h_s.std(dim=0)\n",
    "    ax.plot(x, mean, label=label)\n",
    "    ax.fill_between(x, mean-std, mean+std, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_last_img_l1_err_on_ax(ax: Axes, err_l1_last_img_s_h_s: torch.Tensor, label):\n",
    "    x = torch.arange(0, len(err_l1_last_img_s_h_s[0]))\n",
    "    mean = err_l1_last_img_s_h_s.mean(dim=0)\n",
    "    std = err_l1_last_img_s_h_s.std(dim=0)\n",
    "    ax.plot(x, mean, label=label)\n",
    "    ax.fill_between(x, mean-std, mean+std, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def set_ax_titles(ax: Axes, title, xtitle, ytitle):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xtitle)\n",
    "    ax.set_ylabel(ytitle)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def add_vertical_bar_on_ax(ax: Axes, x):\n",
    "    ax.axvline(x=x, color=\"b\", linestyle=\"--\")\n",
    "\n",
    "def add_horizontal_bar_on_ax(ax: Axes, y, label):\n",
    "    ax.axhline(y=y, color=\"k\", linestyle=\"--\", label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc1bb5",
   "metadata": {},
   "source": [
    "Analytic vs. Iterative pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_mnist_dataset()\n",
    "N_patts = 600\n",
    "data, noisy_data = prepare_data(dataset, N_patts, noise_level='none', device=device)\n",
    "runs=1\n",
    "\n",
    "N_h = 400\n",
    "shapes = [(3,3,3),(4,4,4),]\n",
    "scaffold, mean_h = build_scaffold(shapes, N_h, device=device, sanity_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb326ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"analytic\", \"iterative\"]\n",
    "err_l1_first_img_s_h_s = -torch.ones(2, runs, N_patts)\n",
    "err_l1_last_img_s_h_s = -torch.ones(2, runs, N_patts)\n",
    "avg_accumulated_err_l2 = -torch.ones(2, runs, N_patts)\n",
    "\n",
    "scaffold, mean_h = build_scaffold(shapes, N_h, device=device, sanity_check=True)\n",
    "for i, name in enumerate(names):\n",
    "    for run in range(runs):\n",
    "        if name == \"analytic\":\n",
    "            layer = ExactPseudoInverseHippocampalSensoryLayer(\n",
    "                784, N_h, N_patts, scaffold.H[:N_patts], device=device\n",
    "            )\n",
    "        else:\n",
    "            layer = IterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "                784, N_h, 1, True, 0.1, 0.1, device=device\n",
    "            )\n",
    "        (\n",
    "            err_l1_first_img_s_h_s[i, run],\n",
    "            err_l1_last_img_s_h_s[i, run],\n",
    "            avg_accumulated_err_l2[i, run],\n",
    "        ) = test_layer(layer, scaffold.H, data[torch.randperm(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ced10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_of_dataset = torch.mean(data, dim=0)\n",
    "rand = torch.rand_like(data)\n",
    "err_mean_l2 = torch.mean((mean_of_dataset - data) ** 2).cpu()\n",
    "err_mean_l1 = torch.mean(torch.abs(mean_of_dataset - data[0])).cpu()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    plot_avg_acc_l2_err_on_ax(ax, avg_accumulated_err_l2[i], label=name)\n",
    "  \n",
    "add_vertical_bar_on_ax(ax, N_h)\n",
    "add_horizontal_bar_on_ax(ax, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    ax,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Average L2 error over all patterns\",\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "fig.savefig(\"hipp_sens_result_analytic_vs_iterative_dataset_err\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "for i, name in enumerate(names):\n",
    "    plot_first_img_l1_err_on_ax(ax, err_l1_first_img_s_h_s[i], label=name)\n",
    "    # label=f\"iterative hidden_layer_factor={1}, stationary={True}, epsilon_W_sh={0.1}, epsilon_W_hs={0.1}\",\n",
    "    \n",
    "add_vertical_bar_on_ax(ax, N_h)\n",
    "add_horizontal_bar_on_ax(ax, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    ax,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Error when recovering first pattern\",\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "fig.savefig(\"hipp_sens_result_analytic_vs_iterative_first_img_err\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "for i, name in enumerate(names):\n",
    "    plot_last_img_l1_err_on_ax(ax, err_l1_last_img_s_h_s[i], label=name)\n",
    "    # label=f\"iterative hidden_layer_factor={1}, stationary={True}, epsilon_W_sh={0.1}, epsilon_W_hs={0.1}\",\n",
    "    \n",
    "add_vertical_bar_on_ax(ax, N_h)\n",
    "add_horizontal_bar_on_ax(ax, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    ax,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Error when recovering first pattern\",\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "fig.savefig(\"hipp_sens_result_analytic_vs_iterative_last_img_err\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d55976",
   "metadata": {},
   "source": [
    "Different Hebbian variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc874e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_mnist_dataset()\n",
    "N_patts = 600\n",
    "data, noisy_data = prepare_data(dataset, N_patts, noise_level=\"none\", device=device)\n",
    "\n",
    "N_h = 400\n",
    "shapes = [(3, 3, 3), (4, 4, 4)]\n",
    "means = [None, 0]\n",
    "scaling_updates = [True, False]\n",
    "runs = 10\n",
    "\n",
    "err_l1_first_img_s_h_s = -torch.ones(len(means), len(scaling_updates), runs, N_patts)\n",
    "err_l1_last_img_s_h_s = -torch.ones(len(means), len(scaling_updates), runs, N_patts)\n",
    "avg_accumulated_err_l2 = -torch.ones(len(means), len(scaling_updates), runs, N_patts)\n",
    "\n",
    "for j, mean in enumerate(means):\n",
    "    for k, scaling_update in enumerate(scaling_updates):\n",
    "        for run in range(runs):\n",
    "            scaffold, mean_h = build_scaffold(\n",
    "                shapes, N_h, device=device, sanity_check=True\n",
    "            )\n",
    "            layer = HebbianHippocampalSensoryLayer(\n",
    "                784, N_h, \"norm\", mean != None, mean_h, scaling_update, device\n",
    "            )\n",
    "            (\n",
    "                err_l1_first_img_s_h_s[j, k, run],\n",
    "                err_l1_last_img_s_h_s [j, k, run],\n",
    "                avg_accumulated_err_l2[j, k, run],\n",
    "            ) = test_layer(layer, scaffold.H, data[torch.randperm(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80356b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "\n",
    "mean_of_dataset = torch.mean(data, dim=0)\n",
    "rand = torch.rand_like(data)\n",
    "err_mean_l2 = torch.mean((mean_of_dataset - data) ** 2).cpu()\n",
    "err_mean_l1 = torch.mean(torch.abs(mean_of_dataset - data[0])).cpu()\n",
    "\n",
    "for k, scaling_update in enumerate(scaling_updates):\n",
    "    for j, mean in enumerate(means):\n",
    "        plot_avg_acc_l2_err_on_ax(\n",
    "            ax,\n",
    "            avg_accumulated_err_l2[j, k],\n",
    "            label=f\"mean_fix={mean != None}, scaling_update={scaling_update}\",\n",
    "        )\n",
    "add_vertical_bar_on_ax(ax, N_h)\n",
    "add_horizontal_bar_on_ax(ax, err_mean_l2, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    ax,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Average L2 error over all patterns\",\n",
    ")\n",
    "ax.set_ylim(0, 2)\n",
    "fig.savefig(\"hipp_sens_result_hebb_dataset_err_zoom_y\")\n",
    "ax.set_ylim(0, 70**2)\n",
    "fig.savefig(\"hipp_sens_result_hebb_dataset_err\")\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlim(0, 50)\n",
    "fig.savefig(\"hipp_sens_result_hebb_dataset_err_zoom_xy\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "for k, scaling_update in enumerate(scaling_updates):\n",
    "    for j, mean in enumerate(means):\n",
    "        plot_first_img_l1_err_on_ax(\n",
    "            ax,\n",
    "            err_l1_first_img_s_h_s[j, k],\n",
    "            label=f\"mean_fix={mean != None}, scaling_update={scaling_update}\",\n",
    "        )\n",
    "add_vertical_bar_on_ax(ax, N_h)\n",
    "add_horizontal_bar_on_ax(ax, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    ax,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Error when recovering first pattern\",\n",
    ")\n",
    "ax.set_ylim(0, 2)\n",
    "fig.savefig(\"hipp_sens_result_hebb_first_img_err_zoom_y\")\n",
    "ax.set_ylim(0, 70)\n",
    "fig.savefig(\"hipp_sens_result_hebb_first_img_err\")\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlim(0, 50)\n",
    "fig.savefig(\"hipp_sens_result_hebb_first_img_err_zoom_xy\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "for k, scaling_update in enumerate(scaling_updates):\n",
    "    for j, mean in enumerate(means):\n",
    "        plot_last_img_l1_err_on_ax(\n",
    "            ax,\n",
    "            err_l1_last_img_s_h_s[j, k],\n",
    "            label=f\"mean_fix={mean != None}, scaling_update={scaling_update}\",\n",
    "        )\n",
    "add_vertical_bar_on_ax(ax, N_h)\n",
    "add_horizontal_bar_on_ax(ax, err_mean_l1, label='err using \"mean of dataset\" image')\n",
    "set_ax_titles(\n",
    "    ax,\n",
    "    f\"shapes={shapes}, N_h={N_h}\",\n",
    "    \"Number of images learned\",\n",
    "    \"Error when recovering last pattern\",\n",
    ")\n",
    "ax.set_ylim(0, 2)\n",
    "fig.savefig(\"hipp_sens_result_hebb_last_img_err_zoom_y\")\n",
    "ax.set_ylim(0, 70)\n",
    "fig.savefig(\"hipp_sens_result_hebb_last_img_err\")\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlim(0, 50)\n",
    "fig.savefig(\"hipp_sens_result_hebb_last_img_err_zoom_xy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
