{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e73e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from clean_scaffold import GridHippocampalScaffold\n",
    "from hippocampal_sensory_layers import HippocampalSensoryLayer\n",
    "from vectorhash import build_initializer\n",
    "from clean_scaffold import ArgmaxSmoothing, SoftmaxSmoothing, PolynomialSmoothing\n",
    "from hippocampal_sensory_layers import IterativeBidirectionalPseudoInverseHippocampalSensoryLayer, ExactPseudoInverseHippocampalSensoryLayer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_p_1(codebook, p=0.1):\n",
    "    if p == 0.0:\n",
    "        return codebook\n",
    "    rand_indices = torch.sign(\n",
    "        torch.rand(size=codebook.shape, device=codebook.device) - p\n",
    "    )\n",
    "    return torch.multiply(codebook, rand_indices)\n",
    "\n",
    "def dynamics_patts_signed(\n",
    "    scaffold: GridHippocampalScaffold,\n",
    "    sensory_hippocampal_layer: HippocampalSensoryLayer,\n",
    "    sbook_noisy,  # (Npatts, input_size)\n",
    "    sbook,\n",
    "    hbook,\n",
    "    N_iter=1,\n",
    "):\n",
    "    h_in_original = sensory_hippocampal_layer.hippocampal_from_sensory(sbook_noisy)\n",
    "    h = torch.clone(h_in_original)\n",
    "    for i in range(N_iter):\n",
    "        g = scaffold.denoise(scaffold.grid_from_hippocampal(h))\n",
    "        h = scaffold.hippocampal_from_grid(g)\n",
    "\n",
    "    s_out = torch.sign(sensory_hippocampal_layer.sensory_from_hippocampal(h))\n",
    "    h_l2_err = torch.linalg.vector_norm(h - hbook) / scaffold.N_h\n",
    "    s_l2_err = (\n",
    "        torch.linalg.vector_norm(s_out - sbook) / sensory_hippocampal_layer.input_size\n",
    "    )\n",
    "    s_l1_err = torch.mean(torch.abs(s_out - sbook)) / 2\n",
    "\n",
    "    return h_l2_err, s_l2_err, s_l1_err\n",
    "\n",
    "\n",
    "def dynamics_patts(\n",
    "    scaffold: GridHippocampalScaffold,\n",
    "    sensory_hippocampal_layer: HippocampalSensoryLayer,\n",
    "    sbook_noisy,  # (Npatts, input_size)\n",
    "    sbook,\n",
    "    hbook,\n",
    "    N_iter=1,\n",
    "):\n",
    "    h_in_original = sensory_hippocampal_layer.hippocampal_from_sensory(sbook_noisy)\n",
    "    h = torch.clone(h_in_original)\n",
    "    for i in range(N_iter):\n",
    "        g = scaffold.denoise(scaffold.grid_from_hippocampal(h))\n",
    "        h = scaffold.hippocampal_from_grid(g)\n",
    "\n",
    "    s_out = sensory_hippocampal_layer.sensory_from_hippocampal(h)\n",
    "\n",
    "    h_l2_err = torch.linalg.vector_norm(h - hbook) / scaffold.N_h\n",
    "    s_l2_err = (\n",
    "        torch.linalg.vector_norm(s_out - sbook) / sensory_hippocampal_layer.input_size\n",
    "    )\n",
    "    s_l1_err = torch.mean(torch.abs(s_out - sbook))\n",
    "\n",
    "    return h_l2_err, s_l2_err, s_l1_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capacity_test_signed(\n",
    "    scaffold: GridHippocampalScaffold,\n",
    "    hippocampal_sensory_layer: HippocampalSensoryLayer,\n",
    "    sbook: torch.Tensor,\n",
    "    Npatts_list,\n",
    "    nruns=1,\n",
    "    device=None,\n",
    "    p=0.1,\n",
    "):\n",
    "    err_h_l2 = -1 * torch.ones((len(Npatts_list), nruns), device=device)\n",
    "    err_s_l1 = -1 * torch.ones((len(Npatts_list), nruns), device=device)\n",
    "    err_s_l2 = -1 * torch.ones((len(Npatts_list), nruns), device=device)\n",
    "\n",
    "    for k in tqdm(range(len(Npatts_list))):\n",
    "        Npatts = Npatts_list[k]\n",
    "        if hasattr(hippocampal_sensory_layer, \"learn_batch\"):\n",
    "            hippocampal_sensory_layer.learn_batch(sbook[:Npatts], scaffold.H[:Npatts])\n",
    "        else:\n",
    "            for j in tqdm(range(Npatts)):\n",
    "                hippocampal_sensory_layer.learn(scaffold.H[j], sbook[j])\n",
    "\n",
    "        hbook_subset = scaffold.H[:Npatts]\n",
    "        sbook_subset = sbook[:Npatts]\n",
    "\n",
    "        for r in range(nruns):\n",
    "            sbook_noisy_subset = corrupt_p_1(sbook_subset, p)\n",
    "            err_h_l2[k, r], err_s_l2[k, r], err_s_l1[k, r] = dynamics_patts_signed(\n",
    "                scaffold,\n",
    "                hippocampal_sensory_layer,\n",
    "                sbook_noisy_subset,\n",
    "                sbook_subset,\n",
    "                hbook_subset,\n",
    "                N_iter=1,\n",
    "            )\n",
    "\n",
    "    return err_h_l2, err_s_l2, err_s_l1\n",
    "\n",
    "\n",
    "def capacity_test(\n",
    "    scaffold: GridHippocampalScaffold,\n",
    "    hippocampal_sensory_layer: HippocampalSensoryLayer,\n",
    "    sbook: torch.Tensor,\n",
    "    Npatts_list,\n",
    "    nruns,\n",
    "    device,\n",
    "    p=0\n",
    "):\n",
    "    err_h_l2 = -1 * torch.ones((len(Npatts_list), nruns), device=device)\n",
    "    err_s_l1 = -1 * torch.ones((len(Npatts_list), nruns), device=device)\n",
    "    err_s_l2 = -1 * torch.ones((len(Npatts_list), nruns), device=device)\n",
    "\n",
    "    for k in tqdm(range(len(Npatts_list))):\n",
    "        Npatts = Npatts_list[k]\n",
    "\n",
    "        if hasattr(hippocampal_sensory_layer, \"learn_batch\"):\n",
    "            hippocampal_sensory_layer.learn_batch(sbook[:Npatts], scaffold.H[:Npatts])\n",
    "        else:\n",
    "            for j in tqdm(range(Npatts)):\n",
    "                hippocampal_sensory_layer.learn(scaffold.H[j], sbook[j])\n",
    "\n",
    "        hbook_subset = scaffold.H[:Npatts]\n",
    "        sbook_subset = sbook[:Npatts]\n",
    "\n",
    "        for r in range(nruns):\n",
    "            sbook_noisy_subset = corrupt_p_1(sbook_subset, p)\n",
    "            err_h_l2[k, r], err_s_l2[k, r], err_s_l1[k, r] = dynamics_patts(\n",
    "                scaffold,\n",
    "                hippocampal_sensory_layer,\n",
    "                sbook_noisy_subset,\n",
    "                sbook_subset,\n",
    "                hbook_subset,\n",
    "                N_iter=1,\n",
    "            )\n",
    "\n",
    "    return err_h_l2, err_s_l2, err_s_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aad780",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "N_h_list = [400]\n",
    "input_size = 784\n",
    "nruns = 1\n",
    "Npatts_list = torch.arange(1, torch.tensor(shapes).prod(), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c87df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_methods = [\n",
    "    # SoftmaxSmoothing(T=1),\n",
    "    # SoftmaxSmoothing(T=0.1),\n",
    "    ArgmaxSmoothing(),\n",
    "    # PolynomialSmoothing(k=2),\n",
    "    # PolynomialSmoothing(k=5),\n",
    "    # PolynomialSmoothing(k=8)\n",
    "]\n",
    "pseudoinverse_methods = [\"exact_pseudoinverse\"]  # \",iterative_pseudoinverse\"]\n",
    "initialization_method = \"by_scaling\"\n",
    "relu_options = [False, True]\n",
    "p = 0\n",
    "data = torch.sign(\n",
    "    torch.randn(torch.tensor(shapes).prod().item(), input_size, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_h_l2_results = torch.zeros(\n",
    "    (\n",
    "        len(smoothing_methods),\n",
    "        len(relu_options),\n",
    "        len(pseudoinverse_methods),\n",
    "        len(N_h_list),\n",
    "        len(Npatts_list),\n",
    "        nruns,\n",
    "    ),\n",
    ")\n",
    "err_s_l2_results = torch.zeros(\n",
    "    (\n",
    "        len(smoothing_methods),\n",
    "        len(relu_options),\n",
    "        len(pseudoinverse_methods),\n",
    "        len(N_h_list),\n",
    "        len(Npatts_list),\n",
    "        nruns,\n",
    "    ),\n",
    ")\n",
    "err_s_l1_results = torch.zeros(\n",
    "    (\n",
    "        len(smoothing_methods),\n",
    "        len(relu_options),\n",
    "        len(pseudoinverse_methods),\n",
    "        len(N_h_list),\n",
    "        len(Npatts_list),\n",
    "        nruns,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for k, N_h in enumerate(N_h_list):\n",
    "    for i, smoothing_method in enumerate(smoothing_methods):\n",
    "        for l, relu in enumerate(relu_options):\n",
    "            initializer, relu_theta, mean_h = build_initializer(\n",
    "                shapes,\n",
    "                initalization_method=initialization_method,\n",
    "                percent_nonzero_relu=0.8,\n",
    "                sparse_initialization=0.1,\n",
    "                device=device,\n",
    "            )\n",
    "            scaffold = GridHippocampalScaffold(\n",
    "                shapes,\n",
    "                N_h,\n",
    "                sparse_matrix_initializer=initializer,\n",
    "                smoothing=smoothing_method,\n",
    "                device=device,\n",
    "                relu_theta=0.5,\n",
    "                sanity_check=True,\n",
    "                relu=relu,\n",
    "            )\n",
    "            for j, pseudoinverse_method in enumerate(pseudoinverse_methods):\n",
    "                if pseudoinverse_method == \"exact_pseudoinverse\":\n",
    "                    layer = ExactPseudoInverseHippocampalSensoryLayer(\n",
    "                        input_size=input_size,\n",
    "                        N_h=N_h,\n",
    "                        N_patts=scaffold.N_patts,\n",
    "                        hbook=scaffold.H,\n",
    "                        device=device,\n",
    "                    )\n",
    "                elif pseudoinverse_method == \"iterative_pseudoinverse\":\n",
    "                    layer = IterativeBidirectionalPseudoInverseHippocampalSensoryLayer(\n",
    "                        input_size=input_size,\n",
    "                        N_h=N_h,\n",
    "                        epsilon_hs=0.1,\n",
    "                        epsilon_sh=0.1,\n",
    "                        hidden_layer_factor=1,\n",
    "                        device=device,\n",
    "                    )\n",
    "\n",
    "                err_h_l2, err_s_l2, err_s_l1 = capacity_test_signed(\n",
    "                    scaffold=scaffold,\n",
    "                    hippocampal_sensory_layer=layer,\n",
    "                    sbook=data,\n",
    "                    Npatts_list=Npatts_list,\n",
    "                    nruns=nruns,\n",
    "                    device=device,\n",
    "                    p=p,\n",
    "                )\n",
    "                err_h_l2_results[i, l, j, k] = err_h_l2\n",
    "                err_s_l2_results[i, l, j, k] = err_s_l2\n",
    "                err_s_l1_results[i, l, j, k] = err_s_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda83eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Npatts = torch.tensor(Npatts_list)  # Npatts_lst repeated nruns times\n",
    "Npatts = Npatts.T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_title(\"MI per inp bit vs num patts\")\n",
    "ax.set_xlabel(\"num patts\")\n",
    "ax.set_ylabel(\"MI per inp bit\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(which=\"both\")\n",
    "\n",
    "\n",
    "for k, N_h in enumerate(N_h_list):\n",
    "    for i, smoothing_method in enumerate(smoothing_methods):\n",
    "        for l, relu in enumerate(relu_options):\n",
    "            for j, pseudoinverse_method in enumerate(pseudoinverse_methods):\n",
    "                normlizd_l1 = err_s_l1_results[i, l, j, k]\n",
    "                m = 1 - (2 * normlizd_l1)\n",
    "                a = (1 + m) / 2\n",
    "                b = (1 - m) / 2\n",
    "                a = torch.abs(torch.tensor(a))\n",
    "                b = torch.abs(torch.tensor(b)).cpu()\n",
    "                S = -a * torch.log2(a) - b * torch.log2(b)\n",
    "                S = torch.where(m == 1, torch.zeros_like(S), S)\n",
    "                MI = 1 - S\n",
    "\n",
    "                if pseudoinverse_method == \"iterative_pseudoinverse\":\n",
    "                    label = f\"layer={layer}, hidden_layer_factor=1, smoothing={smoothing_method} relu={relu}\"\n",
    "                elif pseudoinverse_method == \"exact_pseudoinverse\":\n",
    "                    label = f\"analytic pseudoinverse, smoothing={smoothing_method}, relu={relu}\"\n",
    "                ax.errorbar(\n",
    "                    Npatts_list,\n",
    "                    MI.mean(axis=-1),\n",
    "                    yerr=MI.std(axis=-1),\n",
    "                    lw=2,\n",
    "                    label=label,\n",
    "                )\n",
    "\n",
    "vhash_y = [\n",
    "    1.000000000000000000e00,\n",
    "    1.000000000000000000e00,\n",
    "    1.000000000000000000e00,\n",
    "    5.988623183160277641e-01,\n",
    "    3.667958255856974548e-01,\n",
    "    2.624110436154711845e-01,\n",
    "    2.042300801824028511e-01,\n",
    "    1.672434617281599589e-01,\n",
    "    1.414727808416358368e-01,\n",
    "    1.225660944022268772e-01,\n",
    "    1.082352629751366369e-01,\n",
    "    9.674044810282866891e-02,\n",
    "    8.747471863732059205e-02,\n",
    "    7.977915334088647725e-02,\n",
    "    7.342708729082536578e-02,\n",
    "    6.793351052792084843e-02,\n",
    "    6.324575644685004328e-02,\n",
    "    5.912155577074185153e-02,\n",
    "]\n",
    "\n",
    "\n",
    "ax.errorbar(Npatts_list, vhash_y, lw=2, label=\"vectorhash\")\n",
    "ax.legend()\n",
    "\n",
    "# plt.ylim(ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\n",
    "    f\"capacity_test_signed_{initialization_method}_N_h_{N_h_list[0]}_p_{p}.png\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
