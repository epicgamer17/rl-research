{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import torch\n",
    "\n",
    "from fourier_scaffold import (\n",
    "    FourierScaffold,\n",
    "    FourierScaffoldDebug,\n",
    "    HadamardSharpening,\n",
    "    ContractionSharpening,\n",
    "    GaussianFourierSmoothing,\n",
    "    GuassianFourierSmoothingMatrix,\n",
    "    HadamardShiftMatrix,\n",
    "    calculate_padding,\n",
    ")\n",
    "import math\n",
    "\n",
    "device = \"cuda\"\n",
    "shapes = torch.tensor([(5, 5), (7, 7)], device=device)\n",
    "nruns = 5\n",
    "dim_sizes = [int(shapes[:, dim].prod().item()) for dim in range(shapes.shape[1])]\n",
    "rescaling = True\n",
    "Ds = [10, 31, 100, 310, 1000]\n",
    "\n",
    "\n",
    "def zero():\n",
    "    return torch.zeros(*dim_sizes, device=device)\n",
    "\n",
    "\n",
    "def uniform():\n",
    "    t = torch.ones_like(zero())\n",
    "    return t / t.sum()\n",
    "\n",
    "\n",
    "def degenerate():\n",
    "    t = zero()\n",
    "    t[tuple([0] * shapes.shape[1])] = 1\n",
    "    return t\n",
    "\n",
    "\n",
    "def gaussian(sigma=1):\n",
    "    t = degenerate()\n",
    "    kernel_size = 2 * max(10, 3 * math.ceil(sigma)) + 1\n",
    "    x = torch.arange(kernel_size, device=device) - kernel_size // 2\n",
    "    y = torch.arange(kernel_size, device=device) - kernel_size // 2\n",
    "    x, y = torch.meshgrid(x, y)\n",
    "    kernel = torch.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
    "    kernel = kernel / kernel.sum()\n",
    "\n",
    "    x_padding = calculate_padding(kernel_size, kernel.shape[0], 1)\n",
    "    y_padding = calculate_padding(kernel_size, kernel.shape[1], 1)\n",
    "\n",
    "    padded = torch.nn.functional.pad(\n",
    "        t.unsqueeze(0).unsqueeze(0),\n",
    "        y_padding + x_padding,\n",
    "        mode=\"circular\",\n",
    "    )\n",
    "\n",
    "    convoluted = torch.nn.functional.conv2d(\n",
    "        input=padded, weight=kernel.unsqueeze(0).unsqueeze(0)\n",
    "    )\n",
    "\n",
    "    return convoluted.squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "def bimodal():\n",
    "    t = zero()\n",
    "    index = [0] * shapes.shape[1]\n",
    "    for i, size in enumerate(dim_sizes):\n",
    "        index[i] = size // 2\n",
    "\n",
    "    t[tuple([0] * shapes.shape[1])] = 0.5\n",
    "    t[tuple(index)] = 0.5\n",
    "    return t\n",
    "\n",
    "def bimodal2():\n",
    "    t = zero()\n",
    "    index = [0] * shapes.shape[1]\n",
    "    for i, size in enumerate(dim_sizes):\n",
    "        index[i] = 5\n",
    "\n",
    "    t[tuple([1] * shapes.shape[1])] = 0.5\n",
    "    t[tuple(index)] = 0.5\n",
    "    return t\n",
    "\n",
    "def gaussian_mixture():\n",
    "    g1 = gaussian(1.5)\n",
    "    g2 = gaussian(2)\n",
    "    g3 = gaussian(2.5)\n",
    "\n",
    "    t = (\n",
    "        g1.roll(shifts=(7, 7), dims=(0, 1))\n",
    "        + g2.roll(shifts=(30, 13), dims=(0, 1))\n",
    "        + g3.roll(shifts=(17, 28), dims=(0, 1))\n",
    "    )\n",
    "    t = t / t.sum()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f178d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x = gaussian_mixture().cpu()\n",
    "# plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb85143",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = [\n",
    "    (\"degenerate\", degenerate()),\n",
    "    (\"uniform\", uniform()),\n",
    "    (\"gaussian σ=0.5\", gaussian(0.5)),\n",
    "    (\"gaussian σ=1\", gaussian(1)),\n",
    "    (\"gaussian σ=2\", gaussian(2)),\n",
    "    (\"bimodal\", bimodal()),\n",
    "    ('bimodal2', bimodal2()),\n",
    "    (\"gaussian mixture\", gaussian_mixture()),\n",
    "]\n",
    "\n",
    "scaffold_debug = FourierScaffoldDebug(shapes, device=device, rescale=rescaling)\n",
    "\n",
    "\n",
    "def l2_err(v1: torch.Tensor, v2: torch.Tensor):\n",
    "    return torch.linalg.vector_norm(v1 - v2)\n",
    "\n",
    "\n",
    "def similarity(v1: torch.Tensor, v2: torch.Tensor):\n",
    "    \"\"\" v1 and v2 must be flattened. if v1 and v2 are complex, it takes the absolute value of the inner product\n",
    "    \"\"\"\n",
    "    return (v1 * v2.conj()).sum().abs() / (v1.norm() * v2.norm())\n",
    "\n",
    "\n",
    "def run_test(\n",
    "    distribution: torch.Tensor,\n",
    "    scaffold: FourierScaffold,\n",
    "    scaffold_debug: FourierScaffoldDebug,\n",
    "):\n",
    "    scaffold.g = scaffold.encode_probability(distribution)\n",
    "    scaffold_debug.ptensor = distribution\n",
    "\n",
    "    estimated_probs, true_probs = (\n",
    "        scaffold.get_all_probabilities().flatten().abs(),\n",
    "        scaffold_debug.ptensor.flatten(),\n",
    "    )\n",
    "    prob_similarity, prob_l2 = (\n",
    "        similarity(true_probs, estimated_probs),\n",
    "        l2_err(true_probs, estimated_probs),\n",
    "    )\n",
    "\n",
    "    scaffold.sharpen()\n",
    "    scaffold_debug.sharpen()\n",
    "\n",
    "    true_encodings, generated_encodings = (\n",
    "        scaffold.encode_probability(scaffold_debug.ptensor).flatten(),\n",
    "        scaffold.g.flatten(),\n",
    "    )\n",
    "\n",
    "    sharpened_encoding_similarity, sharpened_encoding_l2 = (\n",
    "        similarity(true_encodings, generated_encodings),\n",
    "        l2_err(true_encodings, generated_encodings),\n",
    "    )\n",
    "\n",
    "    estimated_probs, true_probs = (\n",
    "        scaffold.get_all_probabilities().flatten().abs(),\n",
    "        scaffold_debug.ptensor.flatten(),\n",
    "    )\n",
    "\n",
    "    sharpened_prob_similarity, sharpened_prob_l2 = (\n",
    "        similarity(true_probs, estimated_probs),\n",
    "        l2_err(true_probs, estimated_probs),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sharpened_encoding_similarity,\n",
    "        sharpened_encoding_l2,\n",
    "        prob_similarity,\n",
    "        sharpened_prob_similarity,\n",
    "        prob_l2,\n",
    "        sharpened_prob_l2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcad9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpened_encoding_similarities = torch.zeros(len(distributions), len(Ds), 2, nruns)\n",
    "sharpened_encoding_l2s = torch.zeros(len(distributions), len(Ds), 2, nruns)\n",
    "\n",
    "prob_similarities = torch.zeros(len(distributions), len(Ds), 2, nruns)\n",
    "sharpened_prob_similarities = torch.zeros(len(distributions), len(Ds), 2, nruns)\n",
    "\n",
    "prob_l2s = torch.zeros(len(distributions), len(Ds), 2, nruns)\n",
    "sharpened_prob_l2s = torch.zeros(len(distributions), len(Ds), 2, nruns)\n",
    "\n",
    "\n",
    "for i, [name, distribution] in enumerate(distributions):\n",
    "    for j, D in enumerate(Ds):\n",
    "        for run in range(nruns):\n",
    "            print(\n",
    "                f\" ----------------------- running test: {name} ({i}/{len(distributions)}), D={D} ({j}/{len(Ds)}), run {run}/{nruns} --------------------\"\n",
    "            )\n",
    "            scaffold_v = FourierScaffold(\n",
    "                shapes,\n",
    "                D=D**2,\n",
    "                sharpening=HadamardSharpening(2),\n",
    "                smoothing=GaussianFourierSmoothing(\n",
    "                    kernel_radii=[10, 10], kernel_sigmas=[1, 1]\n",
    "                ),  # doesnt matter just so it runs\n",
    "                device=device,\n",
    "                rescaling=rescaling,\n",
    "                _skip_K_calc=True,\n",
    "            )\n",
    "            scaffold_m = FourierScaffold(\n",
    "                shapes,\n",
    "                D=D,\n",
    "                sharpening=ContractionSharpening(2),\n",
    "                shift=HadamardShiftMatrix(),\n",
    "                smoothing=GuassianFourierSmoothingMatrix(\n",
    "                    kernel_radii=[10, 10], kernel_sigmas=[1, 1]\n",
    "                ),  # doesnt matter just so it runs\n",
    "                device=device,\n",
    "                representation=\"matrix\",\n",
    "                rescaling=rescaling,\n",
    "                _skip_K_calc=True,\n",
    "            )\n",
    "            scaffold_debug = FourierScaffoldDebug(\n",
    "                shapes, device=device, rescale=rescaling\n",
    "            )\n",
    "\n",
    "            (\n",
    "                sharpened_encoding_similarity_v,\n",
    "                sharpened_encoding_l2_v,\n",
    "                prob_similarity_v,\n",
    "                sharpened_prob_similarity_v,\n",
    "                prob_l2_v,\n",
    "                sharpened_prob_l2_v,\n",
    "            ) = run_test(distribution.clone(), scaffold_v, scaffold_debug)\n",
    "            (\n",
    "                sharpened_encoding_similarity_m,\n",
    "                sharpened_encoding_l2_m,\n",
    "                prob_similarity_m,\n",
    "                sharpened_prob_similarity_m,\n",
    "                prob_l2_m,\n",
    "                sharpened_prob_l2_m,\n",
    "            ) = run_test(distribution.clone(), scaffold_m, scaffold_debug)\n",
    "\n",
    "            sharpened_encoding_similarities[i, j, 0, run] = (\n",
    "                sharpened_encoding_similarity_v\n",
    "            )\n",
    "            sharpened_encoding_similarities[i, j, 1, run] = (\n",
    "                sharpened_encoding_similarity_m\n",
    "            )\n",
    "\n",
    "            sharpened_encoding_l2s[i, j, 0, run] = sharpened_encoding_l2_v\n",
    "            sharpened_encoding_l2s[i, j, 1, run] = sharpened_encoding_l2_m\n",
    "\n",
    "            prob_similarities[i, j, 0, run] = prob_similarity_v\n",
    "            prob_similarities[i, j, 1, run] = prob_similarity_m\n",
    "\n",
    "            sharpened_prob_similarities[i, j, 0, run] = sharpened_prob_similarity_v\n",
    "            sharpened_prob_similarities[i, j, 1, run] = sharpened_prob_similarity_m\n",
    "\n",
    "            prob_l2s[i, j, 0, run] = prob_l2_v\n",
    "            prob_l2s[i, j, 1, run] = prob_l2_m\n",
    "\n",
    "            sharpened_prob_l2s[i, j, 0, run] = sharpened_prob_l2_v\n",
    "            sharpened_prob_l2s[i, j, 1, run] = sharpened_prob_l2_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {\n",
    "    \"sharpened_encoding_similarities\": sharpened_encoding_similarities,\n",
    "    \"sharpened_encoding_l2s \": sharpened_encoding_l2s,\n",
    "    \"prob_similarities \": prob_similarities,\n",
    "    \"sharpened_prob_similarities \": sharpened_prob_similarities,\n",
    "    \"prob_l2s \": prob_l2s,\n",
    "    \"sharpened_prob_l2s \": sharpened_prob_l2s,\n",
    "}\n",
    "\n",
    "if rescaling:\n",
    "    with open(\"results_rescaling.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "else:\n",
    "    with open(\"results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, [name, distribution] in enumerate(distributions):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds)**2).tolist(),\n",
    "        prob_similarities[i, :, 0].mean(dim=1),\n",
    "        yerr=prob_similarities[i, :, 0].std(dim=1),\n",
    "        label=\"original\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds)**2).tolist(),\n",
    "        sharpened_prob_similarities[i, :, 0].mean(dim=1),\n",
    "        yerr=sharpened_prob_similarities[i, :, 0].std(dim=1),\n",
    "        label=\"sharpened (hadamard)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylabel(\"cosine similarity\")\n",
    "    ax[0].set_xlabel(\"D\")\n",
    "    # ax[0].set_ylim(-0.2,1)\n",
    "    ax[0].set_title(f\"hadamard sharpening\")\n",
    "\n",
    "    ax[1].errorbar(\n",
    "        Ds,\n",
    "        prob_similarities[i, :, 1].mean(dim=1),\n",
    "        yerr=prob_similarities[i, :, 1].std(dim=1),\n",
    "        label=\"original\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].errorbar(\n",
    "        Ds,\n",
    "        prob_similarities[i, :, 1].mean(dim=1),\n",
    "        yerr=prob_similarities[i, :, 1].std(dim=1),\n",
    "        label=\"sharpened (contraction)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].set_xscale(\"log\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylabel(\"cosine similarity\")\n",
    "    ax[1].set_xlabel(\"D\")\n",
    "    # ax[1].set_ylim(-0.2,1)\n",
    "    ax[1].set_title(f\"contraction sharpening\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Mean cosine similarity between true and recovered original distribution and sharpened distribution for hammard and contraction sharpening. distribution={name}.\\nshapes={shapes.cpu().tolist()}\"\n",
    "    )\n",
    "    if rescaling:\n",
    "        fig.savefig(f'fourier_scaffold_contraction_testing_cosine_sim_{i}.png')\n",
    "    else:\n",
    "        fig.savefig(f'fourier_scaffold_contraction_testing_no_rescaling_cosine_sim_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b842686",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, [name, distribution] in enumerate(distributions):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds) ** 2).tolist(),\n",
    "        prob_l2s[i, :, 0].mean(dim=1),\n",
    "        yerr=prob_l2s[i, :, 0].std(dim=1),\n",
    "        label=\"original\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds) ** 2).tolist(),\n",
    "        sharpened_prob_l2s[i, :, 0].mean(dim=1),\n",
    "        yerr=sharpened_prob_l2s[i, :, 0].std(dim=1),\n",
    "        label=\"sharpened (hadamard)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylabel(\"||true-estimated||₂\")\n",
    "    ax[0].set_xlabel(\"D\")\n",
    "    ax[0].set_ylim(0, 1)\n",
    "    ax[0].set_title(f\"hadamard sharpening\")\n",
    "\n",
    "    ax[1].errorbar(\n",
    "        Ds,\n",
    "        prob_l2s[i, :, 1].mean(dim=1),\n",
    "        yerr=prob_l2s[i, :, 1].std(dim=1),\n",
    "        label=\"original\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].errorbar(\n",
    "        Ds,\n",
    "        sharpened_prob_l2s[i, :, 1].mean(dim=1),\n",
    "        yerr=sharpened_prob_l2s[i, :, 1].std(dim=1),\n",
    "        label=\"sharpened (contraction)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].set_xscale(\"log\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylabel(\"||true-estimated||₂\")\n",
    "    ax[1].set_xlabel(\"D\")\n",
    "    ax[1].set_ylim(0, 1)\n",
    "    ax[1].set_title(f\"contraction sharpening\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Mean l2 error between true and recovered original distribution and sharpened distribution for hammard and contraction sharpening. distribution={name}.\\nshapes={shapes.cpu().tolist()}\"\n",
    "    )\n",
    "    if rescaling:\n",
    "        fig.savefig(f\"fourier_scaffold_contraction_testing_l2err_{i}.png\")\n",
    "    else:\n",
    "        fig.savefig(f\"fourier_scaffold_contraction_testing_no_rescaling_l2err_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b02de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, [name, distribution] in enumerate(distributions):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(11, 5))\n",
    "\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds) ** 2).tolist(),\n",
    "        prob_l2s[i, :, 0].mean(dim=1),\n",
    "        yerr=prob_l2s[i, :, 0].std(dim=1),\n",
    "        label=\"original\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds) ** 2).tolist(),\n",
    "        sharpened_prob_l2s[i, :, 0].mean(dim=1),\n",
    "        yerr=sharpened_prob_l2s[i, :, 0].std(dim=1),\n",
    "        label=\"sharpened (hadamard)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylabel(\"||true-estimated||₂\")\n",
    "    ax[0].set_xlabel(\"D\")\n",
    "    ax[0].set_ylim(0, 1)\n",
    "    ax[0].set_title(f\"hadamard sharpening\")\n",
    "\n",
    "    ax[1].errorbar(\n",
    "        Ds,\n",
    "        prob_l2s[i, :, 1].mean(dim=1),\n",
    "        yerr=prob_l2s[i, :, 1].std(dim=1),\n",
    "        label=\"original\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].errorbar(\n",
    "        Ds,\n",
    "        sharpened_prob_l2s[i, :, 1].mean(dim=1),\n",
    "        yerr=sharpened_prob_l2s[i, :, 1].std(dim=1),\n",
    "        label=\"sharpened (contraction)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].set_xscale(\"log\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylabel(\"||true-estimated||₂\")\n",
    "    ax[1].set_xlabel(\"D\")\n",
    "    ax[1].set_ylim(0, 1)\n",
    "    ax[1].set_title(f\"contraction sharpening\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Mean l2 error between true and recovered original distribution and sharpened distribution for hammard and contraction sharpening. distribution={name}.\\nshapes={shapes.cpu().tolist()}\"\n",
    "    )\n",
    "    if rescaling:\n",
    "        fig.savefig(f\"fourier_scaffold_contraction_testing_l2err_{i}.png\")\n",
    "    else:\n",
    "        fig.savefig(f\"fourier_scaffold_contraction_testing_no_rescaling_l2err_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, [name, distribution] in enumerate(distributions):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds)).tolist(),\n",
    "        sharpened_encoding_similarities[i, :, 0].mean(dim=1),\n",
    "        yerr=sharpened_encoding_similarities[i, :, 0].std(dim=1),\n",
    "        label=\"hadamard (uses D² parameters)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].errorbar(\n",
    "        (torch.tensor(Ds)).tolist(),\n",
    "        sharpened_encoding_similarities[i, :, 1].mean(dim=1),\n",
    "        yerr=sharpened_encoding_similarities[i, :, 1].std(dim=1),\n",
    "        label=\"contraction\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylabel(\"similarity(true, computed)\")\n",
    "    ax[0].set_xlabel(\"D\")\n",
    "    ax[0].set_ylim(0, 1)\n",
    "    ax[1].errorbar(\n",
    "        (torch.tensor(Ds)).tolist(),\n",
    "        sharpened_encoding_l2s[i, :, 0].mean(dim=1),\n",
    "        yerr=sharpened_encoding_l2s[i, :, 0].std(dim=1),\n",
    "        label=\"hadamard (uses D² parameters)\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].errorbar(\n",
    "        (torch.tensor(Ds)).tolist(),\n",
    "        sharpened_encoding_l2s[i, :, 1].mean(dim=1),\n",
    "        yerr=sharpened_encoding_l2s[i, :, 1].std(dim=1),\n",
    "        label=\"contraction\",\n",
    "        capsize=3,\n",
    "        fmt=\"--o\",\n",
    "    )\n",
    "    ax[1].set_xscale(\"log\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylabel(\"||true, computed||₂\")\n",
    "    ax[1].set_xlabel(\"D\")\n",
    "    ax[1].set_ylim(0, 1)\n",
    "\n",
    "    ax[0].set_title(f'mean cosine sim between true and computed sharpened encodings')\n",
    "    ax[1].set_title(f'mean l2 error between true and computed sharpened encodings')\n",
    "    fig.suptitle(f\"distribution={name}; shapes={shapes.tolist()}\")\n",
    "    if rescaling:\n",
    "        fig.savefig(\n",
    "            f\"fourier_scaffold_contraction_testing_encodings_{i}.png\"\n",
    "        )\n",
    "    else:\n",
    "        fig.savefig(\n",
    "            f\"fourier_scaffold_contraction_testing_encodings_no_rescaling_{i}.png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772c2e6",
   "metadata": {},
   "source": [
    "Qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fourier_scaffold import (\n",
    "    FourierScaffold,\n",
    "    FourierScaffoldDebug,\n",
    "    HadamardSharpening,\n",
    "    ContractionSharpening,\n",
    "    GaussianFourierSmoothing,\n",
    "    GuassianFourierSmoothingMatrix,\n",
    "    HadamardShiftMatrix,\n",
    "    calculate_padding,\n",
    ")\n",
    "\n",
    "D=1000\n",
    "scaffold_debug = FourierScaffoldDebug(shapes, device=device, rescale=rescaling)\n",
    "distributions = [\n",
    "    (\"degenerate\", degenerate()),\n",
    "    (\"uniform\", uniform()),\n",
    "    (\"gaussian σ=0.5\", gaussian(0.5)),\n",
    "    (\"gaussian σ=1\", gaussian(1)),\n",
    "    (\"gaussian σ=2\", gaussian(2)),\n",
    "    (\"bimodal\", bimodal()),\n",
    "    ('bimodal2', bimodal2()),\n",
    "    (\"gaussian mixture\", gaussian_mixture()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_utils import plot_imgs_side_by_side\n",
    "\n",
    "dummy_scaffold = FourierScaffold(\n",
    "    shapes,\n",
    "    D=D,\n",
    "    smoothing=GaussianFourierSmoothing(\n",
    "        kernel_radii=[10, 10], kernel_sigmas=[1, 1]\n",
    "    ),  # doesnt matter just so it runs\n",
    "    device=device,\n",
    "    representation=\"vector\",\n",
    "    rescaling=rescaling,\n",
    ")\n",
    "\n",
    "scaffolds = [\n",
    "    FourierScaffold(\n",
    "        shapes,\n",
    "        D=D,\n",
    "        smoothing=GaussianFourierSmoothing(\n",
    "            kernel_radii=[10, 10], kernel_sigmas=[1, 1]\n",
    "        ),  # doesnt matter just so it runs\n",
    "        device=device,\n",
    "        representation=\"vector\",\n",
    "        rescaling=rescaling,\n",
    "        features=dummy_scaffold.features,\n",
    "    ),\n",
    "    FourierScaffold(\n",
    "        shapes,\n",
    "        D=D,\n",
    "        sharpening=ContractionSharpening(2),\n",
    "        shift=HadamardShiftMatrix(),\n",
    "        smoothing=GuassianFourierSmoothingMatrix(\n",
    "            kernel_radii=[10, 10], kernel_sigmas=[1, 1]\n",
    "        ),  # doesnt matter just so it runs\n",
    "        device=device,\n",
    "        representation=\"matrix\",\n",
    "        rescaling=rescaling,\n",
    "        features=dummy_scaffold.features,\n",
    "    ),\n",
    "]\n",
    "names = [\"hadamard\", \"contraction\"]\n",
    "\n",
    "for scaffold, contraction_name in zip(scaffolds, names):\n",
    "    for name, distribution in distributions:\n",
    "        scaffold_debug.ptensor = distribution\n",
    "        scaffold.g = scaffold.encode_probability(distribution)\n",
    "        original = scaffold.get_all_probabilities().cpu().clone()\n",
    "\n",
    "        l2_o = l2_err(original, distribution.cpu())\n",
    "        sim_o = similarity(original.flatten(), distribution.cpu().flatten())\n",
    "\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "        plot_imgs_side_by_side(\n",
    "            [distribution.cpu(), original.cpu()],\n",
    "            ax[0],\n",
    "            [\n",
    "                \"distribution\",\n",
    "                f\"encoded distribution\\nsimilarity={sim_o:.3f}, ||true-encoding||₂ = {l2_o:.3f}\",\n",
    "            ],\n",
    "            fig,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        scaffold.sharpen()\n",
    "        scaffold_debug.sharpen()\n",
    "\n",
    "        sharpened = scaffold.get_all_probabilities().cpu().clone()\n",
    "\n",
    "        encoding_similarity = similarity(\n",
    "            scaffold.g.flatten(),\n",
    "            scaffold.encode_probability(scaffold_debug.ptensor).flatten(),\n",
    "        )\n",
    "        encoding_l2err = l2_err(\n",
    "            scaffold.g.flatten(),\n",
    "            scaffold.encode_probability(scaffold_debug.ptensor).flatten(),\n",
    "        )\n",
    "        l2_s = l2_err(sharpened, scaffold_debug.ptensor.cpu())\n",
    "        sim_s = similarity(sharpened.flatten(), scaffold_debug.ptensor.cpu().flatten())\n",
    "        print(l2_o, l2_s)\n",
    "        print(sharpened)\n",
    "        print(sharpened - scaffold_debug.ptensor.cpu())\n",
    "        plot_imgs_side_by_side(\n",
    "            [scaffold_debug.ptensor.cpu(), sharpened.cpu()],\n",
    "            ax[1],\n",
    "            [\n",
    "                \"sharpened\",\n",
    "                f\"encoded sharpened\\nsimilarity={sim_s:.3f}, ||true-encoding||₂ = {l2_s:.3f}, encoding_similarity={encoding_similarity:.3f}, encoding_l2={encoding_l2err:.3f}\",\n",
    "            ],\n",
    "            fig,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        fig.suptitle(\n",
    "            f\"original vs. sharpened {name}, D={D if contraction_name == 'contraction' else D**2}, sharpening={contraction_name}, shapes={shapes.tolist()}\"\n",
    "        )\n",
    "        if rescaling:\n",
    "            fig.savefig(\n",
    "                f\"org_vs_sharp_D={D}, dist={name} sharpening={contraction_name}.png\"\n",
    "            )\n",
    "        else:\n",
    "            fig.savefig(\n",
    "                f\"org_vs_sharp_no_rescaling_D={D}, dist={name} sharpening={contraction_name}.png\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fe6b2",
   "metadata": {},
   "source": [
    "Cross term error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = FourierScaffold(\n",
    "    shapes,\n",
    "    1000,\n",
    "    shift=HadamardShiftMatrix(),\n",
    "    smoothing=GuassianFourierSmoothingMatrix([10,10] * 1, [0.4,0.4] * 1),\n",
    "    sharpening=ContractionSharpening(2),\n",
    "    representation='matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = torch.cartesian_prod(*[torch.arange(0, dim_sizes[dim]) for dim in range(shapes.shape[1])])\n",
    "if (omega.ndim == 1):\n",
    "  omega = omega.unsqueeze(1)\n",
    "print(omega.shape)\n",
    "encodings = x.encode_batch(omega.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(omega.shape)\n",
    "print(encodings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52dc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = encodings.flatten(0,1)\n",
    "print(encodings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (encodings.T @ encodings.conj()) ** 0.5\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(15,15))\n",
    "im = ax.imshow(results.abs())\n",
    "ax.set_xlabel('i')\n",
    "ax.set_xlabel('j')\n",
    "ax.set_title(f'|<g(k_i), g(k_j)>| for each i,j in Ω. shapes={shapes.tolist()}, D=200, sharpening=contraction')\n",
    "fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(15,15))\n",
    "ax.set_xlabel('i')\n",
    "ax.set_xlabel('j')\n",
    "ax.set_title(f'log(<g(k_i), g(k_j)>) for each i,j in Ω[:200]. shapes={shapes.tolist()}, D=200, sharpening=contraction')\n",
    "im = ax.imshow(results[:200, :200].abs().log())\n",
    "fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = results - torch.diag(torch.ones(len(omega)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error.abs().max())\n",
    "print(error.abs().min())\n",
    "print(error.abs().mean())\n",
    "print(error.abs().std())\n",
    "print(error.abs().sum())\n",
    "print(len(error.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(error.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ba222",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = torch.arange(-15, 0, 0.5)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('error')\n",
    "plt.title('counts of logs of error')\n",
    "plt.hist((error.abs()+1e-8).log().flatten(), bins=bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
