{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "lambdas = [3, 4, 5]  # module period\n",
    "shapes = [(l, l) for l in lambdas]\n",
    "M = len(lambdas)  # num modules\n",
    "Ng = np.sum(np.square(lambdas))  # num grid cells\n",
    "Npos = np.prod(lambdas)\n",
    "Npos = Npos * Npos\n",
    "Ns = 2000  # 84*84*3                           # num of sensory cells set at Npos, can be larger\n",
    "Np_lst = [400]  # np.arange(25, 425, 25)     # num place cells\n",
    "pflip = 0.0  # measure of noise injected in s (prob of flipping if binary, gaussian noise if cts)\n",
    "Niter = 1  # number of iterations for scaffold dynamics\n",
    "nruns = 1\n",
    "sparsity = 0  # Dummy param for older code, not used currently\n",
    "Npatts_lst = np.arange(1, 3 * 3 * 4 * 4 * 5 * 5, 200)  # number of patterns to train on\n",
    "# Npatts is 1, 201, 401 etc... 3401\n",
    "\n",
    "smoothing_methods = [\"argmax\", \"softmax\", \"polynomial\"]\n",
    "pseudoinverse_methods = [\"exact\", \"iterative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_utils import capacity1\n",
    "from data_utils import load_mnist_dataset, prepare_data\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# dataset = load_mnist_dataset()\n",
    "# data, _ = prepare_data(dataset, num_imgs=3600, preprocess_sensory=\"false\")\n",
    "# data = data.numpy().T\n",
    "# sign_output=False\n",
    "\n",
    "data = np.sign(randn(Ns, Npos))\n",
    "sign_output=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_h_l2_results = np.zeros(\n",
    "    (\n",
    "        len(smoothing_methods),\n",
    "        len(pseudoinverse_methods),\n",
    "        len(Np_lst),\n",
    "        len(Npatts_lst),\n",
    "        nruns,\n",
    "    ),\n",
    ")\n",
    "err_s_l2_results = np.zeros(\n",
    "    (\n",
    "        len(smoothing_methods),\n",
    "        len(pseudoinverse_methods),\n",
    "        len(Np_lst),\n",
    "        len(Npatts_lst),\n",
    "        nruns,\n",
    "    ),\n",
    ")\n",
    "err_s_l1_results = np.zeros(\n",
    "    (\n",
    "        len(smoothing_methods),\n",
    "        len(pseudoinverse_methods),\n",
    "        len(Np_lst),\n",
    "        len(Npatts_lst),\n",
    "        nruns,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i, smoothing_method in enumerate(smoothing_methods):\n",
    "    for j, pseudoinverse_method in enumerate(pseudoinverse_methods):\n",
    "        err_h_l2, err_s_l2, err_s_l1 = capacity1(\n",
    "            shapes,\n",
    "            Np_lst,\n",
    "            Npatts_lst,\n",
    "            nruns,\n",
    "            data,\n",
    "            device,\n",
    "            pseudoinverse_method=pseudoinverse_method,\n",
    "            smoothing_method=smoothing_method,\n",
    "            sign_output=sign_output,\n",
    "        )\n",
    "        err_h_l2_results[i, j] = err_h_l2.cpu().numpy()\n",
    "        err_s_l2_results[i, j] = err_s_l2.cpu().numpy()\n",
    "        err_s_l1_results[i, j] = err_s_l1.cpu().numpy()\n",
    "\n",
    "\n",
    "# Place states chosen to be random vectors with same sparsity as base case (teal curves in Fig. 3)\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_random_sparse_p, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "# gbook, Npatts_lst, nruns, Ns, sbook, sparsity)\n",
    "# Assuming linear hippocampal activations\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_linear_p, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "# gbook, Npatts_lst, nruns, Ns, sbook, sparsity)\n",
    "\n",
    "# Use gbook as a spiraling outward + linear activation (for SI Fig. S13)\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_linear_p_spiral, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "# gbook, Npatts_lst, nruns, Ns, sbook, sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "Npatts = np.array(nruns * [Npatts_lst])  # Npatts_lst repeated nruns times\n",
    "Npatts = Npatts.T\n",
    "\n",
    "for i, smoothing_method in enumerate(smoothing_methods):\n",
    "    for j, pseudoinverse_method in enumerate(pseudoinverse_methods):\n",
    "        normlizd_l1 = err_s_l1_results[i, j]\n",
    "        m = 1 - (2 * normlizd_l1)\n",
    "        a = (1 + m) / 2\n",
    "        b = (1 - m) / 2\n",
    "        a = torch.abs(torch.tensor(a))\n",
    "        b = torch.abs(torch.tensor(b)).cpu()\n",
    "        S = -a * np.log2(a) - b * np.log2(b)\n",
    "        S = np.where(m == 1, np.zeros_like(S), S)\n",
    "        MI = 1 - S\n",
    "\n",
    "        if pseudoinverse_method == \"iterative\":\n",
    "            label = f\"iterative pseudoinverse (ε_hs = 0.1, ε_sh=0.1, hidden_layer_factor=1, smoothing={smoothing_method}\"\n",
    "        elif pseudoinverse_method == \"exact\":\n",
    "            label = f\"analytic pseudoinverse, smoothing={smoothing_method}\"\n",
    "        plt.errorbar(\n",
    "            Npatts_lst, MI[0].mean(axis=1), yerr=MI[0].std(axis=1), lw=2, label=label\n",
    "        )\n",
    "\n",
    "vhash_y = [\n",
    "    1.000000000000000000e00,\n",
    "    1.000000000000000000e00,\n",
    "    1.000000000000000000e00,\n",
    "    5.988623183160277641e-01,\n",
    "    3.667958255856974548e-01,\n",
    "    2.624110436154711845e-01,\n",
    "    2.042300801824028511e-01,\n",
    "    1.672434617281599589e-01,\n",
    "    1.414727808416358368e-01,\n",
    "    1.225660944022268772e-01,\n",
    "    1.082352629751366369e-01,\n",
    "    9.674044810282866891e-02,\n",
    "    8.747471863732059205e-02,\n",
    "    7.977915334088647725e-02,\n",
    "    7.342708729082536578e-02,\n",
    "    6.793351052792084843e-02,\n",
    "    6.324575644685004328e-02,\n",
    "    5.912155577074185153e-02,\n",
    "]\n",
    "\n",
    "\n",
    "plt.errorbar(Npatts_lst, vhash_y, lw=2, label=\"vectorhash\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    f\"MI per inp bit vs num patts (N_h={Np_lst[0]}, sparsity=0.6, relu_theta=0.5)\"\n",
    ")\n",
    "\n",
    "plt.xlim(xmin=100)\n",
    "# plt.ylim(ymin=0, ymax=1)\n",
    "plt.ylabel(\"MI per inp bit\")\n",
    "plt.xlabel(\"num patts\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Baselines\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "def cap(W,bound):\n",
    "    W1=torch.where(W>bound,bound*torch.ones(W.shape),W)\n",
    "    W2=torch.where(W1<-bound,-bound*torch.ones(W.shape),W1)\n",
    "    return W2\n",
    "\n",
    "def corrupt_p(codebook,p=0.1,booktype='-11'):\n",
    "    rand_indices = torch.sign(torch.random.uniform(size=codebook.shape)- p )\n",
    "    if booktype=='-11':\n",
    "        return torch.multiply(codebook,rand_indices)\n",
    "    elif booktype=='01':\n",
    "        return abs(codebook - 0.5*(-rand_indices+1))\n",
    "    elif booktype=='cts':\n",
    "        return codebook + torch.random.normal(0,1,size=codebook.shape)*p\n",
    "    else:\n",
    "        print(\"codebook should be -11; 01; or cts\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_weights(patterns,connectivity):\n",
    "    if connectivity is 'standard':\n",
    "        if learning == 'hebbian':\n",
    "            W = patts @ patts.T\n",
    "        elif learning == 'sparsehebbian':\n",
    "            prob = sparsity #np.sum(patts)/patts.shape[0]/patts.shape[1]\n",
    "            W =(1/patts.shape[0])* (patts - prob) @ (patts.T - prob)\n",
    "        elif learning == 'pinv':\n",
    "            W= patts @ np.linalg.pinv(patts)\n",
    "        elif learning == 'bounded_hebbian':\n",
    "            num_patts = patts.shape[1]\n",
    "            num_nodes = patts.shape[0]\n",
    "            W = np.zeros((num_nodes,num_nodes))\n",
    "            for i in range(num_patts):\n",
    "                Wtmp = np.outer(patts[:,i] , patts[:,i])/np.sqrt(num_nodes)\n",
    "                # ~ print(np.amax(Wtmp))\n",
    "                W = cap(Wtmp + W,bound)\n",
    "        W = W - torch.diag(torch.diag(W))\n",
    "    else:\n",
    "        N = connectivity.shape[0]\n",
    "        W = sparse.lil_matrix(connectivity.shape)\n",
    "        for i in range(N):\n",
    "            for j in connectivity.rows[i]:\n",
    "                W[i,j] = np.dot(patterns[i],patterns[j])\n",
    "        W.setdiag(0)\n",
    "    return W\n",
    "\n",
    "\n",
    "def entropy(inlist):\n",
    "    ent = np.zeros(len(inlist))\n",
    "    for idx,x in enumerate(inlist):\n",
    "        if x == 0 or x == 1:\n",
    "            ent[idx] = 0\n",
    "        else:\n",
    "            ent[idx] = -1 * ( x*np.log2(x) + (1-x)*np.log2(1-x) )\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns=1\n",
    "iterations=100\n",
    "N = 708\n",
    "corrupt_fraction = 0.0\n",
    "Npatts_list = np.arange(1,800,10)\n",
    "connectivity='standard' # Standard fully connected Hopfield network. For sparse connectivity use the next cell\n",
    "# learning can be 'hebbian', 'bounded_hebbian', 'pinv', or 'sparsehebbian' for sparse hopfield network\n",
    "learning='bounded_hebbian'\n",
    "bound=0.3  #Use bound param if learning='bounded_hebbian'\n",
    "\n",
    "init_overlap = torch.zeros((nruns,*Npatts_list.shape))\n",
    "final_overlap = torch.zeros((nruns,*Npatts_list.shape))\n",
    "MI_hc = torch.zeros((nruns,*Npatts_list.shape))\n",
    "\n",
    "\n",
    "for runidx in range(nruns):\n",
    "    print(\"runidx = \"+str(runidx))\n",
    "    \n",
    "    if learning == 'sparsehebbian':\n",
    "        # sparse hopfiled 0/1 code\n",
    "        sparsity = 0.2\n",
    "        patterns = 1*(torch.random.rand(N,Npatts_list.max()) > (1-sparsity))\n",
    "    else:\n",
    "        patterns = torch.sign(torch.random.normal(0,1,(N,Npatts_list.max())))\n",
    "\n",
    "    \n",
    "    for idx,Npatts in enumerate(tqdm(Npatts_list)):\n",
    "        #print(Npatts)\n",
    "        patts = patterns[:,:Npatts]\n",
    "        cor_patts = patterns[:,:Npatts]\n",
    "        W = get_weights(patts,connectivity)\n",
    "        \n",
    "        if learning == 'sparsehebbian':\n",
    "            # sparse hopfield\n",
    "            theta = torch.sum(W-torch.diag(W), axis=1)\n",
    "            theta=0.05 #0.04 #0\n",
    "            rep = (torch.sign(W@cor_patts - theta)+1)/2            \n",
    "        else:\n",
    "            rep = torch.sign(W@cor_patts)\n",
    "\n",
    "        init_overlap[runidx,idx] = np.average(np.einsum('ij,ij->j',rep,patts)/N) \n",
    "\n",
    "        rep1 = np.copy(rep)\n",
    "        for ite in range(iterations-1):\n",
    "            if learning == 'sparsehebbian':\n",
    "                rep = (np.sign(W@rep - theta)+1)/2\n",
    "            else:\n",
    "                rep = np.sign(W@rep)\n",
    "            \n",
    "            if np.sum(abs(rep - rep1))>0:\n",
    "                rep1 = np.copy(rep)\n",
    "            else:\n",
    "                # print(\"converged at \"+str(ite))\n",
    "                break\n",
    "        err = np.einsum('ij,ij->j',rep,patts)/N\n",
    "        overlap = np.average(err) \n",
    "        final_overlap[runidx,idx] = overlap #err\n",
    "        \n",
    "        if learning=='sparsehebbian':\n",
    "            q = np.sum(np.abs(rep), axis=0) / N  # sparse hopfield\n",
    "            m = err\n",
    "            p = np.sum(patts, axis=0)/patts.shape[0]\n",
    "            P1e = 1 - (m/p)\n",
    "            P0e = (q-m)/(1-p)\n",
    "            MI_hc[runidx,idx] =  np.average( entropy(q) - ( p*entropy(P1e) + (1-p)*entropy(P0e) ) )\n",
    "\n",
    "\n",
    "# print(init_overlap)\n",
    "# print(final_overlap)\n",
    "\n",
    "results_dir = \"continuum_results\"\n",
    "# filename = f\"sparseconnhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_gamma={gamma}_iter={iterations}_nruns={nruns}\"\n",
    "filename = f\"stdhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"pinvhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"sparsehopfield__mutualinfo_N={N}_noise={corrupt_fraction}_p={sparsity}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"boundedhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_bound={bound}_iter={iterations}_nruns={nruns}\"\n",
    "\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "plt.plot(Npatts_list,init_overlap.mean(axis=0), label='single, corrupt='+str(corrupt_fraction));\n",
    "plt.plot(Npatts_list,final_overlap.mean(axis=0), label='final, corrupt='+str(corrupt_fraction));\n",
    "plt.legend()\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel(\"Overlap\");\n",
    "plt.title(r\"N = \"+str(N)+\", $W$\");\n",
    "plt.show()\n",
    "# exit()\n",
    "# fig1.savefig(f\"{results_dir}/Overlap_{filename}.png\")\n",
    "\n",
    "if learning=='sparsehebbian':\n",
    "    print(\"MI already calculated in loop\")\n",
    "else:\n",
    "    m = final_overlap\n",
    "    a = (1+m)/2\n",
    "    b = (1-m)/2\n",
    "\n",
    "    S = - a * np.log2(a) - b * np.log2(b)\n",
    "    S = np.where(m==1, np.zeros_like(S), S)\n",
    "\n",
    "    MI_hc = 1 - S\n",
    "\n",
    "\n",
    "fig2 = plt.figure(1)\n",
    "plt.errorbar(Npatts_list,MI_hc.mean(axis=0),yerr=MI_hc.std(axis=0), label='final, corrupt='+str(corrupt_fraction)); #plt.xscale('log'); plt.yscale('log');\n",
    "plt.legend()\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel(\"MI\");\n",
    "plt.title(r\"N = \"+str(N)+\", $W$\");\n",
    "plt.show()\n",
    "# fig2.savefig(f\"{results_dir}/MI_{filename}.png\")\n",
    "\n",
    "data = {\n",
    "    \"N\": N,\n",
    "    \"init_overlap\": init_overlap,\n",
    "    \"m\": final_overlap,\n",
    "    \"MI\": MI_hc,\n",
    "    \"Npatts_list\": Npatts_list,\n",
    "    \"noise\": corrupt_fraction,\n",
    "    # \"q\": q  #needed for sparse hebbian\n",
    "    # \"bound\": bound #needed for bounded hopfield\n",
    "}\n",
    "# write_pkl(f\"{results_dir}/{filename}\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
