{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "lambdas = [3, 4, 5]  # module period\n",
    "shapes = [(l, l) for l in lambdas]\n",
    "M = len(lambdas)  # num modules\n",
    "Ng = np.sum(np.square(lambdas))  # num grid cells\n",
    "Npos = np.prod(lambdas)\n",
    "Npos = Npos * Npos\n",
    "Ns = 2000  # 84*84*3                           # num of sensory cells set at Npos, can be larger\n",
    "Np_lst = [400]  # np.arange(25, 425, 25)     # num place cells\n",
    "pflip = 0.0  # measure of noise injected in s (prob of flipping if binary, gaussian noise if cts)\n",
    "Niter = 1  # number of iterations for scaffold dynamics\n",
    "nruns = 1\n",
    "sparsity = 0  # Dummy param for older code, not used currently\n",
    "sbook = np.sign(randn(Ns, Npos))\n",
    "Npatts_lst = np.arange(1, 3 * 3 * 4 * 4 * 5 * 5, 200)  # number of patterns to train on\n",
    "\n",
    "# Npatts is 1, 201, 401 etc... 3401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Choose experiment to run\n",
    "\n",
    "# Base case\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1, ga, gd, gt = capacity(senstrans_gs_vectorized_patts, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "#                                           gbook, Npatts_lst, nruns, Ns, sbook, sparsity, noise_level=\"none\", grid_scaffold=gs, W_hg_mean=W_hg_mean, W_hg_std=W_hg_std)\n",
    "from test_utils import capacity1\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "err_h_l2, err_s_l2, err_s_l1 = capacity1(\n",
    "    shapes, Np_lst, Npatts_lst, nruns, sbook, device, \"iterative\"\n",
    ")\n",
    "\n",
    "# Place states chosen to be random vectors with same sparsity as base case (teal curves in Fig. 3)\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_random_sparse_p, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "# gbook, Npatts_lst, nruns, Ns, sbook, sparsity)\n",
    "# Assuming linear hippocampal activations\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_linear_p, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "# gbook, Npatts_lst, nruns, Ns, sbook, sparsity)\n",
    "\n",
    "# Use gbook as a spiraling outward + linear activation (for SI Fig. S13)\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_linear_p_spiral, lambdas, Ng, Np_lst, pflip, Niter, Npos,\n",
    "# gbook, Npatts_lst, nruns, Ns, sbook, sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_h_l2_2, err_s_l2_2, err_s_l1_2 = capacity1(\n",
    "    shapes, Np_lst, Npatts_lst, nruns, sbook, device, \"exact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(err_s_l1)\n",
    "print(err_s_l2)\n",
    "print(err_h_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "normlizd_l1 = err_s_l1.cpu().numpy()\n",
    "normlizd_l1_2 = err_s_l1_2.cpu().numpy()\n",
    "Npatts = np.array(nruns * [Npatts_lst])  # Npatts_lst repeated nruns times\n",
    "Npatts = Npatts.T\n",
    "print(Npatts.shape)\n",
    "\n",
    "m = 1 - (2 * normlizd_l1)\n",
    "m_2 = 1 - (2 * normlizd_l1_2)\n",
    "\n",
    "a = (1 + m) / 2\n",
    "b = (1 - m) / 2\n",
    "a_2 = (1 + m_2) / 2\n",
    "b_2 = (1 - m_2) / 2\n",
    "\n",
    "vhash_y = [\n",
    "    1.000000000000000000e00,\n",
    "    1.000000000000000000e00,\n",
    "    1.000000000000000000e00,\n",
    "    5.988623183160277641e-01,\n",
    "    3.667958255856974548e-01,\n",
    "    2.624110436154711845e-01,\n",
    "    2.042300801824028511e-01,\n",
    "    1.672434617281599589e-01,\n",
    "    1.414727808416358368e-01,\n",
    "    1.225660944022268772e-01,\n",
    "    1.082352629751366369e-01,\n",
    "    9.674044810282866891e-02,\n",
    "    8.747471863732059205e-02,\n",
    "    7.977915334088647725e-02,\n",
    "    7.342708729082536578e-02,\n",
    "    6.793351052792084843e-02,\n",
    "    6.324575644685004328e-02,\n",
    "    5.912155577074185153e-02,\n",
    "]\n",
    "\n",
    "a = torch.abs(torch.tensor(a))\n",
    "b = torch.abs(torch.tensor(b)).cpu()\n",
    "\n",
    "a_2 = torch.abs(torch.tensor(a_2))\n",
    "b_2 = torch.abs(torch.tensor(b_2)).cpu()\n",
    "\n",
    "S = -a * np.log2(a) - b * np.log2(b)\n",
    "S = np.where(m == 1, np.zeros_like(S), S)\n",
    "\n",
    "S_2 = -a_2 * np.log2(a_2) - b_2 * np.log2(b_2)\n",
    "S_2 = np.where(m == 1, np.zeros_like(S_2), S_2)\n",
    "\n",
    "MI = 1 - S\n",
    "MI_2 = 1 - S_2\n",
    "plt.errorbar(\n",
    "    Npatts_lst,\n",
    "    MI[0].mean(axis=1),\n",
    "    yerr=MI[0].std(axis=1),\n",
    "    lw=2,\n",
    "    label=\"iterative pseudoinverse (ε_hs = 0.1, ε_sh=0.1, hidden_layer_factor=1)\",\n",
    ")\n",
    "plt.errorbar(\n",
    "    Npatts_lst,\n",
    "    MI_2[0].mean(axis=1),\n",
    "    yerr=MI_2[0].std(axis=1),\n",
    "    lw=2,\n",
    "    label=\"analytic pseudoinverse\",\n",
    ")\n",
    "plt.errorbar(Npatts_lst, vhash_y, lw=2, label=\"vectorhash\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"MI per inp bit vs num patts (N_h=400, sparsity=0.6, relu_theta=0.5)\")\n",
    "\n",
    "plt.xlim(xmin=100)\n",
    "plt.ylabel(\"MI per inp bit\")\n",
    "plt.xlabel(\"num patts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Baselines\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "def cap(W, bound):\n",
    "    W1 = torch.where(W > bound, bound * torch.ones(W.shape), W)\n",
    "    W2 = torch.where(W1 < -bound, -bound * torch.ones(W.shape), W1)\n",
    "    return W2\n",
    "\n",
    "\n",
    "def corrupt_p(codebook, p=0.1, booktype=\"-11\"):\n",
    "    rand_indices = torch.sign(torch.random.uniform(size=codebook.shape) - p)\n",
    "    if booktype == \"-11\":\n",
    "        return torch.multiply(codebook, rand_indices)\n",
    "    elif booktype == \"01\":\n",
    "        return abs(codebook - 0.5 * (-rand_indices + 1))\n",
    "    elif booktype == \"cts\":\n",
    "        return codebook + torch.random.normal(0, 1, size=codebook.shape) * p\n",
    "    else:\n",
    "        print(\"codebook should be -11; 01; or cts\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_weights(patterns, connectivity):\n",
    "    if connectivity is \"standard\":\n",
    "        if learning == \"hebbian\":\n",
    "            W = patts @ patts.T\n",
    "        elif learning == \"sparsehebbian\":\n",
    "            prob = sparsity  # np.sum(patts)/patts.shape[0]/patts.shape[1]\n",
    "            W = (1 / patts.shape[0]) * (patts - prob) @ (patts.T - prob)\n",
    "        elif learning == \"pinv\":\n",
    "            W = patts @ np.linalg.pinv(patts)\n",
    "        elif learning == \"bounded_hebbian\":\n",
    "            num_patts = patts.shape[1]\n",
    "            num_nodes = patts.shape[0]\n",
    "            W = np.zeros((num_nodes, num_nodes))\n",
    "            for i in range(num_patts):\n",
    "                Wtmp = np.outer(patts[:, i], patts[:, i]) / np.sqrt(num_nodes)\n",
    "                # ~ print(np.amax(Wtmp))\n",
    "                W = cap(Wtmp + W, bound)\n",
    "        W = W - torch.diag(torch.diag(W))\n",
    "    else:\n",
    "        N = connectivity.shape[0]\n",
    "        W = sparse.lil_matrix(connectivity.shape)\n",
    "        for i in range(N):\n",
    "            for j in connectivity.rows[i]:\n",
    "                W[i, j] = np.dot(patterns[i], patterns[j])\n",
    "        W.setdiag(0)\n",
    "    return W\n",
    "\n",
    "\n",
    "def entropy(inlist):\n",
    "    ent = np.zeros(len(inlist))\n",
    "    for idx, x in enumerate(inlist):\n",
    "        if x == 0 or x == 1:\n",
    "            ent[idx] = 0\n",
    "        else:\n",
    "            ent[idx] = -1 * (x * np.log2(x) + (1 - x) * np.log2(1 - x))\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns = 1\n",
    "iterations = 100\n",
    "N = 708\n",
    "corrupt_fraction = 0.0\n",
    "Npatts_list = np.arange(1, 800, 10)\n",
    "connectivity = \"standard\"  # Standard fully connected Hopfield network. For sparse connectivity use the next cell\n",
    "# learning can be 'hebbian', 'bounded_hebbian', 'pinv', or 'sparsehebbian' for sparse hopfield network\n",
    "learning = \"bounded_hebbian\"\n",
    "bound = 0.3  # Use bound param if learning='bounded_hebbian'\n",
    "\n",
    "init_overlap = torch.zeros((nruns, *Npatts_list.shape))\n",
    "final_overlap = torch.zeros((nruns, *Npatts_list.shape))\n",
    "MI_hc = torch.zeros((nruns, *Npatts_list.shape))\n",
    "\n",
    "\n",
    "for runidx in range(nruns):\n",
    "    print(\"runidx = \" + str(runidx))\n",
    "\n",
    "    if learning == \"sparsehebbian\":\n",
    "        # sparse hopfiled 0/1 code\n",
    "        sparsity = 0.2\n",
    "        patterns = 1 * (torch.random.rand(N, Npatts_list.max()) > (1 - sparsity))\n",
    "    else:\n",
    "        patterns = torch.sign(torch.random.normal(0, 1, (N, Npatts_list.max())))\n",
    "\n",
    "    for idx, Npatts in enumerate(tqdm(Npatts_list)):\n",
    "        # print(Npatts)\n",
    "        patts = patterns[:, :Npatts]\n",
    "        cor_patts = patterns[:, :Npatts]\n",
    "        W = get_weights(patts, connectivity)\n",
    "\n",
    "        if learning == \"sparsehebbian\":\n",
    "            # sparse hopfield\n",
    "            theta = torch.sum(W - torch.diag(W), axis=1)\n",
    "            theta = 0.05  # 0.04 #0\n",
    "            rep = (torch.sign(W @ cor_patts - theta) + 1) / 2\n",
    "        else:\n",
    "            rep = torch.sign(W @ cor_patts)\n",
    "\n",
    "        init_overlap[runidx, idx] = np.average(np.einsum(\"ij,ij->j\", rep, patts) / N)\n",
    "\n",
    "        rep1 = np.copy(rep)\n",
    "        for ite in range(iterations - 1):\n",
    "            if learning == \"sparsehebbian\":\n",
    "                rep = (np.sign(W @ rep - theta) + 1) / 2\n",
    "            else:\n",
    "                rep = np.sign(W @ rep)\n",
    "\n",
    "            if np.sum(abs(rep - rep1)) > 0:\n",
    "                rep1 = np.copy(rep)\n",
    "            else:\n",
    "                # print(\"converged at \"+str(ite))\n",
    "                break\n",
    "        err = np.einsum(\"ij,ij->j\", rep, patts) / N\n",
    "        overlap = np.average(err)\n",
    "        final_overlap[runidx, idx] = overlap  # err\n",
    "\n",
    "        if learning == \"sparsehebbian\":\n",
    "            q = np.sum(np.abs(rep), axis=0) / N  # sparse hopfield\n",
    "            m = err\n",
    "            p = np.sum(patts, axis=0) / patts.shape[0]\n",
    "            P1e = 1 - (m / p)\n",
    "            P0e = (q - m) / (1 - p)\n",
    "            MI_hc[runidx, idx] = np.average(\n",
    "                entropy(q) - (p * entropy(P1e) + (1 - p) * entropy(P0e))\n",
    "            )\n",
    "\n",
    "\n",
    "# print(init_overlap)\n",
    "# print(final_overlap)\n",
    "\n",
    "results_dir = \"continuum_results\"\n",
    "# filename = f\"sparseconnhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_gamma={gamma}_iter={iterations}_nruns={nruns}\"\n",
    "filename = f\"stdhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"pinvhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"sparsehopfield__mutualinfo_N={N}_noise={corrupt_fraction}_p={sparsity}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"boundedhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_bound={bound}_iter={iterations}_nruns={nruns}\"\n",
    "\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "plt.plot(\n",
    "    Npatts_list,\n",
    "    init_overlap.mean(axis=0),\n",
    "    label=\"single, corrupt=\" + str(corrupt_fraction),\n",
    ")\n",
    "plt.plot(\n",
    "    Npatts_list,\n",
    "    final_overlap.mean(axis=0),\n",
    "    label=\"final, corrupt=\" + str(corrupt_fraction),\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of patterns\")\n",
    "plt.ylabel(\"Overlap\")\n",
    "plt.title(r\"N = \" + str(N) + \", $W$\")\n",
    "plt.show()\n",
    "# exit()\n",
    "# fig1.savefig(f\"{results_dir}/Overlap_{filename}.png\")\n",
    "\n",
    "if learning == \"sparsehebbian\":\n",
    "    print(\"MI already calculated in loop\")\n",
    "else:\n",
    "    m = final_overlap\n",
    "    a = (1 + m) / 2\n",
    "    b = (1 - m) / 2\n",
    "\n",
    "    S = -a * np.log2(a) - b * np.log2(b)\n",
    "    S = np.where(m == 1, np.zeros_like(S), S)\n",
    "\n",
    "    MI_hc = 1 - S\n",
    "\n",
    "\n",
    "fig2 = plt.figure(1)\n",
    "plt.errorbar(\n",
    "    Npatts_list,\n",
    "    MI_hc.mean(axis=0),\n",
    "    yerr=MI_hc.std(axis=0),\n",
    "    label=\"final, corrupt=\" + str(corrupt_fraction),\n",
    ")\n",
    "# plt.xscale('log'); plt.yscale('log');\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of patterns\")\n",
    "plt.ylabel(\"MI\")\n",
    "plt.title(r\"N = \" + str(N) + \", $W$\")\n",
    "plt.show()\n",
    "# fig2.savefig(f\"{results_dir}/MI_{filename}.png\")\n",
    "\n",
    "data = {\n",
    "    \"N\": N,\n",
    "    \"init_overlap\": init_overlap,\n",
    "    \"m\": final_overlap,\n",
    "    \"MI\": MI_hc,\n",
    "    \"Npatts_list\": Npatts_list,\n",
    "    \"noise\": corrupt_fraction,\n",
    "    # \"q\": q  #needed for sparse hebbian\n",
    "    # \"bound\": bound #needed for bounded hopfield\n",
    "}\n",
    "# write_pkl(f\"{results_dir}/{filename}\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
