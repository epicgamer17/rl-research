{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from vectorhash_imported import *\n",
    "from vectorhash_convered import *\n",
    "from nd_scaffold import GridScaffold as GS\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from matrix_initializers import ConstantInitializer, SparseMatrixByScalingInitializer\n",
    "\n",
    "\n",
    "\n",
    "lambdas = [3, 4, 5]\n",
    "Ng = sum([i**2 for i in lambdas])\n",
    "\n",
    "\n",
    "percent_nonzero_relu=0.01\n",
    "W_gh_var=1.0\n",
    "sparse_initialization=0.1\n",
    "T=0.01\n",
    "W_hg_std = math.sqrt(W_gh_var)\n",
    "W_hg_mean = (\n",
    "    -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(lambdas))\n",
    ")\n",
    "h_normal_mean = len(lambdas) * W_hg_mean\n",
    "h_normal_std = math.sqrt(len(lambdas)) * W_hg_std\n",
    "relu_theta = 0\n",
    "\n",
    "# SparseMatrixByScalingInitializer(mean=W_hg_mean, scale=W_hg_std, device=\"cpu\")\n",
    "\n",
    "gs = GS(\n",
    "    shapes= [(3, 3), (4, 4), (5, 5)],\n",
    "    N_h= 400,\n",
    "    input_size= 2000,\n",
    "    h_normal_mean=h_normal_mean,\n",
    "    h_normal_std=h_normal_std,\n",
    "    device=None,\n",
    "    sparse_matrix_initializer=SparseMatrixBySparsityInitializer(sparsity=0.6, device=\"cpu\"),\n",
    "    relu_theta=0.5,\n",
    "    from_checkpoint=False,\n",
    "    T=1,\n",
    "    continualupdate=True,\n",
    "    ratshift=False,\n",
    "    initialize_W_gh_with_zeroes=False,\n",
    "    pseudo_inverse=False,\n",
    "    batch_update=False,\n",
    "    use_h_fix=False,\n",
    "    learned_pseudo=True,\n",
    "    epsilon=0.01,\n",
    "    calculate_update_scaling_method=\"norm\",\n",
    "    MagicMath=False,\n",
    "    sanity_check=False,\n",
    "    calculate_g_method=\"hairpin\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.G[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "lambdas = [3,4,5]                   # module period\n",
    "M = len(lambdas)                             # num modules\n",
    "Ng = np.sum(np.square(lambdas))                             # num grid cells\n",
    "Npos = np.prod(lambdas)   \n",
    "Npos = Npos*Npos\n",
    "gbook = gs.G\n",
    "Ns = 2000#84*84*3                           # num of sensory cells set at Npos, can be larger\n",
    "Np_lst = [400] #np.arange(25, 425, 25)     # num place cells\n",
    "pflip = 0.0                         #measure of noise injected in s (prob of flipping if binary, gaussian noise if cts)\n",
    "Niter = 1                           # number of iterations for scaffold dynamics\n",
    "nruns=1\n",
    "sparsity=0        #Dummy param for older code, not used currently\n",
    "sbook = np.sign(randn(Ns, Npos))\n",
    "Npatts_lst = np.arange(1,Ns+1,200)  # number of patterns to train on \n",
    "\n",
    "# Npatts is 1, 201, 401 etc... 3401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Choose experiment to run\n",
    "\n",
    "#Base case\n",
    "err_pc, err_gc, err_sens, err_senscup, err_sensl1, ga, gd, gt = capacity(senstrans_gs_vectorized_patts, lambdas, Ng, Np_lst, pflip, Niter, Npos, \n",
    "                                          gbook, Npatts_lst, nruns, Ns, sbook, sparsity, noise_level=\"none\", grid_scaffold=gs, W_hg_mean=W_hg_mean, W_hg_std=W_hg_std)\n",
    "\n",
    "#Place states chosen to be random vectors with same sparsity as base case (teal curves in Fig. 3)\n",
    "#err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_random_sparse_p, lambdas, Ng, Np_lst, pflip, Niter, Npos, \n",
    "                                          #gbook, Npatts_lst, nruns, Ns, sbook, sparsity)\n",
    "#Assuming linear hippocampal activations\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_linear_p, lambdas, Ng, Np_lst, pflip, Niter, Npos, \n",
    "                                          # gbook, Npatts_lst, nruns, Ns, sbook, sparsity)\n",
    "                                          \n",
    "#Use gbook as a spiraling outward + linear activation (for SI Fig. S13)\n",
    "# err_pc, err_gc, err_sens, err_senscup, err_sensl1 = capacity(senstrans_gs_linear_p_spiral, lambdas, Ng, Np_lst, pflip, Niter, Npos, \n",
    "                                          # gbook, Npatts_lst, nruns, Ns, sbook, sparsity)                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(err_pc)\n",
    "print(err_gc)\n",
    "print(err_sens)\n",
    "print(err_senscup)\n",
    "print(err_sensl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normlizd_l1 = err_sensl1\n",
    "Npatts = np.array(nruns*[Npatts_lst])   # Npatts_lst repeated nruns times\n",
    "Npatts = Npatts.T\n",
    "print(Npatts.shape)\n",
    "\n",
    "m = 1 - (2*normlizd_l1) \n",
    "\n",
    "a = (1+m)/2\n",
    "b = (1-m)/2\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "a1 = [[[1.0        ],[0.60587562],[0.57078304],[0.55598336],[0.54750687],[0.541999  ],[0.53836345],[0.53536188],[0.53285103],[0.53089478]]]\n",
    "b1 = [[[0.0      ],[0.39412438],[0.42921696],[0.44401664],[0.45249313],[0.458001  ],[0.46163655],[0.46463812],[0.46714897],[0.46910522]]]\n",
    "a1 = torch.tensor(a1)\n",
    "b1 = torch.tensor(b1)\n",
    "a = torch.abs(torch.tensor(a))\n",
    "b = torch.abs(torch.tensor(b))\n",
    "\n",
    "S = - a * np.log2(a) - b * np.log2(b)\n",
    "S = np.where(m==1, np.zeros_like(S), S)\n",
    "\n",
    "S1 = - a1 * np.log2(a1) - b1 * np.log2(b1)\n",
    "S1 = np.where(m==1, np.zeros_like(S1), S1)\n",
    "\n",
    "\n",
    "MI = 1 - S\n",
    "MI1 = 1 - S1\n",
    "plt.errorbar(Npatts_lst,MI[0].mean(axis=1),yerr=MI[0].std(axis=1),lw=2); plt.xscale('log'); plt.yscale('log')\n",
    "plt.errorbar(Npatts_lst,MI1[0].mean(axis=1),yerr=MI1[0].std(axis=1),lw=2); plt.xscale('log'); plt.yscale('log')\n",
    "plt.legend(['US', 'VECTORHASH'])\n",
    "plt.title('MI per inp bit vs num patts')\n",
    "\n",
    "plt.xlim(xmin=100)\n",
    "plt.ylabel('MI per inp bit')\n",
    "plt.xlabel('num patts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Baselines\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "def cap(W,bound):\n",
    "    W1=torch.where(W>bound,bound*torch.ones(W.shape),W)\n",
    "    W2=torch.where(W1<-bound,-bound*torch.ones(W.shape),W1)\n",
    "    return W2\n",
    "\n",
    "def corrupt_p(codebook,p=0.1,booktype='-11'):\n",
    "    rand_indices = torch.sign(torch.random.uniform(size=codebook.shape)- p )\n",
    "    if booktype=='-11':\n",
    "        return torch.multiply(codebook,rand_indices)\n",
    "    elif booktype=='01':\n",
    "        return abs(codebook - 0.5*(-rand_indices+1))\n",
    "    elif booktype=='cts':\n",
    "        return codebook + torch.random.normal(0,1,size=codebook.shape)*p\n",
    "    else:\n",
    "        print(\"codebook should be -11; 01; or cts\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_weights(patterns,connectivity):\n",
    "    if connectivity is 'standard':\n",
    "        if learning == 'hebbian':\n",
    "            W = patts @ patts.T\n",
    "        elif learning == 'sparsehebbian':\n",
    "            prob = sparsity #np.sum(patts)/patts.shape[0]/patts.shape[1]\n",
    "            W =(1/patts.shape[0])* (patts - prob) @ (patts.T - prob)\n",
    "        elif learning == 'pinv':\n",
    "            W= patts @ np.linalg.pinv(patts)\n",
    "        elif learning == 'bounded_hebbian':\n",
    "            num_patts = patts.shape[1]\n",
    "            num_nodes = patts.shape[0]\n",
    "            W = np.zeros((num_nodes,num_nodes))\n",
    "            for i in range(num_patts):\n",
    "                Wtmp = np.outer(patts[:,i] , patts[:,i])/np.sqrt(num_nodes)\n",
    "                # ~ print(np.amax(Wtmp))\n",
    "                W = cap(Wtmp + W,bound)\n",
    "        W = W - torch.diag(torch.diag(W))\n",
    "    else:\n",
    "        N = connectivity.shape[0]\n",
    "        W = sparse.lil_matrix(connectivity.shape)\n",
    "        for i in range(N):\n",
    "            for j in connectivity.rows[i]:\n",
    "                W[i,j] = np.dot(patterns[i],patterns[j])\n",
    "        W.setdiag(0)\n",
    "    return W\n",
    "\n",
    "\n",
    "def entropy(inlist):\n",
    "    ent = np.zeros(len(inlist))\n",
    "    for idx,x in enumerate(inlist):\n",
    "        if x == 0 or x == 1:\n",
    "            ent[idx] = 0\n",
    "        else:\n",
    "            ent[idx] = -1 * ( x*np.log2(x) + (1-x)*np.log2(1-x) )\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns=1\n",
    "iterations=100\n",
    "N = 708\n",
    "corrupt_fraction = 0.0\n",
    "Npatts_list = np.arange(1,800,10)\n",
    "connectivity='standard' # Standard fully connected Hopfield network. For sparse connectivity use the next cell\n",
    "# learning can be 'hebbian', 'bounded_hebbian', 'pinv', or 'sparsehebbian' for sparse hopfield network\n",
    "learning='bounded_hebbian'\n",
    "bound=0.3  #Use bound param if learning='bounded_hebbian'\n",
    "\n",
    "init_overlap = torch.zeros((nruns,*Npatts_list.shape))\n",
    "final_overlap = torch.zeros((nruns,*Npatts_list.shape))\n",
    "MI_hc = torch.zeros((nruns,*Npatts_list.shape))\n",
    "\n",
    "\n",
    "for runidx in range(nruns):\n",
    "    print(\"runidx = \"+str(runidx))\n",
    "    \n",
    "    if learning == 'sparsehebbian':\n",
    "        # sparse hopfiled 0/1 code\n",
    "        sparsity = 0.2\n",
    "        patterns = 1*(torch.random.rand(N,Npatts_list.max()) > (1-sparsity))\n",
    "    else:\n",
    "        patterns = torch.sign(torch.random.normal(0,1,(N,Npatts_list.max())))\n",
    "\n",
    "    \n",
    "    for idx,Npatts in enumerate(tqdm(Npatts_list)):\n",
    "        #print(Npatts)\n",
    "        patts = patterns[:,:Npatts]\n",
    "        cor_patts = patterns[:,:Npatts]\n",
    "        W = get_weights(patts,connectivity)\n",
    "        \n",
    "        if learning == 'sparsehebbian':\n",
    "            # sparse hopfield\n",
    "            theta = torch.sum(W-torch.diag(W), axis=1)\n",
    "            theta=0.05 #0.04 #0\n",
    "            rep = (torch.sign(W@cor_patts - theta)+1)/2            \n",
    "        else:\n",
    "            rep = torch.sign(W@cor_patts)\n",
    "\n",
    "        init_overlap[runidx,idx] = np.average(np.einsum('ij,ij->j',rep,patts)/N) \n",
    "\n",
    "        rep1 = np.copy(rep)\n",
    "        for ite in range(iterations-1):\n",
    "            if learning == 'sparsehebbian':\n",
    "                rep = (np.sign(W@rep - theta)+1)/2\n",
    "            else:\n",
    "                rep = np.sign(W@rep)\n",
    "            \n",
    "            if np.sum(abs(rep - rep1))>0:\n",
    "                rep1 = np.copy(rep)\n",
    "            else:\n",
    "                # print(\"converged at \"+str(ite))\n",
    "                break\n",
    "        err = np.einsum('ij,ij->j',rep,patts)/N\n",
    "        overlap = np.average(err) \n",
    "        final_overlap[runidx,idx] = overlap #err\n",
    "        \n",
    "        if learning=='sparsehebbian':\n",
    "            q = np.sum(np.abs(rep), axis=0) / N  # sparse hopfield\n",
    "            m = err\n",
    "            p = np.sum(patts, axis=0)/patts.shape[0]\n",
    "            P1e = 1 - (m/p)\n",
    "            P0e = (q-m)/(1-p)\n",
    "            MI_hc[runidx,idx] =  np.average( entropy(q) - ( p*entropy(P1e) + (1-p)*entropy(P0e) ) )\n",
    "\n",
    "\n",
    "# print(init_overlap)\n",
    "# print(final_overlap)\n",
    "\n",
    "results_dir = \"continuum_results\"\n",
    "# filename = f\"sparseconnhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_gamma={gamma}_iter={iterations}_nruns={nruns}\"\n",
    "filename = f\"stdhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"pinvhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"sparsehopfield__mutualinfo_N={N}_noise={corrupt_fraction}_p={sparsity}_iter={iterations}_nruns={nruns}\"\n",
    "# filename = f\"boundedhopfield__mutualinfo_N={N}_noise={corrupt_fraction}_bound={bound}_iter={iterations}_nruns={nruns}\"\n",
    "\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "plt.plot(Npatts_list,init_overlap.mean(axis=0), label='single, corrupt='+str(corrupt_fraction));\n",
    "plt.plot(Npatts_list,final_overlap.mean(axis=0), label='final, corrupt='+str(corrupt_fraction));\n",
    "plt.legend()\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel(\"Overlap\");\n",
    "plt.title(r\"N = \"+str(N)+\", $W$\");\n",
    "plt.show()\n",
    "# exit()\n",
    "# fig1.savefig(f\"{results_dir}/Overlap_{filename}.png\")\n",
    "\n",
    "if learning=='sparsehebbian':\n",
    "    print(\"MI already calculated in loop\")\n",
    "else:\n",
    "    m = final_overlap\n",
    "    a = (1+m)/2\n",
    "    b = (1-m)/2\n",
    "\n",
    "    S = - a * np.log2(a) - b * np.log2(b)\n",
    "    S = np.where(m==1, np.zeros_like(S), S)\n",
    "\n",
    "    MI_hc = 1 - S\n",
    "\n",
    "\n",
    "fig2 = plt.figure(1)\n",
    "plt.errorbar(Npatts_list,MI_hc.mean(axis=0),yerr=MI_hc.std(axis=0), label='final, corrupt='+str(corrupt_fraction)); #plt.xscale('log'); plt.yscale('log');\n",
    "plt.legend()\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel(\"MI\");\n",
    "plt.title(r\"N = \"+str(N)+\", $W$\");\n",
    "plt.show()\n",
    "# fig2.savefig(f\"{results_dir}/MI_{filename}.png\")\n",
    "\n",
    "data = {\n",
    "    \"N\": N,\n",
    "    \"init_overlap\": init_overlap,\n",
    "    \"m\": final_overlap,\n",
    "    \"MI\": MI_hc,\n",
    "    \"Npatts_list\": Npatts_list,\n",
    "    \"noise\": corrupt_fraction,\n",
    "    # \"q\": q  #needed for sparse hebbian\n",
    "    # \"bound\": bound #needed for bounded hopfield\n",
    "}\n",
    "# write_pkl(f\"{results_dir}/{filename}\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
