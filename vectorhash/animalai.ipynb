{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from animalai.environment import AnimalAIEnvironment\n",
    "from wrappers import CustomUnityToGymWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aai_seed = 0\n",
    "port = 5005 + random.randint(\n",
    "    0, 1000\n",
    ")  # uses a random port to avoid problems if a previous version exits slowly\n",
    "env_path = \"/home/ezrahuang/AAI/LINUX/AAI.x86_64\"\n",
    "configuration_file = \"./animal_ai_environments/yroom.yaml\"\n",
    "watch = True\n",
    "\n",
    "aai_env = AnimalAIEnvironment(\n",
    "    file_name=env_path,  # Path to the environment\n",
    "    seed=aai_seed,  # seed for the pseudo random generators\n",
    "    arenas_configurations=configuration_file,\n",
    "    play=False,  # note that this is set to False for training\n",
    "    base_port=port,  # the port to use for communication between python and the Unity environment\n",
    "    inference=watch,  # set to True if you want to watch the agent play\n",
    "    useCamera=True,  # set to False if you don't want to use the camera (no visual observations)\n",
    "    resolution=84,\n",
    "    useRayCasts=False,  # set to True if you want to use raycasts\n",
    "    no_graphics=False,  # set to True if you don't want to use the graphics ('headless' mode)\n",
    "    timescale=1,\n",
    ")\n",
    "\n",
    "env = CustomUnityToGymWrapper(\n",
    "    aai_env, uint8_visual=False, allow_multiple_obs=True, flatten_branched=True\n",
    ")  # the wrapper for the environment\n",
    "\n",
    "# # fix for error generated by the gym wrapper on line 241, python3.10/site-packages/mlagents_envs/rpc_utils.py in _observation_to_np_array\n",
    "# # use img = img.reshape(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "# 0 - nothing\n",
    "# 1 - rotate right by 6 degrees\n",
    "# 2 - rotate left by 6 degrees\n",
    "# 3 - accelerate forward\n",
    "# 4 - accelerate forward and rotate CW by 6 degrees\n",
    "# 5 - accelerate forward and rotate CCW by 6 degrees\n",
    "# 6 - accelerate backward\n",
    "# 7 - accelerate backward and rotate CW by 6 degrees\n",
    "# 8 - accelerate backward and rotate CCW by 6 degrees\n",
    "\n",
    "\n",
    "print(env.observation_space) # see python3.10/site-packages/animalai/environment.py, line 202\n",
    "# Box(84, 84, 3) - rgb image\n",
    "# Box(-inf, inf, (7,)) - (health; v1, v2, v3; p1, p2, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(60):\n",
    "#     obs, reward, done, info = env.step(1)\n",
    "#     print(\"health:\", obs[1][1])\n",
    "#     print(\"vel:\", obs[1][1:4]) # (v?, v?, v forward/backard relative to starting orientation)\n",
    "#     print(\"pos:\", obs[1][4:7]) # (p?, p?, p forward/backard relative to starting orientation)\n",
    "\n",
    "# for i in range(30):\n",
    "#     obs, reward, done, info = env.step(4)\n",
    "#     obs, reward, done, info = env.step(7)\n",
    "#     print(\"health:\", obs[1][1])\n",
    "#     print(\"vel:\", obs[1][1:4])\n",
    "#     print(\"pos:\", obs[1][4:7])\n",
    "\n",
    "obs, reward, done, info = env.step(3)\n",
    "print(\"obs[0] shape\", obs[0].shape)\n",
    "print(\"obs[0][0] \", obs[0][0])\n",
    "print(\"health:\", obs[1][1])\n",
    "print(\"vel:\", obs[1][1:4])\n",
    "print(\"pos:\", obs[1][4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  1200\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "img.shape (3, 84, 84)\n",
      "obs.shape [84, 84, 3]\n",
      "img.shape (3, 84, 84)\n",
      "obs.shape [84, 84, 3]\n",
      "img.shape (3, 84, 84)\n",
      "obs.shape [84, 84, 3]\n",
      "torch.Size([84, 84])\n",
      "torch.Size([7056])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/Projects/rl-research/vectorhash/hippocampal_sensory_layers.py:104: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
      "  1 + input.T @ self.inhibition_matrix_hs @ input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(7.0217) tensor(-2.3052) tensor(1.7003)\n",
      "h_from_s max, min, mean tensor(7.0196) tensor(-2.3045) tensor(1.6997)\n",
      "h_from_s_denoised max, min, mean tensor(7.0214) tensor(-2.3050) tensor(1.7003)\n",
      "avg nonzero/greaterzero h from book: tensor(1200) tensor(1076)\n",
      "avg nonzero/greaterzero h from s: tensor(1200) tensor(1076)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(1200) tensor(1076)\n",
      "mse/cosinesimilarity h from book and h from s tensor(4.2473e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(7.2531e-09) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s tensor(9.5967e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(9.5418e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h tensor(9.5273e-07) tensor([1.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:72: UserWarning: Using a target size (torch.Size([1, 1200])) that is different to the input size (torch.Size([1200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(h, h_from_s),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:79: UserWarning: Using a target size (torch.Size([1, 1200])) that is different to the input size (torch.Size([1200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(h, h_from_s_denoised),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:95: UserWarning: Using a target size (torch.Size([1, 7056])) that is different to the input size (torch.Size([7056])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(s, s_from_h_from_s),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:102: UserWarning: Using a target size (torch.Size([1, 7056])) that is different to the input size (torch.Size([7056])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(s, s_from_h_from_s_denoised),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:109: UserWarning: Using a target size (torch.Size([1, 7056])) that is different to the input size (torch.Size([7056])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(s, s_from_h),\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from animalai.environment import AnimalAIEnvironment\n",
    "from wrappers import CustomUnityToGymWrapper\n",
    "from animalai_agent import AnimalAIVectorhashAgent\n",
    "from vectorhash import build_vectorhash_architecture\n",
    "import random\n",
    "\n",
    "### vhash\n",
    "shapes = [(3,3,4), (4,4,5)]\n",
    "model = build_vectorhash_architecture(shapes, N_h=1200, input_size=84*84, initalization_method=\"by_sparsity\", shift=\"conv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### animalai\n",
    "aai_seed = 0\n",
    "port = 5005 + random.randint(\n",
    "    0, 1000\n",
    ")  # uses a random port to avoid problems if a previous version exits slowly\n",
    "env_path = \"/home/ezrahuang/AAI/LINUX/AAI.x86_64\"\n",
    "configuration_file = \"./animal_ai_environments/yroom.yaml\"\n",
    "watch = True\n",
    "\n",
    "aai_env = AnimalAIEnvironment(\n",
    "    file_name=env_path,  # Path to the environment\n",
    "    seed=aai_seed,  # seed for the pseudo random generators\n",
    "    arenas_configurations=configuration_file,\n",
    "    play=False,  # note that this is set to False for training\n",
    "    base_port=port,  # the port to use for communication between python and the Unity environment\n",
    "    inference=watch,  # set to True if you want to watch the agent play\n",
    "    useCamera=True,  # set to False if you don't want to use the camera (no visual observations)\n",
    "    resolution=84,\n",
    "    useRayCasts=False,  # set to True if you want to use raycasts\n",
    "    no_graphics=False,  # set to True if you don't want to use the graphics ('headless' mode)\n",
    "    timescale=1,\n",
    ")\n",
    "\n",
    "env = CustomUnityToGymWrapper(\n",
    "    aai_env, uint8_visual=False, allow_multiple_obs=True, flatten_branched=True\n",
    ")  # the wrapper for the environment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### agent\n",
    "agent = AnimalAIVectorhashAgent(model, env)\n",
    "agent.vectorhash.certainty = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:  Continuous: 0, Discrete: (3, 3)\n",
      "validate action\n",
      "discrete actions [[0 2]]\n",
      "continuous actions []\n",
      "n_agents 1\n",
      "name AnimalAI?team=0\n",
      "img.shape (3, 84, 84)\n",
      "obs.shape [84, 84, 3]\n",
      "noisydp:  tensor([0., 0.]) noisy_dtheta 6\n",
      "[tensor([0.4752, 0.2624, 0.2624]), tensor([0.4631, 0.2200, 0.0969, 0.2200])]\n",
      "[tensor([0.3167, 0.3662, 0.3171]), tensor([0.2208, 0.3309, 0.2813, 0.1671])]\n",
      "[tensor([0.5421, 0.1161, 0.0714, 0.2704]), tensor([0.2701, 0.5413, 0.1081, 0.0171, 0.0633])]\n",
      "tensor(0.3992)\n",
      "tensor([-4, -3, -2, -1,  0,  1,  2,  3,  4,  5], dtype=torch.int32)\n",
      "tensor(0.4493)\n",
      "tensor([-4, -3, -2, -1,  0,  1,  2,  3,  4,  5], dtype=torch.int32)\n",
      "tensor(0.3911)\n",
      "tensor([-4, -3, -2, -1,  0,  1,  2,  3,  4,  5], dtype=torch.int32)\n",
      "Certainty tensor([0.8900, 0.8500, 0.6200])<2, not storing memory.\n"
     ]
    }
   ],
   "source": [
    "agent.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1212), tensor(2.7599), tensor(1.9133))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_position_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
