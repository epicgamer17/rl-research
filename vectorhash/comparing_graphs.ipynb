{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import axes\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=10,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"medium\",\n",
    "):\n",
    "    import torch\n",
    "    import random\n",
    "\n",
    "    data = dataset.data\n",
    "    # print(num_imgs)\n",
    "    # print(data.shape)\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    # print(data.shape)\n",
    "    data = torch.tensor(data[:num_imgs]).float().to(\"cpu\")\n",
    "    # print(data.shape)\n",
    "\n",
    "    # data = random.sample(dataset.data.flatten(1).float().to(\"cpu\"), num_imgs)\n",
    "    if preprocess_sensory:\n",
    "        data = (data - data.mean()) / data.std()\n",
    "        # print(mnist_data[0])\n",
    "\n",
    "    # noissing the data\n",
    "    if noise_level == \"none\":\n",
    "        return data, data\n",
    "    elif noise_level == \"low\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1, 1)\n",
    "    elif noise_level == \"medium\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.25, 1.25)\n",
    "    elif noise_level == \"high\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.5, 1.5)\n",
    "    noisy_data = data + random_noise\n",
    "    # TODO: DO WE PREPROCESS NOISY IMAGES?\n",
    "    # if preprocess_sensory:\n",
    "    #     noisy_mnist = (noisy_mnist - noisy_mnist.mean()) / noisy_mnist.std()\n",
    "\n",
    "    return data, noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from nd_scaffold import GridScaffold, SparseMatrixBySparsityInitializer\n",
    "from graph_utils import graph_scaffold, print_imgs_side_by_side\n",
    "from matrix_initializers import SparseMatrixByScalingInitializer\n",
    "from vectorhash_functions import solve_mean, spacefillingcurve\n",
    "import math\n",
    "\n",
    "\n",
    "def test_memory_capacity(\n",
    "    data,\n",
    "    noisy_data,\n",
    "    shapes=[(3, 3, 5), (4, 4, 7)],\n",
    "    N_h=1000,\n",
    "    initalization_method=\"by_scaling\",\n",
    "    percent_nonzero_relu=0.01,\n",
    "    var=1.0,\n",
    "    sparse_initialization=0.1,\n",
    "    T=0.01,\n",
    "):\n",
    "\n",
    "    GS = GridScaffold(\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        input_size=data.shape[1],\n",
    "        device=\"cpu\",\n",
    "        sparse_matrix_initializer=(\n",
    "            SparseMatrixByScalingInitializer(\n",
    "                mean=solve_mean(p=percent_nonzero_relu, var=(len(shapes) * var))\n",
    "                / len(shapes),\n",
    "                scale=math.sqrt(var),\n",
    "            )\n",
    "            if initalization_method == \"by_scaling\"\n",
    "            else SparseMatrixBySparsityInitializer(sparsity=sparse_initialization)\n",
    "        ),\n",
    "        relu_theta=(\n",
    "            0.0\n",
    "            if initalization_method == \"by_scaling\"\n",
    "            else solve_mean(\n",
    "                p=percent_nonzero_relu, var=len(shapes) * (1 - sparse_initialization)\n",
    "            )\n",
    "        ),\n",
    "        T=T,\n",
    "        continualupdate=False,\n",
    "        ratshift=False,\n",
    "        initialize_W_gh_with_zeroes=False,\n",
    "        pseudo_inverse=False,\n",
    "    )\n",
    "\n",
    "    # learn over all images\n",
    "    v = spacefillingcurve(shapes)\n",
    "\n",
    "    GS.learn_path(observations=data, velocities=v[: len(data)])\n",
    "    print(len(v[: len(data)]))\n",
    "    recalled_imgs = GS.recall(noisy_data)\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(data, recalled_imgs)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating Capacity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 4, 6, 9, 13, 15, 17, 18, 25, 33, 49, 163]\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 1\n",
      "Unique Hs seen while learning: 1\n",
      "1\n",
      "Unique Hs seen while recalling: 1\n",
      "Unique Gs seen while recalling (after denoising): 1\n",
      "Unique Hs seen while recalling (after denoising): 1\n",
      "avg nonzero H: 13.0\n",
      "avg nonzero H_denoised: 13.0\n",
      "Unique Gs seen while recalling (before denoising): 1\n",
      "Cosine Similarity 0.9999998807907104\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 1\n",
      "Unique Hs seen while learning: 1\n",
      "1\n",
      "Unique Hs seen while recalling: 1\n",
      "Unique Gs seen while recalling (after denoising): 1\n",
      "Unique Hs seen while recalling (after denoising): 1\n",
      "avg nonzero H: 7.0\n",
      "avg nonzero H_denoised: 7.0\n",
      "Unique Gs seen while recalling (before denoising): 1\n",
      "Cosine Similarity 0.9999998807907104\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 2\n",
      "Unique Hs seen while learning: 2\n",
      "2\n",
      "Unique Hs seen while recalling: 2\n",
      "Unique Gs seen while recalling (after denoising): 2\n",
      "Unique Hs seen while recalling (after denoising): 2\n",
      "avg nonzero H: 17.0\n",
      "avg nonzero H_denoised: 10.5\n",
      "Unique Gs seen while recalling (before denoising): 2\n",
      "Cosine Similarity 0.9924113154411316\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 4\n",
      "Unique Hs seen while learning: 4\n",
      "4\n",
      "Unique Hs seen while recalling: 4\n",
      "Unique Gs seen while recalling (after denoising): 4\n",
      "Unique Hs seen while recalling (after denoising): 4\n",
      "avg nonzero H: 44.5\n",
      "avg nonzero H_denoised: 12.0\n",
      "Unique Gs seen while recalling (before denoising): 4\n",
      "Cosine Similarity 0.703313410282135\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 6\n",
      "Unique Hs seen while learning: 6\n",
      "6\n",
      "Unique Hs seen while recalling: 6\n",
      "Unique Gs seen while recalling (after denoising): 6\n",
      "Unique Hs seen while recalling (after denoising): 6\n",
      "avg nonzero H: 45.5\n",
      "avg nonzero H_denoised: 13.666666984558105\n",
      "Unique Gs seen while recalling (before denoising): 6\n",
      "Cosine Similarity 0.9158565998077393\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 9\n",
      "Unique Hs seen while learning: 9\n",
      "9\n",
      "Unique Hs seen while recalling: 9\n",
      "Unique Gs seen while recalling (after denoising): 8\n",
      "Unique Hs seen while recalling (after denoising): 8\n",
      "avg nonzero H: 40.0\n",
      "avg nonzero H_denoised: 10.44444465637207\n",
      "Unique Gs seen while recalling (before denoising): 8\n",
      "Cosine Similarity 0.8488368988037109\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 13\n",
      "Unique Hs seen while learning: 13\n",
      "13\n",
      "Unique Hs seen while recalling: 13\n",
      "Unique Gs seen while recalling (after denoising): 10\n",
      "Unique Hs seen while recalling (after denoising): 10\n",
      "avg nonzero H: 62.846153259277344\n",
      "avg nonzero H_denoised: 12.84615421295166\n",
      "Unique Gs seen while recalling (before denoising): 10\n",
      "Cosine Similarity 0.7514918446540833\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 15\n",
      "Unique Hs seen while learning: 15\n",
      "15\n",
      "Unique Hs seen while recalling: 15\n",
      "Unique Gs seen while recalling (after denoising): 11\n",
      "Unique Hs seen while recalling (after denoising): 11\n",
      "avg nonzero H: 69.33333587646484\n",
      "avg nonzero H_denoised: 10.133333206176758\n",
      "Unique Gs seen while recalling (before denoising): 11\n",
      "Cosine Similarity 0.667506754398346\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 17\n",
      "Unique Hs seen while learning: 17\n",
      "17\n",
      "Unique Hs seen while recalling: 17\n",
      "Unique Gs seen while recalling (after denoising): 10\n",
      "Unique Hs seen while recalling (after denoising): 10\n",
      "avg nonzero H: 63.764705657958984\n",
      "avg nonzero H_denoised: 11.411765098571777\n",
      "Unique Gs seen while recalling (before denoising): 10\n",
      "Cosine Similarity 0.6505301594734192\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 18\n",
      "Unique Hs seen while learning: 18\n",
      "18\n",
      "Unique Hs seen while recalling: 18\n",
      "Unique Gs seen while recalling (after denoising): 13\n",
      "Unique Hs seen while recalling (after denoising): 13\n",
      "avg nonzero H: 70.5\n",
      "avg nonzero H_denoised: 11.333333015441895\n",
      "Unique Gs seen while recalling (before denoising): 13\n",
      "Cosine Similarity 0.6147145628929138\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 25\n",
      "Unique Hs seen while learning: 25\n",
      "25\n",
      "Unique Hs seen while recalling: 25\n",
      "Unique Gs seen while recalling (after denoising): 13\n",
      "Unique Hs seen while recalling (after denoising): 13\n",
      "avg nonzero H: 86.72000122070312\n",
      "avg nonzero H_denoised: 11.600000381469727\n",
      "Unique Gs seen while recalling (before denoising): 13\n",
      "Cosine Similarity 0.5581194162368774\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 33\n",
      "Unique Hs seen while learning: 33\n",
      "33\n",
      "Unique Hs seen while recalling: 33\n",
      "Unique Gs seen while recalling (after denoising): 20\n",
      "Unique Hs seen while recalling (after denoising): 20\n",
      "avg nonzero H: 109.54545593261719\n",
      "avg nonzero H_denoised: 12.121212005615234\n",
      "Unique Gs seen while recalling (before denoising): 20\n",
      "Cosine Similarity 0.5234948396682739\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 49\n",
      "Unique Hs seen while learning: 49\n",
      "49\n",
      "Unique Hs seen while recalling: 49\n",
      "Unique Gs seen while recalling (after denoising): 15\n",
      "Unique Hs seen while recalling (after denoising): 15\n",
      "avg nonzero H: 114.2040786743164\n",
      "avg nonzero H_denoised: 11.244897842407227\n",
      "Unique Gs seen while recalling (before denoising): 15\n",
      "Cosine Similarity 0.4567304253578186\n",
      "==========================================\n",
      "module shapes:  [(3, 3), (4, 4), (5, 5)]\n",
      "N_g     :  50\n",
      "N_patts :  3600\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 163\n",
      "Unique Hs seen while learning: 163\n",
      "163\n",
      "Unique Hs seen while recalling: 163\n",
      "Unique Gs seen while recalling (after denoising): 44\n",
      "Unique Hs seen while recalling (after denoising): 44\n",
      "avg nonzero H: 275.9693298339844\n",
      "avg nonzero H_denoised: 14.190183639526367\n",
      "Unique Gs seen while recalling (before denoising): 44\n",
      "Cosine Similarity 0.2450677901506424\n"
     ]
    }
   ],
   "source": [
    "# Memory Capacity Tests\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "N_h = 1000\n",
    "\n",
    "N_g = 0\n",
    "for shape in shapes:\n",
    "    l = torch.prod(torch.tensor(shape)).item()\n",
    "    N_g += l\n",
    "# print(\"N_g\", N_g)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten())]\n",
    ")\n",
    "# dataset = torchvision.datasets.MNIST(\n",
    "#     root=\"data\", train=True, download=True, transform=transform\n",
    "# )\n",
    "\n",
    "# dataset = torchvision.datasets.FashionMNIST(\n",
    "#     root=\"data\", train=True, download=True, transform=transform\n",
    "# )\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR100(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "input_size = 1\n",
    "for shape in dataset.data[0].shape:\n",
    "    input_size *= shape\n",
    "\n",
    "theoretical_capacity = N_g * N_h / input_size  # 784 is the size of the input for MNIST\n",
    "\n",
    "percents = [\n",
    "    0.01,\n",
    "    0.03,\n",
    "    0.1,\n",
    "    0.2,\n",
    "    0.33,\n",
    "    0.5,\n",
    "    0.75,\n",
    "    0.9,\n",
    "    1.0,\n",
    "    1.1,\n",
    "    1.5,\n",
    "    2.0,\n",
    "    3.0,\n",
    "    10.0,\n",
    "]\n",
    "\n",
    "num_images = [theoretical_capacity * p for p in percents]\n",
    "num_images = [math.ceil(n) for n in num_images]\n",
    "print(num_images)\n",
    "\n",
    "preprocess_sensory = True\n",
    "noise_level = \"medium\"\n",
    "\n",
    "# TODO: Need a hyperparam N_p for dimension of the projection from grid cells to place cells\n",
    "similarities = []\n",
    "for num_imgs in num_images:\n",
    "    print(\"==========================================\")\n",
    "    data, noisy_data = prepare_data(\n",
    "        dataset,\n",
    "        num_imgs=num_imgs,\n",
    "        preprocess_sensory=preprocess_sensory,\n",
    "        noise_level=noise_level,\n",
    "    )\n",
    "    similarity = test_memory_capacity(\n",
    "        data,\n",
    "        noisy_data,\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        initalization_method=\"by_scaling\",\n",
    "        percent_nonzero_relu=10 / N_h,\n",
    "        var=1.0,\n",
    "        sparse_initialization=0.0,\n",
    "    )\n",
    "    print(\"Cosine Similarity\", torch.mean(similarity).item())\n",
    "    similarities.append(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing initialization techniques (at 10%, 50%, 100%, 150% capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare continual learning vs learning once at start (at 10%, 50%, 100%, 150% capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparam tuning for number active hippocampal cells after denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we losing grid states? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
