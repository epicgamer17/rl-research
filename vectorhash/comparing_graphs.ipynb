{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from nd_scaffold import GridScaffold, SparseMatrixBySparsityInitializer\n",
    "from graph_utils import graph_scaffold, print_imgs_side_by_side\n",
    "from matrix_initializers import SparseMatrixByScalingInitializer\n",
    "from vectorhash_functions import solve_mean, spacefillingcurve\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def test_memory_capacity(\n",
    "    data,\n",
    "    noisy_data,\n",
    "    shapes=[(3, 3, 5), (4, 4, 7)],\n",
    "    N_h=1000,\n",
    "    initalization_method=\"by_scaling\",\n",
    "    percent_nonzero_relu=0.01,\n",
    "    W_gh_var=1.0,\n",
    "    sparse_initialization=0.1,\n",
    "    T=0.01,\n",
    "    ratshift=False,\n",
    "    pseudo_inverse=False,\n",
    "    learned_pseudo=True,\n",
    "    plot_figs=False,\n",
    "):\n",
    "    assert initalization_method in [\"by_scaling\", \"by_sparsity\"]\n",
    "\n",
    "    if initalization_method == \"by_scaling\":\n",
    "        W_hg_std = math.sqrt(W_gh_var)\n",
    "        W_hg_mean = (\n",
    "            -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(shapes))\n",
    "        )\n",
    "        h_normal_mean = len(shapes) * W_hg_mean\n",
    "        h_normal_std = math.sqrt(len(shapes)) * W_hg_std\n",
    "        relu_theta = 0\n",
    "    elif initalization_method == \"by_sparsity\":\n",
    "        gamma = 1 - sparse_initialization\n",
    "        relu_theta = math.sqrt(gamma * len(shapes)) * norm.ppf(1 - percent_nonzero_relu)\n",
    "        W_hg_mean = 0\n",
    "        W_hg_std = math.sqrt(gamma * len(shapes))\n",
    "        h_normal_mean = -relu_theta\n",
    "        h_normal_std = (1 - sparse_initialization) * len(shapes)\n",
    "\n",
    "    GS = GridScaffold(\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        input_size=data.shape[1],\n",
    "        device=device,\n",
    "        h_normal_mean=h_normal_mean,\n",
    "        h_normal_std=h_normal_std,\n",
    "        sparse_matrix_initializer=(\n",
    "            SparseMatrixByScalingInitializer(\n",
    "                mean=W_hg_mean, scale=W_hg_std, device=device\n",
    "            )\n",
    "            if initalization_method == \"by_scaling\"\n",
    "            else SparseMatrixBySparsityInitializer(\n",
    "                sparsity=sparse_initialization, device=device\n",
    "            )\n",
    "        ),\n",
    "        relu_theta=relu_theta,\n",
    "        T=T,\n",
    "        ratshift=ratshift,\n",
    "        pseudo_inverse=pseudo_inverse,\n",
    "        learned_pseudo=learned_pseudo,\n",
    "        epsilon=0.01,\n",
    "        use_h_fix=True,\n",
    "    )\n",
    "\n",
    "    # learn over all images\n",
    "    v = spacefillingcurve(shapes)\n",
    "    a = GS.learn_path(observations=data, velocities=v[: len(data)])\n",
    "    recalled_imgs = GS.recall(noisy_data)\n",
    "    # if plot_figs:\n",
    "    #     for i in range(len(data)):\n",
    "    #         original_img = data[i].reshape(28, 28).cpu().numpy()\n",
    "    #         noisy_img = noisy_data[i].reshape(28, 28).cpu().numpy()\n",
    "    #         recalled_img = recalled_imgs[i].reshape(28,28).cpu().numpy()\n",
    "    #         print_imgs_side_by_side(\n",
    "    #             original_img,\n",
    "    #             noisy_img,\n",
    "    #             recalled_img,\n",
    "    #             out=f\"mnist_test_2/{len(data)}shitty{i}.png\",\n",
    "    #             captions=[\"original\", \"noisy\", \"recalled\"],\n",
    "    #             title=\"Learned\",\n",
    "    #         )\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(data, recalled_imgs)\n",
    "    return similarity, a\n",
    "    # return GS.H - GS.mean_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating Capacity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Capacity Tests\n",
    "from data_utils import load_mnist_dataset, prepare_data, determine_input_size\n",
    "from vectorhash_functions import calculate_theoretical_capacity\n",
    "\n",
    "device = \"cpu\"\n",
    "dataset = load_mnist_dataset()\n",
    "input_size = determine_input_size(dataset)\n",
    "N_h = 1000\n",
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "# shapes = [(5, 5), (9, 9), (11, 11)]\n",
    "\n",
    "theoretical_capacity = calculate_theoretical_capacity(\n",
    "    shapes=shapes, N_h=N_h, input_size=input_size\n",
    ")\n",
    "percents = [\n",
    "    0.5,\n",
    "    # 0.01,  # 1\n",
    "    # 0.03,  # 2\n",
    "    # 0.1,  # 3\n",
    "    # 0.2,  # 4\n",
    "    # 0.33,  # 5\n",
    "    # 0.5,  # 6\n",
    "    # 0.75,  # 7\n",
    "    # 0.9,  # 8\n",
    "    # 1.0,  # 9\n",
    "    # 1.1,  # 10\n",
    "    # 1.5,  # 11\n",
    "    # 2.0,  # 12\n",
    "    # 3.0,  # 13\n",
    "    # 10.0,  # 14\n",
    "]\n",
    "\n",
    "num_images = [theoretical_capacity * p for p in percents]\n",
    "num_images = [math.ceil(n) for n in num_images]\n",
    "print(num_images)\n",
    "num_images = [500]\n",
    "preprocess_sensory = True\n",
    "noise_level = \"none\"\n",
    "\n",
    "# TODO: Need a hyperparam N_p for dimension of the projection from grid cells to place cells\n",
    "similarities = []\n",
    "for num_imgs in num_images:\n",
    "    print(\"==========================================\")\n",
    "    data, noisy_data = prepare_data(\n",
    "        dataset,\n",
    "        num_imgs=num_imgs,\n",
    "        preprocess_sensory=preprocess_sensory,\n",
    "        noise_level=noise_level,\n",
    "        device=device,\n",
    "    )\n",
    "    similarity, info = test_memory_capacity(\n",
    "        data,\n",
    "        noisy_data,\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        initalization_method=\"by_scaling\",\n",
    "        percent_nonzero_relu=0.8,  # 800 / N_h,\n",
    "        W_gh_var=1.0,\n",
    "        sparse_initialization=0,\n",
    "        T=0.0001,\n",
    "        ratshift=False,\n",
    "        pseudo_inverse=False,\n",
    "        learned_pseudo=True,\n",
    "        plot_figs=True,\n",
    "    )\n",
    "    print(\"Cosine Similarity\", torch.mean(similarity).item())\n",
    "    similarities.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing initialization techniques (at 10%, 50%, 100%, 150% capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare continual learning vs learning once at start (at 10%, 50%, 100%, 150% capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparam tuning for number active hippocampal cells after denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing valididy and recall of learned pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init of psuedo inverse calculation\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PsuedoInverse:\n",
    "    def __init__(self, N_h, input_size, epsilon=0.01, device=\"cpu\"):\n",
    "        self.N_h = N_h\n",
    "        self.input_size = input_size\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "\n",
    "        self.inhibition_matrix_sh = torch.eye(self.N_h, device=device) / (N_h)\n",
    "        self.inhibition_matrix_hs = torch.eye(self.input_size, device=device) / (\n",
    "            input_size\n",
    "        )\n",
    "\n",
    "        # INITIALIZATION OF HIDDEN LAYER MAGIC\n",
    "\n",
    "        self.lin1HS = nn.Linear(input_size, 10 * input_size)\n",
    "        self.activationHS = nn.LogSigmoid()\n",
    "        torch.nn.init.uniform_(self.lin1HS.weight, -0.5, 0.5)\n",
    "\n",
    "        self.lin1SH = nn.Linear(N_h, 10 * N_h)\n",
    "        self.activationSH = nn.LogSigmoid()\n",
    "        torch.nn.init.uniform_(self.lin1HS.weight, -0.5, 1 / 2)\n",
    "\n",
    "        self.W_sh = torch.zeros((self.input_size, self.N_h), device=device)\n",
    "        self.W_hs = torch.zeros((self.N_h, self.input_size), device=device)\n",
    "\n",
    "    def hiddenlayer(self, input, learned=\"hs\"):\n",
    "        if learned == \"hs\":\n",
    "            hidden = self.activationHS(self.lin1HS(input))\n",
    "        else:\n",
    "            hidden = self.activationSH(self.lin1SH(input))\n",
    "        return hidden\n",
    "\n",
    "    def calculatepseudoinverse(self, input, output, input_size, learned=\"hs\"):\n",
    "        inhibition = (\n",
    "            self.inhibition_matrix_hs if learned == \"hs\" else self.inhibition_matrix_sh\n",
    "        )\n",
    "        W = self.W_hs if learned == \"hs\" else self.W_sh\n",
    "\n",
    "        bk = (inhibition @ input) / (1 + input.T @ inhibition @ input)\n",
    "\n",
    "        # ERROR VECTOR EK\n",
    "        E_k = output - W @ input\n",
    "\n",
    "        # NORMALIZATION FACTOR\n",
    "\n",
    "        E = ((E_k.T @ E_k) / input_size) / (1 + input.T @ inhibition @ input)\n",
    "        L2Enorm = torch.abs(E)\n",
    "\n",
    "        # GAMMA CALCULATION\n",
    "\n",
    "        gamma = 1 / (1 + ((1 - torch.exp(-L2Enorm)) / self.epsilon))\n",
    "\n",
    "        inhibition = inhibition - inhibition * input @ bk.T\n",
    "        # ((1-torch.exp(-L2Enorm))/self.epsilon) * torch.eye(input_size, device=self.device)\n",
    "\n",
    "        if learned == \"hs\":\n",
    "            self.inhibition_matrix_hs = inhibition\n",
    "            self.W_hs += torch.outer((output - W @ input), bk.T)\n",
    "        else:\n",
    "            self.inhibition_matrix_sh = inhibition\n",
    "            self.W_sh += torch.outer((output - W @ input), bk.T)\n",
    "\n",
    "    def learnprojection(self, input, output, learned=\"hs\"):\n",
    "        # input: (N)\n",
    "        # output: (M)\n",
    "        # M: (M x N)\n",
    "        # Eg : Wgh = 1/Nh * sum_i (G_i * H_iT) (outer product)\n",
    "        ret = (torch.einsum(\"j,i->ji\", output, input)) / (\n",
    "            # self.N_h\n",
    "            torch.linalg.norm(input + 1e-10)\n",
    "            ** 2\n",
    "        )\n",
    "        if learned == \"hs\":\n",
    "            self.W_hs += ret\n",
    "        else:\n",
    "            self.W_sh += ret\n",
    "\n",
    "    def learn(self, input, output, learned=\"hs\"):\n",
    "\n",
    "        # input = self.hiddenlayer(input, learned)\n",
    "\n",
    "        self.calculatepseudoinverse(input, output, self.input_size, learned)\n",
    "\n",
    "    def recall(self, input, used=\"hs\"):\n",
    "        W = self.W_hs if used == \"hs\" else self.W_sh\n",
    "        # return W @ self.hiddenlayer(input, used)\n",
    "        return W @ input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "\n",
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "# shapes = [(5, 5), (9, 9), (11, 11)]\n",
    "\n",
    "N_h = 1000\n",
    "\n",
    "N_g = 0\n",
    "for shape in shapes:\n",
    "    l = torch.prod(torch.tensor(shape)).item()\n",
    "    N_g += l\n",
    "# print(\"N_g\", N_g)\n",
    "\n",
    "\n",
    "dataset = load_mnist_dataset()\n",
    "input_size = determine_input_size(dataset)\n",
    "theoretical_capacity = calculate_theoretical_capacity(\n",
    "    shapes=shapes, N_h=N_h, input_size=input_size\n",
    ")\n",
    "\n",
    "num_images = 300\n",
    "print(num_images)\n",
    "\n",
    "preprocess_sensory = True\n",
    "noise_level = \"none\"\n",
    "data, noisy_data = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=num_images,\n",
    "    preprocess_sensory=preprocess_sensory,\n",
    "    noise_level=noise_level,\n",
    ")\n",
    "H = test_memory_capacity(\n",
    "    data,\n",
    "    data,\n",
    "    shapes=shapes,\n",
    "    N_h=N_h,\n",
    "    initalization_method=\"by_scaling\",\n",
    "    percent_nonzero_relu=200 / N_h,\n",
    "    W_gh_var=1.0,\n",
    "    sparse_initialization=0,\n",
    "    T=0.01,\n",
    "    ratshift=False,\n",
    "    pseudo_inverse=False,\n",
    "    learned_pseudo=False,\n",
    "    plot_figs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For each G, The first 9 entries are a 3x3 grid, the next 16 are a 4x4 grid, and the last 25 are a 5x5 grid. I want you to take the first 9 entries and reshape them into a 3x3 grid, the next 16 into a 4x4 grid, and the last 25 into a 5x5 grid. Then I want you to make an array with the grids in the order of 3x3, 4x4, 5x5. Then I want you to store all the versions of the 3x3 matrix, the 4x4 and the 5x5\n",
    "A = G.cpu().numpy()\n",
    "\n",
    "mat3 = []\n",
    "mat4 = []\n",
    "mat5 = []\n",
    "\n",
    "for i in A:\n",
    "    mat3.append(i[:9].reshape(3, 3))\n",
    "    mat4.append(i[9:25].reshape(4, 4))\n",
    "    mat5.append(i[25:].reshape(5, 5))\n",
    "\n",
    "# check all possible combinations of 3x3, 4x4, 5x5 matrices and see if there are any that are the same or that all possible combinations have been computed\n",
    "# from the 3x3, 4x4, 5x5 matrices\n",
    "\n",
    "for i in range(len(mat3)):\n",
    "    for j in range(len(mat4)):\n",
    "        for k in range(len(mat5)):\n",
    "            if i == j and j == k and i == k:\n",
    "                print(i, j, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudoinverse = PsuedoInverse(N_h=1000, input_size=784, epsilon=0.01)\n",
    "n = 100\n",
    "dataset = load_mnist_dataset()\n",
    "data, noisydata = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=n,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"medium\",\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    for i in range(n):\n",
    "        pseudoinverse.learn(data[i], H[i], learned=\"hs\")\n",
    "        pseudoinverse.learn(H[i], data[i], learned=\"sh\")\n",
    "\n",
    "img_reconstruction = []\n",
    "H_reconstruction = []\n",
    "recompfromboth = []\n",
    "\n",
    "print(\"RECALL\")\n",
    "for i in range(n):\n",
    "    recompfromboth.append(\n",
    "        torch.cosine_similarity(\n",
    "            data[i],\n",
    "            pseudoinverse.recall(\n",
    "                input=pseudoinverse.recall(input=data[i], used=\"hs\"), used=\"sh\"\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "    # append(torch.cosine_similarity(H[i], pseudoinverse.recall(input=noisydata[i], used=\"hs\"), dim=0))\n",
    "    H_reconstruction.append(\n",
    "        torch.cosine_similarity(\n",
    "            H[i], pseudoinverse.recall(input=data[i], used=\"hs\"), dim=0\n",
    "        )\n",
    "    )\n",
    "    img_reconstruction.append(\n",
    "        torch.cosine_similarity(data[i], pseudoinverse.recall(H[i], used=\"sh\"), dim=0)\n",
    "    )\n",
    "\n",
    "print(\"Data mean\", torch.mean(torch.tensor(data)))\n",
    "print(\"H mean\", torch.mean(torch.tensor(H)))\n",
    "print(\"Num images\", n)\n",
    "print(\"H Reconstruction\", torch.mean(torch.tensor(H_reconstruction)))\n",
    "print(\"Img Reconstruction\", torch.mean(torch.tensor(img_reconstruction)))\n",
    "print(\"Recomp from both\", torch.mean(torch.tensor(recompfromboth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from matrix_initializers import ConstantInitializer, SparseMatrixByScalingInitializer\n",
    "from vectorhash_functions import solve_mean, spacefillingcurve\n",
    "from nd_scaffold import GridScaffold\n",
    "from graph_utils import print_imgs_side_by_side\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "dataset = load_mnist_dataset()\n",
    "data, noisy_data = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=100,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"none\",\n",
    ")\n",
    "\n",
    "T = 1e-3\n",
    "device = \"cpu\"\n",
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "N_h = 1000\n",
    "target_N_h = 300\n",
    "percent_nonzero_relu = target_N_h / N_h\n",
    "\n",
    "W_hg_std = 1\n",
    "W_hg_mean = -W_hg_std * norm.ppf(1 - percent_nonzero_relu) / math.sqrt(len(shapes))\n",
    "h_normal_mean = len(shapes) * W_hg_mean\n",
    "h_normal_std = math.sqrt(len(shapes)) * W_hg_std\n",
    "\n",
    "const = ConstantInitializer(\n",
    "    value=GridScaffold(\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        input_size=data.shape[1],\n",
    "        device=device,\n",
    "        h_normal_mean=h_normal_mean,\n",
    "        h_normal_std=h_normal_std,\n",
    "        sparse_matrix_initializer=SparseMatrixByScalingInitializer(\n",
    "            mean=W_hg_mean, scale=W_hg_std, device=device\n",
    "        ),\n",
    "        relu_theta=0,\n",
    "        T=T,\n",
    "        use_h_fix=True,\n",
    "        learned_pseudo=True,\n",
    "    ).W_hg\n",
    ")\n",
    "\n",
    "GS = GridScaffold(\n",
    "    shapes=shapes,\n",
    "    N_h=N_h,\n",
    "    input_size=data.shape[1],\n",
    "    device=device,\n",
    "    h_normal_mean=h_normal_mean,\n",
    "    h_normal_std=h_normal_std,\n",
    "    sparse_matrix_initializer=const,\n",
    "    relu_theta=0,\n",
    "    T=T,\n",
    "    use_h_fix=True,\n",
    "    learned_pseudo=True,\n",
    ")\n",
    "GS2 = GridScaffold(\n",
    "    shapes=shapes,\n",
    "    N_h=N_h,\n",
    "    input_size=data.shape[1],\n",
    "    device=device,\n",
    "    h_normal_mean=h_normal_mean,\n",
    "    h_normal_std=h_normal_std,\n",
    "    sparse_matrix_initializer=const,\n",
    "    relu_theta=0,\n",
    "    T=T,\n",
    "    use_h_fix=False,\n",
    "    learned_pseudo=True,\n",
    ")\n",
    "\n",
    "\n",
    "# GS.learn_path(observations=data, velocities=v[: len(data)])\n",
    "# recalled_imgs = GS.recall(noisy_data)\n",
    "# similarity = torch.nn.functional.cosine_similarity(data, recalled_imgs)\n",
    "# print(torch.mean(similarity).item())\n",
    "\n",
    "v = spacefillingcurve(shapes)\n",
    "print(torch.vstack(v))\n",
    "k = 0\n",
    "\n",
    "GS.learn(data[k], velocity=v[k])\n",
    "GS2.learn(data[k], velocity=v[k])\n",
    "print(k)\n",
    "k += 1\n",
    "\n",
    "for i in range(0, k):\n",
    "    print(i)\n",
    "    s = data[i]\n",
    "    h = GS.hippocampal_from_sensory(s)\n",
    "    g = GS.grid_from_hippocampal(h)\n",
    "    g_rec = GS.denoise(g)\n",
    "    h_rec = GS.hippocampal_from_grid(g_rec)\n",
    "    s_rec = GS.sensory_from_hippocampal(h_rec - GS.mean_h)\n",
    "    print(g_rec)\n",
    "    print(\"h_mean:\", (h_rec.mean() - GS.mean_h).item())\n",
    "    print(\"loss:\", torch.nn.functional.mse_loss(s, s_rec).item())\n",
    "    print(\"similarity:\", torch.cosine_similarity(s, s_rec).mean().item())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(k, 6, figsize=(15, 4 * k))\n",
    "for j in range(0, k):\n",
    "    h1 = GS.hippocampal_from_sensory(data[j])\n",
    "    h2 = GS2.hippocampal_from_sensory(data[j])\n",
    "\n",
    "    recovered_s1 = GS.sensory_from_hippocampal(h1 - GS.mean_h)\n",
    "    recovered_s2 = GS2.sensory_from_hippocampal(h2)\n",
    "\n",
    "    g_recovered_s1 = GS.grid_from_hippocampal(h1)\n",
    "    g_recovered_s2 = GS2.grid_from_hippocampal(h2)\n",
    "\n",
    "    g_recovered_s1_1 = GS.sensory_from_hippocampal(\n",
    "        (GS.hippocampal_from_grid(GS.denoise(g_recovered_s1))) - GS.mean_h\n",
    "    )\n",
    "    g_recovered_s1_2 = GS2.sensory_from_hippocampal(\n",
    "        (GS2.hippocampal_from_grid(GS2.denoise(g_recovered_s2)))\n",
    "    )\n",
    "\n",
    "    ax[j][0].imshow(data[j].reshape(28, 28).cpu().numpy())\n",
    "    ax[j][0].set_title(f\"s{j}\")\n",
    "    ax[j][1].imshow(recovered_s1.reshape(28, 28).cpu().numpy())\n",
    "    ax[j][1].set_title(\"s -> h -> s\")\n",
    "    ax[j][2].imshow(recovered_s2.reshape(28, 28).cpu().numpy())\n",
    "    ax[j][2].set_title(\"s -> h -> s\\n (no fix)\")\n",
    "    ax[j][3].imshow(g_recovered_s1_1.reshape(28, 28).cpu().numpy())\n",
    "    ax[j][3].set_title(\"s -> h -> g -> denoised g -> h -> s\")\n",
    "    ax[j][4].imshow(g_recovered_s1_2.reshape(28, 28).cpu().numpy())\n",
    "    ax[j][4].set_title(\"s -> h -> g -> denoised g -> h -> s \\n(no fix)\")\n",
    "    ax[j][5].imshow(\n",
    "        (\n",
    "            -g_recovered_s1_1 / g_recovered_s1_1.mean()\n",
    "            + g_recovered_s1_2 / g_recovered_s1_2.mean()\n",
    "        )\n",
    "        .reshape(28, 28)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    ax[j][5].set_title(\"diff\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(\"test_fix_4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import abs, norm\n",
    "\n",
    "\n",
    "def Rk1MrUpdate(A, A_pinv, c, d, Zero_tol, Case_Print_Flag):\n",
    "    # size c = [n,1]\n",
    "    # size d = [m,1]\n",
    "    c = c.reshape(-1, 1)\n",
    "    d = d.reshape(-1, 1)\n",
    "    n = c.shape[0]\n",
    "    m = d.shape[0]\n",
    "    V = A_pinv @ c\n",
    "\n",
    "    b = 1 + d.T @ V\n",
    "    N = A_pinv.T @ d\n",
    "    W = (torch.eye(n) - A @ A_pinv) @ c\n",
    "    M = (torch.eye(m) - A_pinv @ A) @ d\n",
    "    ## squared norm of the two abovesaid vectors\n",
    "    w_snorm = norm(W, p=2) ** 2\n",
    "    m_snorm = norm(M, p=2) ** 2\n",
    "\n",
    "    if w_snorm >= Zero_tol and m_snorm >= Zero_tol:\n",
    "        if Case_Print_Flag == 1:\n",
    "            print(\"case 1\")\n",
    "        G = (\n",
    "            ((-1 / w_snorm) * V @ W.T)\n",
    "            - ((1 / m_snorm) * (M @ N.T))\n",
    "            + ((b / m_snorm / w_snorm) * (M @ W.T))\n",
    "        )\n",
    "\n",
    "    elif w_snorm < Zero_tol and m_snorm >= Zero_tol and abs(b) < Zero_tol:\n",
    "        if Case_Print_Flag == 1:\n",
    "            print(\"case 2\")\n",
    "        v_snorm = norm(V, 2) ** 2\n",
    "        G = (-1 / v_snorm) * (V @ V.T) @ A_pinv - (1 / m_snorm) * M @ N.T\n",
    "\n",
    "    elif w_snorm < Zero_tol and abs(b) > Zero_tol:\n",
    "        if Case_Print_Flag == 1:\n",
    "            print(\"case 3\")\n",
    "        v_snorm = norm(V, 2) ** 2\n",
    "        G = (\n",
    "            ((1 / b) * M @ V.T @ A_pinv - (b / (v_snorm * m_snorm + b**2)))\n",
    "            @ ((v_snorm / b) * M + V)\n",
    "            @ ((m_snorm / b) * A_pinv.T @ V + N).T\n",
    "        )\n",
    "\n",
    "    elif m_snorm < Zero_tol and w_snorm >= Zero_tol and abs(b) < Zero_tol:\n",
    "        if Case_Print_Flag == 1:\n",
    "            print(\"case 4\")\n",
    "        n_snorm = norm(N, 2) ** 2\n",
    "        G = (-1 / n_snorm) * A_pinv @ (N @ N.T) - (1 / w_snorm) * V @ W.T\n",
    "\n",
    "    elif m_snorm < Zero_tol and abs(b) > Zero_tol:\n",
    "        if Case_Print_Flag == 1:\n",
    "            print(\"case 5\")\n",
    "        n_snorm = norm(N, 2) ** 2\n",
    "        a1 = (1 / b) * A_pinv @ N @ W.T\n",
    "        a2 = (b / (w_snorm * n_snorm + b**2)) * ((w_snorm / b) * A_pinv @ N + V)\n",
    "        a3 = ((n_snorm / b) * W + N).T\n",
    "        G = ((1 / b) * A_pinv @ N @ W.T) - (\n",
    "            ((b / (w_snorm * n_snorm + b**2)) * ((w_snorm / b) * A_pinv @ N + V))\n",
    "            @ (((n_snorm / b) * W + N).T)\n",
    "        )\n",
    "\n",
    "    elif m_snorm < Zero_tol and w_snorm < Zero_tol and abs(b) < Zero_tol:\n",
    "        if Case_Print_Flag == 1:\n",
    "            print(\"case 6\")\n",
    "        v_snorm = norm(V, 2) ** 2\n",
    "        n_snorm = norm(N, 2) ** 2\n",
    "        G = (\n",
    "            (-1 / v_snorm) * (V @ V.T) @ A_pinv\n",
    "            - (1 / n_snorm) * A_pinv @ (N @ N.T)\n",
    "            + (((V.T @ A_pinv @ N) / (v_snorm * n_snorm)) @ (V @ N.T))\n",
    "        )\n",
    "\n",
    "    A_pinv_New = A_pinv + G\n",
    "    return A_pinv_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"norm\"\n",
    "use_h_fix = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_update_Wsh_fix(input: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n",
    "    if use_h_fix:\n",
    "        return calculate_update(input=input - GS.mean_h, output=output)\n",
    "    else:\n",
    "        return calculate_update(input=input, output=output)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_update(input: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n",
    "    # input: (N)\n",
    "    # output: (M)\n",
    "    # M: (M x N)\n",
    "    # Eg : Wgh = 1/Nh * sum_i (G_i * H_iT) (outer product)\n",
    "    if a == \"norm\":\n",
    "        scale = torch.linalg.norm(input) ** 2\n",
    "    elif a == \"n_h\":\n",
    "        scale = GS.N_h\n",
    "    else:\n",
    "        raise ValueError(\"Invalid calculate_update_scaling_method\")\n",
    "\n",
    "    ret = torch.einsum(\"j,i->ji\", output, input) / (scale + 1e-10)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.G[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "i_Dim = 1000\n",
    "o_Dim = 784\n",
    "inputs = []\n",
    "outputs = []\n",
    "n = 3000\n",
    "num_images = n\n",
    "print(num_images)\n",
    "\n",
    "# H -= GS.mean_h\n",
    "\n",
    "dataset = load_mnist_dataset()\n",
    "input_size = determine_input_size(dataset)\n",
    "theoretical_capacity = calculate_theoretical_capacity(\n",
    "    shapes=shapes, N_h=N_h, input_size=input_size\n",
    ")\n",
    "preprocess_sensory = True\n",
    "noise_level = \"none\"\n",
    "data, noisy_data = prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=num_images,\n",
    "    preprocess_sensory=preprocess_sensory,\n",
    "    noise_level=noise_level,\n",
    "    use_fix=True,\n",
    ")\n",
    "\n",
    "\n",
    "ps = PsuedoInverse(N_h=1000, input_size=784, epsilon=0.01)\n",
    "\n",
    "for i in range(n):\n",
    "    inputs.append(torch.randn(i_Dim))\n",
    "    outputs.append(torch.randn(o_Dim))\n",
    "\n",
    "\n",
    "outputs = data\n",
    "\n",
    "A_pinv = torch.eye(o_Dim, i_Dim)\n",
    "A = torch.pinverse(A_pinv)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "Ccossim_learned_s_to_h = []\n",
    "Ccossim_recall_s_to_h = []\n",
    "Ccossim_learned_h_to_s = []\n",
    "Ccossim_recall_h_to_s = []\n",
    "\n",
    "Ccossim_learned_s_to_hPS = []\n",
    "Ccossim_recall_s_to_hPS = []\n",
    "Ccossim_learned_h_to_sPS = []\n",
    "Ccossim_recall_h_to_sPS = []\n",
    "\n",
    "Mcossim_learned_s_to_h = []\n",
    "Mcossim_recall_s_to_h = []\n",
    "Mcossim_learned_h_to_s = []\n",
    "Mcossim_recall_h_to_s = []\n",
    "\n",
    "Mcossim_learned_s_to_hPS = []\n",
    "Mcossim_recall_s_to_hPS = []\n",
    "Mcossim_learned_h_to_sPS = []\n",
    "Mcossim_recall_h_to_sPS = []\n",
    "\n",
    "IMAGERECONSTRUCTION = []\n",
    "\n",
    "Hs = []\n",
    "\n",
    "W_sh = torch.zeros((input_size, N_h), device=device)\n",
    "\n",
    "figs = []\n",
    "\n",
    "for i in range(n):\n",
    "    A_pinv = Rk1MrUpdate(A, A_pinv, inputs[i], outputs[i], 3e-2, 0)\n",
    "\n",
    "    # mean = torch.mean(A_pinv)\n",
    "    # # perform pinverse call on cpu to avoid non terminating error\n",
    "\n",
    "    # D = A_pinv\n",
    "    # # check is nan in D\n",
    "    # if torch.isnan(A_pinv).any():\n",
    "    #     print(\"nan in D\")\n",
    "    #     break\n",
    "    # # check is inf in D\n",
    "    # if torch.isinf(A_pinv).any():\n",
    "    #     print(\"inf in D\")\n",
    "    #     break\n",
    "    # A = torch.pinverse(D)\n",
    "    ps.learn(outputs[i], inputs[i], learned=\"hs\")\n",
    "    W_sh += calculate_update_Wsh_fix(inputs[i], outputs[i])\n",
    "    ps.learn(inputs[i], outputs[i], learned=\"sh\")\n",
    "    # Ccossim_learned_s_to_hPS.append(torch.cosine_similarity(outputs[i], ps.recall(input=inputs[i], used=\"sh\"), dim=0))\n",
    "    Ccossim_learned_s_to_hPS.append(\n",
    "        torch.cosine_similarity(\n",
    "            inputs[i], ps.recall(input=outputs[i], used=\"hs\"), dim=0\n",
    "        )\n",
    "    )\n",
    "    Ccossim_learned_h_to_sPS.append(\n",
    "        torch.cosine_similarity(outputs[i], W_sh @ inputs[i], dim=0)\n",
    "    )\n",
    "\n",
    "    # Mcossim_learned_s_to_hPS.append(loss(outputs[i], ps.recall(input=inputs[i], used=\"sh\")))\n",
    "    Mcossim_learned_s_to_hPS.append(\n",
    "        loss(inputs[i], ps.recall(input=outputs[i], used=\"hs\"))\n",
    "    )\n",
    "    Mcossim_learned_h_to_sPS.append(loss(outputs[i], W_sh @ inputs[i]))\n",
    "\n",
    "    # cossim_learned_s_to_h.append(torch.cosine_similarity(inputs[i], A @ outputs[i], dim=0))\n",
    "    # cossim_learned_h_to_s.append(torch.cosine_similarity(outputs[i], A_pinv @ inputs[i], dim=0))\n",
    "\n",
    "    # print(\"mean A\", torch.mean(A))\n",
    "    # print(\"mean non zero A\", torch.mean(A[A != 0]))\n",
    "    # print(A[0])\n",
    "    if i % 50 == 0:\n",
    "        for o in range(1):\n",
    "            Hs.append(ps.recall(outputs[o], used=\"hs\"))\n",
    "            # print next to each other with plot\n",
    "            fig, ax = plt.subplots(1, 5, figsize=(15, 4))\n",
    "            ax[0].imshow(outputs[o].reshape(28, 28).cpu().numpy())\n",
    "            ax[0].set_title(\"Original Image\")\n",
    "            ax[1].imshow((W_sh @ (inputs[o])).reshape(28, 28).cpu().numpy())\n",
    "            meanhippo = ps.recall((outputs[o]), used=\"hs\")\n",
    "            # meanhippo = meanhippo - torch.mean(meanhippo)\n",
    "            ax[1].set_title(\" HEBB RECALLED Image H -> S\")\n",
    "            ax[2].imshow(\n",
    "                (ps.recall((inputs[o]), used=\"sh\")).reshape(28, 28).cpu().numpy()\n",
    "            )\n",
    "            ax[2].set_title(\"PS RECALLED Image H -> S\")\n",
    "            ax[3].imshow((W_sh @ (meanhippo)).reshape(28, 28).cpu().numpy())\n",
    "            ax[3].set_title(\"Hebb Recalled Image S -> H -> S\")\n",
    "            ax[4].imshow(\n",
    "                (ps.recall((meanhippo), used=\"sh\")).reshape(28, 28).cpu().numpy()\n",
    "            )\n",
    "            ax[4].set_title(\"PS Recalled Image S -> H -> S\")\n",
    "            # plt title\n",
    "            fig.tight_layout()\n",
    "            fig.suptitle(\"STEP \" + str(i))\n",
    "            figs.append(fig)\n",
    "            print(\"Mean of W_sh Hebb\", torch.mean(W_sh))\n",
    "            print(\"Mean of W_sh PS\", torch.mean(ps.W_sh))\n",
    "\n",
    "# show all the figures on top of each other\n",
    "for fig in figs:\n",
    "    plt.show()\n",
    "\n",
    "for i in range(n):\n",
    "    # cossim_recall_s_to_h.append(torch.cosine_similarity(inputs[i], A @ outputs[i], dim=0))\n",
    "    # cossim_recall_h_to_s.append(torch.cosine_similarity(outputs[i], A_pinv @ inputs[i], dim=0))\n",
    "\n",
    "    Ccossim_recall_s_to_hPS.append(\n",
    "        torch.cosine_similarity(\n",
    "            inputs[i], ps.recall(input=outputs[i], used=\"hs\"), dim=0\n",
    "        )\n",
    "    )\n",
    "    # Ccossim_recall_h_to_sPS.append(torch.cosine_similarity(inputs[i], ps.recall(input=outputs[i], used=\"hs\"), dim=0))\n",
    "    Ccossim_recall_h_to_sPS.append(\n",
    "        torch.cosine_similarity(outputs[i], W_sh @ inputs[i], dim=0)\n",
    "    )\n",
    "\n",
    "    Mcossim_recall_s_to_hPS.append(\n",
    "        loss(inputs[i], ps.recall(input=outputs[i], used=\"hs\"))\n",
    "    )\n",
    "    # Mcossim_recall_h_to_sPS.append(loss(inputs[i], ps.recall(input=outputs[i], used=\"hs\")))\n",
    "    Mcossim_recall_h_to_sPS.append(loss(outputs[i], W_sh @ inputs[i]))\n",
    "\n",
    "    IMAGERECONSTRUCTION.append(\n",
    "        torch.cosine_similarity(\n",
    "            outputs[i],\n",
    "            ps.recall((ps.recall((outputs[i]), used=\"hs\")), used=\"sh\"),\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Cossim\")\n",
    "\n",
    "print(\"Mean Learned S to H\", torch.mean(torch.tensor(Ccossim_learned_s_to_hPS)))\n",
    "print(\"Mean Recall S to H\", torch.mean(torch.tensor(Ccossim_recall_s_to_hPS)))\n",
    "print(\"Mean Learned H to S\", torch.mean(torch.tensor(Ccossim_learned_h_to_sPS)))\n",
    "print(\"Mean Recall H to S\", torch.mean(torch.tensor(Ccossim_recall_h_to_sPS)))\n",
    "\n",
    "print(\"MSE LOSS\")\n",
    "\n",
    "print(\"Mean Learned S to H\", torch.mean(torch.tensor(Mcossim_learned_s_to_hPS)))\n",
    "print(\"Mean Recall S to H\", torch.mean(torch.tensor(Mcossim_recall_s_to_hPS)))\n",
    "print(\"Mean Learned H to S\", torch.mean(torch.tensor(Mcossim_learned_h_to_sPS)))\n",
    "print(\"Mean Recall H to S\", torch.mean(torch.tensor(Mcossim_recall_h_to_sPS)))\n",
    "\n",
    "print(\"RAHHHHHHHH\")\n",
    "print(\"Mean Image Reconstruction\", torch.mean(torch.tensor(IMAGERECONSTRUCTION)))\n",
    "\n",
    "# print 5 random images, reconstructed vs not\n",
    "\n",
    "\n",
    "#         for i in range(1):\n",
    "#             # print next to each other with plot\n",
    "#             fig, ax = plt.subplots(1, 5, figsize=(15, 4))\n",
    "#             ax[0].imshow(outputs[i].reshape(28, 28).cpu().numpy())\n",
    "#             ax[0].set_title(\"Original Image\")\n",
    "#             ax[1].imshow((W_sh @ (inputs[i])).reshape(28, 28).cpu().numpy())\n",
    "#             meanhippo = ps.recall((outputs[i]), used=\"hs\")\n",
    "#             meanhippo = meanhippo - torch.mean(meanhippo)\n",
    "#             print(torch.mean(meanhippo))\n",
    "#             ax[1].set_title(\" HEBB RECALLED Image H -> S\")\n",
    "#             ax[2].imshow((ps.recall((inputs[i]), used=\"sh\")).reshape(28, 28).cpu().numpy())\n",
    "#             ax[2].set_title(\"PS RECALLED Image H -> S\")\n",
    "#             ax[3].imshow((W_sh @ (meanhippo)).reshape(28, 28).cpu().numpy())\n",
    "#             ax[3].set_title(\"PS Recalled Image S -> H -> S\")\n",
    "#             ax[4].imshow((ps.recall((meanhippo), used=\"sh\")).reshape(28, 28).cpu().numpy())\n",
    "#             ax[4].set_title(\"Hebb Recalled Image S -> H -> S\")\n",
    "#             # plt title\n",
    "#             plt.title(\"STEP \" + str(i))\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NO FIX, 300 IMAGES, INHIBITION PSEUDO INVERSE, 1000 Nh\n",
    "\n",
    "cos sim Learned a S TO H tensor(0.9826)    \n",
    "cos sim Recall a S TO H tensor(0.9010)    RECALL        S --> H\n",
    "cos sim Learned b H TO S tensor(0.8408)    \n",
    "cos sim Recall b H TO S tensor(0.2089)    RECALL         H --> S\n",
    "mse loss Learned a PS tensor(0.2577)\n",
    "mse loss Recall a PS tensor(0.3314)        RECALL         S --> H\n",
    "mse loss Learned b PS tensor(0.3707)\n",
    "mse loss Recall b PS tensor(1.3567)        RECALL         H --> S\n",
    "\n",
    "\n",
    "YES FIX, 300 IMAGES, INHIBITION PSEUDO INVERSE, 1000 Nh\n",
    "cos sim Learned a S TO H tensor(0.9837)\n",
    "cos sim Recall a S TO H tensor(0.9151)     RECALL        S --> H\n",
    "cos sim Learned b H TO S tensor(0.9013)\n",
    "cos sim Recall b H TO S tensor(0.3045)     RECALL       H --> S\n",
    "mse loss Learned a PS tensor(0.2583)\n",
    "mse loss Recall a PS tensor(0.3276)       RECALL         S --> H\n",
    "mse loss Learned b PS tensor(0.3087)\n",
    "mse loss Recall b PS tensor(0.9600)       RECALL        H --> S\n",
    "\n",
    "above is STD AND MEAN\n",
    "\n",
    "\n",
    "NO FIX, 300  RNDM, INHIBTION PSEUDO INVERSE, 1000 NH\n",
    "\n",
    "cos sim Learned a S TO H tensor(0.9825)\n",
    "cos sim Recall a S TO H tensor(0.9057)\n",
    "cos sim Learned b H TO S tensor(0.9775)\n",
    "cos sim Recall b H TO S tensor(0.8788)\n",
    "mse loss Learned a PS tensor(0.2577)\n",
    "mse loss Recall a PS tensor(0.3327)\n",
    "mse loss Learned b PS tensor(0.2651)\n",
    "mse loss Recall b PS tensor(0.3619)\n",
    "\n",
    "\n",
    "\n",
    "FIX, 300 IMGS, JUST MEAN\n",
    "cos sim Learned a S TO H tensor(0.9908)\n",
    "cos sim Recall a S TO H tensor(0.7363)\n",
    "cos sim Learned b H TO S tensor(0.9515)\n",
    "cos sim Recall b H TO S tensor(0.3840)\n",
    "mse loss Learned a PS tensor(1.2398)\n",
    "mse loss Recall a PS tensor(1.6315)\n",
    "mse loss Learned b PS tensor(0.1746)\n",
    "mse loss Recall b PS tensor(0.8690)\n",
    "\n",
    "\n",
    "FIX, 300 IMGS, JUST STD\n",
    "\n",
    "cos sim Learned a S TO H tensor(0.9851)\n",
    "cos sim Recall a S TO H tensor(0.9173)\n",
    "cos sim Learned b H TO S tensor(0.3782)\n",
    "cos sim Recall b H TO S tensor(-0.0012)\n",
    "mse loss Learned a PS tensor(0.2769)\n",
    "mse loss Recall a PS tensor(0.3495)\n",
    "mse loss Learned b PS tensor(1.7704e+10)\n",
    "mse loss Recall b PS tensor(7.5863e+10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 5 bar graphs with Different methods and y the cosine similarity and another y the MSE loss\n",
    "# there will be 5 METHODS : for each method there will be 4 bar for cossim (s->h, h->s) (learned and recall mean) and 4 bar for mse loss (s->h, h->s) (learned and recall mean)\n",
    "# first method is no fix, 300 imgs, second is yes fix mean and std, 300 imgs, thrid is 300 rnd imgs no fix, fourth is 300 imgs fix only mean, fifth is 300 imgs fix only std\n",
    "\n",
    "# data to plot\n",
    "# NO FIX, 300 IMAGES, INHIBITION PSEUDO INVERSE, 1000 Nh\n",
    "\n",
    "# cos sim Learned a S TO H tensor(0.9826)\n",
    "# cos sim Recall a S TO H tensor(0.9010)    RECALL        S --> H\n",
    "# cos sim Learned b H TO S tensor(0.8408)\n",
    "# cos sim Recall b H TO S tensor(0.2089)    RECALL         H --> S\n",
    "# mse loss Learned a PS tensor(0.2577)\n",
    "# mse loss Recall a PS tensor(0.3314)        RECALL         S --> H\n",
    "# mse loss Learned b PS tensor(0.3707)\n",
    "# mse loss Recall b PS tensor(1.3567)        RECALL         H --> S\n",
    "\n",
    "data1 = [[[0.9826, 0.9010], [0.8408, 0.2089]], [[0.2577, 0.3314], [0.3707, 1.3567]]]\n",
    "\n",
    "\n",
    "# YES FIX, 300 IMAGES, INHIBITION PSEUDO INVERSE, 1000 Nh\n",
    "# cos sim Learned a S TO H tensor(0.9837)\n",
    "# cos sim Recall a S TO H tensor(0.9151)     RECALL        S --> H\n",
    "# cos sim Learned b H TO S tensor(0.9013)\n",
    "# cos sim Recall b H TO S tensor(0.3045)     RECALL       H --> S\n",
    "# mse loss Learned a PS tensor(0.2583)\n",
    "# mse loss Recall a PS tensor(0.3276)       RECALL         S --> H\n",
    "# mse loss Learned b PS tensor(0.3087)\n",
    "# mse loss Recall b PS tensor(0.9600)       RECALL        H --> S\n",
    "\n",
    "data2 = [[[0.9837, 0.9151], [0.9013, 0.3045]], [[0.2583, 0.3276], [0.3087, 1.9600]]]\n",
    "\n",
    "# above is STD AND MEAN\n",
    "\n",
    "\n",
    "# NO FIX, 300  RNDM, INHIBTION PSEUDO INVERSE, 1000 NH\n",
    "\n",
    "# cos sim Learned a S TO H tensor(0.9825)\n",
    "# cos sim Recall a S TO H tensor(0.9057)\n",
    "# cos sim Learned b H TO S tensor(0.9775)\n",
    "# cos sim Recall b H TO S tensor(0.8788)\n",
    "# mse loss Learned a PS tensor(0.2577)\n",
    "# mse loss Recall a PS tensor(0.3327)\n",
    "# mse loss Learned b PS tensor(0.2651)\n",
    "# mse loss Recall b PS tensor(0.3619)\n",
    "\n",
    "data3 = [[[0.9825, 0.9057], [0.9775, 0.8788]], [[0.2577, 0.3327], [0.2651, 0.3619]]]\n",
    "\n",
    "\n",
    "# FIX, 300 IMGS, JUST MEAN\n",
    "# cos sim Learned a S TO H tensor(0.9908)\n",
    "# cos sim Recall a S TO H tensor(0.7363)\n",
    "# cos sim Learned b H TO S tensor(0.9515)\n",
    "# cos sim Recall b H TO S tensor(0.3840)\n",
    "# mse loss Learned a PS tensor(1.2398)\n",
    "# mse loss Recall a PS tensor(1.6315)\n",
    "# mse loss Learned b PS tensor(0.1746)\n",
    "# mse loss Recall b PS tensor(0.8690)\n",
    "\n",
    "data4 = [[[0.9908, 0.7363], [0.9515, 0.3840]], [[1.2398, 1.6315], [0.1746, 0.8690]]]\n",
    "\n",
    "\n",
    "# FIX, 300 IMGS, JUST STD\n",
    "\n",
    "# cos sim Learned a S TO H tensor(0.9851)\n",
    "# cos sim Recall a S TO H tensor(0.9173)\n",
    "# cos sim Learned b H TO S tensor(0.3782)\n",
    "# cos sim Recall b H TO S tensor(-0.0012)\n",
    "# mse loss Learned a PS tensor(0.2769)\n",
    "# mse loss Recall a PS tensor(0.3495)\n",
    "# mse loss Learned b PS tensor(1.7704e+10)\n",
    "# mse loss Recall b PS tensor(7.5863e+10)\n",
    "\n",
    "methods = [\n",
    "    \"NO FIX, 300 IMAGES, INHIBITION PSEUDO INVERSE, 1000 Nh\",\n",
    "    \"YES FIX, 300 IMAGES, INHIBITION PSEUDO INVERSE, 1000 Nh\",\n",
    "    \"NO FIX, 300  RNDM, INHIBTION PSEUDO INVERSE, 1000 NH\",\n",
    "    \"FIX, 300 IMGS, JUST MEAN\",\n",
    "    \"FIX, 300 IMGS, JUST STD, 1000 Nh\",\n",
    "]\n",
    "\n",
    "data5 = [\n",
    "    [[0.9851, 0.9173], [0.3782, -0.0012]],\n",
    "    [[0.2769, 0.3495], [1.7704e10, 7.5863e10]],\n",
    "]\n",
    "\n",
    "data = [data1, data2, data3, data4, data5]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(5, 2, figsize=(15, 20))\n",
    "index = np.arange(1)\n",
    "bar_width = 0.15\n",
    "opacity = 0.8\n",
    "\n",
    "for i in range(5):\n",
    "    # print first method\n",
    "    for o in range(2):\n",
    "        p = data[i]\n",
    "        # first two bars are learned and recall for s -> h\n",
    "        ax[i][o].bar(\n",
    "            index, p[o][0][0], bar_width, alpha=opacity, color=\"b\", label=\"Learned\"\n",
    "        )\n",
    "        ax[i][o].bar(\n",
    "            index + bar_width,\n",
    "            p[o][0][1],\n",
    "            bar_width,\n",
    "            alpha=opacity,\n",
    "            color=\"r\",\n",
    "            label=\"Recall\",\n",
    "        )\n",
    "\n",
    "        # second two bars are learned and recall for h -> s\n",
    "        ax[i][o].bar(\n",
    "            index + 2 * bar_width,\n",
    "            p[o][1][0],\n",
    "            bar_width,\n",
    "            alpha=opacity,\n",
    "            color=\"b\",\n",
    "            label=\"Learned\",\n",
    "        )\n",
    "        ax[i][o].bar(\n",
    "            index + 3 * bar_width,\n",
    "            p[o][1][1],\n",
    "            bar_width,\n",
    "            alpha=opacity,\n",
    "            color=\"r\",\n",
    "            label=\"Recall\",\n",
    "        )\n",
    "\n",
    "    # add another y label for mse scale\n",
    "\n",
    "    ax[i][0].set_xlabel(\"COSINE SIMILARITY \\n\" + methods[i])\n",
    "    ax[i][0].set_ylabel(\"Cossine Similarity\")\n",
    "    ax[i][0].set_xticklabels((\"COSINE SIMILARITY S--> H\", \"COSINE SIMILARITY H-->S\"))\n",
    "    ax[i][0].legend()\n",
    "\n",
    "    ax[i][1].set_xlabel(\"MSE LOSS\\n\" + methods[i])\n",
    "    ax[i][1].set_ylabel(\"MSE LOSS\")\n",
    "    ax[i][1].set_xticklabels((\"MSE LOSS S--> H\", \"MSE LOSS H-->S\"))\n",
    "    ax[i][1].legend()\n",
    "\n",
    "\n",
    "# space out the plots more\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = 1 2 0 1\n",
    "# 0 1 -1 0\n",
    "# O 0 1 -l\n",
    "A = torch.tensor([[1, 2, 0, 1], [0, 1, -1, 0], [0, 0, 1, -1]])\n",
    "A_pinv = (1 / 12) * torch.tensor([[3, -3, 0], [3, 5, 4], [3, -7, 4], [3, -7, -8]])\n",
    "c = torch.tensor([1, 2, 3, 4])\n",
    "d = torch.tensor([1, 2, 3, 4])\n",
    "A_pinv = Rk1MrUpdate(A, A_pinv, c, d, 1e-14, 0)\n",
    "\n",
    "A_pinv @ c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_gbook(lambdas, Ng, Npos):\n",
    "    ginds = [0, lambdas[0], lambdas[0] + lambdas[1]]\n",
    "    gbook = np.zeros((Ng, Npos))\n",
    "    for x in range(Npos):\n",
    "        phis = np.mod(x, lambdas)\n",
    "        gbook[phis + ginds, x] = 1\n",
    "    return gbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbook = gen_gbook([3, 4, 5], (50), 1000)\n",
    "print(gbook[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
