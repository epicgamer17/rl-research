{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import axes\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    dataset,\n",
    "    num_imgs=10,\n",
    "    preprocess_sensory=True,\n",
    "    noise_level=\"medium\",\n",
    "):\n",
    "    import torch\n",
    "    import random\n",
    "\n",
    "    data = dataset.data\n",
    "    # print(num_imgs)\n",
    "    # print(data.shape)\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    # print(data.shape)\n",
    "    data = torch.tensor(data[:num_imgs]).float().to(\"cpu\")\n",
    "    # print(data.shape)\n",
    "\n",
    "    # data = random.sample(dataset.data.flatten(1).float().to(\"cpu\"), num_imgs)\n",
    "    if preprocess_sensory:\n",
    "        data = (data - data.mean()) / data.std()\n",
    "        # print(mnist_data[0])\n",
    "\n",
    "    # noissing the data\n",
    "    if noise_level == \"none\":\n",
    "        return data, data\n",
    "    elif noise_level == \"low\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1, 1)\n",
    "    elif noise_level == \"medium\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.25, 1.25)\n",
    "    elif noise_level == \"high\":\n",
    "        random_noise = torch.zeros_like(data).uniform_(-1.5, 1.5)\n",
    "    noisy_data = data + random_noise\n",
    "    # TODO: DO WE PREPROCESS NOISY IMAGES?\n",
    "    # if preprocess_sensory:\n",
    "    #     noisy_mnist = (noisy_mnist - noisy_mnist.mean()) / noisy_mnist.std()\n",
    "\n",
    "    return data, noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from nd_scaffold import GridScaffold, SparseMatrixBySparsityInitializer\n",
    "from graph_utils import graph_scaffold, print_imgs_side_by_side\n",
    "from matrix_initializers import SparseMatrixByScalingInitializer\n",
    "from vectorhash_functions import solve_mean, spacefillingcurve\n",
    "import math\n",
    "\n",
    "\n",
    "def test_memory_capacity(\n",
    "    data,\n",
    "    noisy_data,\n",
    "    shapes=[(3, 3, 5), (4, 4, 7)],\n",
    "    N_h=1000,\n",
    "    initalization_method=\"by_scaling\",\n",
    "    percent_nonzero_relu=0.01,\n",
    "    var=1.0,\n",
    "    sparse_initialization=0.1,\n",
    "    T=0.01,\n",
    "):\n",
    "\n",
    "    GS = GridScaffold(\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        input_size=data.shape[1],\n",
    "        device=\"cpu\",\n",
    "        sparse_matrix_initializer=(\n",
    "            SparseMatrixByScalingInitializer(\n",
    "                mean=solve_mean(p=percent_nonzero_relu, var=(len(shapes) * var))\n",
    "                / len(shapes),\n",
    "                scale=math.sqrt(var),\n",
    "            )\n",
    "            if initalization_method == \"by_scaling\"\n",
    "            else SparseMatrixBySparsityInitializer(sparsity=sparse_initialization)\n",
    "        ),\n",
    "        relu_theta=(\n",
    "            0.0\n",
    "            if initalization_method == \"by_scaling\"\n",
    "            else solve_mean(\n",
    "                p=percent_nonzero_relu, var=len(shapes) * (1 - sparse_initialization)\n",
    "            )\n",
    "        ),\n",
    "        T=T,\n",
    "        continualupdate=False,\n",
    "        ratshift=False,\n",
    "        initialize_W_gh_with_zeroes=False,\n",
    "    )\n",
    "\n",
    "    # learn over all images\n",
    "    v = spacefillingcurve(shapes)\n",
    "\n",
    "    GS.learn_path(observations=data, velocities=v[: len(data)])\n",
    "    print(len(v[: len(data)]))\n",
    "    recalled_imgs = GS.recall(noisy_data)\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(data, recalled_imgs)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating Capacity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 8, 15, 25, 37, 56, 67, 74, 82, 111, 148, 222, 739]\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 1\n",
      "Unique Hs seen while learning: 1\n",
      "1\n",
      "Unique Hs seen while recalling: 1\n",
      "avg nonzero H: 9.0\n",
      "avg nonzero H_denoised: 9.0\n",
      "Unique Gs seen while recalling (before denoising): 1\n",
      "Cosine Similarity 0.9999998807907104\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 3\n",
      "Unique Hs seen while learning: 3\n",
      "3\n",
      "Unique Hs seen while recalling: 3\n",
      "avg nonzero H: 33.0\n",
      "avg nonzero H_denoised: 11.333333015441895\n",
      "Unique Gs seen while recalling (before denoising): 3\n",
      "Cosine Similarity 0.9990351796150208\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 8\n",
      "Unique Hs seen while learning: 8\n",
      "8\n",
      "Unique Hs seen while recalling: 8\n",
      "avg nonzero H: 47.75\n",
      "avg nonzero H_denoised: 10.25\n",
      "Unique Gs seen while recalling (before denoising): 7\n",
      "Cosine Similarity 0.8618323802947998\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 15\n",
      "Unique Hs seen while learning: 15\n",
      "15\n",
      "Unique Hs seen while recalling: 15\n",
      "avg nonzero H: 84.4000015258789\n",
      "avg nonzero H_denoised: 10.333333015441895\n",
      "Unique Gs seen while recalling (before denoising): 14\n",
      "Cosine Similarity 0.8874110579490662\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 25\n",
      "Unique Hs seen while learning: 25\n",
      "25\n",
      "Unique Hs seen while recalling: 25\n",
      "avg nonzero H: 109.83999633789062\n",
      "avg nonzero H_denoised: 10.079999923706055\n",
      "Unique Gs seen while recalling (before denoising): 22\n",
      "Cosine Similarity 0.7876415252685547\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 37\n",
      "Unique Hs seen while learning: 37\n",
      "37\n",
      "Unique Hs seen while recalling: 37\n",
      "avg nonzero H: 138.91891479492188\n",
      "avg nonzero H_denoised: 8.783783912658691\n",
      "Unique Gs seen while recalling (before denoising): 21\n",
      "Cosine Similarity 0.49793675541877747\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n",
      "Unique Gs seen while learning: 56\n",
      "Unique Hs seen while learning: 56\n",
      "56\n",
      "Unique Hs seen while recalling: 56\n",
      "avg nonzero H: 165.80357360839844\n",
      "avg nonzero H_denoised: 8.410714149475098\n",
      "Unique Gs seen while recalling (before denoising): 33\n",
      "Cosine Similarity 0.5044332146644592\n",
      "==========================================\n",
      "module shapes:  [(5, 5), (9, 9), (11, 11)]\n",
      "N_g     :  227\n",
      "N_patts :  245025\n",
      "N_h     :  1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==========================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m data, noisy_data \u001b[38;5;241m=\u001b[39m prepare_data(\n\u001b[1;32m     66\u001b[0m     dataset,\n\u001b[1;32m     67\u001b[0m     num_imgs\u001b[38;5;241m=\u001b[39mnum_imgs,\n\u001b[1;32m     68\u001b[0m     preprocess_sensory\u001b[38;5;241m=\u001b[39mpreprocess_sensory,\n\u001b[1;32m     69\u001b[0m     noise_level\u001b[38;5;241m=\u001b[39mnoise_level,\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mtest_memory_capacity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mN_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitalization_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mby_scaling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpercent_nonzero_relu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_initialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine Similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mmean(similarity)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     82\u001b[0m similarities\u001b[38;5;241m.\u001b[39mappend(similarity)\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mtest_memory_capacity\u001b[0;34m(data, noisy_data, shapes, N_h, initalization_method, percent_nonzero_relu, var, sparse_initialization, T)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# learn over all images\u001b[39;00m\n\u001b[1;32m     50\u001b[0m v \u001b[38;5;241m=\u001b[39m spacefillingcurve(shapes)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(v[: \u001b[38;5;28mlen\u001b[39m(data)]))\n\u001b[1;32m     54\u001b[0m recalled_imgs \u001b[38;5;241m=\u001b[39m GS\u001b[38;5;241m.\u001b[39mrecall(noisy_data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/vectorhash/nd_scaffold.py:510\u001b[0m, in \u001b[0;36mGridScaffold.learn_path\u001b[0;34m(self, observations, velocities)\u001b[0m\n\u001b[1;32m    508\u001b[0m     seen_gs\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m    509\u001b[0m     seen_hs\u001b[38;5;241m.\u001b[39madd(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hg \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu_theta))\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique Gs seen while learning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(seen_gs))\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique Hs seen while learning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(seen_hs))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/vectorhash/nd_scaffold.py:521\u001b[0m, in \u001b[0;36mGridScaffold.learn\u001b[0;34m(self, observation, velocity)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation, velocity):\n\u001b[1;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add a memory to the memory scaffold and shift the grid coding state by a given velocity.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m    observation shape: `(input_size)`\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    velocity shape: `(D)` where `D` is the dimensionality of the grid modules.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift(velocity)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/rl-stuff/vectorhash/nd_scaffold.py:447\u001b[0m, in \u001b[0;36mGridScaffold.store_memory\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    448\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hg \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu_theta)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinualupdate:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Memory Capacity Tests\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "shapes = [(3, 3), (4, 4), (5, 5)]\n",
    "N_h = 1000\n",
    "\n",
    "N_g = 0\n",
    "for shape in shapes:\n",
    "    l = torch.prod(torch.tensor(shape)).item()\n",
    "    N_g += l\n",
    "# print(\"N_g\", N_g)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten())]\n",
    ")\n",
    "# dataset = torchvision.datasets.MNIST(\n",
    "#     root=\"data\", train=True, download=True, transform=transform\n",
    "# )\n",
    "\n",
    "# dataset = torchvision.datasets.FashionMNIST(\n",
    "#     root=\"data\", train=True, download=True, transform=transform\n",
    "# )\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR100(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "input_size = 1\n",
    "for shape in dataset.data[0].shape:\n",
    "    input_size *= shape\n",
    "\n",
    "theoretical_capacity = N_g * N_h / input_size  # 784 is the size of the input for MNIST\n",
    "\n",
    "percents = [\n",
    "    0.01,\n",
    "    0.03,\n",
    "    0.1,\n",
    "    0.2,\n",
    "    0.33,\n",
    "    0.5,\n",
    "    0.75,\n",
    "    0.9,\n",
    "    1.0,\n",
    "    1.1,\n",
    "    1.5,\n",
    "    2.0,\n",
    "    3.0,\n",
    "    10.0,\n",
    "]\n",
    "\n",
    "num_images = [theoretical_capacity * p for p in percents]\n",
    "num_images = [math.ceil(n) for n in num_images]\n",
    "print(num_images)\n",
    "\n",
    "preprocess_sensory = True\n",
    "noise_level = \"medium\"\n",
    "\n",
    "# TODO: Need a hyperparam N_p for dimension of the projection from grid cells to place cells\n",
    "similarities = []\n",
    "for num_imgs in num_images:\n",
    "    print(\"==========================================\")\n",
    "    data, noisy_data = prepare_data(\n",
    "        dataset,\n",
    "        num_imgs=num_imgs,\n",
    "        preprocess_sensory=preprocess_sensory,\n",
    "        noise_level=noise_level,\n",
    "    )\n",
    "    similarity = test_memory_capacity(\n",
    "        data,\n",
    "        noisy_data,\n",
    "        shapes=shapes,\n",
    "        N_h=N_h,\n",
    "        initalization_method=\"by_scaling\",\n",
    "        percent_nonzero_relu=10 / N_h,\n",
    "        var=1.0,\n",
    "        sparse_initialization=0.0,\n",
    "    )\n",
    "    print(\"Cosine Similarity\", torch.mean(similarity).item())\n",
    "    similarities.append(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing initialization techniques (at 10%, 50%, 100%, 150% capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare continual learning vs learning once at start (at 10%, 50%, 100%, 150% capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparam tuning for number active hippocampal cells after denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we losing grid states? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
