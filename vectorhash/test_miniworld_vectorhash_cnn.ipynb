{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniWorld VectorHash + CNN Performance Tests\n",
    "\n",
    "This notebook runs a grid of VectorHash localization tests on the\n",
    "MiniWorld-Maze environment, comparing raw-flattened pixels against a\n",
    "ResNet-18 pretrained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports, device, reproducibility ──\n",
    "import os, itertools, pickle\n",
    "import torch, numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from miniworld.params import DEFAULT_PARAMS\n",
    "from preprocessing_cnn import PreprocessingCNN\n",
    "from vectorhash import build_vectorhash_architecture\n",
    "from miniworld_agent import MiniworldVectorhashAgent\n",
    "from agent import kidnapping_test\n",
    "from smoothing import (\n",
    "    IdentitySmoothing,\n",
    "    PolynomialSmoothing,\n",
    "    SoftmaxSmoothing,\n",
    "    RatSLAMSmoothing,\n",
    ")\n",
    "from shifts import RatShift\n",
    "\n",
    "# fix seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Env builder, shapes & limits ──\n",
    "\n",
    "shapes = [(3, 3, 4), (4, 4, 5)]\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    params = DEFAULT_PARAMS.copy().no_random()\n",
    "    env = gym.make(\n",
    "        \"MiniWorld-Maze-v0\", max_episode_steps=-1, params=params, domain_rand=False\n",
    "    )\n",
    "    # compute limits from the wrapper attrs\n",
    "    min_x = env.get_wrapper_attr(\"min_x\")\n",
    "    max_x = env.get_wrapper_attr(\"max_x\")\n",
    "    min_z = env.get_wrapper_attr(\"min_z\")\n",
    "    max_z = env.get_wrapper_attr(\"max_z\")\n",
    "    limits = torch.tensor(\n",
    "        [max_x - min_x, max_z - min_z, 2 * np.pi], device=device\n",
    "    ).float()\n",
    "    return env, limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ezrahuang/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ── CNN Preprocessor & Hyperparams ──\n",
    "\n",
    "# 1) CNN encoder (ResNet-18 adapter)\n",
    "cnn_preproc = PreprocessingCNN(\n",
    "    device=device,\n",
    "    latent_dim=128,\n",
    "    input_channels=3,\n",
    "    target_size=(224, 224),\n",
    "    model_path=\"resnet18_adapter.pth\",\n",
    ")\n",
    "\n",
    "# 2) Hyperparameter grid\n",
    "store_opts = [True, False]  # True=\"Always\", False=\"When New\"\n",
    "shift_opts = [\"additive\", \"multiplicative\"]\n",
    "hard_opts = [True, False]  # True=\"Hard\", False=\"Soft\"\n",
    "smooth_opts = [\n",
    "    IdentitySmoothing(),\n",
    "    PolynomialSmoothing(k=1.0),\n",
    "    PolynomialSmoothing(k=1.5),\n",
    "    SoftmaxSmoothing(T=0.1),\n",
    "    RatSLAMSmoothing(device=device),\n",
    "]\n",
    "\n",
    "# 3) Output folder\n",
    "basedir = \"miniworld_cnn_tests\"\n",
    "os.makedirs(basedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path length: 40\n",
      "Visibles length: 40\n"
     ]
    }
   ],
   "source": [
    "# ─── 1) Motion primitives (90° turns) ───\n",
    "# MiniWorld-Maze: 0=turn left 90°, 1=turn right 90°, 2=move forward\n",
    "forward_10 = [2] * 10  # forward ×10\n",
    "forward_3 = [2] * 3  # forward ×3\n",
    "turn_r = [1]  # one 90° right turn\n",
    "turn_l = [0]  # one 90° left turn\n",
    "\n",
    "# ─── 2) Build path ───\n",
    "# Sequence: F10 → R → F3 → R → F10 → L → F3 → L → F10\n",
    "path = (\n",
    "    forward_10\n",
    "    + turn_r\n",
    "    + forward_3\n",
    "    + turn_r\n",
    "    + forward_10\n",
    "    + turn_l\n",
    "    + forward_3\n",
    "    + turn_l\n",
    "    + forward_10\n",
    ")\n",
    "\n",
    "# Check length\n",
    "print(\"Path length:\", len(path))  # should be 10+1+3+1+10+1+3+1+10 = 40\n",
    "\n",
    "# ─── 3) Visibility flags ───\n",
    "# Make a visibility segment for each chunk\n",
    "visible_10 = [True] * 10\n",
    "visible_3 = [True] * 3\n",
    "visible_1 = [True] * 1\n",
    "not_visible_1 = [False] * 1\n",
    "\n",
    "# Now concatenate in the same order as `path`\n",
    "visibles = (\n",
    "    visible_10\n",
    "    + visible_1  # turn_r\n",
    "    + visible_3\n",
    "    + visible_1  # turn_r\n",
    "    + visible_10\n",
    "    + visible_1  # turn_l\n",
    "    + visible_3\n",
    "    + visible_1  # turn_l\n",
    "    + visible_10\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(\"Visibles length:\", len(visibles))  # → 40\n",
    "\n",
    "# ─── 4) Noise list ───\n",
    "# Here we leave each step noise-free\n",
    "noise_list = [[] for _ in path]\n",
    "\n",
    "# Final sanity check\n",
    "assert len(path) == len(visibles) == len(noise_list), \"Lengths must match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cnn__Always__additive__Hard__Identity\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/Projects/rl-research/vectorhash/hippocampal_sensory_layers.py:110: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n",
      "  1 + input.T @ self.inhibition_matrix_hs @ input\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:102: UserWarning: Using a target size (torch.Size([1, 600])) that is different to the input size (torch.Size([600])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(h, h_from_s),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:109: UserWarning: Using a target size (torch.Size([1, 600])) that is different to the input size (torch.Size([600])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(h, h_from_s_denoised),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:125: UserWarning: Using a target size (torch.Size([1, 128])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(s, s_from_h_from_s),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:132: UserWarning: Using a target size (torch.Size([1, 128])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(s, s_from_h_from_s_denoised),\n",
      "/home/ezrahuang/Projects/rl-research/vectorhash/vectorhash.py:139: UserWarning: Using a target size (torch.Size([1, 128])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  torch.nn.functional.mse_loss(s, s_from_h),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(7.5059) tensor(0.) tensor(1.8728)\n",
      "h_from_s max, min, mean tensor(7.5041) tensor(0.) tensor(1.8723)\n",
      "h_from_s_denoised max, min, mean tensor(1235.0961) tensor(0.) tensor(166.9303)\n",
      "avg nonzero/greaterzero h from book: tensor(543) tensor(543)\n",
      "avg nonzero/greaterzero h from s: tensor(543) tensor(543)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(323) tensor(323)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.9945e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(80207.0781) tensor([0.5578])\n",
      "mse/cosinesimilarity s and s from h from s tensor(4.6478e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0170) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(3.1102e-10) tensor([1.])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(7.5059) tensor(0.) tensor(1.8728)\n",
      "h_from_s max, min, mean tensor(7.5039) tensor(0.) tensor(1.8723)\n",
      "h_from_s_denoised max, min, mean tensor(1235.0668) tensor(0.) tensor(166.9264)\n",
      "avg nonzero/greaterzero h from book: tensor(543) tensor(543)\n",
      "avg nonzero/greaterzero h from s: tensor(543) tensor(543)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(323) tensor(323)\n",
      "mse/cosinesimilarity h from book and h from s tensor(3.6117e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(80203.2891) tensor([0.5578])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.9272e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0068) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h tensor(1.2446e-10) tensor([1.])\n",
      "new positions: tensor([4.2338e-03, 3.0560e-02, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(7.1449) tensor(0.) tensor(1.8500)\n",
      "h_from_s max, min, mean tensor(7.1747) tensor(0.) tensor(1.8508)\n",
      "h_from_s_denoised max, min, mean tensor(1220.4991) tensor(0.) tensor(164.9734)\n",
      "avg nonzero/greaterzero h from book: tensor(551) tensor(551)\n",
      "avg nonzero/greaterzero h from s: tensor(552) tensor(552)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(323) tensor(323)\n",
      "mse/cosinesimilarity h from book and h from s tensor(0.0001) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(78372.0703) tensor([0.5669])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.7485e-05) tensor([0.9999])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0071) tensor([0.9986])\n",
      "mse/cosinesimilarity s and s from h tensor(7.8872e-08) tensor([1.0000])\n",
      "new positions: tensor([0.0085, 0.0611, 6.2832])\n",
      "new positions: tensor([0.0127, 0.0917, 6.2832])\n",
      "new positions: tensor([0.0169, 0.1222, 6.2832])\n",
      "new positions: tensor([0.0212, 0.1528, 6.2832])\n",
      "new positions: tensor([0.0254, 0.1834, 6.2832])\n",
      "new positions: tensor([0.0296, 0.2139, 6.2832])\n",
      "new positions: tensor([0.0339, 0.2445, 6.2832])\n",
      "new positions: tensor([0.0381, 0.2750, 6.2832])\n",
      "new positions: tensor([0.0423, 0.3056, 6.2832])\n",
      "new positions: tensor([0.0423, 0.3056, 5.9892])\n",
      "new positions: tensor([0.0368, 0.3361, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.6951])\n",
      "new positions: tensor([0.0312, 0.3665, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 5.9892])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "new positions: tensor([0.0312, 0.3665, 6.2832])\n",
      "Running cnn__Always__additive__Hard__Poly(k=1.0)\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.1058) tensor(0.) tensor(1.7668)\n",
      "h_from_s max, min, mean tensor(5.1045) tensor(0.) tensor(1.7664)\n",
      "h_from_s_denoised max, min, mean tensor(2.3231) tensor(1.1900) tensor(1.7210)\n",
      "avg nonzero/greaterzero h from book: tensor(533) tensor(533)\n",
      "avg nonzero/greaterzero h from s: tensor(533) tensor(533)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.6637e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.4507) tensor([0.8277])\n",
      "mse/cosinesimilarity s and s from h from s tensor(2.4777e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0022) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(1.9030e-10) tensor([1.0000])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.1058) tensor(0.) tensor(1.7668)\n",
      "h_from_s max, min, mean tensor(5.1045) tensor(0.) tensor(1.7664)\n",
      "h_from_s_denoised max, min, mean tensor(2.3231) tensor(1.1900) tensor(1.7210)\n",
      "avg nonzero/greaterzero h from book: tensor(533) tensor(533)\n",
      "avg nonzero/greaterzero h from s: tensor(533) tensor(533)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.7686e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.4507) tensor([0.8277])\n",
      "mse/cosinesimilarity s and s from h from s tensor(3.0363e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0026) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(2.3227e-10) tensor([1.])\n",
      "new positions: tensor([0.0219, 0.0251, 6.2832])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(4.8010) tensor(0.) tensor(1.7442)\n",
      "h_from_s max, min, mean tensor(4.8025) tensor(0.) tensor(1.7438)\n",
      "h_from_s_denoised max, min, mean tensor(2.3232) tensor(1.1903) tensor(1.7210)\n",
      "avg nonzero/greaterzero h from book: tensor(551) tensor(551)\n",
      "avg nonzero/greaterzero h from s: tensor(551) tensor(551)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(3.1198e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.2342) tensor([0.8449])\n",
      "mse/cosinesimilarity s and s from h from s tensor(5.1354e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0282) tensor([0.9022])\n",
      "mse/cosinesimilarity s and s from h tensor(4.8747e-07) tensor([1.0000])\n",
      "new positions: tensor([0.0438, 0.0501, 6.2832])\n",
      "new positions: tensor([0.0657, 0.0752, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.6951])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 5.9892])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "new positions: tensor([0.0876, 0.1002, 6.2832])\n",
      "Running cnn__Always__additive__Hard__Poly(k=1.5)\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.9123) tensor(0.) tensor(1.7379)\n",
      "h_from_s max, min, mean tensor(5.9110) tensor(0.) tensor(1.7375)\n",
      "h_from_s_denoised max, min, mean tensor(2.4080) tensor(1.1156) tensor(1.7284)\n",
      "avg nonzero/greaterzero h from book: tensor(540) tensor(540)\n",
      "avg nonzero/greaterzero h from s: tensor(540) tensor(540)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.3181e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.3826) tensor([0.8299])\n",
      "mse/cosinesimilarity s and s from h from s tensor(3.3911e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0042) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h tensor(2.4558e-10) tensor([1.])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.9123) tensor(0.) tensor(1.7379)\n",
      "h_from_s max, min, mean tensor(5.9110) tensor(0.) tensor(1.7375)\n",
      "h_from_s_denoised max, min, mean tensor(2.4080) tensor(1.1156) tensor(1.7284)\n",
      "avg nonzero/greaterzero h from book: tensor(540) tensor(540)\n",
      "avg nonzero/greaterzero h from s: tensor(540) tensor(540)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.3534e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.3826) tensor([0.8299])\n",
      "mse/cosinesimilarity s and s from h from s tensor(2.2637e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0028) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(1.6328e-10) tensor([1.0000])\n",
      "new positions: tensor([8.7137e-03, 3.4140e-03, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.9118) tensor(0.) tensor(1.7379)\n",
      "h_from_s max, min, mean tensor(5.9103) tensor(0.) tensor(1.7374)\n",
      "h_from_s_denoised max, min, mean tensor(2.4080) tensor(1.1156) tensor(1.7284)\n",
      "avg nonzero/greaterzero h from book: tensor(540) tensor(540)\n",
      "avg nonzero/greaterzero h from s: tensor(540) tensor(540)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.8596e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.3823) tensor([0.8299])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0021) tensor([0.9931])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0023) tensor([0.9988])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0021) tensor([0.9931])\n",
      "new positions: tensor([8.7137e-03, 3.4140e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1310e-03, 3.4882e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1622e-03, 3.4902e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1652e-03, 3.4912e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1652e-03, 3.4902e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1652e-03, 3.4912e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1652e-03, 3.4902e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1652e-03, 3.4912e-03, 6.2832e+00])\n",
      "new positions: tensor([9.1652e-03, 3.4902e-03, 6.2832e+00])\n",
      "new positions: tensor([6.2534e-05, 6.8397e-06, 5.9756e+00])\n",
      "new positions: tensor([0.0065, 0.0060, 5.9692])\n",
      "new positions: tensor([2.8336e-05, 2.2473e-05, 5.9690e+00])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.7630) tensor(0.) tensor(1.7695)\n",
      "h_from_s max, min, mean tensor(5.7554) tensor(0.) tensor(1.7691)\n",
      "h_from_s_denoised max, min, mean tensor(2.3966) tensor(1.1259) tensor(1.7288)\n",
      "avg nonzero/greaterzero h from book: tensor(551) tensor(551)\n",
      "avg nonzero/greaterzero h from s: tensor(595) tensor(595)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.0948e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.3470) tensor([0.8376])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.9910e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0193) tensor([0.9511])\n",
      "mse/cosinesimilarity s and s from h tensor(6.1056e-10) tensor([1.])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6615])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6551])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9624])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9688])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2766])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2830])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "Running cnn__Always__additive__Hard__Softmax(T=0.1)\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "h_from_s max, min, mean tensor(5.6060) tensor(0.) tensor(1.7016)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.3749e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(3.0443e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(2.1210e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(2.1210e-10) tensor([1.])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "h_from_s max, min, mean tensor(5.6060) tensor(0.) tensor(1.7016)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.6596e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.6645e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(1.1368e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(1.1368e-10) tensor([1.])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5995) tensor(0.) tensor(1.7023)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(1.1674e-06) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0004) tensor([0.9983])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0004) tensor([0.9980])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0004) tensor([0.9983])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5969) tensor(0.) tensor(1.7017)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(7.6620e-08) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0022) tensor([0.9897])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0025) tensor([0.9884])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0022) tensor([0.9897])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5953) tensor(0.) tensor(1.7013)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(9.7932e-08) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0055) tensor([0.9750])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0061) tensor([0.9721])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0055) tensor([0.9750])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5975) tensor(0.) tensor(1.7019)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.5656e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0063) tensor([0.9704])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0071) tensor([0.9668])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0063) tensor([0.9703])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5935) tensor(0.) tensor(1.7007)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(1.0009e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0082) tensor([0.9634])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0092) tensor([0.9588])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0082) tensor([0.9635])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5939) tensor(0.) tensor(1.7009)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(6.5590e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0085) tensor([0.9591])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0096) tensor([0.9536])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0085) tensor([0.9590])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5938) tensor(0.) tensor(1.7008)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(7.7924e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0134) tensor([0.9307])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0140) tensor([0.9270])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0134) tensor([0.9307])\n",
      "new positions: tensor([9.7710e-07, 2.5750e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5961) tensor(0.) tensor(1.7015)\n",
      "h_from_s max, min, mean tensor(5.5964) tensor(0.) tensor(1.7016)\n",
      "h_from_s_denoised max, min, mean tensor(5.6073) tensor(0.) tensor(1.7020)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(530) tensor(530)\n",
      "mse/cosinesimilarity h from book and h from s tensor(1.4473e-08) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0466e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0211) tensor([0.8956])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0216) tensor([0.8934])\n",
      "mse/cosinesimilarity s and s from h tensor(0.0212) tensor([0.8956])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9691])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.6549])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  5.9690])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2831])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "new positions: tensor([25.7500, 25.7500,  6.2832])\n",
      "Running cnn__Always__additive__Hard__RatSLAM\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(6.1316) tensor(0.) tensor(1.7495)\n",
      "h_from_s max, min, mean tensor(6.1302) tensor(0.) tensor(1.7491)\n",
      "h_from_s_denoised max, min, mean tensor(2.3109) tensor(1.0072) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.4259e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.4793) tensor([0.8245])\n",
      "mse/cosinesimilarity s and s from h from s tensor(5.2424e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0102) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(4.0037e-10) tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezrahuang/miniconda3/envs/ml/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(6.1316) tensor(0.) tensor(1.7495)\n",
      "h_from_s max, min, mean tensor(6.1302) tensor(0.) tensor(1.7490)\n",
      "h_from_s_denoised max, min, mean tensor(2.3109) tensor(1.0072) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s: tensor(530) tensor(530)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.6314e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.4793) tensor([0.8245])\n",
      "mse/cosinesimilarity s and s from h from s tensor(4.9984e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0096) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(3.7824e-10) tensor([1.])\n",
      "new positions: tensor([1.1360e-02, 2.5730e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.8337) tensor(0.) tensor(1.7322)\n",
      "h_from_s max, min, mean tensor(5.8449) tensor(0.) tensor(1.7326)\n",
      "h_from_s_denoised max, min, mean tensor(2.3111) tensor(1.0077) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(534) tensor(534)\n",
      "avg nonzero/greaterzero h from s: tensor(534) tensor(534)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(1.8373e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.3171) tensor([0.8368])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.2084e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0234) tensor([0.9692])\n",
      "mse/cosinesimilarity s and s from h tensor(5.7939e-07) tensor([1.0000])\n",
      "new positions: tensor([1.5696e-02, 2.5749e+01, 6.2832e+00])\n",
      "new positions: tensor([2.4073e-02, 4.3256e-03, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.3415) tensor(0.) tensor(1.7113)\n",
      "h_from_s max, min, mean tensor(5.3532) tensor(0.) tensor(1.7119)\n",
      "h_from_s_denoised max, min, mean tensor(2.3115) tensor(1.0085) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(556) tensor(556)\n",
      "avg nonzero/greaterzero h from s: tensor(557) tensor(557)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(1.8880e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.0795) tensor([0.8575])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.4574e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0159) tensor([0.9721])\n",
      "mse/cosinesimilarity s and s from h tensor(1.1611e-06) tensor([1.0000])\n",
      "new positions: tensor([0.0289, 0.0415, 6.2832])\n",
      "new positions: tensor([0.0367, 0.0709, 6.2832])\n",
      "new positions: tensor([0.0499, 0.1181, 6.2832])\n",
      "new positions: tensor([0.0630, 0.1882, 6.2832])\n",
      "new positions: tensor([0.0708, 0.3010, 6.2832])\n",
      "new positions: tensor([0.0405, 0.4658, 6.2832])\n",
      "new positions: tensor([25.6497,  0.6952,  6.2832])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.7321) tensor(0.) tensor(1.7051)\n",
      "h_from_s max, min, mean tensor(3.7320) tensor(0.) tensor(1.7048)\n",
      "h_from_s_denoised max, min, mean tensor(2.3129) tensor(1.0088) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(592) tensor(592)\n",
      "avg nonzero/greaterzero h from s: tensor(593) tensor(593)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(5.0712e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.4872) tensor([0.9267])\n",
      "mse/cosinesimilarity s and s from h from s tensor(2.1350e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0107) tensor([0.9635])\n",
      "mse/cosinesimilarity s and s from h tensor(1.8534e-08) tensor([1.0000])\n",
      "new positions: tensor([24.9838,  1.1536,  5.9890])\n",
      "new positions: tensor([24.4077,  1.3249,  5.9924])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.3061) tensor(0.) tensor(1.7019)\n",
      "h_from_s max, min, mean tensor(3.2719) tensor(0.0102) tensor(1.7017)\n",
      "h_from_s_denoised max, min, mean tensor(2.3145) tensor(1.0049) tensor(1.7109)\n",
      "avg nonzero/greaterzero h from book: tensor(599) tensor(599)\n",
      "avg nonzero/greaterzero h from s: tensor(600) tensor(600)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(0.0002) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3050) tensor([0.9518])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.9109e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0031) tensor([0.9871])\n",
      "mse/cosinesimilarity s and s from h tensor(3.3995e-10) tensor([1.])\n",
      "new positions: tensor([22.1614,  1.7153,  6.0094])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.3970) tensor(0.) tensor(1.7029)\n",
      "h_from_s max, min, mean tensor(3.3986) tensor(0.) tensor(1.7032)\n",
      "h_from_s_denoised max, min, mean tensor(2.3151) tensor(1.0045) tensor(1.7109)\n",
      "avg nonzero/greaterzero h from book: tensor(599) tensor(599)\n",
      "avg nonzero/greaterzero h from s: tensor(599) tensor(599)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(8.9362e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3058) tensor([0.9517])\n",
      "mse/cosinesimilarity s and s from h from s tensor(9.8499e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0063) tensor([0.9793])\n",
      "mse/cosinesimilarity s and s from h tensor(7.2679e-07) tensor([1.0000])\n",
      "new positions: tensor([20.1545,  2.1167,  6.0297])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.5010) tensor(0.) tensor(1.7054)\n",
      "h_from_s max, min, mean tensor(3.4929) tensor(0.) tensor(1.7034)\n",
      "h_from_s_denoised max, min, mean tensor(2.3158) tensor(1.0043) tensor(1.7110)\n",
      "avg nonzero/greaterzero h from book: tensor(598) tensor(598)\n",
      "avg nonzero/greaterzero h from s: tensor(599) tensor(599)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(1.8841e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3193) tensor([0.9499])\n",
      "mse/cosinesimilarity s and s from h from s tensor(3.9774e-05) tensor([0.9999])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0136) tensor([0.9459])\n",
      "mse/cosinesimilarity s and s from h tensor(1.3641e-05) tensor([1.0000])\n",
      "new positions: tensor([19.5559,  2.5074,  5.7535])\n",
      "new positions: tensor([19.5131,  2.6403,  5.7688])\n",
      "new positions: tensor([19.6512,  2.7891,  5.7833])\n",
      "new positions: tensor([19.8748,  2.9528,  5.7992])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.4779) tensor(0.) tensor(1.7338)\n",
      "h_from_s max, min, mean tensor(3.4540) tensor(0.0086) tensor(1.7332)\n",
      "h_from_s_denoised max, min, mean tensor(2.3131) tensor(1.0004) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(598) tensor(598)\n",
      "avg nonzero/greaterzero h from s: tensor(600) tensor(600)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(4.6690e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.2895) tensor([0.9556])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.1296e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0165) tensor([0.9453])\n",
      "mse/cosinesimilarity s and s from h tensor(2.1670e-09) tensor([1.])\n",
      "new positions: tensor([20.2124,  3.2403,  5.8310])\n",
      "new positions: tensor([20.3703,  3.3308,  5.8407])\n",
      "new positions: tensor([20.5472,  3.4050,  5.8481])\n",
      "new positions: tensor([20.7313,  3.4560,  5.8521])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.6698) tensor(0.) tensor(1.7361)\n",
      "h_from_s max, min, mean tensor(3.5480) tensor(0.0043) tensor(1.7346)\n",
      "h_from_s_denoised max, min, mean tensor(2.3126) tensor(1.0002) tensor(1.7112)\n",
      "avg nonzero/greaterzero h from book: tensor(595) tensor(595)\n",
      "avg nonzero/greaterzero h from s: tensor(600) tensor(600)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(0.0043) tensor([0.9994])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3916) tensor([0.9414])\n",
      "mse/cosinesimilarity s and s from h from s tensor(6.1174e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0160) tensor([0.9477])\n",
      "mse/cosinesimilarity s and s from h tensor(6.1946e-09) tensor([1.0000])\n",
      "new positions: tensor([21.0405,  3.5425,  5.8557])\n",
      "new positions: tensor([21.1677,  3.5788,  5.8564])\n",
      "new positions: tensor([21.2519,  3.6028,  5.8569])\n",
      "new positions: tensor([21.3021,  3.6221,  6.1517])\n",
      "new positions: tensor([21.3553,  3.6381,  6.1509])\n",
      "new positions: tensor([21.4168,  3.6632,  6.1526])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.5771) tensor(0.) tensor(1.7303)\n",
      "h_from_s max, min, mean tensor(3.5244) tensor(0.0190) tensor(1.7308)\n",
      "h_from_s_denoised max, min, mean tensor(2.3162) tensor(1.0034) tensor(1.7111)\n",
      "avg nonzero/greaterzero h from book: tensor(598) tensor(598)\n",
      "avg nonzero/greaterzero h from s: tensor(600) tensor(600)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(0.0005) tensor([0.9999])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3668) tensor([0.9445])\n",
      "mse/cosinesimilarity s and s from h from s tensor(0.0001) tensor([0.9996])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0382) tensor([0.8825])\n",
      "mse/cosinesimilarity s and s from h tensor(3.4355e-07) tensor([1.0000])\n",
      "new positions: tensor([21.5387,  3.7249,  6.1612])\n",
      "new positions: tensor([21.6191,  3.7512,  0.1746])\n",
      "new positions: tensor([21.8020,  3.7904,  0.1716])\n",
      "new positions: tensor([22.0430,  3.8598,  0.1613])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.6547) tensor(0.) tensor(1.6895)\n",
      "h_from_s max, min, mean tensor(3.5981) tensor(0.0747) tensor(1.6924)\n",
      "h_from_s_denoised max, min, mean tensor(2.3139) tensor(1.0050) tensor(1.7111)\n",
      "avg nonzero/greaterzero h from book: tensor(598) tensor(598)\n",
      "avg nonzero/greaterzero h from s: tensor(600) tensor(600)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(0.0012) tensor([0.9998])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3049) tensor([0.9514])\n",
      "mse/cosinesimilarity s and s from h from s tensor(4.3997e-05) tensor([0.9999])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0244) tensor([0.9199])\n",
      "mse/cosinesimilarity s and s from h tensor(1.2358e-08) tensor([1.])\n",
      "new positions: tensor([22.3819,  3.9568,  0.1502])\n",
      "new positions: tensor([22.5000,  3.9906,  0.1519])\n",
      "new positions: tensor([22.5952,  4.0261,  0.1554])\n",
      "new positions: tensor([22.6564,  4.0450,  0.1599])\n",
      "new positions: tensor([22.7239,  4.0523,  0.1618])\n",
      "new positions: tensor([22.7860,  4.0532,  0.1666])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(3.9427) tensor(0.) tensor(1.6837)\n",
      "h_from_s max, min, mean tensor(3.7651) tensor(0.0385) tensor(1.6882)\n",
      "h_from_s_denoised max, min, mean tensor(2.3138) tensor(1.0052) tensor(1.7111)\n",
      "avg nonzero/greaterzero h from book: tensor(596) tensor(596)\n",
      "avg nonzero/greaterzero h from s: tensor(600) tensor(600)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(0.0032) tensor([0.9995])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(0.3750) tensor([0.9409])\n",
      "mse/cosinesimilarity s and s from h from s tensor(7.3845e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0196) tensor([0.9389])\n",
      "mse/cosinesimilarity s and s from h tensor(1.8367e-07) tensor([1.0000])\n",
      "new positions: tensor([22.9110,  4.0569,  0.1760])\n",
      "new positions: tensor([22.9665,  4.0750,  0.1783])\n",
      "Running cnn__Always__additive__Soft__Identity\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.6274) tensor(0.) tensor(1.8780)\n",
      "h_from_s max, min, mean tensor(5.6260) tensor(0.) tensor(1.8775)\n",
      "h_from_s_denoised max, min, mean tensor(1128.7888) tensor(0.) tensor(161.7239)\n",
      "avg nonzero/greaterzero h from book: tensor(555) tensor(555)\n",
      "avg nonzero/greaterzero h from s: tensor(555) tensor(555)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(310) tensor(310)\n",
      "mse/cosinesimilarity h from book and h from s tensor(3.2914e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(79693.5469) tensor([0.5566])\n",
      "mse/cosinesimilarity s and s from h from s tensor(5.2507e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0140) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(3.8056e-10) tensor([1.])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.6274) tensor(0.) tensor(1.8780)\n",
      "h_from_s max, min, mean tensor(5.6260) tensor(0.) tensor(1.8775)\n",
      "h_from_s_denoised max, min, mean tensor(1128.7897) tensor(0.) tensor(161.7241)\n",
      "avg nonzero/greaterzero h from book: tensor(555) tensor(555)\n",
      "avg nonzero/greaterzero h from s: tensor(555) tensor(555)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(310) tensor(310)\n",
      "mse/cosinesimilarity h from book and h from s tensor(3.2671e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(79693.6797) tensor([0.5566])\n",
      "mse/cosinesimilarity s and s from h from s tensor(2.1617e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0058) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(1.5693e-10) tensor([1.])\n",
      "new positions: tensor([1.3963e-02, 2.5721e+01, 6.2832e+00])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.2830) tensor(0.) tensor(1.8533)\n",
      "h_from_s max, min, mean tensor(5.2948) tensor(0.) tensor(1.8537)\n",
      "h_from_s_denoised max, min, mean tensor(1115.2292) tensor(0.) tensor(159.6451)\n",
      "avg nonzero/greaterzero h from book: tensor(560) tensor(560)\n",
      "avg nonzero/greaterzero h from s: tensor(560) tensor(560)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(310) tensor(310)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.4796e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(77680.2031) tensor([0.5667])\n",
      "mse/cosinesimilarity s and s from h from s tensor(7.5157e-06) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0079) tensor([0.9845])\n",
      "mse/cosinesimilarity s and s from h tensor(2.5927e-07) tensor([1.0000])\n",
      "new positions: tensor([ 0.0279, 25.6921,  6.2832])\n",
      "new positions: tensor([ 0.0419, 25.6631,  6.2832])\n",
      "new positions: tensor([ 0.0558, 25.6342,  6.2832])\n",
      "new positions: tensor([ 0.0698, 25.6052,  6.2832])\n",
      "new positions: tensor([ 0.0838, 25.5763,  6.2832])\n",
      "new positions: tensor([ 0.0977, 25.5473,  6.2832])\n",
      "new positions: tensor([ 0.1117, 25.5184,  6.2832])\n",
      "new positions: tensor([ 0.1257, 25.4894,  6.2832])\n",
      "new positions: tensor([ 0.1396, 25.4605,  6.2832])\n",
      "new positions: tensor([ 0.1396, 25.4605,  5.9892])\n",
      "new positions: tensor([ 0.1606, 25.4347,  5.9892])\n",
      "new positions: tensor([ 0.1815, 25.4090,  5.9892])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.9892])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.6951])\n",
      "new positions: tensor([ 0.2025, 25.3833,  5.9892])\n",
      "new positions: tensor([ 0.2235, 25.3575,  5.9892])\n",
      "new positions: tensor([ 0.2235, 25.3575,  5.9892])\n",
      "new positions: tensor([ 0.2235, 25.3575,  5.9892])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "new positions: tensor([ 0.2235, 25.3575,  6.2832])\n",
      "Running cnn__Always__additive__Soft__Poly(k=1.0)\n",
      "by_sparsity\n",
      "module shapes:  [(3, 3, 4), (4, 4, 5)]\n",
      "N_g     :  116\n",
      "N_patts :  2880\n",
      "N_h     :  600\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5124) tensor(0.) tensor(1.8486)\n",
      "h_from_s max, min, mean tensor(5.5109) tensor(0.) tensor(1.8481)\n",
      "h_from_s_denoised max, min, mean tensor(2.2794) tensor(1.1224) tensor(1.7207)\n",
      "avg nonzero/greaterzero h from book: tensor(550) tensor(550)\n",
      "avg nonzero/greaterzero h from s: tensor(550) tensor(550)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(3.5693e-07) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.4513) tensor([0.8410])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.5492e-10) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0027) tensor([1.])\n",
      "mse/cosinesimilarity s and s from h tensor(1.0704e-10) tensor([1.])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.5124) tensor(0.) tensor(1.8486)\n",
      "h_from_s max, min, mean tensor(5.5111) tensor(0.) tensor(1.8481)\n",
      "h_from_s_denoised max, min, mean tensor(2.2794) tensor(1.1224) tensor(1.7207)\n",
      "avg nonzero/greaterzero h from book: tensor(550) tensor(550)\n",
      "avg nonzero/greaterzero h from s: tensor(550) tensor(550)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(2.7272e-07) tensor([1.])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.4513) tensor([0.8410])\n",
      "mse/cosinesimilarity s and s from h from s tensor(3.8404e-10) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0071) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h tensor(2.7726e-10) tensor([1.0000])\n",
      "new positions: tensor([ 0.0278, 25.7327,  6.2832])\n",
      "info for each h directly after learning it\n",
      "h max, min, mean tensor(5.0195) tensor(0.) tensor(1.8211)\n",
      "h_from_s max, min, mean tensor(5.0414) tensor(0.) tensor(1.8214)\n",
      "h_from_s_denoised max, min, mean tensor(2.2788) tensor(1.1217) tensor(1.7207)\n",
      "avg nonzero/greaterzero h from book: tensor(561) tensor(561)\n",
      "avg nonzero/greaterzero h from s: tensor(561) tensor(561)\n",
      "avg nonzero/greaterzero h from s denoised: tensor(600) tensor(600)\n",
      "mse/cosinesimilarity h from book and h from s tensor(4.2660e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity h from book and h from s denoised tensor(1.2265) tensor([0.8572])\n",
      "mse/cosinesimilarity s and s from h from s tensor(1.5362e-05) tensor([1.0000])\n",
      "mse/cosinesimilarity s and s from h from s denoised tensor(0.0190) tensor([0.9623])\n",
      "mse/cosinesimilarity s and s from h tensor(1.4953e-07) tensor([1.0000])\n",
      "new positions: tensor([ 0.0555, 25.7155,  6.2832])\n",
      "new positions: tensor([ 0.0833, 25.6982,  6.2832])\n",
      "new positions: tensor([ 0.1111, 25.6810,  6.2832])\n",
      "new positions: tensor([ 0.1388, 25.6637,  6.2832])\n",
      "new positions: tensor([ 0.1666, 25.6464,  6.2832])\n",
      "new positions: tensor([ 0.1944, 25.6292,  6.2832])\n",
      "new positions: tensor([ 0.2221, 25.6119,  6.2832])\n",
      "new positions: tensor([ 0.2221, 25.6119,  6.2832])\n",
      "new positions: tensor([ 0.2221, 25.6119,  6.2832])\n",
      "new positions: tensor([ 0.2221, 25.6119,  5.9892])\n",
      "new positions: tensor([ 0.2221, 25.6119,  5.9892])\n",
      "new positions: tensor([ 0.2221, 25.6119,  5.9892])\n",
      "new positions: tensor([ 0.2221, 25.6119,  5.9892])\n",
      "new positions: tensor([ 0.2221, 25.6119,  5.6951])\n",
      "new positions: tensor([ 0.2528, 25.6122,  5.6951])\n",
      "new positions: tensor([ 0.2835, 25.6126,  5.6951])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     37\u001b[0m agent \u001b[38;5;241m=\u001b[39m MiniworldVectorhashAgent(\n\u001b[1;32m     38\u001b[0m     vectorhash\u001b[38;5;241m=\u001b[39mvh,\n\u001b[1;32m     39\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     preprocessor\u001b[38;5;241m=\u001b[39mcnn_preproc,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 3) kidnapping history\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# signature: kidnapping_test(agent, path, visibles, limits, noise_dist=None)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mkidnapping_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisibles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 4a) save history\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasedir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Projects/rl-research/vectorhash/agent.py:313\u001b[0m, in \u001b[0;36mkidnapping_test\u001b[0;34m(agent, path, visibles, limits, noise_dist)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, [action, visible] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(path, visibles)):\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visible:\n\u001b[0;32m--> 313\u001b[0m         true_img, c_o, c_s \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m         est_img \u001b[38;5;241m=\u001b[39m s_from_h_from_g(agent\u001b[38;5;241m.\u001b[39mvectorhash\u001b[38;5;241m.\u001b[39mscaffold\u001b[38;5;241m.\u001b[39mg)\n\u001b[1;32m    315\u001b[0m         history\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    316\u001b[0m             true_image\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mencode(true_img)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m    317\u001b[0m             estimated_image\u001b[38;5;241m=\u001b[39mest_img,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m             seen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m         )\n",
      "File \u001b[0;32m~/Projects/rl-research/vectorhash/agent.py:118\u001b[0m, in \u001b[0;36mVectorhashAgent.step\u001b[0;34m(self, action, limits, noise_dist)\u001b[0m\n\u001b[1;32m    111\u001b[0m scaffold\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m scaffold\u001b[38;5;241m.\u001b[39mmodules_from_g(g_o)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m### estimate sensory certainty\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m### the reason why we take the first element because the hippocampal from sensory fucntion takes a batch of input B and outputs a batch in this case there is no batch so the function will add a batch dimension which is 1\u001b[39;00m\n\u001b[1;32m    116\u001b[0m g_s \u001b[38;5;241m=\u001b[39m scaffold\u001b[38;5;241m.\u001b[39mdenoise(\n\u001b[1;32m    117\u001b[0m     scaffold\u001b[38;5;241m.\u001b[39mgrid_from_hippocampal(\n\u001b[0;32m--> 118\u001b[0m         hs_layer\u001b[38;5;241m.\u001b[39mhippocampal_from_sensory(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_img\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    120\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m sensory_certainty \u001b[38;5;241m=\u001b[39m scaffold\u001b[38;5;241m.\u001b[39mestimate_certainty(limits\u001b[38;5;241m=\u001b[39mlimits, g\u001b[38;5;241m=\u001b[39mg_s)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m### get new position distribution\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rl-research/vectorhash/preprocessing_cnn.py:149\u001b[0m, in \u001b[0;36mPreprocessingCNN.encode\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# 5) run through encoder → (1, latent_dim) → squeeze → (latent_dim,)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m x \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 149\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agent_history import VectorhashAgentKidnappedHistory\n",
    "# ── Run CNN‐only kidnapping tests ──\n",
    "\n",
    "for store_new, shift_m, hard_store, smooth in itertools.product(\n",
    "    store_opts, shift_opts, hard_opts, smooth_opts\n",
    "):\n",
    "    # descriptive run name\n",
    "    sm_str = (\n",
    "        \"Identity\"\n",
    "        if isinstance(smooth, IdentitySmoothing)\n",
    "        else (\n",
    "            f\"Poly(k={smooth.k})\"\n",
    "            if hasattr(smooth, \"k\")\n",
    "            else f\"Softmax(T={smooth.T})\" if hasattr(smooth, \"T\") else \"RatSLAM\"\n",
    "        )\n",
    "    )\n",
    "    run_name = (\n",
    "        f\"cnn__{'Always' if store_new else 'WhenNew'}\"\n",
    "        f\"__{shift_m}__{'Hard' if hard_store else 'Soft'}__{sm_str}\"\n",
    "    )\n",
    "    print(\"Running\", run_name)\n",
    "\n",
    "    # 1) make env & limits\n",
    "    env, limits = make_env()\n",
    "\n",
    "    # 2) build VectorHash + agent\n",
    "    vh = build_vectorhash_architecture(\n",
    "        shapes=shapes,\n",
    "        N_h=600,\n",
    "        input_size=128,  # CNN latent size\n",
    "        initalization_method=\"by_sparsity\",\n",
    "        limits=limits,\n",
    "        device=device,\n",
    "        shift=RatShift(device=device),\n",
    "        smoothing=smooth,\n",
    "    )\n",
    "    agent = MiniworldVectorhashAgent(\n",
    "        vectorhash=vh,\n",
    "        env=env,\n",
    "        hard_store=hard_store,\n",
    "        store_new=store_new,\n",
    "        shift_method=shift_m,\n",
    "        preprocessor=cnn_preproc,\n",
    "    )\n",
    "\n",
    "    # 3) kidnapping history\n",
    "    # signature: kidnapping_test(agent, path, visibles, limits, noise_dist=None)\n",
    "    hist = kidnapping_test(agent, path, visibles, limits / 10)\n",
    "\n",
    "    # 4a) save history\n",
    "    with open(f\"{basedir}/{run_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(hist, f)\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m run_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlways\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mstore_new\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhenNew\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshift_m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHard\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mhard_store\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSoft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msm_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasedir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 18\u001b[0m     hist: VectorhashAgentKidnappedHistory \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 4b) plot error curve\u001b[39;00m\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "for store_new, shift_m, hard_store, smooth in itertools.product(\n",
    "    store_opts, shift_opts, hard_opts, smooth_opts\n",
    "):\n",
    "    sm_str = (\n",
    "        \"Identity\"\n",
    "        if isinstance(smooth, IdentitySmoothing)\n",
    "        else (\n",
    "            f\"Poly(k={smooth.k})\"\n",
    "            if hasattr(smooth, \"k\")\n",
    "            else f\"Softmax(T={smooth.T})\" if hasattr(smooth, \"T\") else \"RatSLAM\"\n",
    "        )\n",
    "    )\n",
    "    run_name = (\n",
    "        f\"cnn__{'Always' if store_new else 'WhenNew'}\"\n",
    "        f\"__{shift_m}__{'Hard' if hard_store else 'Soft'}__{sm_str}\"\n",
    "    )\n",
    "    with open(f\"{basedir}/{run_name}.pkl\", \"rb\") as f:\n",
    "        hist: VectorhashAgentKidnappedHistory = pickle.load(f)\n",
    "    # 4b) plot error curve\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    errors = hist.calculate_errors() # (x/y/theta, N)\n",
    "    plt.plot(errors[0], label=run_name)\n",
    "    plt.title(run_name)\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Position error\")\n",
    "    plt.legend(fontsize=\"x-small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{basedir}/{run_name}_error.png\", dpi=120)\n",
    "    plt.close()\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
